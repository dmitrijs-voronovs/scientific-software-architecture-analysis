id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/scverse/scanpy/pull/1359:315,performance,perform,performance,315,"CC @flying-sheep this is an untested as I don't have a windows machine handy to trigger the platform-int-size problem. I'm also somewhat guessing at the fix! From looking at the scanpy source, I don't think that changing the `dtype` of `ns` to a platform consistent and wider `int` will do anything catastrophic to performance or alter the logic in the alg in which it's used as it seems to be a simple index. Hope this helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1359
https://github.com/scverse/scanpy/pull/1359:340,safety,log,logic,340,"CC @flying-sheep this is an untested as I don't have a windows machine handy to trigger the platform-int-size problem. I'm also somewhat guessing at the fix! From looking at the scanpy source, I don't think that changing the `dtype` of `ns` to a platform consistent and wider `int` will do anything catastrophic to performance or alter the logic in the alg in which it's used as it seems to be a simple index. Hope this helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1359
https://github.com/scverse/scanpy/pull/1359:340,security,log,logic,340,"CC @flying-sheep this is an untested as I don't have a windows machine handy to trigger the platform-int-size problem. I'm also somewhat guessing at the fix! From looking at the scanpy source, I don't think that changing the `dtype` of `ns` to a platform consistent and wider `int` will do anything catastrophic to performance or alter the logic in the alg in which it's used as it seems to be a simple index. Hope this helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1359
https://github.com/scverse/scanpy/pull/1359:340,testability,log,logic,340,"CC @flying-sheep this is an untested as I don't have a windows machine handy to trigger the platform-int-size problem. I'm also somewhat guessing at the fix! From looking at the scanpy source, I don't think that changing the `dtype` of `ns` to a platform consistent and wider `int` will do anything catastrophic to performance or alter the logic in the alg in which it's used as it seems to be a simple index. Hope this helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1359
https://github.com/scverse/scanpy/pull/1359:396,testability,simpl,simple,396,"CC @flying-sheep this is an untested as I don't have a windows machine handy to trigger the platform-int-size problem. I'm also somewhat guessing at the fix! From looking at the scanpy source, I don't think that changing the `dtype` of `ns` to a platform consistent and wider `int` will do anything catastrophic to performance or alter the logic in the alg in which it's used as it seems to be a simple index. Hope this helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1359
https://github.com/scverse/scanpy/pull/1359:255,usability,consist,consistent,255,"CC @flying-sheep this is an untested as I don't have a windows machine handy to trigger the platform-int-size problem. I'm also somewhat guessing at the fix! From looking at the scanpy source, I don't think that changing the `dtype` of `ns` to a platform consistent and wider `int` will do anything catastrophic to performance or alter the logic in the alg in which it's used as it seems to be a simple index. Hope this helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1359
https://github.com/scverse/scanpy/pull/1359:315,usability,perform,performance,315,"CC @flying-sheep this is an untested as I don't have a windows machine handy to trigger the platform-int-size problem. I'm also somewhat guessing at the fix! From looking at the scanpy source, I don't think that changing the `dtype` of `ns` to a platform consistent and wider `int` will do anything catastrophic to performance or alter the logic in the alg in which it's used as it seems to be a simple index. Hope this helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1359
https://github.com/scverse/scanpy/pull/1359:396,usability,simpl,simple,396,"CC @flying-sheep this is an untested as I don't have a windows machine handy to trigger the platform-int-size problem. I'm also somewhat guessing at the fix! From looking at the scanpy source, I don't think that changing the `dtype` of `ns` to a platform consistent and wider `int` will do anything catastrophic to performance or alter the logic in the alg in which it's used as it seems to be a simple index. Hope this helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1359
https://github.com/scverse/scanpy/pull/1359:420,usability,help,helps,420,"CC @flying-sheep this is an untested as I don't have a windows machine handy to trigger the platform-int-size problem. I'm also somewhat guessing at the fix! From looking at the scanpy source, I don't think that changing the `dtype` of `ns` to a platform consistent and wider `int` will do anything catastrophic to performance or alter the logic in the alg in which it's used as it seems to be a simple index. Hope this helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1359
https://github.com/scverse/scanpy/issues/1360:0,reliability,Doe,Does,0,Does `sc.get.rank_genes_groups_df` do what you're looking for?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1360
https://github.com/scverse/scanpy/issues/1360:284,deployability,api,api,284,"Hi @ivirshup,. This looks like a great function but it's not super clear what the ```group``` arg is. Is it supposed to be one of the levels of the ```groupby``` arg in ```sc.tl.rank_genes_groups```? I would guess so based on the example here: https://scanpy.readthedocs.io/en/stable/api/scanpy.get.rank_genes_groups_df.html but that does not work for me. ```. # compare expression levels of mel vs all other cell types in pairwise manner. sc.tl.rank_genes_groups(noncycling_adult, groupby='class_1', groups = ['T-cell', 'eccrine', 'mel', 'dendritic', 'krt'], reference = 'mel', key_added='DE_results', method = 'wilcoxon'). results = sc.get.rank_genes_groups_df(noncycling_adult, key = 'DE_results', group = 'mel'). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-32-73add1f79f3a> in <module>. 1 # save as a data frame. 2 . ----> 3 results = sc.get.rank_genes_groups_df(noncycling_adult, key = 'DE_results', group = 'mel'). 4 . 5 . ~/software/pkg/miniconda3/envs/melanocyte_env/lib/python3.7/site-packages/scanpy/get.py in rank_genes_groups_df(adata, group, key, pval_cutoff, log2fc_min, log2fc_max, gene_symbols). 53 d = pd.DataFrame(). 54 for k in ['scores', 'names', 'logfoldchanges', 'pvals', 'pvals_adj']:. ---> 55 d[k] = adata.uns[""rank_genes_groups""][k][group]. 56 if pval_cutoff is not None:. 57 d = d[d[""pvals_adj""] < pval_cutoff]. ~/software/pkg/miniconda3/envs/melanocyte_env/lib/python3.7/site-packages/numpy/core/records.py in __getitem__(self, indx). 517 . 518 def __getitem__(self, indx):. --> 519 obj = super(recarray, self).__getitem__(indx). 520 . 521 # copy behavior of getattr, except that here. ValueError: no field of name mel. ```. Thanks for your help",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1360
https://github.com/scverse/scanpy/issues/1360:876,deployability,modul,module,876,"Hi @ivirshup,. This looks like a great function but it's not super clear what the ```group``` arg is. Is it supposed to be one of the levels of the ```groupby``` arg in ```sc.tl.rank_genes_groups```? I would guess so based on the example here: https://scanpy.readthedocs.io/en/stable/api/scanpy.get.rank_genes_groups_df.html but that does not work for me. ```. # compare expression levels of mel vs all other cell types in pairwise manner. sc.tl.rank_genes_groups(noncycling_adult, groupby='class_1', groups = ['T-cell', 'eccrine', 'mel', 'dendritic', 'krt'], reference = 'mel', key_added='DE_results', method = 'wilcoxon'). results = sc.get.rank_genes_groups_df(noncycling_adult, key = 'DE_results', group = 'mel'). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-32-73add1f79f3a> in <module>. 1 # save as a data frame. 2 . ----> 3 results = sc.get.rank_genes_groups_df(noncycling_adult, key = 'DE_results', group = 'mel'). 4 . 5 . ~/software/pkg/miniconda3/envs/melanocyte_env/lib/python3.7/site-packages/scanpy/get.py in rank_genes_groups_df(adata, group, key, pval_cutoff, log2fc_min, log2fc_max, gene_symbols). 53 d = pd.DataFrame(). 54 for k in ['scores', 'names', 'logfoldchanges', 'pvals', 'pvals_adj']:. ---> 55 d[k] = adata.uns[""rank_genes_groups""][k][group]. 56 if pval_cutoff is not None:. 57 d = d[d[""pvals_adj""] < pval_cutoff]. ~/software/pkg/miniconda3/envs/melanocyte_env/lib/python3.7/site-packages/numpy/core/records.py in __getitem__(self, indx). 517 . 518 def __getitem__(self, indx):. --> 519 obj = super(recarray, self).__getitem__(indx). 520 . 521 # copy behavior of getattr, except that here. ValueError: no field of name mel. ```. Thanks for your help",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1360
https://github.com/scverse/scanpy/issues/1360:1262,deployability,log,logfoldchanges,1262,"Hi @ivirshup,. This looks like a great function but it's not super clear what the ```group``` arg is. Is it supposed to be one of the levels of the ```groupby``` arg in ```sc.tl.rank_genes_groups```? I would guess so based on the example here: https://scanpy.readthedocs.io/en/stable/api/scanpy.get.rank_genes_groups_df.html but that does not work for me. ```. # compare expression levels of mel vs all other cell types in pairwise manner. sc.tl.rank_genes_groups(noncycling_adult, groupby='class_1', groups = ['T-cell', 'eccrine', 'mel', 'dendritic', 'krt'], reference = 'mel', key_added='DE_results', method = 'wilcoxon'). results = sc.get.rank_genes_groups_df(noncycling_adult, key = 'DE_results', group = 'mel'). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-32-73add1f79f3a> in <module>. 1 # save as a data frame. 2 . ----> 3 results = sc.get.rank_genes_groups_df(noncycling_adult, key = 'DE_results', group = 'mel'). 4 . 5 . ~/software/pkg/miniconda3/envs/melanocyte_env/lib/python3.7/site-packages/scanpy/get.py in rank_genes_groups_df(adata, group, key, pval_cutoff, log2fc_min, log2fc_max, gene_symbols). 53 d = pd.DataFrame(). 54 for k in ['scores', 'names', 'logfoldchanges', 'pvals', 'pvals_adj']:. ---> 55 d[k] = adata.uns[""rank_genes_groups""][k][group]. 56 if pval_cutoff is not None:. 57 d = d[d[""pvals_adj""] < pval_cutoff]. ~/software/pkg/miniconda3/envs/melanocyte_env/lib/python3.7/site-packages/numpy/core/records.py in __getitem__(self, indx). 517 . 518 def __getitem__(self, indx):. --> 519 obj = super(recarray, self).__getitem__(indx). 520 . 521 # copy behavior of getattr, except that here. ValueError: no field of name mel. ```. Thanks for your help",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1360
https://github.com/scverse/scanpy/issues/1360:1512,energy efficiency,core,core,1512,"Hi @ivirshup,. This looks like a great function but it's not super clear what the ```group``` arg is. Is it supposed to be one of the levels of the ```groupby``` arg in ```sc.tl.rank_genes_groups```? I would guess so based on the example here: https://scanpy.readthedocs.io/en/stable/api/scanpy.get.rank_genes_groups_df.html but that does not work for me. ```. # compare expression levels of mel vs all other cell types in pairwise manner. sc.tl.rank_genes_groups(noncycling_adult, groupby='class_1', groups = ['T-cell', 'eccrine', 'mel', 'dendritic', 'krt'], reference = 'mel', key_added='DE_results', method = 'wilcoxon'). results = sc.get.rank_genes_groups_df(noncycling_adult, key = 'DE_results', group = 'mel'). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-32-73add1f79f3a> in <module>. 1 # save as a data frame. 2 . ----> 3 results = sc.get.rank_genes_groups_df(noncycling_adult, key = 'DE_results', group = 'mel'). 4 . 5 . ~/software/pkg/miniconda3/envs/melanocyte_env/lib/python3.7/site-packages/scanpy/get.py in rank_genes_groups_df(adata, group, key, pval_cutoff, log2fc_min, log2fc_max, gene_symbols). 53 d = pd.DataFrame(). 54 for k in ['scores', 'names', 'logfoldchanges', 'pvals', 'pvals_adj']:. ---> 55 d[k] = adata.uns[""rank_genes_groups""][k][group]. 56 if pval_cutoff is not None:. 57 d = d[d[""pvals_adj""] < pval_cutoff]. ~/software/pkg/miniconda3/envs/melanocyte_env/lib/python3.7/site-packages/numpy/core/records.py in __getitem__(self, indx). 517 . 518 def __getitem__(self, indx):. --> 519 obj = super(recarray, self).__getitem__(indx). 520 . 521 # copy behavior of getattr, except that here. ValueError: no field of name mel. ```. Thanks for your help",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1360
https://github.com/scverse/scanpy/issues/1360:284,integrability,api,api,284,"Hi @ivirshup,. This looks like a great function but it's not super clear what the ```group``` arg is. Is it supposed to be one of the levels of the ```groupby``` arg in ```sc.tl.rank_genes_groups```? I would guess so based on the example here: https://scanpy.readthedocs.io/en/stable/api/scanpy.get.rank_genes_groups_df.html but that does not work for me. ```. # compare expression levels of mel vs all other cell types in pairwise manner. sc.tl.rank_genes_groups(noncycling_adult, groupby='class_1', groups = ['T-cell', 'eccrine', 'mel', 'dendritic', 'krt'], reference = 'mel', key_added='DE_results', method = 'wilcoxon'). results = sc.get.rank_genes_groups_df(noncycling_adult, key = 'DE_results', group = 'mel'). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-32-73add1f79f3a> in <module>. 1 # save as a data frame. 2 . ----> 3 results = sc.get.rank_genes_groups_df(noncycling_adult, key = 'DE_results', group = 'mel'). 4 . 5 . ~/software/pkg/miniconda3/envs/melanocyte_env/lib/python3.7/site-packages/scanpy/get.py in rank_genes_groups_df(adata, group, key, pval_cutoff, log2fc_min, log2fc_max, gene_symbols). 53 d = pd.DataFrame(). 54 for k in ['scores', 'names', 'logfoldchanges', 'pvals', 'pvals_adj']:. ---> 55 d[k] = adata.uns[""rank_genes_groups""][k][group]. 56 if pval_cutoff is not None:. 57 d = d[d[""pvals_adj""] < pval_cutoff]. ~/software/pkg/miniconda3/envs/melanocyte_env/lib/python3.7/site-packages/numpy/core/records.py in __getitem__(self, indx). 517 . 518 def __getitem__(self, indx):. --> 519 obj = super(recarray, self).__getitem__(indx). 520 . 521 # copy behavior of getattr, except that here. ValueError: no field of name mel. ```. Thanks for your help",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1360
https://github.com/scverse/scanpy/issues/1360:284,interoperability,api,api,284,"Hi @ivirshup,. This looks like a great function but it's not super clear what the ```group``` arg is. Is it supposed to be one of the levels of the ```groupby``` arg in ```sc.tl.rank_genes_groups```? I would guess so based on the example here: https://scanpy.readthedocs.io/en/stable/api/scanpy.get.rank_genes_groups_df.html but that does not work for me. ```. # compare expression levels of mel vs all other cell types in pairwise manner. sc.tl.rank_genes_groups(noncycling_adult, groupby='class_1', groups = ['T-cell', 'eccrine', 'mel', 'dendritic', 'krt'], reference = 'mel', key_added='DE_results', method = 'wilcoxon'). results = sc.get.rank_genes_groups_df(noncycling_adult, key = 'DE_results', group = 'mel'). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-32-73add1f79f3a> in <module>. 1 # save as a data frame. 2 . ----> 3 results = sc.get.rank_genes_groups_df(noncycling_adult, key = 'DE_results', group = 'mel'). 4 . 5 . ~/software/pkg/miniconda3/envs/melanocyte_env/lib/python3.7/site-packages/scanpy/get.py in rank_genes_groups_df(adata, group, key, pval_cutoff, log2fc_min, log2fc_max, gene_symbols). 53 d = pd.DataFrame(). 54 for k in ['scores', 'names', 'logfoldchanges', 'pvals', 'pvals_adj']:. ---> 55 d[k] = adata.uns[""rank_genes_groups""][k][group]. 56 if pval_cutoff is not None:. 57 d = d[d[""pvals_adj""] < pval_cutoff]. ~/software/pkg/miniconda3/envs/melanocyte_env/lib/python3.7/site-packages/numpy/core/records.py in __getitem__(self, indx). 517 . 518 def __getitem__(self, indx):. --> 519 obj = super(recarray, self).__getitem__(indx). 520 . 521 # copy behavior of getattr, except that here. ValueError: no field of name mel. ```. Thanks for your help",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1360
https://github.com/scverse/scanpy/issues/1360:876,modifiability,modul,module,876,"Hi @ivirshup,. This looks like a great function but it's not super clear what the ```group``` arg is. Is it supposed to be one of the levels of the ```groupby``` arg in ```sc.tl.rank_genes_groups```? I would guess so based on the example here: https://scanpy.readthedocs.io/en/stable/api/scanpy.get.rank_genes_groups_df.html but that does not work for me. ```. # compare expression levels of mel vs all other cell types in pairwise manner. sc.tl.rank_genes_groups(noncycling_adult, groupby='class_1', groups = ['T-cell', 'eccrine', 'mel', 'dendritic', 'krt'], reference = 'mel', key_added='DE_results', method = 'wilcoxon'). results = sc.get.rank_genes_groups_df(noncycling_adult, key = 'DE_results', group = 'mel'). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-32-73add1f79f3a> in <module>. 1 # save as a data frame. 2 . ----> 3 results = sc.get.rank_genes_groups_df(noncycling_adult, key = 'DE_results', group = 'mel'). 4 . 5 . ~/software/pkg/miniconda3/envs/melanocyte_env/lib/python3.7/site-packages/scanpy/get.py in rank_genes_groups_df(adata, group, key, pval_cutoff, log2fc_min, log2fc_max, gene_symbols). 53 d = pd.DataFrame(). 54 for k in ['scores', 'names', 'logfoldchanges', 'pvals', 'pvals_adj']:. ---> 55 d[k] = adata.uns[""rank_genes_groups""][k][group]. 56 if pval_cutoff is not None:. 57 d = d[d[""pvals_adj""] < pval_cutoff]. ~/software/pkg/miniconda3/envs/melanocyte_env/lib/python3.7/site-packages/numpy/core/records.py in __getitem__(self, indx). 517 . 518 def __getitem__(self, indx):. --> 519 obj = super(recarray, self).__getitem__(indx). 520 . 521 # copy behavior of getattr, except that here. ValueError: no field of name mel. ```. Thanks for your help",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1360
https://github.com/scverse/scanpy/issues/1360:1088,modifiability,pac,packages,1088,"Hi @ivirshup,. This looks like a great function but it's not super clear what the ```group``` arg is. Is it supposed to be one of the levels of the ```groupby``` arg in ```sc.tl.rank_genes_groups```? I would guess so based on the example here: https://scanpy.readthedocs.io/en/stable/api/scanpy.get.rank_genes_groups_df.html but that does not work for me. ```. # compare expression levels of mel vs all other cell types in pairwise manner. sc.tl.rank_genes_groups(noncycling_adult, groupby='class_1', groups = ['T-cell', 'eccrine', 'mel', 'dendritic', 'krt'], reference = 'mel', key_added='DE_results', method = 'wilcoxon'). results = sc.get.rank_genes_groups_df(noncycling_adult, key = 'DE_results', group = 'mel'). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-32-73add1f79f3a> in <module>. 1 # save as a data frame. 2 . ----> 3 results = sc.get.rank_genes_groups_df(noncycling_adult, key = 'DE_results', group = 'mel'). 4 . 5 . ~/software/pkg/miniconda3/envs/melanocyte_env/lib/python3.7/site-packages/scanpy/get.py in rank_genes_groups_df(adata, group, key, pval_cutoff, log2fc_min, log2fc_max, gene_symbols). 53 d = pd.DataFrame(). 54 for k in ['scores', 'names', 'logfoldchanges', 'pvals', 'pvals_adj']:. ---> 55 d[k] = adata.uns[""rank_genes_groups""][k][group]. 56 if pval_cutoff is not None:. 57 d = d[d[""pvals_adj""] < pval_cutoff]. ~/software/pkg/miniconda3/envs/melanocyte_env/lib/python3.7/site-packages/numpy/core/records.py in __getitem__(self, indx). 517 . 518 def __getitem__(self, indx):. --> 519 obj = super(recarray, self).__getitem__(indx). 520 . 521 # copy behavior of getattr, except that here. ValueError: no field of name mel. ```. Thanks for your help",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1360
https://github.com/scverse/scanpy/issues/1360:1497,modifiability,pac,packages,1497,"Hi @ivirshup,. This looks like a great function but it's not super clear what the ```group``` arg is. Is it supposed to be one of the levels of the ```groupby``` arg in ```sc.tl.rank_genes_groups```? I would guess so based on the example here: https://scanpy.readthedocs.io/en/stable/api/scanpy.get.rank_genes_groups_df.html but that does not work for me. ```. # compare expression levels of mel vs all other cell types in pairwise manner. sc.tl.rank_genes_groups(noncycling_adult, groupby='class_1', groups = ['T-cell', 'eccrine', 'mel', 'dendritic', 'krt'], reference = 'mel', key_added='DE_results', method = 'wilcoxon'). results = sc.get.rank_genes_groups_df(noncycling_adult, key = 'DE_results', group = 'mel'). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-32-73add1f79f3a> in <module>. 1 # save as a data frame. 2 . ----> 3 results = sc.get.rank_genes_groups_df(noncycling_adult, key = 'DE_results', group = 'mel'). 4 . 5 . ~/software/pkg/miniconda3/envs/melanocyte_env/lib/python3.7/site-packages/scanpy/get.py in rank_genes_groups_df(adata, group, key, pval_cutoff, log2fc_min, log2fc_max, gene_symbols). 53 d = pd.DataFrame(). 54 for k in ['scores', 'names', 'logfoldchanges', 'pvals', 'pvals_adj']:. ---> 55 d[k] = adata.uns[""rank_genes_groups""][k][group]. 56 if pval_cutoff is not None:. 57 d = d[d[""pvals_adj""] < pval_cutoff]. ~/software/pkg/miniconda3/envs/melanocyte_env/lib/python3.7/site-packages/numpy/core/records.py in __getitem__(self, indx). 517 . 518 def __getitem__(self, indx):. --> 519 obj = super(recarray, self).__getitem__(indx). 520 . 521 # copy behavior of getattr, except that here. ValueError: no field of name mel. ```. Thanks for your help",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1360
https://github.com/scverse/scanpy/issues/1360:334,reliability,doe,does,334,"Hi @ivirshup,. This looks like a great function but it's not super clear what the ```group``` arg is. Is it supposed to be one of the levels of the ```groupby``` arg in ```sc.tl.rank_genes_groups```? I would guess so based on the example here: https://scanpy.readthedocs.io/en/stable/api/scanpy.get.rank_genes_groups_df.html but that does not work for me. ```. # compare expression levels of mel vs all other cell types in pairwise manner. sc.tl.rank_genes_groups(noncycling_adult, groupby='class_1', groups = ['T-cell', 'eccrine', 'mel', 'dendritic', 'krt'], reference = 'mel', key_added='DE_results', method = 'wilcoxon'). results = sc.get.rank_genes_groups_df(noncycling_adult, key = 'DE_results', group = 'mel'). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-32-73add1f79f3a> in <module>. 1 # save as a data frame. 2 . ----> 3 results = sc.get.rank_genes_groups_df(noncycling_adult, key = 'DE_results', group = 'mel'). 4 . 5 . ~/software/pkg/miniconda3/envs/melanocyte_env/lib/python3.7/site-packages/scanpy/get.py in rank_genes_groups_df(adata, group, key, pval_cutoff, log2fc_min, log2fc_max, gene_symbols). 53 d = pd.DataFrame(). 54 for k in ['scores', 'names', 'logfoldchanges', 'pvals', 'pvals_adj']:. ---> 55 d[k] = adata.uns[""rank_genes_groups""][k][group]. 56 if pval_cutoff is not None:. 57 d = d[d[""pvals_adj""] < pval_cutoff]. ~/software/pkg/miniconda3/envs/melanocyte_env/lib/python3.7/site-packages/numpy/core/records.py in __getitem__(self, indx). 517 . 518 def __getitem__(self, indx):. --> 519 obj = super(recarray, self).__getitem__(indx). 520 . 521 # copy behavior of getattr, except that here. ValueError: no field of name mel. ```. Thanks for your help",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1360
https://github.com/scverse/scanpy/issues/1360:849,safety,input,input-,849,"Hi @ivirshup,. This looks like a great function but it's not super clear what the ```group``` arg is. Is it supposed to be one of the levels of the ```groupby``` arg in ```sc.tl.rank_genes_groups```? I would guess so based on the example here: https://scanpy.readthedocs.io/en/stable/api/scanpy.get.rank_genes_groups_df.html but that does not work for me. ```. # compare expression levels of mel vs all other cell types in pairwise manner. sc.tl.rank_genes_groups(noncycling_adult, groupby='class_1', groups = ['T-cell', 'eccrine', 'mel', 'dendritic', 'krt'], reference = 'mel', key_added='DE_results', method = 'wilcoxon'). results = sc.get.rank_genes_groups_df(noncycling_adult, key = 'DE_results', group = 'mel'). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-32-73add1f79f3a> in <module>. 1 # save as a data frame. 2 . ----> 3 results = sc.get.rank_genes_groups_df(noncycling_adult, key = 'DE_results', group = 'mel'). 4 . 5 . ~/software/pkg/miniconda3/envs/melanocyte_env/lib/python3.7/site-packages/scanpy/get.py in rank_genes_groups_df(adata, group, key, pval_cutoff, log2fc_min, log2fc_max, gene_symbols). 53 d = pd.DataFrame(). 54 for k in ['scores', 'names', 'logfoldchanges', 'pvals', 'pvals_adj']:. ---> 55 d[k] = adata.uns[""rank_genes_groups""][k][group]. 56 if pval_cutoff is not None:. 57 d = d[d[""pvals_adj""] < pval_cutoff]. ~/software/pkg/miniconda3/envs/melanocyte_env/lib/python3.7/site-packages/numpy/core/records.py in __getitem__(self, indx). 517 . 518 def __getitem__(self, indx):. --> 519 obj = super(recarray, self).__getitem__(indx). 520 . 521 # copy behavior of getattr, except that here. ValueError: no field of name mel. ```. Thanks for your help",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1360
https://github.com/scverse/scanpy/issues/1360:876,safety,modul,module,876,"Hi @ivirshup,. This looks like a great function but it's not super clear what the ```group``` arg is. Is it supposed to be one of the levels of the ```groupby``` arg in ```sc.tl.rank_genes_groups```? I would guess so based on the example here: https://scanpy.readthedocs.io/en/stable/api/scanpy.get.rank_genes_groups_df.html but that does not work for me. ```. # compare expression levels of mel vs all other cell types in pairwise manner. sc.tl.rank_genes_groups(noncycling_adult, groupby='class_1', groups = ['T-cell', 'eccrine', 'mel', 'dendritic', 'krt'], reference = 'mel', key_added='DE_results', method = 'wilcoxon'). results = sc.get.rank_genes_groups_df(noncycling_adult, key = 'DE_results', group = 'mel'). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-32-73add1f79f3a> in <module>. 1 # save as a data frame. 2 . ----> 3 results = sc.get.rank_genes_groups_df(noncycling_adult, key = 'DE_results', group = 'mel'). 4 . 5 . ~/software/pkg/miniconda3/envs/melanocyte_env/lib/python3.7/site-packages/scanpy/get.py in rank_genes_groups_df(adata, group, key, pval_cutoff, log2fc_min, log2fc_max, gene_symbols). 53 d = pd.DataFrame(). 54 for k in ['scores', 'names', 'logfoldchanges', 'pvals', 'pvals_adj']:. ---> 55 d[k] = adata.uns[""rank_genes_groups""][k][group]. 56 if pval_cutoff is not None:. 57 d = d[d[""pvals_adj""] < pval_cutoff]. ~/software/pkg/miniconda3/envs/melanocyte_env/lib/python3.7/site-packages/numpy/core/records.py in __getitem__(self, indx). 517 . 518 def __getitem__(self, indx):. --> 519 obj = super(recarray, self).__getitem__(indx). 520 . 521 # copy behavior of getattr, except that here. ValueError: no field of name mel. ```. Thanks for your help",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1360
https://github.com/scverse/scanpy/issues/1360:1262,safety,log,logfoldchanges,1262,"Hi @ivirshup,. This looks like a great function but it's not super clear what the ```group``` arg is. Is it supposed to be one of the levels of the ```groupby``` arg in ```sc.tl.rank_genes_groups```? I would guess so based on the example here: https://scanpy.readthedocs.io/en/stable/api/scanpy.get.rank_genes_groups_df.html but that does not work for me. ```. # compare expression levels of mel vs all other cell types in pairwise manner. sc.tl.rank_genes_groups(noncycling_adult, groupby='class_1', groups = ['T-cell', 'eccrine', 'mel', 'dendritic', 'krt'], reference = 'mel', key_added='DE_results', method = 'wilcoxon'). results = sc.get.rank_genes_groups_df(noncycling_adult, key = 'DE_results', group = 'mel'). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-32-73add1f79f3a> in <module>. 1 # save as a data frame. 2 . ----> 3 results = sc.get.rank_genes_groups_df(noncycling_adult, key = 'DE_results', group = 'mel'). 4 . 5 . ~/software/pkg/miniconda3/envs/melanocyte_env/lib/python3.7/site-packages/scanpy/get.py in rank_genes_groups_df(adata, group, key, pval_cutoff, log2fc_min, log2fc_max, gene_symbols). 53 d = pd.DataFrame(). 54 for k in ['scores', 'names', 'logfoldchanges', 'pvals', 'pvals_adj']:. ---> 55 d[k] = adata.uns[""rank_genes_groups""][k][group]. 56 if pval_cutoff is not None:. 57 d = d[d[""pvals_adj""] < pval_cutoff]. ~/software/pkg/miniconda3/envs/melanocyte_env/lib/python3.7/site-packages/numpy/core/records.py in __getitem__(self, indx). 517 . 518 def __getitem__(self, indx):. --> 519 obj = super(recarray, self).__getitem__(indx). 520 . 521 # copy behavior of getattr, except that here. ValueError: no field of name mel. ```. Thanks for your help",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1360
https://github.com/scverse/scanpy/issues/1360:1689,safety,except,except,1689,"Hi @ivirshup,. This looks like a great function but it's not super clear what the ```group``` arg is. Is it supposed to be one of the levels of the ```groupby``` arg in ```sc.tl.rank_genes_groups```? I would guess so based on the example here: https://scanpy.readthedocs.io/en/stable/api/scanpy.get.rank_genes_groups_df.html but that does not work for me. ```. # compare expression levels of mel vs all other cell types in pairwise manner. sc.tl.rank_genes_groups(noncycling_adult, groupby='class_1', groups = ['T-cell', 'eccrine', 'mel', 'dendritic', 'krt'], reference = 'mel', key_added='DE_results', method = 'wilcoxon'). results = sc.get.rank_genes_groups_df(noncycling_adult, key = 'DE_results', group = 'mel'). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-32-73add1f79f3a> in <module>. 1 # save as a data frame. 2 . ----> 3 results = sc.get.rank_genes_groups_df(noncycling_adult, key = 'DE_results', group = 'mel'). 4 . 5 . ~/software/pkg/miniconda3/envs/melanocyte_env/lib/python3.7/site-packages/scanpy/get.py in rank_genes_groups_df(adata, group, key, pval_cutoff, log2fc_min, log2fc_max, gene_symbols). 53 d = pd.DataFrame(). 54 for k in ['scores', 'names', 'logfoldchanges', 'pvals', 'pvals_adj']:. ---> 55 d[k] = adata.uns[""rank_genes_groups""][k][group]. 56 if pval_cutoff is not None:. 57 d = d[d[""pvals_adj""] < pval_cutoff]. ~/software/pkg/miniconda3/envs/melanocyte_env/lib/python3.7/site-packages/numpy/core/records.py in __getitem__(self, indx). 517 . 518 def __getitem__(self, indx):. --> 519 obj = super(recarray, self).__getitem__(indx). 520 . 521 # copy behavior of getattr, except that here. ValueError: no field of name mel. ```. Thanks for your help",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1360
https://github.com/scverse/scanpy/issues/1360:1262,security,log,logfoldchanges,1262,"Hi @ivirshup,. This looks like a great function but it's not super clear what the ```group``` arg is. Is it supposed to be one of the levels of the ```groupby``` arg in ```sc.tl.rank_genes_groups```? I would guess so based on the example here: https://scanpy.readthedocs.io/en/stable/api/scanpy.get.rank_genes_groups_df.html but that does not work for me. ```. # compare expression levels of mel vs all other cell types in pairwise manner. sc.tl.rank_genes_groups(noncycling_adult, groupby='class_1', groups = ['T-cell', 'eccrine', 'mel', 'dendritic', 'krt'], reference = 'mel', key_added='DE_results', method = 'wilcoxon'). results = sc.get.rank_genes_groups_df(noncycling_adult, key = 'DE_results', group = 'mel'). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-32-73add1f79f3a> in <module>. 1 # save as a data frame. 2 . ----> 3 results = sc.get.rank_genes_groups_df(noncycling_adult, key = 'DE_results', group = 'mel'). 4 . 5 . ~/software/pkg/miniconda3/envs/melanocyte_env/lib/python3.7/site-packages/scanpy/get.py in rank_genes_groups_df(adata, group, key, pval_cutoff, log2fc_min, log2fc_max, gene_symbols). 53 d = pd.DataFrame(). 54 for k in ['scores', 'names', 'logfoldchanges', 'pvals', 'pvals_adj']:. ---> 55 d[k] = adata.uns[""rank_genes_groups""][k][group]. 56 if pval_cutoff is not None:. 57 d = d[d[""pvals_adj""] < pval_cutoff]. ~/software/pkg/miniconda3/envs/melanocyte_env/lib/python3.7/site-packages/numpy/core/records.py in __getitem__(self, indx). 517 . 518 def __getitem__(self, indx):. --> 519 obj = super(recarray, self).__getitem__(indx). 520 . 521 # copy behavior of getattr, except that here. ValueError: no field of name mel. ```. Thanks for your help",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1360
https://github.com/scverse/scanpy/issues/1360:805,testability,Trace,Traceback,805,"Hi @ivirshup,. This looks like a great function but it's not super clear what the ```group``` arg is. Is it supposed to be one of the levels of the ```groupby``` arg in ```sc.tl.rank_genes_groups```? I would guess so based on the example here: https://scanpy.readthedocs.io/en/stable/api/scanpy.get.rank_genes_groups_df.html but that does not work for me. ```. # compare expression levels of mel vs all other cell types in pairwise manner. sc.tl.rank_genes_groups(noncycling_adult, groupby='class_1', groups = ['T-cell', 'eccrine', 'mel', 'dendritic', 'krt'], reference = 'mel', key_added='DE_results', method = 'wilcoxon'). results = sc.get.rank_genes_groups_df(noncycling_adult, key = 'DE_results', group = 'mel'). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-32-73add1f79f3a> in <module>. 1 # save as a data frame. 2 . ----> 3 results = sc.get.rank_genes_groups_df(noncycling_adult, key = 'DE_results', group = 'mel'). 4 . 5 . ~/software/pkg/miniconda3/envs/melanocyte_env/lib/python3.7/site-packages/scanpy/get.py in rank_genes_groups_df(adata, group, key, pval_cutoff, log2fc_min, log2fc_max, gene_symbols). 53 d = pd.DataFrame(). 54 for k in ['scores', 'names', 'logfoldchanges', 'pvals', 'pvals_adj']:. ---> 55 d[k] = adata.uns[""rank_genes_groups""][k][group]. 56 if pval_cutoff is not None:. 57 d = d[d[""pvals_adj""] < pval_cutoff]. ~/software/pkg/miniconda3/envs/melanocyte_env/lib/python3.7/site-packages/numpy/core/records.py in __getitem__(self, indx). 517 . 518 def __getitem__(self, indx):. --> 519 obj = super(recarray, self).__getitem__(indx). 520 . 521 # copy behavior of getattr, except that here. ValueError: no field of name mel. ```. Thanks for your help",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1360
https://github.com/scverse/scanpy/issues/1360:1262,testability,log,logfoldchanges,1262,"Hi @ivirshup,. This looks like a great function but it's not super clear what the ```group``` arg is. Is it supposed to be one of the levels of the ```groupby``` arg in ```sc.tl.rank_genes_groups```? I would guess so based on the example here: https://scanpy.readthedocs.io/en/stable/api/scanpy.get.rank_genes_groups_df.html but that does not work for me. ```. # compare expression levels of mel vs all other cell types in pairwise manner. sc.tl.rank_genes_groups(noncycling_adult, groupby='class_1', groups = ['T-cell', 'eccrine', 'mel', 'dendritic', 'krt'], reference = 'mel', key_added='DE_results', method = 'wilcoxon'). results = sc.get.rank_genes_groups_df(noncycling_adult, key = 'DE_results', group = 'mel'). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-32-73add1f79f3a> in <module>. 1 # save as a data frame. 2 . ----> 3 results = sc.get.rank_genes_groups_df(noncycling_adult, key = 'DE_results', group = 'mel'). 4 . 5 . ~/software/pkg/miniconda3/envs/melanocyte_env/lib/python3.7/site-packages/scanpy/get.py in rank_genes_groups_df(adata, group, key, pval_cutoff, log2fc_min, log2fc_max, gene_symbols). 53 d = pd.DataFrame(). 54 for k in ['scores', 'names', 'logfoldchanges', 'pvals', 'pvals_adj']:. ---> 55 d[k] = adata.uns[""rank_genes_groups""][k][group]. 56 if pval_cutoff is not None:. 57 d = d[d[""pvals_adj""] < pval_cutoff]. ~/software/pkg/miniconda3/envs/melanocyte_env/lib/python3.7/site-packages/numpy/core/records.py in __getitem__(self, indx). 517 . 518 def __getitem__(self, indx):. --> 519 obj = super(recarray, self).__getitem__(indx). 520 . 521 # copy behavior of getattr, except that here. ValueError: no field of name mel. ```. Thanks for your help",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1360
https://github.com/scverse/scanpy/issues/1360:67,usability,clear,clear,67,"Hi @ivirshup,. This looks like a great function but it's not super clear what the ```group``` arg is. Is it supposed to be one of the levels of the ```groupby``` arg in ```sc.tl.rank_genes_groups```? I would guess so based on the example here: https://scanpy.readthedocs.io/en/stable/api/scanpy.get.rank_genes_groups_df.html but that does not work for me. ```. # compare expression levels of mel vs all other cell types in pairwise manner. sc.tl.rank_genes_groups(noncycling_adult, groupby='class_1', groups = ['T-cell', 'eccrine', 'mel', 'dendritic', 'krt'], reference = 'mel', key_added='DE_results', method = 'wilcoxon'). results = sc.get.rank_genes_groups_df(noncycling_adult, key = 'DE_results', group = 'mel'). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-32-73add1f79f3a> in <module>. 1 # save as a data frame. 2 . ----> 3 results = sc.get.rank_genes_groups_df(noncycling_adult, key = 'DE_results', group = 'mel'). 4 . 5 . ~/software/pkg/miniconda3/envs/melanocyte_env/lib/python3.7/site-packages/scanpy/get.py in rank_genes_groups_df(adata, group, key, pval_cutoff, log2fc_min, log2fc_max, gene_symbols). 53 d = pd.DataFrame(). 54 for k in ['scores', 'names', 'logfoldchanges', 'pvals', 'pvals_adj']:. ---> 55 d[k] = adata.uns[""rank_genes_groups""][k][group]. 56 if pval_cutoff is not None:. 57 d = d[d[""pvals_adj""] < pval_cutoff]. ~/software/pkg/miniconda3/envs/melanocyte_env/lib/python3.7/site-packages/numpy/core/records.py in __getitem__(self, indx). 517 . 518 def __getitem__(self, indx):. --> 519 obj = super(recarray, self).__getitem__(indx). 520 . 521 # copy behavior of getattr, except that here. ValueError: no field of name mel. ```. Thanks for your help",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1360
https://github.com/scverse/scanpy/issues/1360:849,usability,input,input-,849,"Hi @ivirshup,. This looks like a great function but it's not super clear what the ```group``` arg is. Is it supposed to be one of the levels of the ```groupby``` arg in ```sc.tl.rank_genes_groups```? I would guess so based on the example here: https://scanpy.readthedocs.io/en/stable/api/scanpy.get.rank_genes_groups_df.html but that does not work for me. ```. # compare expression levels of mel vs all other cell types in pairwise manner. sc.tl.rank_genes_groups(noncycling_adult, groupby='class_1', groups = ['T-cell', 'eccrine', 'mel', 'dendritic', 'krt'], reference = 'mel', key_added='DE_results', method = 'wilcoxon'). results = sc.get.rank_genes_groups_df(noncycling_adult, key = 'DE_results', group = 'mel'). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-32-73add1f79f3a> in <module>. 1 # save as a data frame. 2 . ----> 3 results = sc.get.rank_genes_groups_df(noncycling_adult, key = 'DE_results', group = 'mel'). 4 . 5 . ~/software/pkg/miniconda3/envs/melanocyte_env/lib/python3.7/site-packages/scanpy/get.py in rank_genes_groups_df(adata, group, key, pval_cutoff, log2fc_min, log2fc_max, gene_symbols). 53 d = pd.DataFrame(). 54 for k in ['scores', 'names', 'logfoldchanges', 'pvals', 'pvals_adj']:. ---> 55 d[k] = adata.uns[""rank_genes_groups""][k][group]. 56 if pval_cutoff is not None:. 57 d = d[d[""pvals_adj""] < pval_cutoff]. ~/software/pkg/miniconda3/envs/melanocyte_env/lib/python3.7/site-packages/numpy/core/records.py in __getitem__(self, indx). 517 . 518 def __getitem__(self, indx):. --> 519 obj = super(recarray, self).__getitem__(indx). 520 . 521 # copy behavior of getattr, except that here. ValueError: no field of name mel. ```. Thanks for your help",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1360
https://github.com/scverse/scanpy/issues/1360:1668,usability,behavi,behavior,1668,"Hi @ivirshup,. This looks like a great function but it's not super clear what the ```group``` arg is. Is it supposed to be one of the levels of the ```groupby``` arg in ```sc.tl.rank_genes_groups```? I would guess so based on the example here: https://scanpy.readthedocs.io/en/stable/api/scanpy.get.rank_genes_groups_df.html but that does not work for me. ```. # compare expression levels of mel vs all other cell types in pairwise manner. sc.tl.rank_genes_groups(noncycling_adult, groupby='class_1', groups = ['T-cell', 'eccrine', 'mel', 'dendritic', 'krt'], reference = 'mel', key_added='DE_results', method = 'wilcoxon'). results = sc.get.rank_genes_groups_df(noncycling_adult, key = 'DE_results', group = 'mel'). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-32-73add1f79f3a> in <module>. 1 # save as a data frame. 2 . ----> 3 results = sc.get.rank_genes_groups_df(noncycling_adult, key = 'DE_results', group = 'mel'). 4 . 5 . ~/software/pkg/miniconda3/envs/melanocyte_env/lib/python3.7/site-packages/scanpy/get.py in rank_genes_groups_df(adata, group, key, pval_cutoff, log2fc_min, log2fc_max, gene_symbols). 53 d = pd.DataFrame(). 54 for k in ['scores', 'names', 'logfoldchanges', 'pvals', 'pvals_adj']:. ---> 55 d[k] = adata.uns[""rank_genes_groups""][k][group]. 56 if pval_cutoff is not None:. 57 d = d[d[""pvals_adj""] < pval_cutoff]. ~/software/pkg/miniconda3/envs/melanocyte_env/lib/python3.7/site-packages/numpy/core/records.py in __getitem__(self, indx). 517 . 518 def __getitem__(self, indx):. --> 519 obj = super(recarray, self).__getitem__(indx). 520 . 521 # copy behavior of getattr, except that here. ValueError: no field of name mel. ```. Thanks for your help",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1360
https://github.com/scverse/scanpy/issues/1360:1762,usability,help,help,1762,"Hi @ivirshup,. This looks like a great function but it's not super clear what the ```group``` arg is. Is it supposed to be one of the levels of the ```groupby``` arg in ```sc.tl.rank_genes_groups```? I would guess so based on the example here: https://scanpy.readthedocs.io/en/stable/api/scanpy.get.rank_genes_groups_df.html but that does not work for me. ```. # compare expression levels of mel vs all other cell types in pairwise manner. sc.tl.rank_genes_groups(noncycling_adult, groupby='class_1', groups = ['T-cell', 'eccrine', 'mel', 'dendritic', 'krt'], reference = 'mel', key_added='DE_results', method = 'wilcoxon'). results = sc.get.rank_genes_groups_df(noncycling_adult, key = 'DE_results', group = 'mel'). ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-32-73add1f79f3a> in <module>. 1 # save as a data frame. 2 . ----> 3 results = sc.get.rank_genes_groups_df(noncycling_adult, key = 'DE_results', group = 'mel'). 4 . 5 . ~/software/pkg/miniconda3/envs/melanocyte_env/lib/python3.7/site-packages/scanpy/get.py in rank_genes_groups_df(adata, group, key, pval_cutoff, log2fc_min, log2fc_max, gene_symbols). 53 d = pd.DataFrame(). 54 for k in ['scores', 'names', 'logfoldchanges', 'pvals', 'pvals_adj']:. ---> 55 d[k] = adata.uns[""rank_genes_groups""][k][group]. 56 if pval_cutoff is not None:. 57 d = d[d[""pvals_adj""] < pval_cutoff]. ~/software/pkg/miniconda3/envs/melanocyte_env/lib/python3.7/site-packages/numpy/core/records.py in __getitem__(self, indx). 517 . 518 def __getitem__(self, indx):. --> 519 obj = super(recarray, self).__getitem__(indx). 520 . 521 # copy behavior of getattr, except that here. ValueError: no field of name mel. ```. Thanks for your help",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1360
https://github.com/scverse/scanpy/issues/1360:159,integrability,sub,submission,159,Also posted here: https://scanpy.discourse.group/t/sc-tl-rank-genes-groups-specify-groups-and-implementation-for-multiple-tests/328 to try to follow the issue submission guidelines. . Expanded documentation on how to to use ```sc.tl.rank_genes_groups``` in conjunction with ```sc.get.rank_genes_groups_df``` would be much appreciated.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1360
https://github.com/scverse/scanpy/issues/1360:75,interoperability,specif,specify-groups-and-implementation-for-multiple-tests,75,Also posted here: https://scanpy.discourse.group/t/sc-tl-rank-genes-groups-specify-groups-and-implementation-for-multiple-tests/328 to try to follow the issue submission guidelines. . Expanded documentation on how to to use ```sc.tl.rank_genes_groups``` in conjunction with ```sc.get.rank_genes_groups_df``` would be much appreciated.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1360
https://github.com/scverse/scanpy/issues/1360:122,safety,test,tests,122,Also posted here: https://scanpy.discourse.group/t/sc-tl-rank-genes-groups-specify-groups-and-implementation-for-multiple-tests/328 to try to follow the issue submission guidelines. . Expanded documentation on how to to use ```sc.tl.rank_genes_groups``` in conjunction with ```sc.get.rank_genes_groups_df``` would be much appreciated.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1360
https://github.com/scverse/scanpy/issues/1360:122,testability,test,tests,122,Also posted here: https://scanpy.discourse.group/t/sc-tl-rank-genes-groups-specify-groups-and-implementation-for-multiple-tests/328 to try to follow the issue submission guidelines. . Expanded documentation on how to to use ```sc.tl.rank_genes_groups``` in conjunction with ```sc.get.rank_genes_groups_df``` would be much appreciated.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1360
https://github.com/scverse/scanpy/issues/1360:170,usability,guid,guidelines,170,Also posted here: https://scanpy.discourse.group/t/sc-tl-rank-genes-groups-specify-groups-and-implementation-for-multiple-tests/328 to try to follow the issue submission guidelines. . Expanded documentation on how to to use ```sc.tl.rank_genes_groups``` in conjunction with ```sc.get.rank_genes_groups_df``` would be much appreciated.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1360
https://github.com/scverse/scanpy/issues/1360:193,usability,document,documentation,193,Also posted here: https://scanpy.discourse.group/t/sc-tl-rank-genes-groups-specify-groups-and-implementation-for-multiple-tests/328 to try to follow the issue submission guidelines. . Expanded documentation on how to to use ```sc.tl.rank_genes_groups``` in conjunction with ```sc.get.rank_genes_groups_df``` would be much appreciated.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1360
https://github.com/scverse/scanpy/issues/1361:133,deployability,scale,scaled,133,"In the described case, it is expected that `adata.raw` is set. Thus, by default `sc.pl.dotplot` will use the raw data instead of the scaled `adata.X`. In the case in which `adata.raw` is not set, then dotplot will wrongly compute the percentage. The suggestion to uncouple the dot size and the dot color is very good and after the 1.6 relase of scanpy is now possible to do this using the `dot_color_df` (see https://scanpy.readthedocs.io/en/stable/api/scanpy.pl.dotplot.html#scanpy.pl.dotplot). However, this requires that the user prepares a pandas dataframe for the color while the dot size (% of cells) can be computed based on raw, or a layer (depending on the settings for `use_raw` and `layer`). I am open to further suggestions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1361
https://github.com/scverse/scanpy/issues/1361:449,deployability,api,api,449,"In the described case, it is expected that `adata.raw` is set. Thus, by default `sc.pl.dotplot` will use the raw data instead of the scaled `adata.X`. In the case in which `adata.raw` is not set, then dotplot will wrongly compute the percentage. The suggestion to uncouple the dot size and the dot color is very good and after the 1.6 relase of scanpy is now possible to do this using the `dot_color_df` (see https://scanpy.readthedocs.io/en/stable/api/scanpy.pl.dotplot.html#scanpy.pl.dotplot). However, this requires that the user prepares a pandas dataframe for the color while the dot size (% of cells) can be computed based on raw, or a layer (depending on the settings for `use_raw` and `layer`). I am open to further suggestions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1361
https://github.com/scverse/scanpy/issues/1361:649,deployability,depend,depending,649,"In the described case, it is expected that `adata.raw` is set. Thus, by default `sc.pl.dotplot` will use the raw data instead of the scaled `adata.X`. In the case in which `adata.raw` is not set, then dotplot will wrongly compute the percentage. The suggestion to uncouple the dot size and the dot color is very good and after the 1.6 relase of scanpy is now possible to do this using the `dot_color_df` (see https://scanpy.readthedocs.io/en/stable/api/scanpy.pl.dotplot.html#scanpy.pl.dotplot). However, this requires that the user prepares a pandas dataframe for the color while the dot size (% of cells) can be computed based on raw, or a layer (depending on the settings for `use_raw` and `layer`). I am open to further suggestions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1361
https://github.com/scverse/scanpy/issues/1361:133,energy efficiency,scale,scaled,133,"In the described case, it is expected that `adata.raw` is set. Thus, by default `sc.pl.dotplot` will use the raw data instead of the scaled `adata.X`. In the case in which `adata.raw` is not set, then dotplot will wrongly compute the percentage. The suggestion to uncouple the dot size and the dot color is very good and after the 1.6 relase of scanpy is now possible to do this using the `dot_color_df` (see https://scanpy.readthedocs.io/en/stable/api/scanpy.pl.dotplot.html#scanpy.pl.dotplot). However, this requires that the user prepares a pandas dataframe for the color while the dot size (% of cells) can be computed based on raw, or a layer (depending on the settings for `use_raw` and `layer`). I am open to further suggestions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1361
https://github.com/scverse/scanpy/issues/1361:449,integrability,api,api,449,"In the described case, it is expected that `adata.raw` is set. Thus, by default `sc.pl.dotplot` will use the raw data instead of the scaled `adata.X`. In the case in which `adata.raw` is not set, then dotplot will wrongly compute the percentage. The suggestion to uncouple the dot size and the dot color is very good and after the 1.6 relase of scanpy is now possible to do this using the `dot_color_df` (see https://scanpy.readthedocs.io/en/stable/api/scanpy.pl.dotplot.html#scanpy.pl.dotplot). However, this requires that the user prepares a pandas dataframe for the color while the dot size (% of cells) can be computed based on raw, or a layer (depending on the settings for `use_raw` and `layer`). I am open to further suggestions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1361
https://github.com/scverse/scanpy/issues/1361:649,integrability,depend,depending,649,"In the described case, it is expected that `adata.raw` is set. Thus, by default `sc.pl.dotplot` will use the raw data instead of the scaled `adata.X`. In the case in which `adata.raw` is not set, then dotplot will wrongly compute the percentage. The suggestion to uncouple the dot size and the dot color is very good and after the 1.6 relase of scanpy is now possible to do this using the `dot_color_df` (see https://scanpy.readthedocs.io/en/stable/api/scanpy.pl.dotplot.html#scanpy.pl.dotplot). However, this requires that the user prepares a pandas dataframe for the color while the dot size (% of cells) can be computed based on raw, or a layer (depending on the settings for `use_raw` and `layer`). I am open to further suggestions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1361
https://github.com/scverse/scanpy/issues/1361:449,interoperability,api,api,449,"In the described case, it is expected that `adata.raw` is set. Thus, by default `sc.pl.dotplot` will use the raw data instead of the scaled `adata.X`. In the case in which `adata.raw` is not set, then dotplot will wrongly compute the percentage. The suggestion to uncouple the dot size and the dot color is very good and after the 1.6 relase of scanpy is now possible to do this using the `dot_color_df` (see https://scanpy.readthedocs.io/en/stable/api/scanpy.pl.dotplot.html#scanpy.pl.dotplot). However, this requires that the user prepares a pandas dataframe for the color while the dot size (% of cells) can be computed based on raw, or a layer (depending on the settings for `use_raw` and `layer`). I am open to further suggestions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1361
https://github.com/scverse/scanpy/issues/1361:133,modifiability,scal,scaled,133,"In the described case, it is expected that `adata.raw` is set. Thus, by default `sc.pl.dotplot` will use the raw data instead of the scaled `adata.X`. In the case in which `adata.raw` is not set, then dotplot will wrongly compute the percentage. The suggestion to uncouple the dot size and the dot color is very good and after the 1.6 relase of scanpy is now possible to do this using the `dot_color_df` (see https://scanpy.readthedocs.io/en/stable/api/scanpy.pl.dotplot.html#scanpy.pl.dotplot). However, this requires that the user prepares a pandas dataframe for the color while the dot size (% of cells) can be computed based on raw, or a layer (depending on the settings for `use_raw` and `layer`). I am open to further suggestions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1361
https://github.com/scverse/scanpy/issues/1361:642,modifiability,layer,layer,642,"In the described case, it is expected that `adata.raw` is set. Thus, by default `sc.pl.dotplot` will use the raw data instead of the scaled `adata.X`. In the case in which `adata.raw` is not set, then dotplot will wrongly compute the percentage. The suggestion to uncouple the dot size and the dot color is very good and after the 1.6 relase of scanpy is now possible to do this using the `dot_color_df` (see https://scanpy.readthedocs.io/en/stable/api/scanpy.pl.dotplot.html#scanpy.pl.dotplot). However, this requires that the user prepares a pandas dataframe for the color while the dot size (% of cells) can be computed based on raw, or a layer (depending on the settings for `use_raw` and `layer`). I am open to further suggestions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1361
https://github.com/scverse/scanpy/issues/1361:649,modifiability,depend,depending,649,"In the described case, it is expected that `adata.raw` is set. Thus, by default `sc.pl.dotplot` will use the raw data instead of the scaled `adata.X`. In the case in which `adata.raw` is not set, then dotplot will wrongly compute the percentage. The suggestion to uncouple the dot size and the dot color is very good and after the 1.6 relase of scanpy is now possible to do this using the `dot_color_df` (see https://scanpy.readthedocs.io/en/stable/api/scanpy.pl.dotplot.html#scanpy.pl.dotplot). However, this requires that the user prepares a pandas dataframe for the color while the dot size (% of cells) can be computed based on raw, or a layer (depending on the settings for `use_raw` and `layer`). I am open to further suggestions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1361
https://github.com/scverse/scanpy/issues/1361:694,modifiability,layer,layer,694,"In the described case, it is expected that `adata.raw` is set. Thus, by default `sc.pl.dotplot` will use the raw data instead of the scaled `adata.X`. In the case in which `adata.raw` is not set, then dotplot will wrongly compute the percentage. The suggestion to uncouple the dot size and the dot color is very good and after the 1.6 relase of scanpy is now possible to do this using the `dot_color_df` (see https://scanpy.readthedocs.io/en/stable/api/scanpy.pl.dotplot.html#scanpy.pl.dotplot). However, this requires that the user prepares a pandas dataframe for the color while the dot size (% of cells) can be computed based on raw, or a layer (depending on the settings for `use_raw` and `layer`). I am open to further suggestions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1361
https://github.com/scverse/scanpy/issues/1361:133,performance,scale,scaled,133,"In the described case, it is expected that `adata.raw` is set. Thus, by default `sc.pl.dotplot` will use the raw data instead of the scaled `adata.X`. In the case in which `adata.raw` is not set, then dotplot will wrongly compute the percentage. The suggestion to uncouple the dot size and the dot color is very good and after the 1.6 relase of scanpy is now possible to do this using the `dot_color_df` (see https://scanpy.readthedocs.io/en/stable/api/scanpy.pl.dotplot.html#scanpy.pl.dotplot). However, this requires that the user prepares a pandas dataframe for the color while the dot size (% of cells) can be computed based on raw, or a layer (depending on the settings for `use_raw` and `layer`). I am open to further suggestions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1361
https://github.com/scverse/scanpy/issues/1361:649,safety,depend,depending,649,"In the described case, it is expected that `adata.raw` is set. Thus, by default `sc.pl.dotplot` will use the raw data instead of the scaled `adata.X`. In the case in which `adata.raw` is not set, then dotplot will wrongly compute the percentage. The suggestion to uncouple the dot size and the dot color is very good and after the 1.6 relase of scanpy is now possible to do this using the `dot_color_df` (see https://scanpy.readthedocs.io/en/stable/api/scanpy.pl.dotplot.html#scanpy.pl.dotplot). However, this requires that the user prepares a pandas dataframe for the color while the dot size (% of cells) can be computed based on raw, or a layer (depending on the settings for `use_raw` and `layer`). I am open to further suggestions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1361
https://github.com/scverse/scanpy/issues/1361:649,testability,depend,depending,649,"In the described case, it is expected that `adata.raw` is set. Thus, by default `sc.pl.dotplot` will use the raw data instead of the scaled `adata.X`. In the case in which `adata.raw` is not set, then dotplot will wrongly compute the percentage. The suggestion to uncouple the dot size and the dot color is very good and after the 1.6 relase of scanpy is now possible to do this using the `dot_color_df` (see https://scanpy.readthedocs.io/en/stable/api/scanpy.pl.dotplot.html#scanpy.pl.dotplot). However, this requires that the user prepares a pandas dataframe for the color while the dot size (% of cells) can be computed based on raw, or a layer (depending on the settings for `use_raw` and `layer`). I am open to further suggestions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1361
https://github.com/scverse/scanpy/issues/1361:528,usability,user,user,528,"In the described case, it is expected that `adata.raw` is set. Thus, by default `sc.pl.dotplot` will use the raw data instead of the scaled `adata.X`. In the case in which `adata.raw` is not set, then dotplot will wrongly compute the percentage. The suggestion to uncouple the dot size and the dot color is very good and after the 1.6 relase of scanpy is now possible to do this using the `dot_color_df` (see https://scanpy.readthedocs.io/en/stable/api/scanpy.pl.dotplot.html#scanpy.pl.dotplot). However, this requires that the user prepares a pandas dataframe for the color while the dot size (% of cells) can be computed based on raw, or a layer (depending on the settings for `use_raw` and `layer`). I am open to further suggestions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1361
https://github.com/scverse/scanpy/pull/1362:359,energy efficiency,core,core,359,"I think this would be more appropriate in `anndata`. Since it's not an on disk format, maybe it the function could be called something like `from_starfish`? * This would also need tests, so some kind of example data. * Why the differences between this and [starfish's `save_anndata` method](https://spacetx-starfish.readthedocs.io/en/latest/_modules/starfish/core/expression_matrix/expression_matrix.html#ExpressionMatrix.save_anndata)?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1362
https://github.com/scverse/scanpy/pull/1362:79,interoperability,format,format,79,"I think this would be more appropriate in `anndata`. Since it's not an on disk format, maybe it the function could be called something like `from_starfish`? * This would also need tests, so some kind of example data. * Why the differences between this and [starfish's `save_anndata` method](https://spacetx-starfish.readthedocs.io/en/latest/_modules/starfish/core/expression_matrix/expression_matrix.html#ExpressionMatrix.save_anndata)?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1362
https://github.com/scverse/scanpy/pull/1362:74,performance,disk,disk,74,"I think this would be more appropriate in `anndata`. Since it's not an on disk format, maybe it the function could be called something like `from_starfish`? * This would also need tests, so some kind of example data. * Why the differences between this and [starfish's `save_anndata` method](https://spacetx-starfish.readthedocs.io/en/latest/_modules/starfish/core/expression_matrix/expression_matrix.html#ExpressionMatrix.save_anndata)?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1362
https://github.com/scverse/scanpy/pull/1362:180,safety,test,tests,180,"I think this would be more appropriate in `anndata`. Since it's not an on disk format, maybe it the function could be called something like `from_starfish`? * This would also need tests, so some kind of example data. * Why the differences between this and [starfish's `save_anndata` method](https://spacetx-starfish.readthedocs.io/en/latest/_modules/starfish/core/expression_matrix/expression_matrix.html#ExpressionMatrix.save_anndata)?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1362
https://github.com/scverse/scanpy/pull/1362:180,testability,test,tests,180,"I think this would be more appropriate in `anndata`. Since it's not an on disk format, maybe it the function could be called something like `from_starfish`? * This would also need tests, so some kind of example data. * Why the differences between this and [starfish's `save_anndata` method](https://spacetx-starfish.readthedocs.io/en/latest/_modules/starfish/core/expression_matrix/expression_matrix.html#ExpressionMatrix.save_anndata)?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1362
https://github.com/scverse/scanpy/pull/1362:273,availability,mask,masks,273,"So I guess basic difference is that this one can convert starfish to anndata ""online"" without having to read from disk the anndata object. Also, my understanding with `save_anndata` method is that it doesn't export all the features we might want (e.g. area of segmentation masks or others, to be stored in obs). However, this could also be fixed by sending a PR there and modifying what is saved in anndata of the expression matrix. What do you guys think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1362
https://github.com/scverse/scanpy/pull/1362:114,performance,disk,disk,114,"So I guess basic difference is that this one can convert starfish to anndata ""online"" without having to read from disk the anndata object. Also, my understanding with `save_anndata` method is that it doesn't export all the features we might want (e.g. area of segmentation masks or others, to be stored in obs). However, this could also be fixed by sending a PR there and modifying what is saved in anndata of the expression matrix. What do you guys think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1362
https://github.com/scverse/scanpy/pull/1362:200,reliability,doe,doesn,200,"So I guess basic difference is that this one can convert starfish to anndata ""online"" without having to read from disk the anndata object. Also, my understanding with `save_anndata` method is that it doesn't export all the features we might want (e.g. area of segmentation masks or others, to be stored in obs). However, this could also be fixed by sending a PR there and modifying what is saved in anndata of the expression matrix. What do you guys think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1362
https://github.com/scverse/scanpy/pull/1362:372,security,modif,modifying,372,"So I guess basic difference is that this one can convert starfish to anndata ""online"" without having to read from disk the anndata object. Also, my understanding with `save_anndata` method is that it doesn't export all the features we might want (e.g. area of segmentation masks or others, to be stored in obs). However, this could also be fixed by sending a PR there and modifying what is saved in anndata of the expression matrix. What do you guys think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1362
https://github.com/scverse/scanpy/pull/1362:148,testability,understand,understanding,148,"So I guess basic difference is that this one can convert starfish to anndata ""online"" without having to read from disk the anndata object. Also, my understanding with `save_anndata` method is that it doesn't export all the features we might want (e.g. area of segmentation masks or others, to be stored in obs). However, this could also be fixed by sending a PR there and modifying what is saved in anndata of the expression matrix. What do you guys think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1362
https://github.com/scverse/scanpy/pull/1362:437,deployability,version,version,437,"> So I guess basic difference is that this one can convert starfish to anndata ""online"" without having to read from disk the anndata object. Sure. I was also wondering why use literal values here instead of the constant variables that starfish uses (e.g. `""cells""` vs. `Features.CELLS`). > this could also be fixed by sending a PR there and modifying what is saved in anndata of the expression matrix. I like the idea of having just one version of conversion, instead of them implementing a `to_anndata` method and us implementing an independent `from_starfish` method. Are either of you already in communication with the starfish developers? . Additionally, it seems like having a `to_anndata` method would be a pretty trivial modification of their code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1362
https://github.com/scverse/scanpy/pull/1362:437,integrability,version,version,437,"> So I guess basic difference is that this one can convert starfish to anndata ""online"" without having to read from disk the anndata object. Sure. I was also wondering why use literal values here instead of the constant variables that starfish uses (e.g. `""cells""` vs. `Features.CELLS`). > this could also be fixed by sending a PR there and modifying what is saved in anndata of the expression matrix. I like the idea of having just one version of conversion, instead of them implementing a `to_anndata` method and us implementing an independent `from_starfish` method. Are either of you already in communication with the starfish developers? . Additionally, it seems like having a `to_anndata` method would be a pretty trivial modification of their code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1362
https://github.com/scverse/scanpy/pull/1362:448,interoperability,convers,conversion,448,"> So I guess basic difference is that this one can convert starfish to anndata ""online"" without having to read from disk the anndata object. Sure. I was also wondering why use literal values here instead of the constant variables that starfish uses (e.g. `""cells""` vs. `Features.CELLS`). > this could also be fixed by sending a PR there and modifying what is saved in anndata of the expression matrix. I like the idea of having just one version of conversion, instead of them implementing a `to_anndata` method and us implementing an independent `from_starfish` method. Are either of you already in communication with the starfish developers? . Additionally, it seems like having a `to_anndata` method would be a pretty trivial modification of their code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1362
https://github.com/scverse/scanpy/pull/1362:220,modifiability,variab,variables,220,"> So I guess basic difference is that this one can convert starfish to anndata ""online"" without having to read from disk the anndata object. Sure. I was also wondering why use literal values here instead of the constant variables that starfish uses (e.g. `""cells""` vs. `Features.CELLS`). > this could also be fixed by sending a PR there and modifying what is saved in anndata of the expression matrix. I like the idea of having just one version of conversion, instead of them implementing a `to_anndata` method and us implementing an independent `from_starfish` method. Are either of you already in communication with the starfish developers? . Additionally, it seems like having a `to_anndata` method would be a pretty trivial modification of their code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1362
https://github.com/scverse/scanpy/pull/1362:437,modifiability,version,version,437,"> So I guess basic difference is that this one can convert starfish to anndata ""online"" without having to read from disk the anndata object. Sure. I was also wondering why use literal values here instead of the constant variables that starfish uses (e.g. `""cells""` vs. `Features.CELLS`). > this could also be fixed by sending a PR there and modifying what is saved in anndata of the expression matrix. I like the idea of having just one version of conversion, instead of them implementing a `to_anndata` method and us implementing an independent `from_starfish` method. Are either of you already in communication with the starfish developers? . Additionally, it seems like having a `to_anndata` method would be a pretty trivial modification of their code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1362
https://github.com/scverse/scanpy/pull/1362:116,performance,disk,disk,116,"> So I guess basic difference is that this one can convert starfish to anndata ""online"" without having to read from disk the anndata object. Sure. I was also wondering why use literal values here instead of the constant variables that starfish uses (e.g. `""cells""` vs. `Features.CELLS`). > this could also be fixed by sending a PR there and modifying what is saved in anndata of the expression matrix. I like the idea of having just one version of conversion, instead of them implementing a `to_anndata` method and us implementing an independent `from_starfish` method. Are either of you already in communication with the starfish developers? . Additionally, it seems like having a `to_anndata` method would be a pretty trivial modification of their code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1362
https://github.com/scverse/scanpy/pull/1362:341,security,modif,modifying,341,"> So I guess basic difference is that this one can convert starfish to anndata ""online"" without having to read from disk the anndata object. Sure. I was also wondering why use literal values here instead of the constant variables that starfish uses (e.g. `""cells""` vs. `Features.CELLS`). > this could also be fixed by sending a PR there and modifying what is saved in anndata of the expression matrix. I like the idea of having just one version of conversion, instead of them implementing a `to_anndata` method and us implementing an independent `from_starfish` method. Are either of you already in communication with the starfish developers? . Additionally, it seems like having a `to_anndata` method would be a pretty trivial modification of their code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1362
https://github.com/scverse/scanpy/pull/1362:728,security,modif,modification,728,"> So I guess basic difference is that this one can convert starfish to anndata ""online"" without having to read from disk the anndata object. Sure. I was also wondering why use literal values here instead of the constant variables that starfish uses (e.g. `""cells""` vs. `Features.CELLS`). > this could also be fixed by sending a PR there and modifying what is saved in anndata of the expression matrix. I like the idea of having just one version of conversion, instead of them implementing a `to_anndata` method and us implementing an independent `from_starfish` method. Are either of you already in communication with the starfish developers? . Additionally, it seems like having a `to_anndata` method would be a pretty trivial modification of their code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1362
https://github.com/scverse/scanpy/pull/1362:17,usability,close,closed,17,This can also be closed. What do you think @Mirkazemi @ivirshup . I'll make a PR in original repo asap,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1362
https://github.com/scverse/scanpy/issues/1363:119,performance,time,times,119,"I do not have a small example, I just wanted to know if it is something that you knew. I noticed it because I run many times the same script, I also noticed few cells located in different positions but very small differences. Maybe it is due tho the umap algorithm. Thank you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1363
https://github.com/scverse/scanpy/issues/1363:33,interoperability,specif,specify,33,"With the same seed? If you dont specify a seed, nondeterminism is to be expected.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1363
https://github.com/scverse/scanpy/issues/1363:15,interoperability,specif,specify,15,"> If you dont specify a seed, nondeterminism is to be expected. In general, I thought we went for determinism with default arguments in scanpy. I believe pynndescent faster, if non-deterministic, but I get constant output from scanpy. <details>. <summary> Test case </summary>. ```python. import scanpy as sc. import numpy as np. pbmc = sc.datasets.pbmc3k(). sc.pp.filter_genes(pbmc, min_counts=1). sc.pp.normalize_total(pbmc). sc.pp.log1p(pbmc). adatas = [pbmc.copy() for _ in range(10)]. for adata in adatas:. sc.pp.pca(adata). sc.pp.neighbors(adata). sc.tl.umap(adata). umap_coords = [adata.obsm[""X_umap""] for adata in adatas]. assert all([np.array_equal(umap_coords[i], umap_coords[i+1]) for i in range(9)]). ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1363
https://github.com/scverse/scanpy/issues/1363:257,safety,Test,Test,257,"> If you dont specify a seed, nondeterminism is to be expected. In general, I thought we went for determinism with default arguments in scanpy. I believe pynndescent faster, if non-deterministic, but I get constant output from scanpy. <details>. <summary> Test case </summary>. ```python. import scanpy as sc. import numpy as np. pbmc = sc.datasets.pbmc3k(). sc.pp.filter_genes(pbmc, min_counts=1). sc.pp.normalize_total(pbmc). sc.pp.log1p(pbmc). adatas = [pbmc.copy() for _ in range(10)]. for adata in adatas:. sc.pp.pca(adata). sc.pp.neighbors(adata). sc.tl.umap(adata). umap_coords = [adata.obsm[""X_umap""] for adata in adatas]. assert all([np.array_equal(umap_coords[i], umap_coords[i+1]) for i in range(9)]). ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1363
https://github.com/scverse/scanpy/issues/1363:257,testability,Test,Test,257,"> If you dont specify a seed, nondeterminism is to be expected. In general, I thought we went for determinism with default arguments in scanpy. I believe pynndescent faster, if non-deterministic, but I get constant output from scanpy. <details>. <summary> Test case </summary>. ```python. import scanpy as sc. import numpy as np. pbmc = sc.datasets.pbmc3k(). sc.pp.filter_genes(pbmc, min_counts=1). sc.pp.normalize_total(pbmc). sc.pp.log1p(pbmc). adatas = [pbmc.copy() for _ in range(10)]. for adata in adatas:. sc.pp.pca(adata). sc.pp.neighbors(adata). sc.tl.umap(adata). umap_coords = [adata.obsm[""X_umap""] for adata in adatas]. assert all([np.array_equal(umap_coords[i], umap_coords[i+1]) for i in range(9)]). ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1363
https://github.com/scverse/scanpy/issues/1363:632,testability,assert,assert,632,"> If you dont specify a seed, nondeterminism is to be expected. In general, I thought we went for determinism with default arguments in scanpy. I believe pynndescent faster, if non-deterministic, but I get constant output from scanpy. <details>. <summary> Test case </summary>. ```python. import scanpy as sc. import numpy as np. pbmc = sc.datasets.pbmc3k(). sc.pp.filter_genes(pbmc, min_counts=1). sc.pp.normalize_total(pbmc). sc.pp.log1p(pbmc). adatas = [pbmc.copy() for _ in range(10)]. for adata in adatas:. sc.pp.pca(adata). sc.pp.neighbors(adata). sc.tl.umap(adata). umap_coords = [adata.obsm[""X_umap""] for adata in adatas]. assert all([np.array_equal(umap_coords[i], umap_coords[i+1]) for i in range(9)]). ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1363
https://github.com/scverse/scanpy/issues/1363:6,availability,sli,slight,6,I got slight differences with the same seed many times in the UMAP outputs when updating the single-cell-tutorial notebook for new versions of scanpy/anndata.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1363
https://github.com/scverse/scanpy/issues/1363:80,deployability,updat,updating,80,I got slight differences with the same seed many times in the UMAP outputs when updating the single-cell-tutorial notebook for new versions of scanpy/anndata.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1363
https://github.com/scverse/scanpy/issues/1363:131,deployability,version,versions,131,I got slight differences with the same seed many times in the UMAP outputs when updating the single-cell-tutorial notebook for new versions of scanpy/anndata.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1363
https://github.com/scverse/scanpy/issues/1363:131,integrability,version,versions,131,I got slight differences with the same seed many times in the UMAP outputs when updating the single-cell-tutorial notebook for new versions of scanpy/anndata.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1363
https://github.com/scverse/scanpy/issues/1363:131,modifiability,version,versions,131,I got slight differences with the same seed many times in the UMAP outputs when updating the single-cell-tutorial notebook for new versions of scanpy/anndata.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1363
https://github.com/scverse/scanpy/issues/1363:49,performance,time,times,49,I got slight differences with the same seed many times in the UMAP outputs when updating the single-cell-tutorial notebook for new versions of scanpy/anndata.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1363
https://github.com/scverse/scanpy/issues/1363:6,reliability,sli,slight,6,I got slight differences with the same seed many times in the UMAP outputs when updating the single-cell-tutorial notebook for new versions of scanpy/anndata.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1363
https://github.com/scverse/scanpy/issues/1363:80,safety,updat,updating,80,I got slight differences with the same seed many times in the UMAP outputs when updating the single-cell-tutorial notebook for new versions of scanpy/anndata.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1363
https://github.com/scverse/scanpy/issues/1363:80,security,updat,updating,80,I got slight differences with the same seed many times in the UMAP outputs when updating the single-cell-tutorial notebook for new versions of scanpy/anndata.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1363
https://github.com/scverse/scanpy/issues/1363:66,energy efficiency,load,loading,66,At the moment I am saving the adata and I am generating the plots loading it. In this way the coordinates are always the same.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1363
https://github.com/scverse/scanpy/issues/1363:94,interoperability,coordinat,coordinates,94,At the moment I am saving the adata and I am generating the plots loading it. In this way the coordinates are always the same.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1363
https://github.com/scverse/scanpy/issues/1363:66,performance,load,loading,66,At the moment I am saving the adata and I am generating the plots loading it. In this way the coordinates are always the same.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1363
https://github.com/scverse/scanpy/issues/1363:53,deployability,version,version,53,"@LuckyMD, do you think you ever saw a change without version updates? I'd like to think we were aware of changes through our tests (in particular tests for plotting and the pbmc notebook). However calculations change for different dataset sizes, so we could be missing cases where there's instability.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1363
https://github.com/scverse/scanpy/issues/1363:61,deployability,updat,updates,61,"@LuckyMD, do you think you ever saw a change without version updates? I'd like to think we were aware of changes through our tests (in particular tests for plotting and the pbmc notebook). However calculations change for different dataset sizes, so we could be missing cases where there's instability.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1363
https://github.com/scverse/scanpy/issues/1363:53,integrability,version,version,53,"@LuckyMD, do you think you ever saw a change without version updates? I'd like to think we were aware of changes through our tests (in particular tests for plotting and the pbmc notebook). However calculations change for different dataset sizes, so we could be missing cases where there's instability.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1363
https://github.com/scverse/scanpy/issues/1363:53,modifiability,version,version,53,"@LuckyMD, do you think you ever saw a change without version updates? I'd like to think we were aware of changes through our tests (in particular tests for plotting and the pbmc notebook). However calculations change for different dataset sizes, so we could be missing cases where there's instability.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1363
https://github.com/scverse/scanpy/issues/1363:61,safety,updat,updates,61,"@LuckyMD, do you think you ever saw a change without version updates? I'd like to think we were aware of changes through our tests (in particular tests for plotting and the pbmc notebook). However calculations change for different dataset sizes, so we could be missing cases where there's instability.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1363
https://github.com/scverse/scanpy/issues/1363:125,safety,test,tests,125,"@LuckyMD, do you think you ever saw a change without version updates? I'd like to think we were aware of changes through our tests (in particular tests for plotting and the pbmc notebook). However calculations change for different dataset sizes, so we could be missing cases where there's instability.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1363
https://github.com/scverse/scanpy/issues/1363:146,safety,test,tests,146,"@LuckyMD, do you think you ever saw a change without version updates? I'd like to think we were aware of changes through our tests (in particular tests for plotting and the pbmc notebook). However calculations change for different dataset sizes, so we could be missing cases where there's instability.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1363
https://github.com/scverse/scanpy/issues/1363:61,security,updat,updates,61,"@LuckyMD, do you think you ever saw a change without version updates? I'd like to think we were aware of changes through our tests (in particular tests for plotting and the pbmc notebook). However calculations change for different dataset sizes, so we could be missing cases where there's instability.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1363
https://github.com/scverse/scanpy/issues/1363:125,testability,test,tests,125,"@LuckyMD, do you think you ever saw a change without version updates? I'd like to think we were aware of changes through our tests (in particular tests for plotting and the pbmc notebook). However calculations change for different dataset sizes, so we could be missing cases where there's instability.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1363
https://github.com/scverse/scanpy/issues/1363:146,testability,test,tests,146,"@LuckyMD, do you think you ever saw a change without version updates? I'd like to think we were aware of changes through our tests (in particular tests for plotting and the pbmc notebook). However calculations change for different dataset sizes, so we could be missing cases where there's instability.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1363
https://github.com/scverse/scanpy/issues/1363:36,deployability,version,version,36,I don't think i saw changes without version updates. The only thing I noticed there are the diffmap coordinates occasionally being mirrored.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1363
https://github.com/scverse/scanpy/issues/1363:44,deployability,updat,updates,44,I don't think i saw changes without version updates. The only thing I noticed there are the diffmap coordinates occasionally being mirrored.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1363
https://github.com/scverse/scanpy/issues/1363:36,integrability,version,version,36,I don't think i saw changes without version updates. The only thing I noticed there are the diffmap coordinates occasionally being mirrored.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1363
https://github.com/scverse/scanpy/issues/1363:100,interoperability,coordinat,coordinates,100,I don't think i saw changes without version updates. The only thing I noticed there are the diffmap coordinates occasionally being mirrored.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1363
https://github.com/scverse/scanpy/issues/1363:36,modifiability,version,version,36,I don't think i saw changes without version updates. The only thing I noticed there are the diffmap coordinates occasionally being mirrored.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1363
https://github.com/scverse/scanpy/issues/1363:44,safety,updat,updates,44,I don't think i saw changes without version updates. The only thing I noticed there are the diffmap coordinates occasionally being mirrored.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1363
https://github.com/scverse/scanpy/issues/1363:44,security,updat,updates,44,I don't think i saw changes without version updates. The only thing I noticed there are the diffmap coordinates occasionally being mirrored.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1363
https://github.com/scverse/scanpy/issues/1364:157,energy efficiency,Load,Load,157,"An example with real data:. ![counts](https://user-images.githubusercontent.com/20436557/89998524-f957c600-dc8d-11ea-9036-a5d165d6bad5.png). Code:. ```py. # Load the PBMC 3k data. adata = sc.read_10x_mtx(. os.path.join(. save_path, ""filtered_gene_bc_matrices/hg19/"". ), # the directory with the `.mtx` file. var_names=""gene_symbols"", # use gene symbols for the variable names (variables-axis index). ). adata.var_names_make_unique(). # Get counts. adata.obs[""n_counts""] = adata.X.sum(axis=1).A1. sc.pp.normalize_per_cell(adata, counts_per_cell_after=1e4). adata.obs[""n_counts_normalized""] = adata.X.sum(axis=1).A1. sc.pp.log1p(adata). adata.obs[""n_counts_normalized_log""] = adata.X.sum(axis=1).A1. # Dim reduction. sc.tl.pca(adata, svd_solver=""arpack""). sc.pp.neighbors(adata). sc.tl.umap(adata). sc.pl.umap(adata, color=[""n_counts"", ""n_counts_normalized"", ""n_counts_normalized_log""]). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:704,energy efficiency,reduc,reduction,704,"An example with real data:. ![counts](https://user-images.githubusercontent.com/20436557/89998524-f957c600-dc8d-11ea-9036-a5d165d6bad5.png). Code:. ```py. # Load the PBMC 3k data. adata = sc.read_10x_mtx(. os.path.join(. save_path, ""filtered_gene_bc_matrices/hg19/"". ), # the directory with the `.mtx` file. var_names=""gene_symbols"", # use gene symbols for the variable names (variables-axis index). ). adata.var_names_make_unique(). # Get counts. adata.obs[""n_counts""] = adata.X.sum(axis=1).A1. sc.pp.normalize_per_cell(adata, counts_per_cell_after=1e4). adata.obs[""n_counts_normalized""] = adata.X.sum(axis=1).A1. sc.pp.log1p(adata). adata.obs[""n_counts_normalized_log""] = adata.X.sum(axis=1).A1. # Dim reduction. sc.tl.pca(adata, svd_solver=""arpack""). sc.pp.neighbors(adata). sc.tl.umap(adata). sc.pl.umap(adata, color=[""n_counts"", ""n_counts_normalized"", ""n_counts_normalized_log""]). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:361,modifiability,variab,variable,361,"An example with real data:. ![counts](https://user-images.githubusercontent.com/20436557/89998524-f957c600-dc8d-11ea-9036-a5d165d6bad5.png). Code:. ```py. # Load the PBMC 3k data. adata = sc.read_10x_mtx(. os.path.join(. save_path, ""filtered_gene_bc_matrices/hg19/"". ), # the directory with the `.mtx` file. var_names=""gene_symbols"", # use gene symbols for the variable names (variables-axis index). ). adata.var_names_make_unique(). # Get counts. adata.obs[""n_counts""] = adata.X.sum(axis=1).A1. sc.pp.normalize_per_cell(adata, counts_per_cell_after=1e4). adata.obs[""n_counts_normalized""] = adata.X.sum(axis=1).A1. sc.pp.log1p(adata). adata.obs[""n_counts_normalized_log""] = adata.X.sum(axis=1).A1. # Dim reduction. sc.tl.pca(adata, svd_solver=""arpack""). sc.pp.neighbors(adata). sc.tl.umap(adata). sc.pl.umap(adata, color=[""n_counts"", ""n_counts_normalized"", ""n_counts_normalized_log""]). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:377,modifiability,variab,variables-axis,377,"An example with real data:. ![counts](https://user-images.githubusercontent.com/20436557/89998524-f957c600-dc8d-11ea-9036-a5d165d6bad5.png). Code:. ```py. # Load the PBMC 3k data. adata = sc.read_10x_mtx(. os.path.join(. save_path, ""filtered_gene_bc_matrices/hg19/"". ), # the directory with the `.mtx` file. var_names=""gene_symbols"", # use gene symbols for the variable names (variables-axis index). ). adata.var_names_make_unique(). # Get counts. adata.obs[""n_counts""] = adata.X.sum(axis=1).A1. sc.pp.normalize_per_cell(adata, counts_per_cell_after=1e4). adata.obs[""n_counts_normalized""] = adata.X.sum(axis=1).A1. sc.pp.log1p(adata). adata.obs[""n_counts_normalized_log""] = adata.X.sum(axis=1).A1. # Dim reduction. sc.tl.pca(adata, svd_solver=""arpack""). sc.pp.neighbors(adata). sc.tl.umap(adata). sc.pl.umap(adata, color=[""n_counts"", ""n_counts_normalized"", ""n_counts_normalized_log""]). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:157,performance,Load,Load,157,"An example with real data:. ![counts](https://user-images.githubusercontent.com/20436557/89998524-f957c600-dc8d-11ea-9036-a5d165d6bad5.png). Code:. ```py. # Load the PBMC 3k data. adata = sc.read_10x_mtx(. os.path.join(. save_path, ""filtered_gene_bc_matrices/hg19/"". ), # the directory with the `.mtx` file. var_names=""gene_symbols"", # use gene symbols for the variable names (variables-axis index). ). adata.var_names_make_unique(). # Get counts. adata.obs[""n_counts""] = adata.X.sum(axis=1).A1. sc.pp.normalize_per_cell(adata, counts_per_cell_after=1e4). adata.obs[""n_counts_normalized""] = adata.X.sum(axis=1).A1. sc.pp.log1p(adata). adata.obs[""n_counts_normalized_log""] = adata.X.sum(axis=1).A1. # Dim reduction. sc.tl.pca(adata, svd_solver=""arpack""). sc.pp.neighbors(adata). sc.tl.umap(adata). sc.pl.umap(adata, color=[""n_counts"", ""n_counts_normalized"", ""n_counts_normalized_log""]). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:46,usability,user,user-images,46,"An example with real data:. ![counts](https://user-images.githubusercontent.com/20436557/89998524-f957c600-dc8d-11ea-9036-a5d165d6bad5.png). Code:. ```py. # Load the PBMC 3k data. adata = sc.read_10x_mtx(. os.path.join(. save_path, ""filtered_gene_bc_matrices/hg19/"". ), # the directory with the `.mtx` file. var_names=""gene_symbols"", # use gene symbols for the variable names (variables-axis index). ). adata.var_names_make_unique(). # Get counts. adata.obs[""n_counts""] = adata.X.sum(axis=1).A1. sc.pp.normalize_per_cell(adata, counts_per_cell_after=1e4). adata.obs[""n_counts_normalized""] = adata.X.sum(axis=1).A1. sc.pp.log1p(adata). adata.obs[""n_counts_normalized_log""] = adata.X.sum(axis=1).A1. # Dim reduction. sc.tl.pca(adata, svd_solver=""arpack""). sc.pp.neighbors(adata). sc.tl.umap(adata). sc.pl.umap(adata, color=[""n_counts"", ""n_counts_normalized"", ""n_counts_normalized_log""]). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:654,availability,down,downstream,654,"Hey @chris-rands,. This is a really interesting topic. Sorry in advance for the wordy reply... You are absolutely correct that log transformation removes the perfect comparison of relative expression values that mean normalization provides. Aside from CPM normalization (as provided by `sc.pp.normalize_total()`) not being a good normalization technique anyway (this is argued by any more advanced normalization methods paper, e.g., the [scran pooling paper](http://genomebiology.biomedcentral.com/articles/10.1186/s13059-016-0947-7)), there are a couple of things to consider here:. 1. Do we even want relative expression counts? 2. What assumptions do downstream methods have on the distribution of expression values. For the first question: relative gene expression values ignore differences in cell sizes/number of molecules in the cell. There are some molecules whose numbers scale with the size of the cell, and others that don't (e.g., many housekeeping genes). Choosing relative over absolute expression values to compare gene expression across cells would be helpful to compare expression of those genes that scale with size, but not the others.... so there's not really a perfect answer here. Thus, removing all effects of total counts may not be the desirable outcome. Secondly, many downstream methods assume normally distributed expression data (e.g., DE methods like: t-tests, limma, MAST, or several batch correction/data integration methods). Log transformation is used as a variance stabilization to approximate a normal distribution (quite often poorly, but better than without). This leads to many methods performing better with log transformation. IMO, the ideal approach is probably something like scVI, GLMPCA, or scTransform, where you fit a model directly to the count data and use the residuals to describe the data. This would address both steps of normalization and variance stabilization at the same time. If we have a good model to describe the data, the residuals should",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:1295,availability,down,downstream,1295,"ze_total()`) not being a good normalization technique anyway (this is argued by any more advanced normalization methods paper, e.g., the [scran pooling paper](http://genomebiology.biomedcentral.com/articles/10.1186/s13059-016-0947-7)), there are a couple of things to consider here:. 1. Do we even want relative expression counts? 2. What assumptions do downstream methods have on the distribution of expression values. For the first question: relative gene expression values ignore differences in cell sizes/number of molecules in the cell. There are some molecules whose numbers scale with the size of the cell, and others that don't (e.g., many housekeeping genes). Choosing relative over absolute expression values to compare gene expression across cells would be helpful to compare expression of those genes that scale with size, but not the others.... so there's not really a perfect answer here. Thus, removing all effects of total counts may not be the desirable outcome. Secondly, many downstream methods assume normally distributed expression data (e.g., DE methods like: t-tests, limma, MAST, or several batch correction/data integration methods). Log transformation is used as a variance stabilization to approximate a normal distribution (quite often poorly, but better than without). This leads to many methods performing better with log transformation. IMO, the ideal approach is probably something like scVI, GLMPCA, or scTransform, where you fit a model directly to the count data and use the residuals to describe the data. This would address both steps of normalization and variance stabilization at the same time. If we have a good model to describe the data, the residuals should quantify the biological variance + normally distributed noise. Overall, I would use other normalization approaches than CPM, and use log-transformation with anything that uses size factors that scale per-cell expression values. . Note also that the effect described in the second paper you mention (",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:127,deployability,log,log,127,"Hey @chris-rands,. This is a really interesting topic. Sorry in advance for the wordy reply... You are absolutely correct that log transformation removes the perfect comparison of relative expression values that mean normalization provides. Aside from CPM normalization (as provided by `sc.pp.normalize_total()`) not being a good normalization technique anyway (this is argued by any more advanced normalization methods paper, e.g., the [scran pooling paper](http://genomebiology.biomedcentral.com/articles/10.1186/s13059-016-0947-7)), there are a couple of things to consider here:. 1. Do we even want relative expression counts? 2. What assumptions do downstream methods have on the distribution of expression values. For the first question: relative gene expression values ignore differences in cell sizes/number of molecules in the cell. There are some molecules whose numbers scale with the size of the cell, and others that don't (e.g., many housekeeping genes). Choosing relative over absolute expression values to compare gene expression across cells would be helpful to compare expression of those genes that scale with size, but not the others.... so there's not really a perfect answer here. Thus, removing all effects of total counts may not be the desirable outcome. Secondly, many downstream methods assume normally distributed expression data (e.g., DE methods like: t-tests, limma, MAST, or several batch correction/data integration methods). Log transformation is used as a variance stabilization to approximate a normal distribution (quite often poorly, but better than without). This leads to many methods performing better with log transformation. IMO, the ideal approach is probably something like scVI, GLMPCA, or scTransform, where you fit a model directly to the count data and use the residuals to describe the data. This would address both steps of normalization and variance stabilization at the same time. If we have a good model to describe the data, the residuals should",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:881,deployability,scale,scale,881,"Hey @chris-rands,. This is a really interesting topic. Sorry in advance for the wordy reply... You are absolutely correct that log transformation removes the perfect comparison of relative expression values that mean normalization provides. Aside from CPM normalization (as provided by `sc.pp.normalize_total()`) not being a good normalization technique anyway (this is argued by any more advanced normalization methods paper, e.g., the [scran pooling paper](http://genomebiology.biomedcentral.com/articles/10.1186/s13059-016-0947-7)), there are a couple of things to consider here:. 1. Do we even want relative expression counts? 2. What assumptions do downstream methods have on the distribution of expression values. For the first question: relative gene expression values ignore differences in cell sizes/number of molecules in the cell. There are some molecules whose numbers scale with the size of the cell, and others that don't (e.g., many housekeeping genes). Choosing relative over absolute expression values to compare gene expression across cells would be helpful to compare expression of those genes that scale with size, but not the others.... so there's not really a perfect answer here. Thus, removing all effects of total counts may not be the desirable outcome. Secondly, many downstream methods assume normally distributed expression data (e.g., DE methods like: t-tests, limma, MAST, or several batch correction/data integration methods). Log transformation is used as a variance stabilization to approximate a normal distribution (quite often poorly, but better than without). This leads to many methods performing better with log transformation. IMO, the ideal approach is probably something like scVI, GLMPCA, or scTransform, where you fit a model directly to the count data and use the residuals to describe the data. This would address both steps of normalization and variance stabilization at the same time. If we have a good model to describe the data, the residuals should",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:1118,deployability,scale,scale,1118," that log transformation removes the perfect comparison of relative expression values that mean normalization provides. Aside from CPM normalization (as provided by `sc.pp.normalize_total()`) not being a good normalization technique anyway (this is argued by any more advanced normalization methods paper, e.g., the [scran pooling paper](http://genomebiology.biomedcentral.com/articles/10.1186/s13059-016-0947-7)), there are a couple of things to consider here:. 1. Do we even want relative expression counts? 2. What assumptions do downstream methods have on the distribution of expression values. For the first question: relative gene expression values ignore differences in cell sizes/number of molecules in the cell. There are some molecules whose numbers scale with the size of the cell, and others that don't (e.g., many housekeeping genes). Choosing relative over absolute expression values to compare gene expression across cells would be helpful to compare expression of those genes that scale with size, but not the others.... so there's not really a perfect answer here. Thus, removing all effects of total counts may not be the desirable outcome. Secondly, many downstream methods assume normally distributed expression data (e.g., DE methods like: t-tests, limma, MAST, or several batch correction/data integration methods). Log transformation is used as a variance stabilization to approximate a normal distribution (quite often poorly, but better than without). This leads to many methods performing better with log transformation. IMO, the ideal approach is probably something like scVI, GLMPCA, or scTransform, where you fit a model directly to the count data and use the residuals to describe the data. This would address both steps of normalization and variance stabilization at the same time. If we have a good model to describe the data, the residuals should quantify the biological variance + normally distributed noise. Overall, I would use other normalization approaches than ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:1437,deployability,integr,integration,1437," pooling paper](http://genomebiology.biomedcentral.com/articles/10.1186/s13059-016-0947-7)), there are a couple of things to consider here:. 1. Do we even want relative expression counts? 2. What assumptions do downstream methods have on the distribution of expression values. For the first question: relative gene expression values ignore differences in cell sizes/number of molecules in the cell. There are some molecules whose numbers scale with the size of the cell, and others that don't (e.g., many housekeeping genes). Choosing relative over absolute expression values to compare gene expression across cells would be helpful to compare expression of those genes that scale with size, but not the others.... so there's not really a perfect answer here. Thus, removing all effects of total counts may not be the desirable outcome. Secondly, many downstream methods assume normally distributed expression data (e.g., DE methods like: t-tests, limma, MAST, or several batch correction/data integration methods). Log transformation is used as a variance stabilization to approximate a normal distribution (quite often poorly, but better than without). This leads to many methods performing better with log transformation. IMO, the ideal approach is probably something like scVI, GLMPCA, or scTransform, where you fit a model directly to the count data and use the residuals to describe the data. This would address both steps of normalization and variance stabilization at the same time. If we have a good model to describe the data, the residuals should quantify the biological variance + normally distributed noise. Overall, I would use other normalization approaches than CPM, and use log-transformation with anything that uses size factors that scale per-cell expression values. . Note also that the effect described in the second paper you mention (from Aaron Lun) will mainly be relevant when you have biased distributions of sequencing depth between two samples that you are comparing. If t",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:1459,deployability,Log,Log,1459,"tp://genomebiology.biomedcentral.com/articles/10.1186/s13059-016-0947-7)), there are a couple of things to consider here:. 1. Do we even want relative expression counts? 2. What assumptions do downstream methods have on the distribution of expression values. For the first question: relative gene expression values ignore differences in cell sizes/number of molecules in the cell. There are some molecules whose numbers scale with the size of the cell, and others that don't (e.g., many housekeeping genes). Choosing relative over absolute expression values to compare gene expression across cells would be helpful to compare expression of those genes that scale with size, but not the others.... so there's not really a perfect answer here. Thus, removing all effects of total counts may not be the desirable outcome. Secondly, many downstream methods assume normally distributed expression data (e.g., DE methods like: t-tests, limma, MAST, or several batch correction/data integration methods). Log transformation is used as a variance stabilization to approximate a normal distribution (quite often poorly, but better than without). This leads to many methods performing better with log transformation. IMO, the ideal approach is probably something like scVI, GLMPCA, or scTransform, where you fit a model directly to the count data and use the residuals to describe the data. This would address both steps of normalization and variance stabilization at the same time. If we have a good model to describe the data, the residuals should quantify the biological variance + normally distributed noise. Overall, I would use other normalization approaches than CPM, and use log-transformation with anything that uses size factors that scale per-cell expression values. . Note also that the effect described in the second paper you mention (from Aaron Lun) will mainly be relevant when you have biased distributions of sequencing depth between two samples that you are comparing. If the size factors ar",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:1648,deployability,log,log,1648,"o we even want relative expression counts? 2. What assumptions do downstream methods have on the distribution of expression values. For the first question: relative gene expression values ignore differences in cell sizes/number of molecules in the cell. There are some molecules whose numbers scale with the size of the cell, and others that don't (e.g., many housekeeping genes). Choosing relative over absolute expression values to compare gene expression across cells would be helpful to compare expression of those genes that scale with size, but not the others.... so there's not really a perfect answer here. Thus, removing all effects of total counts may not be the desirable outcome. Secondly, many downstream methods assume normally distributed expression data (e.g., DE methods like: t-tests, limma, MAST, or several batch correction/data integration methods). Log transformation is used as a variance stabilization to approximate a normal distribution (quite often poorly, but better than without). This leads to many methods performing better with log transformation. IMO, the ideal approach is probably something like scVI, GLMPCA, or scTransform, where you fit a model directly to the count data and use the residuals to describe the data. This would address both steps of normalization and variance stabilization at the same time. If we have a good model to describe the data, the residuals should quantify the biological variance + normally distributed noise. Overall, I would use other normalization approaches than CPM, and use log-transformation with anything that uses size factors that scale per-cell expression values. . Note also that the effect described in the second paper you mention (from Aaron Lun) will mainly be relevant when you have biased distributions of sequencing depth between two samples that you are comparing. If the size factors are similarly distributed between both conditions, then the DE effect will not be so dramatic (as far as I understood it anyway).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:2134,deployability,log,log-transformation,2134,"o we even want relative expression counts? 2. What assumptions do downstream methods have on the distribution of expression values. For the first question: relative gene expression values ignore differences in cell sizes/number of molecules in the cell. There are some molecules whose numbers scale with the size of the cell, and others that don't (e.g., many housekeeping genes). Choosing relative over absolute expression values to compare gene expression across cells would be helpful to compare expression of those genes that scale with size, but not the others.... so there's not really a perfect answer here. Thus, removing all effects of total counts may not be the desirable outcome. Secondly, many downstream methods assume normally distributed expression data (e.g., DE methods like: t-tests, limma, MAST, or several batch correction/data integration methods). Log transformation is used as a variance stabilization to approximate a normal distribution (quite often poorly, but better than without). This leads to many methods performing better with log transformation. IMO, the ideal approach is probably something like scVI, GLMPCA, or scTransform, where you fit a model directly to the count data and use the residuals to describe the data. This would address both steps of normalization and variance stabilization at the same time. If we have a good model to describe the data, the residuals should quantify the biological variance + normally distributed noise. Overall, I would use other normalization approaches than CPM, and use log-transformation with anything that uses size factors that scale per-cell expression values. . Note also that the effect described in the second paper you mention (from Aaron Lun) will mainly be relevant when you have biased distributions of sequencing depth between two samples that you are comparing. If the size factors are similarly distributed between both conditions, then the DE effect will not be so dramatic (as far as I understood it anyway).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:2195,deployability,scale,scale,2195,"o we even want relative expression counts? 2. What assumptions do downstream methods have on the distribution of expression values. For the first question: relative gene expression values ignore differences in cell sizes/number of molecules in the cell. There are some molecules whose numbers scale with the size of the cell, and others that don't (e.g., many housekeeping genes). Choosing relative over absolute expression values to compare gene expression across cells would be helpful to compare expression of those genes that scale with size, but not the others.... so there's not really a perfect answer here. Thus, removing all effects of total counts may not be the desirable outcome. Secondly, many downstream methods assume normally distributed expression data (e.g., DE methods like: t-tests, limma, MAST, or several batch correction/data integration methods). Log transformation is used as a variance stabilization to approximate a normal distribution (quite often poorly, but better than without). This leads to many methods performing better with log transformation. IMO, the ideal approach is probably something like scVI, GLMPCA, or scTransform, where you fit a model directly to the count data and use the residuals to describe the data. This would address both steps of normalization and variance stabilization at the same time. If we have a good model to describe the data, the residuals should quantify the biological variance + normally distributed noise. Overall, I would use other normalization approaches than CPM, and use log-transformation with anything that uses size factors that scale per-cell expression values. . Note also that the effect described in the second paper you mention (from Aaron Lun) will mainly be relevant when you have biased distributions of sequencing depth between two samples that you are comparing. If the size factors are similarly distributed between both conditions, then the DE effect will not be so dramatic (as far as I understood it anyway).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:881,energy efficiency,scale,scale,881,"Hey @chris-rands,. This is a really interesting topic. Sorry in advance for the wordy reply... You are absolutely correct that log transformation removes the perfect comparison of relative expression values that mean normalization provides. Aside from CPM normalization (as provided by `sc.pp.normalize_total()`) not being a good normalization technique anyway (this is argued by any more advanced normalization methods paper, e.g., the [scran pooling paper](http://genomebiology.biomedcentral.com/articles/10.1186/s13059-016-0947-7)), there are a couple of things to consider here:. 1. Do we even want relative expression counts? 2. What assumptions do downstream methods have on the distribution of expression values. For the first question: relative gene expression values ignore differences in cell sizes/number of molecules in the cell. There are some molecules whose numbers scale with the size of the cell, and others that don't (e.g., many housekeeping genes). Choosing relative over absolute expression values to compare gene expression across cells would be helpful to compare expression of those genes that scale with size, but not the others.... so there's not really a perfect answer here. Thus, removing all effects of total counts may not be the desirable outcome. Secondly, many downstream methods assume normally distributed expression data (e.g., DE methods like: t-tests, limma, MAST, or several batch correction/data integration methods). Log transformation is used as a variance stabilization to approximate a normal distribution (quite often poorly, but better than without). This leads to many methods performing better with log transformation. IMO, the ideal approach is probably something like scVI, GLMPCA, or scTransform, where you fit a model directly to the count data and use the residuals to describe the data. This would address both steps of normalization and variance stabilization at the same time. If we have a good model to describe the data, the residuals should",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:1118,energy efficiency,scale,scale,1118," that log transformation removes the perfect comparison of relative expression values that mean normalization provides. Aside from CPM normalization (as provided by `sc.pp.normalize_total()`) not being a good normalization technique anyway (this is argued by any more advanced normalization methods paper, e.g., the [scran pooling paper](http://genomebiology.biomedcentral.com/articles/10.1186/s13059-016-0947-7)), there are a couple of things to consider here:. 1. Do we even want relative expression counts? 2. What assumptions do downstream methods have on the distribution of expression values. For the first question: relative gene expression values ignore differences in cell sizes/number of molecules in the cell. There are some molecules whose numbers scale with the size of the cell, and others that don't (e.g., many housekeeping genes). Choosing relative over absolute expression values to compare gene expression across cells would be helpful to compare expression of those genes that scale with size, but not the others.... so there's not really a perfect answer here. Thus, removing all effects of total counts may not be the desirable outcome. Secondly, many downstream methods assume normally distributed expression data (e.g., DE methods like: t-tests, limma, MAST, or several batch correction/data integration methods). Log transformation is used as a variance stabilization to approximate a normal distribution (quite often poorly, but better than without). This leads to many methods performing better with log transformation. IMO, the ideal approach is probably something like scVI, GLMPCA, or scTransform, where you fit a model directly to the count data and use the residuals to describe the data. This would address both steps of normalization and variance stabilization at the same time. If we have a good model to describe the data, the residuals should quantify the biological variance + normally distributed noise. Overall, I would use other normalization approaches than ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:1765,energy efficiency,model,model,1765,"o we even want relative expression counts? 2. What assumptions do downstream methods have on the distribution of expression values. For the first question: relative gene expression values ignore differences in cell sizes/number of molecules in the cell. There are some molecules whose numbers scale with the size of the cell, and others that don't (e.g., many housekeeping genes). Choosing relative over absolute expression values to compare gene expression across cells would be helpful to compare expression of those genes that scale with size, but not the others.... so there's not really a perfect answer here. Thus, removing all effects of total counts may not be the desirable outcome. Secondly, many downstream methods assume normally distributed expression data (e.g., DE methods like: t-tests, limma, MAST, or several batch correction/data integration methods). Log transformation is used as a variance stabilization to approximate a normal distribution (quite often poorly, but better than without). This leads to many methods performing better with log transformation. IMO, the ideal approach is probably something like scVI, GLMPCA, or scTransform, where you fit a model directly to the count data and use the residuals to describe the data. This would address both steps of normalization and variance stabilization at the same time. If we have a good model to describe the data, the residuals should quantify the biological variance + normally distributed noise. Overall, I would use other normalization approaches than CPM, and use log-transformation with anything that uses size factors that scale per-cell expression values. . Note also that the effect described in the second paper you mention (from Aaron Lun) will mainly be relevant when you have biased distributions of sequencing depth between two samples that you are comparing. If the size factors are similarly distributed between both conditions, then the DE effect will not be so dramatic (as far as I understood it anyway).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:1952,energy efficiency,model,model,1952,"o we even want relative expression counts? 2. What assumptions do downstream methods have on the distribution of expression values. For the first question: relative gene expression values ignore differences in cell sizes/number of molecules in the cell. There are some molecules whose numbers scale with the size of the cell, and others that don't (e.g., many housekeeping genes). Choosing relative over absolute expression values to compare gene expression across cells would be helpful to compare expression of those genes that scale with size, but not the others.... so there's not really a perfect answer here. Thus, removing all effects of total counts may not be the desirable outcome. Secondly, many downstream methods assume normally distributed expression data (e.g., DE methods like: t-tests, limma, MAST, or several batch correction/data integration methods). Log transformation is used as a variance stabilization to approximate a normal distribution (quite often poorly, but better than without). This leads to many methods performing better with log transformation. IMO, the ideal approach is probably something like scVI, GLMPCA, or scTransform, where you fit a model directly to the count data and use the residuals to describe the data. This would address both steps of normalization and variance stabilization at the same time. If we have a good model to describe the data, the residuals should quantify the biological variance + normally distributed noise. Overall, I would use other normalization approaches than CPM, and use log-transformation with anything that uses size factors that scale per-cell expression values. . Note also that the effect described in the second paper you mention (from Aaron Lun) will mainly be relevant when you have biased distributions of sequencing depth between two samples that you are comparing. If the size factors are similarly distributed between both conditions, then the DE effect will not be so dramatic (as far as I understood it anyway).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:2195,energy efficiency,scale,scale,2195,"o we even want relative expression counts? 2. What assumptions do downstream methods have on the distribution of expression values. For the first question: relative gene expression values ignore differences in cell sizes/number of molecules in the cell. There are some molecules whose numbers scale with the size of the cell, and others that don't (e.g., many housekeeping genes). Choosing relative over absolute expression values to compare gene expression across cells would be helpful to compare expression of those genes that scale with size, but not the others.... so there's not really a perfect answer here. Thus, removing all effects of total counts may not be the desirable outcome. Secondly, many downstream methods assume normally distributed expression data (e.g., DE methods like: t-tests, limma, MAST, or several batch correction/data integration methods). Log transformation is used as a variance stabilization to approximate a normal distribution (quite often poorly, but better than without). This leads to many methods performing better with log transformation. IMO, the ideal approach is probably something like scVI, GLMPCA, or scTransform, where you fit a model directly to the count data and use the residuals to describe the data. This would address both steps of normalization and variance stabilization at the same time. If we have a good model to describe the data, the residuals should quantify the biological variance + normally distributed noise. Overall, I would use other normalization approaches than CPM, and use log-transformation with anything that uses size factors that scale per-cell expression values. . Note also that the effect described in the second paper you mention (from Aaron Lun) will mainly be relevant when you have biased distributions of sequencing depth between two samples that you are comparing. If the size factors are similarly distributed between both conditions, then the DE effect will not be so dramatic (as far as I understood it anyway).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:48,integrability,topic,topic,48,"Hey @chris-rands,. This is a really interesting topic. Sorry in advance for the wordy reply... You are absolutely correct that log transformation removes the perfect comparison of relative expression values that mean normalization provides. Aside from CPM normalization (as provided by `sc.pp.normalize_total()`) not being a good normalization technique anyway (this is argued by any more advanced normalization methods paper, e.g., the [scran pooling paper](http://genomebiology.biomedcentral.com/articles/10.1186/s13059-016-0947-7)), there are a couple of things to consider here:. 1. Do we even want relative expression counts? 2. What assumptions do downstream methods have on the distribution of expression values. For the first question: relative gene expression values ignore differences in cell sizes/number of molecules in the cell. There are some molecules whose numbers scale with the size of the cell, and others that don't (e.g., many housekeeping genes). Choosing relative over absolute expression values to compare gene expression across cells would be helpful to compare expression of those genes that scale with size, but not the others.... so there's not really a perfect answer here. Thus, removing all effects of total counts may not be the desirable outcome. Secondly, many downstream methods assume normally distributed expression data (e.g., DE methods like: t-tests, limma, MAST, or several batch correction/data integration methods). Log transformation is used as a variance stabilization to approximate a normal distribution (quite often poorly, but better than without). This leads to many methods performing better with log transformation. IMO, the ideal approach is probably something like scVI, GLMPCA, or scTransform, where you fit a model directly to the count data and use the residuals to describe the data. This would address both steps of normalization and variance stabilization at the same time. If we have a good model to describe the data, the residuals should",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:131,integrability,transform,transformation,131,"Hey @chris-rands,. This is a really interesting topic. Sorry in advance for the wordy reply... You are absolutely correct that log transformation removes the perfect comparison of relative expression values that mean normalization provides. Aside from CPM normalization (as provided by `sc.pp.normalize_total()`) not being a good normalization technique anyway (this is argued by any more advanced normalization methods paper, e.g., the [scran pooling paper](http://genomebiology.biomedcentral.com/articles/10.1186/s13059-016-0947-7)), there are a couple of things to consider here:. 1. Do we even want relative expression counts? 2. What assumptions do downstream methods have on the distribution of expression values. For the first question: relative gene expression values ignore differences in cell sizes/number of molecules in the cell. There are some molecules whose numbers scale with the size of the cell, and others that don't (e.g., many housekeeping genes). Choosing relative over absolute expression values to compare gene expression across cells would be helpful to compare expression of those genes that scale with size, but not the others.... so there's not really a perfect answer here. Thus, removing all effects of total counts may not be the desirable outcome. Secondly, many downstream methods assume normally distributed expression data (e.g., DE methods like: t-tests, limma, MAST, or several batch correction/data integration methods). Log transformation is used as a variance stabilization to approximate a normal distribution (quite often poorly, but better than without). This leads to many methods performing better with log transformation. IMO, the ideal approach is probably something like scVI, GLMPCA, or scTransform, where you fit a model directly to the count data and use the residuals to describe the data. This would address both steps of normalization and variance stabilization at the same time. If we have a good model to describe the data, the residuals should",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:548,integrability,coupl,couple,548,"Hey @chris-rands,. This is a really interesting topic. Sorry in advance for the wordy reply... You are absolutely correct that log transformation removes the perfect comparison of relative expression values that mean normalization provides. Aside from CPM normalization (as provided by `sc.pp.normalize_total()`) not being a good normalization technique anyway (this is argued by any more advanced normalization methods paper, e.g., the [scran pooling paper](http://genomebiology.biomedcentral.com/articles/10.1186/s13059-016-0947-7)), there are a couple of things to consider here:. 1. Do we even want relative expression counts? 2. What assumptions do downstream methods have on the distribution of expression values. For the first question: relative gene expression values ignore differences in cell sizes/number of molecules in the cell. There are some molecules whose numbers scale with the size of the cell, and others that don't (e.g., many housekeeping genes). Choosing relative over absolute expression values to compare gene expression across cells would be helpful to compare expression of those genes that scale with size, but not the others.... so there's not really a perfect answer here. Thus, removing all effects of total counts may not be the desirable outcome. Secondly, many downstream methods assume normally distributed expression data (e.g., DE methods like: t-tests, limma, MAST, or several batch correction/data integration methods). Log transformation is used as a variance stabilization to approximate a normal distribution (quite often poorly, but better than without). This leads to many methods performing better with log transformation. IMO, the ideal approach is probably something like scVI, GLMPCA, or scTransform, where you fit a model directly to the count data and use the residuals to describe the data. This would address both steps of normalization and variance stabilization at the same time. If we have a good model to describe the data, the residuals should",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:1415,integrability,batch,batch,1415,"s paper, e.g., the [scran pooling paper](http://genomebiology.biomedcentral.com/articles/10.1186/s13059-016-0947-7)), there are a couple of things to consider here:. 1. Do we even want relative expression counts? 2. What assumptions do downstream methods have on the distribution of expression values. For the first question: relative gene expression values ignore differences in cell sizes/number of molecules in the cell. There are some molecules whose numbers scale with the size of the cell, and others that don't (e.g., many housekeeping genes). Choosing relative over absolute expression values to compare gene expression across cells would be helpful to compare expression of those genes that scale with size, but not the others.... so there's not really a perfect answer here. Thus, removing all effects of total counts may not be the desirable outcome. Secondly, many downstream methods assume normally distributed expression data (e.g., DE methods like: t-tests, limma, MAST, or several batch correction/data integration methods). Log transformation is used as a variance stabilization to approximate a normal distribution (quite often poorly, but better than without). This leads to many methods performing better with log transformation. IMO, the ideal approach is probably something like scVI, GLMPCA, or scTransform, where you fit a model directly to the count data and use the residuals to describe the data. This would address both steps of normalization and variance stabilization at the same time. If we have a good model to describe the data, the residuals should quantify the biological variance + normally distributed noise. Overall, I would use other normalization approaches than CPM, and use log-transformation with anything that uses size factors that scale per-cell expression values. . Note also that the effect described in the second paper you mention (from Aaron Lun) will mainly be relevant when you have biased distributions of sequencing depth between two samples tha",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:1437,integrability,integr,integration,1437," pooling paper](http://genomebiology.biomedcentral.com/articles/10.1186/s13059-016-0947-7)), there are a couple of things to consider here:. 1. Do we even want relative expression counts? 2. What assumptions do downstream methods have on the distribution of expression values. For the first question: relative gene expression values ignore differences in cell sizes/number of molecules in the cell. There are some molecules whose numbers scale with the size of the cell, and others that don't (e.g., many housekeeping genes). Choosing relative over absolute expression values to compare gene expression across cells would be helpful to compare expression of those genes that scale with size, but not the others.... so there's not really a perfect answer here. Thus, removing all effects of total counts may not be the desirable outcome. Secondly, many downstream methods assume normally distributed expression data (e.g., DE methods like: t-tests, limma, MAST, or several batch correction/data integration methods). Log transformation is used as a variance stabilization to approximate a normal distribution (quite often poorly, but better than without). This leads to many methods performing better with log transformation. IMO, the ideal approach is probably something like scVI, GLMPCA, or scTransform, where you fit a model directly to the count data and use the residuals to describe the data. This would address both steps of normalization and variance stabilization at the same time. If we have a good model to describe the data, the residuals should quantify the biological variance + normally distributed noise. Overall, I would use other normalization approaches than CPM, and use log-transformation with anything that uses size factors that scale per-cell expression values. . Note also that the effect described in the second paper you mention (from Aaron Lun) will mainly be relevant when you have biased distributions of sequencing depth between two samples that you are comparing. If t",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:1463,integrability,transform,transformation,1463,"mebiology.biomedcentral.com/articles/10.1186/s13059-016-0947-7)), there are a couple of things to consider here:. 1. Do we even want relative expression counts? 2. What assumptions do downstream methods have on the distribution of expression values. For the first question: relative gene expression values ignore differences in cell sizes/number of molecules in the cell. There are some molecules whose numbers scale with the size of the cell, and others that don't (e.g., many housekeeping genes). Choosing relative over absolute expression values to compare gene expression across cells would be helpful to compare expression of those genes that scale with size, but not the others.... so there's not really a perfect answer here. Thus, removing all effects of total counts may not be the desirable outcome. Secondly, many downstream methods assume normally distributed expression data (e.g., DE methods like: t-tests, limma, MAST, or several batch correction/data integration methods). Log transformation is used as a variance stabilization to approximate a normal distribution (quite often poorly, but better than without). This leads to many methods performing better with log transformation. IMO, the ideal approach is probably something like scVI, GLMPCA, or scTransform, where you fit a model directly to the count data and use the residuals to describe the data. This would address both steps of normalization and variance stabilization at the same time. If we have a good model to describe the data, the residuals should quantify the biological variance + normally distributed noise. Overall, I would use other normalization approaches than CPM, and use log-transformation with anything that uses size factors that scale per-cell expression values. . Note also that the effect described in the second paper you mention (from Aaron Lun) will mainly be relevant when you have biased distributions of sequencing depth between two samples that you are comparing. If the size factors are similar",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:1652,integrability,transform,transformation,1652,"o we even want relative expression counts? 2. What assumptions do downstream methods have on the distribution of expression values. For the first question: relative gene expression values ignore differences in cell sizes/number of molecules in the cell. There are some molecules whose numbers scale with the size of the cell, and others that don't (e.g., many housekeeping genes). Choosing relative over absolute expression values to compare gene expression across cells would be helpful to compare expression of those genes that scale with size, but not the others.... so there's not really a perfect answer here. Thus, removing all effects of total counts may not be the desirable outcome. Secondly, many downstream methods assume normally distributed expression data (e.g., DE methods like: t-tests, limma, MAST, or several batch correction/data integration methods). Log transformation is used as a variance stabilization to approximate a normal distribution (quite often poorly, but better than without). This leads to many methods performing better with log transformation. IMO, the ideal approach is probably something like scVI, GLMPCA, or scTransform, where you fit a model directly to the count data and use the residuals to describe the data. This would address both steps of normalization and variance stabilization at the same time. If we have a good model to describe the data, the residuals should quantify the biological variance + normally distributed noise. Overall, I would use other normalization approaches than CPM, and use log-transformation with anything that uses size factors that scale per-cell expression values. . Note also that the effect described in the second paper you mention (from Aaron Lun) will mainly be relevant when you have biased distributions of sequencing depth between two samples that you are comparing. If the size factors are similarly distributed between both conditions, then the DE effect will not be so dramatic (as far as I understood it anyway).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:2138,integrability,transform,transformation,2138,"o we even want relative expression counts? 2. What assumptions do downstream methods have on the distribution of expression values. For the first question: relative gene expression values ignore differences in cell sizes/number of molecules in the cell. There are some molecules whose numbers scale with the size of the cell, and others that don't (e.g., many housekeeping genes). Choosing relative over absolute expression values to compare gene expression across cells would be helpful to compare expression of those genes that scale with size, but not the others.... so there's not really a perfect answer here. Thus, removing all effects of total counts may not be the desirable outcome. Secondly, many downstream methods assume normally distributed expression data (e.g., DE methods like: t-tests, limma, MAST, or several batch correction/data integration methods). Log transformation is used as a variance stabilization to approximate a normal distribution (quite often poorly, but better than without). This leads to many methods performing better with log transformation. IMO, the ideal approach is probably something like scVI, GLMPCA, or scTransform, where you fit a model directly to the count data and use the residuals to describe the data. This would address both steps of normalization and variance stabilization at the same time. If we have a good model to describe the data, the residuals should quantify the biological variance + normally distributed noise. Overall, I would use other normalization approaches than CPM, and use log-transformation with anything that uses size factors that scale per-cell expression values. . Note also that the effect described in the second paper you mention (from Aaron Lun) will mainly be relevant when you have biased distributions of sequencing depth between two samples that you are comparing. If the size factors are similarly distributed between both conditions, then the DE effect will not be so dramatic (as far as I understood it anyway).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:131,interoperability,transform,transformation,131,"Hey @chris-rands,. This is a really interesting topic. Sorry in advance for the wordy reply... You are absolutely correct that log transformation removes the perfect comparison of relative expression values that mean normalization provides. Aside from CPM normalization (as provided by `sc.pp.normalize_total()`) not being a good normalization technique anyway (this is argued by any more advanced normalization methods paper, e.g., the [scran pooling paper](http://genomebiology.biomedcentral.com/articles/10.1186/s13059-016-0947-7)), there are a couple of things to consider here:. 1. Do we even want relative expression counts? 2. What assumptions do downstream methods have on the distribution of expression values. For the first question: relative gene expression values ignore differences in cell sizes/number of molecules in the cell. There are some molecules whose numbers scale with the size of the cell, and others that don't (e.g., many housekeeping genes). Choosing relative over absolute expression values to compare gene expression across cells would be helpful to compare expression of those genes that scale with size, but not the others.... so there's not really a perfect answer here. Thus, removing all effects of total counts may not be the desirable outcome. Secondly, many downstream methods assume normally distributed expression data (e.g., DE methods like: t-tests, limma, MAST, or several batch correction/data integration methods). Log transformation is used as a variance stabilization to approximate a normal distribution (quite often poorly, but better than without). This leads to many methods performing better with log transformation. IMO, the ideal approach is probably something like scVI, GLMPCA, or scTransform, where you fit a model directly to the count data and use the residuals to describe the data. This would address both steps of normalization and variance stabilization at the same time. If we have a good model to describe the data, the residuals should",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:685,interoperability,distribut,distribution,685,"Hey @chris-rands,. This is a really interesting topic. Sorry in advance for the wordy reply... You are absolutely correct that log transformation removes the perfect comparison of relative expression values that mean normalization provides. Aside from CPM normalization (as provided by `sc.pp.normalize_total()`) not being a good normalization technique anyway (this is argued by any more advanced normalization methods paper, e.g., the [scran pooling paper](http://genomebiology.biomedcentral.com/articles/10.1186/s13059-016-0947-7)), there are a couple of things to consider here:. 1. Do we even want relative expression counts? 2. What assumptions do downstream methods have on the distribution of expression values. For the first question: relative gene expression values ignore differences in cell sizes/number of molecules in the cell. There are some molecules whose numbers scale with the size of the cell, and others that don't (e.g., many housekeeping genes). Choosing relative over absolute expression values to compare gene expression across cells would be helpful to compare expression of those genes that scale with size, but not the others.... so there's not really a perfect answer here. Thus, removing all effects of total counts may not be the desirable outcome. Secondly, many downstream methods assume normally distributed expression data (e.g., DE methods like: t-tests, limma, MAST, or several batch correction/data integration methods). Log transformation is used as a variance stabilization to approximate a normal distribution (quite often poorly, but better than without). This leads to many methods performing better with log transformation. IMO, the ideal approach is probably something like scVI, GLMPCA, or scTransform, where you fit a model directly to the count data and use the residuals to describe the data. This would address both steps of normalization and variance stabilization at the same time. If we have a good model to describe the data, the residuals should",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:1330,interoperability,distribut,distributed,1330,"ization technique anyway (this is argued by any more advanced normalization methods paper, e.g., the [scran pooling paper](http://genomebiology.biomedcentral.com/articles/10.1186/s13059-016-0947-7)), there are a couple of things to consider here:. 1. Do we even want relative expression counts? 2. What assumptions do downstream methods have on the distribution of expression values. For the first question: relative gene expression values ignore differences in cell sizes/number of molecules in the cell. There are some molecules whose numbers scale with the size of the cell, and others that don't (e.g., many housekeeping genes). Choosing relative over absolute expression values to compare gene expression across cells would be helpful to compare expression of those genes that scale with size, but not the others.... so there's not really a perfect answer here. Thus, removing all effects of total counts may not be the desirable outcome. Secondly, many downstream methods assume normally distributed expression data (e.g., DE methods like: t-tests, limma, MAST, or several batch correction/data integration methods). Log transformation is used as a variance stabilization to approximate a normal distribution (quite often poorly, but better than without). This leads to many methods performing better with log transformation. IMO, the ideal approach is probably something like scVI, GLMPCA, or scTransform, where you fit a model directly to the count data and use the residuals to describe the data. This would address both steps of normalization and variance stabilization at the same time. If we have a good model to describe the data, the residuals should quantify the biological variance + normally distributed noise. Overall, I would use other normalization approaches than CPM, and use log-transformation with anything that uses size factors that scale per-cell expression values. . Note also that the effect described in the second paper you mention (from Aaron Lun) will mainly be relev",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:1437,interoperability,integr,integration,1437," pooling paper](http://genomebiology.biomedcentral.com/articles/10.1186/s13059-016-0947-7)), there are a couple of things to consider here:. 1. Do we even want relative expression counts? 2. What assumptions do downstream methods have on the distribution of expression values. For the first question: relative gene expression values ignore differences in cell sizes/number of molecules in the cell. There are some molecules whose numbers scale with the size of the cell, and others that don't (e.g., many housekeeping genes). Choosing relative over absolute expression values to compare gene expression across cells would be helpful to compare expression of those genes that scale with size, but not the others.... so there's not really a perfect answer here. Thus, removing all effects of total counts may not be the desirable outcome. Secondly, many downstream methods assume normally distributed expression data (e.g., DE methods like: t-tests, limma, MAST, or several batch correction/data integration methods). Log transformation is used as a variance stabilization to approximate a normal distribution (quite often poorly, but better than without). This leads to many methods performing better with log transformation. IMO, the ideal approach is probably something like scVI, GLMPCA, or scTransform, where you fit a model directly to the count data and use the residuals to describe the data. This would address both steps of normalization and variance stabilization at the same time. If we have a good model to describe the data, the residuals should quantify the biological variance + normally distributed noise. Overall, I would use other normalization approaches than CPM, and use log-transformation with anything that uses size factors that scale per-cell expression values. . Note also that the effect described in the second paper you mention (from Aaron Lun) will mainly be relevant when you have biased distributions of sequencing depth between two samples that you are comparing. If t",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:1463,interoperability,transform,transformation,1463,"mebiology.biomedcentral.com/articles/10.1186/s13059-016-0947-7)), there are a couple of things to consider here:. 1. Do we even want relative expression counts? 2. What assumptions do downstream methods have on the distribution of expression values. For the first question: relative gene expression values ignore differences in cell sizes/number of molecules in the cell. There are some molecules whose numbers scale with the size of the cell, and others that don't (e.g., many housekeeping genes). Choosing relative over absolute expression values to compare gene expression across cells would be helpful to compare expression of those genes that scale with size, but not the others.... so there's not really a perfect answer here. Thus, removing all effects of total counts may not be the desirable outcome. Secondly, many downstream methods assume normally distributed expression data (e.g., DE methods like: t-tests, limma, MAST, or several batch correction/data integration methods). Log transformation is used as a variance stabilization to approximate a normal distribution (quite often poorly, but better than without). This leads to many methods performing better with log transformation. IMO, the ideal approach is probably something like scVI, GLMPCA, or scTransform, where you fit a model directly to the count data and use the residuals to describe the data. This would address both steps of normalization and variance stabilization at the same time. If we have a good model to describe the data, the residuals should quantify the biological variance + normally distributed noise. Overall, I would use other normalization approaches than CPM, and use log-transformation with anything that uses size factors that scale per-cell expression values. . Note also that the effect described in the second paper you mention (from Aaron Lun) will mainly be relevant when you have biased distributions of sequencing depth between two samples that you are comparing. If the size factors are similar",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:1538,interoperability,distribut,distribution,1538,"e a couple of things to consider here:. 1. Do we even want relative expression counts? 2. What assumptions do downstream methods have on the distribution of expression values. For the first question: relative gene expression values ignore differences in cell sizes/number of molecules in the cell. There are some molecules whose numbers scale with the size of the cell, and others that don't (e.g., many housekeeping genes). Choosing relative over absolute expression values to compare gene expression across cells would be helpful to compare expression of those genes that scale with size, but not the others.... so there's not really a perfect answer here. Thus, removing all effects of total counts may not be the desirable outcome. Secondly, many downstream methods assume normally distributed expression data (e.g., DE methods like: t-tests, limma, MAST, or several batch correction/data integration methods). Log transformation is used as a variance stabilization to approximate a normal distribution (quite often poorly, but better than without). This leads to many methods performing better with log transformation. IMO, the ideal approach is probably something like scVI, GLMPCA, or scTransform, where you fit a model directly to the count data and use the residuals to describe the data. This would address both steps of normalization and variance stabilization at the same time. If we have a good model to describe the data, the residuals should quantify the biological variance + normally distributed noise. Overall, I would use other normalization approaches than CPM, and use log-transformation with anything that uses size factors that scale per-cell expression values. . Note also that the effect described in the second paper you mention (from Aaron Lun) will mainly be relevant when you have biased distributions of sequencing depth between two samples that you are comparing. If the size factors are similarly distributed between both conditions, then the DE effect will not be so ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:1652,interoperability,transform,transformation,1652,"o we even want relative expression counts? 2. What assumptions do downstream methods have on the distribution of expression values. For the first question: relative gene expression values ignore differences in cell sizes/number of molecules in the cell. There are some molecules whose numbers scale with the size of the cell, and others that don't (e.g., many housekeeping genes). Choosing relative over absolute expression values to compare gene expression across cells would be helpful to compare expression of those genes that scale with size, but not the others.... so there's not really a perfect answer here. Thus, removing all effects of total counts may not be the desirable outcome. Secondly, many downstream methods assume normally distributed expression data (e.g., DE methods like: t-tests, limma, MAST, or several batch correction/data integration methods). Log transformation is used as a variance stabilization to approximate a normal distribution (quite often poorly, but better than without). This leads to many methods performing better with log transformation. IMO, the ideal approach is probably something like scVI, GLMPCA, or scTransform, where you fit a model directly to the count data and use the residuals to describe the data. This would address both steps of normalization and variance stabilization at the same time. If we have a good model to describe the data, the residuals should quantify the biological variance + normally distributed noise. Overall, I would use other normalization approaches than CPM, and use log-transformation with anything that uses size factors that scale per-cell expression values. . Note also that the effect described in the second paper you mention (from Aaron Lun) will mainly be relevant when you have biased distributions of sequencing depth between two samples that you are comparing. If the size factors are similarly distributed between both conditions, then the DE effect will not be so dramatic (as far as I understood it anyway).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:2045,interoperability,distribut,distributed,2045,"o we even want relative expression counts? 2. What assumptions do downstream methods have on the distribution of expression values. For the first question: relative gene expression values ignore differences in cell sizes/number of molecules in the cell. There are some molecules whose numbers scale with the size of the cell, and others that don't (e.g., many housekeeping genes). Choosing relative over absolute expression values to compare gene expression across cells would be helpful to compare expression of those genes that scale with size, but not the others.... so there's not really a perfect answer here. Thus, removing all effects of total counts may not be the desirable outcome. Secondly, many downstream methods assume normally distributed expression data (e.g., DE methods like: t-tests, limma, MAST, or several batch correction/data integration methods). Log transformation is used as a variance stabilization to approximate a normal distribution (quite often poorly, but better than without). This leads to many methods performing better with log transformation. IMO, the ideal approach is probably something like scVI, GLMPCA, or scTransform, where you fit a model directly to the count data and use the residuals to describe the data. This would address both steps of normalization and variance stabilization at the same time. If we have a good model to describe the data, the residuals should quantify the biological variance + normally distributed noise. Overall, I would use other normalization approaches than CPM, and use log-transformation with anything that uses size factors that scale per-cell expression values. . Note also that the effect described in the second paper you mention (from Aaron Lun) will mainly be relevant when you have biased distributions of sequencing depth between two samples that you are comparing. If the size factors are similarly distributed between both conditions, then the DE effect will not be so dramatic (as far as I understood it anyway).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:2138,interoperability,transform,transformation,2138,"o we even want relative expression counts? 2. What assumptions do downstream methods have on the distribution of expression values. For the first question: relative gene expression values ignore differences in cell sizes/number of molecules in the cell. There are some molecules whose numbers scale with the size of the cell, and others that don't (e.g., many housekeeping genes). Choosing relative over absolute expression values to compare gene expression across cells would be helpful to compare expression of those genes that scale with size, but not the others.... so there's not really a perfect answer here. Thus, removing all effects of total counts may not be the desirable outcome. Secondly, many downstream methods assume normally distributed expression data (e.g., DE methods like: t-tests, limma, MAST, or several batch correction/data integration methods). Log transformation is used as a variance stabilization to approximate a normal distribution (quite often poorly, but better than without). This leads to many methods performing better with log transformation. IMO, the ideal approach is probably something like scVI, GLMPCA, or scTransform, where you fit a model directly to the count data and use the residuals to describe the data. This would address both steps of normalization and variance stabilization at the same time. If we have a good model to describe the data, the residuals should quantify the biological variance + normally distributed noise. Overall, I would use other normalization approaches than CPM, and use log-transformation with anything that uses size factors that scale per-cell expression values. . Note also that the effect described in the second paper you mention (from Aaron Lun) will mainly be relevant when you have biased distributions of sequencing depth between two samples that you are comparing. If the size factors are similarly distributed between both conditions, then the DE effect will not be so dramatic (as far as I understood it anyway).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:2361,interoperability,distribut,distributions,2361,"o we even want relative expression counts? 2. What assumptions do downstream methods have on the distribution of expression values. For the first question: relative gene expression values ignore differences in cell sizes/number of molecules in the cell. There are some molecules whose numbers scale with the size of the cell, and others that don't (e.g., many housekeeping genes). Choosing relative over absolute expression values to compare gene expression across cells would be helpful to compare expression of those genes that scale with size, but not the others.... so there's not really a perfect answer here. Thus, removing all effects of total counts may not be the desirable outcome. Secondly, many downstream methods assume normally distributed expression data (e.g., DE methods like: t-tests, limma, MAST, or several batch correction/data integration methods). Log transformation is used as a variance stabilization to approximate a normal distribution (quite often poorly, but better than without). This leads to many methods performing better with log transformation. IMO, the ideal approach is probably something like scVI, GLMPCA, or scTransform, where you fit a model directly to the count data and use the residuals to describe the data. This would address both steps of normalization and variance stabilization at the same time. If we have a good model to describe the data, the residuals should quantify the biological variance + normally distributed noise. Overall, I would use other normalization approaches than CPM, and use log-transformation with anything that uses size factors that scale per-cell expression values. . Note also that the effect described in the second paper you mention (from Aaron Lun) will mainly be relevant when you have biased distributions of sequencing depth between two samples that you are comparing. If the size factors are similarly distributed between both conditions, then the DE effect will not be so dramatic (as far as I understood it anyway).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:2473,interoperability,distribut,distributed,2473,"o we even want relative expression counts? 2. What assumptions do downstream methods have on the distribution of expression values. For the first question: relative gene expression values ignore differences in cell sizes/number of molecules in the cell. There are some molecules whose numbers scale with the size of the cell, and others that don't (e.g., many housekeeping genes). Choosing relative over absolute expression values to compare gene expression across cells would be helpful to compare expression of those genes that scale with size, but not the others.... so there's not really a perfect answer here. Thus, removing all effects of total counts may not be the desirable outcome. Secondly, many downstream methods assume normally distributed expression data (e.g., DE methods like: t-tests, limma, MAST, or several batch correction/data integration methods). Log transformation is used as a variance stabilization to approximate a normal distribution (quite often poorly, but better than without). This leads to many methods performing better with log transformation. IMO, the ideal approach is probably something like scVI, GLMPCA, or scTransform, where you fit a model directly to the count data and use the residuals to describe the data. This would address both steps of normalization and variance stabilization at the same time. If we have a good model to describe the data, the residuals should quantify the biological variance + normally distributed noise. Overall, I would use other normalization approaches than CPM, and use log-transformation with anything that uses size factors that scale per-cell expression values. . Note also that the effect described in the second paper you mention (from Aaron Lun) will mainly be relevant when you have biased distributions of sequencing depth between two samples that you are comparing. If the size factors are similarly distributed between both conditions, then the DE effect will not be so dramatic (as far as I understood it anyway).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:548,modifiability,coupl,couple,548,"Hey @chris-rands,. This is a really interesting topic. Sorry in advance for the wordy reply... You are absolutely correct that log transformation removes the perfect comparison of relative expression values that mean normalization provides. Aside from CPM normalization (as provided by `sc.pp.normalize_total()`) not being a good normalization technique anyway (this is argued by any more advanced normalization methods paper, e.g., the [scran pooling paper](http://genomebiology.biomedcentral.com/articles/10.1186/s13059-016-0947-7)), there are a couple of things to consider here:. 1. Do we even want relative expression counts? 2. What assumptions do downstream methods have on the distribution of expression values. For the first question: relative gene expression values ignore differences in cell sizes/number of molecules in the cell. There are some molecules whose numbers scale with the size of the cell, and others that don't (e.g., many housekeeping genes). Choosing relative over absolute expression values to compare gene expression across cells would be helpful to compare expression of those genes that scale with size, but not the others.... so there's not really a perfect answer here. Thus, removing all effects of total counts may not be the desirable outcome. Secondly, many downstream methods assume normally distributed expression data (e.g., DE methods like: t-tests, limma, MAST, or several batch correction/data integration methods). Log transformation is used as a variance stabilization to approximate a normal distribution (quite often poorly, but better than without). This leads to many methods performing better with log transformation. IMO, the ideal approach is probably something like scVI, GLMPCA, or scTransform, where you fit a model directly to the count data and use the residuals to describe the data. This would address both steps of normalization and variance stabilization at the same time. If we have a good model to describe the data, the residuals should",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:881,modifiability,scal,scale,881,"Hey @chris-rands,. This is a really interesting topic. Sorry in advance for the wordy reply... You are absolutely correct that log transformation removes the perfect comparison of relative expression values that mean normalization provides. Aside from CPM normalization (as provided by `sc.pp.normalize_total()`) not being a good normalization technique anyway (this is argued by any more advanced normalization methods paper, e.g., the [scran pooling paper](http://genomebiology.biomedcentral.com/articles/10.1186/s13059-016-0947-7)), there are a couple of things to consider here:. 1. Do we even want relative expression counts? 2. What assumptions do downstream methods have on the distribution of expression values. For the first question: relative gene expression values ignore differences in cell sizes/number of molecules in the cell. There are some molecules whose numbers scale with the size of the cell, and others that don't (e.g., many housekeeping genes). Choosing relative over absolute expression values to compare gene expression across cells would be helpful to compare expression of those genes that scale with size, but not the others.... so there's not really a perfect answer here. Thus, removing all effects of total counts may not be the desirable outcome. Secondly, many downstream methods assume normally distributed expression data (e.g., DE methods like: t-tests, limma, MAST, or several batch correction/data integration methods). Log transformation is used as a variance stabilization to approximate a normal distribution (quite often poorly, but better than without). This leads to many methods performing better with log transformation. IMO, the ideal approach is probably something like scVI, GLMPCA, or scTransform, where you fit a model directly to the count data and use the residuals to describe the data. This would address both steps of normalization and variance stabilization at the same time. If we have a good model to describe the data, the residuals should",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:1118,modifiability,scal,scale,1118," that log transformation removes the perfect comparison of relative expression values that mean normalization provides. Aside from CPM normalization (as provided by `sc.pp.normalize_total()`) not being a good normalization technique anyway (this is argued by any more advanced normalization methods paper, e.g., the [scran pooling paper](http://genomebiology.biomedcentral.com/articles/10.1186/s13059-016-0947-7)), there are a couple of things to consider here:. 1. Do we even want relative expression counts? 2. What assumptions do downstream methods have on the distribution of expression values. For the first question: relative gene expression values ignore differences in cell sizes/number of molecules in the cell. There are some molecules whose numbers scale with the size of the cell, and others that don't (e.g., many housekeeping genes). Choosing relative over absolute expression values to compare gene expression across cells would be helpful to compare expression of those genes that scale with size, but not the others.... so there's not really a perfect answer here. Thus, removing all effects of total counts may not be the desirable outcome. Secondly, many downstream methods assume normally distributed expression data (e.g., DE methods like: t-tests, limma, MAST, or several batch correction/data integration methods). Log transformation is used as a variance stabilization to approximate a normal distribution (quite often poorly, but better than without). This leads to many methods performing better with log transformation. IMO, the ideal approach is probably something like scVI, GLMPCA, or scTransform, where you fit a model directly to the count data and use the residuals to describe the data. This would address both steps of normalization and variance stabilization at the same time. If we have a good model to describe the data, the residuals should quantify the biological variance + normally distributed noise. Overall, I would use other normalization approaches than ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:1437,modifiability,integr,integration,1437," pooling paper](http://genomebiology.biomedcentral.com/articles/10.1186/s13059-016-0947-7)), there are a couple of things to consider here:. 1. Do we even want relative expression counts? 2. What assumptions do downstream methods have on the distribution of expression values. For the first question: relative gene expression values ignore differences in cell sizes/number of molecules in the cell. There are some molecules whose numbers scale with the size of the cell, and others that don't (e.g., many housekeeping genes). Choosing relative over absolute expression values to compare gene expression across cells would be helpful to compare expression of those genes that scale with size, but not the others.... so there's not really a perfect answer here. Thus, removing all effects of total counts may not be the desirable outcome. Secondly, many downstream methods assume normally distributed expression data (e.g., DE methods like: t-tests, limma, MAST, or several batch correction/data integration methods). Log transformation is used as a variance stabilization to approximate a normal distribution (quite often poorly, but better than without). This leads to many methods performing better with log transformation. IMO, the ideal approach is probably something like scVI, GLMPCA, or scTransform, where you fit a model directly to the count data and use the residuals to describe the data. This would address both steps of normalization and variance stabilization at the same time. If we have a good model to describe the data, the residuals should quantify the biological variance + normally distributed noise. Overall, I would use other normalization approaches than CPM, and use log-transformation with anything that uses size factors that scale per-cell expression values. . Note also that the effect described in the second paper you mention (from Aaron Lun) will mainly be relevant when you have biased distributions of sequencing depth between two samples that you are comparing. If t",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:2195,modifiability,scal,scale,2195,"o we even want relative expression counts? 2. What assumptions do downstream methods have on the distribution of expression values. For the first question: relative gene expression values ignore differences in cell sizes/number of molecules in the cell. There are some molecules whose numbers scale with the size of the cell, and others that don't (e.g., many housekeeping genes). Choosing relative over absolute expression values to compare gene expression across cells would be helpful to compare expression of those genes that scale with size, but not the others.... so there's not really a perfect answer here. Thus, removing all effects of total counts may not be the desirable outcome. Secondly, many downstream methods assume normally distributed expression data (e.g., DE methods like: t-tests, limma, MAST, or several batch correction/data integration methods). Log transformation is used as a variance stabilization to approximate a normal distribution (quite often poorly, but better than without). This leads to many methods performing better with log transformation. IMO, the ideal approach is probably something like scVI, GLMPCA, or scTransform, where you fit a model directly to the count data and use the residuals to describe the data. This would address both steps of normalization and variance stabilization at the same time. If we have a good model to describe the data, the residuals should quantify the biological variance + normally distributed noise. Overall, I would use other normalization approaches than CPM, and use log-transformation with anything that uses size factors that scale per-cell expression values. . Note also that the effect described in the second paper you mention (from Aaron Lun) will mainly be relevant when you have biased distributions of sequencing depth between two samples that you are comparing. If the size factors are similarly distributed between both conditions, then the DE effect will not be so dramatic (as far as I understood it anyway).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:881,performance,scale,scale,881,"Hey @chris-rands,. This is a really interesting topic. Sorry in advance for the wordy reply... You are absolutely correct that log transformation removes the perfect comparison of relative expression values that mean normalization provides. Aside from CPM normalization (as provided by `sc.pp.normalize_total()`) not being a good normalization technique anyway (this is argued by any more advanced normalization methods paper, e.g., the [scran pooling paper](http://genomebiology.biomedcentral.com/articles/10.1186/s13059-016-0947-7)), there are a couple of things to consider here:. 1. Do we even want relative expression counts? 2. What assumptions do downstream methods have on the distribution of expression values. For the first question: relative gene expression values ignore differences in cell sizes/number of molecules in the cell. There are some molecules whose numbers scale with the size of the cell, and others that don't (e.g., many housekeeping genes). Choosing relative over absolute expression values to compare gene expression across cells would be helpful to compare expression of those genes that scale with size, but not the others.... so there's not really a perfect answer here. Thus, removing all effects of total counts may not be the desirable outcome. Secondly, many downstream methods assume normally distributed expression data (e.g., DE methods like: t-tests, limma, MAST, or several batch correction/data integration methods). Log transformation is used as a variance stabilization to approximate a normal distribution (quite often poorly, but better than without). This leads to many methods performing better with log transformation. IMO, the ideal approach is probably something like scVI, GLMPCA, or scTransform, where you fit a model directly to the count data and use the residuals to describe the data. This would address both steps of normalization and variance stabilization at the same time. If we have a good model to describe the data, the residuals should",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:1118,performance,scale,scale,1118," that log transformation removes the perfect comparison of relative expression values that mean normalization provides. Aside from CPM normalization (as provided by `sc.pp.normalize_total()`) not being a good normalization technique anyway (this is argued by any more advanced normalization methods paper, e.g., the [scran pooling paper](http://genomebiology.biomedcentral.com/articles/10.1186/s13059-016-0947-7)), there are a couple of things to consider here:. 1. Do we even want relative expression counts? 2. What assumptions do downstream methods have on the distribution of expression values. For the first question: relative gene expression values ignore differences in cell sizes/number of molecules in the cell. There are some molecules whose numbers scale with the size of the cell, and others that don't (e.g., many housekeeping genes). Choosing relative over absolute expression values to compare gene expression across cells would be helpful to compare expression of those genes that scale with size, but not the others.... so there's not really a perfect answer here. Thus, removing all effects of total counts may not be the desirable outcome. Secondly, many downstream methods assume normally distributed expression data (e.g., DE methods like: t-tests, limma, MAST, or several batch correction/data integration methods). Log transformation is used as a variance stabilization to approximate a normal distribution (quite often poorly, but better than without). This leads to many methods performing better with log transformation. IMO, the ideal approach is probably something like scVI, GLMPCA, or scTransform, where you fit a model directly to the count data and use the residuals to describe the data. This would address both steps of normalization and variance stabilization at the same time. If we have a good model to describe the data, the residuals should quantify the biological variance + normally distributed noise. Overall, I would use other normalization approaches than ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:1415,performance,batch,batch,1415,"s paper, e.g., the [scran pooling paper](http://genomebiology.biomedcentral.com/articles/10.1186/s13059-016-0947-7)), there are a couple of things to consider here:. 1. Do we even want relative expression counts? 2. What assumptions do downstream methods have on the distribution of expression values. For the first question: relative gene expression values ignore differences in cell sizes/number of molecules in the cell. There are some molecules whose numbers scale with the size of the cell, and others that don't (e.g., many housekeeping genes). Choosing relative over absolute expression values to compare gene expression across cells would be helpful to compare expression of those genes that scale with size, but not the others.... so there's not really a perfect answer here. Thus, removing all effects of total counts may not be the desirable outcome. Secondly, many downstream methods assume normally distributed expression data (e.g., DE methods like: t-tests, limma, MAST, or several batch correction/data integration methods). Log transformation is used as a variance stabilization to approximate a normal distribution (quite often poorly, but better than without). This leads to many methods performing better with log transformation. IMO, the ideal approach is probably something like scVI, GLMPCA, or scTransform, where you fit a model directly to the count data and use the residuals to describe the data. This would address both steps of normalization and variance stabilization at the same time. If we have a good model to describe the data, the residuals should quantify the biological variance + normally distributed noise. Overall, I would use other normalization approaches than CPM, and use log-transformation with anything that uses size factors that scale per-cell expression values. . Note also that the effect described in the second paper you mention (from Aaron Lun) will mainly be relevant when you have biased distributions of sequencing depth between two samples tha",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:1625,performance,perform,performing,1625,"o we even want relative expression counts? 2. What assumptions do downstream methods have on the distribution of expression values. For the first question: relative gene expression values ignore differences in cell sizes/number of molecules in the cell. There are some molecules whose numbers scale with the size of the cell, and others that don't (e.g., many housekeeping genes). Choosing relative over absolute expression values to compare gene expression across cells would be helpful to compare expression of those genes that scale with size, but not the others.... so there's not really a perfect answer here. Thus, removing all effects of total counts may not be the desirable outcome. Secondly, many downstream methods assume normally distributed expression data (e.g., DE methods like: t-tests, limma, MAST, or several batch correction/data integration methods). Log transformation is used as a variance stabilization to approximate a normal distribution (quite often poorly, but better than without). This leads to many methods performing better with log transformation. IMO, the ideal approach is probably something like scVI, GLMPCA, or scTransform, where you fit a model directly to the count data and use the residuals to describe the data. This would address both steps of normalization and variance stabilization at the same time. If we have a good model to describe the data, the residuals should quantify the biological variance + normally distributed noise. Overall, I would use other normalization approaches than CPM, and use log-transformation with anything that uses size factors that scale per-cell expression values. . Note also that the effect described in the second paper you mention (from Aaron Lun) will mainly be relevant when you have biased distributions of sequencing depth between two samples that you are comparing. If the size factors are similarly distributed between both conditions, then the DE effect will not be so dramatic (as far as I understood it anyway).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:1928,performance,time,time,1928,"o we even want relative expression counts? 2. What assumptions do downstream methods have on the distribution of expression values. For the first question: relative gene expression values ignore differences in cell sizes/number of molecules in the cell. There are some molecules whose numbers scale with the size of the cell, and others that don't (e.g., many housekeeping genes). Choosing relative over absolute expression values to compare gene expression across cells would be helpful to compare expression of those genes that scale with size, but not the others.... so there's not really a perfect answer here. Thus, removing all effects of total counts may not be the desirable outcome. Secondly, many downstream methods assume normally distributed expression data (e.g., DE methods like: t-tests, limma, MAST, or several batch correction/data integration methods). Log transformation is used as a variance stabilization to approximate a normal distribution (quite often poorly, but better than without). This leads to many methods performing better with log transformation. IMO, the ideal approach is probably something like scVI, GLMPCA, or scTransform, where you fit a model directly to the count data and use the residuals to describe the data. This would address both steps of normalization and variance stabilization at the same time. If we have a good model to describe the data, the residuals should quantify the biological variance + normally distributed noise. Overall, I would use other normalization approaches than CPM, and use log-transformation with anything that uses size factors that scale per-cell expression values. . Note also that the effect described in the second paper you mention (from Aaron Lun) will mainly be relevant when you have biased distributions of sequencing depth between two samples that you are comparing. If the size factors are similarly distributed between both conditions, then the DE effect will not be so dramatic (as far as I understood it anyway).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:2195,performance,scale,scale,2195,"o we even want relative expression counts? 2. What assumptions do downstream methods have on the distribution of expression values. For the first question: relative gene expression values ignore differences in cell sizes/number of molecules in the cell. There are some molecules whose numbers scale with the size of the cell, and others that don't (e.g., many housekeeping genes). Choosing relative over absolute expression values to compare gene expression across cells would be helpful to compare expression of those genes that scale with size, but not the others.... so there's not really a perfect answer here. Thus, removing all effects of total counts may not be the desirable outcome. Secondly, many downstream methods assume normally distributed expression data (e.g., DE methods like: t-tests, limma, MAST, or several batch correction/data integration methods). Log transformation is used as a variance stabilization to approximate a normal distribution (quite often poorly, but better than without). This leads to many methods performing better with log transformation. IMO, the ideal approach is probably something like scVI, GLMPCA, or scTransform, where you fit a model directly to the count data and use the residuals to describe the data. This would address both steps of normalization and variance stabilization at the same time. If we have a good model to describe the data, the residuals should quantify the biological variance + normally distributed noise. Overall, I would use other normalization approaches than CPM, and use log-transformation with anything that uses size factors that scale per-cell expression values. . Note also that the effect described in the second paper you mention (from Aaron Lun) will mainly be relevant when you have biased distributions of sequencing depth between two samples that you are comparing. If the size factors are similarly distributed between both conditions, then the DE effect will not be so dramatic (as far as I understood it anyway).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:1437,reliability,integr,integration,1437," pooling paper](http://genomebiology.biomedcentral.com/articles/10.1186/s13059-016-0947-7)), there are a couple of things to consider here:. 1. Do we even want relative expression counts? 2. What assumptions do downstream methods have on the distribution of expression values. For the first question: relative gene expression values ignore differences in cell sizes/number of molecules in the cell. There are some molecules whose numbers scale with the size of the cell, and others that don't (e.g., many housekeeping genes). Choosing relative over absolute expression values to compare gene expression across cells would be helpful to compare expression of those genes that scale with size, but not the others.... so there's not really a perfect answer here. Thus, removing all effects of total counts may not be the desirable outcome. Secondly, many downstream methods assume normally distributed expression data (e.g., DE methods like: t-tests, limma, MAST, or several batch correction/data integration methods). Log transformation is used as a variance stabilization to approximate a normal distribution (quite often poorly, but better than without). This leads to many methods performing better with log transformation. IMO, the ideal approach is probably something like scVI, GLMPCA, or scTransform, where you fit a model directly to the count data and use the residuals to describe the data. This would address both steps of normalization and variance stabilization at the same time. If we have a good model to describe the data, the residuals should quantify the biological variance + normally distributed noise. Overall, I would use other normalization approaches than CPM, and use log-transformation with anything that uses size factors that scale per-cell expression values. . Note also that the effect described in the second paper you mention (from Aaron Lun) will mainly be relevant when you have biased distributions of sequencing depth between two samples that you are comparing. If t",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:1500,reliability,stabil,stabilization,1500,"10.1186/s13059-016-0947-7)), there are a couple of things to consider here:. 1. Do we even want relative expression counts? 2. What assumptions do downstream methods have on the distribution of expression values. For the first question: relative gene expression values ignore differences in cell sizes/number of molecules in the cell. There are some molecules whose numbers scale with the size of the cell, and others that don't (e.g., many housekeeping genes). Choosing relative over absolute expression values to compare gene expression across cells would be helpful to compare expression of those genes that scale with size, but not the others.... so there's not really a perfect answer here. Thus, removing all effects of total counts may not be the desirable outcome. Secondly, many downstream methods assume normally distributed expression data (e.g., DE methods like: t-tests, limma, MAST, or several batch correction/data integration methods). Log transformation is used as a variance stabilization to approximate a normal distribution (quite often poorly, but better than without). This leads to many methods performing better with log transformation. IMO, the ideal approach is probably something like scVI, GLMPCA, or scTransform, where you fit a model directly to the count data and use the residuals to describe the data. This would address both steps of normalization and variance stabilization at the same time. If we have a good model to describe the data, the residuals should quantify the biological variance + normally distributed noise. Overall, I would use other normalization approaches than CPM, and use log-transformation with anything that uses size factors that scale per-cell expression values. . Note also that the effect described in the second paper you mention (from Aaron Lun) will mainly be relevant when you have biased distributions of sequencing depth between two samples that you are comparing. If the size factors are similarly distributed between both condition",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:1902,reliability,stabil,stabilization,1902,"o we even want relative expression counts? 2. What assumptions do downstream methods have on the distribution of expression values. For the first question: relative gene expression values ignore differences in cell sizes/number of molecules in the cell. There are some molecules whose numbers scale with the size of the cell, and others that don't (e.g., many housekeeping genes). Choosing relative over absolute expression values to compare gene expression across cells would be helpful to compare expression of those genes that scale with size, but not the others.... so there's not really a perfect answer here. Thus, removing all effects of total counts may not be the desirable outcome. Secondly, many downstream methods assume normally distributed expression data (e.g., DE methods like: t-tests, limma, MAST, or several batch correction/data integration methods). Log transformation is used as a variance stabilization to approximate a normal distribution (quite often poorly, but better than without). This leads to many methods performing better with log transformation. IMO, the ideal approach is probably something like scVI, GLMPCA, or scTransform, where you fit a model directly to the count data and use the residuals to describe the data. This would address both steps of normalization and variance stabilization at the same time. If we have a good model to describe the data, the residuals should quantify the biological variance + normally distributed noise. Overall, I would use other normalization approaches than CPM, and use log-transformation with anything that uses size factors that scale per-cell expression values. . Note also that the effect described in the second paper you mention (from Aaron Lun) will mainly be relevant when you have biased distributions of sequencing depth between two samples that you are comparing. If the size factors are similarly distributed between both conditions, then the DE effect will not be so dramatic (as far as I understood it anyway).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:127,safety,log,log,127,"Hey @chris-rands,. This is a really interesting topic. Sorry in advance for the wordy reply... You are absolutely correct that log transformation removes the perfect comparison of relative expression values that mean normalization provides. Aside from CPM normalization (as provided by `sc.pp.normalize_total()`) not being a good normalization technique anyway (this is argued by any more advanced normalization methods paper, e.g., the [scran pooling paper](http://genomebiology.biomedcentral.com/articles/10.1186/s13059-016-0947-7)), there are a couple of things to consider here:. 1. Do we even want relative expression counts? 2. What assumptions do downstream methods have on the distribution of expression values. For the first question: relative gene expression values ignore differences in cell sizes/number of molecules in the cell. There are some molecules whose numbers scale with the size of the cell, and others that don't (e.g., many housekeeping genes). Choosing relative over absolute expression values to compare gene expression across cells would be helpful to compare expression of those genes that scale with size, but not the others.... so there's not really a perfect answer here. Thus, removing all effects of total counts may not be the desirable outcome. Secondly, many downstream methods assume normally distributed expression data (e.g., DE methods like: t-tests, limma, MAST, or several batch correction/data integration methods). Log transformation is used as a variance stabilization to approximate a normal distribution (quite often poorly, but better than without). This leads to many methods performing better with log transformation. IMO, the ideal approach is probably something like scVI, GLMPCA, or scTransform, where you fit a model directly to the count data and use the residuals to describe the data. This would address both steps of normalization and variance stabilization at the same time. If we have a good model to describe the data, the residuals should",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:1384,safety,test,tests,1384,"e advanced normalization methods paper, e.g., the [scran pooling paper](http://genomebiology.biomedcentral.com/articles/10.1186/s13059-016-0947-7)), there are a couple of things to consider here:. 1. Do we even want relative expression counts? 2. What assumptions do downstream methods have on the distribution of expression values. For the first question: relative gene expression values ignore differences in cell sizes/number of molecules in the cell. There are some molecules whose numbers scale with the size of the cell, and others that don't (e.g., many housekeeping genes). Choosing relative over absolute expression values to compare gene expression across cells would be helpful to compare expression of those genes that scale with size, but not the others.... so there's not really a perfect answer here. Thus, removing all effects of total counts may not be the desirable outcome. Secondly, many downstream methods assume normally distributed expression data (e.g., DE methods like: t-tests, limma, MAST, or several batch correction/data integration methods). Log transformation is used as a variance stabilization to approximate a normal distribution (quite often poorly, but better than without). This leads to many methods performing better with log transformation. IMO, the ideal approach is probably something like scVI, GLMPCA, or scTransform, where you fit a model directly to the count data and use the residuals to describe the data. This would address both steps of normalization and variance stabilization at the same time. If we have a good model to describe the data, the residuals should quantify the biological variance + normally distributed noise. Overall, I would use other normalization approaches than CPM, and use log-transformation with anything that uses size factors that scale per-cell expression values. . Note also that the effect described in the second paper you mention (from Aaron Lun) will mainly be relevant when you have biased distributions of sequencin",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:1459,safety,Log,Log,1459,"tp://genomebiology.biomedcentral.com/articles/10.1186/s13059-016-0947-7)), there are a couple of things to consider here:. 1. Do we even want relative expression counts? 2. What assumptions do downstream methods have on the distribution of expression values. For the first question: relative gene expression values ignore differences in cell sizes/number of molecules in the cell. There are some molecules whose numbers scale with the size of the cell, and others that don't (e.g., many housekeeping genes). Choosing relative over absolute expression values to compare gene expression across cells would be helpful to compare expression of those genes that scale with size, but not the others.... so there's not really a perfect answer here. Thus, removing all effects of total counts may not be the desirable outcome. Secondly, many downstream methods assume normally distributed expression data (e.g., DE methods like: t-tests, limma, MAST, or several batch correction/data integration methods). Log transformation is used as a variance stabilization to approximate a normal distribution (quite often poorly, but better than without). This leads to many methods performing better with log transformation. IMO, the ideal approach is probably something like scVI, GLMPCA, or scTransform, where you fit a model directly to the count data and use the residuals to describe the data. This would address both steps of normalization and variance stabilization at the same time. If we have a good model to describe the data, the residuals should quantify the biological variance + normally distributed noise. Overall, I would use other normalization approaches than CPM, and use log-transformation with anything that uses size factors that scale per-cell expression values. . Note also that the effect described in the second paper you mention (from Aaron Lun) will mainly be relevant when you have biased distributions of sequencing depth between two samples that you are comparing. If the size factors ar",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:1648,safety,log,log,1648,"o we even want relative expression counts? 2. What assumptions do downstream methods have on the distribution of expression values. For the first question: relative gene expression values ignore differences in cell sizes/number of molecules in the cell. There are some molecules whose numbers scale with the size of the cell, and others that don't (e.g., many housekeeping genes). Choosing relative over absolute expression values to compare gene expression across cells would be helpful to compare expression of those genes that scale with size, but not the others.... so there's not really a perfect answer here. Thus, removing all effects of total counts may not be the desirable outcome. Secondly, many downstream methods assume normally distributed expression data (e.g., DE methods like: t-tests, limma, MAST, or several batch correction/data integration methods). Log transformation is used as a variance stabilization to approximate a normal distribution (quite often poorly, but better than without). This leads to many methods performing better with log transformation. IMO, the ideal approach is probably something like scVI, GLMPCA, or scTransform, where you fit a model directly to the count data and use the residuals to describe the data. This would address both steps of normalization and variance stabilization at the same time. If we have a good model to describe the data, the residuals should quantify the biological variance + normally distributed noise. Overall, I would use other normalization approaches than CPM, and use log-transformation with anything that uses size factors that scale per-cell expression values. . Note also that the effect described in the second paper you mention (from Aaron Lun) will mainly be relevant when you have biased distributions of sequencing depth between two samples that you are comparing. If the size factors are similarly distributed between both conditions, then the DE effect will not be so dramatic (as far as I understood it anyway).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:2134,safety,log,log-transformation,2134,"o we even want relative expression counts? 2. What assumptions do downstream methods have on the distribution of expression values. For the first question: relative gene expression values ignore differences in cell sizes/number of molecules in the cell. There are some molecules whose numbers scale with the size of the cell, and others that don't (e.g., many housekeeping genes). Choosing relative over absolute expression values to compare gene expression across cells would be helpful to compare expression of those genes that scale with size, but not the others.... so there's not really a perfect answer here. Thus, removing all effects of total counts may not be the desirable outcome. Secondly, many downstream methods assume normally distributed expression data (e.g., DE methods like: t-tests, limma, MAST, or several batch correction/data integration methods). Log transformation is used as a variance stabilization to approximate a normal distribution (quite often poorly, but better than without). This leads to many methods performing better with log transformation. IMO, the ideal approach is probably something like scVI, GLMPCA, or scTransform, where you fit a model directly to the count data and use the residuals to describe the data. This would address both steps of normalization and variance stabilization at the same time. If we have a good model to describe the data, the residuals should quantify the biological variance + normally distributed noise. Overall, I would use other normalization approaches than CPM, and use log-transformation with anything that uses size factors that scale per-cell expression values. . Note also that the effect described in the second paper you mention (from Aaron Lun) will mainly be relevant when you have biased distributions of sequencing depth between two samples that you are comparing. If the size factors are similarly distributed between both conditions, then the DE effect will not be so dramatic (as far as I understood it anyway).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:127,security,log,log,127,"Hey @chris-rands,. This is a really interesting topic. Sorry in advance for the wordy reply... You are absolutely correct that log transformation removes the perfect comparison of relative expression values that mean normalization provides. Aside from CPM normalization (as provided by `sc.pp.normalize_total()`) not being a good normalization technique anyway (this is argued by any more advanced normalization methods paper, e.g., the [scran pooling paper](http://genomebiology.biomedcentral.com/articles/10.1186/s13059-016-0947-7)), there are a couple of things to consider here:. 1. Do we even want relative expression counts? 2. What assumptions do downstream methods have on the distribution of expression values. For the first question: relative gene expression values ignore differences in cell sizes/number of molecules in the cell. There are some molecules whose numbers scale with the size of the cell, and others that don't (e.g., many housekeeping genes). Choosing relative over absolute expression values to compare gene expression across cells would be helpful to compare expression of those genes that scale with size, but not the others.... so there's not really a perfect answer here. Thus, removing all effects of total counts may not be the desirable outcome. Secondly, many downstream methods assume normally distributed expression data (e.g., DE methods like: t-tests, limma, MAST, or several batch correction/data integration methods). Log transformation is used as a variance stabilization to approximate a normal distribution (quite often poorly, but better than without). This leads to many methods performing better with log transformation. IMO, the ideal approach is probably something like scVI, GLMPCA, or scTransform, where you fit a model directly to the count data and use the residuals to describe the data. This would address both steps of normalization and variance stabilization at the same time. If we have a good model to describe the data, the residuals should",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:1437,security,integr,integration,1437," pooling paper](http://genomebiology.biomedcentral.com/articles/10.1186/s13059-016-0947-7)), there are a couple of things to consider here:. 1. Do we even want relative expression counts? 2. What assumptions do downstream methods have on the distribution of expression values. For the first question: relative gene expression values ignore differences in cell sizes/number of molecules in the cell. There are some molecules whose numbers scale with the size of the cell, and others that don't (e.g., many housekeeping genes). Choosing relative over absolute expression values to compare gene expression across cells would be helpful to compare expression of those genes that scale with size, but not the others.... so there's not really a perfect answer here. Thus, removing all effects of total counts may not be the desirable outcome. Secondly, many downstream methods assume normally distributed expression data (e.g., DE methods like: t-tests, limma, MAST, or several batch correction/data integration methods). Log transformation is used as a variance stabilization to approximate a normal distribution (quite often poorly, but better than without). This leads to many methods performing better with log transformation. IMO, the ideal approach is probably something like scVI, GLMPCA, or scTransform, where you fit a model directly to the count data and use the residuals to describe the data. This would address both steps of normalization and variance stabilization at the same time. If we have a good model to describe the data, the residuals should quantify the biological variance + normally distributed noise. Overall, I would use other normalization approaches than CPM, and use log-transformation with anything that uses size factors that scale per-cell expression values. . Note also that the effect described in the second paper you mention (from Aaron Lun) will mainly be relevant when you have biased distributions of sequencing depth between two samples that you are comparing. If t",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:1459,security,Log,Log,1459,"tp://genomebiology.biomedcentral.com/articles/10.1186/s13059-016-0947-7)), there are a couple of things to consider here:. 1. Do we even want relative expression counts? 2. What assumptions do downstream methods have on the distribution of expression values. For the first question: relative gene expression values ignore differences in cell sizes/number of molecules in the cell. There are some molecules whose numbers scale with the size of the cell, and others that don't (e.g., many housekeeping genes). Choosing relative over absolute expression values to compare gene expression across cells would be helpful to compare expression of those genes that scale with size, but not the others.... so there's not really a perfect answer here. Thus, removing all effects of total counts may not be the desirable outcome. Secondly, many downstream methods assume normally distributed expression data (e.g., DE methods like: t-tests, limma, MAST, or several batch correction/data integration methods). Log transformation is used as a variance stabilization to approximate a normal distribution (quite often poorly, but better than without). This leads to many methods performing better with log transformation. IMO, the ideal approach is probably something like scVI, GLMPCA, or scTransform, where you fit a model directly to the count data and use the residuals to describe the data. This would address both steps of normalization and variance stabilization at the same time. If we have a good model to describe the data, the residuals should quantify the biological variance + normally distributed noise. Overall, I would use other normalization approaches than CPM, and use log-transformation with anything that uses size factors that scale per-cell expression values. . Note also that the effect described in the second paper you mention (from Aaron Lun) will mainly be relevant when you have biased distributions of sequencing depth between two samples that you are comparing. If the size factors ar",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:1648,security,log,log,1648,"o we even want relative expression counts? 2. What assumptions do downstream methods have on the distribution of expression values. For the first question: relative gene expression values ignore differences in cell sizes/number of molecules in the cell. There are some molecules whose numbers scale with the size of the cell, and others that don't (e.g., many housekeeping genes). Choosing relative over absolute expression values to compare gene expression across cells would be helpful to compare expression of those genes that scale with size, but not the others.... so there's not really a perfect answer here. Thus, removing all effects of total counts may not be the desirable outcome. Secondly, many downstream methods assume normally distributed expression data (e.g., DE methods like: t-tests, limma, MAST, or several batch correction/data integration methods). Log transformation is used as a variance stabilization to approximate a normal distribution (quite often poorly, but better than without). This leads to many methods performing better with log transformation. IMO, the ideal approach is probably something like scVI, GLMPCA, or scTransform, where you fit a model directly to the count data and use the residuals to describe the data. This would address both steps of normalization and variance stabilization at the same time. If we have a good model to describe the data, the residuals should quantify the biological variance + normally distributed noise. Overall, I would use other normalization approaches than CPM, and use log-transformation with anything that uses size factors that scale per-cell expression values. . Note also that the effect described in the second paper you mention (from Aaron Lun) will mainly be relevant when you have biased distributions of sequencing depth between two samples that you are comparing. If the size factors are similarly distributed between both conditions, then the DE effect will not be so dramatic (as far as I understood it anyway).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:1765,security,model,model,1765,"o we even want relative expression counts? 2. What assumptions do downstream methods have on the distribution of expression values. For the first question: relative gene expression values ignore differences in cell sizes/number of molecules in the cell. There are some molecules whose numbers scale with the size of the cell, and others that don't (e.g., many housekeeping genes). Choosing relative over absolute expression values to compare gene expression across cells would be helpful to compare expression of those genes that scale with size, but not the others.... so there's not really a perfect answer here. Thus, removing all effects of total counts may not be the desirable outcome. Secondly, many downstream methods assume normally distributed expression data (e.g., DE methods like: t-tests, limma, MAST, or several batch correction/data integration methods). Log transformation is used as a variance stabilization to approximate a normal distribution (quite often poorly, but better than without). This leads to many methods performing better with log transformation. IMO, the ideal approach is probably something like scVI, GLMPCA, or scTransform, where you fit a model directly to the count data and use the residuals to describe the data. This would address both steps of normalization and variance stabilization at the same time. If we have a good model to describe the data, the residuals should quantify the biological variance + normally distributed noise. Overall, I would use other normalization approaches than CPM, and use log-transformation with anything that uses size factors that scale per-cell expression values. . Note also that the effect described in the second paper you mention (from Aaron Lun) will mainly be relevant when you have biased distributions of sequencing depth between two samples that you are comparing. If the size factors are similarly distributed between both conditions, then the DE effect will not be so dramatic (as far as I understood it anyway).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:1952,security,model,model,1952,"o we even want relative expression counts? 2. What assumptions do downstream methods have on the distribution of expression values. For the first question: relative gene expression values ignore differences in cell sizes/number of molecules in the cell. There are some molecules whose numbers scale with the size of the cell, and others that don't (e.g., many housekeeping genes). Choosing relative over absolute expression values to compare gene expression across cells would be helpful to compare expression of those genes that scale with size, but not the others.... so there's not really a perfect answer here. Thus, removing all effects of total counts may not be the desirable outcome. Secondly, many downstream methods assume normally distributed expression data (e.g., DE methods like: t-tests, limma, MAST, or several batch correction/data integration methods). Log transformation is used as a variance stabilization to approximate a normal distribution (quite often poorly, but better than without). This leads to many methods performing better with log transformation. IMO, the ideal approach is probably something like scVI, GLMPCA, or scTransform, where you fit a model directly to the count data and use the residuals to describe the data. This would address both steps of normalization and variance stabilization at the same time. If we have a good model to describe the data, the residuals should quantify the biological variance + normally distributed noise. Overall, I would use other normalization approaches than CPM, and use log-transformation with anything that uses size factors that scale per-cell expression values. . Note also that the effect described in the second paper you mention (from Aaron Lun) will mainly be relevant when you have biased distributions of sequencing depth between two samples that you are comparing. If the size factors are similarly distributed between both conditions, then the DE effect will not be so dramatic (as far as I understood it anyway).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:2134,security,log,log-transformation,2134,"o we even want relative expression counts? 2. What assumptions do downstream methods have on the distribution of expression values. For the first question: relative gene expression values ignore differences in cell sizes/number of molecules in the cell. There are some molecules whose numbers scale with the size of the cell, and others that don't (e.g., many housekeeping genes). Choosing relative over absolute expression values to compare gene expression across cells would be helpful to compare expression of those genes that scale with size, but not the others.... so there's not really a perfect answer here. Thus, removing all effects of total counts may not be the desirable outcome. Secondly, many downstream methods assume normally distributed expression data (e.g., DE methods like: t-tests, limma, MAST, or several batch correction/data integration methods). Log transformation is used as a variance stabilization to approximate a normal distribution (quite often poorly, but better than without). This leads to many methods performing better with log transformation. IMO, the ideal approach is probably something like scVI, GLMPCA, or scTransform, where you fit a model directly to the count data and use the residuals to describe the data. This would address both steps of normalization and variance stabilization at the same time. If we have a good model to describe the data, the residuals should quantify the biological variance + normally distributed noise. Overall, I would use other normalization approaches than CPM, and use log-transformation with anything that uses size factors that scale per-cell expression values. . Note also that the effect described in the second paper you mention (from Aaron Lun) will mainly be relevant when you have biased distributions of sequencing depth between two samples that you are comparing. If the size factors are similarly distributed between both conditions, then the DE effect will not be so dramatic (as far as I understood it anyway).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:127,testability,log,log,127,"Hey @chris-rands,. This is a really interesting topic. Sorry in advance for the wordy reply... You are absolutely correct that log transformation removes the perfect comparison of relative expression values that mean normalization provides. Aside from CPM normalization (as provided by `sc.pp.normalize_total()`) not being a good normalization technique anyway (this is argued by any more advanced normalization methods paper, e.g., the [scran pooling paper](http://genomebiology.biomedcentral.com/articles/10.1186/s13059-016-0947-7)), there are a couple of things to consider here:. 1. Do we even want relative expression counts? 2. What assumptions do downstream methods have on the distribution of expression values. For the first question: relative gene expression values ignore differences in cell sizes/number of molecules in the cell. There are some molecules whose numbers scale with the size of the cell, and others that don't (e.g., many housekeeping genes). Choosing relative over absolute expression values to compare gene expression across cells would be helpful to compare expression of those genes that scale with size, but not the others.... so there's not really a perfect answer here. Thus, removing all effects of total counts may not be the desirable outcome. Secondly, many downstream methods assume normally distributed expression data (e.g., DE methods like: t-tests, limma, MAST, or several batch correction/data integration methods). Log transformation is used as a variance stabilization to approximate a normal distribution (quite often poorly, but better than without). This leads to many methods performing better with log transformation. IMO, the ideal approach is probably something like scVI, GLMPCA, or scTransform, where you fit a model directly to the count data and use the residuals to describe the data. This would address both steps of normalization and variance stabilization at the same time. If we have a good model to describe the data, the residuals should",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:548,testability,coupl,couple,548,"Hey @chris-rands,. This is a really interesting topic. Sorry in advance for the wordy reply... You are absolutely correct that log transformation removes the perfect comparison of relative expression values that mean normalization provides. Aside from CPM normalization (as provided by `sc.pp.normalize_total()`) not being a good normalization technique anyway (this is argued by any more advanced normalization methods paper, e.g., the [scran pooling paper](http://genomebiology.biomedcentral.com/articles/10.1186/s13059-016-0947-7)), there are a couple of things to consider here:. 1. Do we even want relative expression counts? 2. What assumptions do downstream methods have on the distribution of expression values. For the first question: relative gene expression values ignore differences in cell sizes/number of molecules in the cell. There are some molecules whose numbers scale with the size of the cell, and others that don't (e.g., many housekeeping genes). Choosing relative over absolute expression values to compare gene expression across cells would be helpful to compare expression of those genes that scale with size, but not the others.... so there's not really a perfect answer here. Thus, removing all effects of total counts may not be the desirable outcome. Secondly, many downstream methods assume normally distributed expression data (e.g., DE methods like: t-tests, limma, MAST, or several batch correction/data integration methods). Log transformation is used as a variance stabilization to approximate a normal distribution (quite often poorly, but better than without). This leads to many methods performing better with log transformation. IMO, the ideal approach is probably something like scVI, GLMPCA, or scTransform, where you fit a model directly to the count data and use the residuals to describe the data. This would address both steps of normalization and variance stabilization at the same time. If we have a good model to describe the data, the residuals should",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:1384,testability,test,tests,1384,"e advanced normalization methods paper, e.g., the [scran pooling paper](http://genomebiology.biomedcentral.com/articles/10.1186/s13059-016-0947-7)), there are a couple of things to consider here:. 1. Do we even want relative expression counts? 2. What assumptions do downstream methods have on the distribution of expression values. For the first question: relative gene expression values ignore differences in cell sizes/number of molecules in the cell. There are some molecules whose numbers scale with the size of the cell, and others that don't (e.g., many housekeeping genes). Choosing relative over absolute expression values to compare gene expression across cells would be helpful to compare expression of those genes that scale with size, but not the others.... so there's not really a perfect answer here. Thus, removing all effects of total counts may not be the desirable outcome. Secondly, many downstream methods assume normally distributed expression data (e.g., DE methods like: t-tests, limma, MAST, or several batch correction/data integration methods). Log transformation is used as a variance stabilization to approximate a normal distribution (quite often poorly, but better than without). This leads to many methods performing better with log transformation. IMO, the ideal approach is probably something like scVI, GLMPCA, or scTransform, where you fit a model directly to the count data and use the residuals to describe the data. This would address both steps of normalization and variance stabilization at the same time. If we have a good model to describe the data, the residuals should quantify the biological variance + normally distributed noise. Overall, I would use other normalization approaches than CPM, and use log-transformation with anything that uses size factors that scale per-cell expression values. . Note also that the effect described in the second paper you mention (from Aaron Lun) will mainly be relevant when you have biased distributions of sequencin",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:1437,testability,integr,integration,1437," pooling paper](http://genomebiology.biomedcentral.com/articles/10.1186/s13059-016-0947-7)), there are a couple of things to consider here:. 1. Do we even want relative expression counts? 2. What assumptions do downstream methods have on the distribution of expression values. For the first question: relative gene expression values ignore differences in cell sizes/number of molecules in the cell. There are some molecules whose numbers scale with the size of the cell, and others that don't (e.g., many housekeeping genes). Choosing relative over absolute expression values to compare gene expression across cells would be helpful to compare expression of those genes that scale with size, but not the others.... so there's not really a perfect answer here. Thus, removing all effects of total counts may not be the desirable outcome. Secondly, many downstream methods assume normally distributed expression data (e.g., DE methods like: t-tests, limma, MAST, or several batch correction/data integration methods). Log transformation is used as a variance stabilization to approximate a normal distribution (quite often poorly, but better than without). This leads to many methods performing better with log transformation. IMO, the ideal approach is probably something like scVI, GLMPCA, or scTransform, where you fit a model directly to the count data and use the residuals to describe the data. This would address both steps of normalization and variance stabilization at the same time. If we have a good model to describe the data, the residuals should quantify the biological variance + normally distributed noise. Overall, I would use other normalization approaches than CPM, and use log-transformation with anything that uses size factors that scale per-cell expression values. . Note also that the effect described in the second paper you mention (from Aaron Lun) will mainly be relevant when you have biased distributions of sequencing depth between two samples that you are comparing. If t",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:1459,testability,Log,Log,1459,"tp://genomebiology.biomedcentral.com/articles/10.1186/s13059-016-0947-7)), there are a couple of things to consider here:. 1. Do we even want relative expression counts? 2. What assumptions do downstream methods have on the distribution of expression values. For the first question: relative gene expression values ignore differences in cell sizes/number of molecules in the cell. There are some molecules whose numbers scale with the size of the cell, and others that don't (e.g., many housekeeping genes). Choosing relative over absolute expression values to compare gene expression across cells would be helpful to compare expression of those genes that scale with size, but not the others.... so there's not really a perfect answer here. Thus, removing all effects of total counts may not be the desirable outcome. Secondly, many downstream methods assume normally distributed expression data (e.g., DE methods like: t-tests, limma, MAST, or several batch correction/data integration methods). Log transformation is used as a variance stabilization to approximate a normal distribution (quite often poorly, but better than without). This leads to many methods performing better with log transformation. IMO, the ideal approach is probably something like scVI, GLMPCA, or scTransform, where you fit a model directly to the count data and use the residuals to describe the data. This would address both steps of normalization and variance stabilization at the same time. If we have a good model to describe the data, the residuals should quantify the biological variance + normally distributed noise. Overall, I would use other normalization approaches than CPM, and use log-transformation with anything that uses size factors that scale per-cell expression values. . Note also that the effect described in the second paper you mention (from Aaron Lun) will mainly be relevant when you have biased distributions of sequencing depth between two samples that you are comparing. If the size factors ar",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:1648,testability,log,log,1648,"o we even want relative expression counts? 2. What assumptions do downstream methods have on the distribution of expression values. For the first question: relative gene expression values ignore differences in cell sizes/number of molecules in the cell. There are some molecules whose numbers scale with the size of the cell, and others that don't (e.g., many housekeeping genes). Choosing relative over absolute expression values to compare gene expression across cells would be helpful to compare expression of those genes that scale with size, but not the others.... so there's not really a perfect answer here. Thus, removing all effects of total counts may not be the desirable outcome. Secondly, many downstream methods assume normally distributed expression data (e.g., DE methods like: t-tests, limma, MAST, or several batch correction/data integration methods). Log transformation is used as a variance stabilization to approximate a normal distribution (quite often poorly, but better than without). This leads to many methods performing better with log transformation. IMO, the ideal approach is probably something like scVI, GLMPCA, or scTransform, where you fit a model directly to the count data and use the residuals to describe the data. This would address both steps of normalization and variance stabilization at the same time. If we have a good model to describe the data, the residuals should quantify the biological variance + normally distributed noise. Overall, I would use other normalization approaches than CPM, and use log-transformation with anything that uses size factors that scale per-cell expression values. . Note also that the effect described in the second paper you mention (from Aaron Lun) will mainly be relevant when you have biased distributions of sequencing depth between two samples that you are comparing. If the size factors are similarly distributed between both conditions, then the DE effect will not be so dramatic (as far as I understood it anyway).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:2134,testability,log,log-transformation,2134,"o we even want relative expression counts? 2. What assumptions do downstream methods have on the distribution of expression values. For the first question: relative gene expression values ignore differences in cell sizes/number of molecules in the cell. There are some molecules whose numbers scale with the size of the cell, and others that don't (e.g., many housekeeping genes). Choosing relative over absolute expression values to compare gene expression across cells would be helpful to compare expression of those genes that scale with size, but not the others.... so there's not really a perfect answer here. Thus, removing all effects of total counts may not be the desirable outcome. Secondly, many downstream methods assume normally distributed expression data (e.g., DE methods like: t-tests, limma, MAST, or several batch correction/data integration methods). Log transformation is used as a variance stabilization to approximate a normal distribution (quite often poorly, but better than without). This leads to many methods performing better with log transformation. IMO, the ideal approach is probably something like scVI, GLMPCA, or scTransform, where you fit a model directly to the count data and use the residuals to describe the data. This would address both steps of normalization and variance stabilization at the same time. If we have a good model to describe the data, the residuals should quantify the biological variance + normally distributed noise. Overall, I would use other normalization approaches than CPM, and use log-transformation with anything that uses size factors that scale per-cell expression values. . Note also that the effect described in the second paper you mention (from Aaron Lun) will mainly be relevant when you have biased distributions of sequencing depth between two samples that you are comparing. If the size factors are similarly distributed between both conditions, then the DE effect will not be so dramatic (as far as I understood it anyway).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:1068,usability,help,helpful,1068,"for the wordy reply... You are absolutely correct that log transformation removes the perfect comparison of relative expression values that mean normalization provides. Aside from CPM normalization (as provided by `sc.pp.normalize_total()`) not being a good normalization technique anyway (this is argued by any more advanced normalization methods paper, e.g., the [scran pooling paper](http://genomebiology.biomedcentral.com/articles/10.1186/s13059-016-0947-7)), there are a couple of things to consider here:. 1. Do we even want relative expression counts? 2. What assumptions do downstream methods have on the distribution of expression values. For the first question: relative gene expression values ignore differences in cell sizes/number of molecules in the cell. There are some molecules whose numbers scale with the size of the cell, and others that don't (e.g., many housekeeping genes). Choosing relative over absolute expression values to compare gene expression across cells would be helpful to compare expression of those genes that scale with size, but not the others.... so there's not really a perfect answer here. Thus, removing all effects of total counts may not be the desirable outcome. Secondly, many downstream methods assume normally distributed expression data (e.g., DE methods like: t-tests, limma, MAST, or several batch correction/data integration methods). Log transformation is used as a variance stabilization to approximate a normal distribution (quite often poorly, but better than without). This leads to many methods performing better with log transformation. IMO, the ideal approach is probably something like scVI, GLMPCA, or scTransform, where you fit a model directly to the count data and use the residuals to describe the data. This would address both steps of normalization and variance stabilization at the same time. If we have a good model to describe the data, the residuals should quantify the biological variance + normally distributed noise. Overall,",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:1625,usability,perform,performing,1625,"o we even want relative expression counts? 2. What assumptions do downstream methods have on the distribution of expression values. For the first question: relative gene expression values ignore differences in cell sizes/number of molecules in the cell. There are some molecules whose numbers scale with the size of the cell, and others that don't (e.g., many housekeeping genes). Choosing relative over absolute expression values to compare gene expression across cells would be helpful to compare expression of those genes that scale with size, but not the others.... so there's not really a perfect answer here. Thus, removing all effects of total counts may not be the desirable outcome. Secondly, many downstream methods assume normally distributed expression data (e.g., DE methods like: t-tests, limma, MAST, or several batch correction/data integration methods). Log transformation is used as a variance stabilization to approximate a normal distribution (quite often poorly, but better than without). This leads to many methods performing better with log transformation. IMO, the ideal approach is probably something like scVI, GLMPCA, or scTransform, where you fit a model directly to the count data and use the residuals to describe the data. This would address both steps of normalization and variance stabilization at the same time. If we have a good model to describe the data, the residuals should quantify the biological variance + normally distributed noise. Overall, I would use other normalization approaches than CPM, and use log-transformation with anything that uses size factors that scale per-cell expression values. . Note also that the effect described in the second paper you mention (from Aaron Lun) will mainly be relevant when you have biased distributions of sequencing depth between two samples that you are comparing. If the size factors are similarly distributed between both conditions, then the DE effect will not be so dramatic (as far as I understood it anyway).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:174,deployability,log,log,174,"Thank you very much @LuckyMD for those insightful comments, that gives me plenty to think about beyond my original question about the mean normalisation being 'distorted' by log transformation. I had not considered how tricky a problem normalisation is. This actually makes me feel that perhaps the long term solution will be mostly experimental rather than computational, through developing better spike-ins. Anyway, I guess this discussion is not a Scanpy issue, so I will close this, but I appreciate your thoughts.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:178,integrability,transform,transformation,178,"Thank you very much @LuckyMD for those insightful comments, that gives me plenty to think about beyond my original question about the mean normalisation being 'distorted' by log transformation. I had not considered how tricky a problem normalisation is. This actually makes me feel that perhaps the long term solution will be mostly experimental rather than computational, through developing better spike-ins. Anyway, I guess this discussion is not a Scanpy issue, so I will close this, but I appreciate your thoughts.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:178,interoperability,transform,transformation,178,"Thank you very much @LuckyMD for those insightful comments, that gives me plenty to think about beyond my original question about the mean normalisation being 'distorted' by log transformation. I had not considered how tricky a problem normalisation is. This actually makes me feel that perhaps the long term solution will be mostly experimental rather than computational, through developing better spike-ins. Anyway, I guess this discussion is not a Scanpy issue, so I will close this, but I appreciate your thoughts.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:174,safety,log,log,174,"Thank you very much @LuckyMD for those insightful comments, that gives me plenty to think about beyond my original question about the mean normalisation being 'distorted' by log transformation. I had not considered how tricky a problem normalisation is. This actually makes me feel that perhaps the long term solution will be mostly experimental rather than computational, through developing better spike-ins. Anyway, I guess this discussion is not a Scanpy issue, so I will close this, but I appreciate your thoughts.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:174,security,log,log,174,"Thank you very much @LuckyMD for those insightful comments, that gives me plenty to think about beyond my original question about the mean normalisation being 'distorted' by log transformation. I had not considered how tricky a problem normalisation is. This actually makes me feel that perhaps the long term solution will be mostly experimental rather than computational, through developing better spike-ins. Anyway, I guess this discussion is not a Scanpy issue, so I will close this, but I appreciate your thoughts.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:174,testability,log,log,174,"Thank you very much @LuckyMD for those insightful comments, that gives me plenty to think about beyond my original question about the mean normalisation being 'distorted' by log transformation. I had not considered how tricky a problem normalisation is. This actually makes me feel that perhaps the long term solution will be mostly experimental rather than computational, through developing better spike-ins. Anyway, I guess this discussion is not a Scanpy issue, so I will close this, but I appreciate your thoughts.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:475,usability,close,close,475,"Thank you very much @LuckyMD for those insightful comments, that gives me plenty to think about beyond my original question about the mean normalisation being 'distorted' by log transformation. I had not considered how tricky a problem normalisation is. This actually makes me feel that perhaps the long term solution will be mostly experimental rather than computational, through developing better spike-ins. Anyway, I guess this discussion is not a Scanpy issue, so I will close this, but I appreciate your thoughts.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:173,energy efficiency,model,model,173,"I'm not sure spike-ins will be the solution, as there will probably always be differences due to tissue preparation and cell lysis efficiency as well, which spike-ins can't model... at least as far as I understand the limitations here. I'd be curious if you think otherwise though :).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:173,security,model,model,173,"I'm not sure spike-ins will be the solution, as there will probably always be differences due to tissue preparation and cell lysis efficiency as well, which spike-ins can't model... at least as far as I understand the limitations here. I'd be curious if you think otherwise though :).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:203,testability,understand,understand,203,"I'm not sure spike-ins will be the solution, as there will probably always be differences due to tissue preparation and cell lysis efficiency as well, which spike-ins can't model... at least as far as I understand the limitations here. I'd be curious if you think otherwise though :).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:131,usability,efficien,efficiency,131,"I'm not sure spike-ins will be the solution, as there will probably always be differences due to tissue preparation and cell lysis efficiency as well, which spike-ins can't model... at least as far as I understand the limitations here. I'd be curious if you think otherwise though :).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:157,modifiability,variab,variable,157,"Good point that spike-ins may not always behave like endogenous transcripts. However, since spike-ins can account for the later biases in the workflow (e.g. variable sequencing depth, capture efficiencies, amplification etc.), I do think that they are still an important part of the solution and a step closer to absolute quantification- would you disagree?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:142,usability,workflow,workflow,142,"Good point that spike-ins may not always behave like endogenous transcripts. However, since spike-ins can account for the later biases in the workflow (e.g. variable sequencing depth, capture efficiencies, amplification etc.), I do think that they are still an important part of the solution and a step closer to absolute quantification- would you disagree?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:192,usability,efficien,efficiencies,192,"Good point that spike-ins may not always behave like endogenous transcripts. However, since spike-ins can account for the later biases in the workflow (e.g. variable sequencing depth, capture efficiencies, amplification etc.), I do think that they are still an important part of the solution and a step closer to absolute quantification- would you disagree?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:303,usability,close,closer,303,"Good point that spike-ins may not always behave like endogenous transcripts. However, since spike-ins can account for the later biases in the workflow (e.g. variable sequencing depth, capture efficiencies, amplification etc.), I do think that they are still an important part of the solution and a step closer to absolute quantification- would you disagree?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:488,energy efficiency,model,modeled,488,"I'm not sure spike-ins are so helpful if they only account for part of the technical effects that have to be normalized out. In the end you will have the same problem with the remaining effects you are not capturing on which you don't have a good experimental handle. Unless you can spike into a tissue directly somehow? I quite like the idea of spike-in cells though for batch effects, but those have similar limitations as well. . In general, I'm yet to see a spike-in approach that if modeled and used to normalize your data would compare well against a good normalization method. That doesn't mean it doesn't exist... but it seems that since droplet-based techniques took off spike-ins are being avoided as they just increase sequencing costs and the larger droplet-based datasets allow us to use model-based normalization techniques.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:801,energy efficiency,model,model-based,801,"I'm not sure spike-ins are so helpful if they only account for part of the technical effects that have to be normalized out. In the end you will have the same problem with the remaining effects you are not capturing on which you don't have a good experimental handle. Unless you can spike into a tissue directly somehow? I quite like the idea of spike-in cells though for batch effects, but those have similar limitations as well. . In general, I'm yet to see a spike-in approach that if modeled and used to normalize your data would compare well against a good normalization method. That doesn't mean it doesn't exist... but it seems that since droplet-based techniques took off spike-ins are being avoided as they just increase sequencing costs and the larger droplet-based datasets allow us to use model-based normalization techniques.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:372,integrability,batch,batch,372,"I'm not sure spike-ins are so helpful if they only account for part of the technical effects that have to be normalized out. In the end you will have the same problem with the remaining effects you are not capturing on which you don't have a good experimental handle. Unless you can spike into a tissue directly somehow? I quite like the idea of spike-in cells though for batch effects, but those have similar limitations as well. . In general, I'm yet to see a spike-in approach that if modeled and used to normalize your data would compare well against a good normalization method. That doesn't mean it doesn't exist... but it seems that since droplet-based techniques took off spike-ins are being avoided as they just increase sequencing costs and the larger droplet-based datasets allow us to use model-based normalization techniques.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:372,performance,batch,batch,372,"I'm not sure spike-ins are so helpful if they only account for part of the technical effects that have to be normalized out. In the end you will have the same problem with the remaining effects you are not capturing on which you don't have a good experimental handle. Unless you can spike into a tissue directly somehow? I quite like the idea of spike-in cells though for batch effects, but those have similar limitations as well. . In general, I'm yet to see a spike-in approach that if modeled and used to normalize your data would compare well against a good normalization method. That doesn't mean it doesn't exist... but it seems that since droplet-based techniques took off spike-ins are being avoided as they just increase sequencing costs and the larger droplet-based datasets allow us to use model-based normalization techniques.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:589,reliability,doe,doesn,589,"I'm not sure spike-ins are so helpful if they only account for part of the technical effects that have to be normalized out. In the end you will have the same problem with the remaining effects you are not capturing on which you don't have a good experimental handle. Unless you can spike into a tissue directly somehow? I quite like the idea of spike-in cells though for batch effects, but those have similar limitations as well. . In general, I'm yet to see a spike-in approach that if modeled and used to normalize your data would compare well against a good normalization method. That doesn't mean it doesn't exist... but it seems that since droplet-based techniques took off spike-ins are being avoided as they just increase sequencing costs and the larger droplet-based datasets allow us to use model-based normalization techniques.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:605,reliability,doe,doesn,605,"I'm not sure spike-ins are so helpful if they only account for part of the technical effects that have to be normalized out. In the end you will have the same problem with the remaining effects you are not capturing on which you don't have a good experimental handle. Unless you can spike into a tissue directly somehow? I quite like the idea of spike-in cells though for batch effects, but those have similar limitations as well. . In general, I'm yet to see a spike-in approach that if modeled and used to normalize your data would compare well against a good normalization method. That doesn't mean it doesn't exist... but it seems that since droplet-based techniques took off spike-ins are being avoided as they just increase sequencing costs and the larger droplet-based datasets allow us to use model-based normalization techniques.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:700,safety,avoid,avoided,700,"I'm not sure spike-ins are so helpful if they only account for part of the technical effects that have to be normalized out. In the end you will have the same problem with the remaining effects you are not capturing on which you don't have a good experimental handle. Unless you can spike into a tissue directly somehow? I quite like the idea of spike-in cells though for batch effects, but those have similar limitations as well. . In general, I'm yet to see a spike-in approach that if modeled and used to normalize your data would compare well against a good normalization method. That doesn't mean it doesn't exist... but it seems that since droplet-based techniques took off spike-ins are being avoided as they just increase sequencing costs and the larger droplet-based datasets allow us to use model-based normalization techniques.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:488,security,model,modeled,488,"I'm not sure spike-ins are so helpful if they only account for part of the technical effects that have to be normalized out. In the end you will have the same problem with the remaining effects you are not capturing on which you don't have a good experimental handle. Unless you can spike into a tissue directly somehow? I quite like the idea of spike-in cells though for batch effects, but those have similar limitations as well. . In general, I'm yet to see a spike-in approach that if modeled and used to normalize your data would compare well against a good normalization method. That doesn't mean it doesn't exist... but it seems that since droplet-based techniques took off spike-ins are being avoided as they just increase sequencing costs and the larger droplet-based datasets allow us to use model-based normalization techniques.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:801,security,model,model-based,801,"I'm not sure spike-ins are so helpful if they only account for part of the technical effects that have to be normalized out. In the end you will have the same problem with the remaining effects you are not capturing on which you don't have a good experimental handle. Unless you can spike into a tissue directly somehow? I quite like the idea of spike-in cells though for batch effects, but those have similar limitations as well. . In general, I'm yet to see a spike-in approach that if modeled and used to normalize your data would compare well against a good normalization method. That doesn't mean it doesn't exist... but it seems that since droplet-based techniques took off spike-ins are being avoided as they just increase sequencing costs and the larger droplet-based datasets allow us to use model-based normalization techniques.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:30,usability,help,helpful,30,"I'm not sure spike-ins are so helpful if they only account for part of the technical effects that have to be normalized out. In the end you will have the same problem with the remaining effects you are not capturing on which you don't have a good experimental handle. Unless you can spike into a tissue directly somehow? I quite like the idea of spike-in cells though for batch effects, but those have similar limitations as well. . In general, I'm yet to see a spike-in approach that if modeled and used to normalize your data would compare well against a good normalization method. That doesn't mean it doesn't exist... but it seems that since droplet-based techniques took off spike-ins are being avoided as they just increase sequencing costs and the larger droplet-based datasets allow us to use model-based normalization techniques.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:137,availability,consist,consistent,137,"> Unless you can spike into a tissue directly somehow? You can transfect RNA into cells, but the amount of RNA taken up will not be very consistent, so unlikely to be useful for quantitative information. Do you have an idea for how much bias is actually introduced by tissue prep and cell lysis? Anyway, all the scRNA-seq data I have actually worked with is droplet based and I certainly agree in the value of model-based normalisation techniques. I suspect most new scanpy users will just follow the tutorial and use `sc.pp.normalize_total()` so thanks again for highlighting the limitations of this",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:410,energy efficiency,model,model-based,410,"> Unless you can spike into a tissue directly somehow? You can transfect RNA into cells, but the amount of RNA taken up will not be very consistent, so unlikely to be useful for quantitative information. Do you have an idea for how much bias is actually introduced by tissue prep and cell lysis? Anyway, all the scRNA-seq data I have actually worked with is droplet based and I certainly agree in the value of model-based normalisation techniques. I suspect most new scanpy users will just follow the tutorial and use `sc.pp.normalize_total()` so thanks again for highlighting the limitations of this",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:410,security,model,model-based,410,"> Unless you can spike into a tissue directly somehow? You can transfect RNA into cells, but the amount of RNA taken up will not be very consistent, so unlikely to be useful for quantitative information. Do you have an idea for how much bias is actually introduced by tissue prep and cell lysis? Anyway, all the scRNA-seq data I have actually worked with is droplet based and I certainly agree in the value of model-based normalisation techniques. I suspect most new scanpy users will just follow the tutorial and use `sc.pp.normalize_total()` so thanks again for highlighting the limitations of this",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:137,usability,consist,consistent,137,"> Unless you can spike into a tissue directly somehow? You can transfect RNA into cells, but the amount of RNA taken up will not be very consistent, so unlikely to be useful for quantitative information. Do you have an idea for how much bias is actually introduced by tissue prep and cell lysis? Anyway, all the scRNA-seq data I have actually worked with is droplet based and I certainly agree in the value of model-based normalisation techniques. I suspect most new scanpy users will just follow the tutorial and use `sc.pp.normalize_total()` so thanks again for highlighting the limitations of this",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1364:474,usability,user,users,474,"> Unless you can spike into a tissue directly somehow? You can transfect RNA into cells, but the amount of RNA taken up will not be very consistent, so unlikely to be useful for quantitative information. Do you have an idea for how much bias is actually introduced by tissue prep and cell lysis? Anyway, all the scRNA-seq data I have actually worked with is droplet based and I certainly agree in the value of model-based normalisation techniques. I suspect most new scanpy users will just follow the tutorial and use `sc.pp.normalize_total()` so thanks again for highlighting the limitations of this",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364
https://github.com/scverse/scanpy/issues/1366:25,availability,cluster,clusters,25,"Theres no way that many clusters will be discernible by color, you need to find another way to get the information you are interested in. If you just want a pretty picture without use, you can of course set `palette` to a list that contains >100 colors.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:25,deployability,cluster,clusters,25,"Theres no way that many clusters will be discernible by color, you need to find another way to get the information you are interested in. If you just want a pretty picture without use, you can of course set `palette` to a list that contains >100 colors.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:233,deployability,contain,contains,233,"Theres no way that many clusters will be discernible by color, you need to find another way to get the information you are interested in. If you just want a pretty picture without use, you can of course set `palette` to a list that contains >100 colors.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:144,availability,cluster,clusters,144,Maybe I was not clear. The goal was not to use a different color for each cell type. The goal was similar to drawing a world map where adjacent clusters have different colors. It doesn't matter if colors are repeated as long as they don't visually merge adjacent clusters. Setting palette abolished the gray dominance and temporarily solved the problem for some local regions. But I guess to have perfectly separated coloring I need to manually pick which color is for which cluster.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:263,availability,cluster,clusters,263,Maybe I was not clear. The goal was not to use a different color for each cell type. The goal was similar to drawing a world map where adjacent clusters have different colors. It doesn't matter if colors are repeated as long as they don't visually merge adjacent clusters. Setting palette abolished the gray dominance and temporarily solved the problem for some local regions. But I guess to have perfectly separated coloring I need to manually pick which color is for which cluster.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:475,availability,cluster,cluster,475,Maybe I was not clear. The goal was not to use a different color for each cell type. The goal was similar to drawing a world map where adjacent clusters have different colors. It doesn't matter if colors are repeated as long as they don't visually merge adjacent clusters. Setting palette abolished the gray dominance and temporarily solved the problem for some local regions. But I guess to have perfectly separated coloring I need to manually pick which color is for which cluster.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:144,deployability,cluster,clusters,144,Maybe I was not clear. The goal was not to use a different color for each cell type. The goal was similar to drawing a world map where adjacent clusters have different colors. It doesn't matter if colors are repeated as long as they don't visually merge adjacent clusters. Setting palette abolished the gray dominance and temporarily solved the problem for some local regions. But I guess to have perfectly separated coloring I need to manually pick which color is for which cluster.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:263,deployability,cluster,clusters,263,Maybe I was not clear. The goal was not to use a different color for each cell type. The goal was similar to drawing a world map where adjacent clusters have different colors. It doesn't matter if colors are repeated as long as they don't visually merge adjacent clusters. Setting palette abolished the gray dominance and temporarily solved the problem for some local regions. But I guess to have perfectly separated coloring I need to manually pick which color is for which cluster.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:475,deployability,cluster,cluster,475,Maybe I was not clear. The goal was not to use a different color for each cell type. The goal was similar to drawing a world map where adjacent clusters have different colors. It doesn't matter if colors are repeated as long as they don't visually merge adjacent clusters. Setting palette abolished the gray dominance and temporarily solved the problem for some local regions. But I guess to have perfectly separated coloring I need to manually pick which color is for which cluster.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:109,energy efficiency,draw,drawing,109,Maybe I was not clear. The goal was not to use a different color for each cell type. The goal was similar to drawing a world map where adjacent clusters have different colors. It doesn't matter if colors are repeated as long as they don't visually merge adjacent clusters. Setting palette abolished the gray dominance and temporarily solved the problem for some local regions. But I guess to have perfectly separated coloring I need to manually pick which color is for which cluster.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:179,reliability,doe,doesn,179,Maybe I was not clear. The goal was not to use a different color for each cell type. The goal was similar to drawing a world map where adjacent clusters have different colors. It doesn't matter if colors are repeated as long as they don't visually merge adjacent clusters. Setting palette abolished the gray dominance and temporarily solved the problem for some local regions. But I guess to have perfectly separated coloring I need to manually pick which color is for which cluster.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:16,usability,clear,clear,16,Maybe I was not clear. The goal was not to use a different color for each cell type. The goal was similar to drawing a world map where adjacent clusters have different colors. It doesn't matter if colors are repeated as long as they don't visually merge adjacent clusters. Setting palette abolished the gray dominance and temporarily solved the problem for some local regions. But I guess to have perfectly separated coloring I need to manually pick which color is for which cluster.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:239,usability,visual,visually,239,Maybe I was not clear. The goal was not to use a different color for each cell type. The goal was similar to drawing a world map where adjacent clusters have different colors. It doesn't matter if colors are repeated as long as they don't visually merge adjacent clusters. Setting palette abolished the gray dominance and temporarily solved the problem for some local regions. But I guess to have perfectly separated coloring I need to manually pick which color is for which cluster.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:163,availability,cluster,clustering,163,"Yes, I think there should be algorithms for that. This is a [graph coloring](https://en.wikipedia.org/wiki/Graph_coloring) problem: You can make a graph from your clustering, where clusters become nodes and touching clusters get an edge between them. If you find a nice way to implement this, please consider adding it in a PR.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:181,availability,cluster,clusters,181,"Yes, I think there should be algorithms for that. This is a [graph coloring](https://en.wikipedia.org/wiki/Graph_coloring) problem: You can make a graph from your clustering, where clusters become nodes and touching clusters get an edge between them. If you find a nice way to implement this, please consider adding it in a PR.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:216,availability,cluster,clusters,216,"Yes, I think there should be algorithms for that. This is a [graph coloring](https://en.wikipedia.org/wiki/Graph_coloring) problem: You can make a graph from your clustering, where clusters become nodes and touching clusters get an edge between them. If you find a nice way to implement this, please consider adding it in a PR.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:163,deployability,cluster,clustering,163,"Yes, I think there should be algorithms for that. This is a [graph coloring](https://en.wikipedia.org/wiki/Graph_coloring) problem: You can make a graph from your clustering, where clusters become nodes and touching clusters get an edge between them. If you find a nice way to implement this, please consider adding it in a PR.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:181,deployability,cluster,clusters,181,"Yes, I think there should be algorithms for that. This is a [graph coloring](https://en.wikipedia.org/wiki/Graph_coloring) problem: You can make a graph from your clustering, where clusters become nodes and touching clusters get an edge between them. If you find a nice way to implement this, please consider adding it in a PR.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:216,deployability,cluster,clusters,216,"Yes, I think there should be algorithms for that. This is a [graph coloring](https://en.wikipedia.org/wiki/Graph_coloring) problem: You can make a graph from your clustering, where clusters become nodes and touching clusters get an edge between them. If you find a nice way to implement this, please consider adding it in a PR.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:89,availability,cluster,cluster,89,On more issue to consider: entities on maps tend to be contiguous. The set of cells in a cluster do not have to be adjacent. How can it be clear two non-adjacent cells are from the same cluster if colors can be repeated?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:186,availability,cluster,cluster,186,On more issue to consider: entities on maps tend to be contiguous. The set of cells in a cluster do not have to be adjacent. How can it be clear two non-adjacent cells are from the same cluster if colors can be repeated?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:89,deployability,cluster,cluster,89,On more issue to consider: entities on maps tend to be contiguous. The set of cells in a cluster do not have to be adjacent. How can it be clear two non-adjacent cells are from the same cluster if colors can be repeated?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:186,deployability,cluster,cluster,186,On more issue to consider: entities on maps tend to be contiguous. The set of cells in a cluster do not have to be adjacent. How can it be clear two non-adjacent cells are from the same cluster if colors can be repeated?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:139,usability,clear,clear,139,On more issue to consider: entities on maps tend to be contiguous. The set of cells in a cluster do not have to be adjacent. How can it be clear two non-adjacent cells are from the same cluster if colors can be repeated?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:15,availability,Cluster,Cluster,15,One idea: . 1) Cluster the graph with leiden. 2) Coarsen the graph (collapse cells in a single cluster into super nodes). 3) Assign each supernode a color -- adjacent supernodes in the coarsened graph cannot be the same color. 4) Assign all cells in each cluster that cluster's color. That way you'd probably get nice-looking patches of colors and wouldn't run into the issue @ivirshup mentioned.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:95,availability,cluster,cluster,95,One idea: . 1) Cluster the graph with leiden. 2) Coarsen the graph (collapse cells in a single cluster into super nodes). 3) Assign each supernode a color -- adjacent supernodes in the coarsened graph cannot be the same color. 4) Assign all cells in each cluster that cluster's color. That way you'd probably get nice-looking patches of colors and wouldn't run into the issue @ivirshup mentioned.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:255,availability,cluster,cluster,255,One idea: . 1) Cluster the graph with leiden. 2) Coarsen the graph (collapse cells in a single cluster into super nodes). 3) Assign each supernode a color -- adjacent supernodes in the coarsened graph cannot be the same color. 4) Assign all cells in each cluster that cluster's color. That way you'd probably get nice-looking patches of colors and wouldn't run into the issue @ivirshup mentioned.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:268,availability,cluster,cluster,268,One idea: . 1) Cluster the graph with leiden. 2) Coarsen the graph (collapse cells in a single cluster into super nodes). 3) Assign each supernode a color -- adjacent supernodes in the coarsened graph cannot be the same color. 4) Assign all cells in each cluster that cluster's color. That way you'd probably get nice-looking patches of colors and wouldn't run into the issue @ivirshup mentioned.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:15,deployability,Cluster,Cluster,15,One idea: . 1) Cluster the graph with leiden. 2) Coarsen the graph (collapse cells in a single cluster into super nodes). 3) Assign each supernode a color -- adjacent supernodes in the coarsened graph cannot be the same color. 4) Assign all cells in each cluster that cluster's color. That way you'd probably get nice-looking patches of colors and wouldn't run into the issue @ivirshup mentioned.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:95,deployability,cluster,cluster,95,One idea: . 1) Cluster the graph with leiden. 2) Coarsen the graph (collapse cells in a single cluster into super nodes). 3) Assign each supernode a color -- adjacent supernodes in the coarsened graph cannot be the same color. 4) Assign all cells in each cluster that cluster's color. That way you'd probably get nice-looking patches of colors and wouldn't run into the issue @ivirshup mentioned.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:255,deployability,cluster,cluster,255,One idea: . 1) Cluster the graph with leiden. 2) Coarsen the graph (collapse cells in a single cluster into super nodes). 3) Assign each supernode a color -- adjacent supernodes in the coarsened graph cannot be the same color. 4) Assign all cells in each cluster that cluster's color. That way you'd probably get nice-looking patches of colors and wouldn't run into the issue @ivirshup mentioned.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:268,deployability,cluster,cluster,268,One idea: . 1) Cluster the graph with leiden. 2) Coarsen the graph (collapse cells in a single cluster into super nodes). 3) Assign each supernode a color -- adjacent supernodes in the coarsened graph cannot be the same color. 4) Assign all cells in each cluster that cluster's color. That way you'd probably get nice-looking patches of colors and wouldn't run into the issue @ivirshup mentioned.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:326,deployability,patch,patches,326,One idea: . 1) Cluster the graph with leiden. 2) Coarsen the graph (collapse cells in a single cluster into super nodes). 3) Assign each supernode a color -- adjacent supernodes in the coarsened graph cannot be the same color. 4) Assign all cells in each cluster that cluster's color. That way you'd probably get nice-looking patches of colors and wouldn't run into the issue @ivirshup mentioned.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:326,safety,patch,patches,326,One idea: . 1) Cluster the graph with leiden. 2) Coarsen the graph (collapse cells in a single cluster into super nodes). 3) Assign each supernode a color -- adjacent supernodes in the coarsened graph cannot be the same color. 4) Assign all cells in each cluster that cluster's color. That way you'd probably get nice-looking patches of colors and wouldn't run into the issue @ivirshup mentioned.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:326,security,patch,patches,326,One idea: . 1) Cluster the graph with leiden. 2) Coarsen the graph (collapse cells in a single cluster into super nodes). 3) Assign each supernode a color -- adjacent supernodes in the coarsened graph cannot be the same color. 4) Assign all cells in each cluster that cluster's color. That way you'd probably get nice-looking patches of colors and wouldn't run into the issue @ivirshup mentioned.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:137,availability,cluster,clusters,137,"I would be very interested in this functionality as well - even with 20 or so cell types the UMAPs are often unreadable. . Shouldn't the clusters be already predefined (e.g. cell types, etc. used for colouring)? . Also, shouldn't one strive that neighbouring clusters are not only coloured with different colour, but also the colours to be as much apart in the spectrum as possible. E.g. orange and brown are easy to distinguish on legend, but when you have overlapping clusters they can not be distinguished anymore. . Also, I think there is something odd with the current default colour palette choice - the colours are more distinct with more clusters than less clusters, see below:. 24 clusters:. ![image](https://user-images.githubusercontent.com/47607471/93882460-8a37ad80-fce0-11ea-8f4f-39cbe3fa3eb1.png). 33 clusters:. ![image](https://user-images.githubusercontent.com/47607471/93882512-9e7baa80-fce0-11ea-9526-6ce2835f04d3.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:259,availability,cluster,clusters,259,"I would be very interested in this functionality as well - even with 20 or so cell types the UMAPs are often unreadable. . Shouldn't the clusters be already predefined (e.g. cell types, etc. used for colouring)? . Also, shouldn't one strive that neighbouring clusters are not only coloured with different colour, but also the colours to be as much apart in the spectrum as possible. E.g. orange and brown are easy to distinguish on legend, but when you have overlapping clusters they can not be distinguished anymore. . Also, I think there is something odd with the current default colour palette choice - the colours are more distinct with more clusters than less clusters, see below:. 24 clusters:. ![image](https://user-images.githubusercontent.com/47607471/93882460-8a37ad80-fce0-11ea-8f4f-39cbe3fa3eb1.png). 33 clusters:. ![image](https://user-images.githubusercontent.com/47607471/93882512-9e7baa80-fce0-11ea-9526-6ce2835f04d3.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:470,availability,cluster,clusters,470,"I would be very interested in this functionality as well - even with 20 or so cell types the UMAPs are often unreadable. . Shouldn't the clusters be already predefined (e.g. cell types, etc. used for colouring)? . Also, shouldn't one strive that neighbouring clusters are not only coloured with different colour, but also the colours to be as much apart in the spectrum as possible. E.g. orange and brown are easy to distinguish on legend, but when you have overlapping clusters they can not be distinguished anymore. . Also, I think there is something odd with the current default colour palette choice - the colours are more distinct with more clusters than less clusters, see below:. 24 clusters:. ![image](https://user-images.githubusercontent.com/47607471/93882460-8a37ad80-fce0-11ea-8f4f-39cbe3fa3eb1.png). 33 clusters:. ![image](https://user-images.githubusercontent.com/47607471/93882512-9e7baa80-fce0-11ea-9526-6ce2835f04d3.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:646,availability,cluster,clusters,646,"I would be very interested in this functionality as well - even with 20 or so cell types the UMAPs are often unreadable. . Shouldn't the clusters be already predefined (e.g. cell types, etc. used for colouring)? . Also, shouldn't one strive that neighbouring clusters are not only coloured with different colour, but also the colours to be as much apart in the spectrum as possible. E.g. orange and brown are easy to distinguish on legend, but when you have overlapping clusters they can not be distinguished anymore. . Also, I think there is something odd with the current default colour palette choice - the colours are more distinct with more clusters than less clusters, see below:. 24 clusters:. ![image](https://user-images.githubusercontent.com/47607471/93882460-8a37ad80-fce0-11ea-8f4f-39cbe3fa3eb1.png). 33 clusters:. ![image](https://user-images.githubusercontent.com/47607471/93882512-9e7baa80-fce0-11ea-9526-6ce2835f04d3.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:665,availability,cluster,clusters,665,"I would be very interested in this functionality as well - even with 20 or so cell types the UMAPs are often unreadable. . Shouldn't the clusters be already predefined (e.g. cell types, etc. used for colouring)? . Also, shouldn't one strive that neighbouring clusters are not only coloured with different colour, but also the colours to be as much apart in the spectrum as possible. E.g. orange and brown are easy to distinguish on legend, but when you have overlapping clusters they can not be distinguished anymore. . Also, I think there is something odd with the current default colour palette choice - the colours are more distinct with more clusters than less clusters, see below:. 24 clusters:. ![image](https://user-images.githubusercontent.com/47607471/93882460-8a37ad80-fce0-11ea-8f4f-39cbe3fa3eb1.png). 33 clusters:. ![image](https://user-images.githubusercontent.com/47607471/93882512-9e7baa80-fce0-11ea-9526-6ce2835f04d3.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:690,availability,cluster,clusters,690,"I would be very interested in this functionality as well - even with 20 or so cell types the UMAPs are often unreadable. . Shouldn't the clusters be already predefined (e.g. cell types, etc. used for colouring)? . Also, shouldn't one strive that neighbouring clusters are not only coloured with different colour, but also the colours to be as much apart in the spectrum as possible. E.g. orange and brown are easy to distinguish on legend, but when you have overlapping clusters they can not be distinguished anymore. . Also, I think there is something odd with the current default colour palette choice - the colours are more distinct with more clusters than less clusters, see below:. 24 clusters:. ![image](https://user-images.githubusercontent.com/47607471/93882460-8a37ad80-fce0-11ea-8f4f-39cbe3fa3eb1.png). 33 clusters:. ![image](https://user-images.githubusercontent.com/47607471/93882512-9e7baa80-fce0-11ea-9526-6ce2835f04d3.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:816,availability,cluster,clusters,816,"I would be very interested in this functionality as well - even with 20 or so cell types the UMAPs are often unreadable. . Shouldn't the clusters be already predefined (e.g. cell types, etc. used for colouring)? . Also, shouldn't one strive that neighbouring clusters are not only coloured with different colour, but also the colours to be as much apart in the spectrum as possible. E.g. orange and brown are easy to distinguish on legend, but when you have overlapping clusters they can not be distinguished anymore. . Also, I think there is something odd with the current default colour palette choice - the colours are more distinct with more clusters than less clusters, see below:. 24 clusters:. ![image](https://user-images.githubusercontent.com/47607471/93882460-8a37ad80-fce0-11ea-8f4f-39cbe3fa3eb1.png). 33 clusters:. ![image](https://user-images.githubusercontent.com/47607471/93882512-9e7baa80-fce0-11ea-9526-6ce2835f04d3.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:137,deployability,cluster,clusters,137,"I would be very interested in this functionality as well - even with 20 or so cell types the UMAPs are often unreadable. . Shouldn't the clusters be already predefined (e.g. cell types, etc. used for colouring)? . Also, shouldn't one strive that neighbouring clusters are not only coloured with different colour, but also the colours to be as much apart in the spectrum as possible. E.g. orange and brown are easy to distinguish on legend, but when you have overlapping clusters they can not be distinguished anymore. . Also, I think there is something odd with the current default colour palette choice - the colours are more distinct with more clusters than less clusters, see below:. 24 clusters:. ![image](https://user-images.githubusercontent.com/47607471/93882460-8a37ad80-fce0-11ea-8f4f-39cbe3fa3eb1.png). 33 clusters:. ![image](https://user-images.githubusercontent.com/47607471/93882512-9e7baa80-fce0-11ea-9526-6ce2835f04d3.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:259,deployability,cluster,clusters,259,"I would be very interested in this functionality as well - even with 20 or so cell types the UMAPs are often unreadable. . Shouldn't the clusters be already predefined (e.g. cell types, etc. used for colouring)? . Also, shouldn't one strive that neighbouring clusters are not only coloured with different colour, but also the colours to be as much apart in the spectrum as possible. E.g. orange and brown are easy to distinguish on legend, but when you have overlapping clusters they can not be distinguished anymore. . Also, I think there is something odd with the current default colour palette choice - the colours are more distinct with more clusters than less clusters, see below:. 24 clusters:. ![image](https://user-images.githubusercontent.com/47607471/93882460-8a37ad80-fce0-11ea-8f4f-39cbe3fa3eb1.png). 33 clusters:. ![image](https://user-images.githubusercontent.com/47607471/93882512-9e7baa80-fce0-11ea-9526-6ce2835f04d3.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:470,deployability,cluster,clusters,470,"I would be very interested in this functionality as well - even with 20 or so cell types the UMAPs are often unreadable. . Shouldn't the clusters be already predefined (e.g. cell types, etc. used for colouring)? . Also, shouldn't one strive that neighbouring clusters are not only coloured with different colour, but also the colours to be as much apart in the spectrum as possible. E.g. orange and brown are easy to distinguish on legend, but when you have overlapping clusters they can not be distinguished anymore. . Also, I think there is something odd with the current default colour palette choice - the colours are more distinct with more clusters than less clusters, see below:. 24 clusters:. ![image](https://user-images.githubusercontent.com/47607471/93882460-8a37ad80-fce0-11ea-8f4f-39cbe3fa3eb1.png). 33 clusters:. ![image](https://user-images.githubusercontent.com/47607471/93882512-9e7baa80-fce0-11ea-9526-6ce2835f04d3.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:646,deployability,cluster,clusters,646,"I would be very interested in this functionality as well - even with 20 or so cell types the UMAPs are often unreadable. . Shouldn't the clusters be already predefined (e.g. cell types, etc. used for colouring)? . Also, shouldn't one strive that neighbouring clusters are not only coloured with different colour, but also the colours to be as much apart in the spectrum as possible. E.g. orange and brown are easy to distinguish on legend, but when you have overlapping clusters they can not be distinguished anymore. . Also, I think there is something odd with the current default colour palette choice - the colours are more distinct with more clusters than less clusters, see below:. 24 clusters:. ![image](https://user-images.githubusercontent.com/47607471/93882460-8a37ad80-fce0-11ea-8f4f-39cbe3fa3eb1.png). 33 clusters:. ![image](https://user-images.githubusercontent.com/47607471/93882512-9e7baa80-fce0-11ea-9526-6ce2835f04d3.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:665,deployability,cluster,clusters,665,"I would be very interested in this functionality as well - even with 20 or so cell types the UMAPs are often unreadable. . Shouldn't the clusters be already predefined (e.g. cell types, etc. used for colouring)? . Also, shouldn't one strive that neighbouring clusters are not only coloured with different colour, but also the colours to be as much apart in the spectrum as possible. E.g. orange and brown are easy to distinguish on legend, but when you have overlapping clusters they can not be distinguished anymore. . Also, I think there is something odd with the current default colour palette choice - the colours are more distinct with more clusters than less clusters, see below:. 24 clusters:. ![image](https://user-images.githubusercontent.com/47607471/93882460-8a37ad80-fce0-11ea-8f4f-39cbe3fa3eb1.png). 33 clusters:. ![image](https://user-images.githubusercontent.com/47607471/93882512-9e7baa80-fce0-11ea-9526-6ce2835f04d3.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:690,deployability,cluster,clusters,690,"I would be very interested in this functionality as well - even with 20 or so cell types the UMAPs are often unreadable. . Shouldn't the clusters be already predefined (e.g. cell types, etc. used for colouring)? . Also, shouldn't one strive that neighbouring clusters are not only coloured with different colour, but also the colours to be as much apart in the spectrum as possible. E.g. orange and brown are easy to distinguish on legend, but when you have overlapping clusters they can not be distinguished anymore. . Also, I think there is something odd with the current default colour palette choice - the colours are more distinct with more clusters than less clusters, see below:. 24 clusters:. ![image](https://user-images.githubusercontent.com/47607471/93882460-8a37ad80-fce0-11ea-8f4f-39cbe3fa3eb1.png). 33 clusters:. ![image](https://user-images.githubusercontent.com/47607471/93882512-9e7baa80-fce0-11ea-9526-6ce2835f04d3.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:816,deployability,cluster,clusters,816,"I would be very interested in this functionality as well - even with 20 or so cell types the UMAPs are often unreadable. . Shouldn't the clusters be already predefined (e.g. cell types, etc. used for colouring)? . Also, shouldn't one strive that neighbouring clusters are not only coloured with different colour, but also the colours to be as much apart in the spectrum as possible. E.g. orange and brown are easy to distinguish on legend, but when you have overlapping clusters they can not be distinguished anymore. . Also, I think there is something odd with the current default colour palette choice - the colours are more distinct with more clusters than less clusters, see below:. 24 clusters:. ![image](https://user-images.githubusercontent.com/47607471/93882460-8a37ad80-fce0-11ea-8f4f-39cbe3fa3eb1.png). 33 clusters:. ![image](https://user-images.githubusercontent.com/47607471/93882512-9e7baa80-fce0-11ea-9526-6ce2835f04d3.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:566,energy efficiency,current,current,566,"I would be very interested in this functionality as well - even with 20 or so cell types the UMAPs are often unreadable. . Shouldn't the clusters be already predefined (e.g. cell types, etc. used for colouring)? . Also, shouldn't one strive that neighbouring clusters are not only coloured with different colour, but also the colours to be as much apart in the spectrum as possible. E.g. orange and brown are easy to distinguish on legend, but when you have overlapping clusters they can not be distinguished anymore. . Also, I think there is something odd with the current default colour palette choice - the colours are more distinct with more clusters than less clusters, see below:. 24 clusters:. ![image](https://user-images.githubusercontent.com/47607471/93882460-8a37ad80-fce0-11ea-8f4f-39cbe3fa3eb1.png). 33 clusters:. ![image](https://user-images.githubusercontent.com/47607471/93882512-9e7baa80-fce0-11ea-9526-6ce2835f04d3.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:718,usability,user,user-images,718,"I would be very interested in this functionality as well - even with 20 or so cell types the UMAPs are often unreadable. . Shouldn't the clusters be already predefined (e.g. cell types, etc. used for colouring)? . Also, shouldn't one strive that neighbouring clusters are not only coloured with different colour, but also the colours to be as much apart in the spectrum as possible. E.g. orange and brown are easy to distinguish on legend, but when you have overlapping clusters they can not be distinguished anymore. . Also, I think there is something odd with the current default colour palette choice - the colours are more distinct with more clusters than less clusters, see below:. 24 clusters:. ![image](https://user-images.githubusercontent.com/47607471/93882460-8a37ad80-fce0-11ea-8f4f-39cbe3fa3eb1.png). 33 clusters:. ![image](https://user-images.githubusercontent.com/47607471/93882512-9e7baa80-fce0-11ea-9526-6ce2835f04d3.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:844,usability,user,user-images,844,"I would be very interested in this functionality as well - even with 20 or so cell types the UMAPs are often unreadable. . Shouldn't the clusters be already predefined (e.g. cell types, etc. used for colouring)? . Also, shouldn't one strive that neighbouring clusters are not only coloured with different colour, but also the colours to be as much apart in the spectrum as possible. E.g. orange and brown are easy to distinguish on legend, but when you have overlapping clusters they can not be distinguished anymore. . Also, I think there is something odd with the current default colour palette choice - the colours are more distinct with more clusters than less clusters, see below:. 24 clusters:. ![image](https://user-images.githubusercontent.com/47607471/93882460-8a37ad80-fce0-11ea-8f4f-39cbe3fa3eb1.png). 33 clusters:. ![image](https://user-images.githubusercontent.com/47607471/93882512-9e7baa80-fce0-11ea-9526-6ce2835f04d3.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:91,availability,cluster,cluster,91,> On more issue to consider: entities on maps tend to be contiguous. The set of cells in a cluster do not have to be adjacent. How can it be clear two non-adjacent cells are from the same cluster if colors can be repeated? It won't be as a big problem for two different clusters to have the same colors because Scanpy already uses very similar or identical colors when cell type number is high. The primary goal for using different colors is to separate clusters that sit next to each other on a dimension-reduced 2D map. Hopefully the world map color problem can be solved by the Scanpy team.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:188,availability,cluster,cluster,188,> On more issue to consider: entities on maps tend to be contiguous. The set of cells in a cluster do not have to be adjacent. How can it be clear two non-adjacent cells are from the same cluster if colors can be repeated? It won't be as a big problem for two different clusters to have the same colors because Scanpy already uses very similar or identical colors when cell type number is high. The primary goal for using different colors is to separate clusters that sit next to each other on a dimension-reduced 2D map. Hopefully the world map color problem can be solved by the Scanpy team.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:270,availability,cluster,clusters,270,> On more issue to consider: entities on maps tend to be contiguous. The set of cells in a cluster do not have to be adjacent. How can it be clear two non-adjacent cells are from the same cluster if colors can be repeated? It won't be as a big problem for two different clusters to have the same colors because Scanpy already uses very similar or identical colors when cell type number is high. The primary goal for using different colors is to separate clusters that sit next to each other on a dimension-reduced 2D map. Hopefully the world map color problem can be solved by the Scanpy team.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:454,availability,cluster,clusters,454,> On more issue to consider: entities on maps tend to be contiguous. The set of cells in a cluster do not have to be adjacent. How can it be clear two non-adjacent cells are from the same cluster if colors can be repeated? It won't be as a big problem for two different clusters to have the same colors because Scanpy already uses very similar or identical colors when cell type number is high. The primary goal for using different colors is to separate clusters that sit next to each other on a dimension-reduced 2D map. Hopefully the world map color problem can be solved by the Scanpy team.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:91,deployability,cluster,cluster,91,> On more issue to consider: entities on maps tend to be contiguous. The set of cells in a cluster do not have to be adjacent. How can it be clear two non-adjacent cells are from the same cluster if colors can be repeated? It won't be as a big problem for two different clusters to have the same colors because Scanpy already uses very similar or identical colors when cell type number is high. The primary goal for using different colors is to separate clusters that sit next to each other on a dimension-reduced 2D map. Hopefully the world map color problem can be solved by the Scanpy team.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:188,deployability,cluster,cluster,188,> On more issue to consider: entities on maps tend to be contiguous. The set of cells in a cluster do not have to be adjacent. How can it be clear two non-adjacent cells are from the same cluster if colors can be repeated? It won't be as a big problem for two different clusters to have the same colors because Scanpy already uses very similar or identical colors when cell type number is high. The primary goal for using different colors is to separate clusters that sit next to each other on a dimension-reduced 2D map. Hopefully the world map color problem can be solved by the Scanpy team.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:270,deployability,cluster,clusters,270,> On more issue to consider: entities on maps tend to be contiguous. The set of cells in a cluster do not have to be adjacent. How can it be clear two non-adjacent cells are from the same cluster if colors can be repeated? It won't be as a big problem for two different clusters to have the same colors because Scanpy already uses very similar or identical colors when cell type number is high. The primary goal for using different colors is to separate clusters that sit next to each other on a dimension-reduced 2D map. Hopefully the world map color problem can be solved by the Scanpy team.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:454,deployability,cluster,clusters,454,> On more issue to consider: entities on maps tend to be contiguous. The set of cells in a cluster do not have to be adjacent. How can it be clear two non-adjacent cells are from the same cluster if colors can be repeated? It won't be as a big problem for two different clusters to have the same colors because Scanpy already uses very similar or identical colors when cell type number is high. The primary goal for using different colors is to separate clusters that sit next to each other on a dimension-reduced 2D map. Hopefully the world map color problem can be solved by the Scanpy team.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:506,energy efficiency,reduc,reduced,506,> On more issue to consider: entities on maps tend to be contiguous. The set of cells in a cluster do not have to be adjacent. How can it be clear two non-adjacent cells are from the same cluster if colors can be repeated? It won't be as a big problem for two different clusters to have the same colors because Scanpy already uses very similar or identical colors when cell type number is high. The primary goal for using different colors is to separate clusters that sit next to each other on a dimension-reduced 2D map. Hopefully the world map color problem can be solved by the Scanpy team.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:347,security,ident,identical,347,> On more issue to consider: entities on maps tend to be contiguous. The set of cells in a cluster do not have to be adjacent. How can it be clear two non-adjacent cells are from the same cluster if colors can be repeated? It won't be as a big problem for two different clusters to have the same colors because Scanpy already uses very similar or identical colors when cell type number is high. The primary goal for using different colors is to separate clusters that sit next to each other on a dimension-reduced 2D map. Hopefully the world map color problem can be solved by the Scanpy team.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:588,security,team,team,588,> On more issue to consider: entities on maps tend to be contiguous. The set of cells in a cluster do not have to be adjacent. How can it be clear two non-adjacent cells are from the same cluster if colors can be repeated? It won't be as a big problem for two different clusters to have the same colors because Scanpy already uses very similar or identical colors when cell type number is high. The primary goal for using different colors is to separate clusters that sit next to each other on a dimension-reduced 2D map. Hopefully the world map color problem can be solved by the Scanpy team.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:141,usability,clear,clear,141,> On more issue to consider: entities on maps tend to be contiguous. The set of cells in a cluster do not have to be adjacent. How can it be clear two non-adjacent cells are from the same cluster if colors can be repeated? It won't be as a big problem for two different clusters to have the same colors because Scanpy already uses very similar or identical colors when cell type number is high. The primary goal for using different colors is to separate clusters that sit next to each other on a dimension-reduced 2D map. Hopefully the world map color problem can be solved by the Scanpy team.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:22,availability,Cluster,Cluster,22,> One idea:. > . > 1. Cluster the graph with leiden. > 2. Coarsen the graph (collapse cells in a single cluster into super nodes). > 3. Assign each supernode a color -- adjacent supernodes in the coarsened graph cannot be the same color. > 4. Assign all cells in each cluster that cluster's color. > . > That way you'd probably get nice-looking patches of colors and wouldn't run into the issue @ivirshup mentioned. that sounds reasonable,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:104,availability,cluster,cluster,104,> One idea:. > . > 1. Cluster the graph with leiden. > 2. Coarsen the graph (collapse cells in a single cluster into super nodes). > 3. Assign each supernode a color -- adjacent supernodes in the coarsened graph cannot be the same color. > 4. Assign all cells in each cluster that cluster's color. > . > That way you'd probably get nice-looking patches of colors and wouldn't run into the issue @ivirshup mentioned. that sounds reasonable,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:268,availability,cluster,cluster,268,> One idea:. > . > 1. Cluster the graph with leiden. > 2. Coarsen the graph (collapse cells in a single cluster into super nodes). > 3. Assign each supernode a color -- adjacent supernodes in the coarsened graph cannot be the same color. > 4. Assign all cells in each cluster that cluster's color. > . > That way you'd probably get nice-looking patches of colors and wouldn't run into the issue @ivirshup mentioned. that sounds reasonable,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:281,availability,cluster,cluster,281,> One idea:. > . > 1. Cluster the graph with leiden. > 2. Coarsen the graph (collapse cells in a single cluster into super nodes). > 3. Assign each supernode a color -- adjacent supernodes in the coarsened graph cannot be the same color. > 4. Assign all cells in each cluster that cluster's color. > . > That way you'd probably get nice-looking patches of colors and wouldn't run into the issue @ivirshup mentioned. that sounds reasonable,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:22,deployability,Cluster,Cluster,22,> One idea:. > . > 1. Cluster the graph with leiden. > 2. Coarsen the graph (collapse cells in a single cluster into super nodes). > 3. Assign each supernode a color -- adjacent supernodes in the coarsened graph cannot be the same color. > 4. Assign all cells in each cluster that cluster's color. > . > That way you'd probably get nice-looking patches of colors and wouldn't run into the issue @ivirshup mentioned. that sounds reasonable,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:104,deployability,cluster,cluster,104,> One idea:. > . > 1. Cluster the graph with leiden. > 2. Coarsen the graph (collapse cells in a single cluster into super nodes). > 3. Assign each supernode a color -- adjacent supernodes in the coarsened graph cannot be the same color. > 4. Assign all cells in each cluster that cluster's color. > . > That way you'd probably get nice-looking patches of colors and wouldn't run into the issue @ivirshup mentioned. that sounds reasonable,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:268,deployability,cluster,cluster,268,> One idea:. > . > 1. Cluster the graph with leiden. > 2. Coarsen the graph (collapse cells in a single cluster into super nodes). > 3. Assign each supernode a color -- adjacent supernodes in the coarsened graph cannot be the same color. > 4. Assign all cells in each cluster that cluster's color. > . > That way you'd probably get nice-looking patches of colors and wouldn't run into the issue @ivirshup mentioned. that sounds reasonable,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:281,deployability,cluster,cluster,281,> One idea:. > . > 1. Cluster the graph with leiden. > 2. Coarsen the graph (collapse cells in a single cluster into super nodes). > 3. Assign each supernode a color -- adjacent supernodes in the coarsened graph cannot be the same color. > 4. Assign all cells in each cluster that cluster's color. > . > That way you'd probably get nice-looking patches of colors and wouldn't run into the issue @ivirshup mentioned. that sounds reasonable,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:345,deployability,patch,patches,345,> One idea:. > . > 1. Cluster the graph with leiden. > 2. Coarsen the graph (collapse cells in a single cluster into super nodes). > 3. Assign each supernode a color -- adjacent supernodes in the coarsened graph cannot be the same color. > 4. Assign all cells in each cluster that cluster's color. > . > That way you'd probably get nice-looking patches of colors and wouldn't run into the issue @ivirshup mentioned. that sounds reasonable,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:345,safety,patch,patches,345,> One idea:. > . > 1. Cluster the graph with leiden. > 2. Coarsen the graph (collapse cells in a single cluster into super nodes). > 3. Assign each supernode a color -- adjacent supernodes in the coarsened graph cannot be the same color. > 4. Assign all cells in each cluster that cluster's color. > . > That way you'd probably get nice-looking patches of colors and wouldn't run into the issue @ivirshup mentioned. that sounds reasonable,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:345,security,patch,patches,345,> One idea:. > . > 1. Cluster the graph with leiden. > 2. Coarsen the graph (collapse cells in a single cluster into super nodes). > 3. Assign each supernode a color -- adjacent supernodes in the coarsened graph cannot be the same color. > 4. Assign all cells in each cluster that cluster's color. > . > That way you'd probably get nice-looking patches of colors and wouldn't run into the issue @ivirshup mentioned. that sounds reasonable,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:11,deployability,updat,updated,11,"Just as an updated thought on this, I don't think using the connectivity graph is the most straightforward way to approach this. We don't really care about closeness of points on the manifold, we care about closeness of the points on the plot. I think you'd want to constrain color assignment by points on the plot. This has the side benefit of being more widely applicable, since it doesn't require the plot to be connected to some graph representation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:384,reliability,doe,doesn,384,"Just as an updated thought on this, I don't think using the connectivity graph is the most straightforward way to approach this. We don't really care about closeness of points on the manifold, we care about closeness of the points on the plot. I think you'd want to constrain color assignment by points on the plot. This has the side benefit of being more widely applicable, since it doesn't require the plot to be connected to some graph representation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:11,safety,updat,updated,11,"Just as an updated thought on this, I don't think using the connectivity graph is the most straightforward way to approach this. We don't really care about closeness of points on the manifold, we care about closeness of the points on the plot. I think you'd want to constrain color assignment by points on the plot. This has the side benefit of being more widely applicable, since it doesn't require the plot to be connected to some graph representation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:11,security,updat,updated,11,"Just as an updated thought on this, I don't think using the connectivity graph is the most straightforward way to approach this. We don't really care about closeness of points on the manifold, we care about closeness of the points on the plot. I think you'd want to constrain color assignment by points on the plot. This has the side benefit of being more widely applicable, since it doesn't require the plot to be connected to some graph representation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:156,usability,close,closeness,156,"Just as an updated thought on this, I don't think using the connectivity graph is the most straightforward way to approach this. We don't really care about closeness of points on the manifold, we care about closeness of the points on the plot. I think you'd want to constrain color assignment by points on the plot. This has the side benefit of being more widely applicable, since it doesn't require the plot to be connected to some graph representation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:207,usability,close,closeness,207,"Just as an updated thought on this, I don't think using the connectivity graph is the most straightforward way to approach this. We don't really care about closeness of points on the manifold, we care about closeness of the points on the plot. I think you'd want to constrain color assignment by points on the plot. This has the side benefit of being more widely applicable, since it doesn't require the plot to be connected to some graph representation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:427,availability,cluster,cluster,427,"Heres one potential solution. Not sure if it works as intended or not. The `graph_coloring` function returns a numpy array of normalized RGB values for each cell in `adata`. Sorry for the ugly list comprehensions at the end  I can clean it up if this is a solution that is worth pursuing. 1) generate a nearest neighbor graph from the UMAP coordinates using the euclidean distance metric. 2) coarsen the graph using provided cluster assignments. 3) do the graph coloring, resulting in N distinct colors. 4) generate N visually distinct colors. 5) map the colors to each cell. Sources:. [Choosing N visually distinct colors](https://stackoverflow.com/a/13781114). [Graph coloring](https://codereview.stackexchange.com/a/203328). ```python. # Choosing N distinct colors. from typing import Iterable, Tuple. import colorsys. import itertools. from fractions import Fraction. def zenos_dichotomy() -> Iterable[Fraction]:. """""". http://en.wikipedia.org/wiki/1/2_%2B_1/4_%2B_1/8_%2B_1/16_%2B_%C2%B7_%C2%B7_%C2%B7. """""". for k in itertools.count():. yield Fraction(1,2**k). def fracs() -> Iterable[Fraction]:. """""". [Fraction(0, 1), Fraction(1, 2), Fraction(1, 4), Fraction(3, 4), Fraction(1, 8), Fraction(3, 8), Fraction(5, 8), Fraction(7, 8), Fraction(1, 16), Fraction(3, 16), ...]. [0.0, 0.5, 0.25, 0.75, 0.125, 0.375, 0.625, 0.875, 0.0625, 0.1875, ...]. """""". yield Fraction(0). for k in zenos_dichotomy():. i = k.denominator # [1,2,4,8,16,...]. for j in range(1,i,2):. yield Fraction(j,i). # can be used for the v in hsv to map linear values 0..1 to something that looks equidistant. # bias = lambda x: (math.sqrt(x/3)/Fraction(2,3)+Fraction(1,3))/Fraction(6,5). HSVTuple = Tuple[Fraction, Fraction, Fraction]. RGBTuple = Tuple[float, float, float]. def hue_to_tones(h: Fraction) -> Iterable[HSVTuple]:. for s in [Fraction(6,10)]: # optionally use range. for v in [Fraction(8,10),Fraction(5,10)]: # could use range too. yield (h, s, v) # use bias for v here if you use range. def hsv_to_rgb(x: HSVTuple) ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:427,deployability,cluster,cluster,427,"Heres one potential solution. Not sure if it works as intended or not. The `graph_coloring` function returns a numpy array of normalized RGB values for each cell in `adata`. Sorry for the ugly list comprehensions at the end  I can clean it up if this is a solution that is worth pursuing. 1) generate a nearest neighbor graph from the UMAP coordinates using the euclidean distance metric. 2) coarsen the graph using provided cluster assignments. 3) do the graph coloring, resulting in N distinct colors. 4) generate N visually distinct colors. 5) map the colors to each cell. Sources:. [Choosing N visually distinct colors](https://stackoverflow.com/a/13781114). [Graph coloring](https://codereview.stackexchange.com/a/203328). ```python. # Choosing N distinct colors. from typing import Iterable, Tuple. import colorsys. import itertools. from fractions import Fraction. def zenos_dichotomy() -> Iterable[Fraction]:. """""". http://en.wikipedia.org/wiki/1/2_%2B_1/4_%2B_1/8_%2B_1/16_%2B_%C2%B7_%C2%B7_%C2%B7. """""". for k in itertools.count():. yield Fraction(1,2**k). def fracs() -> Iterable[Fraction]:. """""". [Fraction(0, 1), Fraction(1, 2), Fraction(1, 4), Fraction(3, 4), Fraction(1, 8), Fraction(3, 8), Fraction(5, 8), Fraction(7, 8), Fraction(1, 16), Fraction(3, 16), ...]. [0.0, 0.5, 0.25, 0.75, 0.125, 0.375, 0.625, 0.875, 0.0625, 0.1875, ...]. """""". yield Fraction(0). for k in zenos_dichotomy():. i = k.denominator # [1,2,4,8,16,...]. for j in range(1,i,2):. yield Fraction(j,i). # can be used for the v in hsv to map linear values 0..1 to something that looks equidistant. # bias = lambda x: (math.sqrt(x/3)/Fraction(2,3)+Fraction(1,3))/Fraction(6,5). HSVTuple = Tuple[Fraction, Fraction, Fraction]. RGBTuple = Tuple[float, float, float]. def hue_to_tones(h: Fraction) -> Iterable[HSVTuple]:. for s in [Fraction(6,10)]: # optionally use range. for v in [Fraction(8,10),Fraction(5,10)]: # could use range too. yield (h, s, v) # use bias for v here if you use range. def hsv_to_rgb(x: HSVTuple) ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:634,deployability,stack,stackoverflow,634,"Heres one potential solution. Not sure if it works as intended or not. The `graph_coloring` function returns a numpy array of normalized RGB values for each cell in `adata`. Sorry for the ugly list comprehensions at the end  I can clean it up if this is a solution that is worth pursuing. 1) generate a nearest neighbor graph from the UMAP coordinates using the euclidean distance metric. 2) coarsen the graph using provided cluster assignments. 3) do the graph coloring, resulting in N distinct colors. 4) generate N visually distinct colors. 5) map the colors to each cell. Sources:. [Choosing N visually distinct colors](https://stackoverflow.com/a/13781114). [Graph coloring](https://codereview.stackexchange.com/a/203328). ```python. # Choosing N distinct colors. from typing import Iterable, Tuple. import colorsys. import itertools. from fractions import Fraction. def zenos_dichotomy() -> Iterable[Fraction]:. """""". http://en.wikipedia.org/wiki/1/2_%2B_1/4_%2B_1/8_%2B_1/16_%2B_%C2%B7_%C2%B7_%C2%B7. """""". for k in itertools.count():. yield Fraction(1,2**k). def fracs() -> Iterable[Fraction]:. """""". [Fraction(0, 1), Fraction(1, 2), Fraction(1, 4), Fraction(3, 4), Fraction(1, 8), Fraction(3, 8), Fraction(5, 8), Fraction(7, 8), Fraction(1, 16), Fraction(3, 16), ...]. [0.0, 0.5, 0.25, 0.75, 0.125, 0.375, 0.625, 0.875, 0.0625, 0.1875, ...]. """""". yield Fraction(0). for k in zenos_dichotomy():. i = k.denominator # [1,2,4,8,16,...]. for j in range(1,i,2):. yield Fraction(j,i). # can be used for the v in hsv to map linear values 0..1 to something that looks equidistant. # bias = lambda x: (math.sqrt(x/3)/Fraction(2,3)+Fraction(1,3))/Fraction(6,5). HSVTuple = Tuple[Fraction, Fraction, Fraction]. RGBTuple = Tuple[float, float, float]. def hue_to_tones(h: Fraction) -> Iterable[HSVTuple]:. for s in [Fraction(6,10)]: # optionally use range. for v in [Fraction(8,10),Fraction(5,10)]: # could use range too. yield (h, s, v) # use bias for v here if you use range. def hsv_to_rgb(x: HSVTuple) ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:701,deployability,stack,stackexchange,701,"Heres one potential solution. Not sure if it works as intended or not. The `graph_coloring` function returns a numpy array of normalized RGB values for each cell in `adata`. Sorry for the ugly list comprehensions at the end  I can clean it up if this is a solution that is worth pursuing. 1) generate a nearest neighbor graph from the UMAP coordinates using the euclidean distance metric. 2) coarsen the graph using provided cluster assignments. 3) do the graph coloring, resulting in N distinct colors. 4) generate N visually distinct colors. 5) map the colors to each cell. Sources:. [Choosing N visually distinct colors](https://stackoverflow.com/a/13781114). [Graph coloring](https://codereview.stackexchange.com/a/203328). ```python. # Choosing N distinct colors. from typing import Iterable, Tuple. import colorsys. import itertools. from fractions import Fraction. def zenos_dichotomy() -> Iterable[Fraction]:. """""". http://en.wikipedia.org/wiki/1/2_%2B_1/4_%2B_1/8_%2B_1/16_%2B_%C2%B7_%C2%B7_%C2%B7. """""". for k in itertools.count():. yield Fraction(1,2**k). def fracs() -> Iterable[Fraction]:. """""". [Fraction(0, 1), Fraction(1, 2), Fraction(1, 4), Fraction(3, 4), Fraction(1, 8), Fraction(3, 8), Fraction(5, 8), Fraction(7, 8), Fraction(1, 16), Fraction(3, 16), ...]. [0.0, 0.5, 0.25, 0.75, 0.125, 0.375, 0.625, 0.875, 0.0625, 0.1875, ...]. """""". yield Fraction(0). for k in zenos_dichotomy():. i = k.denominator # [1,2,4,8,16,...]. for j in range(1,i,2):. yield Fraction(j,i). # can be used for the v in hsv to map linear values 0..1 to something that looks equidistant. # bias = lambda x: (math.sqrt(x/3)/Fraction(2,3)+Fraction(1,3))/Fraction(6,5). HSVTuple = Tuple[Fraction, Fraction, Fraction]. RGBTuple = Tuple[float, float, float]. def hue_to_tones(h: Fraction) -> Iterable[HSVTuple]:. for s in [Fraction(6,10)]: # optionally use range. for v in [Fraction(8,10),Fraction(5,10)]: # could use range too. yield (h, s, v) # use bias for v here if you use range. def hsv_to_rgb(x: HSVTuple) ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:342,interoperability,coordinat,coordinates,342,"Heres one potential solution. Not sure if it works as intended or not. The `graph_coloring` function returns a numpy array of normalized RGB values for each cell in `adata`. Sorry for the ugly list comprehensions at the end  I can clean it up if this is a solution that is worth pursuing. 1) generate a nearest neighbor graph from the UMAP coordinates using the euclidean distance metric. 2) coarsen the graph using provided cluster assignments. 3) do the graph coloring, resulting in N distinct colors. 4) generate N visually distinct colors. 5) map the colors to each cell. Sources:. [Choosing N visually distinct colors](https://stackoverflow.com/a/13781114). [Graph coloring](https://codereview.stackexchange.com/a/203328). ```python. # Choosing N distinct colors. from typing import Iterable, Tuple. import colorsys. import itertools. from fractions import Fraction. def zenos_dichotomy() -> Iterable[Fraction]:. """""". http://en.wikipedia.org/wiki/1/2_%2B_1/4_%2B_1/8_%2B_1/16_%2B_%C2%B7_%C2%B7_%C2%B7. """""". for k in itertools.count():. yield Fraction(1,2**k). def fracs() -> Iterable[Fraction]:. """""". [Fraction(0, 1), Fraction(1, 2), Fraction(1, 4), Fraction(3, 4), Fraction(1, 8), Fraction(3, 8), Fraction(5, 8), Fraction(7, 8), Fraction(1, 16), Fraction(3, 16), ...]. [0.0, 0.5, 0.25, 0.75, 0.125, 0.375, 0.625, 0.875, 0.0625, 0.1875, ...]. """""". yield Fraction(0). for k in zenos_dichotomy():. i = k.denominator # [1,2,4,8,16,...]. for j in range(1,i,2):. yield Fraction(j,i). # can be used for the v in hsv to map linear values 0..1 to something that looks equidistant. # bias = lambda x: (math.sqrt(x/3)/Fraction(2,3)+Fraction(1,3))/Fraction(6,5). HSVTuple = Tuple[Fraction, Fraction, Fraction]. RGBTuple = Tuple[float, float, float]. def hue_to_tones(h: Fraction) -> Iterable[HSVTuple]:. for s in [Fraction(6,10)]: # optionally use range. for v in [Fraction(8,10),Fraction(5,10)]: # could use range too. yield (h, s, v) # use bias for v here if you use range. def hsv_to_rgb(x: HSVTuple) ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:2348,interoperability,format,format,2348,""""". yield Fraction(0). for k in zenos_dichotomy():. i = k.denominator # [1,2,4,8,16,...]. for j in range(1,i,2):. yield Fraction(j,i). # can be used for the v in hsv to map linear values 0..1 to something that looks equidistant. # bias = lambda x: (math.sqrt(x/3)/Fraction(2,3)+Fraction(1,3))/Fraction(6,5). HSVTuple = Tuple[Fraction, Fraction, Fraction]. RGBTuple = Tuple[float, float, float]. def hue_to_tones(h: Fraction) -> Iterable[HSVTuple]:. for s in [Fraction(6,10)]: # optionally use range. for v in [Fraction(8,10),Fraction(5,10)]: # could use range too. yield (h, s, v) # use bias for v here if you use range. def hsv_to_rgb(x: HSVTuple) -> RGBTuple:. return colorsys.hsv_to_rgb(*map(float, x)). flatten = itertools.chain.from_iterable. def hsvs() -> Iterable[HSVTuple]:. return flatten(map(hue_to_tones, fracs())). def rgbs() -> Iterable[RGBTuple]:. return map(hsv_to_rgb, hsvs()). def rgb_to_css(x: RGBTuple) -> str:. uint8tuple = map(lambda y: int(y*255), x). return ""rgb({},{},{})"".format(*uint8tuple). def css_colors() -> Iterable[str]:. return map(rgb_to_css, rgbs()). # Graph coloring. def color_nodes(nnm):. color_map = {}. graph={}. for i in range(nnm.shape[0]):. graph[i] = list(nnm[i].nonzero()[0]). . # Consider nodes in descending degree . for node in sorted(graph, key=lambda x: len(graph[x]), reverse=True):. neighbor_colors = set(color_map.get(neigh) for neigh in graph[node]). color_map[node] = next( . color for color in range(len(graph)) if color not in neighbor_colors. ). return color_map. def graph_coloring(adata, cluster_key):. import scanpy as sc. import numpy as np. import pandas as pd. thr = 0.0. sc.pp.neighbors(adata,metric='euclidean',use_rep='X_umap',key_added='umap_knn'). nnm = adata.obsp['umap_knn_connectivities']. cl = np.array(list(adata.obs[cluster_key])). clu,iv = np.unique(cl,return_inverse=True). A = pd.Series(data = np.arange(clu.size),index = clu). x,y = nnm.nonzero(). pairs,counts = np.unique(np.vstack((cl[x],cl[y])).T,return_counts=True,ax",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:520,usability,visual,visually,520,"Heres one potential solution. Not sure if it works as intended or not. The `graph_coloring` function returns a numpy array of normalized RGB values for each cell in `adata`. Sorry for the ugly list comprehensions at the end  I can clean it up if this is a solution that is worth pursuing. 1) generate a nearest neighbor graph from the UMAP coordinates using the euclidean distance metric. 2) coarsen the graph using provided cluster assignments. 3) do the graph coloring, resulting in N distinct colors. 4) generate N visually distinct colors. 5) map the colors to each cell. Sources:. [Choosing N visually distinct colors](https://stackoverflow.com/a/13781114). [Graph coloring](https://codereview.stackexchange.com/a/203328). ```python. # Choosing N distinct colors. from typing import Iterable, Tuple. import colorsys. import itertools. from fractions import Fraction. def zenos_dichotomy() -> Iterable[Fraction]:. """""". http://en.wikipedia.org/wiki/1/2_%2B_1/4_%2B_1/8_%2B_1/16_%2B_%C2%B7_%C2%B7_%C2%B7. """""". for k in itertools.count():. yield Fraction(1,2**k). def fracs() -> Iterable[Fraction]:. """""". [Fraction(0, 1), Fraction(1, 2), Fraction(1, 4), Fraction(3, 4), Fraction(1, 8), Fraction(3, 8), Fraction(5, 8), Fraction(7, 8), Fraction(1, 16), Fraction(3, 16), ...]. [0.0, 0.5, 0.25, 0.75, 0.125, 0.375, 0.625, 0.875, 0.0625, 0.1875, ...]. """""". yield Fraction(0). for k in zenos_dichotomy():. i = k.denominator # [1,2,4,8,16,...]. for j in range(1,i,2):. yield Fraction(j,i). # can be used for the v in hsv to map linear values 0..1 to something that looks equidistant. # bias = lambda x: (math.sqrt(x/3)/Fraction(2,3)+Fraction(1,3))/Fraction(6,5). HSVTuple = Tuple[Fraction, Fraction, Fraction]. RGBTuple = Tuple[float, float, float]. def hue_to_tones(h: Fraction) -> Iterable[HSVTuple]:. for s in [Fraction(6,10)]: # optionally use range. for v in [Fraction(8,10),Fraction(5,10)]: # could use range too. yield (h, s, v) # use bias for v here if you use range. def hsv_to_rgb(x: HSVTuple) ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:600,usability,visual,visually,600,"Heres one potential solution. Not sure if it works as intended or not. The `graph_coloring` function returns a numpy array of normalized RGB values for each cell in `adata`. Sorry for the ugly list comprehensions at the end  I can clean it up if this is a solution that is worth pursuing. 1) generate a nearest neighbor graph from the UMAP coordinates using the euclidean distance metric. 2) coarsen the graph using provided cluster assignments. 3) do the graph coloring, resulting in N distinct colors. 4) generate N visually distinct colors. 5) map the colors to each cell. Sources:. [Choosing N visually distinct colors](https://stackoverflow.com/a/13781114). [Graph coloring](https://codereview.stackexchange.com/a/203328). ```python. # Choosing N distinct colors. from typing import Iterable, Tuple. import colorsys. import itertools. from fractions import Fraction. def zenos_dichotomy() -> Iterable[Fraction]:. """""". http://en.wikipedia.org/wiki/1/2_%2B_1/4_%2B_1/8_%2B_1/16_%2B_%C2%B7_%C2%B7_%C2%B7. """""". for k in itertools.count():. yield Fraction(1,2**k). def fracs() -> Iterable[Fraction]:. """""". [Fraction(0, 1), Fraction(1, 2), Fraction(1, 4), Fraction(3, 4), Fraction(1, 8), Fraction(3, 8), Fraction(5, 8), Fraction(7, 8), Fraction(1, 16), Fraction(3, 16), ...]. [0.0, 0.5, 0.25, 0.75, 0.125, 0.375, 0.625, 0.875, 0.0625, 0.1875, ...]. """""". yield Fraction(0). for k in zenos_dichotomy():. i = k.denominator # [1,2,4,8,16,...]. for j in range(1,i,2):. yield Fraction(j,i). # can be used for the v in hsv to map linear values 0..1 to something that looks equidistant. # bias = lambda x: (math.sqrt(x/3)/Fraction(2,3)+Fraction(1,3))/Fraction(6,5). HSVTuple = Tuple[Fraction, Fraction, Fraction]. RGBTuple = Tuple[float, float, float]. def hue_to_tones(h: Fraction) -> Iterable[HSVTuple]:. for s in [Fraction(6,10)]: # optionally use range. for v in [Fraction(8,10),Fraction(5,10)]: # could use range too. yield (h, s, v) # use bias for v here if you use range. def hsv_to_rgb(x: HSVTuple) ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:62,testability,simpl,simplified,62,wow. I was thinking...maybe the PAGA graph can be used as the simplified graph for color assignment. But will try your solution here!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:62,usability,simpl,simplified,62,wow. I was thinking...maybe the PAGA graph can be used as the simplified graph for color assignment. But will try your solution here!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:545,availability,cluster,cluster,545,"@atarashansky, that's great! I've built on this a bit, with some points about graph construction. * The problems isn't really nearest K neighbors, it's radius neighbors. * Embedding position is relevant, but I think position in pixel space is more important in this case. You could imagine one dimension of your embedding plot having a range of (0, 100) while the other has (0, 1), but displayed on a square plot. Euclidean distance for neighboring points would not be particularly effective here. To that end, here's some code for building the cluster connectivity graph from pixel space:. <details>. <summary> Code: </summary>. ```python. # imports. import datashader as ds. from datashader import transfer_functions as tf. import matplotlib.pyplot as plt. from natsort import natsorted. import scanpy as sc. import numpy as np. import pandas as pd. import xarray as xr. from sklearn.neighbors import RadiusNeighborsTransformer. # example data. # 1.3 million cells, 38 clusters by louvain. adata = sc.read(""/Users/isaac/data/10x_mouse_13MM_processed.h5ad"", backed=""r""). # Previous colors. louvain_colors_old = dict(. zip(. adata.obs[""louvain""].cat.categories, . adata.uns[""louvain_colors""]. ). ). del adata.uns[""louvain_colors""]. louvain_colors_current = sc.pl._tools.scatterplots._get_palette(adata, values_key=""louvain""). df = sc.get.obs_df(. adata,. [""louvain""],. obsm_keys=[(""X_umap"", 0), (""X_umap"", 1)]. ). # pixel x pixel x n_clusters array (500, 500, 38). # Each value is the count of cells in the cluster at this pixel. pts = (. ds.Canvas(500, 500). .points(df, ""X_umap-0"", ""X_umap-1"", agg=ds.count_cat(""louvain"")). ). def pts_to_coords(pts: xr.DataArray) -> pd.DataFrame:. """""". Turning pixel data into ""long"" sparse format. . One entry for each cluster, for each pixel it would occur in. """""". coords = pd.DataFrame(np.argwhere(np.asarray(pts)), columns=[""x"", ""y"", ""cat""]). coords[""cat""] = pd.Categorical.from_codes(coords[""cat""], categories=list(pts.coords.values())[2]). return coords. de",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:971,availability,cluster,clusters,971,"@atarashansky, that's great! I've built on this a bit, with some points about graph construction. * The problems isn't really nearest K neighbors, it's radius neighbors. * Embedding position is relevant, but I think position in pixel space is more important in this case. You could imagine one dimension of your embedding plot having a range of (0, 100) while the other has (0, 1), but displayed on a square plot. Euclidean distance for neighboring points would not be particularly effective here. To that end, here's some code for building the cluster connectivity graph from pixel space:. <details>. <summary> Code: </summary>. ```python. # imports. import datashader as ds. from datashader import transfer_functions as tf. import matplotlib.pyplot as plt. from natsort import natsorted. import scanpy as sc. import numpy as np. import pandas as pd. import xarray as xr. from sklearn.neighbors import RadiusNeighborsTransformer. # example data. # 1.3 million cells, 38 clusters by louvain. adata = sc.read(""/Users/isaac/data/10x_mouse_13MM_processed.h5ad"", backed=""r""). # Previous colors. louvain_colors_old = dict(. zip(. adata.obs[""louvain""].cat.categories, . adata.uns[""louvain_colors""]. ). ). del adata.uns[""louvain_colors""]. louvain_colors_current = sc.pl._tools.scatterplots._get_palette(adata, values_key=""louvain""). df = sc.get.obs_df(. adata,. [""louvain""],. obsm_keys=[(""X_umap"", 0), (""X_umap"", 1)]. ). # pixel x pixel x n_clusters array (500, 500, 38). # Each value is the count of cells in the cluster at this pixel. pts = (. ds.Canvas(500, 500). .points(df, ""X_umap-0"", ""X_umap-1"", agg=ds.count_cat(""louvain"")). ). def pts_to_coords(pts: xr.DataArray) -> pd.DataFrame:. """""". Turning pixel data into ""long"" sparse format. . One entry for each cluster, for each pixel it would occur in. """""". coords = pd.DataFrame(np.argwhere(np.asarray(pts)), columns=[""x"", ""y"", ""cat""]). coords[""cat""] = pd.Categorical.from_codes(coords[""cat""], categories=list(pts.coords.values())[2]). return coords. de",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:1507,availability,cluster,cluster,1507,"here's some code for building the cluster connectivity graph from pixel space:. <details>. <summary> Code: </summary>. ```python. # imports. import datashader as ds. from datashader import transfer_functions as tf. import matplotlib.pyplot as plt. from natsort import natsorted. import scanpy as sc. import numpy as np. import pandas as pd. import xarray as xr. from sklearn.neighbors import RadiusNeighborsTransformer. # example data. # 1.3 million cells, 38 clusters by louvain. adata = sc.read(""/Users/isaac/data/10x_mouse_13MM_processed.h5ad"", backed=""r""). # Previous colors. louvain_colors_old = dict(. zip(. adata.obs[""louvain""].cat.categories, . adata.uns[""louvain_colors""]. ). ). del adata.uns[""louvain_colors""]. louvain_colors_current = sc.pl._tools.scatterplots._get_palette(adata, values_key=""louvain""). df = sc.get.obs_df(. adata,. [""louvain""],. obsm_keys=[(""X_umap"", 0), (""X_umap"", 1)]. ). # pixel x pixel x n_clusters array (500, 500, 38). # Each value is the count of cells in the cluster at this pixel. pts = (. ds.Canvas(500, 500). .points(df, ""X_umap-0"", ""X_umap-1"", agg=ds.count_cat(""louvain"")). ). def pts_to_coords(pts: xr.DataArray) -> pd.DataFrame:. """""". Turning pixel data into ""long"" sparse format. . One entry for each cluster, for each pixel it would occur in. """""". coords = pd.DataFrame(np.argwhere(np.asarray(pts)), columns=[""x"", ""y"", ""cat""]). coords[""cat""] = pd.Categorical.from_codes(coords[""cat""], categories=list(pts.coords.values())[2]). return coords. def color_nodes(graph: dict) -> dict:. """"""Graph coloring algorithm. . From @atarashansky: https://github.com/theislab/scanpy/issues/1366#issuecomment-763066249. """""". color_map = {}. for node in sorted(graph, key=lambda x: len(graph[x]), reverse=True):. neighbor_colors = set(color_map.get(neigh) for neigh in graph[node]). color_map[node] = next( . color for color in range(len(graph)) if color not in neighbor_colors. ). return color_map. def neighboring_clusters(pts: xr.DataArray) -> dict:. """"""From array of (p",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:1756,availability,cluster,cluster,1756,"rom natsort import natsorted. import scanpy as sc. import numpy as np. import pandas as pd. import xarray as xr. from sklearn.neighbors import RadiusNeighborsTransformer. # example data. # 1.3 million cells, 38 clusters by louvain. adata = sc.read(""/Users/isaac/data/10x_mouse_13MM_processed.h5ad"", backed=""r""). # Previous colors. louvain_colors_old = dict(. zip(. adata.obs[""louvain""].cat.categories, . adata.uns[""louvain_colors""]. ). ). del adata.uns[""louvain_colors""]. louvain_colors_current = sc.pl._tools.scatterplots._get_palette(adata, values_key=""louvain""). df = sc.get.obs_df(. adata,. [""louvain""],. obsm_keys=[(""X_umap"", 0), (""X_umap"", 1)]. ). # pixel x pixel x n_clusters array (500, 500, 38). # Each value is the count of cells in the cluster at this pixel. pts = (. ds.Canvas(500, 500). .points(df, ""X_umap-0"", ""X_umap-1"", agg=ds.count_cat(""louvain"")). ). def pts_to_coords(pts: xr.DataArray) -> pd.DataFrame:. """""". Turning pixel data into ""long"" sparse format. . One entry for each cluster, for each pixel it would occur in. """""". coords = pd.DataFrame(np.argwhere(np.asarray(pts)), columns=[""x"", ""y"", ""cat""]). coords[""cat""] = pd.Categorical.from_codes(coords[""cat""], categories=list(pts.coords.values())[2]). return coords. def color_nodes(graph: dict) -> dict:. """"""Graph coloring algorithm. . From @atarashansky: https://github.com/theislab/scanpy/issues/1366#issuecomment-763066249. """""". color_map = {}. for node in sorted(graph, key=lambda x: len(graph[x]), reverse=True):. neighbor_colors = set(color_map.get(neigh) for neigh in graph[node]). color_map[node] = next( . color for color in range(len(graph)) if color not in neighbor_colors. ). return color_map. def neighboring_clusters(pts: xr.DataArray) -> dict:. """"""From array of (pixel, pixel, cluster) find which clusters neighbor eachother. """""". graph = {}. coords = pts_to_coords(pts). cat = coords[""cat""]. radius_neighbor = RadiusNeighborsTransformer(metric=""euclidean"", radius=1). radius_neighbor.fit(coords.values[:, :2]). g",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:2524,availability,cluster,cluster,2524,". pts = (. ds.Canvas(500, 500). .points(df, ""X_umap-0"", ""X_umap-1"", agg=ds.count_cat(""louvain"")). ). def pts_to_coords(pts: xr.DataArray) -> pd.DataFrame:. """""". Turning pixel data into ""long"" sparse format. . One entry for each cluster, for each pixel it would occur in. """""". coords = pd.DataFrame(np.argwhere(np.asarray(pts)), columns=[""x"", ""y"", ""cat""]). coords[""cat""] = pd.Categorical.from_codes(coords[""cat""], categories=list(pts.coords.values())[2]). return coords. def color_nodes(graph: dict) -> dict:. """"""Graph coloring algorithm. . From @atarashansky: https://github.com/theislab/scanpy/issues/1366#issuecomment-763066249. """""". color_map = {}. for node in sorted(graph, key=lambda x: len(graph[x]), reverse=True):. neighbor_colors = set(color_map.get(neigh) for neigh in graph[node]). color_map[node] = next( . color for color in range(len(graph)) if color not in neighbor_colors. ). return color_map. def neighboring_clusters(pts: xr.DataArray) -> dict:. """"""From array of (pixel, pixel, cluster) find which clusters neighbor eachother. """""". graph = {}. coords = pts_to_coords(pts). cat = coords[""cat""]. radius_neighbor = RadiusNeighborsTransformer(metric=""euclidean"", radius=1). radius_neighbor.fit(coords.values[:, :2]). g = radius_neighbor.radius_neighbors_graph(). for k, v in coords.groupby(""cat"").indices.items():. neighbors = np.unique(g[v].indices). graph[k] = natsorted(pd.unique(cat[neighbors])). return graph. graph = neighboring_clusters(pts). palette = {k: sc.pl.palettes.default_28[v] for k, v in color_nodes(graph).items()}. ```. </details>. Results:. ```python. tf.Images(. tf.shade(pts, color_key=louvain_colors_current, name=""Current scanpy coloring (38 unique colors out of 100)""),. tf.shade(pts, color_key=palette, name=""Graph coloring (25 unique colors)""),. tf.shade(pts, color_key=louvain_colors_old, name=""Old scanpy coloring (20 colors w/ repeats)""),. ). ```. ![image](https://user-images.githubusercontent.com/8238804/105129304-dc76c700-5b38-11eb-9be6-3f5613b24037.pn",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:2544,availability,cluster,clusters,2544,"(500, 500). .points(df, ""X_umap-0"", ""X_umap-1"", agg=ds.count_cat(""louvain"")). ). def pts_to_coords(pts: xr.DataArray) -> pd.DataFrame:. """""". Turning pixel data into ""long"" sparse format. . One entry for each cluster, for each pixel it would occur in. """""". coords = pd.DataFrame(np.argwhere(np.asarray(pts)), columns=[""x"", ""y"", ""cat""]). coords[""cat""] = pd.Categorical.from_codes(coords[""cat""], categories=list(pts.coords.values())[2]). return coords. def color_nodes(graph: dict) -> dict:. """"""Graph coloring algorithm. . From @atarashansky: https://github.com/theislab/scanpy/issues/1366#issuecomment-763066249. """""". color_map = {}. for node in sorted(graph, key=lambda x: len(graph[x]), reverse=True):. neighbor_colors = set(color_map.get(neigh) for neigh in graph[node]). color_map[node] = next( . color for color in range(len(graph)) if color not in neighbor_colors. ). return color_map. def neighboring_clusters(pts: xr.DataArray) -> dict:. """"""From array of (pixel, pixel, cluster) find which clusters neighbor eachother. """""". graph = {}. coords = pts_to_coords(pts). cat = coords[""cat""]. radius_neighbor = RadiusNeighborsTransformer(metric=""euclidean"", radius=1). radius_neighbor.fit(coords.values[:, :2]). g = radius_neighbor.radius_neighbors_graph(). for k, v in coords.groupby(""cat"").indices.items():. neighbors = np.unique(g[v].indices). graph[k] = natsorted(pd.unique(cat[neighbors])). return graph. graph = neighboring_clusters(pts). palette = {k: sc.pl.palettes.default_28[v] for k, v in color_nodes(graph).items()}. ```. </details>. Results:. ```python. tf.Images(. tf.shade(pts, color_key=louvain_colors_current, name=""Current scanpy coloring (38 unique colors out of 100)""),. tf.shade(pts, color_key=palette, name=""Graph coloring (25 unique colors)""),. tf.shade(pts, color_key=louvain_colors_old, name=""Old scanpy coloring (20 colors w/ repeats)""),. ). ```. ![image](https://user-images.githubusercontent.com/8238804/105129304-dc76c700-5b38-11eb-9be6-3f5613b24037.png). *Additional plot",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:4106,availability,cluster,cluster,4106,"on. tf.Images(. tf.shade(pts, color_key=louvain_colors_current, name=""Current scanpy coloring (38 unique colors out of 100)""),. tf.shade(pts, color_key=palette, name=""Graph coloring (25 unique colors)""),. tf.shade(pts, color_key=louvain_colors_old, name=""Old scanpy coloring (20 colors w/ repeats)""),. ). ```. ![image](https://user-images.githubusercontent.com/8238804/105129304-dc76c700-5b38-11eb-9be6-3f5613b24037.png). *Additional plot using color generating code from above:*. <details>. <summary> code </summary>. ```python. from matplotlib.colors import to_hex. colors = list(itertools.islice(rgbs(), len(set(color_map.values())))). tarashansky_palette = {k: to_hex(colors[v]) for k, v in color_map.items()}. tf.shade(pts, color_key=tarashansky_palette). ```. </details>. ![image](https://user-images.githubusercontent.com/8238804/105131791-c3244980-5b3d-11eb-83cc-2691b392b1c1.png). I think it's good, but there's room for improvement. I was initially worried about this example since the cluster connectivity graph is highly connected, but this seems to have worked out alright. Overplotting (points sitting on top of eachother) is still a problem, but I think that's a separate problem from choosing colors. * Even if they do not touch, can we make it so clusters close to each-other are less likely to get similar colors? This would start becoming more of an optimization problem, and more complicated. * Picking a color palette where all colors are very visible is important. (maybe adding a light border when there's nothing in the background? maybe something similar to ""player model contrast boost"" shaders?). * I think there are some ""spurious"" connections in the graph. Many clusters have >20 neighbors. I think this has to do with outlier points and dispersed points. Not completely sure how to deal with this. Maybe less of an issue with smaller datasets/ leiden clustering? * When categories are disconnected, how do we indicate they're the same category if unique color is no long",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:4374,availability,cluster,clusters,4374,"loring (20 colors w/ repeats)""),. ). ```. ![image](https://user-images.githubusercontent.com/8238804/105129304-dc76c700-5b38-11eb-9be6-3f5613b24037.png). *Additional plot using color generating code from above:*. <details>. <summary> code </summary>. ```python. from matplotlib.colors import to_hex. colors = list(itertools.islice(rgbs(), len(set(color_map.values())))). tarashansky_palette = {k: to_hex(colors[v]) for k, v in color_map.items()}. tf.shade(pts, color_key=tarashansky_palette). ```. </details>. ![image](https://user-images.githubusercontent.com/8238804/105131791-c3244980-5b3d-11eb-83cc-2691b392b1c1.png). I think it's good, but there's room for improvement. I was initially worried about this example since the cluster connectivity graph is highly connected, but this seems to have worked out alright. Overplotting (points sitting on top of eachother) is still a problem, but I think that's a separate problem from choosing colors. * Even if they do not touch, can we make it so clusters close to each-other are less likely to get similar colors? This would start becoming more of an optimization problem, and more complicated. * Picking a color palette where all colors are very visible is important. (maybe adding a light border when there's nothing in the background? maybe something similar to ""player model contrast boost"" shaders?). * I think there are some ""spurious"" connections in the graph. Many clusters have >20 neighbors. I think this has to do with outlier points and dispersed points. Not completely sure how to deal with this. Maybe less of an issue with smaller datasets/ leiden clustering? * When categories are disconnected, how do we indicate they're the same category if unique color is no longer an option? Do we require disconnected categories be uniquely assigned a color? Is this a case for interactivity? I've been thinking that it might be worth starting a package for dealing with common issues in plotting single cell data. Largely involving color assign",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:4801,availability,cluster,clusters,4801,"alues())))). tarashansky_palette = {k: to_hex(colors[v]) for k, v in color_map.items()}. tf.shade(pts, color_key=tarashansky_palette). ```. </details>. ![image](https://user-images.githubusercontent.com/8238804/105131791-c3244980-5b3d-11eb-83cc-2691b392b1c1.png). I think it's good, but there's room for improvement. I was initially worried about this example since the cluster connectivity graph is highly connected, but this seems to have worked out alright. Overplotting (points sitting on top of eachother) is still a problem, but I think that's a separate problem from choosing colors. * Even if they do not touch, can we make it so clusters close to each-other are less likely to get similar colors? This would start becoming more of an optimization problem, and more complicated. * Picking a color palette where all colors are very visible is important. (maybe adding a light border when there's nothing in the background? maybe something similar to ""player model contrast boost"" shaders?). * I think there are some ""spurious"" connections in the graph. Many clusters have >20 neighbors. I think this has to do with outlier points and dispersed points. Not completely sure how to deal with this. Maybe less of an issue with smaller datasets/ leiden clustering? * When categories are disconnected, how do we indicate they're the same category if unique color is no longer an option? Do we require disconnected categories be uniquely assigned a color? Is this a case for interactivity? I've been thinking that it might be worth starting a package for dealing with common issues in plotting single cell data. Largely involving color assignment and overplotting. I think this should be, or at least start out as, separate from scanpy since there are a number of dependencies I think are useful here, which aren't required for scanpy. Plus being able to iterate quickly would be nice. I've started collecting some notebooks on this [here](https://github.com/ivirshup/notebooks/tree/master/plotting).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:4991,availability,cluster,clustering,4991,"alues())))). tarashansky_palette = {k: to_hex(colors[v]) for k, v in color_map.items()}. tf.shade(pts, color_key=tarashansky_palette). ```. </details>. ![image](https://user-images.githubusercontent.com/8238804/105131791-c3244980-5b3d-11eb-83cc-2691b392b1c1.png). I think it's good, but there's room for improvement. I was initially worried about this example since the cluster connectivity graph is highly connected, but this seems to have worked out alright. Overplotting (points sitting on top of eachother) is still a problem, but I think that's a separate problem from choosing colors. * Even if they do not touch, can we make it so clusters close to each-other are less likely to get similar colors? This would start becoming more of an optimization problem, and more complicated. * Picking a color palette where all colors are very visible is important. (maybe adding a light border when there's nothing in the background? maybe something similar to ""player model contrast boost"" shaders?). * I think there are some ""spurious"" connections in the graph. Many clusters have >20 neighbors. I think this has to do with outlier points and dispersed points. Not completely sure how to deal with this. Maybe less of an issue with smaller datasets/ leiden clustering? * When categories are disconnected, how do we indicate they're the same category if unique color is no longer an option? Do we require disconnected categories be uniquely assigned a color? Is this a case for interactivity? I've been thinking that it might be worth starting a package for dealing with common issues in plotting single cell data. Largely involving color assignment and overplotting. I think this should be, or at least start out as, separate from scanpy since there are a number of dependencies I think are useful here, which aren't required for scanpy. Plus being able to iterate quickly would be nice. I've started collecting some notebooks on this [here](https://github.com/ivirshup/notebooks/tree/master/plotting).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:532,deployability,build,building,532,"@atarashansky, that's great! I've built on this a bit, with some points about graph construction. * The problems isn't really nearest K neighbors, it's radius neighbors. * Embedding position is relevant, but I think position in pixel space is more important in this case. You could imagine one dimension of your embedding plot having a range of (0, 100) while the other has (0, 1), but displayed on a square plot. Euclidean distance for neighboring points would not be particularly effective here. To that end, here's some code for building the cluster connectivity graph from pixel space:. <details>. <summary> Code: </summary>. ```python. # imports. import datashader as ds. from datashader import transfer_functions as tf. import matplotlib.pyplot as plt. from natsort import natsorted. import scanpy as sc. import numpy as np. import pandas as pd. import xarray as xr. from sklearn.neighbors import RadiusNeighborsTransformer. # example data. # 1.3 million cells, 38 clusters by louvain. adata = sc.read(""/Users/isaac/data/10x_mouse_13MM_processed.h5ad"", backed=""r""). # Previous colors. louvain_colors_old = dict(. zip(. adata.obs[""louvain""].cat.categories, . adata.uns[""louvain_colors""]. ). ). del adata.uns[""louvain_colors""]. louvain_colors_current = sc.pl._tools.scatterplots._get_palette(adata, values_key=""louvain""). df = sc.get.obs_df(. adata,. [""louvain""],. obsm_keys=[(""X_umap"", 0), (""X_umap"", 1)]. ). # pixel x pixel x n_clusters array (500, 500, 38). # Each value is the count of cells in the cluster at this pixel. pts = (. ds.Canvas(500, 500). .points(df, ""X_umap-0"", ""X_umap-1"", agg=ds.count_cat(""louvain"")). ). def pts_to_coords(pts: xr.DataArray) -> pd.DataFrame:. """""". Turning pixel data into ""long"" sparse format. . One entry for each cluster, for each pixel it would occur in. """""". coords = pd.DataFrame(np.argwhere(np.asarray(pts)), columns=[""x"", ""y"", ""cat""]). coords[""cat""] = pd.Categorical.from_codes(coords[""cat""], categories=list(pts.coords.values())[2]). return coords. de",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:545,deployability,cluster,cluster,545,"@atarashansky, that's great! I've built on this a bit, with some points about graph construction. * The problems isn't really nearest K neighbors, it's radius neighbors. * Embedding position is relevant, but I think position in pixel space is more important in this case. You could imagine one dimension of your embedding plot having a range of (0, 100) while the other has (0, 1), but displayed on a square plot. Euclidean distance for neighboring points would not be particularly effective here. To that end, here's some code for building the cluster connectivity graph from pixel space:. <details>. <summary> Code: </summary>. ```python. # imports. import datashader as ds. from datashader import transfer_functions as tf. import matplotlib.pyplot as plt. from natsort import natsorted. import scanpy as sc. import numpy as np. import pandas as pd. import xarray as xr. from sklearn.neighbors import RadiusNeighborsTransformer. # example data. # 1.3 million cells, 38 clusters by louvain. adata = sc.read(""/Users/isaac/data/10x_mouse_13MM_processed.h5ad"", backed=""r""). # Previous colors. louvain_colors_old = dict(. zip(. adata.obs[""louvain""].cat.categories, . adata.uns[""louvain_colors""]. ). ). del adata.uns[""louvain_colors""]. louvain_colors_current = sc.pl._tools.scatterplots._get_palette(adata, values_key=""louvain""). df = sc.get.obs_df(. adata,. [""louvain""],. obsm_keys=[(""X_umap"", 0), (""X_umap"", 1)]. ). # pixel x pixel x n_clusters array (500, 500, 38). # Each value is the count of cells in the cluster at this pixel. pts = (. ds.Canvas(500, 500). .points(df, ""X_umap-0"", ""X_umap-1"", agg=ds.count_cat(""louvain"")). ). def pts_to_coords(pts: xr.DataArray) -> pd.DataFrame:. """""". Turning pixel data into ""long"" sparse format. . One entry for each cluster, for each pixel it would occur in. """""". coords = pd.DataFrame(np.argwhere(np.asarray(pts)), columns=[""x"", ""y"", ""cat""]). coords[""cat""] = pd.Categorical.from_codes(coords[""cat""], categories=list(pts.coords.values())[2]). return coords. de",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:971,deployability,cluster,clusters,971,"@atarashansky, that's great! I've built on this a bit, with some points about graph construction. * The problems isn't really nearest K neighbors, it's radius neighbors. * Embedding position is relevant, but I think position in pixel space is more important in this case. You could imagine one dimension of your embedding plot having a range of (0, 100) while the other has (0, 1), but displayed on a square plot. Euclidean distance for neighboring points would not be particularly effective here. To that end, here's some code for building the cluster connectivity graph from pixel space:. <details>. <summary> Code: </summary>. ```python. # imports. import datashader as ds. from datashader import transfer_functions as tf. import matplotlib.pyplot as plt. from natsort import natsorted. import scanpy as sc. import numpy as np. import pandas as pd. import xarray as xr. from sklearn.neighbors import RadiusNeighborsTransformer. # example data. # 1.3 million cells, 38 clusters by louvain. adata = sc.read(""/Users/isaac/data/10x_mouse_13MM_processed.h5ad"", backed=""r""). # Previous colors. louvain_colors_old = dict(. zip(. adata.obs[""louvain""].cat.categories, . adata.uns[""louvain_colors""]. ). ). del adata.uns[""louvain_colors""]. louvain_colors_current = sc.pl._tools.scatterplots._get_palette(adata, values_key=""louvain""). df = sc.get.obs_df(. adata,. [""louvain""],. obsm_keys=[(""X_umap"", 0), (""X_umap"", 1)]. ). # pixel x pixel x n_clusters array (500, 500, 38). # Each value is the count of cells in the cluster at this pixel. pts = (. ds.Canvas(500, 500). .points(df, ""X_umap-0"", ""X_umap-1"", agg=ds.count_cat(""louvain"")). ). def pts_to_coords(pts: xr.DataArray) -> pd.DataFrame:. """""". Turning pixel data into ""long"" sparse format. . One entry for each cluster, for each pixel it would occur in. """""". coords = pd.DataFrame(np.argwhere(np.asarray(pts)), columns=[""x"", ""y"", ""cat""]). coords[""cat""] = pd.Categorical.from_codes(coords[""cat""], categories=list(pts.coords.values())[2]). return coords. de",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:1507,deployability,cluster,cluster,1507,"here's some code for building the cluster connectivity graph from pixel space:. <details>. <summary> Code: </summary>. ```python. # imports. import datashader as ds. from datashader import transfer_functions as tf. import matplotlib.pyplot as plt. from natsort import natsorted. import scanpy as sc. import numpy as np. import pandas as pd. import xarray as xr. from sklearn.neighbors import RadiusNeighborsTransformer. # example data. # 1.3 million cells, 38 clusters by louvain. adata = sc.read(""/Users/isaac/data/10x_mouse_13MM_processed.h5ad"", backed=""r""). # Previous colors. louvain_colors_old = dict(. zip(. adata.obs[""louvain""].cat.categories, . adata.uns[""louvain_colors""]. ). ). del adata.uns[""louvain_colors""]. louvain_colors_current = sc.pl._tools.scatterplots._get_palette(adata, values_key=""louvain""). df = sc.get.obs_df(. adata,. [""louvain""],. obsm_keys=[(""X_umap"", 0), (""X_umap"", 1)]. ). # pixel x pixel x n_clusters array (500, 500, 38). # Each value is the count of cells in the cluster at this pixel. pts = (. ds.Canvas(500, 500). .points(df, ""X_umap-0"", ""X_umap-1"", agg=ds.count_cat(""louvain"")). ). def pts_to_coords(pts: xr.DataArray) -> pd.DataFrame:. """""". Turning pixel data into ""long"" sparse format. . One entry for each cluster, for each pixel it would occur in. """""". coords = pd.DataFrame(np.argwhere(np.asarray(pts)), columns=[""x"", ""y"", ""cat""]). coords[""cat""] = pd.Categorical.from_codes(coords[""cat""], categories=list(pts.coords.values())[2]). return coords. def color_nodes(graph: dict) -> dict:. """"""Graph coloring algorithm. . From @atarashansky: https://github.com/theislab/scanpy/issues/1366#issuecomment-763066249. """""". color_map = {}. for node in sorted(graph, key=lambda x: len(graph[x]), reverse=True):. neighbor_colors = set(color_map.get(neigh) for neigh in graph[node]). color_map[node] = next( . color for color in range(len(graph)) if color not in neighbor_colors. ). return color_map. def neighboring_clusters(pts: xr.DataArray) -> dict:. """"""From array of (p",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:1756,deployability,cluster,cluster,1756,"rom natsort import natsorted. import scanpy as sc. import numpy as np. import pandas as pd. import xarray as xr. from sklearn.neighbors import RadiusNeighborsTransformer. # example data. # 1.3 million cells, 38 clusters by louvain. adata = sc.read(""/Users/isaac/data/10x_mouse_13MM_processed.h5ad"", backed=""r""). # Previous colors. louvain_colors_old = dict(. zip(. adata.obs[""louvain""].cat.categories, . adata.uns[""louvain_colors""]. ). ). del adata.uns[""louvain_colors""]. louvain_colors_current = sc.pl._tools.scatterplots._get_palette(adata, values_key=""louvain""). df = sc.get.obs_df(. adata,. [""louvain""],. obsm_keys=[(""X_umap"", 0), (""X_umap"", 1)]. ). # pixel x pixel x n_clusters array (500, 500, 38). # Each value is the count of cells in the cluster at this pixel. pts = (. ds.Canvas(500, 500). .points(df, ""X_umap-0"", ""X_umap-1"", agg=ds.count_cat(""louvain"")). ). def pts_to_coords(pts: xr.DataArray) -> pd.DataFrame:. """""". Turning pixel data into ""long"" sparse format. . One entry for each cluster, for each pixel it would occur in. """""". coords = pd.DataFrame(np.argwhere(np.asarray(pts)), columns=[""x"", ""y"", ""cat""]). coords[""cat""] = pd.Categorical.from_codes(coords[""cat""], categories=list(pts.coords.values())[2]). return coords. def color_nodes(graph: dict) -> dict:. """"""Graph coloring algorithm. . From @atarashansky: https://github.com/theislab/scanpy/issues/1366#issuecomment-763066249. """""". color_map = {}. for node in sorted(graph, key=lambda x: len(graph[x]), reverse=True):. neighbor_colors = set(color_map.get(neigh) for neigh in graph[node]). color_map[node] = next( . color for color in range(len(graph)) if color not in neighbor_colors. ). return color_map. def neighboring_clusters(pts: xr.DataArray) -> dict:. """"""From array of (pixel, pixel, cluster) find which clusters neighbor eachother. """""". graph = {}. coords = pts_to_coords(pts). cat = coords[""cat""]. radius_neighbor = RadiusNeighborsTransformer(metric=""euclidean"", radius=1). radius_neighbor.fit(coords.values[:, :2]). g",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:2524,deployability,cluster,cluster,2524,". pts = (. ds.Canvas(500, 500). .points(df, ""X_umap-0"", ""X_umap-1"", agg=ds.count_cat(""louvain"")). ). def pts_to_coords(pts: xr.DataArray) -> pd.DataFrame:. """""". Turning pixel data into ""long"" sparse format. . One entry for each cluster, for each pixel it would occur in. """""". coords = pd.DataFrame(np.argwhere(np.asarray(pts)), columns=[""x"", ""y"", ""cat""]). coords[""cat""] = pd.Categorical.from_codes(coords[""cat""], categories=list(pts.coords.values())[2]). return coords. def color_nodes(graph: dict) -> dict:. """"""Graph coloring algorithm. . From @atarashansky: https://github.com/theislab/scanpy/issues/1366#issuecomment-763066249. """""". color_map = {}. for node in sorted(graph, key=lambda x: len(graph[x]), reverse=True):. neighbor_colors = set(color_map.get(neigh) for neigh in graph[node]). color_map[node] = next( . color for color in range(len(graph)) if color not in neighbor_colors. ). return color_map. def neighboring_clusters(pts: xr.DataArray) -> dict:. """"""From array of (pixel, pixel, cluster) find which clusters neighbor eachother. """""". graph = {}. coords = pts_to_coords(pts). cat = coords[""cat""]. radius_neighbor = RadiusNeighborsTransformer(metric=""euclidean"", radius=1). radius_neighbor.fit(coords.values[:, :2]). g = radius_neighbor.radius_neighbors_graph(). for k, v in coords.groupby(""cat"").indices.items():. neighbors = np.unique(g[v].indices). graph[k] = natsorted(pd.unique(cat[neighbors])). return graph. graph = neighboring_clusters(pts). palette = {k: sc.pl.palettes.default_28[v] for k, v in color_nodes(graph).items()}. ```. </details>. Results:. ```python. tf.Images(. tf.shade(pts, color_key=louvain_colors_current, name=""Current scanpy coloring (38 unique colors out of 100)""),. tf.shade(pts, color_key=palette, name=""Graph coloring (25 unique colors)""),. tf.shade(pts, color_key=louvain_colors_old, name=""Old scanpy coloring (20 colors w/ repeats)""),. ). ```. ![image](https://user-images.githubusercontent.com/8238804/105129304-dc76c700-5b38-11eb-9be6-3f5613b24037.pn",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:2544,deployability,cluster,clusters,2544,"(500, 500). .points(df, ""X_umap-0"", ""X_umap-1"", agg=ds.count_cat(""louvain"")). ). def pts_to_coords(pts: xr.DataArray) -> pd.DataFrame:. """""". Turning pixel data into ""long"" sparse format. . One entry for each cluster, for each pixel it would occur in. """""". coords = pd.DataFrame(np.argwhere(np.asarray(pts)), columns=[""x"", ""y"", ""cat""]). coords[""cat""] = pd.Categorical.from_codes(coords[""cat""], categories=list(pts.coords.values())[2]). return coords. def color_nodes(graph: dict) -> dict:. """"""Graph coloring algorithm. . From @atarashansky: https://github.com/theislab/scanpy/issues/1366#issuecomment-763066249. """""". color_map = {}. for node in sorted(graph, key=lambda x: len(graph[x]), reverse=True):. neighbor_colors = set(color_map.get(neigh) for neigh in graph[node]). color_map[node] = next( . color for color in range(len(graph)) if color not in neighbor_colors. ). return color_map. def neighboring_clusters(pts: xr.DataArray) -> dict:. """"""From array of (pixel, pixel, cluster) find which clusters neighbor eachother. """""". graph = {}. coords = pts_to_coords(pts). cat = coords[""cat""]. radius_neighbor = RadiusNeighborsTransformer(metric=""euclidean"", radius=1). radius_neighbor.fit(coords.values[:, :2]). g = radius_neighbor.radius_neighbors_graph(). for k, v in coords.groupby(""cat"").indices.items():. neighbors = np.unique(g[v].indices). graph[k] = natsorted(pd.unique(cat[neighbors])). return graph. graph = neighboring_clusters(pts). palette = {k: sc.pl.palettes.default_28[v] for k, v in color_nodes(graph).items()}. ```. </details>. Results:. ```python. tf.Images(. tf.shade(pts, color_key=louvain_colors_current, name=""Current scanpy coloring (38 unique colors out of 100)""),. tf.shade(pts, color_key=palette, name=""Graph coloring (25 unique colors)""),. tf.shade(pts, color_key=louvain_colors_old, name=""Old scanpy coloring (20 colors w/ repeats)""),. ). ```. ![image](https://user-images.githubusercontent.com/8238804/105129304-dc76c700-5b38-11eb-9be6-3f5613b24037.png). *Additional plot",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:4106,deployability,cluster,cluster,4106,"on. tf.Images(. tf.shade(pts, color_key=louvain_colors_current, name=""Current scanpy coloring (38 unique colors out of 100)""),. tf.shade(pts, color_key=palette, name=""Graph coloring (25 unique colors)""),. tf.shade(pts, color_key=louvain_colors_old, name=""Old scanpy coloring (20 colors w/ repeats)""),. ). ```. ![image](https://user-images.githubusercontent.com/8238804/105129304-dc76c700-5b38-11eb-9be6-3f5613b24037.png). *Additional plot using color generating code from above:*. <details>. <summary> code </summary>. ```python. from matplotlib.colors import to_hex. colors = list(itertools.islice(rgbs(), len(set(color_map.values())))). tarashansky_palette = {k: to_hex(colors[v]) for k, v in color_map.items()}. tf.shade(pts, color_key=tarashansky_palette). ```. </details>. ![image](https://user-images.githubusercontent.com/8238804/105131791-c3244980-5b3d-11eb-83cc-2691b392b1c1.png). I think it's good, but there's room for improvement. I was initially worried about this example since the cluster connectivity graph is highly connected, but this seems to have worked out alright. Overplotting (points sitting on top of eachother) is still a problem, but I think that's a separate problem from choosing colors. * Even if they do not touch, can we make it so clusters close to each-other are less likely to get similar colors? This would start becoming more of an optimization problem, and more complicated. * Picking a color palette where all colors are very visible is important. (maybe adding a light border when there's nothing in the background? maybe something similar to ""player model contrast boost"" shaders?). * I think there are some ""spurious"" connections in the graph. Many clusters have >20 neighbors. I think this has to do with outlier points and dispersed points. Not completely sure how to deal with this. Maybe less of an issue with smaller datasets/ leiden clustering? * When categories are disconnected, how do we indicate they're the same category if unique color is no long",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:4374,deployability,cluster,clusters,4374,"loring (20 colors w/ repeats)""),. ). ```. ![image](https://user-images.githubusercontent.com/8238804/105129304-dc76c700-5b38-11eb-9be6-3f5613b24037.png). *Additional plot using color generating code from above:*. <details>. <summary> code </summary>. ```python. from matplotlib.colors import to_hex. colors = list(itertools.islice(rgbs(), len(set(color_map.values())))). tarashansky_palette = {k: to_hex(colors[v]) for k, v in color_map.items()}. tf.shade(pts, color_key=tarashansky_palette). ```. </details>. ![image](https://user-images.githubusercontent.com/8238804/105131791-c3244980-5b3d-11eb-83cc-2691b392b1c1.png). I think it's good, but there's room for improvement. I was initially worried about this example since the cluster connectivity graph is highly connected, but this seems to have worked out alright. Overplotting (points sitting on top of eachother) is still a problem, but I think that's a separate problem from choosing colors. * Even if they do not touch, can we make it so clusters close to each-other are less likely to get similar colors? This would start becoming more of an optimization problem, and more complicated. * Picking a color palette where all colors are very visible is important. (maybe adding a light border when there's nothing in the background? maybe something similar to ""player model contrast boost"" shaders?). * I think there are some ""spurious"" connections in the graph. Many clusters have >20 neighbors. I think this has to do with outlier points and dispersed points. Not completely sure how to deal with this. Maybe less of an issue with smaller datasets/ leiden clustering? * When categories are disconnected, how do we indicate they're the same category if unique color is no longer an option? Do we require disconnected categories be uniquely assigned a color? Is this a case for interactivity? I've been thinking that it might be worth starting a package for dealing with common issues in plotting single cell data. Largely involving color assign",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:4801,deployability,cluster,clusters,4801,"alues())))). tarashansky_palette = {k: to_hex(colors[v]) for k, v in color_map.items()}. tf.shade(pts, color_key=tarashansky_palette). ```. </details>. ![image](https://user-images.githubusercontent.com/8238804/105131791-c3244980-5b3d-11eb-83cc-2691b392b1c1.png). I think it's good, but there's room for improvement. I was initially worried about this example since the cluster connectivity graph is highly connected, but this seems to have worked out alright. Overplotting (points sitting on top of eachother) is still a problem, but I think that's a separate problem from choosing colors. * Even if they do not touch, can we make it so clusters close to each-other are less likely to get similar colors? This would start becoming more of an optimization problem, and more complicated. * Picking a color palette where all colors are very visible is important. (maybe adding a light border when there's nothing in the background? maybe something similar to ""player model contrast boost"" shaders?). * I think there are some ""spurious"" connections in the graph. Many clusters have >20 neighbors. I think this has to do with outlier points and dispersed points. Not completely sure how to deal with this. Maybe less of an issue with smaller datasets/ leiden clustering? * When categories are disconnected, how do we indicate they're the same category if unique color is no longer an option? Do we require disconnected categories be uniquely assigned a color? Is this a case for interactivity? I've been thinking that it might be worth starting a package for dealing with common issues in plotting single cell data. Largely involving color assignment and overplotting. I think this should be, or at least start out as, separate from scanpy since there are a number of dependencies I think are useful here, which aren't required for scanpy. Plus being able to iterate quickly would be nice. I've started collecting some notebooks on this [here](https://github.com/ivirshup/notebooks/tree/master/plotting).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:4991,deployability,cluster,clustering,4991,"alues())))). tarashansky_palette = {k: to_hex(colors[v]) for k, v in color_map.items()}. tf.shade(pts, color_key=tarashansky_palette). ```. </details>. ![image](https://user-images.githubusercontent.com/8238804/105131791-c3244980-5b3d-11eb-83cc-2691b392b1c1.png). I think it's good, but there's room for improvement. I was initially worried about this example since the cluster connectivity graph is highly connected, but this seems to have worked out alright. Overplotting (points sitting on top of eachother) is still a problem, but I think that's a separate problem from choosing colors. * Even if they do not touch, can we make it so clusters close to each-other are less likely to get similar colors? This would start becoming more of an optimization problem, and more complicated. * Picking a color palette where all colors are very visible is important. (maybe adding a light border when there's nothing in the background? maybe something similar to ""player model contrast boost"" shaders?). * I think there are some ""spurious"" connections in the graph. Many clusters have >20 neighbors. I think this has to do with outlier points and dispersed points. Not completely sure how to deal with this. Maybe less of an issue with smaller datasets/ leiden clustering? * When categories are disconnected, how do we indicate they're the same category if unique color is no longer an option? Do we require disconnected categories be uniquely assigned a color? Is this a case for interactivity? I've been thinking that it might be worth starting a package for dealing with common issues in plotting single cell data. Largely involving color assignment and overplotting. I think this should be, or at least start out as, separate from scanpy since there are a number of dependencies I think are useful here, which aren't required for scanpy. Plus being able to iterate quickly would be nice. I've started collecting some notebooks on this [here](https://github.com/ivirshup/notebooks/tree/master/plotting).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:5500,deployability,depend,dependencies,5500,"alues())))). tarashansky_palette = {k: to_hex(colors[v]) for k, v in color_map.items()}. tf.shade(pts, color_key=tarashansky_palette). ```. </details>. ![image](https://user-images.githubusercontent.com/8238804/105131791-c3244980-5b3d-11eb-83cc-2691b392b1c1.png). I think it's good, but there's room for improvement. I was initially worried about this example since the cluster connectivity graph is highly connected, but this seems to have worked out alright. Overplotting (points sitting on top of eachother) is still a problem, but I think that's a separate problem from choosing colors. * Even if they do not touch, can we make it so clusters close to each-other are less likely to get similar colors? This would start becoming more of an optimization problem, and more complicated. * Picking a color palette where all colors are very visible is important. (maybe adding a light border when there's nothing in the background? maybe something similar to ""player model contrast boost"" shaders?). * I think there are some ""spurious"" connections in the graph. Many clusters have >20 neighbors. I think this has to do with outlier points and dispersed points. Not completely sure how to deal with this. Maybe less of an issue with smaller datasets/ leiden clustering? * When categories are disconnected, how do we indicate they're the same category if unique color is no longer an option? Do we require disconnected categories be uniquely assigned a color? Is this a case for interactivity? I've been thinking that it might be worth starting a package for dealing with common issues in plotting single cell data. Largely involving color assignment and overplotting. I think this should be, or at least start out as, separate from scanpy since there are a number of dependencies I think are useful here, which aren't required for scanpy. Plus being able to iterate quickly would be nice. I've started collecting some notebooks on this [here](https://github.com/ivirshup/notebooks/tree/master/plotting).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:3180,energy efficiency,Current,Current,3180,"node in sorted(graph, key=lambda x: len(graph[x]), reverse=True):. neighbor_colors = set(color_map.get(neigh) for neigh in graph[node]). color_map[node] = next( . color for color in range(len(graph)) if color not in neighbor_colors. ). return color_map. def neighboring_clusters(pts: xr.DataArray) -> dict:. """"""From array of (pixel, pixel, cluster) find which clusters neighbor eachother. """""". graph = {}. coords = pts_to_coords(pts). cat = coords[""cat""]. radius_neighbor = RadiusNeighborsTransformer(metric=""euclidean"", radius=1). radius_neighbor.fit(coords.values[:, :2]). g = radius_neighbor.radius_neighbors_graph(). for k, v in coords.groupby(""cat"").indices.items():. neighbors = np.unique(g[v].indices). graph[k] = natsorted(pd.unique(cat[neighbors])). return graph. graph = neighboring_clusters(pts). palette = {k: sc.pl.palettes.default_28[v] for k, v in color_nodes(graph).items()}. ```. </details>. Results:. ```python. tf.Images(. tf.shade(pts, color_key=louvain_colors_current, name=""Current scanpy coloring (38 unique colors out of 100)""),. tf.shade(pts, color_key=palette, name=""Graph coloring (25 unique colors)""),. tf.shade(pts, color_key=louvain_colors_old, name=""Old scanpy coloring (20 colors w/ repeats)""),. ). ```. ![image](https://user-images.githubusercontent.com/8238804/105129304-dc76c700-5b38-11eb-9be6-3f5613b24037.png). *Additional plot using color generating code from above:*. <details>. <summary> code </summary>. ```python. from matplotlib.colors import to_hex. colors = list(itertools.islice(rgbs(), len(set(color_map.values())))). tarashansky_palette = {k: to_hex(colors[v]) for k, v in color_map.items()}. tf.shade(pts, color_key=tarashansky_palette). ```. </details>. ![image](https://user-images.githubusercontent.com/8238804/105131791-c3244980-5b3d-11eb-83cc-2691b392b1c1.png). I think it's good, but there's room for improvement. I was initially worried about this example since the cluster connectivity graph is highly connected, but this seems to have worked ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:4479,energy efficiency,optim,optimization,4479,"304-dc76c700-5b38-11eb-9be6-3f5613b24037.png). *Additional plot using color generating code from above:*. <details>. <summary> code </summary>. ```python. from matplotlib.colors import to_hex. colors = list(itertools.islice(rgbs(), len(set(color_map.values())))). tarashansky_palette = {k: to_hex(colors[v]) for k, v in color_map.items()}. tf.shade(pts, color_key=tarashansky_palette). ```. </details>. ![image](https://user-images.githubusercontent.com/8238804/105131791-c3244980-5b3d-11eb-83cc-2691b392b1c1.png). I think it's good, but there's room for improvement. I was initially worried about this example since the cluster connectivity graph is highly connected, but this seems to have worked out alright. Overplotting (points sitting on top of eachother) is still a problem, but I think that's a separate problem from choosing colors. * Even if they do not touch, can we make it so clusters close to each-other are less likely to get similar colors? This would start becoming more of an optimization problem, and more complicated. * Picking a color palette where all colors are very visible is important. (maybe adding a light border when there's nothing in the background? maybe something similar to ""player model contrast boost"" shaders?). * I think there are some ""spurious"" connections in the graph. Many clusters have >20 neighbors. I think this has to do with outlier points and dispersed points. Not completely sure how to deal with this. Maybe less of an issue with smaller datasets/ leiden clustering? * When categories are disconnected, how do we indicate they're the same category if unique color is no longer an option? Do we require disconnected categories be uniquely assigned a color? Is this a case for interactivity? I've been thinking that it might be worth starting a package for dealing with common issues in plotting single cell data. Largely involving color assignment and overplotting. I think this should be, or at least start out as, separate from scanpy since there a",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:4701,energy efficiency,model,model,4701,"lice(rgbs(), len(set(color_map.values())))). tarashansky_palette = {k: to_hex(colors[v]) for k, v in color_map.items()}. tf.shade(pts, color_key=tarashansky_palette). ```. </details>. ![image](https://user-images.githubusercontent.com/8238804/105131791-c3244980-5b3d-11eb-83cc-2691b392b1c1.png). I think it's good, but there's room for improvement. I was initially worried about this example since the cluster connectivity graph is highly connected, but this seems to have worked out alright. Overplotting (points sitting on top of eachother) is still a problem, but I think that's a separate problem from choosing colors. * Even if they do not touch, can we make it so clusters close to each-other are less likely to get similar colors? This would start becoming more of an optimization problem, and more complicated. * Picking a color palette where all colors are very visible is important. (maybe adding a light border when there's nothing in the background? maybe something similar to ""player model contrast boost"" shaders?). * I think there are some ""spurious"" connections in the graph. Many clusters have >20 neighbors. I think this has to do with outlier points and dispersed points. Not completely sure how to deal with this. Maybe less of an issue with smaller datasets/ leiden clustering? * When categories are disconnected, how do we indicate they're the same category if unique color is no longer an option? Do we require disconnected categories be uniquely assigned a color? Is this a case for interactivity? I've been thinking that it might be worth starting a package for dealing with common issues in plotting single cell data. Largely involving color assignment and overplotting. I think this should be, or at least start out as, separate from scanpy since there are a number of dependencies I think are useful here, which aren't required for scanpy. Plus being able to iterate quickly would be nice. I've started collecting some notebooks on this [here](https://github.com/ivirshup/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:5500,integrability,depend,dependencies,5500,"alues())))). tarashansky_palette = {k: to_hex(colors[v]) for k, v in color_map.items()}. tf.shade(pts, color_key=tarashansky_palette). ```. </details>. ![image](https://user-images.githubusercontent.com/8238804/105131791-c3244980-5b3d-11eb-83cc-2691b392b1c1.png). I think it's good, but there's room for improvement. I was initially worried about this example since the cluster connectivity graph is highly connected, but this seems to have worked out alright. Overplotting (points sitting on top of eachother) is still a problem, but I think that's a separate problem from choosing colors. * Even if they do not touch, can we make it so clusters close to each-other are less likely to get similar colors? This would start becoming more of an optimization problem, and more complicated. * Picking a color palette where all colors are very visible is important. (maybe adding a light border when there's nothing in the background? maybe something similar to ""player model contrast boost"" shaders?). * I think there are some ""spurious"" connections in the graph. Many clusters have >20 neighbors. I think this has to do with outlier points and dispersed points. Not completely sure how to deal with this. Maybe less of an issue with smaller datasets/ leiden clustering? * When categories are disconnected, how do we indicate they're the same category if unique color is no longer an option? Do we require disconnected categories be uniquely assigned a color? Is this a case for interactivity? I've been thinking that it might be worth starting a package for dealing with common issues in plotting single cell data. Largely involving color assignment and overplotting. I think this should be, or at least start out as, separate from scanpy since there are a number of dependencies I think are useful here, which aren't required for scanpy. Plus being able to iterate quickly would be nice. I've started collecting some notebooks on this [here](https://github.com/ivirshup/notebooks/tree/master/plotting).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:1727,interoperability,format,format,1727,"rt matplotlib.pyplot as plt. from natsort import natsorted. import scanpy as sc. import numpy as np. import pandas as pd. import xarray as xr. from sklearn.neighbors import RadiusNeighborsTransformer. # example data. # 1.3 million cells, 38 clusters by louvain. adata = sc.read(""/Users/isaac/data/10x_mouse_13MM_processed.h5ad"", backed=""r""). # Previous colors. louvain_colors_old = dict(. zip(. adata.obs[""louvain""].cat.categories, . adata.uns[""louvain_colors""]. ). ). del adata.uns[""louvain_colors""]. louvain_colors_current = sc.pl._tools.scatterplots._get_palette(adata, values_key=""louvain""). df = sc.get.obs_df(. adata,. [""louvain""],. obsm_keys=[(""X_umap"", 0), (""X_umap"", 1)]. ). # pixel x pixel x n_clusters array (500, 500, 38). # Each value is the count of cells in the cluster at this pixel. pts = (. ds.Canvas(500, 500). .points(df, ""X_umap-0"", ""X_umap-1"", agg=ds.count_cat(""louvain"")). ). def pts_to_coords(pts: xr.DataArray) -> pd.DataFrame:. """""". Turning pixel data into ""long"" sparse format. . One entry for each cluster, for each pixel it would occur in. """""". coords = pd.DataFrame(np.argwhere(np.asarray(pts)), columns=[""x"", ""y"", ""cat""]). coords[""cat""] = pd.Categorical.from_codes(coords[""cat""], categories=list(pts.coords.values())[2]). return coords. def color_nodes(graph: dict) -> dict:. """"""Graph coloring algorithm. . From @atarashansky: https://github.com/theislab/scanpy/issues/1366#issuecomment-763066249. """""". color_map = {}. for node in sorted(graph, key=lambda x: len(graph[x]), reverse=True):. neighbor_colors = set(color_map.get(neigh) for neigh in graph[node]). color_map[node] = next( . color for color in range(len(graph)) if color not in neighbor_colors. ). return color_map. def neighboring_clusters(pts: xr.DataArray) -> dict:. """"""From array of (pixel, pixel, cluster) find which clusters neighbor eachother. """""". graph = {}. coords = pts_to_coords(pts). cat = coords[""cat""]. radius_neighbor = RadiusNeighborsTransformer(metric=""euclidean"", radius=1). radius_neighbo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:5279,modifiability,pac,package,5279,"alues())))). tarashansky_palette = {k: to_hex(colors[v]) for k, v in color_map.items()}. tf.shade(pts, color_key=tarashansky_palette). ```. </details>. ![image](https://user-images.githubusercontent.com/8238804/105131791-c3244980-5b3d-11eb-83cc-2691b392b1c1.png). I think it's good, but there's room for improvement. I was initially worried about this example since the cluster connectivity graph is highly connected, but this seems to have worked out alright. Overplotting (points sitting on top of eachother) is still a problem, but I think that's a separate problem from choosing colors. * Even if they do not touch, can we make it so clusters close to each-other are less likely to get similar colors? This would start becoming more of an optimization problem, and more complicated. * Picking a color palette where all colors are very visible is important. (maybe adding a light border when there's nothing in the background? maybe something similar to ""player model contrast boost"" shaders?). * I think there are some ""spurious"" connections in the graph. Many clusters have >20 neighbors. I think this has to do with outlier points and dispersed points. Not completely sure how to deal with this. Maybe less of an issue with smaller datasets/ leiden clustering? * When categories are disconnected, how do we indicate they're the same category if unique color is no longer an option? Do we require disconnected categories be uniquely assigned a color? Is this a case for interactivity? I've been thinking that it might be worth starting a package for dealing with common issues in plotting single cell data. Largely involving color assignment and overplotting. I think this should be, or at least start out as, separate from scanpy since there are a number of dependencies I think are useful here, which aren't required for scanpy. Plus being able to iterate quickly would be nice. I've started collecting some notebooks on this [here](https://github.com/ivirshup/notebooks/tree/master/plotting).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:5500,modifiability,depend,dependencies,5500,"alues())))). tarashansky_palette = {k: to_hex(colors[v]) for k, v in color_map.items()}. tf.shade(pts, color_key=tarashansky_palette). ```. </details>. ![image](https://user-images.githubusercontent.com/8238804/105131791-c3244980-5b3d-11eb-83cc-2691b392b1c1.png). I think it's good, but there's room for improvement. I was initially worried about this example since the cluster connectivity graph is highly connected, but this seems to have worked out alright. Overplotting (points sitting on top of eachother) is still a problem, but I think that's a separate problem from choosing colors. * Even if they do not touch, can we make it so clusters close to each-other are less likely to get similar colors? This would start becoming more of an optimization problem, and more complicated. * Picking a color palette where all colors are very visible is important. (maybe adding a light border when there's nothing in the background? maybe something similar to ""player model contrast boost"" shaders?). * I think there are some ""spurious"" connections in the graph. Many clusters have >20 neighbors. I think this has to do with outlier points and dispersed points. Not completely sure how to deal with this. Maybe less of an issue with smaller datasets/ leiden clustering? * When categories are disconnected, how do we indicate they're the same category if unique color is no longer an option? Do we require disconnected categories be uniquely assigned a color? Is this a case for interactivity? I've been thinking that it might be worth starting a package for dealing with common issues in plotting single cell data. Largely involving color assignment and overplotting. I think this should be, or at least start out as, separate from scanpy since there are a number of dependencies I think are useful here, which aren't required for scanpy. Plus being able to iterate quickly would be nice. I've started collecting some notebooks on this [here](https://github.com/ivirshup/notebooks/tree/master/plotting).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:4479,performance,optimiz,optimization,4479,"304-dc76c700-5b38-11eb-9be6-3f5613b24037.png). *Additional plot using color generating code from above:*. <details>. <summary> code </summary>. ```python. from matplotlib.colors import to_hex. colors = list(itertools.islice(rgbs(), len(set(color_map.values())))). tarashansky_palette = {k: to_hex(colors[v]) for k, v in color_map.items()}. tf.shade(pts, color_key=tarashansky_palette). ```. </details>. ![image](https://user-images.githubusercontent.com/8238804/105131791-c3244980-5b3d-11eb-83cc-2691b392b1c1.png). I think it's good, but there's room for improvement. I was initially worried about this example since the cluster connectivity graph is highly connected, but this seems to have worked out alright. Overplotting (points sitting on top of eachother) is still a problem, but I think that's a separate problem from choosing colors. * Even if they do not touch, can we make it so clusters close to each-other are less likely to get similar colors? This would start becoming more of an optimization problem, and more complicated. * Picking a color palette where all colors are very visible is important. (maybe adding a light border when there's nothing in the background? maybe something similar to ""player model contrast boost"" shaders?). * I think there are some ""spurious"" connections in the graph. Many clusters have >20 neighbors. I think this has to do with outlier points and dispersed points. Not completely sure how to deal with this. Maybe less of an issue with smaller datasets/ leiden clustering? * When categories are disconnected, how do we indicate they're the same category if unique color is no longer an option? Do we require disconnected categories be uniquely assigned a color? Is this a case for interactivity? I've been thinking that it might be worth starting a package for dealing with common issues in plotting single cell data. Largely involving color assignment and overplotting. I think this should be, or at least start out as, separate from scanpy since there a",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:4510,safety,compl,complicated,4510,"613b24037.png). *Additional plot using color generating code from above:*. <details>. <summary> code </summary>. ```python. from matplotlib.colors import to_hex. colors = list(itertools.islice(rgbs(), len(set(color_map.values())))). tarashansky_palette = {k: to_hex(colors[v]) for k, v in color_map.items()}. tf.shade(pts, color_key=tarashansky_palette). ```. </details>. ![image](https://user-images.githubusercontent.com/8238804/105131791-c3244980-5b3d-11eb-83cc-2691b392b1c1.png). I think it's good, but there's room for improvement. I was initially worried about this example since the cluster connectivity graph is highly connected, but this seems to have worked out alright. Overplotting (points sitting on top of eachother) is still a problem, but I think that's a separate problem from choosing colors. * Even if they do not touch, can we make it so clusters close to each-other are less likely to get similar colors? This would start becoming more of an optimization problem, and more complicated. * Picking a color palette where all colors are very visible is important. (maybe adding a light border when there's nothing in the background? maybe something similar to ""player model contrast boost"" shaders?). * I think there are some ""spurious"" connections in the graph. Many clusters have >20 neighbors. I think this has to do with outlier points and dispersed points. Not completely sure how to deal with this. Maybe less of an issue with smaller datasets/ leiden clustering? * When categories are disconnected, how do we indicate they're the same category if unique color is no longer an option? Do we require disconnected categories be uniquely assigned a color? Is this a case for interactivity? I've been thinking that it might be worth starting a package for dealing with common issues in plotting single cell data. Largely involving color assignment and overplotting. I think this should be, or at least start out as, separate from scanpy since there are a number of dependencies I t",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:4899,safety,compl,completely,4899,"alues())))). tarashansky_palette = {k: to_hex(colors[v]) for k, v in color_map.items()}. tf.shade(pts, color_key=tarashansky_palette). ```. </details>. ![image](https://user-images.githubusercontent.com/8238804/105131791-c3244980-5b3d-11eb-83cc-2691b392b1c1.png). I think it's good, but there's room for improvement. I was initially worried about this example since the cluster connectivity graph is highly connected, but this seems to have worked out alright. Overplotting (points sitting on top of eachother) is still a problem, but I think that's a separate problem from choosing colors. * Even if they do not touch, can we make it so clusters close to each-other are less likely to get similar colors? This would start becoming more of an optimization problem, and more complicated. * Picking a color palette where all colors are very visible is important. (maybe adding a light border when there's nothing in the background? maybe something similar to ""player model contrast boost"" shaders?). * I think there are some ""spurious"" connections in the graph. Many clusters have >20 neighbors. I think this has to do with outlier points and dispersed points. Not completely sure how to deal with this. Maybe less of an issue with smaller datasets/ leiden clustering? * When categories are disconnected, how do we indicate they're the same category if unique color is no longer an option? Do we require disconnected categories be uniquely assigned a color? Is this a case for interactivity? I've been thinking that it might be worth starting a package for dealing with common issues in plotting single cell data. Largely involving color assignment and overplotting. I think this should be, or at least start out as, separate from scanpy since there are a number of dependencies I think are useful here, which aren't required for scanpy. Plus being able to iterate quickly would be nice. I've started collecting some notebooks on this [here](https://github.com/ivirshup/notebooks/tree/master/plotting).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:5500,safety,depend,dependencies,5500,"alues())))). tarashansky_palette = {k: to_hex(colors[v]) for k, v in color_map.items()}. tf.shade(pts, color_key=tarashansky_palette). ```. </details>. ![image](https://user-images.githubusercontent.com/8238804/105131791-c3244980-5b3d-11eb-83cc-2691b392b1c1.png). I think it's good, but there's room for improvement. I was initially worried about this example since the cluster connectivity graph is highly connected, but this seems to have worked out alright. Overplotting (points sitting on top of eachother) is still a problem, but I think that's a separate problem from choosing colors. * Even if they do not touch, can we make it so clusters close to each-other are less likely to get similar colors? This would start becoming more of an optimization problem, and more complicated. * Picking a color palette where all colors are very visible is important. (maybe adding a light border when there's nothing in the background? maybe something similar to ""player model contrast boost"" shaders?). * I think there are some ""spurious"" connections in the graph. Many clusters have >20 neighbors. I think this has to do with outlier points and dispersed points. Not completely sure how to deal with this. Maybe less of an issue with smaller datasets/ leiden clustering? * When categories are disconnected, how do we indicate they're the same category if unique color is no longer an option? Do we require disconnected categories be uniquely assigned a color? Is this a case for interactivity? I've been thinking that it might be worth starting a package for dealing with common issues in plotting single cell data. Largely involving color assignment and overplotting. I think this should be, or at least start out as, separate from scanpy since there are a number of dependencies I think are useful here, which aren't required for scanpy. Plus being able to iterate quickly would be nice. I've started collecting some notebooks on this [here](https://github.com/ivirshup/notebooks/tree/master/plotting).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:4510,security,compl,complicated,4510,"613b24037.png). *Additional plot using color generating code from above:*. <details>. <summary> code </summary>. ```python. from matplotlib.colors import to_hex. colors = list(itertools.islice(rgbs(), len(set(color_map.values())))). tarashansky_palette = {k: to_hex(colors[v]) for k, v in color_map.items()}. tf.shade(pts, color_key=tarashansky_palette). ```. </details>. ![image](https://user-images.githubusercontent.com/8238804/105131791-c3244980-5b3d-11eb-83cc-2691b392b1c1.png). I think it's good, but there's room for improvement. I was initially worried about this example since the cluster connectivity graph is highly connected, but this seems to have worked out alright. Overplotting (points sitting on top of eachother) is still a problem, but I think that's a separate problem from choosing colors. * Even if they do not touch, can we make it so clusters close to each-other are less likely to get similar colors? This would start becoming more of an optimization problem, and more complicated. * Picking a color palette where all colors are very visible is important. (maybe adding a light border when there's nothing in the background? maybe something similar to ""player model contrast boost"" shaders?). * I think there are some ""spurious"" connections in the graph. Many clusters have >20 neighbors. I think this has to do with outlier points and dispersed points. Not completely sure how to deal with this. Maybe less of an issue with smaller datasets/ leiden clustering? * When categories are disconnected, how do we indicate they're the same category if unique color is no longer an option? Do we require disconnected categories be uniquely assigned a color? Is this a case for interactivity? I've been thinking that it might be worth starting a package for dealing with common issues in plotting single cell data. Largely involving color assignment and overplotting. I think this should be, or at least start out as, separate from scanpy since there are a number of dependencies I t",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:4701,security,model,model,4701,"lice(rgbs(), len(set(color_map.values())))). tarashansky_palette = {k: to_hex(colors[v]) for k, v in color_map.items()}. tf.shade(pts, color_key=tarashansky_palette). ```. </details>. ![image](https://user-images.githubusercontent.com/8238804/105131791-c3244980-5b3d-11eb-83cc-2691b392b1c1.png). I think it's good, but there's room for improvement. I was initially worried about this example since the cluster connectivity graph is highly connected, but this seems to have worked out alright. Overplotting (points sitting on top of eachother) is still a problem, but I think that's a separate problem from choosing colors. * Even if they do not touch, can we make it so clusters close to each-other are less likely to get similar colors? This would start becoming more of an optimization problem, and more complicated. * Picking a color palette where all colors are very visible is important. (maybe adding a light border when there's nothing in the background? maybe something similar to ""player model contrast boost"" shaders?). * I think there are some ""spurious"" connections in the graph. Many clusters have >20 neighbors. I think this has to do with outlier points and dispersed points. Not completely sure how to deal with this. Maybe less of an issue with smaller datasets/ leiden clustering? * When categories are disconnected, how do we indicate they're the same category if unique color is no longer an option? Do we require disconnected categories be uniquely assigned a color? Is this a case for interactivity? I've been thinking that it might be worth starting a package for dealing with common issues in plotting single cell data. Largely involving color assignment and overplotting. I think this should be, or at least start out as, separate from scanpy since there are a number of dependencies I think are useful here, which aren't required for scanpy. Plus being able to iterate quickly would be nice. I've started collecting some notebooks on this [here](https://github.com/ivirshup/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:4899,security,compl,completely,4899,"alues())))). tarashansky_palette = {k: to_hex(colors[v]) for k, v in color_map.items()}. tf.shade(pts, color_key=tarashansky_palette). ```. </details>. ![image](https://user-images.githubusercontent.com/8238804/105131791-c3244980-5b3d-11eb-83cc-2691b392b1c1.png). I think it's good, but there's room for improvement. I was initially worried about this example since the cluster connectivity graph is highly connected, but this seems to have worked out alright. Overplotting (points sitting on top of eachother) is still a problem, but I think that's a separate problem from choosing colors. * Even if they do not touch, can we make it so clusters close to each-other are less likely to get similar colors? This would start becoming more of an optimization problem, and more complicated. * Picking a color palette where all colors are very visible is important. (maybe adding a light border when there's nothing in the background? maybe something similar to ""player model contrast boost"" shaders?). * I think there are some ""spurious"" connections in the graph. Many clusters have >20 neighbors. I think this has to do with outlier points and dispersed points. Not completely sure how to deal with this. Maybe less of an issue with smaller datasets/ leiden clustering? * When categories are disconnected, how do we indicate they're the same category if unique color is no longer an option? Do we require disconnected categories be uniquely assigned a color? Is this a case for interactivity? I've been thinking that it might be worth starting a package for dealing with common issues in plotting single cell data. Largely involving color assignment and overplotting. I think this should be, or at least start out as, separate from scanpy since there are a number of dependencies I think are useful here, which aren't required for scanpy. Plus being able to iterate quickly would be nice. I've started collecting some notebooks on this [here](https://github.com/ivirshup/notebooks/tree/master/plotting).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:5500,testability,depend,dependencies,5500,"alues())))). tarashansky_palette = {k: to_hex(colors[v]) for k, v in color_map.items()}. tf.shade(pts, color_key=tarashansky_palette). ```. </details>. ![image](https://user-images.githubusercontent.com/8238804/105131791-c3244980-5b3d-11eb-83cc-2691b392b1c1.png). I think it's good, but there's room for improvement. I was initially worried about this example since the cluster connectivity graph is highly connected, but this seems to have worked out alright. Overplotting (points sitting on top of eachother) is still a problem, but I think that's a separate problem from choosing colors. * Even if they do not touch, can we make it so clusters close to each-other are less likely to get similar colors? This would start becoming more of an optimization problem, and more complicated. * Picking a color palette where all colors are very visible is important. (maybe adding a light border when there's nothing in the background? maybe something similar to ""player model contrast boost"" shaders?). * I think there are some ""spurious"" connections in the graph. Many clusters have >20 neighbors. I think this has to do with outlier points and dispersed points. Not completely sure how to deal with this. Maybe less of an issue with smaller datasets/ leiden clustering? * When categories are disconnected, how do we indicate they're the same category if unique color is no longer an option? Do we require disconnected categories be uniquely assigned a color? Is this a case for interactivity? I've been thinking that it might be worth starting a package for dealing with common issues in plotting single cell data. Largely involving color assignment and overplotting. I think this should be, or at least start out as, separate from scanpy since there are a number of dependencies I think are useful here, which aren't required for scanpy. Plus being able to iterate quickly would be nice. I've started collecting some notebooks on this [here](https://github.com/ivirshup/notebooks/tree/master/plotting).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:482,usability,effectiv,effective,482,"@atarashansky, that's great! I've built on this a bit, with some points about graph construction. * The problems isn't really nearest K neighbors, it's radius neighbors. * Embedding position is relevant, but I think position in pixel space is more important in this case. You could imagine one dimension of your embedding plot having a range of (0, 100) while the other has (0, 1), but displayed on a square plot. Euclidean distance for neighboring points would not be particularly effective here. To that end, here's some code for building the cluster connectivity graph from pixel space:. <details>. <summary> Code: </summary>. ```python. # imports. import datashader as ds. from datashader import transfer_functions as tf. import matplotlib.pyplot as plt. from natsort import natsorted. import scanpy as sc. import numpy as np. import pandas as pd. import xarray as xr. from sklearn.neighbors import RadiusNeighborsTransformer. # example data. # 1.3 million cells, 38 clusters by louvain. adata = sc.read(""/Users/isaac/data/10x_mouse_13MM_processed.h5ad"", backed=""r""). # Previous colors. louvain_colors_old = dict(. zip(. adata.obs[""louvain""].cat.categories, . adata.uns[""louvain_colors""]. ). ). del adata.uns[""louvain_colors""]. louvain_colors_current = sc.pl._tools.scatterplots._get_palette(adata, values_key=""louvain""). df = sc.get.obs_df(. adata,. [""louvain""],. obsm_keys=[(""X_umap"", 0), (""X_umap"", 1)]. ). # pixel x pixel x n_clusters array (500, 500, 38). # Each value is the count of cells in the cluster at this pixel. pts = (. ds.Canvas(500, 500). .points(df, ""X_umap-0"", ""X_umap-1"", agg=ds.count_cat(""louvain"")). ). def pts_to_coords(pts: xr.DataArray) -> pd.DataFrame:. """""". Turning pixel data into ""long"" sparse format. . One entry for each cluster, for each pixel it would occur in. """""". coords = pd.DataFrame(np.argwhere(np.asarray(pts)), columns=[""x"", ""y"", ""cat""]). coords[""cat""] = pd.Categorical.from_codes(coords[""cat""], categories=list(pts.coords.values())[2]). return coords. de",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:1010,usability,User,Users,1010,", that's great! I've built on this a bit, with some points about graph construction. * The problems isn't really nearest K neighbors, it's radius neighbors. * Embedding position is relevant, but I think position in pixel space is more important in this case. You could imagine one dimension of your embedding plot having a range of (0, 100) while the other has (0, 1), but displayed on a square plot. Euclidean distance for neighboring points would not be particularly effective here. To that end, here's some code for building the cluster connectivity graph from pixel space:. <details>. <summary> Code: </summary>. ```python. # imports. import datashader as ds. from datashader import transfer_functions as tf. import matplotlib.pyplot as plt. from natsort import natsorted. import scanpy as sc. import numpy as np. import pandas as pd. import xarray as xr. from sklearn.neighbors import RadiusNeighborsTransformer. # example data. # 1.3 million cells, 38 clusters by louvain. adata = sc.read(""/Users/isaac/data/10x_mouse_13MM_processed.h5ad"", backed=""r""). # Previous colors. louvain_colors_old = dict(. zip(. adata.obs[""louvain""].cat.categories, . adata.uns[""louvain_colors""]. ). ). del adata.uns[""louvain_colors""]. louvain_colors_current = sc.pl._tools.scatterplots._get_palette(adata, values_key=""louvain""). df = sc.get.obs_df(. adata,. [""louvain""],. obsm_keys=[(""X_umap"", 0), (""X_umap"", 1)]. ). # pixel x pixel x n_clusters array (500, 500, 38). # Each value is the count of cells in the cluster at this pixel. pts = (. ds.Canvas(500, 500). .points(df, ""X_umap-0"", ""X_umap-1"", agg=ds.count_cat(""louvain"")). ). def pts_to_coords(pts: xr.DataArray) -> pd.DataFrame:. """""". Turning pixel data into ""long"" sparse format. . One entry for each cluster, for each pixel it would occur in. """""". coords = pd.DataFrame(np.argwhere(np.asarray(pts)), columns=[""x"", ""y"", ""cat""]). coords[""cat""] = pd.Categorical.from_codes(coords[""cat""], categories=list(pts.coords.values())[2]). return coords. def color_nodes",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:3437,usability,user,user-images,3437,"eighboring_clusters(pts: xr.DataArray) -> dict:. """"""From array of (pixel, pixel, cluster) find which clusters neighbor eachother. """""". graph = {}. coords = pts_to_coords(pts). cat = coords[""cat""]. radius_neighbor = RadiusNeighborsTransformer(metric=""euclidean"", radius=1). radius_neighbor.fit(coords.values[:, :2]). g = radius_neighbor.radius_neighbors_graph(). for k, v in coords.groupby(""cat"").indices.items():. neighbors = np.unique(g[v].indices). graph[k] = natsorted(pd.unique(cat[neighbors])). return graph. graph = neighboring_clusters(pts). palette = {k: sc.pl.palettes.default_28[v] for k, v in color_nodes(graph).items()}. ```. </details>. Results:. ```python. tf.Images(. tf.shade(pts, color_key=louvain_colors_current, name=""Current scanpy coloring (38 unique colors out of 100)""),. tf.shade(pts, color_key=palette, name=""Graph coloring (25 unique colors)""),. tf.shade(pts, color_key=louvain_colors_old, name=""Old scanpy coloring (20 colors w/ repeats)""),. ). ```. ![image](https://user-images.githubusercontent.com/8238804/105129304-dc76c700-5b38-11eb-9be6-3f5613b24037.png). *Additional plot using color generating code from above:*. <details>. <summary> code </summary>. ```python. from matplotlib.colors import to_hex. colors = list(itertools.islice(rgbs(), len(set(color_map.values())))). tarashansky_palette = {k: to_hex(colors[v]) for k, v in color_map.items()}. tf.shade(pts, color_key=tarashansky_palette). ```. </details>. ![image](https://user-images.githubusercontent.com/8238804/105131791-c3244980-5b3d-11eb-83cc-2691b392b1c1.png). I think it's good, but there's room for improvement. I was initially worried about this example since the cluster connectivity graph is highly connected, but this seems to have worked out alright. Overplotting (points sitting on top of eachother) is still a problem, but I think that's a separate problem from choosing colors. * Even if they do not touch, can we make it so clusters close to each-other are less likely to get similar colors? T",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:3905,usability,user,user-images,3905,"ted(pd.unique(cat[neighbors])). return graph. graph = neighboring_clusters(pts). palette = {k: sc.pl.palettes.default_28[v] for k, v in color_nodes(graph).items()}. ```. </details>. Results:. ```python. tf.Images(. tf.shade(pts, color_key=louvain_colors_current, name=""Current scanpy coloring (38 unique colors out of 100)""),. tf.shade(pts, color_key=palette, name=""Graph coloring (25 unique colors)""),. tf.shade(pts, color_key=louvain_colors_old, name=""Old scanpy coloring (20 colors w/ repeats)""),. ). ```. ![image](https://user-images.githubusercontent.com/8238804/105129304-dc76c700-5b38-11eb-9be6-3f5613b24037.png). *Additional plot using color generating code from above:*. <details>. <summary> code </summary>. ```python. from matplotlib.colors import to_hex. colors = list(itertools.islice(rgbs(), len(set(color_map.values())))). tarashansky_palette = {k: to_hex(colors[v]) for k, v in color_map.items()}. tf.shade(pts, color_key=tarashansky_palette). ```. </details>. ![image](https://user-images.githubusercontent.com/8238804/105131791-c3244980-5b3d-11eb-83cc-2691b392b1c1.png). I think it's good, but there's room for improvement. I was initially worried about this example since the cluster connectivity graph is highly connected, but this seems to have worked out alright. Overplotting (points sitting on top of eachother) is still a problem, but I think that's a separate problem from choosing colors. * Even if they do not touch, can we make it so clusters close to each-other are less likely to get similar colors? This would start becoming more of an optimization problem, and more complicated. * Picking a color palette where all colors are very visible is important. (maybe adding a light border when there's nothing in the background? maybe something similar to ""player model contrast boost"" shaders?). * I think there are some ""spurious"" connections in the graph. Many clusters have >20 neighbors. I think this has to do with outlier points and dispersed points. Not completely s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:4383,usability,close,close,4383,"20 colors w/ repeats)""),. ). ```. ![image](https://user-images.githubusercontent.com/8238804/105129304-dc76c700-5b38-11eb-9be6-3f5613b24037.png). *Additional plot using color generating code from above:*. <details>. <summary> code </summary>. ```python. from matplotlib.colors import to_hex. colors = list(itertools.islice(rgbs(), len(set(color_map.values())))). tarashansky_palette = {k: to_hex(colors[v]) for k, v in color_map.items()}. tf.shade(pts, color_key=tarashansky_palette). ```. </details>. ![image](https://user-images.githubusercontent.com/8238804/105131791-c3244980-5b3d-11eb-83cc-2691b392b1c1.png). I think it's good, but there's room for improvement. I was initially worried about this example since the cluster connectivity graph is highly connected, but this seems to have worked out alright. Overplotting (points sitting on top of eachother) is still a problem, but I think that's a separate problem from choosing colors. * Even if they do not touch, can we make it so clusters close to each-other are less likely to get similar colors? This would start becoming more of an optimization problem, and more complicated. * Picking a color palette where all colors are very visible is important. (maybe adding a light border when there's nothing in the background? maybe something similar to ""player model contrast boost"" shaders?). * I think there are some ""spurious"" connections in the graph. Many clusters have >20 neighbors. I think this has to do with outlier points and dispersed points. Not completely sure how to deal with this. Maybe less of an issue with smaller datasets/ leiden clustering? * When categories are disconnected, how do we indicate they're the same category if unique color is no longer an option? Do we require disconnected categories be uniquely assigned a color? Is this a case for interactivity? I've been thinking that it might be worth starting a package for dealing with common issues in plotting single cell data. Largely involving color assignment and",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:5049,usability,indicat,indicate,5049,"alues())))). tarashansky_palette = {k: to_hex(colors[v]) for k, v in color_map.items()}. tf.shade(pts, color_key=tarashansky_palette). ```. </details>. ![image](https://user-images.githubusercontent.com/8238804/105131791-c3244980-5b3d-11eb-83cc-2691b392b1c1.png). I think it's good, but there's room for improvement. I was initially worried about this example since the cluster connectivity graph is highly connected, but this seems to have worked out alright. Overplotting (points sitting on top of eachother) is still a problem, but I think that's a separate problem from choosing colors. * Even if they do not touch, can we make it so clusters close to each-other are less likely to get similar colors? This would start becoming more of an optimization problem, and more complicated. * Picking a color palette where all colors are very visible is important. (maybe adding a light border when there's nothing in the background? maybe something similar to ""player model contrast boost"" shaders?). * I think there are some ""spurious"" connections in the graph. Many clusters have >20 neighbors. I think this has to do with outlier points and dispersed points. Not completely sure how to deal with this. Maybe less of an issue with smaller datasets/ leiden clustering? * When categories are disconnected, how do we indicate they're the same category if unique color is no longer an option? Do we require disconnected categories be uniquely assigned a color? Is this a case for interactivity? I've been thinking that it might be worth starting a package for dealing with common issues in plotting single cell data. Largely involving color assignment and overplotting. I think this should be, or at least start out as, separate from scanpy since there are a number of dependencies I think are useful here, which aren't required for scanpy. Plus being able to iterate quickly would be nice. I've started collecting some notebooks on this [here](https://github.com/ivirshup/notebooks/tree/master/plotting).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:5211,usability,interact,interactivity,5211,"alues())))). tarashansky_palette = {k: to_hex(colors[v]) for k, v in color_map.items()}. tf.shade(pts, color_key=tarashansky_palette). ```. </details>. ![image](https://user-images.githubusercontent.com/8238804/105131791-c3244980-5b3d-11eb-83cc-2691b392b1c1.png). I think it's good, but there's room for improvement. I was initially worried about this example since the cluster connectivity graph is highly connected, but this seems to have worked out alright. Overplotting (points sitting on top of eachother) is still a problem, but I think that's a separate problem from choosing colors. * Even if they do not touch, can we make it so clusters close to each-other are less likely to get similar colors? This would start becoming more of an optimization problem, and more complicated. * Picking a color palette where all colors are very visible is important. (maybe adding a light border when there's nothing in the background? maybe something similar to ""player model contrast boost"" shaders?). * I think there are some ""spurious"" connections in the graph. Many clusters have >20 neighbors. I think this has to do with outlier points and dispersed points. Not completely sure how to deal with this. Maybe less of an issue with smaller datasets/ leiden clustering? * When categories are disconnected, how do we indicate they're the same category if unique color is no longer an option? Do we require disconnected categories be uniquely assigned a color? Is this a case for interactivity? I've been thinking that it might be worth starting a package for dealing with common issues in plotting single cell data. Largely involving color assignment and overplotting. I think this should be, or at least start out as, separate from scanpy since there are a number of dependencies I think are useful here, which aren't required for scanpy. Plus being able to iterate quickly would be nice. I've started collecting some notebooks on this [here](https://github.com/ivirshup/notebooks/tree/master/plotting).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:49,availability,cluster,clusters,49,"I tried your codes. It nicely separated neighbor clusters. However, it seemed that some colors have been used multiple times while other colors were less favored. . Is that true? ![image](https://user-images.githubusercontent.com/4110443/106396307-f947b000-63fe-11eb-92ba-eef16c69aa7c.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:49,deployability,cluster,clusters,49,"I tried your codes. It nicely separated neighbor clusters. However, it seemed that some colors have been used multiple times while other colors were less favored. . Is that true? ![image](https://user-images.githubusercontent.com/4110443/106396307-f947b000-63fe-11eb-92ba-eef16c69aa7c.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:119,performance,time,times,119,"I tried your codes. It nicely separated neighbor clusters. However, it seemed that some colors have been used multiple times while other colors were less favored. . Is that true? ![image](https://user-images.githubusercontent.com/4110443/106396307-f947b000-63fe-11eb-92ba-eef16c69aa7c.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:196,usability,user,user-images,196,"I tried your codes. It nicely separated neighbor clusters. However, it seemed that some colors have been used multiple times while other colors were less favored. . Is that true? ![image](https://user-images.githubusercontent.com/4110443/106396307-f947b000-63fe-11eb-92ba-eef16c69aa7c.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:645,deployability,fail,fail,645,"Yeah, looking at the code, the color used will just be the first item in the color list that hasn't been assigned to a neighbor yet. Maybe you could replace color selection to pick from a queue prioritizing less used colors? This would be less likely to converge to a minimal number of colors though. Might need to consider a different algorithm for this property. <details>. <summary> Example </summary>. ```. A. /. C - B. ```. Lets say we have two colors [""red"", ""blue""] and we want to assign them. We iterate in order [A, B, C]. * If we don't use the priority queue, we'd end up with `{""A"": ""red"", ""B"": ""red"", ""C"": ""blue""}`. * If we do, we'd fail to converge after ending up at {""A"": ""red"", ""B"": ""blue""}`. </details>. Do you think you could share some part of the object you're plotting? I think it would make for a good use case to play around with plotting.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:188,integrability,queue,queue,188,"Yeah, looking at the code, the color used will just be the first item in the color list that hasn't been assigned to a neighbor yet. Maybe you could replace color selection to pick from a queue prioritizing less used colors? This would be less likely to converge to a minimal number of colors though. Might need to consider a different algorithm for this property. <details>. <summary> Example </summary>. ```. A. /. C - B. ```. Lets say we have two colors [""red"", ""blue""] and we want to assign them. We iterate in order [A, B, C]. * If we don't use the priority queue, we'd end up with `{""A"": ""red"", ""B"": ""red"", ""C"": ""blue""}`. * If we do, we'd fail to converge after ending up at {""A"": ""red"", ""B"": ""blue""}`. </details>. Do you think you could share some part of the object you're plotting? I think it would make for a good use case to play around with plotting.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:563,integrability,queue,queue,563,"Yeah, looking at the code, the color used will just be the first item in the color list that hasn't been assigned to a neighbor yet. Maybe you could replace color selection to pick from a queue prioritizing less used colors? This would be less likely to converge to a minimal number of colors though. Might need to consider a different algorithm for this property. <details>. <summary> Example </summary>. ```. A. /. C - B. ```. Lets say we have two colors [""red"", ""blue""] and we want to assign them. We iterate in order [A, B, C]. * If we don't use the priority queue, we'd end up with `{""A"": ""red"", ""B"": ""red"", ""C"": ""blue""}`. * If we do, we'd fail to converge after ending up at {""A"": ""red"", ""B"": ""blue""}`. </details>. Do you think you could share some part of the object you're plotting? I think it would make for a good use case to play around with plotting.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:744,interoperability,share,share,744,"Yeah, looking at the code, the color used will just be the first item in the color list that hasn't been assigned to a neighbor yet. Maybe you could replace color selection to pick from a queue prioritizing less used colors? This would be less likely to converge to a minimal number of colors though. Might need to consider a different algorithm for this property. <details>. <summary> Example </summary>. ```. A. /. C - B. ```. Lets say we have two colors [""red"", ""blue""] and we want to assign them. We iterate in order [A, B, C]. * If we don't use the priority queue, we'd end up with `{""A"": ""red"", ""B"": ""red"", ""C"": ""blue""}`. * If we do, we'd fail to converge after ending up at {""A"": ""red"", ""B"": ""blue""}`. </details>. Do you think you could share some part of the object you're plotting? I think it would make for a good use case to play around with plotting.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:188,performance,queue,queue,188,"Yeah, looking at the code, the color used will just be the first item in the color list that hasn't been assigned to a neighbor yet. Maybe you could replace color selection to pick from a queue prioritizing less used colors? This would be less likely to converge to a minimal number of colors though. Might need to consider a different algorithm for this property. <details>. <summary> Example </summary>. ```. A. /. C - B. ```. Lets say we have two colors [""red"", ""blue""] and we want to assign them. We iterate in order [A, B, C]. * If we don't use the priority queue, we'd end up with `{""A"": ""red"", ""B"": ""red"", ""C"": ""blue""}`. * If we do, we'd fail to converge after ending up at {""A"": ""red"", ""B"": ""blue""}`. </details>. Do you think you could share some part of the object you're plotting? I think it would make for a good use case to play around with plotting.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:563,performance,queue,queue,563,"Yeah, looking at the code, the color used will just be the first item in the color list that hasn't been assigned to a neighbor yet. Maybe you could replace color selection to pick from a queue prioritizing less used colors? This would be less likely to converge to a minimal number of colors though. Might need to consider a different algorithm for this property. <details>. <summary> Example </summary>. ```. A. /. C - B. ```. Lets say we have two colors [""red"", ""blue""] and we want to assign them. We iterate in order [A, B, C]. * If we don't use the priority queue, we'd end up with `{""A"": ""red"", ""B"": ""red"", ""C"": ""blue""}`. * If we do, we'd fail to converge after ending up at {""A"": ""red"", ""B"": ""blue""}`. </details>. Do you think you could share some part of the object you're plotting? I think it would make for a good use case to play around with plotting.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:645,reliability,fail,fail,645,"Yeah, looking at the code, the color used will just be the first item in the color list that hasn't been assigned to a neighbor yet. Maybe you could replace color selection to pick from a queue prioritizing less used colors? This would be less likely to converge to a minimal number of colors though. Might need to consider a different algorithm for this property. <details>. <summary> Example </summary>. ```. A. /. C - B. ```. Lets say we have two colors [""red"", ""blue""] and we want to assign them. We iterate in order [A, B, C]. * If we don't use the priority queue, we'd end up with `{""A"": ""red"", ""B"": ""red"", ""C"": ""blue""}`. * If we do, we'd fail to converge after ending up at {""A"": ""red"", ""B"": ""blue""}`. </details>. Do you think you could share some part of the object you're plotting? I think it would make for a good use case to play around with plotting.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1366:268,usability,minim,minimal,268,"Yeah, looking at the code, the color used will just be the first item in the color list that hasn't been assigned to a neighbor yet. Maybe you could replace color selection to pick from a queue prioritizing less used colors? This would be less likely to converge to a minimal number of colors though. Might need to consider a different algorithm for this property. <details>. <summary> Example </summary>. ```. A. /. C - B. ```. Lets say we have two colors [""red"", ""blue""] and we want to assign them. We iterate in order [A, B, C]. * If we don't use the priority queue, we'd end up with `{""A"": ""red"", ""B"": ""red"", ""C"": ""blue""}`. * If we do, we'd fail to converge after ending up at {""A"": ""red"", ""B"": ""blue""}`. </details>. Do you think you could share some part of the object you're plotting? I think it would make for a good use case to play around with plotting.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366
https://github.com/scverse/scanpy/issues/1367:163,availability,error,error,163,Hi. I cant copy/paste/run the above code sample. Theres nowhere you define `adata`. Please add some code that uses some builtin dataset or so and reproduces the error.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1367
https://github.com/scverse/scanpy/issues/1367:163,performance,error,error,163,Hi. I cant copy/paste/run the above code sample. Theres nowhere you define `adata`. Please add some code that uses some builtin dataset or so and reproduces the error.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1367
https://github.com/scverse/scanpy/issues/1367:163,safety,error,error,163,Hi. I cant copy/paste/run the above code sample. Theres nowhere you define `adata`. Please add some code that uses some builtin dataset or so and reproduces the error.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1367
https://github.com/scverse/scanpy/issues/1367:163,usability,error,error,163,Hi. I cant copy/paste/run the above code sample. Theres nowhere you define `adata`. Please add some code that uses some builtin dataset or so and reproduces the error.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1367
https://github.com/scverse/scanpy/issues/1367:218,availability,error,error,218,"Hi, . That was from my own datasets, but I also used the data from here, https://github.com/theislab/single-cell-tutorial/blob/master/latest_notebook/Case-study_Mouse-intestinal-epithelium_1906.ipynb, and got the same error. ```. adata_mnn = adata.copy(). adata_list = [adata_mnn[adata_mnn.obs['sample'] == i] for i in adata_mnn.obs['sample'].unique()]. adata_list. ```. ```. [View of AnnData object with n_obs  n_vars = 2267  12818. obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts',. View of AnnData object with n_obs  n_vars = 1976  12818. obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts',. View of AnnData object with n_obs  n_vars = 1663  12818. obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts',. View of AnnData object with n_obs  n_vars = 2356  12818. obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts',. View of AnnData object with n_obs  n_vars = 2422  12818. obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts',. View of AnnData object with n_obs  n_vars = 1773  12818. obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts']. ```. ```. import mnnpy. corrected = mnnpy.mnn_correct(*adata_list, batch_key=""sample""). ```. ```. ---------------------------------------------------------------------------. IndexError Traceback (most recent call last). <ipython-input-35-7ad830fcd907> in <module>. 1 import mnnpy. ----> 2 corrected = mnnpy.mnn_correct(*",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1367
https://github.com/scverse/scanpy/issues/1367:1936,deployability,modul,module,1936,", 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts',. View of AnnData object with n_obs  n_vars = 2356  12818. obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts',. View of AnnData object with n_obs  n_vars = 2422  12818. obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts',. View of AnnData object with n_obs  n_vars = 1773  12818. obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts']. ```. ```. import mnnpy. corrected = mnnpy.mnn_correct(*adata_list, batch_key=""sample""). ```. ```. ---------------------------------------------------------------------------. IndexError Traceback (most recent call last). <ipython-input-35-7ad830fcd907> in <module>. 1 import mnnpy. ----> 2 corrected = mnnpy.mnn_correct(*adata_list, batch_key=""sample""). /Anaconda_python3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 124 cos_norm_out=cos_norm_out, svd_dim=svd_dim, var_adj=var_adj,. 125 compute_angle=compute_angle, mnn_order=mnn_order,. --> 126 svd_mode=svd_mode, do_concatenate=do_concatenate, **kwargs). 127 print('Packing AnnData object...'). 128 if do_concatenate:. /Anaconda_python3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 180 print(' Computing correction vectors...'). 181 correction_in = compute_correctio",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1367
https://github.com/scverse/scanpy/issues/1367:575,modifiability,layer,layers,575,"Hi, . That was from my own datasets, but I also used the data from here, https://github.com/theislab/single-cell-tutorial/blob/master/latest_notebook/Case-study_Mouse-intestinal-epithelium_1906.ipynb, and got the same error. ```. adata_mnn = adata.copy(). adata_list = [adata_mnn[adata_mnn.obs['sample'] == i] for i in adata_mnn.obs['sample'].unique()]. adata_list. ```. ```. [View of AnnData object with n_obs  n_vars = 2267  12818. obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts',. View of AnnData object with n_obs  n_vars = 1976  12818. obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts',. View of AnnData object with n_obs  n_vars = 1663  12818. obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts',. View of AnnData object with n_obs  n_vars = 2356  12818. obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts',. View of AnnData object with n_obs  n_vars = 2422  12818. obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts',. View of AnnData object with n_obs  n_vars = 1773  12818. obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts']. ```. ```. import mnnpy. corrected = mnnpy.mnn_correct(*adata_list, batch_key=""sample""). ```. ```. ---------------------------------------------------------------------------. IndexError Traceback (most recent call last). <ipython-input-35-7ad830fcd907> in <module>. 1 import mnnpy. ----> 2 corrected = mnnpy.mnn_correct(*",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1367
https://github.com/scverse/scanpy/issues/1367:792,modifiability,layer,layers,792,"Hi, . That was from my own datasets, but I also used the data from here, https://github.com/theislab/single-cell-tutorial/blob/master/latest_notebook/Case-study_Mouse-intestinal-epithelium_1906.ipynb, and got the same error. ```. adata_mnn = adata.copy(). adata_list = [adata_mnn[adata_mnn.obs['sample'] == i] for i in adata_mnn.obs['sample'].unique()]. adata_list. ```. ```. [View of AnnData object with n_obs  n_vars = 2267  12818. obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts',. View of AnnData object with n_obs  n_vars = 1976  12818. obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts',. View of AnnData object with n_obs  n_vars = 1663  12818. obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts',. View of AnnData object with n_obs  n_vars = 2356  12818. obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts',. View of AnnData object with n_obs  n_vars = 2422  12818. obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts',. View of AnnData object with n_obs  n_vars = 1773  12818. obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts']. ```. ```. import mnnpy. corrected = mnnpy.mnn_correct(*adata_list, batch_key=""sample""). ```. ```. ---------------------------------------------------------------------------. IndexError Traceback (most recent call last). <ipython-input-35-7ad830fcd907> in <module>. 1 import mnnpy. ----> 2 corrected = mnnpy.mnn_correct(*",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1367
https://github.com/scverse/scanpy/issues/1367:1009,modifiability,layer,layers,1009,"as from my own datasets, but I also used the data from here, https://github.com/theislab/single-cell-tutorial/blob/master/latest_notebook/Case-study_Mouse-intestinal-epithelium_1906.ipynb, and got the same error. ```. adata_mnn = adata.copy(). adata_list = [adata_mnn[adata_mnn.obs['sample'] == i] for i in adata_mnn.obs['sample'].unique()]. adata_list. ```. ```. [View of AnnData object with n_obs  n_vars = 2267  12818. obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts',. View of AnnData object with n_obs  n_vars = 1976  12818. obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts',. View of AnnData object with n_obs  n_vars = 1663  12818. obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts',. View of AnnData object with n_obs  n_vars = 2356  12818. obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts',. View of AnnData object with n_obs  n_vars = 2422  12818. obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts',. View of AnnData object with n_obs  n_vars = 1773  12818. obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts']. ```. ```. import mnnpy. corrected = mnnpy.mnn_correct(*adata_list, batch_key=""sample""). ```. ```. ---------------------------------------------------------------------------. IndexError Traceback (most recent call last). <ipython-input-35-7ad830fcd907> in <module>. 1 import mnnpy. ----> 2 corrected = mnnpy.mnn_correct(*adata_list, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1367
https://github.com/scverse/scanpy/issues/1367:1226,modifiability,layer,layers,1226," adata_mnn = adata.copy(). adata_list = [adata_mnn[adata_mnn.obs['sample'] == i] for i in adata_mnn.obs['sample'].unique()]. adata_list. ```. ```. [View of AnnData object with n_obs  n_vars = 2267  12818. obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts',. View of AnnData object with n_obs  n_vars = 1976  12818. obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts',. View of AnnData object with n_obs  n_vars = 1663  12818. obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts',. View of AnnData object with n_obs  n_vars = 2356  12818. obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts',. View of AnnData object with n_obs  n_vars = 2422  12818. obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts',. View of AnnData object with n_obs  n_vars = 1773  12818. obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts']. ```. ```. import mnnpy. corrected = mnnpy.mnn_correct(*adata_list, batch_key=""sample""). ```. ```. ---------------------------------------------------------------------------. IndexError Traceback (most recent call last). <ipython-input-35-7ad830fcd907> in <module>. 1 import mnnpy. ----> 2 corrected = mnnpy.mnn_correct(*adata_list, batch_key=""sample""). /Anaconda_python3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, c",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1367
https://github.com/scverse/scanpy/issues/1367:1443,modifiability,layer,layers,1443,"le', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts',. View of AnnData object with n_obs  n_vars = 1976  12818. obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts',. View of AnnData object with n_obs  n_vars = 1663  12818. obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts',. View of AnnData object with n_obs  n_vars = 2356  12818. obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts',. View of AnnData object with n_obs  n_vars = 2422  12818. obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts',. View of AnnData object with n_obs  n_vars = 1773  12818. obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts']. ```. ```. import mnnpy. corrected = mnnpy.mnn_correct(*adata_list, batch_key=""sample""). ```. ```. ---------------------------------------------------------------------------. IndexError Traceback (most recent call last). <ipython-input-35-7ad830fcd907> in <module>. 1 import mnnpy. ----> 2 corrected = mnnpy.mnn_correct(*adata_list, batch_key=""sample""). /Anaconda_python3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 124 cos_norm_out=cos_norm_out, svd_dim=svd_dim, var_adj=var_adj,. 125 compute_angle=compute_angle, mnn_order=mnn_order,. --> 126 ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1367
https://github.com/scverse/scanpy/issues/1367:1660,modifiability,layer,layers,1660,"le', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts',. View of AnnData object with n_obs  n_vars = 1663  12818. obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts',. View of AnnData object with n_obs  n_vars = 2356  12818. obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts',. View of AnnData object with n_obs  n_vars = 2422  12818. obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts',. View of AnnData object with n_obs  n_vars = 1773  12818. obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts']. ```. ```. import mnnpy. corrected = mnnpy.mnn_correct(*adata_list, batch_key=""sample""). ```. ```. ---------------------------------------------------------------------------. IndexError Traceback (most recent call last). <ipython-input-35-7ad830fcd907> in <module>. 1 import mnnpy. ----> 2 corrected = mnnpy.mnn_correct(*adata_list, batch_key=""sample""). /Anaconda_python3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 124 cos_norm_out=cos_norm_out, svd_dim=svd_dim, var_adj=var_adj,. 125 compute_angle=compute_angle, mnn_order=mnn_order,. --> 126 svd_mode=svd_mode, do_concatenate=do_concatenate, **kwargs). 127 print('Packing AnnData object...'). 128 if do_concatenate:. /Anaconda_python3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_sub",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1367
https://github.com/scverse/scanpy/issues/1367:1936,modifiability,modul,module,1936,", 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts',. View of AnnData object with n_obs  n_vars = 2356  12818. obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts',. View of AnnData object with n_obs  n_vars = 2422  12818. obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts',. View of AnnData object with n_obs  n_vars = 1773  12818. obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts']. ```. ```. import mnnpy. corrected = mnnpy.mnn_correct(*adata_list, batch_key=""sample""). ```. ```. ---------------------------------------------------------------------------. IndexError Traceback (most recent call last). <ipython-input-35-7ad830fcd907> in <module>. 1 import mnnpy. ----> 2 corrected = mnnpy.mnn_correct(*adata_list, batch_key=""sample""). /Anaconda_python3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 124 cos_norm_out=cos_norm_out, svd_dim=svd_dim, var_adj=var_adj,. 125 compute_angle=compute_angle, mnn_order=mnn_order,. --> 126 svd_mode=svd_mode, do_concatenate=do_concatenate, **kwargs). 127 print('Packing AnnData object...'). 128 if do_concatenate:. /Anaconda_python3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 180 print(' Computing correction vectors...'). 181 correction_in = compute_correctio",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1367
https://github.com/scverse/scanpy/issues/1367:2070,modifiability,pac,packages,2070,"356  12818. obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts',. View of AnnData object with n_obs  n_vars = 2422  12818. obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts',. View of AnnData object with n_obs  n_vars = 1773  12818. obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts']. ```. ```. import mnnpy. corrected = mnnpy.mnn_correct(*adata_list, batch_key=""sample""). ```. ```. ---------------------------------------------------------------------------. IndexError Traceback (most recent call last). <ipython-input-35-7ad830fcd907> in <module>. 1 import mnnpy. ----> 2 corrected = mnnpy.mnn_correct(*adata_list, batch_key=""sample""). /Anaconda_python3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 124 cos_norm_out=cos_norm_out, svd_dim=svd_dim, var_adj=var_adj,. 125 compute_angle=compute_angle, mnn_order=mnn_order,. --> 126 svd_mode=svd_mode, do_concatenate=do_concatenate, **kwargs). 127 print('Packing AnnData object...'). 128 if do_concatenate:. /Anaconda_python3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 180 print(' Computing correction vectors...'). 181 correction_in = compute_correction(ref_batch_in, new_batch_in, mnn_ref, mnn_new,. --> 182 new_batch_in, sigma). 183 if not same_set:. 184 correction_out = compute_corre",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1367
https://github.com/scverse/scanpy/issues/1367:2518,modifiability,Pac,Packing,2518,"ells'. uns: 'log1p'. layers: 'counts',. View of AnnData object with n_obs  n_vars = 2422  12818. obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts',. View of AnnData object with n_obs  n_vars = 1773  12818. obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts']. ```. ```. import mnnpy. corrected = mnnpy.mnn_correct(*adata_list, batch_key=""sample""). ```. ```. ---------------------------------------------------------------------------. IndexError Traceback (most recent call last). <ipython-input-35-7ad830fcd907> in <module>. 1 import mnnpy. ----> 2 corrected = mnnpy.mnn_correct(*adata_list, batch_key=""sample""). /Anaconda_python3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 124 cos_norm_out=cos_norm_out, svd_dim=svd_dim, var_adj=var_adj,. 125 compute_angle=compute_angle, mnn_order=mnn_order,. --> 126 svd_mode=svd_mode, do_concatenate=do_concatenate, **kwargs). 127 print('Packing AnnData object...'). 128 if do_concatenate:. /Anaconda_python3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 180 print(' Computing correction vectors...'). 181 correction_in = compute_correction(ref_batch_in, new_batch_in, mnn_ref, mnn_new,. --> 182 new_batch_in, sigma). 183 if not same_set:. 184 correction_out = compute_correction(ref_batch_out, new_batch_out, mnn_ref, mnn_new,. IndexError: arrays used as indices must be of integer (or boolean) type. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1367
https://github.com/scverse/scanpy/issues/1367:2608,modifiability,pac,packages,2608,"ells'. uns: 'log1p'. layers: 'counts',. View of AnnData object with n_obs  n_vars = 2422  12818. obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts',. View of AnnData object with n_obs  n_vars = 1773  12818. obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts']. ```. ```. import mnnpy. corrected = mnnpy.mnn_correct(*adata_list, batch_key=""sample""). ```. ```. ---------------------------------------------------------------------------. IndexError Traceback (most recent call last). <ipython-input-35-7ad830fcd907> in <module>. 1 import mnnpy. ----> 2 corrected = mnnpy.mnn_correct(*adata_list, batch_key=""sample""). /Anaconda_python3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 124 cos_norm_out=cos_norm_out, svd_dim=svd_dim, var_adj=var_adj,. 125 compute_angle=compute_angle, mnn_order=mnn_order,. --> 126 svd_mode=svd_mode, do_concatenate=do_concatenate, **kwargs). 127 print('Packing AnnData object...'). 128 if do_concatenate:. /Anaconda_python3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 180 print(' Computing correction vectors...'). 181 correction_in = compute_correction(ref_batch_in, new_batch_in, mnn_ref, mnn_new,. --> 182 new_batch_in, sigma). 183 if not same_set:. 184 correction_out = compute_correction(ref_batch_out, new_batch_out, mnn_ref, mnn_new,. IndexError: arrays used as indices must be of integer (or boolean) type. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1367
https://github.com/scverse/scanpy/issues/1367:218,performance,error,error,218,"Hi, . That was from my own datasets, but I also used the data from here, https://github.com/theislab/single-cell-tutorial/blob/master/latest_notebook/Case-study_Mouse-intestinal-epithelium_1906.ipynb, and got the same error. ```. adata_mnn = adata.copy(). adata_list = [adata_mnn[adata_mnn.obs['sample'] == i] for i in adata_mnn.obs['sample'].unique()]. adata_list. ```. ```. [View of AnnData object with n_obs  n_vars = 2267  12818. obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts',. View of AnnData object with n_obs  n_vars = 1976  12818. obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts',. View of AnnData object with n_obs  n_vars = 1663  12818. obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts',. View of AnnData object with n_obs  n_vars = 2356  12818. obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts',. View of AnnData object with n_obs  n_vars = 2422  12818. obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts',. View of AnnData object with n_obs  n_vars = 1773  12818. obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts']. ```. ```. import mnnpy. corrected = mnnpy.mnn_correct(*adata_list, batch_key=""sample""). ```. ```. ---------------------------------------------------------------------------. IndexError Traceback (most recent call last). <ipython-input-35-7ad830fcd907> in <module>. 1 import mnnpy. ----> 2 corrected = mnnpy.mnn_correct(*",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1367
https://github.com/scverse/scanpy/issues/1367:218,safety,error,error,218,"Hi, . That was from my own datasets, but I also used the data from here, https://github.com/theislab/single-cell-tutorial/blob/master/latest_notebook/Case-study_Mouse-intestinal-epithelium_1906.ipynb, and got the same error. ```. adata_mnn = adata.copy(). adata_list = [adata_mnn[adata_mnn.obs['sample'] == i] for i in adata_mnn.obs['sample'].unique()]. adata_list. ```. ```. [View of AnnData object with n_obs  n_vars = 2267  12818. obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts',. View of AnnData object with n_obs  n_vars = 1976  12818. obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts',. View of AnnData object with n_obs  n_vars = 1663  12818. obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts',. View of AnnData object with n_obs  n_vars = 2356  12818. obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts',. View of AnnData object with n_obs  n_vars = 2422  12818. obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts',. View of AnnData object with n_obs  n_vars = 1773  12818. obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts']. ```. ```. import mnnpy. corrected = mnnpy.mnn_correct(*adata_list, batch_key=""sample""). ```. ```. ---------------------------------------------------------------------------. IndexError Traceback (most recent call last). <ipython-input-35-7ad830fcd907> in <module>. 1 import mnnpy. ----> 2 corrected = mnnpy.mnn_correct(*",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1367
https://github.com/scverse/scanpy/issues/1367:1909,safety,input,input-,1909,"s', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts',. View of AnnData object with n_obs  n_vars = 2356  12818. obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts',. View of AnnData object with n_obs  n_vars = 2422  12818. obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts',. View of AnnData object with n_obs  n_vars = 1773  12818. obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts']. ```. ```. import mnnpy. corrected = mnnpy.mnn_correct(*adata_list, batch_key=""sample""). ```. ```. ---------------------------------------------------------------------------. IndexError Traceback (most recent call last). <ipython-input-35-7ad830fcd907> in <module>. 1 import mnnpy. ----> 2 corrected = mnnpy.mnn_correct(*adata_list, batch_key=""sample""). /Anaconda_python3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 124 cos_norm_out=cos_norm_out, svd_dim=svd_dim, var_adj=var_adj,. 125 compute_angle=compute_angle, mnn_order=mnn_order,. --> 126 svd_mode=svd_mode, do_concatenate=do_concatenate, **kwargs). 127 print('Packing AnnData object...'). 128 if do_concatenate:. /Anaconda_python3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 180 print(' Computing correction vectors...'). 181 correc",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1367
https://github.com/scverse/scanpy/issues/1367:1936,safety,modul,module,1936,", 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts',. View of AnnData object with n_obs  n_vars = 2356  12818. obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts',. View of AnnData object with n_obs  n_vars = 2422  12818. obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts',. View of AnnData object with n_obs  n_vars = 1773  12818. obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts']. ```. ```. import mnnpy. corrected = mnnpy.mnn_correct(*adata_list, batch_key=""sample""). ```. ```. ---------------------------------------------------------------------------. IndexError Traceback (most recent call last). <ipython-input-35-7ad830fcd907> in <module>. 1 import mnnpy. ----> 2 corrected = mnnpy.mnn_correct(*adata_list, batch_key=""sample""). /Anaconda_python3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 124 cos_norm_out=cos_norm_out, svd_dim=svd_dim, var_adj=var_adj,. 125 compute_angle=compute_angle, mnn_order=mnn_order,. --> 126 svd_mode=svd_mode, do_concatenate=do_concatenate, **kwargs). 127 print('Packing AnnData object...'). 128 if do_concatenate:. /Anaconda_python3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 180 print(' Computing correction vectors...'). 181 correction_in = compute_correctio",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1367
https://github.com/scverse/scanpy/issues/1367:1865,testability,Trace,Traceback,1865,"obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts',. View of AnnData object with n_obs  n_vars = 2356  12818. obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts',. View of AnnData object with n_obs  n_vars = 2422  12818. obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts',. View of AnnData object with n_obs  n_vars = 1773  12818. obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts']. ```. ```. import mnnpy. corrected = mnnpy.mnn_correct(*adata_list, batch_key=""sample""). ```. ```. ---------------------------------------------------------------------------. IndexError Traceback (most recent call last). <ipython-input-35-7ad830fcd907> in <module>. 1 import mnnpy. ----> 2 corrected = mnnpy.mnn_correct(*adata_list, batch_key=""sample""). /Anaconda_python3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 124 cos_norm_out=cos_norm_out, svd_dim=svd_dim, var_adj=var_adj,. 125 compute_angle=compute_angle, mnn_order=mnn_order,. --> 126 svd_mode=svd_mode, do_concatenate=do_concatenate, **kwargs). 127 print('Packing AnnData object...'). 128 if do_concatenate:. /Anaconda_python3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 180 print(' Com",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1367
https://github.com/scverse/scanpy/issues/1367:218,usability,error,error,218,"Hi, . That was from my own datasets, but I also used the data from here, https://github.com/theislab/single-cell-tutorial/blob/master/latest_notebook/Case-study_Mouse-intestinal-epithelium_1906.ipynb, and got the same error. ```. adata_mnn = adata.copy(). adata_list = [adata_mnn[adata_mnn.obs['sample'] == i] for i in adata_mnn.obs['sample'].unique()]. adata_list. ```. ```. [View of AnnData object with n_obs  n_vars = 2267  12818. obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts',. View of AnnData object with n_obs  n_vars = 1976  12818. obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts',. View of AnnData object with n_obs  n_vars = 1663  12818. obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts',. View of AnnData object with n_obs  n_vars = 2356  12818. obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts',. View of AnnData object with n_obs  n_vars = 2422  12818. obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts',. View of AnnData object with n_obs  n_vars = 1773  12818. obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts']. ```. ```. import mnnpy. corrected = mnnpy.mnn_correct(*adata_list, batch_key=""sample""). ```. ```. ---------------------------------------------------------------------------. IndexError Traceback (most recent call last). <ipython-input-35-7ad830fcd907> in <module>. 1 import mnnpy. ----> 2 corrected = mnnpy.mnn_correct(*",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1367
https://github.com/scverse/scanpy/issues/1367:1909,usability,input,input-,1909,"s', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts',. View of AnnData object with n_obs  n_vars = 2356  12818. obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts',. View of AnnData object with n_obs  n_vars = 2422  12818. obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts',. View of AnnData object with n_obs  n_vars = 1773  12818. obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'. var: 'gene_id', 'n_cells'. uns: 'log1p'. layers: 'counts']. ```. ```. import mnnpy. corrected = mnnpy.mnn_correct(*adata_list, batch_key=""sample""). ```. ```. ---------------------------------------------------------------------------. IndexError Traceback (most recent call last). <ipython-input-35-7ad830fcd907> in <module>. 1 import mnnpy. ----> 2 corrected = mnnpy.mnn_correct(*adata_list, batch_key=""sample""). /Anaconda_python3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 124 cos_norm_out=cos_norm_out, svd_dim=svd_dim, var_adj=var_adj,. 125 compute_angle=compute_angle, mnn_order=mnn_order,. --> 126 svd_mode=svd_mode, do_concatenate=do_concatenate, **kwargs). 127 print('Packing AnnData object...'). 128 if do_concatenate:. /Anaconda_python3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs). 180 print(' Computing correction vectors...'). 181 correc",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1367
https://github.com/scverse/scanpy/issues/1367:69,usability,close,close,69,"As we haven't heard back after the followup link of the fix, we will close the issue for now, hopefully you obtained the expected behaviour in the end :). However, please don't hesitate to reopen this issue or create a new one if you have any more questions or run into any related problems in the future. Thanks for being a part of our community! :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1367
https://github.com/scverse/scanpy/issues/1367:130,usability,behavi,behaviour,130,"As we haven't heard back after the followup link of the fix, we will close the issue for now, hopefully you obtained the expected behaviour in the end :). However, please don't hesitate to reopen this issue or create a new one if you have any more questions or run into any related problems in the future. Thanks for being a part of our community! :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1367
https://github.com/scverse/scanpy/issues/1368:216,availability,replic,replicate,216,"At first blush, this looks like an issue with spyder. I'd suggest trying to update your versions of installed packages (it looks like your scanpy and anndata are out of date) and trying again. If that fails, can you replicate in a different python environment?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1368
https://github.com/scverse/scanpy/issues/1368:76,deployability,updat,update,76,"At first blush, this looks like an issue with spyder. I'd suggest trying to update your versions of installed packages (it looks like your scanpy and anndata are out of date) and trying again. If that fails, can you replicate in a different python environment?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1368
https://github.com/scverse/scanpy/issues/1368:88,deployability,version,versions,88,"At first blush, this looks like an issue with spyder. I'd suggest trying to update your versions of installed packages (it looks like your scanpy and anndata are out of date) and trying again. If that fails, can you replicate in a different python environment?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1368
https://github.com/scverse/scanpy/issues/1368:100,deployability,instal,installed,100,"At first blush, this looks like an issue with spyder. I'd suggest trying to update your versions of installed packages (it looks like your scanpy and anndata are out of date) and trying again. If that fails, can you replicate in a different python environment?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1368
https://github.com/scverse/scanpy/issues/1368:201,deployability,fail,fails,201,"At first blush, this looks like an issue with spyder. I'd suggest trying to update your versions of installed packages (it looks like your scanpy and anndata are out of date) and trying again. If that fails, can you replicate in a different python environment?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1368
https://github.com/scverse/scanpy/issues/1368:88,integrability,version,versions,88,"At first blush, this looks like an issue with spyder. I'd suggest trying to update your versions of installed packages (it looks like your scanpy and anndata are out of date) and trying again. If that fails, can you replicate in a different python environment?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1368
https://github.com/scverse/scanpy/issues/1368:88,modifiability,version,versions,88,"At first blush, this looks like an issue with spyder. I'd suggest trying to update your versions of installed packages (it looks like your scanpy and anndata are out of date) and trying again. If that fails, can you replicate in a different python environment?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1368
https://github.com/scverse/scanpy/issues/1368:110,modifiability,pac,packages,110,"At first blush, this looks like an issue with spyder. I'd suggest trying to update your versions of installed packages (it looks like your scanpy and anndata are out of date) and trying again. If that fails, can you replicate in a different python environment?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1368
https://github.com/scverse/scanpy/issues/1368:201,reliability,fail,fails,201,"At first blush, this looks like an issue with spyder. I'd suggest trying to update your versions of installed packages (it looks like your scanpy and anndata are out of date) and trying again. If that fails, can you replicate in a different python environment?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1368
https://github.com/scverse/scanpy/issues/1368:76,safety,updat,update,76,"At first blush, this looks like an issue with spyder. I'd suggest trying to update your versions of installed packages (it looks like your scanpy and anndata are out of date) and trying again. If that fails, can you replicate in a different python environment?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1368
https://github.com/scverse/scanpy/issues/1368:76,security,updat,update,76,"At first blush, this looks like an issue with spyder. I'd suggest trying to update your versions of installed packages (it looks like your scanpy and anndata are out of date) and trying again. If that fails, can you replicate in a different python environment?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1368
https://github.com/scverse/scanpy/issues/1368:46,testability,spy,spyder,46,"At first blush, this looks like an issue with spyder. I'd suggest trying to update your versions of installed packages (it looks like your scanpy and anndata are out of date) and trying again. If that fails, can you replicate in a different python environment?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1368
https://github.com/scverse/scanpy/issues/1368:53,usability,close,close,53,"As we haven't heard back after the follow-up we will close the issue for now, hopefully you obtained the expected behaviour in the end :). However, please don't hesitate to reopen this issue or create a new one if you have any more questions or run into any related problems in the future. Thanks for being a part of our community! :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1368
https://github.com/scverse/scanpy/issues/1368:114,usability,behavi,behaviour,114,"As we haven't heard back after the follow-up we will close the issue for now, hopefully you obtained the expected behaviour in the end :). However, please don't hesitate to reopen this issue or create a new one if you have any more questions or run into any related problems in the future. Thanks for being a part of our community! :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1368
https://github.com/scverse/scanpy/issues/1369:0,deployability,Instal,Install,0,"Install it, its an optional dependency.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1369
https://github.com/scverse/scanpy/issues/1369:29,deployability,depend,dependency,29,"Install it, its an optional dependency.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1369
https://github.com/scverse/scanpy/issues/1369:29,integrability,depend,dependency,29,"Install it, its an optional dependency.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1369
https://github.com/scverse/scanpy/issues/1369:29,modifiability,depend,dependency,29,"Install it, its an optional dependency.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1369
https://github.com/scverse/scanpy/issues/1369:29,safety,depend,dependency,29,"Install it, its an optional dependency.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1369
https://github.com/scverse/scanpy/issues/1369:29,testability,depend,dependency,29,"Install it, its an optional dependency.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1369
https://github.com/scverse/scanpy/issues/1370:55,deployability,integr,integrated,55,"I think the issue here is that BBKNN only generates an integrated graph, while the tsne computation creates a new graph from some matrix representation of the data. There has been the suggestion of allowing a tsne layout (#1233) to be generated from a precomputed connectivity matrix, but that hasn't been implemented here yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1370
https://github.com/scverse/scanpy/issues/1370:55,integrability,integr,integrated,55,"I think the issue here is that BBKNN only generates an integrated graph, while the tsne computation creates a new graph from some matrix representation of the data. There has been the suggestion of allowing a tsne layout (#1233) to be generated from a precomputed connectivity matrix, but that hasn't been implemented here yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1370
https://github.com/scverse/scanpy/issues/1370:55,interoperability,integr,integrated,55,"I think the issue here is that BBKNN only generates an integrated graph, while the tsne computation creates a new graph from some matrix representation of the data. There has been the suggestion of allowing a tsne layout (#1233) to be generated from a precomputed connectivity matrix, but that hasn't been implemented here yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1370
https://github.com/scverse/scanpy/issues/1370:55,modifiability,integr,integrated,55,"I think the issue here is that BBKNN only generates an integrated graph, while the tsne computation creates a new graph from some matrix representation of the data. There has been the suggestion of allowing a tsne layout (#1233) to be generated from a precomputed connectivity matrix, but that hasn't been implemented here yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1370
https://github.com/scverse/scanpy/issues/1370:55,reliability,integr,integrated,55,"I think the issue here is that BBKNN only generates an integrated graph, while the tsne computation creates a new graph from some matrix representation of the data. There has been the suggestion of allowing a tsne layout (#1233) to be generated from a precomputed connectivity matrix, but that hasn't been implemented here yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1370
https://github.com/scverse/scanpy/issues/1370:55,security,integr,integrated,55,"I think the issue here is that BBKNN only generates an integrated graph, while the tsne computation creates a new graph from some matrix representation of the data. There has been the suggestion of allowing a tsne layout (#1233) to be generated from a precomputed connectivity matrix, but that hasn't been implemented here yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1370
https://github.com/scverse/scanpy/issues/1370:55,testability,integr,integrated,55,"I think the issue here is that BBKNN only generates an integrated graph, while the tsne computation creates a new graph from some matrix representation of the data. There has been the suggestion of allowing a tsne layout (#1233) to be generated from a precomputed connectivity matrix, but that hasn't been implemented here yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1370
https://github.com/scverse/scanpy/issues/1370:52,reliability,doe,doesn,52,"Ah, I wasn't aware that the `sc.tl.tsne()` function doesn't use the `sc.pp.neighbors()` output. Although I do recall the issue about this... Thanks for the context!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1370
https://github.com/scverse/scanpy/issues/1370:156,testability,context,context,156,"Ah, I wasn't aware that the `sc.tl.tsne()` function doesn't use the `sc.pp.neighbors()` output. Although I do recall the issue about this... Thanks for the context!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1370
https://github.com/scverse/scanpy/issues/1370:41,deployability,integr,integrated,41,"One potential solution is to convert the integrated connectivity matrix, C, into a pseudo-distance matrix (1-C) (this probably won't work for datasets much larger than 10k cells due to memory limitations) and run t-SNE with the 'precomputed' metric on that fake distance matrix. If scanpy's t-SNE wrapper does not allow passing a precomputed distance matrix, I would recommend using the sklearn implementation directly:. https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1370
https://github.com/scverse/scanpy/issues/1370:453,deployability,modul,modules,453,"One potential solution is to convert the integrated connectivity matrix, C, into a pseudo-distance matrix (1-C) (this probably won't work for datasets much larger than 10k cells due to memory limitations) and run t-SNE with the 'precomputed' metric on that fake distance matrix. If scanpy's t-SNE wrapper does not allow passing a precomputed distance matrix, I would recommend using the sklearn implementation directly:. https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1370
https://github.com/scverse/scanpy/issues/1370:41,integrability,integr,integrated,41,"One potential solution is to convert the integrated connectivity matrix, C, into a pseudo-distance matrix (1-C) (this probably won't work for datasets much larger than 10k cells due to memory limitations) and run t-SNE with the 'precomputed' metric on that fake distance matrix. If scanpy's t-SNE wrapper does not allow passing a precomputed distance matrix, I would recommend using the sklearn implementation directly:. https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1370
https://github.com/scverse/scanpy/issues/1370:297,integrability,wrap,wrapper,297,"One potential solution is to convert the integrated connectivity matrix, C, into a pseudo-distance matrix (1-C) (this probably won't work for datasets much larger than 10k cells due to memory limitations) and run t-SNE with the 'precomputed' metric on that fake distance matrix. If scanpy's t-SNE wrapper does not allow passing a precomputed distance matrix, I would recommend using the sklearn implementation directly:. https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1370
https://github.com/scverse/scanpy/issues/1370:41,interoperability,integr,integrated,41,"One potential solution is to convert the integrated connectivity matrix, C, into a pseudo-distance matrix (1-C) (this probably won't work for datasets much larger than 10k cells due to memory limitations) and run t-SNE with the 'precomputed' metric on that fake distance matrix. If scanpy's t-SNE wrapper does not allow passing a precomputed distance matrix, I would recommend using the sklearn implementation directly:. https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1370
https://github.com/scverse/scanpy/issues/1370:297,interoperability,wrapper,wrapper,297,"One potential solution is to convert the integrated connectivity matrix, C, into a pseudo-distance matrix (1-C) (this probably won't work for datasets much larger than 10k cells due to memory limitations) and run t-SNE with the 'precomputed' metric on that fake distance matrix. If scanpy's t-SNE wrapper does not allow passing a precomputed distance matrix, I would recommend using the sklearn implementation directly:. https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1370
https://github.com/scverse/scanpy/issues/1370:41,modifiability,integr,integrated,41,"One potential solution is to convert the integrated connectivity matrix, C, into a pseudo-distance matrix (1-C) (this probably won't work for datasets much larger than 10k cells due to memory limitations) and run t-SNE with the 'precomputed' metric on that fake distance matrix. If scanpy's t-SNE wrapper does not allow passing a precomputed distance matrix, I would recommend using the sklearn implementation directly:. https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1370
https://github.com/scverse/scanpy/issues/1370:453,modifiability,modul,modules,453,"One potential solution is to convert the integrated connectivity matrix, C, into a pseudo-distance matrix (1-C) (this probably won't work for datasets much larger than 10k cells due to memory limitations) and run t-SNE with the 'precomputed' metric on that fake distance matrix. If scanpy's t-SNE wrapper does not allow passing a precomputed distance matrix, I would recommend using the sklearn implementation directly:. https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1370
https://github.com/scverse/scanpy/issues/1370:185,performance,memor,memory,185,"One potential solution is to convert the integrated connectivity matrix, C, into a pseudo-distance matrix (1-C) (this probably won't work for datasets much larger than 10k cells due to memory limitations) and run t-SNE with the 'precomputed' metric on that fake distance matrix. If scanpy's t-SNE wrapper does not allow passing a precomputed distance matrix, I would recommend using the sklearn implementation directly:. https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1370
https://github.com/scverse/scanpy/issues/1370:41,reliability,integr,integrated,41,"One potential solution is to convert the integrated connectivity matrix, C, into a pseudo-distance matrix (1-C) (this probably won't work for datasets much larger than 10k cells due to memory limitations) and run t-SNE with the 'precomputed' metric on that fake distance matrix. If scanpy's t-SNE wrapper does not allow passing a precomputed distance matrix, I would recommend using the sklearn implementation directly:. https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1370
https://github.com/scverse/scanpy/issues/1370:305,reliability,doe,does,305,"One potential solution is to convert the integrated connectivity matrix, C, into a pseudo-distance matrix (1-C) (this probably won't work for datasets much larger than 10k cells due to memory limitations) and run t-SNE with the 'precomputed' metric on that fake distance matrix. If scanpy's t-SNE wrapper does not allow passing a precomputed distance matrix, I would recommend using the sklearn implementation directly:. https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1370
https://github.com/scverse/scanpy/issues/1370:453,safety,modul,modules,453,"One potential solution is to convert the integrated connectivity matrix, C, into a pseudo-distance matrix (1-C) (this probably won't work for datasets much larger than 10k cells due to memory limitations) and run t-SNE with the 'precomputed' metric on that fake distance matrix. If scanpy's t-SNE wrapper does not allow passing a precomputed distance matrix, I would recommend using the sklearn implementation directly:. https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1370
https://github.com/scverse/scanpy/issues/1370:41,security,integr,integrated,41,"One potential solution is to convert the integrated connectivity matrix, C, into a pseudo-distance matrix (1-C) (this probably won't work for datasets much larger than 10k cells due to memory limitations) and run t-SNE with the 'precomputed' metric on that fake distance matrix. If scanpy's t-SNE wrapper does not allow passing a precomputed distance matrix, I would recommend using the sklearn implementation directly:. https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1370
https://github.com/scverse/scanpy/issues/1370:41,testability,integr,integrated,41,"One potential solution is to convert the integrated connectivity matrix, C, into a pseudo-distance matrix (1-C) (this probably won't work for datasets much larger than 10k cells due to memory limitations) and run t-SNE with the 'precomputed' metric on that fake distance matrix. If scanpy's t-SNE wrapper does not allow passing a precomputed distance matrix, I would recommend using the sklearn implementation directly:. https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1370
https://github.com/scverse/scanpy/issues/1370:185,usability,memor,memory,185,"One potential solution is to convert the integrated connectivity matrix, C, into a pseudo-distance matrix (1-C) (this probably won't work for datasets much larger than 10k cells due to memory limitations) and run t-SNE with the 'precomputed' metric on that fake distance matrix. If scanpy's t-SNE wrapper does not allow passing a precomputed distance matrix, I would recommend using the sklearn implementation directly:. https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1370
https://github.com/scverse/scanpy/issues/1370:436,usability,learn,learn,436,"One potential solution is to convert the integrated connectivity matrix, C, into a pseudo-distance matrix (1-C) (this probably won't work for datasets much larger than 10k cells due to memory limitations) and run t-SNE with the 'precomputed' metric on that fake distance matrix. If scanpy's t-SNE wrapper does not allow passing a precomputed distance matrix, I would recommend using the sklearn implementation directly:. https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1370
https://github.com/scverse/scanpy/issues/1371:10,deployability,releas,release,10,"Hi, after release 1.6 this is now partially possible. If you set the parameter `return_fig=True` then you have access to the `style()` method (see: https://scanpy.readthedocs.io/en/stable/api/scanpy.pl.DotPlot.style.html#scanpy.pl.DotPlot.style). ```PYTHON. adata = sc.datasets.pbmc68k_reduced(). marker_genes = ['S100A8', 'GNLY', 'NKG7', 'KLRB1', 'FCGR3A', 'FCER1A', 'CST3']. ax_dict = sc.pl.dotplot(adata,marker_genes,groupby='bulk_labels', return_fig=True).style(grid=True).show(). ```. ![image](https://user-images.githubusercontent.com/4964309/90759033-2a647600-e2e0-11ea-86e4-2a0e060955ad.png). What is not possible is to change the linewidth.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1371
https://github.com/scverse/scanpy/issues/1371:188,deployability,api,api,188,"Hi, after release 1.6 this is now partially possible. If you set the parameter `return_fig=True` then you have access to the `style()` method (see: https://scanpy.readthedocs.io/en/stable/api/scanpy.pl.DotPlot.style.html#scanpy.pl.DotPlot.style). ```PYTHON. adata = sc.datasets.pbmc68k_reduced(). marker_genes = ['S100A8', 'GNLY', 'NKG7', 'KLRB1', 'FCGR3A', 'FCER1A', 'CST3']. ax_dict = sc.pl.dotplot(adata,marker_genes,groupby='bulk_labels', return_fig=True).style(grid=True).show(). ```. ![image](https://user-images.githubusercontent.com/4964309/90759033-2a647600-e2e0-11ea-86e4-2a0e060955ad.png). What is not possible is to change the linewidth.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1371
https://github.com/scverse/scanpy/issues/1371:188,integrability,api,api,188,"Hi, after release 1.6 this is now partially possible. If you set the parameter `return_fig=True` then you have access to the `style()` method (see: https://scanpy.readthedocs.io/en/stable/api/scanpy.pl.DotPlot.style.html#scanpy.pl.DotPlot.style). ```PYTHON. adata = sc.datasets.pbmc68k_reduced(). marker_genes = ['S100A8', 'GNLY', 'NKG7', 'KLRB1', 'FCGR3A', 'FCER1A', 'CST3']. ax_dict = sc.pl.dotplot(adata,marker_genes,groupby='bulk_labels', return_fig=True).style(grid=True).show(). ```. ![image](https://user-images.githubusercontent.com/4964309/90759033-2a647600-e2e0-11ea-86e4-2a0e060955ad.png). What is not possible is to change the linewidth.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1371
https://github.com/scverse/scanpy/issues/1371:188,interoperability,api,api,188,"Hi, after release 1.6 this is now partially possible. If you set the parameter `return_fig=True` then you have access to the `style()` method (see: https://scanpy.readthedocs.io/en/stable/api/scanpy.pl.DotPlot.style.html#scanpy.pl.DotPlot.style). ```PYTHON. adata = sc.datasets.pbmc68k_reduced(). marker_genes = ['S100A8', 'GNLY', 'NKG7', 'KLRB1', 'FCGR3A', 'FCER1A', 'CST3']. ax_dict = sc.pl.dotplot(adata,marker_genes,groupby='bulk_labels', return_fig=True).style(grid=True).show(). ```. ![image](https://user-images.githubusercontent.com/4964309/90759033-2a647600-e2e0-11ea-86e4-2a0e060955ad.png). What is not possible is to change the linewidth.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1371
https://github.com/scverse/scanpy/issues/1371:69,modifiability,paramet,parameter,69,"Hi, after release 1.6 this is now partially possible. If you set the parameter `return_fig=True` then you have access to the `style()` method (see: https://scanpy.readthedocs.io/en/stable/api/scanpy.pl.DotPlot.style.html#scanpy.pl.DotPlot.style). ```PYTHON. adata = sc.datasets.pbmc68k_reduced(). marker_genes = ['S100A8', 'GNLY', 'NKG7', 'KLRB1', 'FCGR3A', 'FCER1A', 'CST3']. ax_dict = sc.pl.dotplot(adata,marker_genes,groupby='bulk_labels', return_fig=True).style(grid=True).show(). ```. ![image](https://user-images.githubusercontent.com/4964309/90759033-2a647600-e2e0-11ea-86e4-2a0e060955ad.png). What is not possible is to change the linewidth.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1371
https://github.com/scverse/scanpy/issues/1371:111,security,access,access,111,"Hi, after release 1.6 this is now partially possible. If you set the parameter `return_fig=True` then you have access to the `style()` method (see: https://scanpy.readthedocs.io/en/stable/api/scanpy.pl.DotPlot.style.html#scanpy.pl.DotPlot.style). ```PYTHON. adata = sc.datasets.pbmc68k_reduced(). marker_genes = ['S100A8', 'GNLY', 'NKG7', 'KLRB1', 'FCGR3A', 'FCER1A', 'CST3']. ax_dict = sc.pl.dotplot(adata,marker_genes,groupby='bulk_labels', return_fig=True).style(grid=True).show(). ```. ![image](https://user-images.githubusercontent.com/4964309/90759033-2a647600-e2e0-11ea-86e4-2a0e060955ad.png). What is not possible is to change the linewidth.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1371
https://github.com/scverse/scanpy/issues/1371:507,usability,user,user-images,507,"Hi, after release 1.6 this is now partially possible. If you set the parameter `return_fig=True` then you have access to the `style()` method (see: https://scanpy.readthedocs.io/en/stable/api/scanpy.pl.DotPlot.style.html#scanpy.pl.DotPlot.style). ```PYTHON. adata = sc.datasets.pbmc68k_reduced(). marker_genes = ['S100A8', 'GNLY', 'NKG7', 'KLRB1', 'FCGR3A', 'FCER1A', 'CST3']. ax_dict = sc.pl.dotplot(adata,marker_genes,groupby='bulk_labels', return_fig=True).style(grid=True).show(). ```. ![image](https://user-images.githubusercontent.com/4964309/90759033-2a647600-e2e0-11ea-86e4-2a0e060955ad.png). What is not possible is to change the linewidth.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1371
https://github.com/scverse/scanpy/issues/1371:40,security,control,control,40,Actually a quick follow-up. How can one control the spacing between the dots in the dot size legend? They're often too close to each other for me. Thank you!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1371
https://github.com/scverse/scanpy/issues/1371:40,testability,control,control,40,Actually a quick follow-up. How can one control the spacing between the dots in the dot size legend? They're often too close to each other for me. Thank you!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1371
https://github.com/scverse/scanpy/issues/1371:119,usability,close,close,119,Actually a quick follow-up. How can one control the spacing between the dots in the dot size legend? They're often too close to each other for me. Thank you!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1371
https://github.com/scverse/scanpy/issues/1371:237,deployability,api,api,237,"you can set the legend width. Using the previous example:. ```PYTHON. sc.pl.dotplot(adata,marker_genes,groupby='bulk_labels', return_fig=True).style(grid=True).legend(width=1.8).show(). ````. See: https://scanpy.readthedocs.io/en/latest/api/scanpy.pl.DotPlot.legend.html#scanpy.pl.DotPlot.legend",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1371
https://github.com/scverse/scanpy/issues/1371:237,integrability,api,api,237,"you can set the legend width. Using the previous example:. ```PYTHON. sc.pl.dotplot(adata,marker_genes,groupby='bulk_labels', return_fig=True).style(grid=True).legend(width=1.8).show(). ````. See: https://scanpy.readthedocs.io/en/latest/api/scanpy.pl.DotPlot.legend.html#scanpy.pl.DotPlot.legend",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1371
https://github.com/scverse/scanpy/issues/1371:237,interoperability,api,api,237,"you can set the legend width. Using the previous example:. ```PYTHON. sc.pl.dotplot(adata,marker_genes,groupby='bulk_labels', return_fig=True).style(grid=True).legend(width=1.8).show(). ````. See: https://scanpy.readthedocs.io/en/latest/api/scanpy.pl.DotPlot.legend.html#scanpy.pl.DotPlot.legend",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1371
https://github.com/scverse/scanpy/issues/1371:92,deployability,automat,automatic,92,"Hi,. great options with the styles... just one comment/question... to me, it seems that the automatic detection of the background for the color of the dot is not working:. dot_edge_color : str, Tuple[float, ], None (default: 'black'). Dot edge color. When color_on='dot' the default is no edge. When color_on='square', edge color is white for darker colors and black for lighter background square colors. in my case, all dots are black independent of the background color:. ![image](https://user-images.githubusercontent.com/59560120/100715263-3c61b480-33b7-11eb-8874-2a85364acea4.png). Stefan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1371
https://github.com/scverse/scanpy/issues/1371:102,safety,detect,detection,102,"Hi,. great options with the styles... just one comment/question... to me, it seems that the automatic detection of the background for the color of the dot is not working:. dot_edge_color : str, Tuple[float, ], None (default: 'black'). Dot edge color. When color_on='dot' the default is no edge. When color_on='square', edge color is white for darker colors and black for lighter background square colors. in my case, all dots are black independent of the background color:. ![image](https://user-images.githubusercontent.com/59560120/100715263-3c61b480-33b7-11eb-8874-2a85364acea4.png). Stefan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1371
https://github.com/scverse/scanpy/issues/1371:102,security,detect,detection,102,"Hi,. great options with the styles... just one comment/question... to me, it seems that the automatic detection of the background for the color of the dot is not working:. dot_edge_color : str, Tuple[float, ], None (default: 'black'). Dot edge color. When color_on='dot' the default is no edge. When color_on='square', edge color is white for darker colors and black for lighter background square colors. in my case, all dots are black independent of the background color:. ![image](https://user-images.githubusercontent.com/59560120/100715263-3c61b480-33b7-11eb-8874-2a85364acea4.png). Stefan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1371
https://github.com/scverse/scanpy/issues/1371:92,testability,automat,automatic,92,"Hi,. great options with the styles... just one comment/question... to me, it seems that the automatic detection of the background for the color of the dot is not working:. dot_edge_color : str, Tuple[float, ], None (default: 'black'). Dot edge color. When color_on='dot' the default is no edge. When color_on='square', edge color is white for darker colors and black for lighter background square colors. in my case, all dots are black independent of the background color:. ![image](https://user-images.githubusercontent.com/59560120/100715263-3c61b480-33b7-11eb-8874-2a85364acea4.png). Stefan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1371
https://github.com/scverse/scanpy/issues/1371:492,usability,user,user-images,492,"Hi,. great options with the styles... just one comment/question... to me, it seems that the automatic detection of the background for the color of the dot is not working:. dot_edge_color : str, Tuple[float, ], None (default: 'black'). Dot edge color. When color_on='dot' the default is no edge. When color_on='square', edge color is white for darker colors and black for lighter background square colors. in my case, all dots are black independent of the background color:. ![image](https://user-images.githubusercontent.com/59560120/100715263-3c61b480-33b7-11eb-8874-2a85364acea4.png). Stefan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1371
https://github.com/scverse/scanpy/pull/1377:13,deployability,instal,installs,13,"Development installs arent standard. What they do is linking the package into your PYTHONPATH and adding distro metadata (`.egg-info` directories). You can do that manually using `ln -s package path/to/env/site-packages` or by setting `PYTHONPATH=""path/to/package/..:$PYTHONPATH""` (And moving/linking a `.egg-info`/`.dist-info` directory over), or you can use a tool to help you:. - `python3 setup.py delvelop` or `pip install -e .` for a nonstandard setuptools project. - Whatever tool you want to use (in our case `flit install -s` does it). The reason `pip` doesnt support it yet for pyproject.toml-based installs is that pip want to support the old nonstandard way and the new standard way  and the new way has no spec for a dev install yet, so pip waits until there is one.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1377
https://github.com/scverse/scanpy/pull/1377:422,deployability,instal,install,422,"Development installs arent standard. What they do is linking the package into your PYTHONPATH and adding distro metadata (`.egg-info` directories). You can do that manually using `ln -s package path/to/env/site-packages` or by setting `PYTHONPATH=""path/to/package/..:$PYTHONPATH""` (And moving/linking a `.egg-info`/`.dist-info` directory over), or you can use a tool to help you:. - `python3 setup.py delvelop` or `pip install -e .` for a nonstandard setuptools project. - Whatever tool you want to use (in our case `flit install -s` does it). The reason `pip` doesnt support it yet for pyproject.toml-based installs is that pip want to support the old nonstandard way and the new standard way  and the new way has no spec for a dev install yet, so pip waits until there is one.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1377
https://github.com/scverse/scanpy/pull/1377:525,deployability,instal,install,525,"Development installs arent standard. What they do is linking the package into your PYTHONPATH and adding distro metadata (`.egg-info` directories). You can do that manually using `ln -s package path/to/env/site-packages` or by setting `PYTHONPATH=""path/to/package/..:$PYTHONPATH""` (And moving/linking a `.egg-info`/`.dist-info` directory over), or you can use a tool to help you:. - `python3 setup.py delvelop` or `pip install -e .` for a nonstandard setuptools project. - Whatever tool you want to use (in our case `flit install -s` does it). The reason `pip` doesnt support it yet for pyproject.toml-based installs is that pip want to support the old nonstandard way and the new standard way  and the new way has no spec for a dev install yet, so pip waits until there is one.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1377
https://github.com/scverse/scanpy/pull/1377:612,deployability,instal,installs,612,"Development installs arent standard. What they do is linking the package into your PYTHONPATH and adding distro metadata (`.egg-info` directories). You can do that manually using `ln -s package path/to/env/site-packages` or by setting `PYTHONPATH=""path/to/package/..:$PYTHONPATH""` (And moving/linking a `.egg-info`/`.dist-info` directory over), or you can use a tool to help you:. - `python3 setup.py delvelop` or `pip install -e .` for a nonstandard setuptools project. - Whatever tool you want to use (in our case `flit install -s` does it). The reason `pip` doesnt support it yet for pyproject.toml-based installs is that pip want to support the old nonstandard way and the new standard way  and the new way has no spec for a dev install yet, so pip waits until there is one.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1377
https://github.com/scverse/scanpy/pull/1377:739,deployability,instal,install,739,"Development installs arent standard. What they do is linking the package into your PYTHONPATH and adding distro metadata (`.egg-info` directories). You can do that manually using `ln -s package path/to/env/site-packages` or by setting `PYTHONPATH=""path/to/package/..:$PYTHONPATH""` (And moving/linking a `.egg-info`/`.dist-info` directory over), or you can use a tool to help you:. - `python3 setup.py delvelop` or `pip install -e .` for a nonstandard setuptools project. - Whatever tool you want to use (in our case `flit install -s` does it). The reason `pip` doesnt support it yet for pyproject.toml-based installs is that pip want to support the old nonstandard way and the new standard way  and the new way has no spec for a dev install yet, so pip waits until there is one.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1377
https://github.com/scverse/scanpy/pull/1377:30,interoperability,standard,standard,30,"Development installs arent standard. What they do is linking the package into your PYTHONPATH and adding distro metadata (`.egg-info` directories). You can do that manually using `ln -s package path/to/env/site-packages` or by setting `PYTHONPATH=""path/to/package/..:$PYTHONPATH""` (And moving/linking a `.egg-info`/`.dist-info` directory over), or you can use a tool to help you:. - `python3 setup.py delvelop` or `pip install -e .` for a nonstandard setuptools project. - Whatever tool you want to use (in our case `flit install -s` does it). The reason `pip` doesnt support it yet for pyproject.toml-based installs is that pip want to support the old nonstandard way and the new standard way  and the new way has no spec for a dev install yet, so pip waits until there is one.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1377
https://github.com/scverse/scanpy/pull/1377:685,interoperability,standard,standard,685,"Development installs arent standard. What they do is linking the package into your PYTHONPATH and adding distro metadata (`.egg-info` directories). You can do that manually using `ln -s package path/to/env/site-packages` or by setting `PYTHONPATH=""path/to/package/..:$PYTHONPATH""` (And moving/linking a `.egg-info`/`.dist-info` directory over), or you can use a tool to help you:. - `python3 setup.py delvelop` or `pip install -e .` for a nonstandard setuptools project. - Whatever tool you want to use (in our case `flit install -s` does it). The reason `pip` doesnt support it yet for pyproject.toml-based installs is that pip want to support the old nonstandard way and the new standard way  and the new way has no spec for a dev install yet, so pip waits until there is one.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1377
https://github.com/scverse/scanpy/pull/1377:68,modifiability,pac,package,68,"Development installs arent standard. What they do is linking the package into your PYTHONPATH and adding distro metadata (`.egg-info` directories). You can do that manually using `ln -s package path/to/env/site-packages` or by setting `PYTHONPATH=""path/to/package/..:$PYTHONPATH""` (And moving/linking a `.egg-info`/`.dist-info` directory over), or you can use a tool to help you:. - `python3 setup.py delvelop` or `pip install -e .` for a nonstandard setuptools project. - Whatever tool you want to use (in our case `flit install -s` does it). The reason `pip` doesnt support it yet for pyproject.toml-based installs is that pip want to support the old nonstandard way and the new standard way  and the new way has no spec for a dev install yet, so pip waits until there is one.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1377
https://github.com/scverse/scanpy/pull/1377:189,modifiability,pac,package,189,"Development installs arent standard. What they do is linking the package into your PYTHONPATH and adding distro metadata (`.egg-info` directories). You can do that manually using `ln -s package path/to/env/site-packages` or by setting `PYTHONPATH=""path/to/package/..:$PYTHONPATH""` (And moving/linking a `.egg-info`/`.dist-info` directory over), or you can use a tool to help you:. - `python3 setup.py delvelop` or `pip install -e .` for a nonstandard setuptools project. - Whatever tool you want to use (in our case `flit install -s` does it). The reason `pip` doesnt support it yet for pyproject.toml-based installs is that pip want to support the old nonstandard way and the new standard way  and the new way has no spec for a dev install yet, so pip waits until there is one.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1377
https://github.com/scverse/scanpy/pull/1377:214,modifiability,pac,packages,214,"Development installs arent standard. What they do is linking the package into your PYTHONPATH and adding distro metadata (`.egg-info` directories). You can do that manually using `ln -s package path/to/env/site-packages` or by setting `PYTHONPATH=""path/to/package/..:$PYTHONPATH""` (And moving/linking a `.egg-info`/`.dist-info` directory over), or you can use a tool to help you:. - `python3 setup.py delvelop` or `pip install -e .` for a nonstandard setuptools project. - Whatever tool you want to use (in our case `flit install -s` does it). The reason `pip` doesnt support it yet for pyproject.toml-based installs is that pip want to support the old nonstandard way and the new standard way  and the new way has no spec for a dev install yet, so pip waits until there is one.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1377
https://github.com/scverse/scanpy/pull/1377:259,modifiability,pac,package,259,"Development installs arent standard. What they do is linking the package into your PYTHONPATH and adding distro metadata (`.egg-info` directories). You can do that manually using `ln -s package path/to/env/site-packages` or by setting `PYTHONPATH=""path/to/package/..:$PYTHONPATH""` (And moving/linking a `.egg-info`/`.dist-info` directory over), or you can use a tool to help you:. - `python3 setup.py delvelop` or `pip install -e .` for a nonstandard setuptools project. - Whatever tool you want to use (in our case `flit install -s` does it). The reason `pip` doesnt support it yet for pyproject.toml-based installs is that pip want to support the old nonstandard way and the new standard way  and the new way has no spec for a dev install yet, so pip waits until there is one.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1377
https://github.com/scverse/scanpy/pull/1377:537,reliability,doe,does,537,"Development installs arent standard. What they do is linking the package into your PYTHONPATH and adding distro metadata (`.egg-info` directories). You can do that manually using `ln -s package path/to/env/site-packages` or by setting `PYTHONPATH=""path/to/package/..:$PYTHONPATH""` (And moving/linking a `.egg-info`/`.dist-info` directory over), or you can use a tool to help you:. - `python3 setup.py delvelop` or `pip install -e .` for a nonstandard setuptools project. - Whatever tool you want to use (in our case `flit install -s` does it). The reason `pip` doesnt support it yet for pyproject.toml-based installs is that pip want to support the old nonstandard way and the new standard way  and the new way has no spec for a dev install yet, so pip waits until there is one.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1377
https://github.com/scverse/scanpy/pull/1377:564,reliability,doe,doesn,564,"Development installs arent standard. What they do is linking the package into your PYTHONPATH and adding distro metadata (`.egg-info` directories). You can do that manually using `ln -s package path/to/env/site-packages` or by setting `PYTHONPATH=""path/to/package/..:$PYTHONPATH""` (And moving/linking a `.egg-info`/`.dist-info` directory over), or you can use a tool to help you:. - `python3 setup.py delvelop` or `pip install -e .` for a nonstandard setuptools project. - Whatever tool you want to use (in our case `flit install -s` does it). The reason `pip` doesnt support it yet for pyproject.toml-based installs is that pip want to support the old nonstandard way and the new standard way  and the new way has no spec for a dev install yet, so pip waits until there is one.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1377
https://github.com/scverse/scanpy/pull/1377:365,usability,tool,tool,365,"Development installs arent standard. What they do is linking the package into your PYTHONPATH and adding distro metadata (`.egg-info` directories). You can do that manually using `ln -s package path/to/env/site-packages` or by setting `PYTHONPATH=""path/to/package/..:$PYTHONPATH""` (And moving/linking a `.egg-info`/`.dist-info` directory over), or you can use a tool to help you:. - `python3 setup.py delvelop` or `pip install -e .` for a nonstandard setuptools project. - Whatever tool you want to use (in our case `flit install -s` does it). The reason `pip` doesnt support it yet for pyproject.toml-based installs is that pip want to support the old nonstandard way and the new standard way  and the new way has no spec for a dev install yet, so pip waits until there is one.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1377
https://github.com/scverse/scanpy/pull/1377:373,usability,help,help,373,"Development installs arent standard. What they do is linking the package into your PYTHONPATH and adding distro metadata (`.egg-info` directories). You can do that manually using `ln -s package path/to/env/site-packages` or by setting `PYTHONPATH=""path/to/package/..:$PYTHONPATH""` (And moving/linking a `.egg-info`/`.dist-info` directory over), or you can use a tool to help you:. - `python3 setup.py delvelop` or `pip install -e .` for a nonstandard setuptools project. - Whatever tool you want to use (in our case `flit install -s` does it). The reason `pip` doesnt support it yet for pyproject.toml-based installs is that pip want to support the old nonstandard way and the new standard way  and the new way has no spec for a dev install yet, so pip waits until there is one.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1377
https://github.com/scverse/scanpy/pull/1377:485,usability,tool,tool,485,"Development installs arent standard. What they do is linking the package into your PYTHONPATH and adding distro metadata (`.egg-info` directories). You can do that manually using `ln -s package path/to/env/site-packages` or by setting `PYTHONPATH=""path/to/package/..:$PYTHONPATH""` (And moving/linking a `.egg-info`/`.dist-info` directory over), or you can use a tool to help you:. - `python3 setup.py delvelop` or `pip install -e .` for a nonstandard setuptools project. - Whatever tool you want to use (in our case `flit install -s` does it). The reason `pip` doesnt support it yet for pyproject.toml-based installs is that pip want to support the old nonstandard way and the new standard way  and the new way has no spec for a dev install yet, so pip waits until there is one.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1377
https://github.com/scverse/scanpy/pull/1377:572,usability,support,support,572,"Development installs arent standard. What they do is linking the package into your PYTHONPATH and adding distro metadata (`.egg-info` directories). You can do that manually using `ln -s package path/to/env/site-packages` or by setting `PYTHONPATH=""path/to/package/..:$PYTHONPATH""` (And moving/linking a `.egg-info`/`.dist-info` directory over), or you can use a tool to help you:. - `python3 setup.py delvelop` or `pip install -e .` for a nonstandard setuptools project. - Whatever tool you want to use (in our case `flit install -s` does it). The reason `pip` doesnt support it yet for pyproject.toml-based installs is that pip want to support the old nonstandard way and the new standard way  and the new way has no spec for a dev install yet, so pip waits until there is one.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1377
https://github.com/scverse/scanpy/pull/1377:641,usability,support,support,641,"Development installs arent standard. What they do is linking the package into your PYTHONPATH and adding distro metadata (`.egg-info` directories). You can do that manually using `ln -s package path/to/env/site-packages` or by setting `PYTHONPATH=""path/to/package/..:$PYTHONPATH""` (And moving/linking a `.egg-info`/`.dist-info` directory over), or you can use a tool to help you:. - `python3 setup.py delvelop` or `pip install -e .` for a nonstandard setuptools project. - Whatever tool you want to use (in our case `flit install -s` does it). The reason `pip` doesnt support it yet for pyproject.toml-based installs is that pip want to support the old nonstandard way and the new standard way  and the new way has no spec for a dev install yet, so pip waits until there is one.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1377
https://github.com/scverse/scanpy/pull/1377:6,deployability,instal,install,6,"`flit install --deps none -s` breaks `conda list` for me. using `PYTHONPATH` for scanpy **and** anndata won't allow importing scanpy because `importlib_metadata.PackageNotFoundError: anndata`. This might be windows specific problems, but `pip setup.py develop` worked perfectly for me.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1377
https://github.com/scverse/scanpy/pull/1377:215,interoperability,specif,specific,215,"`flit install --deps none -s` breaks `conda list` for me. using `PYTHONPATH` for scanpy **and** anndata won't allow importing scanpy because `importlib_metadata.PackageNotFoundError: anndata`. This might be windows specific problems, but `pip setup.py develop` worked perfectly for me.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1377
https://github.com/scverse/scanpy/pull/1377:161,modifiability,Pac,PackageNotFoundError,161,"`flit install --deps none -s` breaks `conda list` for me. using `PYTHONPATH` for scanpy **and** anndata won't allow importing scanpy because `importlib_metadata.PackageNotFoundError: anndata`. This might be windows specific problems, but `pip setup.py develop` worked perfectly for me.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1377
https://github.com/scverse/scanpy/pull/1377:426,availability,error,error,426,"Thats weird, but that might be another issue, please check out #1378. /edit: seems to be a conda bug that only occurs on windows due to flit ([legally](https://www.python.org/dev/peps/pep-0376/#record)) writing windows newlines into the RECORD file, and conda reading them as two newlines each and then crashing. ---. This PR adds instructions on how to integrate with conda, which I screenshotted. It fails for me with this error:. ```. Collecting package metadata (repodata.json): done. Solving environment: failed. ResolvePackageNotFound: . - loompy[version='>=3.0.5']. ```. But since loompy 3.x isnt on conda-forge, thats correct. Seems that resolving anndatas dependencies on conda is currently not possible and you need to wait until loompy gets upgraded on conda-forge. Or until Quansight-Labs/beni#3 is resolved and you can specify that you dont want all deps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1377
https://github.com/scverse/scanpy/pull/1377:355,deployability,integr,integrate,355,"Thats weird, but that might be another issue, please check out #1378. /edit: seems to be a conda bug that only occurs on windows due to flit ([legally](https://www.python.org/dev/peps/pep-0376/#record)) writing windows newlines into the RECORD file, and conda reading them as two newlines each and then crashing. ---. This PR adds instructions on how to integrate with conda, which I screenshotted. It fails for me with this error:. ```. Collecting package metadata (repodata.json): done. Solving environment: failed. ResolvePackageNotFound: . - loompy[version='>=3.0.5']. ```. But since loompy 3.x isnt on conda-forge, thats correct. Seems that resolving anndatas dependencies on conda is currently not possible and you need to wait until loompy gets upgraded on conda-forge. Or until Quansight-Labs/beni#3 is resolved and you can specify that you dont want all deps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1377
https://github.com/scverse/scanpy/pull/1377:403,deployability,fail,fails,403,"Thats weird, but that might be another issue, please check out #1378. /edit: seems to be a conda bug that only occurs on windows due to flit ([legally](https://www.python.org/dev/peps/pep-0376/#record)) writing windows newlines into the RECORD file, and conda reading them as two newlines each and then crashing. ---. This PR adds instructions on how to integrate with conda, which I screenshotted. It fails for me with this error:. ```. Collecting package metadata (repodata.json): done. Solving environment: failed. ResolvePackageNotFound: . - loompy[version='>=3.0.5']. ```. But since loompy 3.x isnt on conda-forge, thats correct. Seems that resolving anndatas dependencies on conda is currently not possible and you need to wait until loompy gets upgraded on conda-forge. Or until Quansight-Labs/beni#3 is resolved and you can specify that you dont want all deps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1377
https://github.com/scverse/scanpy/pull/1377:511,deployability,fail,failed,511,"Thats weird, but that might be another issue, please check out #1378. /edit: seems to be a conda bug that only occurs on windows due to flit ([legally](https://www.python.org/dev/peps/pep-0376/#record)) writing windows newlines into the RECORD file, and conda reading them as two newlines each and then crashing. ---. This PR adds instructions on how to integrate with conda, which I screenshotted. It fails for me with this error:. ```. Collecting package metadata (repodata.json): done. Solving environment: failed. ResolvePackageNotFound: . - loompy[version='>=3.0.5']. ```. But since loompy 3.x isnt on conda-forge, thats correct. Seems that resolving anndatas dependencies on conda is currently not possible and you need to wait until loompy gets upgraded on conda-forge. Or until Quansight-Labs/beni#3 is resolved and you can specify that you dont want all deps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1377
https://github.com/scverse/scanpy/pull/1377:554,deployability,version,version,554,"Thats weird, but that might be another issue, please check out #1378. /edit: seems to be a conda bug that only occurs on windows due to flit ([legally](https://www.python.org/dev/peps/pep-0376/#record)) writing windows newlines into the RECORD file, and conda reading them as two newlines each and then crashing. ---. This PR adds instructions on how to integrate with conda, which I screenshotted. It fails for me with this error:. ```. Collecting package metadata (repodata.json): done. Solving environment: failed. ResolvePackageNotFound: . - loompy[version='>=3.0.5']. ```. But since loompy 3.x isnt on conda-forge, thats correct. Seems that resolving anndatas dependencies on conda is currently not possible and you need to wait until loompy gets upgraded on conda-forge. Or until Quansight-Labs/beni#3 is resolved and you can specify that you dont want all deps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1377
https://github.com/scverse/scanpy/pull/1377:669,deployability,depend,dependencies,669,"Thats weird, but that might be another issue, please check out #1378. /edit: seems to be a conda bug that only occurs on windows due to flit ([legally](https://www.python.org/dev/peps/pep-0376/#record)) writing windows newlines into the RECORD file, and conda reading them as two newlines each and then crashing. ---. This PR adds instructions on how to integrate with conda, which I screenshotted. It fails for me with this error:. ```. Collecting package metadata (repodata.json): done. Solving environment: failed. ResolvePackageNotFound: . - loompy[version='>=3.0.5']. ```. But since loompy 3.x isnt on conda-forge, thats correct. Seems that resolving anndatas dependencies on conda is currently not possible and you need to wait until loompy gets upgraded on conda-forge. Or until Quansight-Labs/beni#3 is resolved and you can specify that you dont want all deps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1377
https://github.com/scverse/scanpy/pull/1377:756,deployability,upgrad,upgraded,756,"Thats weird, but that might be another issue, please check out #1378. /edit: seems to be a conda bug that only occurs on windows due to flit ([legally](https://www.python.org/dev/peps/pep-0376/#record)) writing windows newlines into the RECORD file, and conda reading them as two newlines each and then crashing. ---. This PR adds instructions on how to integrate with conda, which I screenshotted. It fails for me with this error:. ```. Collecting package metadata (repodata.json): done. Solving environment: failed. ResolvePackageNotFound: . - loompy[version='>=3.0.5']. ```. But since loompy 3.x isnt on conda-forge, thats correct. Seems that resolving anndatas dependencies on conda is currently not possible and you need to wait until loompy gets upgraded on conda-forge. Or until Quansight-Labs/beni#3 is resolved and you can specify that you dont want all deps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1377
https://github.com/scverse/scanpy/pull/1377:694,energy efficiency,current,currently,694,"Thats weird, but that might be another issue, please check out #1378. /edit: seems to be a conda bug that only occurs on windows due to flit ([legally](https://www.python.org/dev/peps/pep-0376/#record)) writing windows newlines into the RECORD file, and conda reading them as two newlines each and then crashing. ---. This PR adds instructions on how to integrate with conda, which I screenshotted. It fails for me with this error:. ```. Collecting package metadata (repodata.json): done. Solving environment: failed. ResolvePackageNotFound: . - loompy[version='>=3.0.5']. ```. But since loompy 3.x isnt on conda-forge, thats correct. Seems that resolving anndatas dependencies on conda is currently not possible and you need to wait until loompy gets upgraded on conda-forge. Or until Quansight-Labs/beni#3 is resolved and you can specify that you dont want all deps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1377
https://github.com/scverse/scanpy/pull/1377:355,integrability,integr,integrate,355,"Thats weird, but that might be another issue, please check out #1378. /edit: seems to be a conda bug that only occurs on windows due to flit ([legally](https://www.python.org/dev/peps/pep-0376/#record)) writing windows newlines into the RECORD file, and conda reading them as two newlines each and then crashing. ---. This PR adds instructions on how to integrate with conda, which I screenshotted. It fails for me with this error:. ```. Collecting package metadata (repodata.json): done. Solving environment: failed. ResolvePackageNotFound: . - loompy[version='>=3.0.5']. ```. But since loompy 3.x isnt on conda-forge, thats correct. Seems that resolving anndatas dependencies on conda is currently not possible and you need to wait until loompy gets upgraded on conda-forge. Or until Quansight-Labs/beni#3 is resolved and you can specify that you dont want all deps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1377
https://github.com/scverse/scanpy/pull/1377:554,integrability,version,version,554,"Thats weird, but that might be another issue, please check out #1378. /edit: seems to be a conda bug that only occurs on windows due to flit ([legally](https://www.python.org/dev/peps/pep-0376/#record)) writing windows newlines into the RECORD file, and conda reading them as two newlines each and then crashing. ---. This PR adds instructions on how to integrate with conda, which I screenshotted. It fails for me with this error:. ```. Collecting package metadata (repodata.json): done. Solving environment: failed. ResolvePackageNotFound: . - loompy[version='>=3.0.5']. ```. But since loompy 3.x isnt on conda-forge, thats correct. Seems that resolving anndatas dependencies on conda is currently not possible and you need to wait until loompy gets upgraded on conda-forge. Or until Quansight-Labs/beni#3 is resolved and you can specify that you dont want all deps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1377
https://github.com/scverse/scanpy/pull/1377:669,integrability,depend,dependencies,669,"Thats weird, but that might be another issue, please check out #1378. /edit: seems to be a conda bug that only occurs on windows due to flit ([legally](https://www.python.org/dev/peps/pep-0376/#record)) writing windows newlines into the RECORD file, and conda reading them as two newlines each and then crashing. ---. This PR adds instructions on how to integrate with conda, which I screenshotted. It fails for me with this error:. ```. Collecting package metadata (repodata.json): done. Solving environment: failed. ResolvePackageNotFound: . - loompy[version='>=3.0.5']. ```. But since loompy 3.x isnt on conda-forge, thats correct. Seems that resolving anndatas dependencies on conda is currently not possible and you need to wait until loompy gets upgraded on conda-forge. Or until Quansight-Labs/beni#3 is resolved and you can specify that you dont want all deps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1377
https://github.com/scverse/scanpy/pull/1377:355,interoperability,integr,integrate,355,"Thats weird, but that might be another issue, please check out #1378. /edit: seems to be a conda bug that only occurs on windows due to flit ([legally](https://www.python.org/dev/peps/pep-0376/#record)) writing windows newlines into the RECORD file, and conda reading them as two newlines each and then crashing. ---. This PR adds instructions on how to integrate with conda, which I screenshotted. It fails for me with this error:. ```. Collecting package metadata (repodata.json): done. Solving environment: failed. ResolvePackageNotFound: . - loompy[version='>=3.0.5']. ```. But since loompy 3.x isnt on conda-forge, thats correct. Seems that resolving anndatas dependencies on conda is currently not possible and you need to wait until loompy gets upgraded on conda-forge. Or until Quansight-Labs/beni#3 is resolved and you can specify that you dont want all deps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1377
https://github.com/scverse/scanpy/pull/1377:836,interoperability,specif,specify,836,"Thats weird, but that might be another issue, please check out #1378. /edit: seems to be a conda bug that only occurs on windows due to flit ([legally](https://www.python.org/dev/peps/pep-0376/#record)) writing windows newlines into the RECORD file, and conda reading them as two newlines each and then crashing. ---. This PR adds instructions on how to integrate with conda, which I screenshotted. It fails for me with this error:. ```. Collecting package metadata (repodata.json): done. Solving environment: failed. ResolvePackageNotFound: . - loompy[version='>=3.0.5']. ```. But since loompy 3.x isnt on conda-forge, thats correct. Seems that resolving anndatas dependencies on conda is currently not possible and you need to wait until loompy gets upgraded on conda-forge. Or until Quansight-Labs/beni#3 is resolved and you can specify that you dont want all deps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1377
https://github.com/scverse/scanpy/pull/1377:355,modifiability,integr,integrate,355,"Thats weird, but that might be another issue, please check out #1378. /edit: seems to be a conda bug that only occurs on windows due to flit ([legally](https://www.python.org/dev/peps/pep-0376/#record)) writing windows newlines into the RECORD file, and conda reading them as two newlines each and then crashing. ---. This PR adds instructions on how to integrate with conda, which I screenshotted. It fails for me with this error:. ```. Collecting package metadata (repodata.json): done. Solving environment: failed. ResolvePackageNotFound: . - loompy[version='>=3.0.5']. ```. But since loompy 3.x isnt on conda-forge, thats correct. Seems that resolving anndatas dependencies on conda is currently not possible and you need to wait until loompy gets upgraded on conda-forge. Or until Quansight-Labs/beni#3 is resolved and you can specify that you dont want all deps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1377
https://github.com/scverse/scanpy/pull/1377:450,modifiability,pac,package,450,"Thats weird, but that might be another issue, please check out #1378. /edit: seems to be a conda bug that only occurs on windows due to flit ([legally](https://www.python.org/dev/peps/pep-0376/#record)) writing windows newlines into the RECORD file, and conda reading them as two newlines each and then crashing. ---. This PR adds instructions on how to integrate with conda, which I screenshotted. It fails for me with this error:. ```. Collecting package metadata (repodata.json): done. Solving environment: failed. ResolvePackageNotFound: . - loompy[version='>=3.0.5']. ```. But since loompy 3.x isnt on conda-forge, thats correct. Seems that resolving anndatas dependencies on conda is currently not possible and you need to wait until loompy gets upgraded on conda-forge. Or until Quansight-Labs/beni#3 is resolved and you can specify that you dont want all deps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1377
https://github.com/scverse/scanpy/pull/1377:554,modifiability,version,version,554,"Thats weird, but that might be another issue, please check out #1378. /edit: seems to be a conda bug that only occurs on windows due to flit ([legally](https://www.python.org/dev/peps/pep-0376/#record)) writing windows newlines into the RECORD file, and conda reading them as two newlines each and then crashing. ---. This PR adds instructions on how to integrate with conda, which I screenshotted. It fails for me with this error:. ```. Collecting package metadata (repodata.json): done. Solving environment: failed. ResolvePackageNotFound: . - loompy[version='>=3.0.5']. ```. But since loompy 3.x isnt on conda-forge, thats correct. Seems that resolving anndatas dependencies on conda is currently not possible and you need to wait until loompy gets upgraded on conda-forge. Or until Quansight-Labs/beni#3 is resolved and you can specify that you dont want all deps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1377
https://github.com/scverse/scanpy/pull/1377:669,modifiability,depend,dependencies,669,"Thats weird, but that might be another issue, please check out #1378. /edit: seems to be a conda bug that only occurs on windows due to flit ([legally](https://www.python.org/dev/peps/pep-0376/#record)) writing windows newlines into the RECORD file, and conda reading them as two newlines each and then crashing. ---. This PR adds instructions on how to integrate with conda, which I screenshotted. It fails for me with this error:. ```. Collecting package metadata (repodata.json): done. Solving environment: failed. ResolvePackageNotFound: . - loompy[version='>=3.0.5']. ```. But since loompy 3.x isnt on conda-forge, thats correct. Seems that resolving anndatas dependencies on conda is currently not possible and you need to wait until loompy gets upgraded on conda-forge. Or until Quansight-Labs/beni#3 is resolved and you can specify that you dont want all deps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1377
https://github.com/scverse/scanpy/pull/1377:756,modifiability,upgrad,upgraded,756,"Thats weird, but that might be another issue, please check out #1378. /edit: seems to be a conda bug that only occurs on windows due to flit ([legally](https://www.python.org/dev/peps/pep-0376/#record)) writing windows newlines into the RECORD file, and conda reading them as two newlines each and then crashing. ---. This PR adds instructions on how to integrate with conda, which I screenshotted. It fails for me with this error:. ```. Collecting package metadata (repodata.json): done. Solving environment: failed. ResolvePackageNotFound: . - loompy[version='>=3.0.5']. ```. But since loompy 3.x isnt on conda-forge, thats correct. Seems that resolving anndatas dependencies on conda is currently not possible and you need to wait until loompy gets upgraded on conda-forge. Or until Quansight-Labs/beni#3 is resolved and you can specify that you dont want all deps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1377
https://github.com/scverse/scanpy/pull/1377:426,performance,error,error,426,"Thats weird, but that might be another issue, please check out #1378. /edit: seems to be a conda bug that only occurs on windows due to flit ([legally](https://www.python.org/dev/peps/pep-0376/#record)) writing windows newlines into the RECORD file, and conda reading them as two newlines each and then crashing. ---. This PR adds instructions on how to integrate with conda, which I screenshotted. It fails for me with this error:. ```. Collecting package metadata (repodata.json): done. Solving environment: failed. ResolvePackageNotFound: . - loompy[version='>=3.0.5']. ```. But since loompy 3.x isnt on conda-forge, thats correct. Seems that resolving anndatas dependencies on conda is currently not possible and you need to wait until loompy gets upgraded on conda-forge. Or until Quansight-Labs/beni#3 is resolved and you can specify that you dont want all deps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1377
https://github.com/scverse/scanpy/pull/1377:355,reliability,integr,integrate,355,"Thats weird, but that might be another issue, please check out #1378. /edit: seems to be a conda bug that only occurs on windows due to flit ([legally](https://www.python.org/dev/peps/pep-0376/#record)) writing windows newlines into the RECORD file, and conda reading them as two newlines each and then crashing. ---. This PR adds instructions on how to integrate with conda, which I screenshotted. It fails for me with this error:. ```. Collecting package metadata (repodata.json): done. Solving environment: failed. ResolvePackageNotFound: . - loompy[version='>=3.0.5']. ```. But since loompy 3.x isnt on conda-forge, thats correct. Seems that resolving anndatas dependencies on conda is currently not possible and you need to wait until loompy gets upgraded on conda-forge. Or until Quansight-Labs/beni#3 is resolved and you can specify that you dont want all deps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1377
https://github.com/scverse/scanpy/pull/1377:403,reliability,fail,fails,403,"Thats weird, but that might be another issue, please check out #1378. /edit: seems to be a conda bug that only occurs on windows due to flit ([legally](https://www.python.org/dev/peps/pep-0376/#record)) writing windows newlines into the RECORD file, and conda reading them as two newlines each and then crashing. ---. This PR adds instructions on how to integrate with conda, which I screenshotted. It fails for me with this error:. ```. Collecting package metadata (repodata.json): done. Solving environment: failed. ResolvePackageNotFound: . - loompy[version='>=3.0.5']. ```. But since loompy 3.x isnt on conda-forge, thats correct. Seems that resolving anndatas dependencies on conda is currently not possible and you need to wait until loompy gets upgraded on conda-forge. Or until Quansight-Labs/beni#3 is resolved and you can specify that you dont want all deps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1377
https://github.com/scverse/scanpy/pull/1377:511,reliability,fail,failed,511,"Thats weird, but that might be another issue, please check out #1378. /edit: seems to be a conda bug that only occurs on windows due to flit ([legally](https://www.python.org/dev/peps/pep-0376/#record)) writing windows newlines into the RECORD file, and conda reading them as two newlines each and then crashing. ---. This PR adds instructions on how to integrate with conda, which I screenshotted. It fails for me with this error:. ```. Collecting package metadata (repodata.json): done. Solving environment: failed. ResolvePackageNotFound: . - loompy[version='>=3.0.5']. ```. But since loompy 3.x isnt on conda-forge, thats correct. Seems that resolving anndatas dependencies on conda is currently not possible and you need to wait until loompy gets upgraded on conda-forge. Or until Quansight-Labs/beni#3 is resolved and you can specify that you dont want all deps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1377
https://github.com/scverse/scanpy/pull/1377:426,safety,error,error,426,"Thats weird, but that might be another issue, please check out #1378. /edit: seems to be a conda bug that only occurs on windows due to flit ([legally](https://www.python.org/dev/peps/pep-0376/#record)) writing windows newlines into the RECORD file, and conda reading them as two newlines each and then crashing. ---. This PR adds instructions on how to integrate with conda, which I screenshotted. It fails for me with this error:. ```. Collecting package metadata (repodata.json): done. Solving environment: failed. ResolvePackageNotFound: . - loompy[version='>=3.0.5']. ```. But since loompy 3.x isnt on conda-forge, thats correct. Seems that resolving anndatas dependencies on conda is currently not possible and you need to wait until loompy gets upgraded on conda-forge. Or until Quansight-Labs/beni#3 is resolved and you can specify that you dont want all deps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1377
https://github.com/scverse/scanpy/pull/1377:669,safety,depend,dependencies,669,"Thats weird, but that might be another issue, please check out #1378. /edit: seems to be a conda bug that only occurs on windows due to flit ([legally](https://www.python.org/dev/peps/pep-0376/#record)) writing windows newlines into the RECORD file, and conda reading them as two newlines each and then crashing. ---. This PR adds instructions on how to integrate with conda, which I screenshotted. It fails for me with this error:. ```. Collecting package metadata (repodata.json): done. Solving environment: failed. ResolvePackageNotFound: . - loompy[version='>=3.0.5']. ```. But since loompy 3.x isnt on conda-forge, thats correct. Seems that resolving anndatas dependencies on conda is currently not possible and you need to wait until loompy gets upgraded on conda-forge. Or until Quansight-Labs/beni#3 is resolved and you can specify that you dont want all deps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1377
https://github.com/scverse/scanpy/pull/1377:355,security,integr,integrate,355,"Thats weird, but that might be another issue, please check out #1378. /edit: seems to be a conda bug that only occurs on windows due to flit ([legally](https://www.python.org/dev/peps/pep-0376/#record)) writing windows newlines into the RECORD file, and conda reading them as two newlines each and then crashing. ---. This PR adds instructions on how to integrate with conda, which I screenshotted. It fails for me with this error:. ```. Collecting package metadata (repodata.json): done. Solving environment: failed. ResolvePackageNotFound: . - loompy[version='>=3.0.5']. ```. But since loompy 3.x isnt on conda-forge, thats correct. Seems that resolving anndatas dependencies on conda is currently not possible and you need to wait until loompy gets upgraded on conda-forge. Or until Quansight-Labs/beni#3 is resolved and you can specify that you dont want all deps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1377
https://github.com/scverse/scanpy/pull/1377:355,testability,integr,integrate,355,"Thats weird, but that might be another issue, please check out #1378. /edit: seems to be a conda bug that only occurs on windows due to flit ([legally](https://www.python.org/dev/peps/pep-0376/#record)) writing windows newlines into the RECORD file, and conda reading them as two newlines each and then crashing. ---. This PR adds instructions on how to integrate with conda, which I screenshotted. It fails for me with this error:. ```. Collecting package metadata (repodata.json): done. Solving environment: failed. ResolvePackageNotFound: . - loompy[version='>=3.0.5']. ```. But since loompy 3.x isnt on conda-forge, thats correct. Seems that resolving anndatas dependencies on conda is currently not possible and you need to wait until loompy gets upgraded on conda-forge. Or until Quansight-Labs/beni#3 is resolved and you can specify that you dont want all deps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1377
https://github.com/scverse/scanpy/pull/1377:669,testability,depend,dependencies,669,"Thats weird, but that might be another issue, please check out #1378. /edit: seems to be a conda bug that only occurs on windows due to flit ([legally](https://www.python.org/dev/peps/pep-0376/#record)) writing windows newlines into the RECORD file, and conda reading them as two newlines each and then crashing. ---. This PR adds instructions on how to integrate with conda, which I screenshotted. It fails for me with this error:. ```. Collecting package metadata (repodata.json): done. Solving environment: failed. ResolvePackageNotFound: . - loompy[version='>=3.0.5']. ```. But since loompy 3.x isnt on conda-forge, thats correct. Seems that resolving anndatas dependencies on conda is currently not possible and you need to wait until loompy gets upgraded on conda-forge. Or until Quansight-Labs/beni#3 is resolved and you can specify that you dont want all deps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1377
https://github.com/scverse/scanpy/pull/1377:426,usability,error,error,426,"Thats weird, but that might be another issue, please check out #1378. /edit: seems to be a conda bug that only occurs on windows due to flit ([legally](https://www.python.org/dev/peps/pep-0376/#record)) writing windows newlines into the RECORD file, and conda reading them as two newlines each and then crashing. ---. This PR adds instructions on how to integrate with conda, which I screenshotted. It fails for me with this error:. ```. Collecting package metadata (repodata.json): done. Solving environment: failed. ResolvePackageNotFound: . - loompy[version='>=3.0.5']. ```. But since loompy 3.x isnt on conda-forge, thats correct. Seems that resolving anndatas dependencies on conda is currently not possible and you need to wait until loompy gets upgraded on conda-forge. Or until Quansight-Labs/beni#3 is resolved and you can specify that you dont want all deps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1377
https://github.com/scverse/scanpy/pull/1378:314,availability,error,error,314,"This solves the problem with PYTHONPATH approach (without flit). The problem with flit is:. I didn't want to create a new environment or get my conda packages accidentally replaced by installations from pip, so i tried. `flit install --deps none -s` and `flit install --pth-file --deps none` and received the same error after running `conda list`. It has been reported [here](https://github.com/conda/conda/issues/9074) already. Yes, it has dist-info there. Importing works fine with the flit installed packages, but i also want to be able to use `conda list`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1378
https://github.com/scverse/scanpy/pull/1378:184,deployability,instal,installations,184,"This solves the problem with PYTHONPATH approach (without flit). The problem with flit is:. I didn't want to create a new environment or get my conda packages accidentally replaced by installations from pip, so i tried. `flit install --deps none -s` and `flit install --pth-file --deps none` and received the same error after running `conda list`. It has been reported [here](https://github.com/conda/conda/issues/9074) already. Yes, it has dist-info there. Importing works fine with the flit installed packages, but i also want to be able to use `conda list`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1378
https://github.com/scverse/scanpy/pull/1378:226,deployability,instal,install,226,"This solves the problem with PYTHONPATH approach (without flit). The problem with flit is:. I didn't want to create a new environment or get my conda packages accidentally replaced by installations from pip, so i tried. `flit install --deps none -s` and `flit install --pth-file --deps none` and received the same error after running `conda list`. It has been reported [here](https://github.com/conda/conda/issues/9074) already. Yes, it has dist-info there. Importing works fine with the flit installed packages, but i also want to be able to use `conda list`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1378
https://github.com/scverse/scanpy/pull/1378:260,deployability,instal,install,260,"This solves the problem with PYTHONPATH approach (without flit). The problem with flit is:. I didn't want to create a new environment or get my conda packages accidentally replaced by installations from pip, so i tried. `flit install --deps none -s` and `flit install --pth-file --deps none` and received the same error after running `conda list`. It has been reported [here](https://github.com/conda/conda/issues/9074) already. Yes, it has dist-info there. Importing works fine with the flit installed packages, but i also want to be able to use `conda list`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1378
https://github.com/scverse/scanpy/pull/1378:493,deployability,instal,installed,493,"This solves the problem with PYTHONPATH approach (without flit). The problem with flit is:. I didn't want to create a new environment or get my conda packages accidentally replaced by installations from pip, so i tried. `flit install --deps none -s` and `flit install --pth-file --deps none` and received the same error after running `conda list`. It has been reported [here](https://github.com/conda/conda/issues/9074) already. Yes, it has dist-info there. Importing works fine with the flit installed packages, but i also want to be able to use `conda list`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1378
https://github.com/scverse/scanpy/pull/1378:150,modifiability,pac,packages,150,"This solves the problem with PYTHONPATH approach (without flit). The problem with flit is:. I didn't want to create a new environment or get my conda packages accidentally replaced by installations from pip, so i tried. `flit install --deps none -s` and `flit install --pth-file --deps none` and received the same error after running `conda list`. It has been reported [here](https://github.com/conda/conda/issues/9074) already. Yes, it has dist-info there. Importing works fine with the flit installed packages, but i also want to be able to use `conda list`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1378
https://github.com/scverse/scanpy/pull/1378:503,modifiability,pac,packages,503,"This solves the problem with PYTHONPATH approach (without flit). The problem with flit is:. I didn't want to create a new environment or get my conda packages accidentally replaced by installations from pip, so i tried. `flit install --deps none -s` and `flit install --pth-file --deps none` and received the same error after running `conda list`. It has been reported [here](https://github.com/conda/conda/issues/9074) already. Yes, it has dist-info there. Importing works fine with the flit installed packages, but i also want to be able to use `conda list`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1378
https://github.com/scverse/scanpy/pull/1378:314,performance,error,error,314,"This solves the problem with PYTHONPATH approach (without flit). The problem with flit is:. I didn't want to create a new environment or get my conda packages accidentally replaced by installations from pip, so i tried. `flit install --deps none -s` and `flit install --pth-file --deps none` and received the same error after running `conda list`. It has been reported [here](https://github.com/conda/conda/issues/9074) already. Yes, it has dist-info there. Importing works fine with the flit installed packages, but i also want to be able to use `conda list`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1378
https://github.com/scverse/scanpy/pull/1378:159,safety,accid,accidentally,159,"This solves the problem with PYTHONPATH approach (without flit). The problem with flit is:. I didn't want to create a new environment or get my conda packages accidentally replaced by installations from pip, so i tried. `flit install --deps none -s` and `flit install --pth-file --deps none` and received the same error after running `conda list`. It has been reported [here](https://github.com/conda/conda/issues/9074) already. Yes, it has dist-info there. Importing works fine with the flit installed packages, but i also want to be able to use `conda list`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1378
https://github.com/scverse/scanpy/pull/1378:314,safety,error,error,314,"This solves the problem with PYTHONPATH approach (without flit). The problem with flit is:. I didn't want to create a new environment or get my conda packages accidentally replaced by installations from pip, so i tried. `flit install --deps none -s` and `flit install --pth-file --deps none` and received the same error after running `conda list`. It has been reported [here](https://github.com/conda/conda/issues/9074) already. Yes, it has dist-info there. Importing works fine with the flit installed packages, but i also want to be able to use `conda list`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1378
https://github.com/scverse/scanpy/pull/1378:314,usability,error,error,314,"This solves the problem with PYTHONPATH approach (without flit). The problem with flit is:. I didn't want to create a new environment or get my conda packages accidentally replaced by installations from pip, so i tried. `flit install --deps none -s` and `flit install --pth-file --deps none` and received the same error after running `conda list`. It has been reported [here](https://github.com/conda/conda/issues/9074) already. Yes, it has dist-info there. Importing works fine with the flit installed packages, but i also want to be able to use `conda list`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1378
https://github.com/scverse/scanpy/pull/1378:123,deployability,Version,Version,123,"Sorry, cant reproduce:. ```console. $ conda list. # packages in environment at /home/phil/.conda/envs/anndata:. #. # Name Version Build Channel. _libgcc_mutex 0.1 conda_forge conda-forge. _openmp_mutex 4.5 1_gnu conda-forge. alabaster 0.7.12 py_0 conda-forge. anndata 0.7.5.dev9+gd32f11b pypi_0 pypi. ... ```. Seems like its a windows-specific Conda bug, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1378
https://github.com/scverse/scanpy/pull/1378:131,deployability,Build,Build,131,"Sorry, cant reproduce:. ```console. $ conda list. # packages in environment at /home/phil/.conda/envs/anndata:. #. # Name Version Build Channel. _libgcc_mutex 0.1 conda_forge conda-forge. _openmp_mutex 4.5 1_gnu conda-forge. alabaster 0.7.12 py_0 conda-forge. anndata 0.7.5.dev9+gd32f11b pypi_0 pypi. ... ```. Seems like its a windows-specific Conda bug, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1378
https://github.com/scverse/scanpy/pull/1378:123,integrability,Version,Version,123,"Sorry, cant reproduce:. ```console. $ conda list. # packages in environment at /home/phil/.conda/envs/anndata:. #. # Name Version Build Channel. _libgcc_mutex 0.1 conda_forge conda-forge. _openmp_mutex 4.5 1_gnu conda-forge. alabaster 0.7.12 py_0 conda-forge. anndata 0.7.5.dev9+gd32f11b pypi_0 pypi. ... ```. Seems like its a windows-specific Conda bug, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1378
https://github.com/scverse/scanpy/pull/1378:337,interoperability,specif,specific,337,"Sorry, cant reproduce:. ```console. $ conda list. # packages in environment at /home/phil/.conda/envs/anndata:. #. # Name Version Build Channel. _libgcc_mutex 0.1 conda_forge conda-forge. _openmp_mutex 4.5 1_gnu conda-forge. alabaster 0.7.12 py_0 conda-forge. anndata 0.7.5.dev9+gd32f11b pypi_0 pypi. ... ```. Seems like its a windows-specific Conda bug, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1378
https://github.com/scverse/scanpy/pull/1378:53,modifiability,pac,packages,53,"Sorry, cant reproduce:. ```console. $ conda list. # packages in environment at /home/phil/.conda/envs/anndata:. #. # Name Version Build Channel. _libgcc_mutex 0.1 conda_forge conda-forge. _openmp_mutex 4.5 1_gnu conda-forge. alabaster 0.7.12 py_0 conda-forge. anndata 0.7.5.dev9+gd32f11b pypi_0 pypi. ... ```. Seems like its a windows-specific Conda bug, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1378
https://github.com/scverse/scanpy/pull/1378:123,modifiability,Version,Version,123,"Sorry, cant reproduce:. ```console. $ conda list. # packages in environment at /home/phil/.conda/envs/anndata:. #. # Name Version Build Channel. _libgcc_mutex 0.1 conda_forge conda-forge. _openmp_mutex 4.5 1_gnu conda-forge. alabaster 0.7.12 py_0 conda-forge. anndata 0.7.5.dev9+gd32f11b pypi_0 pypi. ... ```. Seems like its a windows-specific Conda bug, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1378
https://github.com/scverse/scanpy/pull/1378:156,safety,permiss,permissions,156,"Details: https://github.com/conda/conda/issues/9074#issuecomment-675552817. It wont fix anything, but you should be able to give yourself symlink creation permissions on windows and just use `-s`: https://docs.microsoft.com/en-us/windows/security/threat-protection/security-policy-settings/create-symbolic-links",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1378
https://github.com/scverse/scanpy/pull/1378:239,security,secur,security,239,"Details: https://github.com/conda/conda/issues/9074#issuecomment-675552817. It wont fix anything, but you should be able to give yourself symlink creation permissions on windows and just use `-s`: https://docs.microsoft.com/en-us/windows/security/threat-protection/security-policy-settings/create-symbolic-links",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1378
https://github.com/scverse/scanpy/pull/1378:248,security,threat,threat-protection,248,"Details: https://github.com/conda/conda/issues/9074#issuecomment-675552817. It wont fix anything, but you should be able to give yourself symlink creation permissions on windows and just use `-s`: https://docs.microsoft.com/en-us/windows/security/threat-protection/security-policy-settings/create-symbolic-links",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1378
https://github.com/scverse/scanpy/pull/1378:266,security,secur,security-policy-settings,266,"Details: https://github.com/conda/conda/issues/9074#issuecomment-675552817. It wont fix anything, but you should be able to give yourself symlink creation permissions on windows and just use `-s`: https://docs.microsoft.com/en-us/windows/security/threat-protection/security-policy-settings/create-symbolic-links",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1378
https://github.com/scverse/scanpy/issues/1379:113,safety,prevent,prevent,113,"@flying-sheep, can we keep the type annotation, but defer the import of `louvain` until it's called? That should prevent the warning on import of scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1379
https://github.com/scverse/scanpy/issues/1379:113,security,preven,prevent,113,"@flying-sheep, can we keep the type annotation, but defer the import of `louvain` until it's called? That should prevent the warning on import of scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1379
https://github.com/scverse/scanpy/pull/1382:21,availability,error,error,21,"Currently there's an error being raised because the following images in don't match. path: `scanpy/tests/notebooks/_images_paga_paul15_subsampled/paga_path.png`. **Expected**. ![paga_path](https://user-images.githubusercontent.com/8322751/90666060-de033280-e21a-11ea-83f9-684908586f6e.png). **Actual**. ![paga_path](https://user-images.githubusercontent.com/8322751/90666074-e2c7e680-e21a-11ea-9f08-fc495d6762b0.png). **Diff**. ![paga_path-failed-diff](https://user-images.githubusercontent.com/8322751/90666089-e78c9a80-e21a-11ea-9e0c-4e7e6a80d140.png). I'm going to update expected to match actual, but I need some help to see if this is okay.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1382
https://github.com/scverse/scanpy/pull/1382:440,deployability,fail,failed-diff,440,"Currently there's an error being raised because the following images in don't match. path: `scanpy/tests/notebooks/_images_paga_paul15_subsampled/paga_path.png`. **Expected**. ![paga_path](https://user-images.githubusercontent.com/8322751/90666060-de033280-e21a-11ea-83f9-684908586f6e.png). **Actual**. ![paga_path](https://user-images.githubusercontent.com/8322751/90666074-e2c7e680-e21a-11ea-9f08-fc495d6762b0.png). **Diff**. ![paga_path-failed-diff](https://user-images.githubusercontent.com/8322751/90666089-e78c9a80-e21a-11ea-9e0c-4e7e6a80d140.png). I'm going to update expected to match actual, but I need some help to see if this is okay.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1382
https://github.com/scverse/scanpy/pull/1382:568,deployability,updat,update,568,"Currently there's an error being raised because the following images in don't match. path: `scanpy/tests/notebooks/_images_paga_paul15_subsampled/paga_path.png`. **Expected**. ![paga_path](https://user-images.githubusercontent.com/8322751/90666060-de033280-e21a-11ea-83f9-684908586f6e.png). **Actual**. ![paga_path](https://user-images.githubusercontent.com/8322751/90666074-e2c7e680-e21a-11ea-9f08-fc495d6762b0.png). **Diff**. ![paga_path-failed-diff](https://user-images.githubusercontent.com/8322751/90666089-e78c9a80-e21a-11ea-9e0c-4e7e6a80d140.png). I'm going to update expected to match actual, but I need some help to see if this is okay.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1382
https://github.com/scverse/scanpy/pull/1382:0,energy efficiency,Current,Currently,0,"Currently there's an error being raised because the following images in don't match. path: `scanpy/tests/notebooks/_images_paga_paul15_subsampled/paga_path.png`. **Expected**. ![paga_path](https://user-images.githubusercontent.com/8322751/90666060-de033280-e21a-11ea-83f9-684908586f6e.png). **Actual**. ![paga_path](https://user-images.githubusercontent.com/8322751/90666074-e2c7e680-e21a-11ea-9f08-fc495d6762b0.png). **Diff**. ![paga_path-failed-diff](https://user-images.githubusercontent.com/8322751/90666089-e78c9a80-e21a-11ea-9e0c-4e7e6a80d140.png). I'm going to update expected to match actual, but I need some help to see if this is okay.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1382
https://github.com/scverse/scanpy/pull/1382:21,performance,error,error,21,"Currently there's an error being raised because the following images in don't match. path: `scanpy/tests/notebooks/_images_paga_paul15_subsampled/paga_path.png`. **Expected**. ![paga_path](https://user-images.githubusercontent.com/8322751/90666060-de033280-e21a-11ea-83f9-684908586f6e.png). **Actual**. ![paga_path](https://user-images.githubusercontent.com/8322751/90666074-e2c7e680-e21a-11ea-9f08-fc495d6762b0.png). **Diff**. ![paga_path-failed-diff](https://user-images.githubusercontent.com/8322751/90666089-e78c9a80-e21a-11ea-9e0c-4e7e6a80d140.png). I'm going to update expected to match actual, but I need some help to see if this is okay.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1382
https://github.com/scverse/scanpy/pull/1382:440,reliability,fail,failed-diff,440,"Currently there's an error being raised because the following images in don't match. path: `scanpy/tests/notebooks/_images_paga_paul15_subsampled/paga_path.png`. **Expected**. ![paga_path](https://user-images.githubusercontent.com/8322751/90666060-de033280-e21a-11ea-83f9-684908586f6e.png). **Actual**. ![paga_path](https://user-images.githubusercontent.com/8322751/90666074-e2c7e680-e21a-11ea-9f08-fc495d6762b0.png). **Diff**. ![paga_path-failed-diff](https://user-images.githubusercontent.com/8322751/90666089-e78c9a80-e21a-11ea-9e0c-4e7e6a80d140.png). I'm going to update expected to match actual, but I need some help to see if this is okay.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1382
https://github.com/scverse/scanpy/pull/1382:21,safety,error,error,21,"Currently there's an error being raised because the following images in don't match. path: `scanpy/tests/notebooks/_images_paga_paul15_subsampled/paga_path.png`. **Expected**. ![paga_path](https://user-images.githubusercontent.com/8322751/90666060-de033280-e21a-11ea-83f9-684908586f6e.png). **Actual**. ![paga_path](https://user-images.githubusercontent.com/8322751/90666074-e2c7e680-e21a-11ea-9f08-fc495d6762b0.png). **Diff**. ![paga_path-failed-diff](https://user-images.githubusercontent.com/8322751/90666089-e78c9a80-e21a-11ea-9e0c-4e7e6a80d140.png). I'm going to update expected to match actual, but I need some help to see if this is okay.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1382
https://github.com/scverse/scanpy/pull/1382:99,safety,test,tests,99,"Currently there's an error being raised because the following images in don't match. path: `scanpy/tests/notebooks/_images_paga_paul15_subsampled/paga_path.png`. **Expected**. ![paga_path](https://user-images.githubusercontent.com/8322751/90666060-de033280-e21a-11ea-83f9-684908586f6e.png). **Actual**. ![paga_path](https://user-images.githubusercontent.com/8322751/90666074-e2c7e680-e21a-11ea-9f08-fc495d6762b0.png). **Diff**. ![paga_path-failed-diff](https://user-images.githubusercontent.com/8322751/90666089-e78c9a80-e21a-11ea-9e0c-4e7e6a80d140.png). I'm going to update expected to match actual, but I need some help to see if this is okay.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1382
https://github.com/scverse/scanpy/pull/1382:568,safety,updat,update,568,"Currently there's an error being raised because the following images in don't match. path: `scanpy/tests/notebooks/_images_paga_paul15_subsampled/paga_path.png`. **Expected**. ![paga_path](https://user-images.githubusercontent.com/8322751/90666060-de033280-e21a-11ea-83f9-684908586f6e.png). **Actual**. ![paga_path](https://user-images.githubusercontent.com/8322751/90666074-e2c7e680-e21a-11ea-9f08-fc495d6762b0.png). **Diff**. ![paga_path-failed-diff](https://user-images.githubusercontent.com/8322751/90666089-e78c9a80-e21a-11ea-9e0c-4e7e6a80d140.png). I'm going to update expected to match actual, but I need some help to see if this is okay.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1382
https://github.com/scverse/scanpy/pull/1382:568,security,updat,update,568,"Currently there's an error being raised because the following images in don't match. path: `scanpy/tests/notebooks/_images_paga_paul15_subsampled/paga_path.png`. **Expected**. ![paga_path](https://user-images.githubusercontent.com/8322751/90666060-de033280-e21a-11ea-83f9-684908586f6e.png). **Actual**. ![paga_path](https://user-images.githubusercontent.com/8322751/90666074-e2c7e680-e21a-11ea-9f08-fc495d6762b0.png). **Diff**. ![paga_path-failed-diff](https://user-images.githubusercontent.com/8322751/90666089-e78c9a80-e21a-11ea-9e0c-4e7e6a80d140.png). I'm going to update expected to match actual, but I need some help to see if this is okay.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1382
https://github.com/scverse/scanpy/pull/1382:99,testability,test,tests,99,"Currently there's an error being raised because the following images in don't match. path: `scanpy/tests/notebooks/_images_paga_paul15_subsampled/paga_path.png`. **Expected**. ![paga_path](https://user-images.githubusercontent.com/8322751/90666060-de033280-e21a-11ea-83f9-684908586f6e.png). **Actual**. ![paga_path](https://user-images.githubusercontent.com/8322751/90666074-e2c7e680-e21a-11ea-9f08-fc495d6762b0.png). **Diff**. ![paga_path-failed-diff](https://user-images.githubusercontent.com/8322751/90666089-e78c9a80-e21a-11ea-9e0c-4e7e6a80d140.png). I'm going to update expected to match actual, but I need some help to see if this is okay.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1382
https://github.com/scverse/scanpy/pull/1382:21,usability,error,error,21,"Currently there's an error being raised because the following images in don't match. path: `scanpy/tests/notebooks/_images_paga_paul15_subsampled/paga_path.png`. **Expected**. ![paga_path](https://user-images.githubusercontent.com/8322751/90666060-de033280-e21a-11ea-83f9-684908586f6e.png). **Actual**. ![paga_path](https://user-images.githubusercontent.com/8322751/90666074-e2c7e680-e21a-11ea-9f08-fc495d6762b0.png). **Diff**. ![paga_path-failed-diff](https://user-images.githubusercontent.com/8322751/90666089-e78c9a80-e21a-11ea-9e0c-4e7e6a80d140.png). I'm going to update expected to match actual, but I need some help to see if this is okay.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1382
https://github.com/scverse/scanpy/pull/1382:197,usability,user,user-images,197,"Currently there's an error being raised because the following images in don't match. path: `scanpy/tests/notebooks/_images_paga_paul15_subsampled/paga_path.png`. **Expected**. ![paga_path](https://user-images.githubusercontent.com/8322751/90666060-de033280-e21a-11ea-83f9-684908586f6e.png). **Actual**. ![paga_path](https://user-images.githubusercontent.com/8322751/90666074-e2c7e680-e21a-11ea-9f08-fc495d6762b0.png). **Diff**. ![paga_path-failed-diff](https://user-images.githubusercontent.com/8322751/90666089-e78c9a80-e21a-11ea-9e0c-4e7e6a80d140.png). I'm going to update expected to match actual, but I need some help to see if this is okay.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1382
https://github.com/scverse/scanpy/pull/1382:324,usability,user,user-images,324,"Currently there's an error being raised because the following images in don't match. path: `scanpy/tests/notebooks/_images_paga_paul15_subsampled/paga_path.png`. **Expected**. ![paga_path](https://user-images.githubusercontent.com/8322751/90666060-de033280-e21a-11ea-83f9-684908586f6e.png). **Actual**. ![paga_path](https://user-images.githubusercontent.com/8322751/90666074-e2c7e680-e21a-11ea-9f08-fc495d6762b0.png). **Diff**. ![paga_path-failed-diff](https://user-images.githubusercontent.com/8322751/90666089-e78c9a80-e21a-11ea-9e0c-4e7e6a80d140.png). I'm going to update expected to match actual, but I need some help to see if this is okay.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1382
https://github.com/scverse/scanpy/pull/1382:461,usability,user,user-images,461,"Currently there's an error being raised because the following images in don't match. path: `scanpy/tests/notebooks/_images_paga_paul15_subsampled/paga_path.png`. **Expected**. ![paga_path](https://user-images.githubusercontent.com/8322751/90666060-de033280-e21a-11ea-83f9-684908586f6e.png). **Actual**. ![paga_path](https://user-images.githubusercontent.com/8322751/90666074-e2c7e680-e21a-11ea-9f08-fc495d6762b0.png). **Diff**. ![paga_path-failed-diff](https://user-images.githubusercontent.com/8322751/90666089-e78c9a80-e21a-11ea-9e0c-4e7e6a80d140.png). I'm going to update expected to match actual, but I need some help to see if this is okay.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1382
https://github.com/scverse/scanpy/pull/1382:617,usability,help,help,617,"Currently there's an error being raised because the following images in don't match. path: `scanpy/tests/notebooks/_images_paga_paul15_subsampled/paga_path.png`. **Expected**. ![paga_path](https://user-images.githubusercontent.com/8322751/90666060-de033280-e21a-11ea-83f9-684908586f6e.png). **Actual**. ![paga_path](https://user-images.githubusercontent.com/8322751/90666074-e2c7e680-e21a-11ea-9f08-fc495d6762b0.png). **Diff**. ![paga_path-failed-diff](https://user-images.githubusercontent.com/8322751/90666089-e78c9a80-e21a-11ea-9e0c-4e7e6a80d140.png). I'm going to update expected to match actual, but I need some help to see if this is okay.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1382
https://github.com/scverse/scanpy/pull/1382:110,modifiability,concern,concern,110,"Up to you all. I think it's neater to have the output returned in the same order as the input array. The only concern would be the overhead of sorting, but that should be minimal even for large arrays never mind the expectation that we should only be choosing a small number of cells given the intended use of the function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1382
https://github.com/scverse/scanpy/pull/1382:131,performance,overhead,overhead,131,"Up to you all. I think it's neater to have the output returned in the same order as the input array. The only concern would be the overhead of sorting, but that should be minimal even for large arrays never mind the expectation that we should only be choosing a small number of cells given the intended use of the function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1382
https://github.com/scverse/scanpy/pull/1382:88,safety,input,input,88,"Up to you all. I think it's neater to have the output returned in the same order as the input array. The only concern would be the overhead of sorting, but that should be minimal even for large arrays never mind the expectation that we should only be choosing a small number of cells given the intended use of the function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1382
https://github.com/scverse/scanpy/pull/1382:110,testability,concern,concern,110,"Up to you all. I think it's neater to have the output returned in the same order as the input array. The only concern would be the overhead of sorting, but that should be minimal even for large arrays never mind the expectation that we should only be choosing a small number of cells given the intended use of the function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1382
https://github.com/scverse/scanpy/pull/1382:88,usability,input,input,88,"Up to you all. I think it's neater to have the output returned in the same order as the input array. The only concern would be the overhead of sorting, but that should be minimal even for large arrays never mind the expectation that we should only be choosing a small number of cells given the intended use of the function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1382
https://github.com/scverse/scanpy/pull/1382:171,usability,minim,minimal,171,"Up to you all. I think it's neater to have the output returned in the same order as the input array. The only concern would be the overhead of sorting, but that should be minimal even for large arrays never mind the expectation that we should only be choosing a small number of cells given the intended use of the function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1382
https://github.com/scverse/scanpy/pull/1382:258,usability,behavi,behavior,258,"For me, if I'm sampling from my data I probably don't want to preserve any effects from the ordering. There are cases where this might be useful, but it's less frequent. I don't think it's common enough to change the default. I could be convinced about this behavior being turned on with a keyword argument. IIRC @gokceneraslan was thinking about this function. Any opinion?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1382
https://github.com/scverse/scanpy/pull/1382:201,integrability,sub,subsample,201,"I just added a shuffle option (off by default). I don't think it matters very much, but if you have an algorithm that is sensitive to the order of the input, I don't think you should be relying on the subsample function to shuffle your data. I'd rather encourage people to explicitly shuffle their data if that's necessary (like for plotting a scatterplot)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1382
https://github.com/scverse/scanpy/pull/1382:151,safety,input,input,151,"I just added a shuffle option (off by default). I don't think it matters very much, but if you have an algorithm that is sensitive to the order of the input, I don't think you should be relying on the subsample function to shuffle your data. I'd rather encourage people to explicitly shuffle their data if that's necessary (like for plotting a scatterplot)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1382
https://github.com/scverse/scanpy/pull/1382:151,usability,input,input,151,"I just added a shuffle option (off by default). I don't think it matters very much, but if you have an algorithm that is sensitive to the order of the input, I don't think you should be relying on the subsample function to shuffle your data. I'd rather encourage people to explicitly shuffle their data if that's necessary (like for plotting a scatterplot)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1382
https://github.com/scverse/scanpy/pull/1383:325,deployability,manag,managed,325,"It looks great! ![image](https://user-images.githubusercontent.com/25887487/90914608-7990d080-e3de-11ea-81ed-15fdf5c3be80.png). One first improvement could be to expose a parameters that explicitly ask for the number of rings in the neighbors (first 6 ring etc.). Btw, how do I push to this specific PR from my local repo? I managed to pull it but can't figure out a way to push here and not on my fork.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:325,energy efficiency,manag,managed,325,"It looks great! ![image](https://user-images.githubusercontent.com/25887487/90914608-7990d080-e3de-11ea-81ed-15fdf5c3be80.png). One first improvement could be to expose a parameters that explicitly ask for the number of rings in the neighbors (first 6 ring etc.). Btw, how do I push to this specific PR from my local repo? I managed to pull it but can't figure out a way to push here and not on my fork.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:291,interoperability,specif,specific,291,"It looks great! ![image](https://user-images.githubusercontent.com/25887487/90914608-7990d080-e3de-11ea-81ed-15fdf5c3be80.png). One first improvement could be to expose a parameters that explicitly ask for the number of rings in the neighbors (first 6 ring etc.). Btw, how do I push to this specific PR from my local repo? I managed to pull it but can't figure out a way to push here and not on my fork.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:171,modifiability,paramet,parameters,171,"It looks great! ![image](https://user-images.githubusercontent.com/25887487/90914608-7990d080-e3de-11ea-81ed-15fdf5c3be80.png). One first improvement could be to expose a parameters that explicitly ask for the number of rings in the neighbors (first 6 ring etc.). Btw, how do I push to this specific PR from my local repo? I managed to pull it but can't figure out a way to push here and not on my fork.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:325,safety,manag,managed,325,"It looks great! ![image](https://user-images.githubusercontent.com/25887487/90914608-7990d080-e3de-11ea-81ed-15fdf5c3be80.png). One first improvement could be to expose a parameters that explicitly ask for the number of rings in the neighbors (first 6 ring etc.). Btw, how do I push to this specific PR from my local repo? I managed to pull it but can't figure out a way to push here and not on my fork.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:162,security,expos,expose,162,"It looks great! ![image](https://user-images.githubusercontent.com/25887487/90914608-7990d080-e3de-11ea-81ed-15fdf5c3be80.png). One first improvement could be to expose a parameters that explicitly ask for the number of rings in the neighbors (first 6 ring etc.). Btw, how do I push to this specific PR from my local repo? I managed to pull it but can't figure out a way to push here and not on my fork.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:33,usability,user,user-images,33,"It looks great! ![image](https://user-images.githubusercontent.com/25887487/90914608-7990d080-e3de-11ea-81ed-15fdf5c3be80.png). One first improvement could be to expose a parameters that explicitly ask for the number of rings in the neighbors (first 6 ring etc.). Btw, how do I push to this specific PR from my local repo? I managed to pull it but can't figure out a way to push here and not on my fork.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:110,availability,cluster,clustering,110,"Oh, sorry, I had completely missed your comment here! > It looks great! Thanks! Can I ask why you used leiden clustering on this? > One first improvement could be to expose a parameters that explicitly ask for the number of rings in the neighbors. This should be easy enough. I'm curious as to whether this it's better to leave this up to whatever algorithm is being used however, since the one step graph has some nice properties. It'd probably be important to include distance in the multistep graph. > Btw, how do I push to this specific PR from my local repo? This should be fairly straight forward. If you're using the github cli, I think it should just be:. ```sh. gh pr checkout 1383. # whatever changes. git push. ```. Let me know if that gives you errors, since it might be a repository permissions issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:757,availability,error,errors,757,"Oh, sorry, I had completely missed your comment here! > It looks great! Thanks! Can I ask why you used leiden clustering on this? > One first improvement could be to expose a parameters that explicitly ask for the number of rings in the neighbors. This should be easy enough. I'm curious as to whether this it's better to leave this up to whatever algorithm is being used however, since the one step graph has some nice properties. It'd probably be important to include distance in the multistep graph. > Btw, how do I push to this specific PR from my local repo? This should be fairly straight forward. If you're using the github cli, I think it should just be:. ```sh. gh pr checkout 1383. # whatever changes. git push. ```. Let me know if that gives you errors, since it might be a repository permissions issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:110,deployability,cluster,clustering,110,"Oh, sorry, I had completely missed your comment here! > It looks great! Thanks! Can I ask why you used leiden clustering on this? > One first improvement could be to expose a parameters that explicitly ask for the number of rings in the neighbors. This should be easy enough. I'm curious as to whether this it's better to leave this up to whatever algorithm is being used however, since the one step graph has some nice properties. It'd probably be important to include distance in the multistep graph. > Btw, how do I push to this specific PR from my local repo? This should be fairly straight forward. If you're using the github cli, I think it should just be:. ```sh. gh pr checkout 1383. # whatever changes. git push. ```. Let me know if that gives you errors, since it might be a repository permissions issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:785,integrability,repositor,repository,785,"Oh, sorry, I had completely missed your comment here! > It looks great! Thanks! Can I ask why you used leiden clustering on this? > One first improvement could be to expose a parameters that explicitly ask for the number of rings in the neighbors. This should be easy enough. I'm curious as to whether this it's better to leave this up to whatever algorithm is being used however, since the one step graph has some nice properties. It'd probably be important to include distance in the multistep graph. > Btw, how do I push to this specific PR from my local repo? This should be fairly straight forward. If you're using the github cli, I think it should just be:. ```sh. gh pr checkout 1383. # whatever changes. git push. ```. Let me know if that gives you errors, since it might be a repository permissions issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:532,interoperability,specif,specific,532,"Oh, sorry, I had completely missed your comment here! > It looks great! Thanks! Can I ask why you used leiden clustering on this? > One first improvement could be to expose a parameters that explicitly ask for the number of rings in the neighbors. This should be easy enough. I'm curious as to whether this it's better to leave this up to whatever algorithm is being used however, since the one step graph has some nice properties. It'd probably be important to include distance in the multistep graph. > Btw, how do I push to this specific PR from my local repo? This should be fairly straight forward. If you're using the github cli, I think it should just be:. ```sh. gh pr checkout 1383. # whatever changes. git push. ```. Let me know if that gives you errors, since it might be a repository permissions issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:785,interoperability,repositor,repository,785,"Oh, sorry, I had completely missed your comment here! > It looks great! Thanks! Can I ask why you used leiden clustering on this? > One first improvement could be to expose a parameters that explicitly ask for the number of rings in the neighbors. This should be easy enough. I'm curious as to whether this it's better to leave this up to whatever algorithm is being used however, since the one step graph has some nice properties. It'd probably be important to include distance in the multistep graph. > Btw, how do I push to this specific PR from my local repo? This should be fairly straight forward. If you're using the github cli, I think it should just be:. ```sh. gh pr checkout 1383. # whatever changes. git push. ```. Let me know if that gives you errors, since it might be a repository permissions issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:175,modifiability,paramet,parameters,175,"Oh, sorry, I had completely missed your comment here! > It looks great! Thanks! Can I ask why you used leiden clustering on this? > One first improvement could be to expose a parameters that explicitly ask for the number of rings in the neighbors. This should be easy enough. I'm curious as to whether this it's better to leave this up to whatever algorithm is being used however, since the one step graph has some nice properties. It'd probably be important to include distance in the multistep graph. > Btw, how do I push to this specific PR from my local repo? This should be fairly straight forward. If you're using the github cli, I think it should just be:. ```sh. gh pr checkout 1383. # whatever changes. git push. ```. Let me know if that gives you errors, since it might be a repository permissions issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:757,performance,error,errors,757,"Oh, sorry, I had completely missed your comment here! > It looks great! Thanks! Can I ask why you used leiden clustering on this? > One first improvement could be to expose a parameters that explicitly ask for the number of rings in the neighbors. This should be easy enough. I'm curious as to whether this it's better to leave this up to whatever algorithm is being used however, since the one step graph has some nice properties. It'd probably be important to include distance in the multistep graph. > Btw, how do I push to this specific PR from my local repo? This should be fairly straight forward. If you're using the github cli, I think it should just be:. ```sh. gh pr checkout 1383. # whatever changes. git push. ```. Let me know if that gives you errors, since it might be a repository permissions issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:17,safety,compl,completely,17,"Oh, sorry, I had completely missed your comment here! > It looks great! Thanks! Can I ask why you used leiden clustering on this? > One first improvement could be to expose a parameters that explicitly ask for the number of rings in the neighbors. This should be easy enough. I'm curious as to whether this it's better to leave this up to whatever algorithm is being used however, since the one step graph has some nice properties. It'd probably be important to include distance in the multistep graph. > Btw, how do I push to this specific PR from my local repo? This should be fairly straight forward. If you're using the github cli, I think it should just be:. ```sh. gh pr checkout 1383. # whatever changes. git push. ```. Let me know if that gives you errors, since it might be a repository permissions issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:757,safety,error,errors,757,"Oh, sorry, I had completely missed your comment here! > It looks great! Thanks! Can I ask why you used leiden clustering on this? > One first improvement could be to expose a parameters that explicitly ask for the number of rings in the neighbors. This should be easy enough. I'm curious as to whether this it's better to leave this up to whatever algorithm is being used however, since the one step graph has some nice properties. It'd probably be important to include distance in the multistep graph. > Btw, how do I push to this specific PR from my local repo? This should be fairly straight forward. If you're using the github cli, I think it should just be:. ```sh. gh pr checkout 1383. # whatever changes. git push. ```. Let me know if that gives you errors, since it might be a repository permissions issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:796,safety,permiss,permissions,796,"Oh, sorry, I had completely missed your comment here! > It looks great! Thanks! Can I ask why you used leiden clustering on this? > One first improvement could be to expose a parameters that explicitly ask for the number of rings in the neighbors. This should be easy enough. I'm curious as to whether this it's better to leave this up to whatever algorithm is being used however, since the one step graph has some nice properties. It'd probably be important to include distance in the multistep graph. > Btw, how do I push to this specific PR from my local repo? This should be fairly straight forward. If you're using the github cli, I think it should just be:. ```sh. gh pr checkout 1383. # whatever changes. git push. ```. Let me know if that gives you errors, since it might be a repository permissions issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:17,security,compl,completely,17,"Oh, sorry, I had completely missed your comment here! > It looks great! Thanks! Can I ask why you used leiden clustering on this? > One first improvement could be to expose a parameters that explicitly ask for the number of rings in the neighbors. This should be easy enough. I'm curious as to whether this it's better to leave this up to whatever algorithm is being used however, since the one step graph has some nice properties. It'd probably be important to include distance in the multistep graph. > Btw, how do I push to this specific PR from my local repo? This should be fairly straight forward. If you're using the github cli, I think it should just be:. ```sh. gh pr checkout 1383. # whatever changes. git push. ```. Let me know if that gives you errors, since it might be a repository permissions issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:166,security,expos,expose,166,"Oh, sorry, I had completely missed your comment here! > It looks great! Thanks! Can I ask why you used leiden clustering on this? > One first improvement could be to expose a parameters that explicitly ask for the number of rings in the neighbors. This should be easy enough. I'm curious as to whether this it's better to leave this up to whatever algorithm is being used however, since the one step graph has some nice properties. It'd probably be important to include distance in the multistep graph. > Btw, how do I push to this specific PR from my local repo? This should be fairly straight forward. If you're using the github cli, I think it should just be:. ```sh. gh pr checkout 1383. # whatever changes. git push. ```. Let me know if that gives you errors, since it might be a repository permissions issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:757,usability,error,errors,757,"Oh, sorry, I had completely missed your comment here! > It looks great! Thanks! Can I ask why you used leiden clustering on this? > One first improvement could be to expose a parameters that explicitly ask for the number of rings in the neighbors. This should be easy enough. I'm curious as to whether this it's better to leave this up to whatever algorithm is being used however, since the one step graph has some nice properties. It'd probably be important to include distance in the multistep graph. > Btw, how do I push to this specific PR from my local repo? This should be fairly straight forward. If you're using the github cli, I think it should just be:. ```sh. gh pr checkout 1383. # whatever changes. git push. ```. Let me know if that gives you errors, since it might be a repository permissions issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:24,deployability,updat,updated,24,"@ivirshup @giovp i have updated the function, now it can process non-visium coordinates and have the number of rings option for visium. https://github.com/theislab/spatial-tools/blob/graph/notebooks/build_spatial_graph.ipynb",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:76,interoperability,coordinat,coordinates,76,"@ivirshup @giovp i have updated the function, now it can process non-visium coordinates and have the number of rings option for visium. https://github.com/theislab/spatial-tools/blob/graph/notebooks/build_spatial_graph.ipynb",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:24,safety,updat,updated,24,"@ivirshup @giovp i have updated the function, now it can process non-visium coordinates and have the number of rings option for visium. https://github.com/theislab/spatial-tools/blob/graph/notebooks/build_spatial_graph.ipynb",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:24,security,updat,updated,24,"@ivirshup @giovp i have updated the function, now it can process non-visium coordinates and have the number of rings option for visium. https://github.com/theislab/spatial-tools/blob/graph/notebooks/build_spatial_graph.ipynb",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:172,usability,tool,tools,172,"@ivirshup @giovp i have updated the function, now it can process non-visium coordinates and have the number of rings option for visium. https://github.com/theislab/spatial-tools/blob/graph/notebooks/build_spatial_graph.ipynb",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:108,deployability,integr,integrate,108,"That's great, I'll add Isaac to that project so he can see code (repo is private). Let's discuss whether to integrate in scanpy at next meeting! Thank you Sergei !",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:108,integrability,integr,integrate,108,"That's great, I'll add Isaac to that project so he can see code (repo is private). Let's discuss whether to integrate in scanpy at next meeting! Thank you Sergei !",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:108,interoperability,integr,integrate,108,"That's great, I'll add Isaac to that project so he can see code (repo is private). Let's discuss whether to integrate in scanpy at next meeting! Thank you Sergei !",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:108,modifiability,integr,integrate,108,"That's great, I'll add Isaac to that project so he can see code (repo is private). Let's discuss whether to integrate in scanpy at next meeting! Thank you Sergei !",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:108,reliability,integr,integrate,108,"That's great, I'll add Isaac to that project so he can see code (repo is private). Let's discuss whether to integrate in scanpy at next meeting! Thank you Sergei !",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:108,security,integr,integrate,108,"That's great, I'll add Isaac to that project so he can see code (repo is private). Let's discuss whether to integrate in scanpy at next meeting! Thank you Sergei !",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:108,testability,integr,integrate,108,"That's great, I'll add Isaac to that project so he can see code (repo is private). Let's discuss whether to integrate in scanpy at next meeting! Thank you Sergei !",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:96,deployability,updat,updates,96,"Hey! I was just getting back to this, and I'm not sure I agree with all the choices made in the updates. For example, I would probably rather have separate functions for different spatial neighbor strategies. Also this won't work if `coord_type` isn't `""visium""`. Should I be making changes to this PR, or are you relying on it? # hexagonal connectivity. I would propose a separate function just for visium data, maybe called `visium_connectivity`. It works with the assumption of a hexagonal grid. This removes the `neigh`, `radius`, and `coord_type` arguments. I think if we have an argument for `n_rings` there should be some way to weight the connectivity graph by how many steps away each extra point is. Also I'm pretty sure the current implementation of `n_rings` is incorrect when `n_rings>2`. Without weighting, I think it should be more like this:. ```python. def walk_nsteps(adj, n):. """"""Expand adjacency matrix adj by walking out n steps from each node."""""". adj = adj.astype(bool). cur_step = adj. result = adj.copy(). for i in range(n):. cur_step = adj @ cur_step. cur_step.setdiag(False). result = result + cur_step. return result. ```. <details>. <summary> An example showing this works </summary>. ```python. import networkx as nx. from scipy import sparse. from matplotlib import pyplot as plt. import numpy as np. def walk_nsteps(adj, n):. """"""Expand adjacency matrix adj by walking out n steps from each node."""""". adj = adj.astype(bool). cur_step = adj. result = adj.copy(). for i in range(n):. cur_step = adj @ cur_step. cur_step.setdiag(False). result = result + cur_step. return result. # Test data (path graph). G = nx.Graph(). G.add_nodes_from([0,1,2,3]). G.add_edges_from([(0, 1), (1, 2), (2, 3), (3, 4)]). adj = nx.adjacency_matrix(G).astype(bool). fig, axes = plt.subplots(nrows=3). # Fixed circle layout. pos = {i: (np.cos(-np.pi + (np.pi * i) / 4), np.sin(-np.pi + (np.pi * i) / 4)) for i in range(5)}. for n, ax in enumerate(axes):. nx.draw(. nx.Graph(walk_nsteps(adj, n)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:735,energy efficiency,current,current,735,"Hey! I was just getting back to this, and I'm not sure I agree with all the choices made in the updates. For example, I would probably rather have separate functions for different spatial neighbor strategies. Also this won't work if `coord_type` isn't `""visium""`. Should I be making changes to this PR, or are you relying on it? # hexagonal connectivity. I would propose a separate function just for visium data, maybe called `visium_connectivity`. It works with the assumption of a hexagonal grid. This removes the `neigh`, `radius`, and `coord_type` arguments. I think if we have an argument for `n_rings` there should be some way to weight the connectivity graph by how many steps away each extra point is. Also I'm pretty sure the current implementation of `n_rings` is incorrect when `n_rings>2`. Without weighting, I think it should be more like this:. ```python. def walk_nsteps(adj, n):. """"""Expand adjacency matrix adj by walking out n steps from each node."""""". adj = adj.astype(bool). cur_step = adj. result = adj.copy(). for i in range(n):. cur_step = adj @ cur_step. cur_step.setdiag(False). result = result + cur_step. return result. ```. <details>. <summary> An example showing this works </summary>. ```python. import networkx as nx. from scipy import sparse. from matplotlib import pyplot as plt. import numpy as np. def walk_nsteps(adj, n):. """"""Expand adjacency matrix adj by walking out n steps from each node."""""". adj = adj.astype(bool). cur_step = adj. result = adj.copy(). for i in range(n):. cur_step = adj @ cur_step. cur_step.setdiag(False). result = result + cur_step. return result. # Test data (path graph). G = nx.Graph(). G.add_nodes_from([0,1,2,3]). G.add_edges_from([(0, 1), (1, 2), (2, 3), (3, 4)]). adj = nx.adjacency_matrix(G).astype(bool). fig, axes = plt.subplots(nrows=3). # Fixed circle layout. pos = {i: (np.cos(-np.pi + (np.pi * i) / 4), np.sin(-np.pi + (np.pi * i) / 4)) for i in range(5)}. for n, ax in enumerate(axes):. nx.draw(. nx.Graph(walk_nsteps(adj, n)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:1965,energy efficiency,draw,draw,1965,"ments. I think if we have an argument for `n_rings` there should be some way to weight the connectivity graph by how many steps away each extra point is. Also I'm pretty sure the current implementation of `n_rings` is incorrect when `n_rings>2`. Without weighting, I think it should be more like this:. ```python. def walk_nsteps(adj, n):. """"""Expand adjacency matrix adj by walking out n steps from each node."""""". adj = adj.astype(bool). cur_step = adj. result = adj.copy(). for i in range(n):. cur_step = adj @ cur_step. cur_step.setdiag(False). result = result + cur_step. return result. ```. <details>. <summary> An example showing this works </summary>. ```python. import networkx as nx. from scipy import sparse. from matplotlib import pyplot as plt. import numpy as np. def walk_nsteps(adj, n):. """"""Expand adjacency matrix adj by walking out n steps from each node."""""". adj = adj.astype(bool). cur_step = adj. result = adj.copy(). for i in range(n):. cur_step = adj @ cur_step. cur_step.setdiag(False). result = result + cur_step. return result. # Test data (path graph). G = nx.Graph(). G.add_nodes_from([0,1,2,3]). G.add_edges_from([(0, 1), (1, 2), (2, 3), (3, 4)]). adj = nx.adjacency_matrix(G).astype(bool). fig, axes = plt.subplots(nrows=3). # Fixed circle layout. pos = {i: (np.cos(-np.pi + (np.pi * i) / 4), np.sin(-np.pi + (np.pi * i) / 4)) for i in range(5)}. for n, ax in enumerate(axes):. nx.draw(. nx.Graph(walk_nsteps(adj, n)),. pos=pos,. ax=ax,. ). plt.show(). ```. ![image](https://user-images.githubusercontent.com/8238804/94650946-f480cb80-033a-11eb-8d59-9716408d0bbd.png). </details>. # Radius/ n-neighbors coordinates. I'm a little less sure about this use case. Since cells come in all shapes and sizes, I'm not sure if representing each cell as a centroid with a fixed radius or number of neighbors is the way to go. That said, maybe it is. This could be a separate function `spatial_connectivity`? In this case, I think it would also be useful to return a distance matrix.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:1790,integrability,sub,subplots,1790,"ments. I think if we have an argument for `n_rings` there should be some way to weight the connectivity graph by how many steps away each extra point is. Also I'm pretty sure the current implementation of `n_rings` is incorrect when `n_rings>2`. Without weighting, I think it should be more like this:. ```python. def walk_nsteps(adj, n):. """"""Expand adjacency matrix adj by walking out n steps from each node."""""". adj = adj.astype(bool). cur_step = adj. result = adj.copy(). for i in range(n):. cur_step = adj @ cur_step. cur_step.setdiag(False). result = result + cur_step. return result. ```. <details>. <summary> An example showing this works </summary>. ```python. import networkx as nx. from scipy import sparse. from matplotlib import pyplot as plt. import numpy as np. def walk_nsteps(adj, n):. """"""Expand adjacency matrix adj by walking out n steps from each node."""""". adj = adj.astype(bool). cur_step = adj. result = adj.copy(). for i in range(n):. cur_step = adj @ cur_step. cur_step.setdiag(False). result = result + cur_step. return result. # Test data (path graph). G = nx.Graph(). G.add_nodes_from([0,1,2,3]). G.add_edges_from([(0, 1), (1, 2), (2, 3), (3, 4)]). adj = nx.adjacency_matrix(G).astype(bool). fig, axes = plt.subplots(nrows=3). # Fixed circle layout. pos = {i: (np.cos(-np.pi + (np.pi * i) / 4), np.sin(-np.pi + (np.pi * i) / 4)) for i in range(5)}. for n, ax in enumerate(axes):. nx.draw(. nx.Graph(walk_nsteps(adj, n)),. pos=pos,. ax=ax,. ). plt.show(). ```. ![image](https://user-images.githubusercontent.com/8238804/94650946-f480cb80-033a-11eb-8d59-9716408d0bbd.png). </details>. # Radius/ n-neighbors coordinates. I'm a little less sure about this use case. Since cells come in all shapes and sizes, I'm not sure if representing each cell as a centroid with a fixed radius or number of neighbors is the way to go. That said, maybe it is. This could be a separate function `spatial_connectivity`? In this case, I think it would also be useful to return a distance matrix.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:2187,interoperability,coordinat,coordinates,2187,"ments. I think if we have an argument for `n_rings` there should be some way to weight the connectivity graph by how many steps away each extra point is. Also I'm pretty sure the current implementation of `n_rings` is incorrect when `n_rings>2`. Without weighting, I think it should be more like this:. ```python. def walk_nsteps(adj, n):. """"""Expand adjacency matrix adj by walking out n steps from each node."""""". adj = adj.astype(bool). cur_step = adj. result = adj.copy(). for i in range(n):. cur_step = adj @ cur_step. cur_step.setdiag(False). result = result + cur_step. return result. ```. <details>. <summary> An example showing this works </summary>. ```python. import networkx as nx. from scipy import sparse. from matplotlib import pyplot as plt. import numpy as np. def walk_nsteps(adj, n):. """"""Expand adjacency matrix adj by walking out n steps from each node."""""". adj = adj.astype(bool). cur_step = adj. result = adj.copy(). for i in range(n):. cur_step = adj @ cur_step. cur_step.setdiag(False). result = result + cur_step. return result. # Test data (path graph). G = nx.Graph(). G.add_nodes_from([0,1,2,3]). G.add_edges_from([(0, 1), (1, 2), (2, 3), (3, 4)]). adj = nx.adjacency_matrix(G).astype(bool). fig, axes = plt.subplots(nrows=3). # Fixed circle layout. pos = {i: (np.cos(-np.pi + (np.pi * i) / 4), np.sin(-np.pi + (np.pi * i) / 4)) for i in range(5)}. for n, ax in enumerate(axes):. nx.draw(. nx.Graph(walk_nsteps(adj, n)),. pos=pos,. ax=ax,. ). plt.show(). ```. ![image](https://user-images.githubusercontent.com/8238804/94650946-f480cb80-033a-11eb-8d59-9716408d0bbd.png). </details>. # Radius/ n-neighbors coordinates. I'm a little less sure about this use case. Since cells come in all shapes and sizes, I'm not sure if representing each cell as a centroid with a fixed radius or number of neighbors is the way to go. That said, maybe it is. This could be a separate function `spatial_connectivity`? In this case, I think it would also be useful to return a distance matrix.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:1232,performance,network,networkx,1232,"ord_type` isn't `""visium""`. Should I be making changes to this PR, or are you relying on it? # hexagonal connectivity. I would propose a separate function just for visium data, maybe called `visium_connectivity`. It works with the assumption of a hexagonal grid. This removes the `neigh`, `radius`, and `coord_type` arguments. I think if we have an argument for `n_rings` there should be some way to weight the connectivity graph by how many steps away each extra point is. Also I'm pretty sure the current implementation of `n_rings` is incorrect when `n_rings>2`. Without weighting, I think it should be more like this:. ```python. def walk_nsteps(adj, n):. """"""Expand adjacency matrix adj by walking out n steps from each node."""""". adj = adj.astype(bool). cur_step = adj. result = adj.copy(). for i in range(n):. cur_step = adj @ cur_step. cur_step.setdiag(False). result = result + cur_step. return result. ```. <details>. <summary> An example showing this works </summary>. ```python. import networkx as nx. from scipy import sparse. from matplotlib import pyplot as plt. import numpy as np. def walk_nsteps(adj, n):. """"""Expand adjacency matrix adj by walking out n steps from each node."""""". adj = adj.astype(bool). cur_step = adj. result = adj.copy(). for i in range(n):. cur_step = adj @ cur_step. cur_step.setdiag(False). result = result + cur_step. return result. # Test data (path graph). G = nx.Graph(). G.add_nodes_from([0,1,2,3]). G.add_edges_from([(0, 1), (1, 2), (2, 3), (3, 4)]). adj = nx.adjacency_matrix(G).astype(bool). fig, axes = plt.subplots(nrows=3). # Fixed circle layout. pos = {i: (np.cos(-np.pi + (np.pi * i) / 4), np.sin(-np.pi + (np.pi * i) / 4)) for i in range(5)}. for n, ax in enumerate(axes):. nx.draw(. nx.Graph(walk_nsteps(adj, n)),. pos=pos,. ax=ax,. ). plt.show(). ```. ![image](https://user-images.githubusercontent.com/8238804/94650946-f480cb80-033a-11eb-8d59-9716408d0bbd.png). </details>. # Radius/ n-neighbors coordinates. I'm a little less sure about this us",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:96,safety,updat,updates,96,"Hey! I was just getting back to this, and I'm not sure I agree with all the choices made in the updates. For example, I would probably rather have separate functions for different spatial neighbor strategies. Also this won't work if `coord_type` isn't `""visium""`. Should I be making changes to this PR, or are you relying on it? # hexagonal connectivity. I would propose a separate function just for visium data, maybe called `visium_connectivity`. It works with the assumption of a hexagonal grid. This removes the `neigh`, `radius`, and `coord_type` arguments. I think if we have an argument for `n_rings` there should be some way to weight the connectivity graph by how many steps away each extra point is. Also I'm pretty sure the current implementation of `n_rings` is incorrect when `n_rings>2`. Without weighting, I think it should be more like this:. ```python. def walk_nsteps(adj, n):. """"""Expand adjacency matrix adj by walking out n steps from each node."""""". adj = adj.astype(bool). cur_step = adj. result = adj.copy(). for i in range(n):. cur_step = adj @ cur_step. cur_step.setdiag(False). result = result + cur_step. return result. ```. <details>. <summary> An example showing this works </summary>. ```python. import networkx as nx. from scipy import sparse. from matplotlib import pyplot as plt. import numpy as np. def walk_nsteps(adj, n):. """"""Expand adjacency matrix adj by walking out n steps from each node."""""". adj = adj.astype(bool). cur_step = adj. result = adj.copy(). for i in range(n):. cur_step = adj @ cur_step. cur_step.setdiag(False). result = result + cur_step. return result. # Test data (path graph). G = nx.Graph(). G.add_nodes_from([0,1,2,3]). G.add_edges_from([(0, 1), (1, 2), (2, 3), (3, 4)]). adj = nx.adjacency_matrix(G).astype(bool). fig, axes = plt.subplots(nrows=3). # Fixed circle layout. pos = {i: (np.cos(-np.pi + (np.pi * i) / 4), np.sin(-np.pi + (np.pi * i) / 4)) for i in range(5)}. for n, ax in enumerate(axes):. nx.draw(. nx.Graph(walk_nsteps(adj, n)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:1610,safety,Test,Test,1610,"ments. I think if we have an argument for `n_rings` there should be some way to weight the connectivity graph by how many steps away each extra point is. Also I'm pretty sure the current implementation of `n_rings` is incorrect when `n_rings>2`. Without weighting, I think it should be more like this:. ```python. def walk_nsteps(adj, n):. """"""Expand adjacency matrix adj by walking out n steps from each node."""""". adj = adj.astype(bool). cur_step = adj. result = adj.copy(). for i in range(n):. cur_step = adj @ cur_step. cur_step.setdiag(False). result = result + cur_step. return result. ```. <details>. <summary> An example showing this works </summary>. ```python. import networkx as nx. from scipy import sparse. from matplotlib import pyplot as plt. import numpy as np. def walk_nsteps(adj, n):. """"""Expand adjacency matrix adj by walking out n steps from each node."""""". adj = adj.astype(bool). cur_step = adj. result = adj.copy(). for i in range(n):. cur_step = adj @ cur_step. cur_step.setdiag(False). result = result + cur_step. return result. # Test data (path graph). G = nx.Graph(). G.add_nodes_from([0,1,2,3]). G.add_edges_from([(0, 1), (1, 2), (2, 3), (3, 4)]). adj = nx.adjacency_matrix(G).astype(bool). fig, axes = plt.subplots(nrows=3). # Fixed circle layout. pos = {i: (np.cos(-np.pi + (np.pi * i) / 4), np.sin(-np.pi + (np.pi * i) / 4)) for i in range(5)}. for n, ax in enumerate(axes):. nx.draw(. nx.Graph(walk_nsteps(adj, n)),. pos=pos,. ax=ax,. ). plt.show(). ```. ![image](https://user-images.githubusercontent.com/8238804/94650946-f480cb80-033a-11eb-8d59-9716408d0bbd.png). </details>. # Radius/ n-neighbors coordinates. I'm a little less sure about this use case. Since cells come in all shapes and sizes, I'm not sure if representing each cell as a centroid with a fixed radius or number of neighbors is the way to go. That said, maybe it is. This could be a separate function `spatial_connectivity`? In this case, I think it would also be useful to return a distance matrix.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:96,security,updat,updates,96,"Hey! I was just getting back to this, and I'm not sure I agree with all the choices made in the updates. For example, I would probably rather have separate functions for different spatial neighbor strategies. Also this won't work if `coord_type` isn't `""visium""`. Should I be making changes to this PR, or are you relying on it? # hexagonal connectivity. I would propose a separate function just for visium data, maybe called `visium_connectivity`. It works with the assumption of a hexagonal grid. This removes the `neigh`, `radius`, and `coord_type` arguments. I think if we have an argument for `n_rings` there should be some way to weight the connectivity graph by how many steps away each extra point is. Also I'm pretty sure the current implementation of `n_rings` is incorrect when `n_rings>2`. Without weighting, I think it should be more like this:. ```python. def walk_nsteps(adj, n):. """"""Expand adjacency matrix adj by walking out n steps from each node."""""". adj = adj.astype(bool). cur_step = adj. result = adj.copy(). for i in range(n):. cur_step = adj @ cur_step. cur_step.setdiag(False). result = result + cur_step. return result. ```. <details>. <summary> An example showing this works </summary>. ```python. import networkx as nx. from scipy import sparse. from matplotlib import pyplot as plt. import numpy as np. def walk_nsteps(adj, n):. """"""Expand adjacency matrix adj by walking out n steps from each node."""""". adj = adj.astype(bool). cur_step = adj. result = adj.copy(). for i in range(n):. cur_step = adj @ cur_step. cur_step.setdiag(False). result = result + cur_step. return result. # Test data (path graph). G = nx.Graph(). G.add_nodes_from([0,1,2,3]). G.add_edges_from([(0, 1), (1, 2), (2, 3), (3, 4)]). adj = nx.adjacency_matrix(G).astype(bool). fig, axes = plt.subplots(nrows=3). # Fixed circle layout. pos = {i: (np.cos(-np.pi + (np.pi * i) / 4), np.sin(-np.pi + (np.pi * i) / 4)) for i in range(5)}. for n, ax in enumerate(axes):. nx.draw(. nx.Graph(walk_nsteps(adj, n)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:1232,security,network,networkx,1232,"ord_type` isn't `""visium""`. Should I be making changes to this PR, or are you relying on it? # hexagonal connectivity. I would propose a separate function just for visium data, maybe called `visium_connectivity`. It works with the assumption of a hexagonal grid. This removes the `neigh`, `radius`, and `coord_type` arguments. I think if we have an argument for `n_rings` there should be some way to weight the connectivity graph by how many steps away each extra point is. Also I'm pretty sure the current implementation of `n_rings` is incorrect when `n_rings>2`. Without weighting, I think it should be more like this:. ```python. def walk_nsteps(adj, n):. """"""Expand adjacency matrix adj by walking out n steps from each node."""""". adj = adj.astype(bool). cur_step = adj. result = adj.copy(). for i in range(n):. cur_step = adj @ cur_step. cur_step.setdiag(False). result = result + cur_step. return result. ```. <details>. <summary> An example showing this works </summary>. ```python. import networkx as nx. from scipy import sparse. from matplotlib import pyplot as plt. import numpy as np. def walk_nsteps(adj, n):. """"""Expand adjacency matrix adj by walking out n steps from each node."""""". adj = adj.astype(bool). cur_step = adj. result = adj.copy(). for i in range(n):. cur_step = adj @ cur_step. cur_step.setdiag(False). result = result + cur_step. return result. # Test data (path graph). G = nx.Graph(). G.add_nodes_from([0,1,2,3]). G.add_edges_from([(0, 1), (1, 2), (2, 3), (3, 4)]). adj = nx.adjacency_matrix(G).astype(bool). fig, axes = plt.subplots(nrows=3). # Fixed circle layout. pos = {i: (np.cos(-np.pi + (np.pi * i) / 4), np.sin(-np.pi + (np.pi * i) / 4)) for i in range(5)}. for n, ax in enumerate(axes):. nx.draw(. nx.Graph(walk_nsteps(adj, n)),. pos=pos,. ax=ax,. ). plt.show(). ```. ![image](https://user-images.githubusercontent.com/8238804/94650946-f480cb80-033a-11eb-8d59-9716408d0bbd.png). </details>. # Radius/ n-neighbors coordinates. I'm a little less sure about this us",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:1610,testability,Test,Test,1610,"ments. I think if we have an argument for `n_rings` there should be some way to weight the connectivity graph by how many steps away each extra point is. Also I'm pretty sure the current implementation of `n_rings` is incorrect when `n_rings>2`. Without weighting, I think it should be more like this:. ```python. def walk_nsteps(adj, n):. """"""Expand adjacency matrix adj by walking out n steps from each node."""""". adj = adj.astype(bool). cur_step = adj. result = adj.copy(). for i in range(n):. cur_step = adj @ cur_step. cur_step.setdiag(False). result = result + cur_step. return result. ```. <details>. <summary> An example showing this works </summary>. ```python. import networkx as nx. from scipy import sparse. from matplotlib import pyplot as plt. import numpy as np. def walk_nsteps(adj, n):. """"""Expand adjacency matrix adj by walking out n steps from each node."""""". adj = adj.astype(bool). cur_step = adj. result = adj.copy(). for i in range(n):. cur_step = adj @ cur_step. cur_step.setdiag(False). result = result + cur_step. return result. # Test data (path graph). G = nx.Graph(). G.add_nodes_from([0,1,2,3]). G.add_edges_from([(0, 1), (1, 2), (2, 3), (3, 4)]). adj = nx.adjacency_matrix(G).astype(bool). fig, axes = plt.subplots(nrows=3). # Fixed circle layout. pos = {i: (np.cos(-np.pi + (np.pi * i) / 4), np.sin(-np.pi + (np.pi * i) / 4)) for i in range(5)}. for n, ax in enumerate(axes):. nx.draw(. nx.Graph(walk_nsteps(adj, n)),. pos=pos,. ax=ax,. ). plt.show(). ```. ![image](https://user-images.githubusercontent.com/8238804/94650946-f480cb80-033a-11eb-8d59-9716408d0bbd.png). </details>. # Radius/ n-neighbors coordinates. I'm a little less sure about this use case. Since cells come in all shapes and sizes, I'm not sure if representing each cell as a centroid with a fixed radius or number of neighbors is the way to go. That said, maybe it is. This could be a separate function `spatial_connectivity`? In this case, I think it would also be useful to return a distance matrix.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:2059,usability,user,user-images,2059,"ments. I think if we have an argument for `n_rings` there should be some way to weight the connectivity graph by how many steps away each extra point is. Also I'm pretty sure the current implementation of `n_rings` is incorrect when `n_rings>2`. Without weighting, I think it should be more like this:. ```python. def walk_nsteps(adj, n):. """"""Expand adjacency matrix adj by walking out n steps from each node."""""". adj = adj.astype(bool). cur_step = adj. result = adj.copy(). for i in range(n):. cur_step = adj @ cur_step. cur_step.setdiag(False). result = result + cur_step. return result. ```. <details>. <summary> An example showing this works </summary>. ```python. import networkx as nx. from scipy import sparse. from matplotlib import pyplot as plt. import numpy as np. def walk_nsteps(adj, n):. """"""Expand adjacency matrix adj by walking out n steps from each node."""""". adj = adj.astype(bool). cur_step = adj. result = adj.copy(). for i in range(n):. cur_step = adj @ cur_step. cur_step.setdiag(False). result = result + cur_step. return result. # Test data (path graph). G = nx.Graph(). G.add_nodes_from([0,1,2,3]). G.add_edges_from([(0, 1), (1, 2), (2, 3), (3, 4)]). adj = nx.adjacency_matrix(G).astype(bool). fig, axes = plt.subplots(nrows=3). # Fixed circle layout. pos = {i: (np.cos(-np.pi + (np.pi * i) / 4), np.sin(-np.pi + (np.pi * i) / 4)) for i in range(5)}. for n, ax in enumerate(axes):. nx.draw(. nx.Graph(walk_nsteps(adj, n)),. pos=pos,. ax=ax,. ). plt.show(). ```. ![image](https://user-images.githubusercontent.com/8238804/94650946-f480cb80-033a-11eb-8d59-9716408d0bbd.png). </details>. # Radius/ n-neighbors coordinates. I'm a little less sure about this use case. Since cells come in all shapes and sizes, I'm not sure if representing each cell as a centroid with a fixed radius or number of neighbors is the way to go. That said, maybe it is. This could be a separate function `spatial_connectivity`? In this case, I think it would also be useful to return a distance matrix.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:115,availability,redund,redundant,115,"About adding all powers of adjacency matrix - i implemented it at first as you did, but then i thought that it was redundant and changed to the present variant. My thought was that the hexagonal connectivity structure would allow to get all paths of less than n_rings with only n_rings power. And this works in practice, but i agree that there can be some edge cases with isolated node blocks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:115,deployability,redundan,redundant,115,"About adding all powers of adjacency matrix - i implemented it at first as you did, but then i thought that it was redundant and changed to the present variant. My thought was that the hexagonal connectivity structure would allow to get all paths of less than n_rings with only n_rings power. And this works in practice, but i agree that there can be some edge cases with isolated node blocks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:17,energy efficiency,power,powers,17,"About adding all powers of adjacency matrix - i implemented it at first as you did, but then i thought that it was redundant and changed to the present variant. My thought was that the hexagonal connectivity structure would allow to get all paths of less than n_rings with only n_rings power. And this works in practice, but i agree that there can be some edge cases with isolated node blocks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:286,energy efficiency,power,power,286,"About adding all powers of adjacency matrix - i implemented it at first as you did, but then i thought that it was redundant and changed to the present variant. My thought was that the hexagonal connectivity structure would allow to get all paths of less than n_rings with only n_rings power. And this works in practice, but i agree that there can be some edge cases with isolated node blocks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:115,reliability,redundan,redundant,115,"About adding all powers of adjacency matrix - i implemented it at first as you did, but then i thought that it was redundant and changed to the present variant. My thought was that the hexagonal connectivity structure would allow to get all paths of less than n_rings with only n_rings power. And this works in practice, but i agree that there can be some edge cases with isolated node blocks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:311,reliability,pra,practice,311,"About adding all powers of adjacency matrix - i implemented it at first as you did, but then i thought that it was redundant and changed to the present variant. My thought was that the hexagonal connectivity structure would allow to get all paths of less than n_rings with only n_rings power. And this works in practice, but i agree that there can be some edge cases with isolated node blocks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:115,safety,redund,redundant,115,"About adding all powers of adjacency matrix - i implemented it at first as you did, but then i thought that it was redundant and changed to the present variant. My thought was that the hexagonal connectivity structure would allow to get all paths of less than n_rings with only n_rings power. And this works in practice, but i agree that there can be some edge cases with isolated node blocks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:372,safety,isol,isolated,372,"About adding all powers of adjacency matrix - i implemented it at first as you did, but then i thought that it was redundant and changed to the present variant. My thought was that the hexagonal connectivity structure would allow to get all paths of less than n_rings with only n_rings power. And this works in practice, but i agree that there can be some edge cases with isolated node blocks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:372,security,iso,isolated,372,"About adding all powers of adjacency matrix - i implemented it at first as you did, but then i thought that it was redundant and changed to the present variant. My thought was that the hexagonal connectivity structure would allow to get all paths of less than n_rings with only n_rings power. And this works in practice, but i agree that there can be some edge cases with isolated node blocks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:372,testability,isol,isolated,372,"About adding all powers of adjacency matrix - i implemented it at first as you did, but then i thought that it was redundant and changed to the present variant. My thought was that the hexagonal connectivity structure would allow to get all paths of less than n_rings with only n_rings power. And this works in practice, but i agree that there can be some edge cases with isolated node blocks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:66,availability,down,down,66,"@Koncopd here's an example of the current implementation breaking down at `n=3`. ```python. def walk_nsteps_current(adj, n=1):. adj = adj.copy(). if n > 1:. # get up to n_rings order connections. adj += adj ** n. adj.setdiag(0). adj.eliminate_zeros(). adj.data[:] = 1.0. return adj. fig, axes = plt.subplots(nrows=3). # Fixed circle layout. pos = {i: (np.cos(-np.pi + (np.pi * i) / 4), np.sin(-np.pi + (np.pi * i) / 4)) for i in range(5)}. for n, ax in enumerate(axes):. nx.draw(. nx.Graph(walk_nsteps_current(adj, n + 1)), # making sure we start at 1. pos=pos,. ax=ax,. ). . plt.show(). ```. ![image](https://user-images.githubusercontent.com/8238804/94672185-3cfab200-0358-11eb-84d6-11abe07f7f1c.png). For `n=3` you're missing the neighbors at depth=2. Basically, you're just jumping to the 3rd step, instead of accumulating neighbors up to that step.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:34,energy efficiency,current,current,34,"@Koncopd here's an example of the current implementation breaking down at `n=3`. ```python. def walk_nsteps_current(adj, n=1):. adj = adj.copy(). if n > 1:. # get up to n_rings order connections. adj += adj ** n. adj.setdiag(0). adj.eliminate_zeros(). adj.data[:] = 1.0. return adj. fig, axes = plt.subplots(nrows=3). # Fixed circle layout. pos = {i: (np.cos(-np.pi + (np.pi * i) / 4), np.sin(-np.pi + (np.pi * i) / 4)) for i in range(5)}. for n, ax in enumerate(axes):. nx.draw(. nx.Graph(walk_nsteps_current(adj, n + 1)), # making sure we start at 1. pos=pos,. ax=ax,. ). . plt.show(). ```. ![image](https://user-images.githubusercontent.com/8238804/94672185-3cfab200-0358-11eb-84d6-11abe07f7f1c.png). For `n=3` you're missing the neighbors at depth=2. Basically, you're just jumping to the 3rd step, instead of accumulating neighbors up to that step.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:474,energy efficiency,draw,draw,474,"@Koncopd here's an example of the current implementation breaking down at `n=3`. ```python. def walk_nsteps_current(adj, n=1):. adj = adj.copy(). if n > 1:. # get up to n_rings order connections. adj += adj ** n. adj.setdiag(0). adj.eliminate_zeros(). adj.data[:] = 1.0. return adj. fig, axes = plt.subplots(nrows=3). # Fixed circle layout. pos = {i: (np.cos(-np.pi + (np.pi * i) / 4), np.sin(-np.pi + (np.pi * i) / 4)) for i in range(5)}. for n, ax in enumerate(axes):. nx.draw(. nx.Graph(walk_nsteps_current(adj, n + 1)), # making sure we start at 1. pos=pos,. ax=ax,. ). . plt.show(). ```. ![image](https://user-images.githubusercontent.com/8238804/94672185-3cfab200-0358-11eb-84d6-11abe07f7f1c.png). For `n=3` you're missing the neighbors at depth=2. Basically, you're just jumping to the 3rd step, instead of accumulating neighbors up to that step.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:299,integrability,sub,subplots,299,"@Koncopd here's an example of the current implementation breaking down at `n=3`. ```python. def walk_nsteps_current(adj, n=1):. adj = adj.copy(). if n > 1:. # get up to n_rings order connections. adj += adj ** n. adj.setdiag(0). adj.eliminate_zeros(). adj.data[:] = 1.0. return adj. fig, axes = plt.subplots(nrows=3). # Fixed circle layout. pos = {i: (np.cos(-np.pi + (np.pi * i) / 4), np.sin(-np.pi + (np.pi * i) / 4)) for i in range(5)}. for n, ax in enumerate(axes):. nx.draw(. nx.Graph(walk_nsteps_current(adj, n + 1)), # making sure we start at 1. pos=pos,. ax=ax,. ). . plt.show(). ```. ![image](https://user-images.githubusercontent.com/8238804/94672185-3cfab200-0358-11eb-84d6-11abe07f7f1c.png). For `n=3` you're missing the neighbors at depth=2. Basically, you're just jumping to the 3rd step, instead of accumulating neighbors up to that step.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:610,usability,user,user-images,610,"@Koncopd here's an example of the current implementation breaking down at `n=3`. ```python. def walk_nsteps_current(adj, n=1):. adj = adj.copy(). if n > 1:. # get up to n_rings order connections. adj += adj ** n. adj.setdiag(0). adj.eliminate_zeros(). adj.data[:] = 1.0. return adj. fig, axes = plt.subplots(nrows=3). # Fixed circle layout. pos = {i: (np.cos(-np.pi + (np.pi * i) / 4), np.sin(-np.pi + (np.pi * i) / 4)) for i in range(5)}. for n, ax in enumerate(axes):. nx.draw(. nx.Graph(walk_nsteps_current(adj, n + 1)), # making sure we start at 1. pos=pos,. ax=ax,. ). . plt.show(). ```. ![image](https://user-images.githubusercontent.com/8238804/94672185-3cfab200-0358-11eb-84d6-11abe07f7f1c.png). For `n=3` you're missing the neighbors at depth=2. Basically, you're just jumping to the 3rd step, instead of accumulating neighbors up to that step.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:133,usability,user,user-images,133,"Yes, but if we have hexagonal connections, then, for example, n-1, n-2 and so on paths will be counted. 2 examples. ![image](https://user-images.githubusercontent.com/3065736/94674173-1583f500-0318-11eb-8385-7978beac0562.png). You could check also the notebook i posted above.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:117,energy efficiency,power,powers,117,"Just reading along.... if all you want is to find neighbors within a certain number of hops, then non-zero values of powers of the adjacency matrix is a bit inefficient i think. There should be simple breadth-first-search or depth-first-search algorithms implemented in `networkx` I imagine. And if you're bent on this approach, adding self-loops (diag = 1) will mean you can just do powers.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:384,energy efficiency,power,powers,384,"Just reading along.... if all you want is to find neighbors within a certain number of hops, then non-zero values of powers of the adjacency matrix is a bit inefficient i think. There should be simple breadth-first-search or depth-first-search algorithms implemented in `networkx` I imagine. And if you're bent on this approach, adding self-loops (diag = 1) will mean you can just do powers.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:271,performance,network,networkx,271,"Just reading along.... if all you want is to find neighbors within a certain number of hops, then non-zero values of powers of the adjacency matrix is a bit inefficient i think. There should be simple breadth-first-search or depth-first-search algorithms implemented in `networkx` I imagine. And if you're bent on this approach, adding self-loops (diag = 1) will mean you can just do powers.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:271,security,network,networkx,271,"Just reading along.... if all you want is to find neighbors within a certain number of hops, then non-zero values of powers of the adjacency matrix is a bit inefficient i think. There should be simple breadth-first-search or depth-first-search algorithms implemented in `networkx` I imagine. And if you're bent on this approach, adding self-loops (diag = 1) will mean you can just do powers.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:194,testability,simpl,simple,194,"Just reading along.... if all you want is to find neighbors within a certain number of hops, then non-zero values of powers of the adjacency matrix is a bit inefficient i think. There should be simple breadth-first-search or depth-first-search algorithms implemented in `networkx` I imagine. And if you're bent on this approach, adding self-loops (diag = 1) will mean you can just do powers.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:194,usability,simpl,simple,194,"Just reading along.... if all you want is to find neighbors within a certain number of hops, then non-zero values of powers of the adjacency matrix is a bit inefficient i think. There should be simple breadth-first-search or depth-first-search algorithms implemented in `networkx` I imagine. And if you're bent on this approach, adding self-loops (diag = 1) will mean you can just do powers.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:97,reliability,doe,does,97,"I'm sorry but I'm not sure I'm following you guys in the discussion, although from the figure it does seem that @Koncopd implementation is missing the n=2. Regarding radius, this is definitely super useful for anything that is not a grid, and also agree that a distance matrix can be useful, maybe then creating a graph by binarizing that distance matrix based on a threshold?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:263,performance,network,networkx,263,@LuckyMD thanks for your comment. So it seems that it is enough to do this. ```. adj.setdiag(1) # or 2? adj += adj ** n. adj.setdiag(0). adj.eliminate_zeros(). adj.data[:] = 1.0. ```. And this should eliminate all edge cases. Not sure if it is a good idea to use networkx for this.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:263,security,network,networkx,263,@LuckyMD thanks for your comment. So it seems that it is enough to do this. ```. adj.setdiag(1) # or 2? adj += adj ** n. adj.setdiag(0). adj.eliminate_zeros(). adj.data[:] = 1.0. ```. And this should eliminate all edge cases. Not sure if it is a good idea to use networkx for this.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:152,energy efficiency,power,powers,152,"@Koncopd yes, I believe that should cover everything (maybe test to make sure I'm not missing sth here). However, I still think taking adjacency matrix powers will not be as fast as a BFS/DFS.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:60,safety,test,test,60,"@Koncopd yes, I believe that should cover everything (maybe test to make sure I'm not missing sth here). However, I still think taking adjacency matrix powers will not be as fast as a BFS/DFS.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:60,testability,test,test,60,"@Koncopd yes, I believe that should cover everything (maybe test to make sure I'm not missing sth here). However, I still think taking adjacency matrix powers will not be as fast as a BFS/DFS.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:508,availability,redund,redundancy,508,"@LuckyMD matrix multiplication on sparse matrices is actually a pretty efficient version of breadth first search, as [used by graphBLAS](http://arxiv.org/abs/1606.05790v2). Here's an example for a BFS from a specific point (which you can expand to all points by using the identity matrix):. <img width=""447"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/95168561-360eec00-07fd-11eb-96b9-19fbf3871d02.png"">. @Koncopd, I think that should work, since you're adding a self edge so you have redundancy at each step. . A separate point of contention on handling it like this: do you want the first step neighbors to have the same weight as the second step neighbors? My assumption would be no.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:81,deployability,version,version,81,"@LuckyMD matrix multiplication on sparse matrices is actually a pretty efficient version of breadth first search, as [used by graphBLAS](http://arxiv.org/abs/1606.05790v2). Here's an example for a BFS from a specific point (which you can expand to all points by using the identity matrix):. <img width=""447"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/95168561-360eec00-07fd-11eb-96b9-19fbf3871d02.png"">. @Koncopd, I think that should work, since you're adding a self edge so you have redundancy at each step. . A separate point of contention on handling it like this: do you want the first step neighbors to have the same weight as the second step neighbors? My assumption would be no.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:508,deployability,redundan,redundancy,508,"@LuckyMD matrix multiplication on sparse matrices is actually a pretty efficient version of breadth first search, as [used by graphBLAS](http://arxiv.org/abs/1606.05790v2). Here's an example for a BFS from a specific point (which you can expand to all points by using the identity matrix):. <img width=""447"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/95168561-360eec00-07fd-11eb-96b9-19fbf3871d02.png"">. @Koncopd, I think that should work, since you're adding a self edge so you have redundancy at each step. . A separate point of contention on handling it like this: do you want the first step neighbors to have the same weight as the second step neighbors? My assumption would be no.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:81,integrability,version,version,81,"@LuckyMD matrix multiplication on sparse matrices is actually a pretty efficient version of breadth first search, as [used by graphBLAS](http://arxiv.org/abs/1606.05790v2). Here's an example for a BFS from a specific point (which you can expand to all points by using the identity matrix):. <img width=""447"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/95168561-360eec00-07fd-11eb-96b9-19fbf3871d02.png"">. @Koncopd, I think that should work, since you're adding a self edge so you have redundancy at each step. . A separate point of contention on handling it like this: do you want the first step neighbors to have the same weight as the second step neighbors? My assumption would be no.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:208,interoperability,specif,specific,208,"@LuckyMD matrix multiplication on sparse matrices is actually a pretty efficient version of breadth first search, as [used by graphBLAS](http://arxiv.org/abs/1606.05790v2). Here's an example for a BFS from a specific point (which you can expand to all points by using the identity matrix):. <img width=""447"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/95168561-360eec00-07fd-11eb-96b9-19fbf3871d02.png"">. @Koncopd, I think that should work, since you're adding a self edge so you have redundancy at each step. . A separate point of contention on handling it like this: do you want the first step neighbors to have the same weight as the second step neighbors? My assumption would be no.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:81,modifiability,version,version,81,"@LuckyMD matrix multiplication on sparse matrices is actually a pretty efficient version of breadth first search, as [used by graphBLAS](http://arxiv.org/abs/1606.05790v2). Here's an example for a BFS from a specific point (which you can expand to all points by using the identity matrix):. <img width=""447"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/95168561-360eec00-07fd-11eb-96b9-19fbf3871d02.png"">. @Koncopd, I think that should work, since you're adding a self edge so you have redundancy at each step. . A separate point of contention on handling it like this: do you want the first step neighbors to have the same weight as the second step neighbors? My assumption would be no.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:555,performance,content,contention,555,"@LuckyMD matrix multiplication on sparse matrices is actually a pretty efficient version of breadth first search, as [used by graphBLAS](http://arxiv.org/abs/1606.05790v2). Here's an example for a BFS from a specific point (which you can expand to all points by using the identity matrix):. <img width=""447"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/95168561-360eec00-07fd-11eb-96b9-19fbf3871d02.png"">. @Koncopd, I think that should work, since you're adding a self edge so you have redundancy at each step. . A separate point of contention on handling it like this: do you want the first step neighbors to have the same weight as the second step neighbors? My assumption would be no.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:508,reliability,redundan,redundancy,508,"@LuckyMD matrix multiplication on sparse matrices is actually a pretty efficient version of breadth first search, as [used by graphBLAS](http://arxiv.org/abs/1606.05790v2). Here's an example for a BFS from a specific point (which you can expand to all points by using the identity matrix):. <img width=""447"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/95168561-360eec00-07fd-11eb-96b9-19fbf3871d02.png"">. @Koncopd, I think that should work, since you're adding a self edge so you have redundancy at each step. . A separate point of contention on handling it like this: do you want the first step neighbors to have the same weight as the second step neighbors? My assumption would be no.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:508,safety,redund,redundancy,508,"@LuckyMD matrix multiplication on sparse matrices is actually a pretty efficient version of breadth first search, as [used by graphBLAS](http://arxiv.org/abs/1606.05790v2). Here's an example for a BFS from a specific point (which you can expand to all points by using the identity matrix):. <img width=""447"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/95168561-360eec00-07fd-11eb-96b9-19fbf3871d02.png"">. @Koncopd, I think that should work, since you're adding a self edge so you have redundancy at each step. . A separate point of contention on handling it like this: do you want the first step neighbors to have the same weight as the second step neighbors? My assumption would be no.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:272,security,ident,identity,272,"@LuckyMD matrix multiplication on sparse matrices is actually a pretty efficient version of breadth first search, as [used by graphBLAS](http://arxiv.org/abs/1606.05790v2). Here's an example for a BFS from a specific point (which you can expand to all points by using the identity matrix):. <img width=""447"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/95168561-360eec00-07fd-11eb-96b9-19fbf3871d02.png"">. @Koncopd, I think that should work, since you're adding a self edge so you have redundancy at each step. . A separate point of contention on handling it like this: do you want the first step neighbors to have the same weight as the second step neighbors? My assumption would be no.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:71,usability,efficien,efficient,71,"@LuckyMD matrix multiplication on sparse matrices is actually a pretty efficient version of breadth first search, as [used by graphBLAS](http://arxiv.org/abs/1606.05790v2). Here's an example for a BFS from a specific point (which you can expand to all points by using the identity matrix):. <img width=""447"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/95168561-360eec00-07fd-11eb-96b9-19fbf3871d02.png"">. @Koncopd, I think that should work, since you're adding a self edge so you have redundancy at each step. . A separate point of contention on handling it like this: do you want the first step neighbors to have the same weight as the second step neighbors? My assumption would be no.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:333,usability,user,user-images,333,"@LuckyMD matrix multiplication on sparse matrices is actually a pretty efficient version of breadth first search, as [used by graphBLAS](http://arxiv.org/abs/1606.05790v2). Here's an example for a BFS from a specific point (which you can expand to all points by using the identity matrix):. <img width=""447"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/95168561-360eec00-07fd-11eb-96b9-19fbf3871d02.png"">. @Koncopd, I think that should work, since you're adding a self edge so you have redundancy at each step. . A separate point of contention on handling it like this: do you want the first step neighbors to have the same weight as the second step neighbors? My assumption would be no.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:69,interoperability,share,share,69,"Btw, I've separated the visium and non-visium case (since they don't share much code), added tests for the visium case, and removed the warnings. I've rebased so I could re-use some test data from another PR.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:93,safety,test,tests,93,"Btw, I've separated the visium and non-visium case (since they don't share much code), added tests for the visium case, and removed the warnings. I've rebased so I could re-use some test data from another PR.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:182,safety,test,test,182,"Btw, I've separated the visium and non-visium case (since they don't share much code), added tests for the visium case, and removed the warnings. I've rebased so I could re-use some test data from another PR.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:93,testability,test,tests,93,"Btw, I've separated the visium and non-visium case (since they don't share much code), added tests for the visium case, and removed the warnings. I've rebased so I could re-use some test data from another PR.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:182,testability,test,test,182,"Btw, I've separated the visium and non-visium case (since they don't share much code), added tests for the visium case, and removed the warnings. I've rebased so I could re-use some test data from another PR.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:83,deployability,version,version,83,"> @LuckyMD matrix multiplication on sparse matrices is actually a pretty efficient version of breadth first search, as [used by graphBLAS](http://arxiv.org/abs/1606.05790v2). Hmm... i had no idea it was more efficient than using queues, but Figure 10 suggests otherwise. Matrix multiplication definitely seems easier. > do you want the first step neighbors to have the same weight as the second step neighbors? You could keep these separate by binarizing the adjacency matrix before doing the multiplication. The neighbors that are only reachable via the Nth-hop should always have a 1 in the N-th matrix product that way.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:83,integrability,version,version,83,"> @LuckyMD matrix multiplication on sparse matrices is actually a pretty efficient version of breadth first search, as [used by graphBLAS](http://arxiv.org/abs/1606.05790v2). Hmm... i had no idea it was more efficient than using queues, but Figure 10 suggests otherwise. Matrix multiplication definitely seems easier. > do you want the first step neighbors to have the same weight as the second step neighbors? You could keep these separate by binarizing the adjacency matrix before doing the multiplication. The neighbors that are only reachable via the Nth-hop should always have a 1 in the N-th matrix product that way.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:229,integrability,queue,queues,229,"> @LuckyMD matrix multiplication on sparse matrices is actually a pretty efficient version of breadth first search, as [used by graphBLAS](http://arxiv.org/abs/1606.05790v2). Hmm... i had no idea it was more efficient than using queues, but Figure 10 suggests otherwise. Matrix multiplication definitely seems easier. > do you want the first step neighbors to have the same weight as the second step neighbors? You could keep these separate by binarizing the adjacency matrix before doing the multiplication. The neighbors that are only reachable via the Nth-hop should always have a 1 in the N-th matrix product that way.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:83,modifiability,version,version,83,"> @LuckyMD matrix multiplication on sparse matrices is actually a pretty efficient version of breadth first search, as [used by graphBLAS](http://arxiv.org/abs/1606.05790v2). Hmm... i had no idea it was more efficient than using queues, but Figure 10 suggests otherwise. Matrix multiplication definitely seems easier. > do you want the first step neighbors to have the same weight as the second step neighbors? You could keep these separate by binarizing the adjacency matrix before doing the multiplication. The neighbors that are only reachable via the Nth-hop should always have a 1 in the N-th matrix product that way.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:229,performance,queue,queues,229,"> @LuckyMD matrix multiplication on sparse matrices is actually a pretty efficient version of breadth first search, as [used by graphBLAS](http://arxiv.org/abs/1606.05790v2). Hmm... i had no idea it was more efficient than using queues, but Figure 10 suggests otherwise. Matrix multiplication definitely seems easier. > do you want the first step neighbors to have the same weight as the second step neighbors? You could keep these separate by binarizing the adjacency matrix before doing the multiplication. The neighbors that are only reachable via the Nth-hop should always have a 1 in the N-th matrix product that way.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:73,usability,efficien,efficient,73,"> @LuckyMD matrix multiplication on sparse matrices is actually a pretty efficient version of breadth first search, as [used by graphBLAS](http://arxiv.org/abs/1606.05790v2). Hmm... i had no idea it was more efficient than using queues, but Figure 10 suggests otherwise. Matrix multiplication definitely seems easier. > do you want the first step neighbors to have the same weight as the second step neighbors? You could keep these separate by binarizing the adjacency matrix before doing the multiplication. The neighbors that are only reachable via the Nth-hop should always have a 1 in the N-th matrix product that way.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:208,usability,efficien,efficient,208,"> @LuckyMD matrix multiplication on sparse matrices is actually a pretty efficient version of breadth first search, as [used by graphBLAS](http://arxiv.org/abs/1606.05790v2). Hmm... i had no idea it was more efficient than using queues, but Figure 10 suggests otherwise. Matrix multiplication definitely seems easier. > do you want the first step neighbors to have the same weight as the second step neighbors? You could keep these separate by binarizing the adjacency matrix before doing the multiplication. The neighbors that are only reachable via the Nth-hop should always have a 1 in the N-th matrix product that way.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:700,deployability,updat,update,700,"> You could keep these separate by binarizing the adjacency matrix before doing the multiplication. The neighbors that are only reachable via the Nth-hop should always have a 1 in the N-th matrix product that way. Thats a problem with this strategy. Since it's an undirected graph, a node is it's own second neighbor. Since it's a hexagonal grid my first neighbors are also my second neighbors as well as my nth neighbors (in most cases, if there are edges or missing cells this might not be the case). We would either have to do our own BFS which precludes back tracking (i.e. for each search from each node remove previously visited edges), or we could take the difference of the edge sets at each update. Taking the difference would probably be easier.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:700,safety,updat,update,700,"> You could keep these separate by binarizing the adjacency matrix before doing the multiplication. The neighbors that are only reachable via the Nth-hop should always have a 1 in the N-th matrix product that way. Thats a problem with this strategy. Since it's an undirected graph, a node is it's own second neighbor. Since it's a hexagonal grid my first neighbors are also my second neighbors as well as my nth neighbors (in most cases, if there are edges or missing cells this might not be the case). We would either have to do our own BFS which precludes back tracking (i.e. for each search from each node remove previously visited edges), or we could take the difference of the edge sets at each update. Taking the difference would probably be easier.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:700,security,updat,update,700,"> You could keep these separate by binarizing the adjacency matrix before doing the multiplication. The neighbors that are only reachable via the Nth-hop should always have a 1 in the N-th matrix product that way. Thats a problem with this strategy. Since it's an undirected graph, a node is it's own second neighbor. Since it's a hexagonal grid my first neighbors are also my second neighbors as well as my nth neighbors (in most cases, if there are edges or missing cells this might not be the case). We would either have to do our own BFS which precludes back tracking (i.e. for each search from each node remove previously visited edges), or we could take the difference of the edge sets at each update. Taking the difference would probably be easier.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:158,energy efficiency,power,power,158,"With self-loops a node is also its first neighbor. That means if you binarize the adjacency matrix and you have a 1 in the adjacency matrix after taking some power, that means there is only 1 way to get to that neighbor and it must thus only be reachable in the N-th hope (N being the power you just multiplied with). You could therefore also just look for the 1s in the matrix after every multiplication.... that might be a bit easier than taking the difference.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:285,energy efficiency,power,power,285,"With self-loops a node is also its first neighbor. That means if you binarize the adjacency matrix and you have a 1 in the adjacency matrix after taking some power, that means there is only 1 way to get to that neighbor and it must thus only be reachable in the N-th hope (N being the power you just multiplied with). You could therefore also just look for the 1s in the matrix after every multiplication.... that might be a bit easier than taking the difference.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:297,availability,operat,operations,297,"Just checked this, and some of the members of the nth ring have more than one neighbor in the nth-1, so checking for values of one doesn't quite work. Also I've been [collecting some functions](https://github.com/ivirshup/sparse_wrapper/blob/master/sparse_wrapper/_core/coordinate_ops.py) for set operations on sparse indices so taking the difference was quite easy . Here's the check:. <details>. <summary> Code </summary>. ```python. import scanpy as sc. from sparse_wrapper._core.coordinate_ops import difference_indices. import numpy as np. from scipy import sparse. def find_steps_val(adj, n_steps):. """"""Finding positions with value of 1."""""". cur = adj.astype(int). diffs = [adj.copy()]. for i in range(n_steps):. cur.data[:] = 1. cur = adj @ cur. diffs.append(cur == 1). return diffs. def find_steps(adj, n_steps):. """"""Finding differences in sparsity patterns."""""". diffs = [adj.copy()]. prev = adj + sparse.eye(adj.shape[0]). for i in range(n_steps):. cur = prev @ adj. diffs.append(difference_indices(cur, prev)). prev = cur. return diffs. adata = sc.datasets.visium_sge(""V1_Adult_Mouse_Brain""). sc.pp.visium_connectivity(adata). adj = adata.obsp[""spatial_connectivity""]. N = 3. diff_res = find_steps(adj, N-1). val_res = find_steps_val(adj, N-1). for i in range(N):. adata.obs[f""val_res_{i}""] = np.ravel(val_res[i][50, :].toarray()).astype(int). adata.obs[f""diff_res_{i}""] = np.ravel(diff_res[i][50, :].toarray()).astype(int). sc.pl.spatial(. adata,. color = [f""val_res_{i}"" for i in range(N)] + [f""diff_res_{i}"" for i in range(N)],. ncols=N. ). ```. </details>. This is a plot of the neighbors of the 50th well at steps 1, 2, and 3 from each method. Top is finding positions that equaled 1, bottom is taking the differences of the sparsity patterns. ![image](https://user-images.githubusercontent.com/8238804/95439554-8031d200-09a3-11eb-890d-9bc633863d55.png). Either way, these look kinda pretty:. <details>. <summary> </summary>. ```python. from operator import add. from functools import",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:1958,availability,operat,operator,1958,"heck:. <details>. <summary> Code </summary>. ```python. import scanpy as sc. from sparse_wrapper._core.coordinate_ops import difference_indices. import numpy as np. from scipy import sparse. def find_steps_val(adj, n_steps):. """"""Finding positions with value of 1."""""". cur = adj.astype(int). diffs = [adj.copy()]. for i in range(n_steps):. cur.data[:] = 1. cur = adj @ cur. diffs.append(cur == 1). return diffs. def find_steps(adj, n_steps):. """"""Finding differences in sparsity patterns."""""". diffs = [adj.copy()]. prev = adj + sparse.eye(adj.shape[0]). for i in range(n_steps):. cur = prev @ adj. diffs.append(difference_indices(cur, prev)). prev = cur. return diffs. adata = sc.datasets.visium_sge(""V1_Adult_Mouse_Brain""). sc.pp.visium_connectivity(adata). adj = adata.obsp[""spatial_connectivity""]. N = 3. diff_res = find_steps(adj, N-1). val_res = find_steps_val(adj, N-1). for i in range(N):. adata.obs[f""val_res_{i}""] = np.ravel(val_res[i][50, :].toarray()).astype(int). adata.obs[f""diff_res_{i}""] = np.ravel(diff_res[i][50, :].toarray()).astype(int). sc.pl.spatial(. adata,. color = [f""val_res_{i}"" for i in range(N)] + [f""diff_res_{i}"" for i in range(N)],. ncols=N. ). ```. </details>. This is a plot of the neighbors of the 50th well at steps 1, 2, and 3 from each method. Top is finding positions that equaled 1, bottom is taking the differences of the sparsity patterns. ![image](https://user-images.githubusercontent.com/8238804/95439554-8031d200-09a3-11eb-890d-9bc633863d55.png). Either way, these look kinda pretty:. <details>. <summary> </summary>. ```python. from operator import add. from functools import reduce. cells = np.random.choice(adata.n_obs, 20, replace=False). adata.obs[""fireworks""] = (. reduce(add, (diff_res[i][cells] * (3 - i) for i in range(3))). .max(axis=0). .toarray(). .ravel(). ). sc.pl.spatial(adata, color=""fireworks"", cmap=""inferno""). ```. </details>. ![image](https://user-images.githubusercontent.com/8238804/95443265-1962e780-09a8-11eb-9683-92f28574f0f7.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:2001,energy efficiency,reduc,reduce,2001,"heck:. <details>. <summary> Code </summary>. ```python. import scanpy as sc. from sparse_wrapper._core.coordinate_ops import difference_indices. import numpy as np. from scipy import sparse. def find_steps_val(adj, n_steps):. """"""Finding positions with value of 1."""""". cur = adj.astype(int). diffs = [adj.copy()]. for i in range(n_steps):. cur.data[:] = 1. cur = adj @ cur. diffs.append(cur == 1). return diffs. def find_steps(adj, n_steps):. """"""Finding differences in sparsity patterns."""""". diffs = [adj.copy()]. prev = adj + sparse.eye(adj.shape[0]). for i in range(n_steps):. cur = prev @ adj. diffs.append(difference_indices(cur, prev)). prev = cur. return diffs. adata = sc.datasets.visium_sge(""V1_Adult_Mouse_Brain""). sc.pp.visium_connectivity(adata). adj = adata.obsp[""spatial_connectivity""]. N = 3. diff_res = find_steps(adj, N-1). val_res = find_steps_val(adj, N-1). for i in range(N):. adata.obs[f""val_res_{i}""] = np.ravel(val_res[i][50, :].toarray()).astype(int). adata.obs[f""diff_res_{i}""] = np.ravel(diff_res[i][50, :].toarray()).astype(int). sc.pl.spatial(. adata,. color = [f""val_res_{i}"" for i in range(N)] + [f""diff_res_{i}"" for i in range(N)],. ncols=N. ). ```. </details>. This is a plot of the neighbors of the 50th well at steps 1, 2, and 3 from each method. Top is finding positions that equaled 1, bottom is taking the differences of the sparsity patterns. ![image](https://user-images.githubusercontent.com/8238804/95439554-8031d200-09a3-11eb-890d-9bc633863d55.png). Either way, these look kinda pretty:. <details>. <summary> </summary>. ```python. from operator import add. from functools import reduce. cells = np.random.choice(adata.n_obs, 20, replace=False). adata.obs[""fireworks""] = (. reduce(add, (diff_res[i][cells] * (3 - i) for i in range(3))). .max(axis=0). .toarray(). .ravel(). ). sc.pl.spatial(adata, color=""fireworks"", cmap=""inferno""). ```. </details>. ![image](https://user-images.githubusercontent.com/8238804/95443265-1962e780-09a8-11eb-9683-92f28574f0f7.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:2095,energy efficiency,reduc,reduce,2095,"heck:. <details>. <summary> Code </summary>. ```python. import scanpy as sc. from sparse_wrapper._core.coordinate_ops import difference_indices. import numpy as np. from scipy import sparse. def find_steps_val(adj, n_steps):. """"""Finding positions with value of 1."""""". cur = adj.astype(int). diffs = [adj.copy()]. for i in range(n_steps):. cur.data[:] = 1. cur = adj @ cur. diffs.append(cur == 1). return diffs. def find_steps(adj, n_steps):. """"""Finding differences in sparsity patterns."""""". diffs = [adj.copy()]. prev = adj + sparse.eye(adj.shape[0]). for i in range(n_steps):. cur = prev @ adj. diffs.append(difference_indices(cur, prev)). prev = cur. return diffs. adata = sc.datasets.visium_sge(""V1_Adult_Mouse_Brain""). sc.pp.visium_connectivity(adata). adj = adata.obsp[""spatial_connectivity""]. N = 3. diff_res = find_steps(adj, N-1). val_res = find_steps_val(adj, N-1). for i in range(N):. adata.obs[f""val_res_{i}""] = np.ravel(val_res[i][50, :].toarray()).astype(int). adata.obs[f""diff_res_{i}""] = np.ravel(diff_res[i][50, :].toarray()).astype(int). sc.pl.spatial(. adata,. color = [f""val_res_{i}"" for i in range(N)] + [f""diff_res_{i}"" for i in range(N)],. ncols=N. ). ```. </details>. This is a plot of the neighbors of the 50th well at steps 1, 2, and 3 from each method. Top is finding positions that equaled 1, bottom is taking the differences of the sparsity patterns. ![image](https://user-images.githubusercontent.com/8238804/95439554-8031d200-09a3-11eb-890d-9bc633863d55.png). Either way, these look kinda pretty:. <details>. <summary> </summary>. ```python. from operator import add. from functools import reduce. cells = np.random.choice(adata.n_obs, 20, replace=False). adata.obs[""fireworks""] = (. reduce(add, (diff_res[i][cells] * (3 - i) for i in range(3))). .max(axis=0). .toarray(). .ravel(). ). sc.pl.spatial(adata, color=""fireworks"", cmap=""inferno""). ```. </details>. ![image](https://user-images.githubusercontent.com/8238804/95443265-1962e780-09a8-11eb-9683-92f28574f0f7.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:131,reliability,doe,doesn,131,"Just checked this, and some of the members of the nth ring have more than one neighbor in the nth-1, so checking for values of one doesn't quite work. Also I've been [collecting some functions](https://github.com/ivirshup/sparse_wrapper/blob/master/sparse_wrapper/_core/coordinate_ops.py) for set operations on sparse indices so taking the difference was quite easy . Here's the check:. <details>. <summary> Code </summary>. ```python. import scanpy as sc. from sparse_wrapper._core.coordinate_ops import difference_indices. import numpy as np. from scipy import sparse. def find_steps_val(adj, n_steps):. """"""Finding positions with value of 1."""""". cur = adj.astype(int). diffs = [adj.copy()]. for i in range(n_steps):. cur.data[:] = 1. cur = adj @ cur. diffs.append(cur == 1). return diffs. def find_steps(adj, n_steps):. """"""Finding differences in sparsity patterns."""""". diffs = [adj.copy()]. prev = adj + sparse.eye(adj.shape[0]). for i in range(n_steps):. cur = prev @ adj. diffs.append(difference_indices(cur, prev)). prev = cur. return diffs. adata = sc.datasets.visium_sge(""V1_Adult_Mouse_Brain""). sc.pp.visium_connectivity(adata). adj = adata.obsp[""spatial_connectivity""]. N = 3. diff_res = find_steps(adj, N-1). val_res = find_steps_val(adj, N-1). for i in range(N):. adata.obs[f""val_res_{i}""] = np.ravel(val_res[i][50, :].toarray()).astype(int). adata.obs[f""diff_res_{i}""] = np.ravel(diff_res[i][50, :].toarray()).astype(int). sc.pl.spatial(. adata,. color = [f""val_res_{i}"" for i in range(N)] + [f""diff_res_{i}"" for i in range(N)],. ncols=N. ). ```. </details>. This is a plot of the neighbors of the 50th well at steps 1, 2, and 3 from each method. Top is finding positions that equaled 1, bottom is taking the differences of the sparsity patterns. ![image](https://user-images.githubusercontent.com/8238804/95439554-8031d200-09a3-11eb-890d-9bc633863d55.png). Either way, these look kinda pretty:. <details>. <summary> </summary>. ```python. from operator import add. from functools import",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:1777,usability,user,user-images,1777,"heck:. <details>. <summary> Code </summary>. ```python. import scanpy as sc. from sparse_wrapper._core.coordinate_ops import difference_indices. import numpy as np. from scipy import sparse. def find_steps_val(adj, n_steps):. """"""Finding positions with value of 1."""""". cur = adj.astype(int). diffs = [adj.copy()]. for i in range(n_steps):. cur.data[:] = 1. cur = adj @ cur. diffs.append(cur == 1). return diffs. def find_steps(adj, n_steps):. """"""Finding differences in sparsity patterns."""""". diffs = [adj.copy()]. prev = adj + sparse.eye(adj.shape[0]). for i in range(n_steps):. cur = prev @ adj. diffs.append(difference_indices(cur, prev)). prev = cur. return diffs. adata = sc.datasets.visium_sge(""V1_Adult_Mouse_Brain""). sc.pp.visium_connectivity(adata). adj = adata.obsp[""spatial_connectivity""]. N = 3. diff_res = find_steps(adj, N-1). val_res = find_steps_val(adj, N-1). for i in range(N):. adata.obs[f""val_res_{i}""] = np.ravel(val_res[i][50, :].toarray()).astype(int). adata.obs[f""diff_res_{i}""] = np.ravel(diff_res[i][50, :].toarray()).astype(int). sc.pl.spatial(. adata,. color = [f""val_res_{i}"" for i in range(N)] + [f""diff_res_{i}"" for i in range(N)],. ncols=N. ). ```. </details>. This is a plot of the neighbors of the 50th well at steps 1, 2, and 3 from each method. Top is finding positions that equaled 1, bottom is taking the differences of the sparsity patterns. ![image](https://user-images.githubusercontent.com/8238804/95439554-8031d200-09a3-11eb-890d-9bc633863d55.png). Either way, these look kinda pretty:. <details>. <summary> </summary>. ```python. from operator import add. from functools import reduce. cells = np.random.choice(adata.n_obs, 20, replace=False). adata.obs[""fireworks""] = (. reduce(add, (diff_res[i][cells] * (3 - i) for i in range(3))). .max(axis=0). .toarray(). .ravel(). ). sc.pl.spatial(adata, color=""fireworks"", cmap=""inferno""). ```. </details>. ![image](https://user-images.githubusercontent.com/8238804/95443265-1962e780-09a8-11eb-9683-92f28574f0f7.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:2288,usability,user,user-images,2288,"heck:. <details>. <summary> Code </summary>. ```python. import scanpy as sc. from sparse_wrapper._core.coordinate_ops import difference_indices. import numpy as np. from scipy import sparse. def find_steps_val(adj, n_steps):. """"""Finding positions with value of 1."""""". cur = adj.astype(int). diffs = [adj.copy()]. for i in range(n_steps):. cur.data[:] = 1. cur = adj @ cur. diffs.append(cur == 1). return diffs. def find_steps(adj, n_steps):. """"""Finding differences in sparsity patterns."""""". diffs = [adj.copy()]. prev = adj + sparse.eye(adj.shape[0]). for i in range(n_steps):. cur = prev @ adj. diffs.append(difference_indices(cur, prev)). prev = cur. return diffs. adata = sc.datasets.visium_sge(""V1_Adult_Mouse_Brain""). sc.pp.visium_connectivity(adata). adj = adata.obsp[""spatial_connectivity""]. N = 3. diff_res = find_steps(adj, N-1). val_res = find_steps_val(adj, N-1). for i in range(N):. adata.obs[f""val_res_{i}""] = np.ravel(val_res[i][50, :].toarray()).astype(int). adata.obs[f""diff_res_{i}""] = np.ravel(diff_res[i][50, :].toarray()).astype(int). sc.pl.spatial(. adata,. color = [f""val_res_{i}"" for i in range(N)] + [f""diff_res_{i}"" for i in range(N)],. ncols=N. ). ```. </details>. This is a plot of the neighbors of the 50th well at steps 1, 2, and 3 from each method. Top is finding positions that equaled 1, bottom is taking the differences of the sparsity patterns. ![image](https://user-images.githubusercontent.com/8238804/95439554-8031d200-09a3-11eb-890d-9bc633863d55.png). Either way, these look kinda pretty:. <details>. <summary> </summary>. ```python. from operator import add. from functools import reduce. cells = np.random.choice(adata.n_obs, 20, replace=False). adata.obs[""fireworks""] = (. reduce(add, (diff_res[i][cells] * (3 - i) for i in range(3))). .max(axis=0). .toarray(). .ravel(). ). sc.pl.spatial(adata, color=""fireworks"", cmap=""inferno""). ```. </details>. ![image](https://user-images.githubusercontent.com/8238804/95443265-1962e780-09a8-11eb-9683-92f28574f0f7.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:210,availability,replic,replicating,210,"> some of the members of the nth ring have more than one neighbor in the nth-1. Damn... yeah, that won't work then. Back to differences of adjacency matrices then i guess. It looks as though your functions are replicating what I assume is going on in the backend of `networkx` anyway. Are you trying to avoid the heavy dependency or why replicate the effort? I would assume that once we need things like label propagation (maybe to denoise label assignment after deconvolving spatial spots?) then `networkx` might come in handy, no?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:337,availability,replic,replicate,337,"> some of the members of the nth ring have more than one neighbor in the nth-1. Damn... yeah, that won't work then. Back to differences of adjacency matrices then i guess. It looks as though your functions are replicating what I assume is going on in the backend of `networkx` anyway. Are you trying to avoid the heavy dependency or why replicate the effort? I would assume that once we need things like label propagation (maybe to denoise label assignment after deconvolving spatial spots?) then `networkx` might come in handy, no?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:319,deployability,depend,dependency,319,"> some of the members of the nth ring have more than one neighbor in the nth-1. Damn... yeah, that won't work then. Back to differences of adjacency matrices then i guess. It looks as though your functions are replicating what I assume is going on in the backend of `networkx` anyway. Are you trying to avoid the heavy dependency or why replicate the effort? I would assume that once we need things like label propagation (maybe to denoise label assignment after deconvolving spatial spots?) then `networkx` might come in handy, no?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:319,integrability,depend,dependency,319,"> some of the members of the nth ring have more than one neighbor in the nth-1. Damn... yeah, that won't work then. Back to differences of adjacency matrices then i guess. It looks as though your functions are replicating what I assume is going on in the backend of `networkx` anyway. Are you trying to avoid the heavy dependency or why replicate the effort? I would assume that once we need things like label propagation (maybe to denoise label assignment after deconvolving spatial spots?) then `networkx` might come in handy, no?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:319,modifiability,depend,dependency,319,"> some of the members of the nth ring have more than one neighbor in the nth-1. Damn... yeah, that won't work then. Back to differences of adjacency matrices then i guess. It looks as though your functions are replicating what I assume is going on in the backend of `networkx` anyway. Are you trying to avoid the heavy dependency or why replicate the effort? I would assume that once we need things like label propagation (maybe to denoise label assignment after deconvolving spatial spots?) then `networkx` might come in handy, no?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:463,modifiability,deco,deconvolving,463,"> some of the members of the nth ring have more than one neighbor in the nth-1. Damn... yeah, that won't work then. Back to differences of adjacency matrices then i guess. It looks as though your functions are replicating what I assume is going on in the backend of `networkx` anyway. Are you trying to avoid the heavy dependency or why replicate the effort? I would assume that once we need things like label propagation (maybe to denoise label assignment after deconvolving spatial spots?) then `networkx` might come in handy, no?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:267,performance,network,networkx,267,"> some of the members of the nth ring have more than one neighbor in the nth-1. Damn... yeah, that won't work then. Back to differences of adjacency matrices then i guess. It looks as though your functions are replicating what I assume is going on in the backend of `networkx` anyway. Are you trying to avoid the heavy dependency or why replicate the effort? I would assume that once we need things like label propagation (maybe to denoise label assignment after deconvolving spatial spots?) then `networkx` might come in handy, no?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:498,performance,network,networkx,498,"> some of the members of the nth ring have more than one neighbor in the nth-1. Damn... yeah, that won't work then. Back to differences of adjacency matrices then i guess. It looks as though your functions are replicating what I assume is going on in the backend of `networkx` anyway. Are you trying to avoid the heavy dependency or why replicate the effort? I would assume that once we need things like label propagation (maybe to denoise label assignment after deconvolving spatial spots?) then `networkx` might come in handy, no?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:303,safety,avoid,avoid,303,"> some of the members of the nth ring have more than one neighbor in the nth-1. Damn... yeah, that won't work then. Back to differences of adjacency matrices then i guess. It looks as though your functions are replicating what I assume is going on in the backend of `networkx` anyway. Are you trying to avoid the heavy dependency or why replicate the effort? I would assume that once we need things like label propagation (maybe to denoise label assignment after deconvolving spatial spots?) then `networkx` might come in handy, no?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:319,safety,depend,dependency,319,"> some of the members of the nth ring have more than one neighbor in the nth-1. Damn... yeah, that won't work then. Back to differences of adjacency matrices then i guess. It looks as though your functions are replicating what I assume is going on in the backend of `networkx` anyway. Are you trying to avoid the heavy dependency or why replicate the effort? I would assume that once we need things like label propagation (maybe to denoise label assignment after deconvolving spatial spots?) then `networkx` might come in handy, no?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:267,security,network,networkx,267,"> some of the members of the nth ring have more than one neighbor in the nth-1. Damn... yeah, that won't work then. Back to differences of adjacency matrices then i guess. It looks as though your functions are replicating what I assume is going on in the backend of `networkx` anyway. Are you trying to avoid the heavy dependency or why replicate the effort? I would assume that once we need things like label propagation (maybe to denoise label assignment after deconvolving spatial spots?) then `networkx` might come in handy, no?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:498,security,network,networkx,498,"> some of the members of the nth ring have more than one neighbor in the nth-1. Damn... yeah, that won't work then. Back to differences of adjacency matrices then i guess. It looks as though your functions are replicating what I assume is going on in the backend of `networkx` anyway. Are you trying to avoid the heavy dependency or why replicate the effort? I would assume that once we need things like label propagation (maybe to denoise label assignment after deconvolving spatial spots?) then `networkx` might come in handy, no?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:319,testability,depend,dependency,319,"> some of the members of the nth ring have more than one neighbor in the nth-1. Damn... yeah, that won't work then. Back to differences of adjacency matrices then i guess. It looks as though your functions are replicating what I assume is going on in the backend of `networkx` anyway. Are you trying to avoid the heavy dependency or why replicate the effort? I would assume that once we need things like label propagation (maybe to denoise label assignment after deconvolving spatial spots?) then `networkx` might come in handy, no?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:40,availability,replic,replicating,40,"> It looks as though your functions are replicating what I assume is going on in the backend of networkx anyway. So that repo is actually me trying to make it so we can have CSR and CSC matrices that act the same as numpy ndarrays. Those functions are just some building blocks for broadcast operations on those arrays, which are otherwise implemented in C++ in scipy. A lot of this could probably be done with networkx. networkx is just very slow, since it's all pure python.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:292,availability,operat,operations,292,"> It looks as though your functions are replicating what I assume is going on in the backend of networkx anyway. So that repo is actually me trying to make it so we can have CSR and CSC matrices that act the same as numpy ndarrays. Those functions are just some building blocks for broadcast operations on those arrays, which are otherwise implemented in C++ in scipy. A lot of this could probably be done with networkx. networkx is just very slow, since it's all pure python.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:443,availability,slo,slow,443,"> It looks as though your functions are replicating what I assume is going on in the backend of networkx anyway. So that repo is actually me trying to make it so we can have CSR and CSC matrices that act the same as numpy ndarrays. Those functions are just some building blocks for broadcast operations on those arrays, which are otherwise implemented in C++ in scipy. A lot of this could probably be done with networkx. networkx is just very slow, since it's all pure python.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:262,deployability,build,building,262,"> It looks as though your functions are replicating what I assume is going on in the backend of networkx anyway. So that repo is actually me trying to make it so we can have CSR and CSC matrices that act the same as numpy ndarrays. Those functions are just some building blocks for broadcast operations on those arrays, which are otherwise implemented in C++ in scipy. A lot of this could probably be done with networkx. networkx is just very slow, since it's all pure python.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:96,performance,network,networkx,96,"> It looks as though your functions are replicating what I assume is going on in the backend of networkx anyway. So that repo is actually me trying to make it so we can have CSR and CSC matrices that act the same as numpy ndarrays. Those functions are just some building blocks for broadcast operations on those arrays, which are otherwise implemented in C++ in scipy. A lot of this could probably be done with networkx. networkx is just very slow, since it's all pure python.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:411,performance,network,networkx,411,"> It looks as though your functions are replicating what I assume is going on in the backend of networkx anyway. So that repo is actually me trying to make it so we can have CSR and CSC matrices that act the same as numpy ndarrays. Those functions are just some building blocks for broadcast operations on those arrays, which are otherwise implemented in C++ in scipy. A lot of this could probably be done with networkx. networkx is just very slow, since it's all pure python.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:421,performance,network,networkx,421,"> It looks as though your functions are replicating what I assume is going on in the backend of networkx anyway. So that repo is actually me trying to make it so we can have CSR and CSC matrices that act the same as numpy ndarrays. Those functions are just some building blocks for broadcast operations on those arrays, which are otherwise implemented in C++ in scipy. A lot of this could probably be done with networkx. networkx is just very slow, since it's all pure python.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:443,reliability,slo,slow,443,"> It looks as though your functions are replicating what I assume is going on in the backend of networkx anyway. So that repo is actually me trying to make it so we can have CSR and CSC matrices that act the same as numpy ndarrays. Those functions are just some building blocks for broadcast operations on those arrays, which are otherwise implemented in C++ in scipy. A lot of this could probably be done with networkx. networkx is just very slow, since it's all pure python.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:96,security,network,networkx,96,"> It looks as though your functions are replicating what I assume is going on in the backend of networkx anyway. So that repo is actually me trying to make it so we can have CSR and CSC matrices that act the same as numpy ndarrays. Those functions are just some building blocks for broadcast operations on those arrays, which are otherwise implemented in C++ in scipy. A lot of this could probably be done with networkx. networkx is just very slow, since it's all pure python.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:411,security,network,networkx,411,"> It looks as though your functions are replicating what I assume is going on in the backend of networkx anyway. So that repo is actually me trying to make it so we can have CSR and CSC matrices that act the same as numpy ndarrays. Those functions are just some building blocks for broadcast operations on those arrays, which are otherwise implemented in C++ in scipy. A lot of this could probably be done with networkx. networkx is just very slow, since it's all pure python.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:421,security,network,networkx,421,"> It looks as though your functions are replicating what I assume is going on in the backend of networkx anyway. So that repo is actually me trying to make it so we can have CSR and CSC matrices that act the same as numpy ndarrays. Those functions are just some building blocks for broadcast operations on those arrays, which are otherwise implemented in C++ in scipy. A lot of this could probably be done with networkx. networkx is just very slow, since it's all pure python.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:76,availability,slo,slow,76,"> A lot of this could probably be done with networkx. networkx is just very slow, since it's all pure python. Ah, I wasn't aware of that. Cool... then just trying to work with arrays I guess. I hope using spatial graphs doesn't require more than some basic operations in that case.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:257,availability,operat,operations,257,"> A lot of this could probably be done with networkx. networkx is just very slow, since it's all pure python. Ah, I wasn't aware of that. Cool... then just trying to work with arrays I guess. I hope using spatial graphs doesn't require more than some basic operations in that case.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:138,energy efficiency,Cool,Cool,138,"> A lot of this could probably be done with networkx. networkx is just very slow, since it's all pure python. Ah, I wasn't aware of that. Cool... then just trying to work with arrays I guess. I hope using spatial graphs doesn't require more than some basic operations in that case.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:44,performance,network,networkx,44,"> A lot of this could probably be done with networkx. networkx is just very slow, since it's all pure python. Ah, I wasn't aware of that. Cool... then just trying to work with arrays I guess. I hope using spatial graphs doesn't require more than some basic operations in that case.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:54,performance,network,networkx,54,"> A lot of this could probably be done with networkx. networkx is just very slow, since it's all pure python. Ah, I wasn't aware of that. Cool... then just trying to work with arrays I guess. I hope using spatial graphs doesn't require more than some basic operations in that case.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:76,reliability,slo,slow,76,"> A lot of this could probably be done with networkx. networkx is just very slow, since it's all pure python. Ah, I wasn't aware of that. Cool... then just trying to work with arrays I guess. I hope using spatial graphs doesn't require more than some basic operations in that case.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:220,reliability,doe,doesn,220,"> A lot of this could probably be done with networkx. networkx is just very slow, since it's all pure python. Ah, I wasn't aware of that. Cool... then just trying to work with arrays I guess. I hope using spatial graphs doesn't require more than some basic operations in that case.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:44,security,network,networkx,44,"> A lot of this could probably be done with networkx. networkx is just very slow, since it's all pure python. Ah, I wasn't aware of that. Cool... then just trying to work with arrays I guess. I hope using spatial graphs doesn't require more than some basic operations in that case.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:54,security,network,networkx,54,"> A lot of this could probably be done with networkx. networkx is just very slow, since it's all pure python. Ah, I wasn't aware of that. Cool... then just trying to work with arrays I guess. I hope using spatial graphs doesn't require more than some basic operations in that case.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:242,availability,Ping,Pinging,242,"quick practical comment on this very interesting discussion. @ivirshup shall we have this here or moved to the other package under development? We need to take a decision on this because we'll have to see how it works with rest of functions. Pinging @Koncopd as well. Sorry to put pressure but we are on a tight schedule  . re: networkx discussion, also agree with Isaac on having these operations external to networkx.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:388,availability,operat,operations,388,"quick practical comment on this very interesting discussion. @ivirshup shall we have this here or moved to the other package under development? We need to take a decision on this because we'll have to see how it works with rest of functions. Pinging @Koncopd as well. Sorry to put pressure but we are on a tight schedule  . re: networkx discussion, also agree with Isaac on having these operations external to networkx.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:312,energy efficiency,schedul,schedule,312,"quick practical comment on this very interesting discussion. @ivirshup shall we have this here or moved to the other package under development? We need to take a decision on this because we'll have to see how it works with rest of functions. Pinging @Koncopd as well. Sorry to put pressure but we are on a tight schedule  . re: networkx discussion, also agree with Isaac on having these operations external to networkx.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:117,modifiability,pac,package,117,"quick practical comment on this very interesting discussion. @ivirshup shall we have this here or moved to the other package under development? We need to take a decision on this because we'll have to see how it works with rest of functions. Pinging @Koncopd as well. Sorry to put pressure but we are on a tight schedule  . re: networkx discussion, also agree with Isaac on having these operations external to networkx.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:312,performance,schedul,schedule,312,"quick practical comment on this very interesting discussion. @ivirshup shall we have this here or moved to the other package under development? We need to take a decision on this because we'll have to see how it works with rest of functions. Pinging @Koncopd as well. Sorry to put pressure but we are on a tight schedule  . re: networkx discussion, also agree with Isaac on having these operations external to networkx.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:329,performance,network,networkx,329,"quick practical comment on this very interesting discussion. @ivirshup shall we have this here or moved to the other package under development? We need to take a decision on this because we'll have to see how it works with rest of functions. Pinging @Koncopd as well. Sorry to put pressure but we are on a tight schedule  . re: networkx discussion, also agree with Isaac on having these operations external to networkx.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:411,performance,network,networkx,411,"quick practical comment on this very interesting discussion. @ivirshup shall we have this here or moved to the other package under development? We need to take a decision on this because we'll have to see how it works with rest of functions. Pinging @Koncopd as well. Sorry to put pressure but we are on a tight schedule  . re: networkx discussion, also agree with Isaac on having these operations external to networkx.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:6,reliability,pra,practical,6,"quick practical comment on this very interesting discussion. @ivirshup shall we have this here or moved to the other package under development? We need to take a decision on this because we'll have to see how it works with rest of functions. Pinging @Koncopd as well. Sorry to put pressure but we are on a tight schedule  . re: networkx discussion, also agree with Isaac on having these operations external to networkx.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:329,security,network,networkx,329,"quick practical comment on this very interesting discussion. @ivirshup shall we have this here or moved to the other package under development? We need to take a decision on this because we'll have to see how it works with rest of functions. Pinging @Koncopd as well. Sorry to put pressure but we are on a tight schedule  . re: networkx discussion, also agree with Isaac on having these operations external to networkx.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:411,security,network,networkx,411,"quick practical comment on this very interesting discussion. @ivirshup shall we have this here or moved to the other package under development? We need to take a decision on this because we'll have to see how it works with rest of functions. Pinging @Koncopd as well. Sorry to put pressure but we are on a tight schedule  . re: networkx discussion, also agree with Isaac on having these operations external to networkx.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:35,safety,test,tests,35,"I'd like to add this function plus tests to scanpy. I think I'll leave out `n_rings` argument and the `radius_neighbors` functions until there are clear use-cases. I would recommend just having a copy of the code in spatial-tools, which can be deduplicated later.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:35,testability,test,tests,35,"I'd like to add this function plus tests to scanpy. I think I'll leave out `n_rings` argument and the `radius_neighbors` functions until there are clear use-cases. I would recommend just having a copy of the code in spatial-tools, which can be deduplicated later.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:147,usability,clear,clear,147,"I'd like to add this function plus tests to scanpy. I think I'll leave out `n_rings` argument and the `radius_neighbors` functions until there are clear use-cases. I would recommend just having a copy of the code in spatial-tools, which can be deduplicated later.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:224,usability,tool,tools,224,"I'd like to add this function plus tests to scanpy. I think I'll leave out `n_rings` argument and the `radius_neighbors` functions until there are clear use-cases. I would recommend just having a copy of the code in spatial-tools, which can be deduplicated later.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:192,testability,context,context,192,"ok that's great, thank you! The `radius_neighbors` have very clear applications in fish-like data, and we are assmebling a tutorial to show exactly that. The n-rings as well especially in the context of cell-cell communication (although did not check that systematically yet). So shall we add this functionality to spatial-tools ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:61,usability,clear,clear,61,"ok that's great, thank you! The `radius_neighbors` have very clear applications in fish-like data, and we are assmebling a tutorial to show exactly that. The n-rings as well especially in the context of cell-cell communication (although did not check that systematically yet). So shall we add this functionality to spatial-tools ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:323,usability,tool,tools,323,"ok that's great, thank you! The `radius_neighbors` have very clear applications in fish-like data, and we are assmebling a tutorial to show exactly that. The n-rings as well especially in the context of cell-cell communication (although did not check that systematically yet). So shall we add this functionality to spatial-tools ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:79,safety,test,test,79,"I could probably be convinced to include `radius_neighbors`. I'd mainly need a test case. My initial opposition was that (1) it's a pretty trivial implementation and (2) without a real example I'm not sure if it's missing any obvious edge cases. For n-rings, I think that walking some steps from each node generalizes beyond graph construction, and might be reasonable to have as a separate method. I also haven't seen any recommendations about how to weigh the edges, which I think is pretty important.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:79,testability,test,test,79,"I could probably be convinced to include `radius_neighbors`. I'd mainly need a test case. My initial opposition was that (1) it's a pretty trivial implementation and (2) without a real example I'm not sure if it's missing any obvious edge cases. For n-rings, I think that walking some steps from each node generalizes beyond graph construction, and might be reasonable to have as a separate method. I also haven't seen any recommendations about how to weigh the edges, which I think is pretty important.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:62,safety,test,test,62,Yeah the edge weighing is definitely not clear we would've to test that. ok so only tests are missing ?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:84,safety,test,tests,84,Yeah the edge weighing is definitely not clear we would've to test that. ok so only tests are missing ?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:62,testability,test,test,62,Yeah the edge weighing is definitely not clear we would've to test that. ok so only tests are missing ?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:84,testability,test,tests,84,Yeah the edge weighing is definitely not clear we would've to test that. ok so only tests are missing ?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:41,usability,clear,clear,41,Yeah the edge weighing is definitely not clear we would've to test that. ok so only tests are missing ?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:118,integrability,transform,transform,118,"ok, after talking with @davidsebfischer we definitely want to have both radius as well as n_rings and also have graph transform functions so to weight edges differently (based on different kernels). I'd suggest to keep the function in spatial-tools and then decide what to port to scanpy. @ivirshup regarding use cases (for radius, n_rings, and edge weighing) there are multiple ones from graph convolutional networks application, but it could be interesting also just for the other graph analysis that we have implemented in spatial-tools. We'd have to do extensive interpretation of results of course to prove that it really is useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:118,interoperability,transform,transform,118,"ok, after talking with @davidsebfischer we definitely want to have both radius as well as n_rings and also have graph transform functions so to weight edges differently (based on different kernels). I'd suggest to keep the function in spatial-tools and then decide what to port to scanpy. @ivirshup regarding use cases (for radius, n_rings, and edge weighing) there are multiple ones from graph convolutional networks application, but it could be interesting also just for the other graph analysis that we have implemented in spatial-tools. We'd have to do extensive interpretation of results of course to prove that it really is useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:557,modifiability,extens,extensive,557,"ok, after talking with @davidsebfischer we definitely want to have both radius as well as n_rings and also have graph transform functions so to weight edges differently (based on different kernels). I'd suggest to keep the function in spatial-tools and then decide what to port to scanpy. @ivirshup regarding use cases (for radius, n_rings, and edge weighing) there are multiple ones from graph convolutional networks application, but it could be interesting also just for the other graph analysis that we have implemented in spatial-tools. We'd have to do extensive interpretation of results of course to prove that it really is useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:409,performance,network,networks,409,"ok, after talking with @davidsebfischer we definitely want to have both radius as well as n_rings and also have graph transform functions so to weight edges differently (based on different kernels). I'd suggest to keep the function in spatial-tools and then decide what to port to scanpy. @ivirshup regarding use cases (for radius, n_rings, and edge weighing) there are multiple ones from graph convolutional networks application, but it could be interesting also just for the other graph analysis that we have implemented in spatial-tools. We'd have to do extensive interpretation of results of course to prove that it really is useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:409,security,network,networks,409,"ok, after talking with @davidsebfischer we definitely want to have both radius as well as n_rings and also have graph transform functions so to weight edges differently (based on different kernels). I'd suggest to keep the function in spatial-tools and then decide what to port to scanpy. @ivirshup regarding use cases (for radius, n_rings, and edge weighing) there are multiple ones from graph convolutional networks application, but it could be interesting also just for the other graph analysis that we have implemented in spatial-tools. We'd have to do extensive interpretation of results of course to prove that it really is useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:243,usability,tool,tools,243,"ok, after talking with @davidsebfischer we definitely want to have both radius as well as n_rings and also have graph transform functions so to weight edges differently (based on different kernels). I'd suggest to keep the function in spatial-tools and then decide what to port to scanpy. @ivirshup regarding use cases (for radius, n_rings, and edge weighing) there are multiple ones from graph convolutional networks application, but it could be interesting also just for the other graph analysis that we have implemented in spatial-tools. We'd have to do extensive interpretation of results of course to prove that it really is useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:534,usability,tool,tools,534,"ok, after talking with @davidsebfischer we definitely want to have both radius as well as n_rings and also have graph transform functions so to weight edges differently (based on different kernels). I'd suggest to keep the function in spatial-tools and then decide what to port to scanpy. @ivirshup regarding use cases (for radius, n_rings, and edge weighing) there are multiple ones from graph convolutional networks application, but it could be interesting also just for the other graph analysis that we have implemented in spatial-tools. We'd have to do extensive interpretation of results of course to prove that it really is useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1383:56,usability,close,closed,56,super interesting discussion btw. . I think this can be closed,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383
https://github.com/scverse/scanpy/pull/1384:34,usability,tool,tool,34,"@gokceneraslan I'm going to try a tool for doing some back porting here, sorry about the noise!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1384
https://github.com/scverse/scanpy/pull/1384:25,safety,test,test-backports,25,@meeseeksdev backport to test-backports,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1384
https://github.com/scverse/scanpy/pull/1384:25,testability,test,test-backports,25,@meeseeksdev backport to test-backports,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1384
https://github.com/scverse/scanpy/issues/1387:559,deployability,instal,installation,559,"This makes me think of a few things. ### Moving 10x reading functions to `anndata`. Initially the idea was all single cell stuff should go into scanpy. Since we read loom into anndata, I think we'd be okay with putting the 10x readers into anndata, especially if there is broad consensus this would make development of other packages easier. However, they would need to be re-written to use h5py instead of pytables. ### `scanpy` as a requirement. Is the requirement of scanpy so bad? If there are pain points here, should we be trying to make a basic scanpy installation lighter weight? ### Splitting off new modules. Finally, the idea of having IO functions go into their own package. I think this is a much bigger change, and I'd like to see a more fleshed out case for it. This would add a fair bit of complexity to development, so I'd want to be sure it's worth it. Some general questions I have:. * What are the advantages/ disadvantages of having smaller sub-packages? * How does this impact users vs. developers? * Is IO special, or should more parts go into sub-packages? * What gets re-exported from ""main"" modules? * Who manages the sub-packages? A more specific question: how modular of a component is IO? In some cases, like reading a transcriptomic only datasets, I'd say very. For more recent developments, like visium, I'm not sure this is the case. What we read in, and how we represent it, is very tightly coupled to the methods we have. Until the data's been around for a bit longer, I think it would make sense to keep visium IO in the same package as methods for it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:610,deployability,modul,modules,610,"This makes me think of a few things. ### Moving 10x reading functions to `anndata`. Initially the idea was all single cell stuff should go into scanpy. Since we read loom into anndata, I think we'd be okay with putting the 10x readers into anndata, especially if there is broad consensus this would make development of other packages easier. However, they would need to be re-written to use h5py instead of pytables. ### `scanpy` as a requirement. Is the requirement of scanpy so bad? If there are pain points here, should we be trying to make a basic scanpy installation lighter weight? ### Splitting off new modules. Finally, the idea of having IO functions go into their own package. I think this is a much bigger change, and I'd like to see a more fleshed out case for it. This would add a fair bit of complexity to development, so I'd want to be sure it's worth it. Some general questions I have:. * What are the advantages/ disadvantages of having smaller sub-packages? * How does this impact users vs. developers? * Is IO special, or should more parts go into sub-packages? * What gets re-exported from ""main"" modules? * Who manages the sub-packages? A more specific question: how modular of a component is IO? In some cases, like reading a transcriptomic only datasets, I'd say very. For more recent developments, like visium, I'm not sure this is the case. What we read in, and how we represent it, is very tightly coupled to the methods we have. Until the data's been around for a bit longer, I think it would make sense to keep visium IO in the same package as methods for it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1117,deployability,modul,modules,1117,"This makes me think of a few things. ### Moving 10x reading functions to `anndata`. Initially the idea was all single cell stuff should go into scanpy. Since we read loom into anndata, I think we'd be okay with putting the 10x readers into anndata, especially if there is broad consensus this would make development of other packages easier. However, they would need to be re-written to use h5py instead of pytables. ### `scanpy` as a requirement. Is the requirement of scanpy so bad? If there are pain points here, should we be trying to make a basic scanpy installation lighter weight? ### Splitting off new modules. Finally, the idea of having IO functions go into their own package. I think this is a much bigger change, and I'd like to see a more fleshed out case for it. This would add a fair bit of complexity to development, so I'd want to be sure it's worth it. Some general questions I have:. * What are the advantages/ disadvantages of having smaller sub-packages? * How does this impact users vs. developers? * Is IO special, or should more parts go into sub-packages? * What gets re-exported from ""main"" modules? * Who manages the sub-packages? A more specific question: how modular of a component is IO? In some cases, like reading a transcriptomic only datasets, I'd say very. For more recent developments, like visium, I'm not sure this is the case. What we read in, and how we represent it, is very tightly coupled to the methods we have. Until the data's been around for a bit longer, I think it would make sense to keep visium IO in the same package as methods for it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1132,deployability,manag,manages,1132,"This makes me think of a few things. ### Moving 10x reading functions to `anndata`. Initially the idea was all single cell stuff should go into scanpy. Since we read loom into anndata, I think we'd be okay with putting the 10x readers into anndata, especially if there is broad consensus this would make development of other packages easier. However, they would need to be re-written to use h5py instead of pytables. ### `scanpy` as a requirement. Is the requirement of scanpy so bad? If there are pain points here, should we be trying to make a basic scanpy installation lighter weight? ### Splitting off new modules. Finally, the idea of having IO functions go into their own package. I think this is a much bigger change, and I'd like to see a more fleshed out case for it. This would add a fair bit of complexity to development, so I'd want to be sure it's worth it. Some general questions I have:. * What are the advantages/ disadvantages of having smaller sub-packages? * How does this impact users vs. developers? * Is IO special, or should more parts go into sub-packages? * What gets re-exported from ""main"" modules? * Who manages the sub-packages? A more specific question: how modular of a component is IO? In some cases, like reading a transcriptomic only datasets, I'd say very. For more recent developments, like visium, I'm not sure this is the case. What we read in, and how we represent it, is very tightly coupled to the methods we have. Until the data's been around for a bit longer, I think it would make sense to keep visium IO in the same package as methods for it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1188,deployability,modul,modular,1188,"This makes me think of a few things. ### Moving 10x reading functions to `anndata`. Initially the idea was all single cell stuff should go into scanpy. Since we read loom into anndata, I think we'd be okay with putting the 10x readers into anndata, especially if there is broad consensus this would make development of other packages easier. However, they would need to be re-written to use h5py instead of pytables. ### `scanpy` as a requirement. Is the requirement of scanpy so bad? If there are pain points here, should we be trying to make a basic scanpy installation lighter weight? ### Splitting off new modules. Finally, the idea of having IO functions go into their own package. I think this is a much bigger change, and I'd like to see a more fleshed out case for it. This would add a fair bit of complexity to development, so I'd want to be sure it's worth it. Some general questions I have:. * What are the advantages/ disadvantages of having smaller sub-packages? * How does this impact users vs. developers? * Is IO special, or should more parts go into sub-packages? * What gets re-exported from ""main"" modules? * Who manages the sub-packages? A more specific question: how modular of a component is IO? In some cases, like reading a transcriptomic only datasets, I'd say very. For more recent developments, like visium, I'm not sure this is the case. What we read in, and how we represent it, is very tightly coupled to the methods we have. Until the data's been around for a bit longer, I think it would make sense to keep visium IO in the same package as methods for it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1132,energy efficiency,manag,manages,1132,"This makes me think of a few things. ### Moving 10x reading functions to `anndata`. Initially the idea was all single cell stuff should go into scanpy. Since we read loom into anndata, I think we'd be okay with putting the 10x readers into anndata, especially if there is broad consensus this would make development of other packages easier. However, they would need to be re-written to use h5py instead of pytables. ### `scanpy` as a requirement. Is the requirement of scanpy so bad? If there are pain points here, should we be trying to make a basic scanpy installation lighter weight? ### Splitting off new modules. Finally, the idea of having IO functions go into their own package. I think this is a much bigger change, and I'd like to see a more fleshed out case for it. This would add a fair bit of complexity to development, so I'd want to be sure it's worth it. Some general questions I have:. * What are the advantages/ disadvantages of having smaller sub-packages? * How does this impact users vs. developers? * Is IO special, or should more parts go into sub-packages? * What gets re-exported from ""main"" modules? * Who manages the sub-packages? A more specific question: how modular of a component is IO? In some cases, like reading a transcriptomic only datasets, I'd say very. For more recent developments, like visium, I'm not sure this is the case. What we read in, and how we represent it, is very tightly coupled to the methods we have. Until the data's been around for a bit longer, I think it would make sense to keep visium IO in the same package as methods for it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:962,integrability,sub,sub-packages,962,"This makes me think of a few things. ### Moving 10x reading functions to `anndata`. Initially the idea was all single cell stuff should go into scanpy. Since we read loom into anndata, I think we'd be okay with putting the 10x readers into anndata, especially if there is broad consensus this would make development of other packages easier. However, they would need to be re-written to use h5py instead of pytables. ### `scanpy` as a requirement. Is the requirement of scanpy so bad? If there are pain points here, should we be trying to make a basic scanpy installation lighter weight? ### Splitting off new modules. Finally, the idea of having IO functions go into their own package. I think this is a much bigger change, and I'd like to see a more fleshed out case for it. This would add a fair bit of complexity to development, so I'd want to be sure it's worth it. Some general questions I have:. * What are the advantages/ disadvantages of having smaller sub-packages? * How does this impact users vs. developers? * Is IO special, or should more parts go into sub-packages? * What gets re-exported from ""main"" modules? * Who manages the sub-packages? A more specific question: how modular of a component is IO? In some cases, like reading a transcriptomic only datasets, I'd say very. For more recent developments, like visium, I'm not sure this is the case. What we read in, and how we represent it, is very tightly coupled to the methods we have. Until the data's been around for a bit longer, I think it would make sense to keep visium IO in the same package as methods for it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1067,integrability,sub,sub-packages,1067,"This makes me think of a few things. ### Moving 10x reading functions to `anndata`. Initially the idea was all single cell stuff should go into scanpy. Since we read loom into anndata, I think we'd be okay with putting the 10x readers into anndata, especially if there is broad consensus this would make development of other packages easier. However, they would need to be re-written to use h5py instead of pytables. ### `scanpy` as a requirement. Is the requirement of scanpy so bad? If there are pain points here, should we be trying to make a basic scanpy installation lighter weight? ### Splitting off new modules. Finally, the idea of having IO functions go into their own package. I think this is a much bigger change, and I'd like to see a more fleshed out case for it. This would add a fair bit of complexity to development, so I'd want to be sure it's worth it. Some general questions I have:. * What are the advantages/ disadvantages of having smaller sub-packages? * How does this impact users vs. developers? * Is IO special, or should more parts go into sub-packages? * What gets re-exported from ""main"" modules? * Who manages the sub-packages? A more specific question: how modular of a component is IO? In some cases, like reading a transcriptomic only datasets, I'd say very. For more recent developments, like visium, I'm not sure this is the case. What we read in, and how we represent it, is very tightly coupled to the methods we have. Until the data's been around for a bit longer, I think it would make sense to keep visium IO in the same package as methods for it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1144,integrability,sub,sub-packages,1144,"This makes me think of a few things. ### Moving 10x reading functions to `anndata`. Initially the idea was all single cell stuff should go into scanpy. Since we read loom into anndata, I think we'd be okay with putting the 10x readers into anndata, especially if there is broad consensus this would make development of other packages easier. However, they would need to be re-written to use h5py instead of pytables. ### `scanpy` as a requirement. Is the requirement of scanpy so bad? If there are pain points here, should we be trying to make a basic scanpy installation lighter weight? ### Splitting off new modules. Finally, the idea of having IO functions go into their own package. I think this is a much bigger change, and I'd like to see a more fleshed out case for it. This would add a fair bit of complexity to development, so I'd want to be sure it's worth it. Some general questions I have:. * What are the advantages/ disadvantages of having smaller sub-packages? * How does this impact users vs. developers? * Is IO special, or should more parts go into sub-packages? * What gets re-exported from ""main"" modules? * Who manages the sub-packages? A more specific question: how modular of a component is IO? In some cases, like reading a transcriptomic only datasets, I'd say very. For more recent developments, like visium, I'm not sure this is the case. What we read in, and how we represent it, is very tightly coupled to the methods we have. Until the data's been around for a bit longer, I think it would make sense to keep visium IO in the same package as methods for it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1188,integrability,modular,modular,1188,"This makes me think of a few things. ### Moving 10x reading functions to `anndata`. Initially the idea was all single cell stuff should go into scanpy. Since we read loom into anndata, I think we'd be okay with putting the 10x readers into anndata, especially if there is broad consensus this would make development of other packages easier. However, they would need to be re-written to use h5py instead of pytables. ### `scanpy` as a requirement. Is the requirement of scanpy so bad? If there are pain points here, should we be trying to make a basic scanpy installation lighter weight? ### Splitting off new modules. Finally, the idea of having IO functions go into their own package. I think this is a much bigger change, and I'd like to see a more fleshed out case for it. This would add a fair bit of complexity to development, so I'd want to be sure it's worth it. Some general questions I have:. * What are the advantages/ disadvantages of having smaller sub-packages? * How does this impact users vs. developers? * Is IO special, or should more parts go into sub-packages? * What gets re-exported from ""main"" modules? * Who manages the sub-packages? A more specific question: how modular of a component is IO? In some cases, like reading a transcriptomic only datasets, I'd say very. For more recent developments, like visium, I'm not sure this is the case. What we read in, and how we represent it, is very tightly coupled to the methods we have. Until the data's been around for a bit longer, I think it would make sense to keep visium IO in the same package as methods for it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1201,integrability,compon,component,1201,"This makes me think of a few things. ### Moving 10x reading functions to `anndata`. Initially the idea was all single cell stuff should go into scanpy. Since we read loom into anndata, I think we'd be okay with putting the 10x readers into anndata, especially if there is broad consensus this would make development of other packages easier. However, they would need to be re-written to use h5py instead of pytables. ### `scanpy` as a requirement. Is the requirement of scanpy so bad? If there are pain points here, should we be trying to make a basic scanpy installation lighter weight? ### Splitting off new modules. Finally, the idea of having IO functions go into their own package. I think this is a much bigger change, and I'd like to see a more fleshed out case for it. This would add a fair bit of complexity to development, so I'd want to be sure it's worth it. Some general questions I have:. * What are the advantages/ disadvantages of having smaller sub-packages? * How does this impact users vs. developers? * Is IO special, or should more parts go into sub-packages? * What gets re-exported from ""main"" modules? * Who manages the sub-packages? A more specific question: how modular of a component is IO? In some cases, like reading a transcriptomic only datasets, I'd say very. For more recent developments, like visium, I'm not sure this is the case. What we read in, and how we represent it, is very tightly coupled to the methods we have. Until the data's been around for a bit longer, I think it would make sense to keep visium IO in the same package as methods for it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1424,integrability,coupl,coupled,1424,"This makes me think of a few things. ### Moving 10x reading functions to `anndata`. Initially the idea was all single cell stuff should go into scanpy. Since we read loom into anndata, I think we'd be okay with putting the 10x readers into anndata, especially if there is broad consensus this would make development of other packages easier. However, they would need to be re-written to use h5py instead of pytables. ### `scanpy` as a requirement. Is the requirement of scanpy so bad? If there are pain points here, should we be trying to make a basic scanpy installation lighter weight? ### Splitting off new modules. Finally, the idea of having IO functions go into their own package. I think this is a much bigger change, and I'd like to see a more fleshed out case for it. This would add a fair bit of complexity to development, so I'd want to be sure it's worth it. Some general questions I have:. * What are the advantages/ disadvantages of having smaller sub-packages? * How does this impact users vs. developers? * Is IO special, or should more parts go into sub-packages? * What gets re-exported from ""main"" modules? * Who manages the sub-packages? A more specific question: how modular of a component is IO? In some cases, like reading a transcriptomic only datasets, I'd say very. For more recent developments, like visium, I'm not sure this is the case. What we read in, and how we represent it, is very tightly coupled to the methods we have. Until the data's been around for a bit longer, I think it would make sense to keep visium IO in the same package as methods for it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1165,interoperability,specif,specific,1165,"This makes me think of a few things. ### Moving 10x reading functions to `anndata`. Initially the idea was all single cell stuff should go into scanpy. Since we read loom into anndata, I think we'd be okay with putting the 10x readers into anndata, especially if there is broad consensus this would make development of other packages easier. However, they would need to be re-written to use h5py instead of pytables. ### `scanpy` as a requirement. Is the requirement of scanpy so bad? If there are pain points here, should we be trying to make a basic scanpy installation lighter weight? ### Splitting off new modules. Finally, the idea of having IO functions go into their own package. I think this is a much bigger change, and I'd like to see a more fleshed out case for it. This would add a fair bit of complexity to development, so I'd want to be sure it's worth it. Some general questions I have:. * What are the advantages/ disadvantages of having smaller sub-packages? * How does this impact users vs. developers? * Is IO special, or should more parts go into sub-packages? * What gets re-exported from ""main"" modules? * Who manages the sub-packages? A more specific question: how modular of a component is IO? In some cases, like reading a transcriptomic only datasets, I'd say very. For more recent developments, like visium, I'm not sure this is the case. What we read in, and how we represent it, is very tightly coupled to the methods we have. Until the data's been around for a bit longer, I think it would make sense to keep visium IO in the same package as methods for it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1201,interoperability,compon,component,1201,"This makes me think of a few things. ### Moving 10x reading functions to `anndata`. Initially the idea was all single cell stuff should go into scanpy. Since we read loom into anndata, I think we'd be okay with putting the 10x readers into anndata, especially if there is broad consensus this would make development of other packages easier. However, they would need to be re-written to use h5py instead of pytables. ### `scanpy` as a requirement. Is the requirement of scanpy so bad? If there are pain points here, should we be trying to make a basic scanpy installation lighter weight? ### Splitting off new modules. Finally, the idea of having IO functions go into their own package. I think this is a much bigger change, and I'd like to see a more fleshed out case for it. This would add a fair bit of complexity to development, so I'd want to be sure it's worth it. Some general questions I have:. * What are the advantages/ disadvantages of having smaller sub-packages? * How does this impact users vs. developers? * Is IO special, or should more parts go into sub-packages? * What gets re-exported from ""main"" modules? * Who manages the sub-packages? A more specific question: how modular of a component is IO? In some cases, like reading a transcriptomic only datasets, I'd say very. For more recent developments, like visium, I'm not sure this is the case. What we read in, and how we represent it, is very tightly coupled to the methods we have. Until the data's been around for a bit longer, I think it would make sense to keep visium IO in the same package as methods for it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:325,modifiability,pac,packages,325,"This makes me think of a few things. ### Moving 10x reading functions to `anndata`. Initially the idea was all single cell stuff should go into scanpy. Since we read loom into anndata, I think we'd be okay with putting the 10x readers into anndata, especially if there is broad consensus this would make development of other packages easier. However, they would need to be re-written to use h5py instead of pytables. ### `scanpy` as a requirement. Is the requirement of scanpy so bad? If there are pain points here, should we be trying to make a basic scanpy installation lighter weight? ### Splitting off new modules. Finally, the idea of having IO functions go into their own package. I think this is a much bigger change, and I'd like to see a more fleshed out case for it. This would add a fair bit of complexity to development, so I'd want to be sure it's worth it. Some general questions I have:. * What are the advantages/ disadvantages of having smaller sub-packages? * How does this impact users vs. developers? * Is IO special, or should more parts go into sub-packages? * What gets re-exported from ""main"" modules? * Who manages the sub-packages? A more specific question: how modular of a component is IO? In some cases, like reading a transcriptomic only datasets, I'd say very. For more recent developments, like visium, I'm not sure this is the case. What we read in, and how we represent it, is very tightly coupled to the methods we have. Until the data's been around for a bit longer, I think it would make sense to keep visium IO in the same package as methods for it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:610,modifiability,modul,modules,610,"This makes me think of a few things. ### Moving 10x reading functions to `anndata`. Initially the idea was all single cell stuff should go into scanpy. Since we read loom into anndata, I think we'd be okay with putting the 10x readers into anndata, especially if there is broad consensus this would make development of other packages easier. However, they would need to be re-written to use h5py instead of pytables. ### `scanpy` as a requirement. Is the requirement of scanpy so bad? If there are pain points here, should we be trying to make a basic scanpy installation lighter weight? ### Splitting off new modules. Finally, the idea of having IO functions go into their own package. I think this is a much bigger change, and I'd like to see a more fleshed out case for it. This would add a fair bit of complexity to development, so I'd want to be sure it's worth it. Some general questions I have:. * What are the advantages/ disadvantages of having smaller sub-packages? * How does this impact users vs. developers? * Is IO special, or should more parts go into sub-packages? * What gets re-exported from ""main"" modules? * Who manages the sub-packages? A more specific question: how modular of a component is IO? In some cases, like reading a transcriptomic only datasets, I'd say very. For more recent developments, like visium, I'm not sure this is the case. What we read in, and how we represent it, is very tightly coupled to the methods we have. Until the data's been around for a bit longer, I think it would make sense to keep visium IO in the same package as methods for it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:678,modifiability,pac,package,678,"This makes me think of a few things. ### Moving 10x reading functions to `anndata`. Initially the idea was all single cell stuff should go into scanpy. Since we read loom into anndata, I think we'd be okay with putting the 10x readers into anndata, especially if there is broad consensus this would make development of other packages easier. However, they would need to be re-written to use h5py instead of pytables. ### `scanpy` as a requirement. Is the requirement of scanpy so bad? If there are pain points here, should we be trying to make a basic scanpy installation lighter weight? ### Splitting off new modules. Finally, the idea of having IO functions go into their own package. I think this is a much bigger change, and I'd like to see a more fleshed out case for it. This would add a fair bit of complexity to development, so I'd want to be sure it's worth it. Some general questions I have:. * What are the advantages/ disadvantages of having smaller sub-packages? * How does this impact users vs. developers? * Is IO special, or should more parts go into sub-packages? * What gets re-exported from ""main"" modules? * Who manages the sub-packages? A more specific question: how modular of a component is IO? In some cases, like reading a transcriptomic only datasets, I'd say very. For more recent developments, like visium, I'm not sure this is the case. What we read in, and how we represent it, is very tightly coupled to the methods we have. Until the data's been around for a bit longer, I think it would make sense to keep visium IO in the same package as methods for it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:966,modifiability,pac,packages,966,"This makes me think of a few things. ### Moving 10x reading functions to `anndata`. Initially the idea was all single cell stuff should go into scanpy. Since we read loom into anndata, I think we'd be okay with putting the 10x readers into anndata, especially if there is broad consensus this would make development of other packages easier. However, they would need to be re-written to use h5py instead of pytables. ### `scanpy` as a requirement. Is the requirement of scanpy so bad? If there are pain points here, should we be trying to make a basic scanpy installation lighter weight? ### Splitting off new modules. Finally, the idea of having IO functions go into their own package. I think this is a much bigger change, and I'd like to see a more fleshed out case for it. This would add a fair bit of complexity to development, so I'd want to be sure it's worth it. Some general questions I have:. * What are the advantages/ disadvantages of having smaller sub-packages? * How does this impact users vs. developers? * Is IO special, or should more parts go into sub-packages? * What gets re-exported from ""main"" modules? * Who manages the sub-packages? A more specific question: how modular of a component is IO? In some cases, like reading a transcriptomic only datasets, I'd say very. For more recent developments, like visium, I'm not sure this is the case. What we read in, and how we represent it, is very tightly coupled to the methods we have. Until the data's been around for a bit longer, I think it would make sense to keep visium IO in the same package as methods for it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1071,modifiability,pac,packages,1071,"This makes me think of a few things. ### Moving 10x reading functions to `anndata`. Initially the idea was all single cell stuff should go into scanpy. Since we read loom into anndata, I think we'd be okay with putting the 10x readers into anndata, especially if there is broad consensus this would make development of other packages easier. However, they would need to be re-written to use h5py instead of pytables. ### `scanpy` as a requirement. Is the requirement of scanpy so bad? If there are pain points here, should we be trying to make a basic scanpy installation lighter weight? ### Splitting off new modules. Finally, the idea of having IO functions go into their own package. I think this is a much bigger change, and I'd like to see a more fleshed out case for it. This would add a fair bit of complexity to development, so I'd want to be sure it's worth it. Some general questions I have:. * What are the advantages/ disadvantages of having smaller sub-packages? * How does this impact users vs. developers? * Is IO special, or should more parts go into sub-packages? * What gets re-exported from ""main"" modules? * Who manages the sub-packages? A more specific question: how modular of a component is IO? In some cases, like reading a transcriptomic only datasets, I'd say very. For more recent developments, like visium, I'm not sure this is the case. What we read in, and how we represent it, is very tightly coupled to the methods we have. Until the data's been around for a bit longer, I think it would make sense to keep visium IO in the same package as methods for it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1117,modifiability,modul,modules,1117,"This makes me think of a few things. ### Moving 10x reading functions to `anndata`. Initially the idea was all single cell stuff should go into scanpy. Since we read loom into anndata, I think we'd be okay with putting the 10x readers into anndata, especially if there is broad consensus this would make development of other packages easier. However, they would need to be re-written to use h5py instead of pytables. ### `scanpy` as a requirement. Is the requirement of scanpy so bad? If there are pain points here, should we be trying to make a basic scanpy installation lighter weight? ### Splitting off new modules. Finally, the idea of having IO functions go into their own package. I think this is a much bigger change, and I'd like to see a more fleshed out case for it. This would add a fair bit of complexity to development, so I'd want to be sure it's worth it. Some general questions I have:. * What are the advantages/ disadvantages of having smaller sub-packages? * How does this impact users vs. developers? * Is IO special, or should more parts go into sub-packages? * What gets re-exported from ""main"" modules? * Who manages the sub-packages? A more specific question: how modular of a component is IO? In some cases, like reading a transcriptomic only datasets, I'd say very. For more recent developments, like visium, I'm not sure this is the case. What we read in, and how we represent it, is very tightly coupled to the methods we have. Until the data's been around for a bit longer, I think it would make sense to keep visium IO in the same package as methods for it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1148,modifiability,pac,packages,1148,"This makes me think of a few things. ### Moving 10x reading functions to `anndata`. Initially the idea was all single cell stuff should go into scanpy. Since we read loom into anndata, I think we'd be okay with putting the 10x readers into anndata, especially if there is broad consensus this would make development of other packages easier. However, they would need to be re-written to use h5py instead of pytables. ### `scanpy` as a requirement. Is the requirement of scanpy so bad? If there are pain points here, should we be trying to make a basic scanpy installation lighter weight? ### Splitting off new modules. Finally, the idea of having IO functions go into their own package. I think this is a much bigger change, and I'd like to see a more fleshed out case for it. This would add a fair bit of complexity to development, so I'd want to be sure it's worth it. Some general questions I have:. * What are the advantages/ disadvantages of having smaller sub-packages? * How does this impact users vs. developers? * Is IO special, or should more parts go into sub-packages? * What gets re-exported from ""main"" modules? * Who manages the sub-packages? A more specific question: how modular of a component is IO? In some cases, like reading a transcriptomic only datasets, I'd say very. For more recent developments, like visium, I'm not sure this is the case. What we read in, and how we represent it, is very tightly coupled to the methods we have. Until the data's been around for a bit longer, I think it would make sense to keep visium IO in the same package as methods for it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1188,modifiability,modul,modular,1188,"This makes me think of a few things. ### Moving 10x reading functions to `anndata`. Initially the idea was all single cell stuff should go into scanpy. Since we read loom into anndata, I think we'd be okay with putting the 10x readers into anndata, especially if there is broad consensus this would make development of other packages easier. However, they would need to be re-written to use h5py instead of pytables. ### `scanpy` as a requirement. Is the requirement of scanpy so bad? If there are pain points here, should we be trying to make a basic scanpy installation lighter weight? ### Splitting off new modules. Finally, the idea of having IO functions go into their own package. I think this is a much bigger change, and I'd like to see a more fleshed out case for it. This would add a fair bit of complexity to development, so I'd want to be sure it's worth it. Some general questions I have:. * What are the advantages/ disadvantages of having smaller sub-packages? * How does this impact users vs. developers? * Is IO special, or should more parts go into sub-packages? * What gets re-exported from ""main"" modules? * Who manages the sub-packages? A more specific question: how modular of a component is IO? In some cases, like reading a transcriptomic only datasets, I'd say very. For more recent developments, like visium, I'm not sure this is the case. What we read in, and how we represent it, is very tightly coupled to the methods we have. Until the data's been around for a bit longer, I think it would make sense to keep visium IO in the same package as methods for it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1201,modifiability,compon,component,1201,"This makes me think of a few things. ### Moving 10x reading functions to `anndata`. Initially the idea was all single cell stuff should go into scanpy. Since we read loom into anndata, I think we'd be okay with putting the 10x readers into anndata, especially if there is broad consensus this would make development of other packages easier. However, they would need to be re-written to use h5py instead of pytables. ### `scanpy` as a requirement. Is the requirement of scanpy so bad? If there are pain points here, should we be trying to make a basic scanpy installation lighter weight? ### Splitting off new modules. Finally, the idea of having IO functions go into their own package. I think this is a much bigger change, and I'd like to see a more fleshed out case for it. This would add a fair bit of complexity to development, so I'd want to be sure it's worth it. Some general questions I have:. * What are the advantages/ disadvantages of having smaller sub-packages? * How does this impact users vs. developers? * Is IO special, or should more parts go into sub-packages? * What gets re-exported from ""main"" modules? * Who manages the sub-packages? A more specific question: how modular of a component is IO? In some cases, like reading a transcriptomic only datasets, I'd say very. For more recent developments, like visium, I'm not sure this is the case. What we read in, and how we represent it, is very tightly coupled to the methods we have. Until the data's been around for a bit longer, I think it would make sense to keep visium IO in the same package as methods for it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1424,modifiability,coupl,coupled,1424,"This makes me think of a few things. ### Moving 10x reading functions to `anndata`. Initially the idea was all single cell stuff should go into scanpy. Since we read loom into anndata, I think we'd be okay with putting the 10x readers into anndata, especially if there is broad consensus this would make development of other packages easier. However, they would need to be re-written to use h5py instead of pytables. ### `scanpy` as a requirement. Is the requirement of scanpy so bad? If there are pain points here, should we be trying to make a basic scanpy installation lighter weight? ### Splitting off new modules. Finally, the idea of having IO functions go into their own package. I think this is a much bigger change, and I'd like to see a more fleshed out case for it. This would add a fair bit of complexity to development, so I'd want to be sure it's worth it. Some general questions I have:. * What are the advantages/ disadvantages of having smaller sub-packages? * How does this impact users vs. developers? * Is IO special, or should more parts go into sub-packages? * What gets re-exported from ""main"" modules? * Who manages the sub-packages? A more specific question: how modular of a component is IO? In some cases, like reading a transcriptomic only datasets, I'd say very. For more recent developments, like visium, I'm not sure this is the case. What we read in, and how we represent it, is very tightly coupled to the methods we have. Until the data's been around for a bit longer, I think it would make sense to keep visium IO in the same package as methods for it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1561,modifiability,pac,package,1561,"This makes me think of a few things. ### Moving 10x reading functions to `anndata`. Initially the idea was all single cell stuff should go into scanpy. Since we read loom into anndata, I think we'd be okay with putting the 10x readers into anndata, especially if there is broad consensus this would make development of other packages easier. However, they would need to be re-written to use h5py instead of pytables. ### `scanpy` as a requirement. Is the requirement of scanpy so bad? If there are pain points here, should we be trying to make a basic scanpy installation lighter weight? ### Splitting off new modules. Finally, the idea of having IO functions go into their own package. I think this is a much bigger change, and I'd like to see a more fleshed out case for it. This would add a fair bit of complexity to development, so I'd want to be sure it's worth it. Some general questions I have:. * What are the advantages/ disadvantages of having smaller sub-packages? * How does this impact users vs. developers? * Is IO special, or should more parts go into sub-packages? * What gets re-exported from ""main"" modules? * Who manages the sub-packages? A more specific question: how modular of a component is IO? In some cases, like reading a transcriptomic only datasets, I'd say very. For more recent developments, like visium, I'm not sure this is the case. What we read in, and how we represent it, is very tightly coupled to the methods we have. Until the data's been around for a bit longer, I think it would make sense to keep visium IO in the same package as methods for it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:982,reliability,doe,does,982,"This makes me think of a few things. ### Moving 10x reading functions to `anndata`. Initially the idea was all single cell stuff should go into scanpy. Since we read loom into anndata, I think we'd be okay with putting the 10x readers into anndata, especially if there is broad consensus this would make development of other packages easier. However, they would need to be re-written to use h5py instead of pytables. ### `scanpy` as a requirement. Is the requirement of scanpy so bad? If there are pain points here, should we be trying to make a basic scanpy installation lighter weight? ### Splitting off new modules. Finally, the idea of having IO functions go into their own package. I think this is a much bigger change, and I'd like to see a more fleshed out case for it. This would add a fair bit of complexity to development, so I'd want to be sure it's worth it. Some general questions I have:. * What are the advantages/ disadvantages of having smaller sub-packages? * How does this impact users vs. developers? * Is IO special, or should more parts go into sub-packages? * What gets re-exported from ""main"" modules? * Who manages the sub-packages? A more specific question: how modular of a component is IO? In some cases, like reading a transcriptomic only datasets, I'd say very. For more recent developments, like visium, I'm not sure this is the case. What we read in, and how we represent it, is very tightly coupled to the methods we have. Until the data's been around for a bit longer, I think it would make sense to keep visium IO in the same package as methods for it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:610,safety,modul,modules,610,"This makes me think of a few things. ### Moving 10x reading functions to `anndata`. Initially the idea was all single cell stuff should go into scanpy. Since we read loom into anndata, I think we'd be okay with putting the 10x readers into anndata, especially if there is broad consensus this would make development of other packages easier. However, they would need to be re-written to use h5py instead of pytables. ### `scanpy` as a requirement. Is the requirement of scanpy so bad? If there are pain points here, should we be trying to make a basic scanpy installation lighter weight? ### Splitting off new modules. Finally, the idea of having IO functions go into their own package. I think this is a much bigger change, and I'd like to see a more fleshed out case for it. This would add a fair bit of complexity to development, so I'd want to be sure it's worth it. Some general questions I have:. * What are the advantages/ disadvantages of having smaller sub-packages? * How does this impact users vs. developers? * Is IO special, or should more parts go into sub-packages? * What gets re-exported from ""main"" modules? * Who manages the sub-packages? A more specific question: how modular of a component is IO? In some cases, like reading a transcriptomic only datasets, I'd say very. For more recent developments, like visium, I'm not sure this is the case. What we read in, and how we represent it, is very tightly coupled to the methods we have. Until the data's been around for a bit longer, I think it would make sense to keep visium IO in the same package as methods for it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:806,safety,compl,complexity,806,"This makes me think of a few things. ### Moving 10x reading functions to `anndata`. Initially the idea was all single cell stuff should go into scanpy. Since we read loom into anndata, I think we'd be okay with putting the 10x readers into anndata, especially if there is broad consensus this would make development of other packages easier. However, they would need to be re-written to use h5py instead of pytables. ### `scanpy` as a requirement. Is the requirement of scanpy so bad? If there are pain points here, should we be trying to make a basic scanpy installation lighter weight? ### Splitting off new modules. Finally, the idea of having IO functions go into their own package. I think this is a much bigger change, and I'd like to see a more fleshed out case for it. This would add a fair bit of complexity to development, so I'd want to be sure it's worth it. Some general questions I have:. * What are the advantages/ disadvantages of having smaller sub-packages? * How does this impact users vs. developers? * Is IO special, or should more parts go into sub-packages? * What gets re-exported from ""main"" modules? * Who manages the sub-packages? A more specific question: how modular of a component is IO? In some cases, like reading a transcriptomic only datasets, I'd say very. For more recent developments, like visium, I'm not sure this is the case. What we read in, and how we represent it, is very tightly coupled to the methods we have. Until the data's been around for a bit longer, I think it would make sense to keep visium IO in the same package as methods for it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1117,safety,modul,modules,1117,"This makes me think of a few things. ### Moving 10x reading functions to `anndata`. Initially the idea was all single cell stuff should go into scanpy. Since we read loom into anndata, I think we'd be okay with putting the 10x readers into anndata, especially if there is broad consensus this would make development of other packages easier. However, they would need to be re-written to use h5py instead of pytables. ### `scanpy` as a requirement. Is the requirement of scanpy so bad? If there are pain points here, should we be trying to make a basic scanpy installation lighter weight? ### Splitting off new modules. Finally, the idea of having IO functions go into their own package. I think this is a much bigger change, and I'd like to see a more fleshed out case for it. This would add a fair bit of complexity to development, so I'd want to be sure it's worth it. Some general questions I have:. * What are the advantages/ disadvantages of having smaller sub-packages? * How does this impact users vs. developers? * Is IO special, or should more parts go into sub-packages? * What gets re-exported from ""main"" modules? * Who manages the sub-packages? A more specific question: how modular of a component is IO? In some cases, like reading a transcriptomic only datasets, I'd say very. For more recent developments, like visium, I'm not sure this is the case. What we read in, and how we represent it, is very tightly coupled to the methods we have. Until the data's been around for a bit longer, I think it would make sense to keep visium IO in the same package as methods for it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1132,safety,manag,manages,1132,"This makes me think of a few things. ### Moving 10x reading functions to `anndata`. Initially the idea was all single cell stuff should go into scanpy. Since we read loom into anndata, I think we'd be okay with putting the 10x readers into anndata, especially if there is broad consensus this would make development of other packages easier. However, they would need to be re-written to use h5py instead of pytables. ### `scanpy` as a requirement. Is the requirement of scanpy so bad? If there are pain points here, should we be trying to make a basic scanpy installation lighter weight? ### Splitting off new modules. Finally, the idea of having IO functions go into their own package. I think this is a much bigger change, and I'd like to see a more fleshed out case for it. This would add a fair bit of complexity to development, so I'd want to be sure it's worth it. Some general questions I have:. * What are the advantages/ disadvantages of having smaller sub-packages? * How does this impact users vs. developers? * Is IO special, or should more parts go into sub-packages? * What gets re-exported from ""main"" modules? * Who manages the sub-packages? A more specific question: how modular of a component is IO? In some cases, like reading a transcriptomic only datasets, I'd say very. For more recent developments, like visium, I'm not sure this is the case. What we read in, and how we represent it, is very tightly coupled to the methods we have. Until the data's been around for a bit longer, I think it would make sense to keep visium IO in the same package as methods for it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1188,safety,modul,modular,1188,"This makes me think of a few things. ### Moving 10x reading functions to `anndata`. Initially the idea was all single cell stuff should go into scanpy. Since we read loom into anndata, I think we'd be okay with putting the 10x readers into anndata, especially if there is broad consensus this would make development of other packages easier. However, they would need to be re-written to use h5py instead of pytables. ### `scanpy` as a requirement. Is the requirement of scanpy so bad? If there are pain points here, should we be trying to make a basic scanpy installation lighter weight? ### Splitting off new modules. Finally, the idea of having IO functions go into their own package. I think this is a much bigger change, and I'd like to see a more fleshed out case for it. This would add a fair bit of complexity to development, so I'd want to be sure it's worth it. Some general questions I have:. * What are the advantages/ disadvantages of having smaller sub-packages? * How does this impact users vs. developers? * Is IO special, or should more parts go into sub-packages? * What gets re-exported from ""main"" modules? * Who manages the sub-packages? A more specific question: how modular of a component is IO? In some cases, like reading a transcriptomic only datasets, I'd say very. For more recent developments, like visium, I'm not sure this is the case. What we read in, and how we represent it, is very tightly coupled to the methods we have. Until the data's been around for a bit longer, I think it would make sense to keep visium IO in the same package as methods for it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:806,security,compl,complexity,806,"This makes me think of a few things. ### Moving 10x reading functions to `anndata`. Initially the idea was all single cell stuff should go into scanpy. Since we read loom into anndata, I think we'd be okay with putting the 10x readers into anndata, especially if there is broad consensus this would make development of other packages easier. However, they would need to be re-written to use h5py instead of pytables. ### `scanpy` as a requirement. Is the requirement of scanpy so bad? If there are pain points here, should we be trying to make a basic scanpy installation lighter weight? ### Splitting off new modules. Finally, the idea of having IO functions go into their own package. I think this is a much bigger change, and I'd like to see a more fleshed out case for it. This would add a fair bit of complexity to development, so I'd want to be sure it's worth it. Some general questions I have:. * What are the advantages/ disadvantages of having smaller sub-packages? * How does this impact users vs. developers? * Is IO special, or should more parts go into sub-packages? * What gets re-exported from ""main"" modules? * Who manages the sub-packages? A more specific question: how modular of a component is IO? In some cases, like reading a transcriptomic only datasets, I'd say very. For more recent developments, like visium, I'm not sure this is the case. What we read in, and how we represent it, is very tightly coupled to the methods we have. Until the data's been around for a bit longer, I think it would make sense to keep visium IO in the same package as methods for it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1188,testability,modula,modular,1188,"This makes me think of a few things. ### Moving 10x reading functions to `anndata`. Initially the idea was all single cell stuff should go into scanpy. Since we read loom into anndata, I think we'd be okay with putting the 10x readers into anndata, especially if there is broad consensus this would make development of other packages easier. However, they would need to be re-written to use h5py instead of pytables. ### `scanpy` as a requirement. Is the requirement of scanpy so bad? If there are pain points here, should we be trying to make a basic scanpy installation lighter weight? ### Splitting off new modules. Finally, the idea of having IO functions go into their own package. I think this is a much bigger change, and I'd like to see a more fleshed out case for it. This would add a fair bit of complexity to development, so I'd want to be sure it's worth it. Some general questions I have:. * What are the advantages/ disadvantages of having smaller sub-packages? * How does this impact users vs. developers? * Is IO special, or should more parts go into sub-packages? * What gets re-exported from ""main"" modules? * Who manages the sub-packages? A more specific question: how modular of a component is IO? In some cases, like reading a transcriptomic only datasets, I'd say very. For more recent developments, like visium, I'm not sure this is the case. What we read in, and how we represent it, is very tightly coupled to the methods we have. Until the data's been around for a bit longer, I think it would make sense to keep visium IO in the same package as methods for it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1424,testability,coupl,coupled,1424,"This makes me think of a few things. ### Moving 10x reading functions to `anndata`. Initially the idea was all single cell stuff should go into scanpy. Since we read loom into anndata, I think we'd be okay with putting the 10x readers into anndata, especially if there is broad consensus this would make development of other packages easier. However, they would need to be re-written to use h5py instead of pytables. ### `scanpy` as a requirement. Is the requirement of scanpy so bad? If there are pain points here, should we be trying to make a basic scanpy installation lighter weight? ### Splitting off new modules. Finally, the idea of having IO functions go into their own package. I think this is a much bigger change, and I'd like to see a more fleshed out case for it. This would add a fair bit of complexity to development, so I'd want to be sure it's worth it. Some general questions I have:. * What are the advantages/ disadvantages of having smaller sub-packages? * How does this impact users vs. developers? * Is IO special, or should more parts go into sub-packages? * What gets re-exported from ""main"" modules? * Who manages the sub-packages? A more specific question: how modular of a component is IO? In some cases, like reading a transcriptomic only datasets, I'd say very. For more recent developments, like visium, I'm not sure this is the case. What we read in, and how we represent it, is very tightly coupled to the methods we have. Until the data's been around for a bit longer, I think it would make sense to keep visium IO in the same package as methods for it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:999,usability,user,users,999,"This makes me think of a few things. ### Moving 10x reading functions to `anndata`. Initially the idea was all single cell stuff should go into scanpy. Since we read loom into anndata, I think we'd be okay with putting the 10x readers into anndata, especially if there is broad consensus this would make development of other packages easier. However, they would need to be re-written to use h5py instead of pytables. ### `scanpy` as a requirement. Is the requirement of scanpy so bad? If there are pain points here, should we be trying to make a basic scanpy installation lighter weight? ### Splitting off new modules. Finally, the idea of having IO functions go into their own package. I think this is a much bigger change, and I'd like to see a more fleshed out case for it. This would add a fair bit of complexity to development, so I'd want to be sure it's worth it. Some general questions I have:. * What are the advantages/ disadvantages of having smaller sub-packages? * How does this impact users vs. developers? * Is IO special, or should more parts go into sub-packages? * What gets re-exported from ""main"" modules? * Who manages the sub-packages? A more specific question: how modular of a component is IO? In some cases, like reading a transcriptomic only datasets, I'd say very. For more recent developments, like visium, I'm not sure this is the case. What we read in, and how we represent it, is very tightly coupled to the methods we have. Until the data's been around for a bit longer, I think it would make sense to keep visium IO in the same package as methods for it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:214,deployability,log,logically,214,"> Moving 10x reading functions to anndata. I haven't worked much with h5py or tables, is it time-consuming to refactor these functions? It seems like moving to anndata is the most straightforward solution at least logically to me. > scanpy as a requirement. I like scanpy, but the only thing we really *require* in scvi is the data loading part. A user could take their scvi outputs and go use Seurat if that makes them happy. And then like the data loading functions are simple enough that we could just implement them ourselves. I'm sure a lot of people are currently doing this, which inspired the idea to have a standalone package. > Splitting off new modules. Your questions are very valid. I don't really have good answers for them. I could just see a standalone package being widely used and community driven, especially if there is some scanpy backing + maybe optional dependencies/functionality to get your objects ready for R analysis pipelines.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:656,deployability,modul,modules,656,"> Moving 10x reading functions to anndata. I haven't worked much with h5py or tables, is it time-consuming to refactor these functions? It seems like moving to anndata is the most straightforward solution at least logically to me. > scanpy as a requirement. I like scanpy, but the only thing we really *require* in scvi is the data loading part. A user could take their scvi outputs and go use Seurat if that makes them happy. And then like the data loading functions are simple enough that we could just implement them ourselves. I'm sure a lot of people are currently doing this, which inspired the idea to have a standalone package. > Splitting off new modules. Your questions are very valid. I don't really have good answers for them. I could just see a standalone package being widely used and community driven, especially if there is some scanpy backing + maybe optional dependencies/functionality to get your objects ready for R analysis pipelines.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:877,deployability,depend,dependencies,877,"> Moving 10x reading functions to anndata. I haven't worked much with h5py or tables, is it time-consuming to refactor these functions? It seems like moving to anndata is the most straightforward solution at least logically to me. > scanpy as a requirement. I like scanpy, but the only thing we really *require* in scvi is the data loading part. A user could take their scvi outputs and go use Seurat if that makes them happy. And then like the data loading functions are simple enough that we could just implement them ourselves. I'm sure a lot of people are currently doing this, which inspired the idea to have a standalone package. > Splitting off new modules. Your questions are very valid. I don't really have good answers for them. I could just see a standalone package being widely used and community driven, especially if there is some scanpy backing + maybe optional dependencies/functionality to get your objects ready for R analysis pipelines.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:945,deployability,pipelin,pipelines,945,"> Moving 10x reading functions to anndata. I haven't worked much with h5py or tables, is it time-consuming to refactor these functions? It seems like moving to anndata is the most straightforward solution at least logically to me. > scanpy as a requirement. I like scanpy, but the only thing we really *require* in scvi is the data loading part. A user could take their scvi outputs and go use Seurat if that makes them happy. And then like the data loading functions are simple enough that we could just implement them ourselves. I'm sure a lot of people are currently doing this, which inspired the idea to have a standalone package. > Splitting off new modules. Your questions are very valid. I don't really have good answers for them. I could just see a standalone package being widely used and community driven, especially if there is some scanpy backing + maybe optional dependencies/functionality to get your objects ready for R analysis pipelines.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:332,energy efficiency,load,loading,332,"> Moving 10x reading functions to anndata. I haven't worked much with h5py or tables, is it time-consuming to refactor these functions? It seems like moving to anndata is the most straightforward solution at least logically to me. > scanpy as a requirement. I like scanpy, but the only thing we really *require* in scvi is the data loading part. A user could take their scvi outputs and go use Seurat if that makes them happy. And then like the data loading functions are simple enough that we could just implement them ourselves. I'm sure a lot of people are currently doing this, which inspired the idea to have a standalone package. > Splitting off new modules. Your questions are very valid. I don't really have good answers for them. I could just see a standalone package being widely used and community driven, especially if there is some scanpy backing + maybe optional dependencies/functionality to get your objects ready for R analysis pipelines.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:450,energy efficiency,load,loading,450,"> Moving 10x reading functions to anndata. I haven't worked much with h5py or tables, is it time-consuming to refactor these functions? It seems like moving to anndata is the most straightforward solution at least logically to me. > scanpy as a requirement. I like scanpy, but the only thing we really *require* in scvi is the data loading part. A user could take their scvi outputs and go use Seurat if that makes them happy. And then like the data loading functions are simple enough that we could just implement them ourselves. I'm sure a lot of people are currently doing this, which inspired the idea to have a standalone package. > Splitting off new modules. Your questions are very valid. I don't really have good answers for them. I could just see a standalone package being widely used and community driven, especially if there is some scanpy backing + maybe optional dependencies/functionality to get your objects ready for R analysis pipelines.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:560,energy efficiency,current,currently,560,"> Moving 10x reading functions to anndata. I haven't worked much with h5py or tables, is it time-consuming to refactor these functions? It seems like moving to anndata is the most straightforward solution at least logically to me. > scanpy as a requirement. I like scanpy, but the only thing we really *require* in scvi is the data loading part. A user could take their scvi outputs and go use Seurat if that makes them happy. And then like the data loading functions are simple enough that we could just implement them ourselves. I'm sure a lot of people are currently doing this, which inspired the idea to have a standalone package. > Splitting off new modules. Your questions are very valid. I don't really have good answers for them. I could just see a standalone package being widely used and community driven, especially if there is some scanpy backing + maybe optional dependencies/functionality to get your objects ready for R analysis pipelines.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:877,integrability,depend,dependencies,877,"> Moving 10x reading functions to anndata. I haven't worked much with h5py or tables, is it time-consuming to refactor these functions? It seems like moving to anndata is the most straightforward solution at least logically to me. > scanpy as a requirement. I like scanpy, but the only thing we really *require* in scvi is the data loading part. A user could take their scvi outputs and go use Seurat if that makes them happy. And then like the data loading functions are simple enough that we could just implement them ourselves. I'm sure a lot of people are currently doing this, which inspired the idea to have a standalone package. > Splitting off new modules. Your questions are very valid. I don't really have good answers for them. I could just see a standalone package being widely used and community driven, especially if there is some scanpy backing + maybe optional dependencies/functionality to get your objects ready for R analysis pipelines.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:945,integrability,pipelin,pipelines,945,"> Moving 10x reading functions to anndata. I haven't worked much with h5py or tables, is it time-consuming to refactor these functions? It seems like moving to anndata is the most straightforward solution at least logically to me. > scanpy as a requirement. I like scanpy, but the only thing we really *require* in scvi is the data loading part. A user could take their scvi outputs and go use Seurat if that makes them happy. And then like the data loading functions are simple enough that we could just implement them ourselves. I'm sure a lot of people are currently doing this, which inspired the idea to have a standalone package. > Splitting off new modules. Your questions are very valid. I don't really have good answers for them. I could just see a standalone package being widely used and community driven, especially if there is some scanpy backing + maybe optional dependencies/functionality to get your objects ready for R analysis pipelines.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:110,modifiability,refact,refactor,110,"> Moving 10x reading functions to anndata. I haven't worked much with h5py or tables, is it time-consuming to refactor these functions? It seems like moving to anndata is the most straightforward solution at least logically to me. > scanpy as a requirement. I like scanpy, but the only thing we really *require* in scvi is the data loading part. A user could take their scvi outputs and go use Seurat if that makes them happy. And then like the data loading functions are simple enough that we could just implement them ourselves. I'm sure a lot of people are currently doing this, which inspired the idea to have a standalone package. > Splitting off new modules. Your questions are very valid. I don't really have good answers for them. I could just see a standalone package being widely used and community driven, especially if there is some scanpy backing + maybe optional dependencies/functionality to get your objects ready for R analysis pipelines.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:627,modifiability,pac,package,627,"> Moving 10x reading functions to anndata. I haven't worked much with h5py or tables, is it time-consuming to refactor these functions? It seems like moving to anndata is the most straightforward solution at least logically to me. > scanpy as a requirement. I like scanpy, but the only thing we really *require* in scvi is the data loading part. A user could take their scvi outputs and go use Seurat if that makes them happy. And then like the data loading functions are simple enough that we could just implement them ourselves. I'm sure a lot of people are currently doing this, which inspired the idea to have a standalone package. > Splitting off new modules. Your questions are very valid. I don't really have good answers for them. I could just see a standalone package being widely used and community driven, especially if there is some scanpy backing + maybe optional dependencies/functionality to get your objects ready for R analysis pipelines.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:656,modifiability,modul,modules,656,"> Moving 10x reading functions to anndata. I haven't worked much with h5py or tables, is it time-consuming to refactor these functions? It seems like moving to anndata is the most straightforward solution at least logically to me. > scanpy as a requirement. I like scanpy, but the only thing we really *require* in scvi is the data loading part. A user could take their scvi outputs and go use Seurat if that makes them happy. And then like the data loading functions are simple enough that we could just implement them ourselves. I'm sure a lot of people are currently doing this, which inspired the idea to have a standalone package. > Splitting off new modules. Your questions are very valid. I don't really have good answers for them. I could just see a standalone package being widely used and community driven, especially if there is some scanpy backing + maybe optional dependencies/functionality to get your objects ready for R analysis pipelines.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:769,modifiability,pac,package,769,"> Moving 10x reading functions to anndata. I haven't worked much with h5py or tables, is it time-consuming to refactor these functions? It seems like moving to anndata is the most straightforward solution at least logically to me. > scanpy as a requirement. I like scanpy, but the only thing we really *require* in scvi is the data loading part. A user could take their scvi outputs and go use Seurat if that makes them happy. And then like the data loading functions are simple enough that we could just implement them ourselves. I'm sure a lot of people are currently doing this, which inspired the idea to have a standalone package. > Splitting off new modules. Your questions are very valid. I don't really have good answers for them. I could just see a standalone package being widely used and community driven, especially if there is some scanpy backing + maybe optional dependencies/functionality to get your objects ready for R analysis pipelines.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:877,modifiability,depend,dependencies,877,"> Moving 10x reading functions to anndata. I haven't worked much with h5py or tables, is it time-consuming to refactor these functions? It seems like moving to anndata is the most straightforward solution at least logically to me. > scanpy as a requirement. I like scanpy, but the only thing we really *require* in scvi is the data loading part. A user could take their scvi outputs and go use Seurat if that makes them happy. And then like the data loading functions are simple enough that we could just implement them ourselves. I'm sure a lot of people are currently doing this, which inspired the idea to have a standalone package. > Splitting off new modules. Your questions are very valid. I don't really have good answers for them. I could just see a standalone package being widely used and community driven, especially if there is some scanpy backing + maybe optional dependencies/functionality to get your objects ready for R analysis pipelines.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:92,performance,time,time-consuming,92,"> Moving 10x reading functions to anndata. I haven't worked much with h5py or tables, is it time-consuming to refactor these functions? It seems like moving to anndata is the most straightforward solution at least logically to me. > scanpy as a requirement. I like scanpy, but the only thing we really *require* in scvi is the data loading part. A user could take their scvi outputs and go use Seurat if that makes them happy. And then like the data loading functions are simple enough that we could just implement them ourselves. I'm sure a lot of people are currently doing this, which inspired the idea to have a standalone package. > Splitting off new modules. Your questions are very valid. I don't really have good answers for them. I could just see a standalone package being widely used and community driven, especially if there is some scanpy backing + maybe optional dependencies/functionality to get your objects ready for R analysis pipelines.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:110,performance,refactor,refactor,110,"> Moving 10x reading functions to anndata. I haven't worked much with h5py or tables, is it time-consuming to refactor these functions? It seems like moving to anndata is the most straightforward solution at least logically to me. > scanpy as a requirement. I like scanpy, but the only thing we really *require* in scvi is the data loading part. A user could take their scvi outputs and go use Seurat if that makes them happy. And then like the data loading functions are simple enough that we could just implement them ourselves. I'm sure a lot of people are currently doing this, which inspired the idea to have a standalone package. > Splitting off new modules. Your questions are very valid. I don't really have good answers for them. I could just see a standalone package being widely used and community driven, especially if there is some scanpy backing + maybe optional dependencies/functionality to get your objects ready for R analysis pipelines.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:332,performance,load,loading,332,"> Moving 10x reading functions to anndata. I haven't worked much with h5py or tables, is it time-consuming to refactor these functions? It seems like moving to anndata is the most straightforward solution at least logically to me. > scanpy as a requirement. I like scanpy, but the only thing we really *require* in scvi is the data loading part. A user could take their scvi outputs and go use Seurat if that makes them happy. And then like the data loading functions are simple enough that we could just implement them ourselves. I'm sure a lot of people are currently doing this, which inspired the idea to have a standalone package. > Splitting off new modules. Your questions are very valid. I don't really have good answers for them. I could just see a standalone package being widely used and community driven, especially if there is some scanpy backing + maybe optional dependencies/functionality to get your objects ready for R analysis pipelines.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:450,performance,load,loading,450,"> Moving 10x reading functions to anndata. I haven't worked much with h5py or tables, is it time-consuming to refactor these functions? It seems like moving to anndata is the most straightforward solution at least logically to me. > scanpy as a requirement. I like scanpy, but the only thing we really *require* in scvi is the data loading part. A user could take their scvi outputs and go use Seurat if that makes them happy. And then like the data loading functions are simple enough that we could just implement them ourselves. I'm sure a lot of people are currently doing this, which inspired the idea to have a standalone package. > Splitting off new modules. Your questions are very valid. I don't really have good answers for them. I could just see a standalone package being widely used and community driven, especially if there is some scanpy backing + maybe optional dependencies/functionality to get your objects ready for R analysis pipelines.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:214,safety,log,logically,214,"> Moving 10x reading functions to anndata. I haven't worked much with h5py or tables, is it time-consuming to refactor these functions? It seems like moving to anndata is the most straightforward solution at least logically to me. > scanpy as a requirement. I like scanpy, but the only thing we really *require* in scvi is the data loading part. A user could take their scvi outputs and go use Seurat if that makes them happy. And then like the data loading functions are simple enough that we could just implement them ourselves. I'm sure a lot of people are currently doing this, which inspired the idea to have a standalone package. > Splitting off new modules. Your questions are very valid. I don't really have good answers for them. I could just see a standalone package being widely used and community driven, especially if there is some scanpy backing + maybe optional dependencies/functionality to get your objects ready for R analysis pipelines.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:656,safety,modul,modules,656,"> Moving 10x reading functions to anndata. I haven't worked much with h5py or tables, is it time-consuming to refactor these functions? It seems like moving to anndata is the most straightforward solution at least logically to me. > scanpy as a requirement. I like scanpy, but the only thing we really *require* in scvi is the data loading part. A user could take their scvi outputs and go use Seurat if that makes them happy. And then like the data loading functions are simple enough that we could just implement them ourselves. I'm sure a lot of people are currently doing this, which inspired the idea to have a standalone package. > Splitting off new modules. Your questions are very valid. I don't really have good answers for them. I could just see a standalone package being widely used and community driven, especially if there is some scanpy backing + maybe optional dependencies/functionality to get your objects ready for R analysis pipelines.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:689,safety,valid,valid,689,"> Moving 10x reading functions to anndata. I haven't worked much with h5py or tables, is it time-consuming to refactor these functions? It seems like moving to anndata is the most straightforward solution at least logically to me. > scanpy as a requirement. I like scanpy, but the only thing we really *require* in scvi is the data loading part. A user could take their scvi outputs and go use Seurat if that makes them happy. And then like the data loading functions are simple enough that we could just implement them ourselves. I'm sure a lot of people are currently doing this, which inspired the idea to have a standalone package. > Splitting off new modules. Your questions are very valid. I don't really have good answers for them. I could just see a standalone package being widely used and community driven, especially if there is some scanpy backing + maybe optional dependencies/functionality to get your objects ready for R analysis pipelines.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:877,safety,depend,dependencies,877,"> Moving 10x reading functions to anndata. I haven't worked much with h5py or tables, is it time-consuming to refactor these functions? It seems like moving to anndata is the most straightforward solution at least logically to me. > scanpy as a requirement. I like scanpy, but the only thing we really *require* in scvi is the data loading part. A user could take their scvi outputs and go use Seurat if that makes them happy. And then like the data loading functions are simple enough that we could just implement them ourselves. I'm sure a lot of people are currently doing this, which inspired the idea to have a standalone package. > Splitting off new modules. Your questions are very valid. I don't really have good answers for them. I could just see a standalone package being widely used and community driven, especially if there is some scanpy backing + maybe optional dependencies/functionality to get your objects ready for R analysis pipelines.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:214,security,log,logically,214,"> Moving 10x reading functions to anndata. I haven't worked much with h5py or tables, is it time-consuming to refactor these functions? It seems like moving to anndata is the most straightforward solution at least logically to me. > scanpy as a requirement. I like scanpy, but the only thing we really *require* in scvi is the data loading part. A user could take their scvi outputs and go use Seurat if that makes them happy. And then like the data loading functions are simple enough that we could just implement them ourselves. I'm sure a lot of people are currently doing this, which inspired the idea to have a standalone package. > Splitting off new modules. Your questions are very valid. I don't really have good answers for them. I could just see a standalone package being widely used and community driven, especially if there is some scanpy backing + maybe optional dependencies/functionality to get your objects ready for R analysis pipelines.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:214,testability,log,logically,214,"> Moving 10x reading functions to anndata. I haven't worked much with h5py or tables, is it time-consuming to refactor these functions? It seems like moving to anndata is the most straightforward solution at least logically to me. > scanpy as a requirement. I like scanpy, but the only thing we really *require* in scvi is the data loading part. A user could take their scvi outputs and go use Seurat if that makes them happy. And then like the data loading functions are simple enough that we could just implement them ourselves. I'm sure a lot of people are currently doing this, which inspired the idea to have a standalone package. > Splitting off new modules. Your questions are very valid. I don't really have good answers for them. I could just see a standalone package being widely used and community driven, especially if there is some scanpy backing + maybe optional dependencies/functionality to get your objects ready for R analysis pipelines.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:472,testability,simpl,simple,472,"> Moving 10x reading functions to anndata. I haven't worked much with h5py or tables, is it time-consuming to refactor these functions? It seems like moving to anndata is the most straightforward solution at least logically to me. > scanpy as a requirement. I like scanpy, but the only thing we really *require* in scvi is the data loading part. A user could take their scvi outputs and go use Seurat if that makes them happy. And then like the data loading functions are simple enough that we could just implement them ourselves. I'm sure a lot of people are currently doing this, which inspired the idea to have a standalone package. > Splitting off new modules. Your questions are very valid. I don't really have good answers for them. I could just see a standalone package being widely used and community driven, especially if there is some scanpy backing + maybe optional dependencies/functionality to get your objects ready for R analysis pipelines.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:877,testability,depend,dependencies,877,"> Moving 10x reading functions to anndata. I haven't worked much with h5py or tables, is it time-consuming to refactor these functions? It seems like moving to anndata is the most straightforward solution at least logically to me. > scanpy as a requirement. I like scanpy, but the only thing we really *require* in scvi is the data loading part. A user could take their scvi outputs and go use Seurat if that makes them happy. And then like the data loading functions are simple enough that we could just implement them ourselves. I'm sure a lot of people are currently doing this, which inspired the idea to have a standalone package. > Splitting off new modules. Your questions are very valid. I don't really have good answers for them. I could just see a standalone package being widely used and community driven, especially if there is some scanpy backing + maybe optional dependencies/functionality to get your objects ready for R analysis pipelines.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:348,usability,user,user,348,"> Moving 10x reading functions to anndata. I haven't worked much with h5py or tables, is it time-consuming to refactor these functions? It seems like moving to anndata is the most straightforward solution at least logically to me. > scanpy as a requirement. I like scanpy, but the only thing we really *require* in scvi is the data loading part. A user could take their scvi outputs and go use Seurat if that makes them happy. And then like the data loading functions are simple enough that we could just implement them ourselves. I'm sure a lot of people are currently doing this, which inspired the idea to have a standalone package. > Splitting off new modules. Your questions are very valid. I don't really have good answers for them. I could just see a standalone package being widely used and community driven, especially if there is some scanpy backing + maybe optional dependencies/functionality to get your objects ready for R analysis pipelines.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:472,usability,simpl,simple,472,"> Moving 10x reading functions to anndata. I haven't worked much with h5py or tables, is it time-consuming to refactor these functions? It seems like moving to anndata is the most straightforward solution at least logically to me. > scanpy as a requirement. I like scanpy, but the only thing we really *require* in scvi is the data loading part. A user could take their scvi outputs and go use Seurat if that makes them happy. And then like the data loading functions are simple enough that we could just implement them ourselves. I'm sure a lot of people are currently doing this, which inspired the idea to have a standalone package. > Splitting off new modules. Your questions are very valid. I don't really have good answers for them. I could just see a standalone package being widely used and community driven, especially if there is some scanpy backing + maybe optional dependencies/functionality to get your objects ready for R analysis pipelines.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:173,deployability,log,logically,173,"> I haven't worked much with h5py or tables, is it time-consuming to refactor these functions? It seems like moving to anndata is the most straightforward solution at least logically to me. In this case, I think it should be fine. It might not happen too soon if we're left to our own devices, so a PR is welcome. > I could just see a standalone package being widely used and community driven. What formats that aren't in `anndata` would you see in this package? I'm trying to get an idea of the kind of scope you're thinking of here. I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:399,interoperability,format,formats,399,"> I haven't worked much with h5py or tables, is it time-consuming to refactor these functions? It seems like moving to anndata is the most straightforward solution at least logically to me. In this case, I think it should be fine. It might not happen too soon if we're left to our own devices, so a PR is welcome. > I could just see a standalone package being widely used and community driven. What formats that aren't in `anndata` would you see in this package? I'm trying to get an idea of the kind of scope you're thinking of here. I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:553,interoperability,format,formats,553,"> I haven't worked much with h5py or tables, is it time-consuming to refactor these functions? It seems like moving to anndata is the most straightforward solution at least logically to me. In this case, I think it should be fine. It might not happen too soon if we're left to our own devices, so a PR is welcome. > I could just see a standalone package being widely used and community driven. What formats that aren't in `anndata` would you see in this package? I'm trying to get an idea of the kind of scope you're thinking of here. I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:69,modifiability,refact,refactor,69,"> I haven't worked much with h5py or tables, is it time-consuming to refactor these functions? It seems like moving to anndata is the most straightforward solution at least logically to me. In this case, I think it should be fine. It might not happen too soon if we're left to our own devices, so a PR is welcome. > I could just see a standalone package being widely used and community driven. What formats that aren't in `anndata` would you see in this package? I'm trying to get an idea of the kind of scope you're thinking of here. I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:346,modifiability,pac,package,346,"> I haven't worked much with h5py or tables, is it time-consuming to refactor these functions? It seems like moving to anndata is the most straightforward solution at least logically to me. In this case, I think it should be fine. It might not happen too soon if we're left to our own devices, so a PR is welcome. > I could just see a standalone package being widely used and community driven. What formats that aren't in `anndata` would you see in this package? I'm trying to get an idea of the kind of scope you're thinking of here. I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:454,modifiability,pac,package,454,"> I haven't worked much with h5py or tables, is it time-consuming to refactor these functions? It seems like moving to anndata is the most straightforward solution at least logically to me. In this case, I think it should be fine. It might not happen too soon if we're left to our own devices, so a PR is welcome. > I could just see a standalone package being widely used and community driven. What formats that aren't in `anndata` would you see in this package? I'm trying to get an idea of the kind of scope you're thinking of here. I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:51,performance,time,time-consuming,51,"> I haven't worked much with h5py or tables, is it time-consuming to refactor these functions? It seems like moving to anndata is the most straightforward solution at least logically to me. In this case, I think it should be fine. It might not happen too soon if we're left to our own devices, so a PR is welcome. > I could just see a standalone package being widely used and community driven. What formats that aren't in `anndata` would you see in this package? I'm trying to get an idea of the kind of scope you're thinking of here. I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:69,performance,refactor,refactor,69,"> I haven't worked much with h5py or tables, is it time-consuming to refactor these functions? It seems like moving to anndata is the most straightforward solution at least logically to me. In this case, I think it should be fine. It might not happen too soon if we're left to our own devices, so a PR is welcome. > I could just see a standalone package being widely used and community driven. What formats that aren't in `anndata` would you see in this package? I'm trying to get an idea of the kind of scope you're thinking of here. I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:173,safety,log,logically,173,"> I haven't worked much with h5py or tables, is it time-consuming to refactor these functions? It seems like moving to anndata is the most straightforward solution at least logically to me. In this case, I think it should be fine. It might not happen too soon if we're left to our own devices, so a PR is welcome. > I could just see a standalone package being widely used and community driven. What formats that aren't in `anndata` would you see in this package? I'm trying to get an idea of the kind of scope you're thinking of here. I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:173,security,log,logically,173,"> I haven't worked much with h5py or tables, is it time-consuming to refactor these functions? It seems like moving to anndata is the most straightforward solution at least logically to me. In this case, I think it should be fine. It might not happen too soon if we're left to our own devices, so a PR is welcome. > I could just see a standalone package being widely used and community driven. What formats that aren't in `anndata` would you see in this package? I'm trying to get an idea of the kind of scope you're thinking of here. I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:173,testability,log,logically,173,"> I haven't worked much with h5py or tables, is it time-consuming to refactor these functions? It seems like moving to anndata is the most straightforward solution at least logically to me. In this case, I think it should be fine. It might not happen too soon if we're left to our own devices, so a PR is welcome. > I could just see a standalone package being widely used and community driven. What formats that aren't in `anndata` would you see in this package? I'm trying to get an idea of the kind of scope you're thinking of here. I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:53,performance,time,time,53,"pytables is starting to throw warnings, so it may be time for a rewrite and moving the function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:279,energy efficiency,load,loading,279,I would love to see file I/O in Anndata. I imagine this would make things easier for episcanpy as well. That package can then focus more on setting up count tables where they are not nicely provided. Otherwise it becomes a bit difficult for the new user (me) to distinguish data loading and setting up new tables.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:109,modifiability,pac,package,109,I would love to see file I/O in Anndata. I imagine this would make things easier for episcanpy as well. That package can then focus more on setting up count tables where they are not nicely provided. Otherwise it becomes a bit difficult for the new user (me) to distinguish data loading and setting up new tables.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:25,performance,I/O,I/O,25,I would love to see file I/O in Anndata. I imagine this would make things easier for episcanpy as well. That package can then focus more on setting up count tables where they are not nicely provided. Otherwise it becomes a bit difficult for the new user (me) to distinguish data loading and setting up new tables.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:279,performance,load,loading,279,I would love to see file I/O in Anndata. I imagine this would make things easier for episcanpy as well. That package can then focus more on setting up count tables where they are not nicely provided. Otherwise it becomes a bit difficult for the new user (me) to distinguish data loading and setting up new tables.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:249,usability,user,user,249,I would love to see file I/O in Anndata. I imagine this would make things easier for episcanpy as well. That package can then focus more on setting up count tables where they are not nicely provided. Otherwise it becomes a bit difficult for the new user (me) to distinguish data loading and setting up new tables.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:20,deployability,modul,modules,20,"> Splitting off new modules. Finally, the idea of having IO functions go into their own package. I think this is a much bigger change, and I'd like to see a more fleshed out case for it. This would add a fair bit of complexity to development, so I'd want to be sure it's worth it. Some general questions I have:. > What are the advantages/ disadvantages of having smaller sub-packages? method developer would just depend on those instead of (multiple) analysis package. > How does this impact users vs. developers? user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. > Is IO special, or should more parts go into sub-packages? it kind of is imho, it's all about having whatever data there is in an anndata/mudata shape. I must say that I'd also think plotting could be it's own separate package but it would probably require a lot of refactoring across packages (thinking about duplication of scanpy/scvelo code). > What gets re-exported from ""main"" modules? didn't get this sorry. > Who manages the sub-packages? the IO subpackage? everyone  . so beside being in favour, it might also be that other issues arise. For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:414,deployability,depend,depend,414,"> Splitting off new modules. Finally, the idea of having IO functions go into their own package. I think this is a much bigger change, and I'd like to see a more fleshed out case for it. This would add a fair bit of complexity to development, so I'd want to be sure it's worth it. Some general questions I have:. > What are the advantages/ disadvantages of having smaller sub-packages? method developer would just depend on those instead of (multiple) analysis package. > How does this impact users vs. developers? user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. > Is IO special, or should more parts go into sub-packages? it kind of is imho, it's all about having whatever data there is in an anndata/mudata shape. I must say that I'd also think plotting could be it's own separate package but it would probably require a lot of refactoring across packages (thinking about duplication of scanpy/scvelo code). > What gets re-exported from ""main"" modules? didn't get this sorry. > Who manages the sub-packages? the IO subpackage? everyone  . so beside being in favour, it might also be that other issues arise. For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1013,deployability,modul,modules,1013,"> Splitting off new modules. Finally, the idea of having IO functions go into their own package. I think this is a much bigger change, and I'd like to see a more fleshed out case for it. This would add a fair bit of complexity to development, so I'd want to be sure it's worth it. Some general questions I have:. > What are the advantages/ disadvantages of having smaller sub-packages? method developer would just depend on those instead of (multiple) analysis package. > How does this impact users vs. developers? user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. > Is IO special, or should more parts go into sub-packages? it kind of is imho, it's all about having whatever data there is in an anndata/mudata shape. I must say that I'd also think plotting could be it's own separate package but it would probably require a lot of refactoring across packages (thinking about duplication of scanpy/scvelo code). > What gets re-exported from ""main"" modules? didn't get this sorry. > Who manages the sub-packages? the IO subpackage? everyone  . so beside being in favour, it might also be that other issues arise. For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1051,deployability,manag,manages,1051,"> Splitting off new modules. Finally, the idea of having IO functions go into their own package. I think this is a much bigger change, and I'd like to see a more fleshed out case for it. This would add a fair bit of complexity to development, so I'd want to be sure it's worth it. Some general questions I have:. > What are the advantages/ disadvantages of having smaller sub-packages? method developer would just depend on those instead of (multiple) analysis package. > How does this impact users vs. developers? user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. > Is IO special, or should more parts go into sub-packages? it kind of is imho, it's all about having whatever data there is in an anndata/mudata shape. I must say that I'd also think plotting could be it's own separate package but it would probably require a lot of refactoring across packages (thinking about duplication of scanpy/scvelo code). > What gets re-exported from ""main"" modules? didn't get this sorry. > Who manages the sub-packages? the IO subpackage? everyone  . so beside being in favour, it might also be that other issues arise. For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1051,energy efficiency,manag,manages,1051,"> Splitting off new modules. Finally, the idea of having IO functions go into their own package. I think this is a much bigger change, and I'd like to see a more fleshed out case for it. This would add a fair bit of complexity to development, so I'd want to be sure it's worth it. Some general questions I have:. > What are the advantages/ disadvantages of having smaller sub-packages? method developer would just depend on those instead of (multiple) analysis package. > How does this impact users vs. developers? user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. > Is IO special, or should more parts go into sub-packages? it kind of is imho, it's all about having whatever data there is in an anndata/mudata shape. I must say that I'd also think plotting could be it's own separate package but it would probably require a lot of refactoring across packages (thinking about duplication of scanpy/scvelo code). > What gets re-exported from ""main"" modules? didn't get this sorry. > Who manages the sub-packages? the IO subpackage? everyone  . so beside being in favour, it might also be that other issues arise. For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:372,integrability,sub,sub-packages,372,"> Splitting off new modules. Finally, the idea of having IO functions go into their own package. I think this is a much bigger change, and I'd like to see a more fleshed out case for it. This would add a fair bit of complexity to development, so I'd want to be sure it's worth it. Some general questions I have:. > What are the advantages/ disadvantages of having smaller sub-packages? method developer would just depend on those instead of (multiple) analysis package. > How does this impact users vs. developers? user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. > Is IO special, or should more parts go into sub-packages? it kind of is imho, it's all about having whatever data there is in an anndata/mudata shape. I must say that I'd also think plotting could be it's own separate package but it would probably require a lot of refactoring across packages (thinking about duplication of scanpy/scvelo code). > What gets re-exported from ""main"" modules? didn't get this sorry. > Who manages the sub-packages? the IO subpackage? everyone  . so beside being in favour, it might also be that other issues arise. For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:414,integrability,depend,depend,414,"> Splitting off new modules. Finally, the idea of having IO functions go into their own package. I think this is a much bigger change, and I'd like to see a more fleshed out case for it. This would add a fair bit of complexity to development, so I'd want to be sure it's worth it. Some general questions I have:. > What are the advantages/ disadvantages of having smaller sub-packages? method developer would just depend on those instead of (multiple) analysis package. > How does this impact users vs. developers? user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. > Is IO special, or should more parts go into sub-packages? it kind of is imho, it's all about having whatever data there is in an anndata/mudata shape. I must say that I'd also think plotting could be it's own separate package but it would probably require a lot of refactoring across packages (thinking about duplication of scanpy/scvelo code). > What gets re-exported from ""main"" modules? didn't get this sorry. > Who manages the sub-packages? the IO subpackage? everyone  . so beside being in favour, it might also be that other issues arise. For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:676,integrability,sub,sub-packages,676,"> Splitting off new modules. Finally, the idea of having IO functions go into their own package. I think this is a much bigger change, and I'd like to see a more fleshed out case for it. This would add a fair bit of complexity to development, so I'd want to be sure it's worth it. Some general questions I have:. > What are the advantages/ disadvantages of having smaller sub-packages? method developer would just depend on those instead of (multiple) analysis package. > How does this impact users vs. developers? user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. > Is IO special, or should more parts go into sub-packages? it kind of is imho, it's all about having whatever data there is in an anndata/mudata shape. I must say that I'd also think plotting could be it's own separate package but it would probably require a lot of refactoring across packages (thinking about duplication of scanpy/scvelo code). > What gets re-exported from ""main"" modules? didn't get this sorry. > Who manages the sub-packages? the IO subpackage? everyone  . so beside being in favour, it might also be that other issues arise. For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1063,integrability,sub,sub-packages,1063,"> Splitting off new modules. Finally, the idea of having IO functions go into their own package. I think this is a much bigger change, and I'd like to see a more fleshed out case for it. This would add a fair bit of complexity to development, so I'd want to be sure it's worth it. Some general questions I have:. > What are the advantages/ disadvantages of having smaller sub-packages? method developer would just depend on those instead of (multiple) analysis package. > How does this impact users vs. developers? user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. > Is IO special, or should more parts go into sub-packages? it kind of is imho, it's all about having whatever data there is in an anndata/mudata shape. I must say that I'd also think plotting could be it's own separate package but it would probably require a lot of refactoring across packages (thinking about duplication of scanpy/scvelo code). > What gets re-exported from ""main"" modules? didn't get this sorry. > Who manages the sub-packages? the IO subpackage? everyone  . so beside being in favour, it might also be that other issues arise. For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1084,integrability,sub,subpackage,1084,"> Splitting off new modules. Finally, the idea of having IO functions go into their own package. I think this is a much bigger change, and I'd like to see a more fleshed out case for it. This would add a fair bit of complexity to development, so I'd want to be sure it's worth it. Some general questions I have:. > What are the advantages/ disadvantages of having smaller sub-packages? method developer would just depend on those instead of (multiple) analysis package. > How does this impact users vs. developers? user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. > Is IO special, or should more parts go into sub-packages? it kind of is imho, it's all about having whatever data there is in an anndata/mudata shape. I must say that I'd also think plotting could be it's own separate package but it would probably require a lot of refactoring across packages (thinking about duplication of scanpy/scvelo code). > What gets re-exported from ""main"" modules? didn't get this sorry. > Who manages the sub-packages? the IO subpackage? everyone  . so beside being in favour, it might also be that other issues arise. For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1205,interoperability,specif,specific,1205,"> Splitting off new modules. Finally, the idea of having IO functions go into their own package. I think this is a much bigger change, and I'd like to see a more fleshed out case for it. This would add a fair bit of complexity to development, so I'd want to be sure it's worth it. Some general questions I have:. > What are the advantages/ disadvantages of having smaller sub-packages? method developer would just depend on those instead of (multiple) analysis package. > How does this impact users vs. developers? user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. > Is IO special, or should more parts go into sub-packages? it kind of is imho, it's all about having whatever data there is in an anndata/mudata shape. I must say that I'd also think plotting could be it's own separate package but it would probably require a lot of refactoring across packages (thinking about duplication of scanpy/scvelo code). > What gets re-exported from ""main"" modules? didn't get this sorry. > Who manages the sub-packages? the IO subpackage? everyone  . so beside being in favour, it might also be that other issues arise. For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1214,interoperability,format,formats,1214,"> Splitting off new modules. Finally, the idea of having IO functions go into their own package. I think this is a much bigger change, and I'd like to see a more fleshed out case for it. This would add a fair bit of complexity to development, so I'd want to be sure it's worth it. Some general questions I have:. > What are the advantages/ disadvantages of having smaller sub-packages? method developer would just depend on those instead of (multiple) analysis package. > How does this impact users vs. developers? user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. > Is IO special, or should more parts go into sub-packages? it kind of is imho, it's all about having whatever data there is in an anndata/mudata shape. I must say that I'd also think plotting could be it's own separate package but it would probably require a lot of refactoring across packages (thinking about duplication of scanpy/scvelo code). > What gets re-exported from ""main"" modules? didn't get this sorry. > Who manages the sub-packages? the IO subpackage? everyone  . so beside being in favour, it might also be that other issues arise. For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1243,interoperability,specif,specific,1243,"> Splitting off new modules. Finally, the idea of having IO functions go into their own package. I think this is a much bigger change, and I'd like to see a more fleshed out case for it. This would add a fair bit of complexity to development, so I'd want to be sure it's worth it. Some general questions I have:. > What are the advantages/ disadvantages of having smaller sub-packages? method developer would just depend on those instead of (multiple) analysis package. > How does this impact users vs. developers? user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. > Is IO special, or should more parts go into sub-packages? it kind of is imho, it's all about having whatever data there is in an anndata/mudata shape. I must say that I'd also think plotting could be it's own separate package but it would probably require a lot of refactoring across packages (thinking about duplication of scanpy/scvelo code). > What gets re-exported from ""main"" modules? didn't get this sorry. > Who manages the sub-packages? the IO subpackage? everyone  . so beside being in favour, it might also be that other issues arise. For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1405,interoperability,specif,specific,1405,"> Splitting off new modules. Finally, the idea of having IO functions go into their own package. I think this is a much bigger change, and I'd like to see a more fleshed out case for it. This would add a fair bit of complexity to development, so I'd want to be sure it's worth it. Some general questions I have:. > What are the advantages/ disadvantages of having smaller sub-packages? method developer would just depend on those instead of (multiple) analysis package. > How does this impact users vs. developers? user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. > Is IO special, or should more parts go into sub-packages? it kind of is imho, it's all about having whatever data there is in an anndata/mudata shape. I must say that I'd also think plotting could be it's own separate package but it would probably require a lot of refactoring across packages (thinking about duplication of scanpy/scvelo code). > What gets re-exported from ""main"" modules? didn't get this sorry. > Who manages the sub-packages? the IO subpackage? everyone  . so beside being in favour, it might also be that other issues arise. For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:20,modifiability,modul,modules,20,"> Splitting off new modules. Finally, the idea of having IO functions go into their own package. I think this is a much bigger change, and I'd like to see a more fleshed out case for it. This would add a fair bit of complexity to development, so I'd want to be sure it's worth it. Some general questions I have:. > What are the advantages/ disadvantages of having smaller sub-packages? method developer would just depend on those instead of (multiple) analysis package. > How does this impact users vs. developers? user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. > Is IO special, or should more parts go into sub-packages? it kind of is imho, it's all about having whatever data there is in an anndata/mudata shape. I must say that I'd also think plotting could be it's own separate package but it would probably require a lot of refactoring across packages (thinking about duplication of scanpy/scvelo code). > What gets re-exported from ""main"" modules? didn't get this sorry. > Who manages the sub-packages? the IO subpackage? everyone  . so beside being in favour, it might also be that other issues arise. For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:88,modifiability,pac,package,88,"> Splitting off new modules. Finally, the idea of having IO functions go into their own package. I think this is a much bigger change, and I'd like to see a more fleshed out case for it. This would add a fair bit of complexity to development, so I'd want to be sure it's worth it. Some general questions I have:. > What are the advantages/ disadvantages of having smaller sub-packages? method developer would just depend on those instead of (multiple) analysis package. > How does this impact users vs. developers? user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. > Is IO special, or should more parts go into sub-packages? it kind of is imho, it's all about having whatever data there is in an anndata/mudata shape. I must say that I'd also think plotting could be it's own separate package but it would probably require a lot of refactoring across packages (thinking about duplication of scanpy/scvelo code). > What gets re-exported from ""main"" modules? didn't get this sorry. > Who manages the sub-packages? the IO subpackage? everyone  . so beside being in favour, it might also be that other issues arise. For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:376,modifiability,pac,packages,376,"> Splitting off new modules. Finally, the idea of having IO functions go into their own package. I think this is a much bigger change, and I'd like to see a more fleshed out case for it. This would add a fair bit of complexity to development, so I'd want to be sure it's worth it. Some general questions I have:. > What are the advantages/ disadvantages of having smaller sub-packages? method developer would just depend on those instead of (multiple) analysis package. > How does this impact users vs. developers? user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. > Is IO special, or should more parts go into sub-packages? it kind of is imho, it's all about having whatever data there is in an anndata/mudata shape. I must say that I'd also think plotting could be it's own separate package but it would probably require a lot of refactoring across packages (thinking about duplication of scanpy/scvelo code). > What gets re-exported from ""main"" modules? didn't get this sorry. > Who manages the sub-packages? the IO subpackage? everyone  . so beside being in favour, it might also be that other issues arise. For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:414,modifiability,depend,depend,414,"> Splitting off new modules. Finally, the idea of having IO functions go into their own package. I think this is a much bigger change, and I'd like to see a more fleshed out case for it. This would add a fair bit of complexity to development, so I'd want to be sure it's worth it. Some general questions I have:. > What are the advantages/ disadvantages of having smaller sub-packages? method developer would just depend on those instead of (multiple) analysis package. > How does this impact users vs. developers? user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. > Is IO special, or should more parts go into sub-packages? it kind of is imho, it's all about having whatever data there is in an anndata/mudata shape. I must say that I'd also think plotting could be it's own separate package but it would probably require a lot of refactoring across packages (thinking about duplication of scanpy/scvelo code). > What gets re-exported from ""main"" modules? didn't get this sorry. > Who manages the sub-packages? the IO subpackage? everyone  . so beside being in favour, it might also be that other issues arise. For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:461,modifiability,pac,package,461,"> Splitting off new modules. Finally, the idea of having IO functions go into their own package. I think this is a much bigger change, and I'd like to see a more fleshed out case for it. This would add a fair bit of complexity to development, so I'd want to be sure it's worth it. Some general questions I have:. > What are the advantages/ disadvantages of having smaller sub-packages? method developer would just depend on those instead of (multiple) analysis package. > How does this impact users vs. developers? user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. > Is IO special, or should more parts go into sub-packages? it kind of is imho, it's all about having whatever data there is in an anndata/mudata shape. I must say that I'd also think plotting could be it's own separate package but it would probably require a lot of refactoring across packages (thinking about duplication of scanpy/scvelo code). > What gets re-exported from ""main"" modules? didn't get this sorry. > Who manages the sub-packages? the IO subpackage? everyone  . so beside being in favour, it might also be that other issues arise. For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:542,modifiability,pac,package,542,"> Splitting off new modules. Finally, the idea of having IO functions go into their own package. I think this is a much bigger change, and I'd like to see a more fleshed out case for it. This would add a fair bit of complexity to development, so I'd want to be sure it's worth it. Some general questions I have:. > What are the advantages/ disadvantages of having smaller sub-packages? method developer would just depend on those instead of (multiple) analysis package. > How does this impact users vs. developers? user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. > Is IO special, or should more parts go into sub-packages? it kind of is imho, it's all about having whatever data there is in an anndata/mudata shape. I must say that I'd also think plotting could be it's own separate package but it would probably require a lot of refactoring across packages (thinking about duplication of scanpy/scvelo code). > What gets re-exported from ""main"" modules? didn't get this sorry. > Who manages the sub-packages? the IO subpackage? everyone  . so beside being in favour, it might also be that other issues arise. For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:680,modifiability,pac,packages,680,"> Splitting off new modules. Finally, the idea of having IO functions go into their own package. I think this is a much bigger change, and I'd like to see a more fleshed out case for it. This would add a fair bit of complexity to development, so I'd want to be sure it's worth it. Some general questions I have:. > What are the advantages/ disadvantages of having smaller sub-packages? method developer would just depend on those instead of (multiple) analysis package. > How does this impact users vs. developers? user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. > Is IO special, or should more parts go into sub-packages? it kind of is imho, it's all about having whatever data there is in an anndata/mudata shape. I must say that I'd also think plotting could be it's own separate package but it would probably require a lot of refactoring across packages (thinking about duplication of scanpy/scvelo code). > What gets re-exported from ""main"" modules? didn't get this sorry. > Who manages the sub-packages? the IO subpackage? everyone  . so beside being in favour, it might also be that other issues arise. For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:850,modifiability,pac,package,850,"> Splitting off new modules. Finally, the idea of having IO functions go into their own package. I think this is a much bigger change, and I'd like to see a more fleshed out case for it. This would add a fair bit of complexity to development, so I'd want to be sure it's worth it. Some general questions I have:. > What are the advantages/ disadvantages of having smaller sub-packages? method developer would just depend on those instead of (multiple) analysis package. > How does this impact users vs. developers? user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. > Is IO special, or should more parts go into sub-packages? it kind of is imho, it's all about having whatever data there is in an anndata/mudata shape. I must say that I'd also think plotting could be it's own separate package but it would probably require a lot of refactoring across packages (thinking about duplication of scanpy/scvelo code). > What gets re-exported from ""main"" modules? didn't get this sorry. > Who manages the sub-packages? the IO subpackage? everyone  . so beside being in favour, it might also be that other issues arise. For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:897,modifiability,refact,refactoring,897,"> Splitting off new modules. Finally, the idea of having IO functions go into their own package. I think this is a much bigger change, and I'd like to see a more fleshed out case for it. This would add a fair bit of complexity to development, so I'd want to be sure it's worth it. Some general questions I have:. > What are the advantages/ disadvantages of having smaller sub-packages? method developer would just depend on those instead of (multiple) analysis package. > How does this impact users vs. developers? user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. > Is IO special, or should more parts go into sub-packages? it kind of is imho, it's all about having whatever data there is in an anndata/mudata shape. I must say that I'd also think plotting could be it's own separate package but it would probably require a lot of refactoring across packages (thinking about duplication of scanpy/scvelo code). > What gets re-exported from ""main"" modules? didn't get this sorry. > Who manages the sub-packages? the IO subpackage? everyone  . so beside being in favour, it might also be that other issues arise. For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:916,modifiability,pac,packages,916,"> Splitting off new modules. Finally, the idea of having IO functions go into their own package. I think this is a much bigger change, and I'd like to see a more fleshed out case for it. This would add a fair bit of complexity to development, so I'd want to be sure it's worth it. Some general questions I have:. > What are the advantages/ disadvantages of having smaller sub-packages? method developer would just depend on those instead of (multiple) analysis package. > How does this impact users vs. developers? user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. > Is IO special, or should more parts go into sub-packages? it kind of is imho, it's all about having whatever data there is in an anndata/mudata shape. I must say that I'd also think plotting could be it's own separate package but it would probably require a lot of refactoring across packages (thinking about duplication of scanpy/scvelo code). > What gets re-exported from ""main"" modules? didn't get this sorry. > Who manages the sub-packages? the IO subpackage? everyone  . so beside being in favour, it might also be that other issues arise. For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1013,modifiability,modul,modules,1013,"> Splitting off new modules. Finally, the idea of having IO functions go into their own package. I think this is a much bigger change, and I'd like to see a more fleshed out case for it. This would add a fair bit of complexity to development, so I'd want to be sure it's worth it. Some general questions I have:. > What are the advantages/ disadvantages of having smaller sub-packages? method developer would just depend on those instead of (multiple) analysis package. > How does this impact users vs. developers? user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. > Is IO special, or should more parts go into sub-packages? it kind of is imho, it's all about having whatever data there is in an anndata/mudata shape. I must say that I'd also think plotting could be it's own separate package but it would probably require a lot of refactoring across packages (thinking about duplication of scanpy/scvelo code). > What gets re-exported from ""main"" modules? didn't get this sorry. > Who manages the sub-packages? the IO subpackage? everyone  . so beside being in favour, it might also be that other issues arise. For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1067,modifiability,pac,packages,1067,"> Splitting off new modules. Finally, the idea of having IO functions go into their own package. I think this is a much bigger change, and I'd like to see a more fleshed out case for it. This would add a fair bit of complexity to development, so I'd want to be sure it's worth it. Some general questions I have:. > What are the advantages/ disadvantages of having smaller sub-packages? method developer would just depend on those instead of (multiple) analysis package. > How does this impact users vs. developers? user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. > Is IO special, or should more parts go into sub-packages? it kind of is imho, it's all about having whatever data there is in an anndata/mudata shape. I must say that I'd also think plotting could be it's own separate package but it would probably require a lot of refactoring across packages (thinking about duplication of scanpy/scvelo code). > What gets re-exported from ""main"" modules? didn't get this sorry. > Who manages the sub-packages? the IO subpackage? everyone  . so beside being in favour, it might also be that other issues arise. For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:897,performance,refactor,refactoring,897,"> Splitting off new modules. Finally, the idea of having IO functions go into their own package. I think this is a much bigger change, and I'd like to see a more fleshed out case for it. This would add a fair bit of complexity to development, so I'd want to be sure it's worth it. Some general questions I have:. > What are the advantages/ disadvantages of having smaller sub-packages? method developer would just depend on those instead of (multiple) analysis package. > How does this impact users vs. developers? user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. > Is IO special, or should more parts go into sub-packages? it kind of is imho, it's all about having whatever data there is in an anndata/mudata shape. I must say that I'd also think plotting could be it's own separate package but it would probably require a lot of refactoring across packages (thinking about duplication of scanpy/scvelo code). > What gets re-exported from ""main"" modules? didn't get this sorry. > Who manages the sub-packages? the IO subpackage? everyone  . so beside being in favour, it might also be that other issues arise. For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:476,reliability,doe,does,476,"> Splitting off new modules. Finally, the idea of having IO functions go into their own package. I think this is a much bigger change, and I'd like to see a more fleshed out case for it. This would add a fair bit of complexity to development, so I'd want to be sure it's worth it. Some general questions I have:. > What are the advantages/ disadvantages of having smaller sub-packages? method developer would just depend on those instead of (multiple) analysis package. > How does this impact users vs. developers? user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. > Is IO special, or should more parts go into sub-packages? it kind of is imho, it's all about having whatever data there is in an anndata/mudata shape. I must say that I'd also think plotting could be it's own separate package but it would probably require a lot of refactoring across packages (thinking about duplication of scanpy/scvelo code). > What gets re-exported from ""main"" modules? didn't get this sorry. > Who manages the sub-packages? the IO subpackage? everyone  . so beside being in favour, it might also be that other issues arise. For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1464,reliability,pra,practice,1464,"> Splitting off new modules. Finally, the idea of having IO functions go into their own package. I think this is a much bigger change, and I'd like to see a more fleshed out case for it. This would add a fair bit of complexity to development, so I'd want to be sure it's worth it. Some general questions I have:. > What are the advantages/ disadvantages of having smaller sub-packages? method developer would just depend on those instead of (multiple) analysis package. > How does this impact users vs. developers? user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. > Is IO special, or should more parts go into sub-packages? it kind of is imho, it's all about having whatever data there is in an anndata/mudata shape. I must say that I'd also think plotting could be it's own separate package but it would probably require a lot of refactoring across packages (thinking about duplication of scanpy/scvelo code). > What gets re-exported from ""main"" modules? didn't get this sorry. > Who manages the sub-packages? the IO subpackage? everyone  . so beside being in favour, it might also be that other issues arise. For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:20,safety,modul,modules,20,"> Splitting off new modules. Finally, the idea of having IO functions go into their own package. I think this is a much bigger change, and I'd like to see a more fleshed out case for it. This would add a fair bit of complexity to development, so I'd want to be sure it's worth it. Some general questions I have:. > What are the advantages/ disadvantages of having smaller sub-packages? method developer would just depend on those instead of (multiple) analysis package. > How does this impact users vs. developers? user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. > Is IO special, or should more parts go into sub-packages? it kind of is imho, it's all about having whatever data there is in an anndata/mudata shape. I must say that I'd also think plotting could be it's own separate package but it would probably require a lot of refactoring across packages (thinking about duplication of scanpy/scvelo code). > What gets re-exported from ""main"" modules? didn't get this sorry. > Who manages the sub-packages? the IO subpackage? everyone  . so beside being in favour, it might also be that other issues arise. For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:216,safety,compl,complexity,216,"> Splitting off new modules. Finally, the idea of having IO functions go into their own package. I think this is a much bigger change, and I'd like to see a more fleshed out case for it. This would add a fair bit of complexity to development, so I'd want to be sure it's worth it. Some general questions I have:. > What are the advantages/ disadvantages of having smaller sub-packages? method developer would just depend on those instead of (multiple) analysis package. > How does this impact users vs. developers? user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. > Is IO special, or should more parts go into sub-packages? it kind of is imho, it's all about having whatever data there is in an anndata/mudata shape. I must say that I'd also think plotting could be it's own separate package but it would probably require a lot of refactoring across packages (thinking about duplication of scanpy/scvelo code). > What gets re-exported from ""main"" modules? didn't get this sorry. > Who manages the sub-packages? the IO subpackage? everyone  . so beside being in favour, it might also be that other issues arise. For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:414,safety,depend,depend,414,"> Splitting off new modules. Finally, the idea of having IO functions go into their own package. I think this is a much bigger change, and I'd like to see a more fleshed out case for it. This would add a fair bit of complexity to development, so I'd want to be sure it's worth it. Some general questions I have:. > What are the advantages/ disadvantages of having smaller sub-packages? method developer would just depend on those instead of (multiple) analysis package. > How does this impact users vs. developers? user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. > Is IO special, or should more parts go into sub-packages? it kind of is imho, it's all about having whatever data there is in an anndata/mudata shape. I must say that I'd also think plotting could be it's own separate package but it would probably require a lot of refactoring across packages (thinking about duplication of scanpy/scvelo code). > What gets re-exported from ""main"" modules? didn't get this sorry. > Who manages the sub-packages? the IO subpackage? everyone  . so beside being in favour, it might also be that other issues arise. For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1013,safety,modul,modules,1013,"> Splitting off new modules. Finally, the idea of having IO functions go into their own package. I think this is a much bigger change, and I'd like to see a more fleshed out case for it. This would add a fair bit of complexity to development, so I'd want to be sure it's worth it. Some general questions I have:. > What are the advantages/ disadvantages of having smaller sub-packages? method developer would just depend on those instead of (multiple) analysis package. > How does this impact users vs. developers? user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. > Is IO special, or should more parts go into sub-packages? it kind of is imho, it's all about having whatever data there is in an anndata/mudata shape. I must say that I'd also think plotting could be it's own separate package but it would probably require a lot of refactoring across packages (thinking about duplication of scanpy/scvelo code). > What gets re-exported from ""main"" modules? didn't get this sorry. > Who manages the sub-packages? the IO subpackage? everyone  . so beside being in favour, it might also be that other issues arise. For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1051,safety,manag,manages,1051,"> Splitting off new modules. Finally, the idea of having IO functions go into their own package. I think this is a much bigger change, and I'd like to see a more fleshed out case for it. This would add a fair bit of complexity to development, so I'd want to be sure it's worth it. Some general questions I have:. > What are the advantages/ disadvantages of having smaller sub-packages? method developer would just depend on those instead of (multiple) analysis package. > How does this impact users vs. developers? user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. > Is IO special, or should more parts go into sub-packages? it kind of is imho, it's all about having whatever data there is in an anndata/mudata shape. I must say that I'd also think plotting could be it's own separate package but it would probably require a lot of refactoring across packages (thinking about duplication of scanpy/scvelo code). > What gets re-exported from ""main"" modules? didn't get this sorry. > Who manages the sub-packages? the IO subpackage? everyone  . so beside being in favour, it might also be that other issues arise. For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:216,security,compl,complexity,216,"> Splitting off new modules. Finally, the idea of having IO functions go into their own package. I think this is a much bigger change, and I'd like to see a more fleshed out case for it. This would add a fair bit of complexity to development, so I'd want to be sure it's worth it. Some general questions I have:. > What are the advantages/ disadvantages of having smaller sub-packages? method developer would just depend on those instead of (multiple) analysis package. > How does this impact users vs. developers? user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. > Is IO special, or should more parts go into sub-packages? it kind of is imho, it's all about having whatever data there is in an anndata/mudata shape. I must say that I'd also think plotting could be it's own separate package but it would probably require a lot of refactoring across packages (thinking about duplication of scanpy/scvelo code). > What gets re-exported from ""main"" modules? didn't get this sorry. > Who manages the sub-packages? the IO subpackage? everyone  . so beside being in favour, it might also be that other issues arise. For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:414,testability,depend,depend,414,"> Splitting off new modules. Finally, the idea of having IO functions go into their own package. I think this is a much bigger change, and I'd like to see a more fleshed out case for it. This would add a fair bit of complexity to development, so I'd want to be sure it's worth it. Some general questions I have:. > What are the advantages/ disadvantages of having smaller sub-packages? method developer would just depend on those instead of (multiple) analysis package. > How does this impact users vs. developers? user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. > Is IO special, or should more parts go into sub-packages? it kind of is imho, it's all about having whatever data there is in an anndata/mudata shape. I must say that I'd also think plotting could be it's own separate package but it would probably require a lot of refactoring across packages (thinking about duplication of scanpy/scvelo code). > What gets re-exported from ""main"" modules? didn't get this sorry. > Who manages the sub-packages? the IO subpackage? everyone  . so beside being in favour, it might also be that other issues arise. For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:493,usability,user,users,493,"> Splitting off new modules. Finally, the idea of having IO functions go into their own package. I think this is a much bigger change, and I'd like to see a more fleshed out case for it. This would add a fair bit of complexity to development, so I'd want to be sure it's worth it. Some general questions I have:. > What are the advantages/ disadvantages of having smaller sub-packages? method developer would just depend on those instead of (multiple) analysis package. > How does this impact users vs. developers? user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. > Is IO special, or should more parts go into sub-packages? it kind of is imho, it's all about having whatever data there is in an anndata/mudata shape. I must say that I'd also think plotting could be it's own separate package but it would probably require a lot of refactoring across packages (thinking about duplication of scanpy/scvelo code). > What gets re-exported from ""main"" modules? didn't get this sorry. > Who manages the sub-packages? the IO subpackage? everyone  . so beside being in favour, it might also be that other issues arise. For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:515,usability,user,user,515,"> Splitting off new modules. Finally, the idea of having IO functions go into their own package. I think this is a much bigger change, and I'd like to see a more fleshed out case for it. This would add a fair bit of complexity to development, so I'd want to be sure it's worth it. Some general questions I have:. > What are the advantages/ disadvantages of having smaller sub-packages? method developer would just depend on those instead of (multiple) analysis package. > How does this impact users vs. developers? user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. > Is IO special, or should more parts go into sub-packages? it kind of is imho, it's all about having whatever data there is in an anndata/mudata shape. I must say that I'd also think plotting could be it's own separate package but it would probably require a lot of refactoring across packages (thinking about duplication of scanpy/scvelo code). > What gets re-exported from ""main"" modules? didn't get this sorry. > Who manages the sub-packages? the IO subpackage? everyone  . so beside being in favour, it might also be that other issues arise. For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:162,modifiability,pac,package,162,"> it kind of is imho, it's all about having whatever data there is in an anndata/mudata shape. I must say that I'd also think plotting could be it's own separate package but it would probably require a lot of refactoring across packages (thinking about duplication of scanpy/scvelo code). @giovp and I are synced on these ideas :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:209,modifiability,refact,refactoring,209,"> it kind of is imho, it's all about having whatever data there is in an anndata/mudata shape. I must say that I'd also think plotting could be it's own separate package but it would probably require a lot of refactoring across packages (thinking about duplication of scanpy/scvelo code). @giovp and I are synced on these ideas :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:228,modifiability,pac,packages,228,"> it kind of is imho, it's all about having whatever data there is in an anndata/mudata shape. I must say that I'd also think plotting could be it's own separate package but it would probably require a lot of refactoring across packages (thinking about duplication of scanpy/scvelo code). @giovp and I are synced on these ideas :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:209,performance,refactor,refactoring,209,"> it kind of is imho, it's all about having whatever data there is in an anndata/mudata shape. I must say that I'd also think plotting could be it's own separate package but it would probably require a lot of refactoring across packages (thinking about duplication of scanpy/scvelo code). @giovp and I are synced on these ideas :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:67,interoperability,specif,specific,67,"> Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know. Could be, but even in this case, I think centralization is so so important and this package could receive a lot of community interaction....",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:234,modifiability,pac,package,234,"> Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know. Could be, but even in this case, I think centralization is so so important and this package could receive a lot of community interaction....",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:126,reliability,pra,practice,126,"> Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know. Could be, but even in this case, I think centralization is so so important and this package could receive a lot of community interaction....",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:275,usability,interact,interaction,275,"> Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know. Could be, but even in this case, I think centralization is so so important and this package could receive a lot of community interaction....",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:310,usability,tool,tools,310,Here are instances that could have leveraged `scio`. 1. [PyLiger](https://github.com/welch-lab/pyliger/blob/87300b43501d9a59219baad9025e9eaccbee0199/pyliger/read_write.py). 2. [Maestro](https://github.com/liulab-dfci/MAESTRO/blob/2c87d7a624745cc7e4fefc202042b221844e722c/MAESTRO/scATAC_H5Process.py). 3. [scvi-tools](https://github.com/scverse/scvi-tools/blob/3c838fadd484ede0e67c4a8d1a133202ef91cee3/scvi/data/_read.py#L10). 4. [scCloud](https://github.com/broadinstitute/scRNA-Seq/blob/03aafb92274a97f4d634ac9e42f0e0feca91ed98/scCloud/scCloud/tools/manage_10x_h5_matrices.py). 5. [Simba](https://github.com/pinellolab/simba/blob/1506e70f60b0fbc0e77e092e43aebd26c17961ac/simba/_utils.py). 6. [celltools](https://github.com/mvinyard/cell-tools/blob/fb56be6cfbb6c24bb46ba454afd1aa1a6a0fb8e3/cell_tools/_readwrite/_10X/_read_10X_ATAC_aggr.py). 7. [cellbender](https://github.com/broadinstitute/CellBender/blob/2507742203a3ba57251aa7b09721453e14e80c6d/cellbender/remove_background/data/dataset.py#L792),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:349,usability,tool,tools,349,Here are instances that could have leveraged `scio`. 1. [PyLiger](https://github.com/welch-lab/pyliger/blob/87300b43501d9a59219baad9025e9eaccbee0199/pyliger/read_write.py). 2. [Maestro](https://github.com/liulab-dfci/MAESTRO/blob/2c87d7a624745cc7e4fefc202042b221844e722c/MAESTRO/scATAC_H5Process.py). 3. [scvi-tools](https://github.com/scverse/scvi-tools/blob/3c838fadd484ede0e67c4a8d1a133202ef91cee3/scvi/data/_read.py#L10). 4. [scCloud](https://github.com/broadinstitute/scRNA-Seq/blob/03aafb92274a97f4d634ac9e42f0e0feca91ed98/scCloud/scCloud/tools/manage_10x_h5_matrices.py). 5. [Simba](https://github.com/pinellolab/simba/blob/1506e70f60b0fbc0e77e092e43aebd26c17961ac/simba/_utils.py). 6. [celltools](https://github.com/mvinyard/cell-tools/blob/fb56be6cfbb6c24bb46ba454afd1aa1a6a0fb8e3/cell_tools/_readwrite/_10X/_read_10X_ATAC_aggr.py). 7. [cellbender](https://github.com/broadinstitute/CellBender/blob/2507742203a3ba57251aa7b09721453e14e80c6d/cellbender/remove_background/data/dataset.py#L792),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:545,usability,tool,tools,545,Here are instances that could have leveraged `scio`. 1. [PyLiger](https://github.com/welch-lab/pyliger/blob/87300b43501d9a59219baad9025e9eaccbee0199/pyliger/read_write.py). 2. [Maestro](https://github.com/liulab-dfci/MAESTRO/blob/2c87d7a624745cc7e4fefc202042b221844e722c/MAESTRO/scATAC_H5Process.py). 3. [scvi-tools](https://github.com/scverse/scvi-tools/blob/3c838fadd484ede0e67c4a8d1a133202ef91cee3/scvi/data/_read.py#L10). 4. [scCloud](https://github.com/broadinstitute/scRNA-Seq/blob/03aafb92274a97f4d634ac9e42f0e0feca91ed98/scCloud/scCloud/tools/manage_10x_h5_matrices.py). 5. [Simba](https://github.com/pinellolab/simba/blob/1506e70f60b0fbc0e77e092e43aebd26c17961ac/simba/_utils.py). 6. [celltools](https://github.com/mvinyard/cell-tools/blob/fb56be6cfbb6c24bb46ba454afd1aa1a6a0fb8e3/cell_tools/_readwrite/_10X/_read_10X_ATAC_aggr.py). 7. [cellbender](https://github.com/broadinstitute/CellBender/blob/2507742203a3ba57251aa7b09721453e14e80c6d/cellbender/remove_background/data/dataset.py#L792),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:738,usability,tool,tools,738,Here are instances that could have leveraged `scio`. 1. [PyLiger](https://github.com/welch-lab/pyliger/blob/87300b43501d9a59219baad9025e9eaccbee0199/pyliger/read_write.py). 2. [Maestro](https://github.com/liulab-dfci/MAESTRO/blob/2c87d7a624745cc7e4fefc202042b221844e722c/MAESTRO/scATAC_H5Process.py). 3. [scvi-tools](https://github.com/scverse/scvi-tools/blob/3c838fadd484ede0e67c4a8d1a133202ef91cee3/scvi/data/_read.py#L10). 4. [scCloud](https://github.com/broadinstitute/scRNA-Seq/blob/03aafb92274a97f4d634ac9e42f0e0feca91ed98/scCloud/scCloud/tools/manage_10x_h5_matrices.py). 5. [Simba](https://github.com/pinellolab/simba/blob/1506e70f60b0fbc0e77e092e43aebd26c17961ac/simba/_utils.py). 6. [celltools](https://github.com/mvinyard/cell-tools/blob/fb56be6cfbb6c24bb46ba454afd1aa1a6a0fb8e3/cell_tools/_readwrite/_10X/_read_10X_ATAC_aggr.py). 7. [cellbender](https://github.com/broadinstitute/CellBender/blob/2507742203a3ba57251aa7b09721453e14e80c6d/cellbender/remove_background/data/dataset.py#L792),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:52,availability,sli,slight,52,"I'm wondering if we can come to some agreement on a slight modification to this proposal. > > How does this impact users vs. developers? >. > user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. This seems good. > > Who manages the sub-packages? >. > the IO subpackage? everyone .  indeed. > For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know. I feel like complicated dependency management was what we were trying to avoid here. Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. --------------. ## An alternative: project specific IO. `squidpy_io`, `muon_io`. Packages which read in package specific formats with a minimal set of dependencies. We can keep `muon.read_10x_atac`, so nothing changes for users. We skip out on complicated ownership and complicated dependencies. This should be very low overhead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:282,deployability,manag,manages,282,"I'm wondering if we can come to some agreement on a slight modification to this proposal. > > How does this impact users vs. developers? >. > user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. This seems good. > > Who manages the sub-packages? >. > the IO subpackage? everyone .  indeed. > For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know. I feel like complicated dependency management was what we were trying to avoid here. Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. --------------. ## An alternative: project specific IO. `squidpy_io`, `muon_io`. Packages which read in package specific formats with a minimal set of dependencies. We can keep `muon.read_10x_atac`, so nothing changes for users. We skip out on complicated ownership and complicated dependencies. This should be very low overhead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:690,deployability,depend,dependency,690,"I'm wondering if we can come to some agreement on a slight modification to this proposal. > > How does this impact users vs. developers? >. > user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. This seems good. > > Who manages the sub-packages? >. > the IO subpackage? everyone .  indeed. > For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know. I feel like complicated dependency management was what we were trying to avoid here. Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. --------------. ## An alternative: project specific IO. `squidpy_io`, `muon_io`. Packages which read in package specific formats with a minimal set of dependencies. We can keep `muon.read_10x_atac`, so nothing changes for users. We skip out on complicated ownership and complicated dependencies. This should be very low overhead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:701,deployability,manag,management,701,"I'm wondering if we can come to some agreement on a slight modification to this proposal. > > How does this impact users vs. developers? >. > user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. This seems good. > > Who manages the sub-packages? >. > the IO subpackage? everyone .  indeed. > For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know. I feel like complicated dependency management was what we were trying to avoid here. Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. --------------. ## An alternative: project specific IO. `squidpy_io`, `muon_io`. Packages which read in package specific formats with a minimal set of dependencies. We can keep `muon.read_10x_atac`, so nothing changes for users. We skip out on complicated ownership and complicated dependencies. This should be very low overhead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:775,deployability,instal,install,775,"I'm wondering if we can come to some agreement on a slight modification to this proposal. > > How does this impact users vs. developers? >. > user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. This seems good. > > Who manages the sub-packages? >. > the IO subpackage? everyone .  indeed. > For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know. I feel like complicated dependency management was what we were trying to avoid here. Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. --------------. ## An alternative: project specific IO. `squidpy_io`, `muon_io`. Packages which read in package specific formats with a minimal set of dependencies. We can keep `muon.read_10x_atac`, so nothing changes for users. We skip out on complicated ownership and complicated dependencies. This should be very low overhead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:870,deployability,depend,dependencies,870,"I'm wondering if we can come to some agreement on a slight modification to this proposal. > > How does this impact users vs. developers? >. > user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. This seems good. > > Who manages the sub-packages? >. > the IO subpackage? everyone .  indeed. > For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know. I feel like complicated dependency management was what we were trying to avoid here. Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. --------------. ## An alternative: project specific IO. `squidpy_io`, `muon_io`. Packages which read in package specific formats with a minimal set of dependencies. We can keep `muon.read_10x_atac`, so nothing changes for users. We skip out on complicated ownership and complicated dependencies. This should be very low overhead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1035,deployability,depend,dependencies,1035,"I'm wondering if we can come to some agreement on a slight modification to this proposal. > > How does this impact users vs. developers? >. > user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. This seems good. > > Who manages the sub-packages? >. > the IO subpackage? everyone .  indeed. > For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know. I feel like complicated dependency management was what we were trying to avoid here. Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. --------------. ## An alternative: project specific IO. `squidpy_io`, `muon_io`. Packages which read in package specific formats with a minimal set of dependencies. We can keep `muon.read_10x_atac`, so nothing changes for users. We skip out on complicated ownership and complicated dependencies. This should be very low overhead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1166,deployability,depend,dependencies,1166,"I'm wondering if we can come to some agreement on a slight modification to this proposal. > > How does this impact users vs. developers? >. > user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. This seems good. > > Who manages the sub-packages? >. > the IO subpackage? everyone .  indeed. > For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know. I feel like complicated dependency management was what we were trying to avoid here. Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. --------------. ## An alternative: project specific IO. `squidpy_io`, `muon_io`. Packages which read in package specific formats with a minimal set of dependencies. We can keep `muon.read_10x_atac`, so nothing changes for users. We skip out on complicated ownership and complicated dependencies. This should be very low overhead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:282,energy efficiency,manag,manages,282,"I'm wondering if we can come to some agreement on a slight modification to this proposal. > > How does this impact users vs. developers? >. > user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. This seems good. > > Who manages the sub-packages? >. > the IO subpackage? everyone .  indeed. > For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know. I feel like complicated dependency management was what we were trying to avoid here. Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. --------------. ## An alternative: project specific IO. `squidpy_io`, `muon_io`. Packages which read in package specific formats with a minimal set of dependencies. We can keep `muon.read_10x_atac`, so nothing changes for users. We skip out on complicated ownership and complicated dependencies. This should be very low overhead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:701,energy efficiency,manag,management,701,"I'm wondering if we can come to some agreement on a slight modification to this proposal. > > How does this impact users vs. developers? >. > user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. This seems good. > > Who manages the sub-packages? >. > the IO subpackage? everyone .  indeed. > For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know. I feel like complicated dependency management was what we were trying to avoid here. Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. --------------. ## An alternative: project specific IO. `squidpy_io`, `muon_io`. Packages which read in package specific formats with a minimal set of dependencies. We can keep `muon.read_10x_atac`, so nothing changes for users. We skip out on complicated ownership and complicated dependencies. This should be very low overhead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:294,integrability,sub,sub-packages,294,"I'm wondering if we can come to some agreement on a slight modification to this proposal. > > How does this impact users vs. developers? >. > user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. This seems good. > > Who manages the sub-packages? >. > the IO subpackage? everyone .  indeed. > For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know. I feel like complicated dependency management was what we were trying to avoid here. Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. --------------. ## An alternative: project specific IO. `squidpy_io`, `muon_io`. Packages which read in package specific formats with a minimal set of dependencies. We can keep `muon.read_10x_atac`, so nothing changes for users. We skip out on complicated ownership and complicated dependencies. This should be very low overhead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:320,integrability,sub,subpackage,320,"I'm wondering if we can come to some agreement on a slight modification to this proposal. > > How does this impact users vs. developers? >. > user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. This seems good. > > Who manages the sub-packages? >. > the IO subpackage? everyone .  indeed. > For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know. I feel like complicated dependency management was what we were trying to avoid here. Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. --------------. ## An alternative: project specific IO. `squidpy_io`, `muon_io`. Packages which read in package specific formats with a minimal set of dependencies. We can keep `muon.read_10x_atac`, so nothing changes for users. We skip out on complicated ownership and complicated dependencies. This should be very low overhead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:690,integrability,depend,dependency,690,"I'm wondering if we can come to some agreement on a slight modification to this proposal. > > How does this impact users vs. developers? >. > user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. This seems good. > > Who manages the sub-packages? >. > the IO subpackage? everyone .  indeed. > For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know. I feel like complicated dependency management was what we were trying to avoid here. Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. --------------. ## An alternative: project specific IO. `squidpy_io`, `muon_io`. Packages which read in package specific formats with a minimal set of dependencies. We can keep `muon.read_10x_atac`, so nothing changes for users. We skip out on complicated ownership and complicated dependencies. This should be very low overhead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:870,integrability,depend,dependencies,870,"I'm wondering if we can come to some agreement on a slight modification to this proposal. > > How does this impact users vs. developers? >. > user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. This seems good. > > Who manages the sub-packages? >. > the IO subpackage? everyone .  indeed. > For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know. I feel like complicated dependency management was what we were trying to avoid here. Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. --------------. ## An alternative: project specific IO. `squidpy_io`, `muon_io`. Packages which read in package specific formats with a minimal set of dependencies. We can keep `muon.read_10x_atac`, so nothing changes for users. We skip out on complicated ownership and complicated dependencies. This should be very low overhead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1035,integrability,depend,dependencies,1035,"I'm wondering if we can come to some agreement on a slight modification to this proposal. > > How does this impact users vs. developers? >. > user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. This seems good. > > Who manages the sub-packages? >. > the IO subpackage? everyone .  indeed. > For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know. I feel like complicated dependency management was what we were trying to avoid here. Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. --------------. ## An alternative: project specific IO. `squidpy_io`, `muon_io`. Packages which read in package specific formats with a minimal set of dependencies. We can keep `muon.read_10x_atac`, so nothing changes for users. We skip out on complicated ownership and complicated dependencies. This should be very low overhead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1166,integrability,depend,dependencies,1166,"I'm wondering if we can come to some agreement on a slight modification to this proposal. > > How does this impact users vs. developers? >. > user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. This seems good. > > Who manages the sub-packages? >. > the IO subpackage? everyone .  indeed. > For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know. I feel like complicated dependency management was what we were trying to avoid here. Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. --------------. ## An alternative: project specific IO. `squidpy_io`, `muon_io`. Packages which read in package specific formats with a minimal set of dependencies. We can keep `muon.read_10x_atac`, so nothing changes for users. We skip out on complicated ownership and complicated dependencies. This should be very low overhead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:383,interoperability,specif,specific,383,"I'm wondering if we can come to some agreement on a slight modification to this proposal. > > How does this impact users vs. developers? >. > user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. This seems good. > > Who manages the sub-packages? >. > the IO subpackage? everyone .  indeed. > For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know. I feel like complicated dependency management was what we were trying to avoid here. Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. --------------. ## An alternative: project specific IO. `squidpy_io`, `muon_io`. Packages which read in package specific formats with a minimal set of dependencies. We can keep `muon.read_10x_atac`, so nothing changes for users. We skip out on complicated ownership and complicated dependencies. This should be very low overhead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:392,interoperability,format,formats,392,"I'm wondering if we can come to some agreement on a slight modification to this proposal. > > How does this impact users vs. developers? >. > user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. This seems good. > > Who manages the sub-packages? >. > the IO subpackage? everyone .  indeed. > For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know. I feel like complicated dependency management was what we were trying to avoid here. Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. --------------. ## An alternative: project specific IO. `squidpy_io`, `muon_io`. Packages which read in package specific formats with a minimal set of dependencies. We can keep `muon.read_10x_atac`, so nothing changes for users. We skip out on complicated ownership and complicated dependencies. This should be very low overhead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:421,interoperability,specif,specific,421,"I'm wondering if we can come to some agreement on a slight modification to this proposal. > > How does this impact users vs. developers? >. > user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. This seems good. > > Who manages the sub-packages? >. > the IO subpackage? everyone .  indeed. > For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know. I feel like complicated dependency management was what we were trying to avoid here. Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. --------------. ## An alternative: project specific IO. `squidpy_io`, `muon_io`. Packages which read in package specific formats with a minimal set of dependencies. We can keep `muon.read_10x_atac`, so nothing changes for users. We skip out on complicated ownership and complicated dependencies. This should be very low overhead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:583,interoperability,specif,specific,583,"I'm wondering if we can come to some agreement on a slight modification to this proposal. > > How does this impact users vs. developers? >. > user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. This seems good. > > Who manages the sub-packages? >. > the IO subpackage? everyone .  indeed. > For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know. I feel like complicated dependency management was what we were trying to avoid here. Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. --------------. ## An alternative: project specific IO. `squidpy_io`, `muon_io`. Packages which read in package specific formats with a minimal set of dependencies. We can keep `muon.read_10x_atac`, so nothing changes for users. We skip out on complicated ownership and complicated dependencies. This should be very low overhead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:927,interoperability,specif,specific,927,"I'm wondering if we can come to some agreement on a slight modification to this proposal. > > How does this impact users vs. developers? >. > user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. This seems good. > > Who manages the sub-packages? >. > the IO subpackage? everyone .  indeed. > For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know. I feel like complicated dependency management was what we were trying to avoid here. Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. --------------. ## An alternative: project specific IO. `squidpy_io`, `muon_io`. Packages which read in package specific formats with a minimal set of dependencies. We can keep `muon.read_10x_atac`, so nothing changes for users. We skip out on complicated ownership and complicated dependencies. This should be very low overhead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:996,interoperability,specif,specific,996,"I'm wondering if we can come to some agreement on a slight modification to this proposal. > > How does this impact users vs. developers? >. > user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. This seems good. > > Who manages the sub-packages? >. > the IO subpackage? everyone .  indeed. > For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know. I feel like complicated dependency management was what we were trying to avoid here. Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. --------------. ## An alternative: project specific IO. `squidpy_io`, `muon_io`. Packages which read in package specific formats with a minimal set of dependencies. We can keep `muon.read_10x_atac`, so nothing changes for users. We skip out on complicated ownership and complicated dependencies. This should be very low overhead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1005,interoperability,format,formats,1005,"I'm wondering if we can come to some agreement on a slight modification to this proposal. > > How does this impact users vs. developers? >. > user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. This seems good. > > Who manages the sub-packages? >. > the IO subpackage? everyone .  indeed. > For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know. I feel like complicated dependency management was what we were trying to avoid here. Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. --------------. ## An alternative: project specific IO. `squidpy_io`, `muon_io`. Packages which read in package specific formats with a minimal set of dependencies. We can keep `muon.read_10x_atac`, so nothing changes for users. We skip out on complicated ownership and complicated dependencies. This should be very low overhead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:169,modifiability,pac,package,169,"I'm wondering if we can come to some agreement on a slight modification to this proposal. > > How does this impact users vs. developers? >. > user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. This seems good. > > Who manages the sub-packages? >. > the IO subpackage? everyone .  indeed. > For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know. I feel like complicated dependency management was what we were trying to avoid here. Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. --------------. ## An alternative: project specific IO. `squidpy_io`, `muon_io`. Packages which read in package specific formats with a minimal set of dependencies. We can keep `muon.read_10x_atac`, so nothing changes for users. We skip out on complicated ownership and complicated dependencies. This should be very low overhead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:298,modifiability,pac,packages,298,"I'm wondering if we can come to some agreement on a slight modification to this proposal. > > How does this impact users vs. developers? >. > user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. This seems good. > > Who manages the sub-packages? >. > the IO subpackage? everyone .  indeed. > For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know. I feel like complicated dependency management was what we were trying to avoid here. Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. --------------. ## An alternative: project specific IO. `squidpy_io`, `muon_io`. Packages which read in package specific formats with a minimal set of dependencies. We can keep `muon.read_10x_atac`, so nothing changes for users. We skip out on complicated ownership and complicated dependencies. This should be very low overhead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:690,modifiability,depend,dependency,690,"I'm wondering if we can come to some agreement on a slight modification to this proposal. > > How does this impact users vs. developers? >. > user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. This seems good. > > Who manages the sub-packages? >. > the IO subpackage? everyone .  indeed. > For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know. I feel like complicated dependency management was what we were trying to avoid here. Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. --------------. ## An alternative: project specific IO. `squidpy_io`, `muon_io`. Packages which read in package specific formats with a minimal set of dependencies. We can keep `muon.read_10x_atac`, so nothing changes for users. We skip out on complicated ownership and complicated dependencies. This should be very low overhead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:785,modifiability,pac,package,785,"I'm wondering if we can come to some agreement on a slight modification to this proposal. > > How does this impact users vs. developers? >. > user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. This seems good. > > Who manages the sub-packages? >. > the IO subpackage? everyone .  indeed. > For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know. I feel like complicated dependency management was what we were trying to avoid here. Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. --------------. ## An alternative: project specific IO. `squidpy_io`, `muon_io`. Packages which read in package specific formats with a minimal set of dependencies. We can keep `muon.read_10x_atac`, so nothing changes for users. We skip out on complicated ownership and complicated dependencies. This should be very low overhead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:870,modifiability,depend,dependencies,870,"I'm wondering if we can come to some agreement on a slight modification to this proposal. > > How does this impact users vs. developers? >. > user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. This seems good. > > Who manages the sub-packages? >. > the IO subpackage? everyone .  indeed. > For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know. I feel like complicated dependency management was what we were trying to avoid here. Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. --------------. ## An alternative: project specific IO. `squidpy_io`, `muon_io`. Packages which read in package specific formats with a minimal set of dependencies. We can keep `muon.read_10x_atac`, so nothing changes for users. We skip out on complicated ownership and complicated dependencies. This should be very low overhead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:965,modifiability,Pac,Packages,965,"I'm wondering if we can come to some agreement on a slight modification to this proposal. > > How does this impact users vs. developers? >. > user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. This seems good. > > Who manages the sub-packages? >. > the IO subpackage? everyone .  indeed. > For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know. I feel like complicated dependency management was what we were trying to avoid here. Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. --------------. ## An alternative: project specific IO. `squidpy_io`, `muon_io`. Packages which read in package specific formats with a minimal set of dependencies. We can keep `muon.read_10x_atac`, so nothing changes for users. We skip out on complicated ownership and complicated dependencies. This should be very low overhead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:988,modifiability,pac,package,988,"I'm wondering if we can come to some agreement on a slight modification to this proposal. > > How does this impact users vs. developers? >. > user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. This seems good. > > Who manages the sub-packages? >. > the IO subpackage? everyone .  indeed. > For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know. I feel like complicated dependency management was what we were trying to avoid here. Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. --------------. ## An alternative: project specific IO. `squidpy_io`, `muon_io`. Packages which read in package specific formats with a minimal set of dependencies. We can keep `muon.read_10x_atac`, so nothing changes for users. We skip out on complicated ownership and complicated dependencies. This should be very low overhead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1035,modifiability,depend,dependencies,1035,"I'm wondering if we can come to some agreement on a slight modification to this proposal. > > How does this impact users vs. developers? >. > user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. This seems good. > > Who manages the sub-packages? >. > the IO subpackage? everyone .  indeed. > For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know. I feel like complicated dependency management was what we were trying to avoid here. Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. --------------. ## An alternative: project specific IO. `squidpy_io`, `muon_io`. Packages which read in package specific formats with a minimal set of dependencies. We can keep `muon.read_10x_atac`, so nothing changes for users. We skip out on complicated ownership and complicated dependencies. This should be very low overhead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1166,modifiability,depend,dependencies,1166,"I'm wondering if we can come to some agreement on a slight modification to this proposal. > > How does this impact users vs. developers? >. > user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. This seems good. > > Who manages the sub-packages? >. > the IO subpackage? everyone .  indeed. > For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know. I feel like complicated dependency management was what we were trying to avoid here. Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. --------------. ## An alternative: project specific IO. `squidpy_io`, `muon_io`. Packages which read in package specific formats with a minimal set of dependencies. We can keep `muon.read_10x_atac`, so nothing changes for users. We skip out on complicated ownership and complicated dependencies. This should be very low overhead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1204,performance,overhead,overhead,1204,"I'm wondering if we can come to some agreement on a slight modification to this proposal. > > How does this impact users vs. developers? >. > user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. This seems good. > > Who manages the sub-packages? >. > the IO subpackage? everyone .  indeed. > For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know. I feel like complicated dependency management was what we were trying to avoid here. Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. --------------. ## An alternative: project specific IO. `squidpy_io`, `muon_io`. Packages which read in package specific formats with a minimal set of dependencies. We can keep `muon.read_10x_atac`, so nothing changes for users. We skip out on complicated ownership and complicated dependencies. This should be very low overhead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:52,reliability,sli,slight,52,"I'm wondering if we can come to some agreement on a slight modification to this proposal. > > How does this impact users vs. developers? >. > user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. This seems good. > > Who manages the sub-packages? >. > the IO subpackage? everyone .  indeed. > For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know. I feel like complicated dependency management was what we were trying to avoid here. Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. --------------. ## An alternative: project specific IO. `squidpy_io`, `muon_io`. Packages which read in package specific formats with a minimal set of dependencies. We can keep `muon.read_10x_atac`, so nothing changes for users. We skip out on complicated ownership and complicated dependencies. This should be very low overhead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:98,reliability,doe,does,98,"I'm wondering if we can come to some agreement on a slight modification to this proposal. > > How does this impact users vs. developers? >. > user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. This seems good. > > Who manages the sub-packages? >. > the IO subpackage? everyone .  indeed. > For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know. I feel like complicated dependency management was what we were trying to avoid here. Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. --------------. ## An alternative: project specific IO. `squidpy_io`, `muon_io`. Packages which read in package specific formats with a minimal set of dependencies. We can keep `muon.read_10x_atac`, so nothing changes for users. We skip out on complicated ownership and complicated dependencies. This should be very low overhead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:642,reliability,pra,practice,642,"I'm wondering if we can come to some agreement on a slight modification to this proposal. > > How does this impact users vs. developers? >. > user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. This seems good. > > Who manages the sub-packages? >. > the IO subpackage? everyone .  indeed. > For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know. I feel like complicated dependency management was what we were trying to avoid here. Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. --------------. ## An alternative: project specific IO. `squidpy_io`, `muon_io`. Packages which read in package specific formats with a minimal set of dependencies. We can keep `muon.read_10x_atac`, so nothing changes for users. We skip out on complicated ownership and complicated dependencies. This should be very low overhead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:282,safety,manag,manages,282,"I'm wondering if we can come to some agreement on a slight modification to this proposal. > > How does this impact users vs. developers? >. > user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. This seems good. > > Who manages the sub-packages? >. > the IO subpackage? everyone .  indeed. > For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know. I feel like complicated dependency management was what we were trying to avoid here. Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. --------------. ## An alternative: project specific IO. `squidpy_io`, `muon_io`. Packages which read in package specific formats with a minimal set of dependencies. We can keep `muon.read_10x_atac`, so nothing changes for users. We skip out on complicated ownership and complicated dependencies. This should be very low overhead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:678,safety,compl,complicated,678,"I'm wondering if we can come to some agreement on a slight modification to this proposal. > > How does this impact users vs. developers? >. > user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. This seems good. > > Who manages the sub-packages? >. > the IO subpackage? everyone .  indeed. > For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know. I feel like complicated dependency management was what we were trying to avoid here. Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. --------------. ## An alternative: project specific IO. `squidpy_io`, `muon_io`. Packages which read in package specific formats with a minimal set of dependencies. We can keep `muon.read_10x_atac`, so nothing changes for users. We skip out on complicated ownership and complicated dependencies. This should be very low overhead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:690,safety,depend,dependency,690,"I'm wondering if we can come to some agreement on a slight modification to this proposal. > > How does this impact users vs. developers? >. > user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. This seems good. > > Who manages the sub-packages? >. > the IO subpackage? everyone .  indeed. > For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know. I feel like complicated dependency management was what we were trying to avoid here. Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. --------------. ## An alternative: project specific IO. `squidpy_io`, `muon_io`. Packages which read in package specific formats with a minimal set of dependencies. We can keep `muon.read_10x_atac`, so nothing changes for users. We skip out on complicated ownership and complicated dependencies. This should be very low overhead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:701,safety,manag,management,701,"I'm wondering if we can come to some agreement on a slight modification to this proposal. > > How does this impact users vs. developers? >. > user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. This seems good. > > Who manages the sub-packages? >. > the IO subpackage? everyone .  indeed. > For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know. I feel like complicated dependency management was what we were trying to avoid here. Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. --------------. ## An alternative: project specific IO. `squidpy_io`, `muon_io`. Packages which read in package specific formats with a minimal set of dependencies. We can keep `muon.read_10x_atac`, so nothing changes for users. We skip out on complicated ownership and complicated dependencies. This should be very low overhead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:739,safety,avoid,avoid,739,"I'm wondering if we can come to some agreement on a slight modification to this proposal. > > How does this impact users vs. developers? >. > user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. This seems good. > > Who manages the sub-packages? >. > the IO subpackage? everyone .  indeed. > For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know. I feel like complicated dependency management was what we were trying to avoid here. Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. --------------. ## An alternative: project specific IO. `squidpy_io`, `muon_io`. Packages which read in package specific formats with a minimal set of dependencies. We can keep `muon.read_10x_atac`, so nothing changes for users. We skip out on complicated ownership and complicated dependencies. This should be very low overhead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:870,safety,depend,dependencies,870,"I'm wondering if we can come to some agreement on a slight modification to this proposal. > > How does this impact users vs. developers? >. > user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. This seems good. > > Who manages the sub-packages? >. > the IO subpackage? everyone .  indeed. > For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know. I feel like complicated dependency management was what we were trying to avoid here. Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. --------------. ## An alternative: project specific IO. `squidpy_io`, `muon_io`. Packages which read in package specific formats with a minimal set of dependencies. We can keep `muon.read_10x_atac`, so nothing changes for users. We skip out on complicated ownership and complicated dependencies. This should be very low overhead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1035,safety,depend,dependencies,1035,"I'm wondering if we can come to some agreement on a slight modification to this proposal. > > How does this impact users vs. developers? >. > user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. This seems good. > > Who manages the sub-packages? >. > the IO subpackage? everyone .  indeed. > For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know. I feel like complicated dependency management was what we were trying to avoid here. Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. --------------. ## An alternative: project specific IO. `squidpy_io`, `muon_io`. Packages which read in package specific formats with a minimal set of dependencies. We can keep `muon.read_10x_atac`, so nothing changes for users. We skip out on complicated ownership and complicated dependencies. This should be very low overhead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1128,safety,compl,complicated,1128,"I'm wondering if we can come to some agreement on a slight modification to this proposal. > > How does this impact users vs. developers? >. > user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. This seems good. > > Who manages the sub-packages? >. > the IO subpackage? everyone .  indeed. > For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know. I feel like complicated dependency management was what we were trying to avoid here. Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. --------------. ## An alternative: project specific IO. `squidpy_io`, `muon_io`. Packages which read in package specific formats with a minimal set of dependencies. We can keep `muon.read_10x_atac`, so nothing changes for users. We skip out on complicated ownership and complicated dependencies. This should be very low overhead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1154,safety,compl,complicated,1154,"I'm wondering if we can come to some agreement on a slight modification to this proposal. > > How does this impact users vs. developers? >. > user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. This seems good. > > Who manages the sub-packages? >. > the IO subpackage? everyone .  indeed. > For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know. I feel like complicated dependency management was what we were trying to avoid here. Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. --------------. ## An alternative: project specific IO. `squidpy_io`, `muon_io`. Packages which read in package specific formats with a minimal set of dependencies. We can keep `muon.read_10x_atac`, so nothing changes for users. We skip out on complicated ownership and complicated dependencies. This should be very low overhead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1166,safety,depend,dependencies,1166,"I'm wondering if we can come to some agreement on a slight modification to this proposal. > > How does this impact users vs. developers? >. > user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. This seems good. > > Who manages the sub-packages? >. > the IO subpackage? everyone .  indeed. > For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know. I feel like complicated dependency management was what we were trying to avoid here. Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. --------------. ## An alternative: project specific IO. `squidpy_io`, `muon_io`. Packages which read in package specific formats with a minimal set of dependencies. We can keep `muon.read_10x_atac`, so nothing changes for users. We skip out on complicated ownership and complicated dependencies. This should be very low overhead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:59,security,modif,modification,59,"I'm wondering if we can come to some agreement on a slight modification to this proposal. > > How does this impact users vs. developers? >. > user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. This seems good. > > Who manages the sub-packages? >. > the IO subpackage? everyone .  indeed. > For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know. I feel like complicated dependency management was what we were trying to avoid here. Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. --------------. ## An alternative: project specific IO. `squidpy_io`, `muon_io`. Packages which read in package specific formats with a minimal set of dependencies. We can keep `muon.read_10x_atac`, so nothing changes for users. We skip out on complicated ownership and complicated dependencies. This should be very low overhead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:678,security,compl,complicated,678,"I'm wondering if we can come to some agreement on a slight modification to this proposal. > > How does this impact users vs. developers? >. > user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. This seems good. > > Who manages the sub-packages? >. > the IO subpackage? everyone .  indeed. > For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know. I feel like complicated dependency management was what we were trying to avoid here. Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. --------------. ## An alternative: project specific IO. `squidpy_io`, `muon_io`. Packages which read in package specific formats with a minimal set of dependencies. We can keep `muon.read_10x_atac`, so nothing changes for users. We skip out on complicated ownership and complicated dependencies. This should be very low overhead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1128,security,compl,complicated,1128,"I'm wondering if we can come to some agreement on a slight modification to this proposal. > > How does this impact users vs. developers? >. > user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. This seems good. > > Who manages the sub-packages? >. > the IO subpackage? everyone .  indeed. > For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know. I feel like complicated dependency management was what we were trying to avoid here. Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. --------------. ## An alternative: project specific IO. `squidpy_io`, `muon_io`. Packages which read in package specific formats with a minimal set of dependencies. We can keep `muon.read_10x_atac`, so nothing changes for users. We skip out on complicated ownership and complicated dependencies. This should be very low overhead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1154,security,compl,complicated,1154,"I'm wondering if we can come to some agreement on a slight modification to this proposal. > > How does this impact users vs. developers? >. > user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. This seems good. > > Who manages the sub-packages? >. > the IO subpackage? everyone .  indeed. > For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know. I feel like complicated dependency management was what we were trying to avoid here. Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. --------------. ## An alternative: project specific IO. `squidpy_io`, `muon_io`. Packages which read in package specific formats with a minimal set of dependencies. We can keep `muon.read_10x_atac`, so nothing changes for users. We skip out on complicated ownership and complicated dependencies. This should be very low overhead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:690,testability,depend,dependency,690,"I'm wondering if we can come to some agreement on a slight modification to this proposal. > > How does this impact users vs. developers? >. > user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. This seems good. > > Who manages the sub-packages? >. > the IO subpackage? everyone .  indeed. > For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know. I feel like complicated dependency management was what we were trying to avoid here. Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. --------------. ## An alternative: project specific IO. `squidpy_io`, `muon_io`. Packages which read in package specific formats with a minimal set of dependencies. We can keep `muon.read_10x_atac`, so nothing changes for users. We skip out on complicated ownership and complicated dependencies. This should be very low overhead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:870,testability,depend,dependencies,870,"I'm wondering if we can come to some agreement on a slight modification to this proposal. > > How does this impact users vs. developers? >. > user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. This seems good. > > Who manages the sub-packages? >. > the IO subpackage? everyone .  indeed. > For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know. I feel like complicated dependency management was what we were trying to avoid here. Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. --------------. ## An alternative: project specific IO. `squidpy_io`, `muon_io`. Packages which read in package specific formats with a minimal set of dependencies. We can keep `muon.read_10x_atac`, so nothing changes for users. We skip out on complicated ownership and complicated dependencies. This should be very low overhead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1035,testability,depend,dependencies,1035,"I'm wondering if we can come to some agreement on a slight modification to this proposal. > > How does this impact users vs. developers? >. > user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. This seems good. > > Who manages the sub-packages? >. > the IO subpackage? everyone .  indeed. > For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know. I feel like complicated dependency management was what we were trying to avoid here. Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. --------------. ## An alternative: project specific IO. `squidpy_io`, `muon_io`. Packages which read in package specific formats with a minimal set of dependencies. We can keep `muon.read_10x_atac`, so nothing changes for users. We skip out on complicated ownership and complicated dependencies. This should be very low overhead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1166,testability,depend,dependencies,1166,"I'm wondering if we can come to some agreement on a slight modification to this proposal. > > How does this impact users vs. developers? >. > user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. This seems good. > > Who manages the sub-packages? >. > the IO subpackage? everyone .  indeed. > For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know. I feel like complicated dependency management was what we were trying to avoid here. Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. --------------. ## An alternative: project specific IO. `squidpy_io`, `muon_io`. Packages which read in package specific formats with a minimal set of dependencies. We can keep `muon.read_10x_atac`, so nothing changes for users. We skip out on complicated ownership and complicated dependencies. This should be very low overhead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:115,usability,user,users,115,"I'm wondering if we can come to some agreement on a slight modification to this proposal. > > How does this impact users vs. developers? >. > user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. This seems good. > > Who manages the sub-packages? >. > the IO subpackage? everyone .  indeed. > For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know. I feel like complicated dependency management was what we were trying to avoid here. Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. --------------. ## An alternative: project specific IO. `squidpy_io`, `muon_io`. Packages which read in package specific formats with a minimal set of dependencies. We can keep `muon.read_10x_atac`, so nothing changes for users. We skip out on complicated ownership and complicated dependencies. This should be very low overhead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:142,usability,user,user,142,"I'm wondering if we can come to some agreement on a slight modification to this proposal. > > How does this impact users vs. developers? >. > user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. This seems good. > > Who manages the sub-packages? >. > the IO subpackage? everyone .  indeed. > For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know. I feel like complicated dependency management was what we were trying to avoid here. Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. --------------. ## An alternative: project specific IO. `squidpy_io`, `muon_io`. Packages which read in package specific formats with a minimal set of dependencies. We can keep `muon.read_10x_atac`, so nothing changes for users. We skip out on complicated ownership and complicated dependencies. This should be very low overhead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1020,usability,minim,minimal,1020,"I'm wondering if we can come to some agreement on a slight modification to this proposal. > > How does this impact users vs. developers? >. > user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. This seems good. > > Who manages the sub-packages? >. > the IO subpackage? everyone .  indeed. > For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know. I feel like complicated dependency management was what we were trying to avoid here. Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. --------------. ## An alternative: project specific IO. `squidpy_io`, `muon_io`. Packages which read in package specific formats with a minimal set of dependencies. We can keep `muon.read_10x_atac`, so nothing changes for users. We skip out on complicated ownership and complicated dependencies. This should be very low overhead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1106,usability,user,users,1106,"I'm wondering if we can come to some agreement on a slight modification to this proposal. > > How does this impact users vs. developers? >. > user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. This seems good. > > Who manages the sub-packages? >. > the IO subpackage? everyone .  indeed. > For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know. I feel like complicated dependency management was what we were trying to avoid here. Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. --------------. ## An alternative: project specific IO. `squidpy_io`, `muon_io`. Packages which read in package specific formats with a minimal set of dependencies. We can keep `muon.read_10x_atac`, so nothing changes for users. We skip out on complicated ownership and complicated dependencies. This should be very low overhead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:468,deployability,manag,manages,468,"Project specific IO is interesting but IMO makes it even more complicated in some ways. The current biggest problem we face is that no one knows where to go to read certain formats.... scanpy? muon? squidpy? Scanpy has read visium but squidpy is the spatial package? I can analyze atac data in scanpy but need to use muon to read the file? Seurat has basically every reader one would need. This kind of fractured environment is not going to help us gain ground. > Who manages the sub-packages? Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places? > How does this impact users vs. developers? Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:731,deployability,releas,releases,731,"Project specific IO is interesting but IMO makes it even more complicated in some ways. The current biggest problem we face is that no one knows where to go to read certain formats.... scanpy? muon? squidpy? Scanpy has read visium but squidpy is the spatial package? I can analyze atac data in scanpy but need to use muon to read the file? Seurat has basically every reader one would need. This kind of fractured environment is not going to help us gain ground. > Who manages the sub-packages? Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places? > How does this impact users vs. developers? Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:794,deployability,depend,dependency,794,"Project specific IO is interesting but IMO makes it even more complicated in some ways. The current biggest problem we face is that no one knows where to go to read certain formats.... scanpy? muon? squidpy? Scanpy has read visium but squidpy is the spatial package? I can analyze atac data in scanpy but need to use muon to read the file? Seurat has basically every reader one would need. This kind of fractured environment is not going to help us gain ground. > Who manages the sub-packages? Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places? > How does this impact users vs. developers? Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:805,deployability,manag,management,805,"Project specific IO is interesting but IMO makes it even more complicated in some ways. The current biggest problem we face is that no one knows where to go to read certain formats.... scanpy? muon? squidpy? Scanpy has read visium but squidpy is the spatial package? I can analyze atac data in scanpy but need to use muon to read the file? Seurat has basically every reader one would need. This kind of fractured environment is not going to help us gain ground. > Who manages the sub-packages? Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places? > How does this impact users vs. developers? Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:880,deployability,depend,dependency,880,"Project specific IO is interesting but IMO makes it even more complicated in some ways. The current biggest problem we face is that no one knows where to go to read certain formats.... scanpy? muon? squidpy? Scanpy has read visium but squidpy is the spatial package? I can analyze atac data in scanpy but need to use muon to read the file? Seurat has basically every reader one would need. This kind of fractured environment is not going to help us gain ground. > Who manages the sub-packages? Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places? > How does this impact users vs. developers? Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:891,deployability,manag,management,891,"Project specific IO is interesting but IMO makes it even more complicated in some ways. The current biggest problem we face is that no one knows where to go to read certain formats.... scanpy? muon? squidpy? Scanpy has read visium but squidpy is the spatial package? I can analyze atac data in scanpy but need to use muon to read the file? Seurat has basically every reader one would need. This kind of fractured environment is not going to help us gain ground. > Who manages the sub-packages? Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places? > How does this impact users vs. developers? Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1255,deployability,depend,dependencies,1255,"kage? I can analyze atac data in scanpy but need to use muon to read the file? Seurat has basically every reader one would need. This kind of fractured environment is not going to help us gain ground. > Who manages the sub-packages? Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places? > How does this impact users vs. developers? Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use `scio` and then be on their way (a reality that many people do not feel the need to use scanpy/muon). If there are R converters, this would be a major use case. > What we read in, and how we represent it, is very tightly coupled to the methods we have. Up for ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1816,deployability,depend,depend,1816,"n ground. > Who manages the sub-packages? Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places? > How does this impact users vs. developers? Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use `scio` and then be on their way (a reality that many people do not feel the need to use scanpy/muon). If there are R converters, this would be a major use case. > What we read in, and how we represent it, is very tightly coupled to the methods we have. Up for discussion, but read the maximal amount of information by default. If necessary (don't see any particular cases at the moment), package devs use the underlying `scio` function and reorganize.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1881,deployability,continu,continue,1881,"n ground. > Who manages the sub-packages? Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places? > How does this impact users vs. developers? Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use `scio` and then be on their way (a reality that many people do not feel the need to use scanpy/muon). If there are R converters, this would be a major use case. > What we read in, and how we represent it, is very tightly coupled to the methods we have. Up for discussion, but read the maximal amount of information by default. If necessary (don't see any particular cases at the moment), package devs use the underlying `scio` function and reorganize.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:92,energy efficiency,current,current,92,"Project specific IO is interesting but IMO makes it even more complicated in some ways. The current biggest problem we face is that no one knows where to go to read certain formats.... scanpy? muon? squidpy? Scanpy has read visium but squidpy is the spatial package? I can analyze atac data in scanpy but need to use muon to read the file? Seurat has basically every reader one would need. This kind of fractured environment is not going to help us gain ground. > Who manages the sub-packages? Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places? > How does this impact users vs. developers? Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:468,energy efficiency,manag,manages,468,"Project specific IO is interesting but IMO makes it even more complicated in some ways. The current biggest problem we face is that no one knows where to go to read certain formats.... scanpy? muon? squidpy? Scanpy has read visium but squidpy is the spatial package? I can analyze atac data in scanpy but need to use muon to read the file? Seurat has basically every reader one would need. This kind of fractured environment is not going to help us gain ground. > Who manages the sub-packages? Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places? > How does this impact users vs. developers? Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:805,energy efficiency,manag,management,805,"Project specific IO is interesting but IMO makes it even more complicated in some ways. The current biggest problem we face is that no one knows where to go to read certain formats.... scanpy? muon? squidpy? Scanpy has read visium but squidpy is the spatial package? I can analyze atac data in scanpy but need to use muon to read the file? Seurat has basically every reader one would need. This kind of fractured environment is not going to help us gain ground. > Who manages the sub-packages? Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places? > How does this impact users vs. developers? Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:891,energy efficiency,manag,management,891,"Project specific IO is interesting but IMO makes it even more complicated in some ways. The current biggest problem we face is that no one knows where to go to read certain formats.... scanpy? muon? squidpy? Scanpy has read visium but squidpy is the spatial package? I can analyze atac data in scanpy but need to use muon to read the file? Seurat has basically every reader one would need. This kind of fractured environment is not going to help us gain ground. > Who manages the sub-packages? Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places? > How does this impact users vs. developers? Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:913,energy efficiency,core,core,913,"Project specific IO is interesting but IMO makes it even more complicated in some ways. The current biggest problem we face is that no one knows where to go to read certain formats.... scanpy? muon? squidpy? Scanpy has read visium but squidpy is the spatial package? I can analyze atac data in scanpy but need to use muon to read the file? Seurat has basically every reader one would need. This kind of fractured environment is not going to help us gain ground. > Who manages the sub-packages? Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places? > How does this impact users vs. developers? Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1467,energy efficiency,load,loading,1467,"n ground. > Who manages the sub-packages? Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places? > How does this impact users vs. developers? Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use `scio` and then be on their way (a reality that many people do not feel the need to use scanpy/muon). If there are R converters, this would be a major use case. > What we read in, and how we represent it, is very tightly coupled to the methods we have. Up for discussion, but read the maximal amount of information by default. If necessary (don't see any particular cases at the moment), package devs use the underlying `scio` function and reorganize.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:480,integrability,sub,sub-packages,480,"Project specific IO is interesting but IMO makes it even more complicated in some ways. The current biggest problem we face is that no one knows where to go to read certain formats.... scanpy? muon? squidpy? Scanpy has read visium but squidpy is the spatial package? I can analyze atac data in scanpy but need to use muon to read the file? Seurat has basically every reader one would need. This kind of fractured environment is not going to help us gain ground. > Who manages the sub-packages? Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places? > How does this impact users vs. developers? Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:794,integrability,depend,dependency,794,"Project specific IO is interesting but IMO makes it even more complicated in some ways. The current biggest problem we face is that no one knows where to go to read certain formats.... scanpy? muon? squidpy? Scanpy has read visium but squidpy is the spatial package? I can analyze atac data in scanpy but need to use muon to read the file? Seurat has basically every reader one would need. This kind of fractured environment is not going to help us gain ground. > Who manages the sub-packages? Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places? > How does this impact users vs. developers? Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:880,integrability,depend,dependency,880,"Project specific IO is interesting but IMO makes it even more complicated in some ways. The current biggest problem we face is that no one knows where to go to read certain formats.... scanpy? muon? squidpy? Scanpy has read visium but squidpy is the spatial package? I can analyze atac data in scanpy but need to use muon to read the file? Seurat has basically every reader one would need. This kind of fractured environment is not going to help us gain ground. > Who manages the sub-packages? Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places? > How does this impact users vs. developers? Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1255,integrability,depend,dependencies,1255,"kage? I can analyze atac data in scanpy but need to use muon to read the file? Seurat has basically every reader one would need. This kind of fractured environment is not going to help us gain ground. > Who manages the sub-packages? Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places? > How does this impact users vs. developers? Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use `scio` and then be on their way (a reality that many people do not feel the need to use scanpy/muon). If there are R converters, this would be a major use case. > What we read in, and how we represent it, is very tightly coupled to the methods we have. Up for ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1816,integrability,depend,depend,1816,"n ground. > Who manages the sub-packages? Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places? > How does this impact users vs. developers? Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use `scio` and then be on their way (a reality that many people do not feel the need to use scanpy/muon). If there are R converters, this would be a major use case. > What we read in, and how we represent it, is very tightly coupled to the methods we have. Up for discussion, but read the maximal amount of information by default. If necessary (don't see any particular cases at the moment), package devs use the underlying `scio` function and reorganize.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:2222,integrability,coupl,coupled,2222,"n ground. > Who manages the sub-packages? Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places? > How does this impact users vs. developers? Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use `scio` and then be on their way (a reality that many people do not feel the need to use scanpy/muon). If there are R converters, this would be a major use case. > What we read in, and how we represent it, is very tightly coupled to the methods we have. Up for discussion, but read the maximal amount of information by default. If necessary (don't see any particular cases at the moment), package devs use the underlying `scio` function and reorganize.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:8,interoperability,specif,specific,8,"Project specific IO is interesting but IMO makes it even more complicated in some ways. The current biggest problem we face is that no one knows where to go to read certain formats.... scanpy? muon? squidpy? Scanpy has read visium but squidpy is the spatial package? I can analyze atac data in scanpy but need to use muon to read the file? Seurat has basically every reader one would need. This kind of fractured environment is not going to help us gain ground. > Who manages the sub-packages? Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places? > How does this impact users vs. developers? Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:173,interoperability,format,formats,173,"Project specific IO is interesting but IMO makes it even more complicated in some ways. The current biggest problem we face is that no one knows where to go to read certain formats.... scanpy? muon? squidpy? Scanpy has read visium but squidpy is the spatial package? I can analyze atac data in scanpy but need to use muon to read the file? Seurat has basically every reader one would need. This kind of fractured environment is not going to help us gain ground. > Who manages the sub-packages? Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places? > How does this impact users vs. developers? Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1216,interoperability,specif,specific,1216,"ead visium but squidpy is the spatial package? I can analyze atac data in scanpy but need to use muon to read the file? Seurat has basically every reader one would need. This kind of fractured environment is not going to help us gain ground. > Who manages the sub-packages? Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places? > How does this impact users vs. developers? Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use `scio` and then be on their way (a reality that many people do not feel the need to use scanpy/muon). If there are R converters, this would be a major use case. > What we read in, and how we represent it, is very tightl",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1225,interoperability,format,formats,1225,"m but squidpy is the spatial package? I can analyze atac data in scanpy but need to use muon to read the file? Seurat has basically every reader one would need. This kind of fractured environment is not going to help us gain ground. > Who manages the sub-packages? Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places? > How does this impact users vs. developers? Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use `scio` and then be on their way (a reality that many people do not feel the need to use scanpy/muon). If there are R converters, this would be a major use case. > What we read in, and how we represent it, is very tightly coupled",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1306,interoperability,specif,specific,1306,"to use muon to read the file? Seurat has basically every reader one would need. This kind of fractured environment is not going to help us gain ground. > Who manages the sub-packages? Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places? > How does this impact users vs. developers? Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use `scio` and then be on their way (a reality that many people do not feel the need to use scanpy/muon). If there are R converters, this would be a major use case. > What we read in, and how we represent it, is very tightly coupled to the methods we have. Up for discussion, but read the maximal amount of inform",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1526,interoperability,specif,specific,1526,"n ground. > Who manages the sub-packages? Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places? > How does this impact users vs. developers? Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use `scio` and then be on their way (a reality that many people do not feel the need to use scanpy/muon). If there are R converters, this would be a major use case. > What we read in, and how we represent it, is very tightly coupled to the methods we have. Up for discussion, but read the maximal amount of information by default. If necessary (don't see any particular cases at the moment), package devs use the underlying `scio` function and reorganize.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:258,modifiability,pac,package,258,"Project specific IO is interesting but IMO makes it even more complicated in some ways. The current biggest problem we face is that no one knows where to go to read certain formats.... scanpy? muon? squidpy? Scanpy has read visium but squidpy is the spatial package? I can analyze atac data in scanpy but need to use muon to read the file? Seurat has basically every reader one would need. This kind of fractured environment is not going to help us gain ground. > Who manages the sub-packages? Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places? > How does this impact users vs. developers? Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:484,modifiability,pac,packages,484,"Project specific IO is interesting but IMO makes it even more complicated in some ways. The current biggest problem we face is that no one knows where to go to read certain formats.... scanpy? muon? squidpy? Scanpy has read visium but squidpy is the spatial package? I can analyze atac data in scanpy but need to use muon to read the file? Seurat has basically every reader one would need. This kind of fractured environment is not going to help us gain ground. > Who manages the sub-packages? Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places? > How does this impact users vs. developers? Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:517,modifiability,pac,package,517,"Project specific IO is interesting but IMO makes it even more complicated in some ways. The current biggest problem we face is that no one knows where to go to read certain formats.... scanpy? muon? squidpy? Scanpy has read visium but squidpy is the spatial package? I can analyze atac data in scanpy but need to use muon to read the file? Seurat has basically every reader one would need. This kind of fractured environment is not going to help us gain ground. > Who manages the sub-packages? Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places? > How does this impact users vs. developers? Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:662,modifiability,pac,package,662,"Project specific IO is interesting but IMO makes it even more complicated in some ways. The current biggest problem we face is that no one knows where to go to read certain formats.... scanpy? muon? squidpy? Scanpy has read visium but squidpy is the spatial package? I can analyze atac data in scanpy but need to use muon to read the file? Seurat has basically every reader one would need. This kind of fractured environment is not going to help us gain ground. > Who manages the sub-packages? Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places? > How does this impact users vs. developers? Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:673,modifiability,maintain,maintain,673,"Project specific IO is interesting but IMO makes it even more complicated in some ways. The current biggest problem we face is that no one knows where to go to read certain formats.... scanpy? muon? squidpy? Scanpy has read visium but squidpy is the spatial package? I can analyze atac data in scanpy but need to use muon to read the file? Seurat has basically every reader one would need. This kind of fractured environment is not going to help us gain ground. > Who manages the sub-packages? Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places? > How does this impact users vs. developers? Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:794,modifiability,depend,dependency,794,"Project specific IO is interesting but IMO makes it even more complicated in some ways. The current biggest problem we face is that no one knows where to go to read certain formats.... scanpy? muon? squidpy? Scanpy has read visium but squidpy is the spatial package? I can analyze atac data in scanpy but need to use muon to read the file? Seurat has basically every reader one would need. This kind of fractured environment is not going to help us gain ground. > Who manages the sub-packages? Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places? > How does this impact users vs. developers? Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:880,modifiability,depend,dependency,880,"Project specific IO is interesting but IMO makes it even more complicated in some ways. The current biggest problem we face is that no one knows where to go to read certain formats.... scanpy? muon? squidpy? Scanpy has read visium but squidpy is the spatial package? I can analyze atac data in scanpy but need to use muon to read the file? Seurat has basically every reader one would need. This kind of fractured environment is not going to help us gain ground. > Who manages the sub-packages? Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places? > How does this impact users vs. developers? Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1185,modifiability,Pac,Packages,1185,"py? muon? squidpy? Scanpy has read visium but squidpy is the spatial package? I can analyze atac data in scanpy but need to use muon to read the file? Seurat has basically every reader one would need. This kind of fractured environment is not going to help us gain ground. > Who manages the sub-packages? Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places? > How does this impact users vs. developers? Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use `scio` and then be on their way (a reality that many people do not feel the need to use scanpy/muon). If there are R converters, this would be a major use case. > What we read in, and how ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1208,modifiability,pac,package,1208,"py has read visium but squidpy is the spatial package? I can analyze atac data in scanpy but need to use muon to read the file? Seurat has basically every reader one would need. This kind of fractured environment is not going to help us gain ground. > Who manages the sub-packages? Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places? > How does this impact users vs. developers? Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use `scio` and then be on their way (a reality that many people do not feel the need to use scanpy/muon). If there are R converters, this would be a major use case. > What we read in, and how we represent it, is ver",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1255,modifiability,depend,dependencies,1255,"kage? I can analyze atac data in scanpy but need to use muon to read the file? Seurat has basically every reader one would need. This kind of fractured environment is not going to help us gain ground. > Who manages the sub-packages? Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places? > How does this impact users vs. developers? Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use `scio` and then be on their way (a reality that many people do not feel the need to use scanpy/muon). If there are R converters, this would be a major use case. > What we read in, and how we represent it, is very tightly coupled to the methods we have. Up for ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1298,modifiability,pac,package,1298,"ut need to use muon to read the file? Seurat has basically every reader one would need. This kind of fractured environment is not going to help us gain ground. > Who manages the sub-packages? Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places? > How does this impact users vs. developers? Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use `scio` and then be on their way (a reality that many people do not feel the need to use scanpy/muon). If there are R converters, this would be a major use case. > What we read in, and how we represent it, is very tightly coupled to the methods we have. Up for discussion, but read the maximal amount o",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1518,modifiability,pac,package,1518,"n ground. > Who manages the sub-packages? Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places? > How does this impact users vs. developers? Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use `scio` and then be on their way (a reality that many people do not feel the need to use scanpy/muon). If there are R converters, this would be a major use case. > What we read in, and how we represent it, is very tightly coupled to the methods we have. Up for discussion, but read the maximal amount of information by default. If necessary (don't see any particular cases at the moment), package devs use the underlying `scio` function and reorganize.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1637,modifiability,pac,packages,1637,"n ground. > Who manages the sub-packages? Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places? > How does this impact users vs. developers? Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use `scio` and then be on their way (a reality that many people do not feel the need to use scanpy/muon). If there are R converters, this would be a major use case. > What we read in, and how we represent it, is very tightly coupled to the methods we have. Up for discussion, but read the maximal amount of information by default. If necessary (don't see any particular cases at the moment), package devs use the underlying `scio` function and reorganize.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1740,modifiability,pac,packages,1740,"n ground. > Who manages the sub-packages? Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places? > How does this impact users vs. developers? Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use `scio` and then be on their way (a reality that many people do not feel the need to use scanpy/muon). If there are R converters, this would be a major use case. > What we read in, and how we represent it, is very tightly coupled to the methods we have. Up for discussion, but read the maximal amount of information by default. If necessary (don't see any particular cases at the moment), package devs use the underlying `scio` function and reorganize.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1816,modifiability,depend,depend,1816,"n ground. > Who manages the sub-packages? Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places? > How does this impact users vs. developers? Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use `scio` and then be on their way (a reality that many people do not feel the need to use scanpy/muon). If there are R converters, this would be a major use case. > What we read in, and how we represent it, is very tightly coupled to the methods we have. Up for discussion, but read the maximal amount of information by default. If necessary (don't see any particular cases at the moment), package devs use the underlying `scio` function and reorganize.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1900,modifiability,pac,packages,1900,"n ground. > Who manages the sub-packages? Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places? > How does this impact users vs. developers? Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use `scio` and then be on their way (a reality that many people do not feel the need to use scanpy/muon). If there are R converters, this would be a major use case. > What we read in, and how we represent it, is very tightly coupled to the methods we have. Up for discussion, but read the maximal amount of information by default. If necessary (don't see any particular cases at the moment), package devs use the underlying `scio` function and reorganize.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:2222,modifiability,coupl,coupled,2222,"n ground. > Who manages the sub-packages? Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places? > How does this impact users vs. developers? Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use `scio` and then be on their way (a reality that many people do not feel the need to use scanpy/muon). If there are R converters, this would be a major use case. > What we read in, and how we represent it, is very tightly coupled to the methods we have. Up for discussion, but read the maximal amount of information by default. If necessary (don't see any particular cases at the moment), package devs use the underlying `scio` function and reorganize.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:2389,modifiability,pac,package,2389,"n ground. > Who manages the sub-packages? Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places? > How does this impact users vs. developers? Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use `scio` and then be on their way (a reality that many people do not feel the need to use scanpy/muon). If there are R converters, this would be a major use case. > What we read in, and how we represent it, is very tightly coupled to the methods we have. Up for discussion, but read the maximal amount of information by default. If necessary (don't see any particular cases at the moment), package devs use the underlying `scio` function and reorganize.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:606,performance,time,times,606,"Project specific IO is interesting but IMO makes it even more complicated in some ways. The current biggest problem we face is that no one knows where to go to read certain formats.... scanpy? muon? squidpy? Scanpy has read visium but squidpy is the spatial package? I can analyze atac data in scanpy but need to use muon to read the file? Seurat has basically every reader one would need. This kind of fractured environment is not going to help us gain ground. > Who manages the sub-packages? Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places? > How does this impact users vs. developers? Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1137,performance,synch,synchronization,1137,"where to go to read certain formats.... scanpy? muon? squidpy? Scanpy has read visium but squidpy is the spatial package? I can analyze atac data in scanpy but need to use muon to read the file? Seurat has basically every reader one would need. This kind of fractured environment is not going to help us gain ground. > Who manages the sub-packages? Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places? > How does this impact users vs. developers? Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use `scio` and then be on their way (a reality that many people do not feel the need to use scanpy/muon). If there are R converters, this would be a",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1467,performance,load,loading,1467,"n ground. > Who manages the sub-packages? Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places? > How does this impact users vs. developers? Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use `scio` and then be on their way (a reality that many people do not feel the need to use scanpy/muon). If there are R converters, this would be a major use case. > What we read in, and how we represent it, is very tightly coupled to the methods we have. Up for discussion, but read the maximal amount of information by default. If necessary (don't see any particular cases at the moment), package devs use the underlying `scio` function and reorganize.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1549,reliability,doe,does,1549,"n ground. > Who manages the sub-packages? Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places? > How does this impact users vs. developers? Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use `scio` and then be on their way (a reality that many people do not feel the need to use scanpy/muon). If there are R converters, this would be a major use case. > What we read in, and how we represent it, is very tightly coupled to the methods we have. Up for discussion, but read the maximal amount of information by default. If necessary (don't see any particular cases at the moment), package devs use the underlying `scio` function and reorganize.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1712,reliability,pra,practical,1712,"n ground. > Who manages the sub-packages? Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places? > How does this impact users vs. developers? Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use `scio` and then be on their way (a reality that many people do not feel the need to use scanpy/muon). If there are R converters, this would be a major use case. > What we read in, and how we represent it, is very tightly coupled to the methods we have. Up for discussion, but read the maximal amount of information by default. If necessary (don't see any particular cases at the moment), package devs use the underlying `scio` function and reorganize.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:62,safety,compl,complicated,62,"Project specific IO is interesting but IMO makes it even more complicated in some ways. The current biggest problem we face is that no one knows where to go to read certain formats.... scanpy? muon? squidpy? Scanpy has read visium but squidpy is the spatial package? I can analyze atac data in scanpy but need to use muon to read the file? Seurat has basically every reader one would need. This kind of fractured environment is not going to help us gain ground. > Who manages the sub-packages? Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places? > How does this impact users vs. developers? Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:468,safety,manag,manages,468,"Project specific IO is interesting but IMO makes it even more complicated in some ways. The current biggest problem we face is that no one knows where to go to read certain formats.... scanpy? muon? squidpy? Scanpy has read visium but squidpy is the spatial package? I can analyze atac data in scanpy but need to use muon to read the file? Seurat has basically every reader one would need. This kind of fractured environment is not going to help us gain ground. > Who manages the sub-packages? Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places? > How does this impact users vs. developers? Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:650,safety,compl,complicated,650,"Project specific IO is interesting but IMO makes it even more complicated in some ways. The current biggest problem we face is that no one knows where to go to read certain formats.... scanpy? muon? squidpy? Scanpy has read visium but squidpy is the spatial package? I can analyze atac data in scanpy but need to use muon to read the file? Seurat has basically every reader one would need. This kind of fractured environment is not going to help us gain ground. > Who manages the sub-packages? Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places? > How does this impact users vs. developers? Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:673,safety,maintain,maintain,673,"Project specific IO is interesting but IMO makes it even more complicated in some ways. The current biggest problem we face is that no one knows where to go to read certain formats.... scanpy? muon? squidpy? Scanpy has read visium but squidpy is the spatial package? I can analyze atac data in scanpy but need to use muon to read the file? Seurat has basically every reader one would need. This kind of fractured environment is not going to help us gain ground. > Who manages the sub-packages? Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places? > How does this impact users vs. developers? Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:782,safety,compl,complicated,782,"Project specific IO is interesting but IMO makes it even more complicated in some ways. The current biggest problem we face is that no one knows where to go to read certain formats.... scanpy? muon? squidpy? Scanpy has read visium but squidpy is the spatial package? I can analyze atac data in scanpy but need to use muon to read the file? Seurat has basically every reader one would need. This kind of fractured environment is not going to help us gain ground. > Who manages the sub-packages? Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places? > How does this impact users vs. developers? Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:794,safety,depend,dependency,794,"Project specific IO is interesting but IMO makes it even more complicated in some ways. The current biggest problem we face is that no one knows where to go to read certain formats.... scanpy? muon? squidpy? Scanpy has read visium but squidpy is the spatial package? I can analyze atac data in scanpy but need to use muon to read the file? Seurat has basically every reader one would need. This kind of fractured environment is not going to help us gain ground. > Who manages the sub-packages? Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places? > How does this impact users vs. developers? Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:805,safety,manag,management,805,"Project specific IO is interesting but IMO makes it even more complicated in some ways. The current biggest problem we face is that no one knows where to go to read certain formats.... scanpy? muon? squidpy? Scanpy has read visium but squidpy is the spatial package? I can analyze atac data in scanpy but need to use muon to read the file? Seurat has basically every reader one would need. This kind of fractured environment is not going to help us gain ground. > Who manages the sub-packages? Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places? > How does this impact users vs. developers? Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:843,safety,avoid,avoid,843,"Project specific IO is interesting but IMO makes it even more complicated in some ways. The current biggest problem we face is that no one knows where to go to read certain formats.... scanpy? muon? squidpy? Scanpy has read visium but squidpy is the spatial package? I can analyze atac data in scanpy but need to use muon to read the file? Seurat has basically every reader one would need. This kind of fractured environment is not going to help us gain ground. > Who manages the sub-packages? Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places? > How does this impact users vs. developers? Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:868,safety,compl,complicated,868,"Project specific IO is interesting but IMO makes it even more complicated in some ways. The current biggest problem we face is that no one knows where to go to read certain formats.... scanpy? muon? squidpy? Scanpy has read visium but squidpy is the spatial package? I can analyze atac data in scanpy but need to use muon to read the file? Seurat has basically every reader one would need. This kind of fractured environment is not going to help us gain ground. > Who manages the sub-packages? Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places? > How does this impact users vs. developers? Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:880,safety,depend,dependency,880,"Project specific IO is interesting but IMO makes it even more complicated in some ways. The current biggest problem we face is that no one knows where to go to read certain formats.... scanpy? muon? squidpy? Scanpy has read visium but squidpy is the spatial package? I can analyze atac data in scanpy but need to use muon to read the file? Seurat has basically every reader one would need. This kind of fractured environment is not going to help us gain ground. > Who manages the sub-packages? Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places? > How does this impact users vs. developers? Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:891,safety,manag,management,891,"Project specific IO is interesting but IMO makes it even more complicated in some ways. The current biggest problem we face is that no one knows where to go to read certain formats.... scanpy? muon? squidpy? Scanpy has read visium but squidpy is the spatial package? I can analyze atac data in scanpy but need to use muon to read the file? Seurat has basically every reader one would need. This kind of fractured environment is not going to help us gain ground. > Who manages the sub-packages? Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places? > How does this impact users vs. developers? Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:962,safety,compl,complex,962,"Project specific IO is interesting but IMO makes it even more complicated in some ways. The current biggest problem we face is that no one knows where to go to read certain formats.... scanpy? muon? squidpy? Scanpy has read visium but squidpy is the spatial package? I can analyze atac data in scanpy but need to use muon to read the file? Seurat has basically every reader one would need. This kind of fractured environment is not going to help us gain ground. > Who manages the sub-packages? Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places? > How does this impact users vs. developers? Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1255,safety,depend,dependencies,1255,"kage? I can analyze atac data in scanpy but need to use muon to read the file? Seurat has basically every reader one would need. This kind of fractured environment is not going to help us gain ground. > Who manages the sub-packages? Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places? > How does this impact users vs. developers? Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use `scio` and then be on their way (a reality that many people do not feel the need to use scanpy/muon). If there are R converters, this would be a major use case. > What we read in, and how we represent it, is very tightly coupled to the methods we have. Up for ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1816,safety,depend,depend,1816,"n ground. > Who manages the sub-packages? Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places? > How does this impact users vs. developers? Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use `scio` and then be on their way (a reality that many people do not feel the need to use scanpy/muon). If there are R converters, this would be a major use case. > What we read in, and how we represent it, is very tightly coupled to the methods we have. Up for discussion, but read the maximal amount of information by default. If necessary (don't see any particular cases at the moment), package devs use the underlying `scio` function and reorganize.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1948,safety,compl,completely,1948,"n ground. > Who manages the sub-packages? Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places? > How does this impact users vs. developers? Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use `scio` and then be on their way (a reality that many people do not feel the need to use scanpy/muon). If there are R converters, this would be a major use case. > What we read in, and how we represent it, is very tightly coupled to the methods we have. Up for discussion, but read the maximal amount of information by default. If necessary (don't see any particular cases at the moment), package devs use the underlying `scio` function and reorganize.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:62,security,compl,complicated,62,"Project specific IO is interesting but IMO makes it even more complicated in some ways. The current biggest problem we face is that no one knows where to go to read certain formats.... scanpy? muon? squidpy? Scanpy has read visium but squidpy is the spatial package? I can analyze atac data in scanpy but need to use muon to read the file? Seurat has basically every reader one would need. This kind of fractured environment is not going to help us gain ground. > Who manages the sub-packages? Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places? > How does this impact users vs. developers? Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:650,security,compl,complicated,650,"Project specific IO is interesting but IMO makes it even more complicated in some ways. The current biggest problem we face is that no one knows where to go to read certain formats.... scanpy? muon? squidpy? Scanpy has read visium but squidpy is the spatial package? I can analyze atac data in scanpy but need to use muon to read the file? Seurat has basically every reader one would need. This kind of fractured environment is not going to help us gain ground. > Who manages the sub-packages? Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places? > How does this impact users vs. developers? Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:782,security,compl,complicated,782,"Project specific IO is interesting but IMO makes it even more complicated in some ways. The current biggest problem we face is that no one knows where to go to read certain formats.... scanpy? muon? squidpy? Scanpy has read visium but squidpy is the spatial package? I can analyze atac data in scanpy but need to use muon to read the file? Seurat has basically every reader one would need. This kind of fractured environment is not going to help us gain ground. > Who manages the sub-packages? Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places? > How does this impact users vs. developers? Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:868,security,compl,complicated,868,"Project specific IO is interesting but IMO makes it even more complicated in some ways. The current biggest problem we face is that no one knows where to go to read certain formats.... scanpy? muon? squidpy? Scanpy has read visium but squidpy is the spatial package? I can analyze atac data in scanpy but need to use muon to read the file? Seurat has basically every reader one would need. This kind of fractured environment is not going to help us gain ground. > Who manages the sub-packages? Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places? > How does this impact users vs. developers? Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:962,security,compl,complex,962,"Project specific IO is interesting but IMO makes it even more complicated in some ways. The current biggest problem we face is that no one knows where to go to read certain formats.... scanpy? muon? squidpy? Scanpy has read visium but squidpy is the spatial package? I can analyze atac data in scanpy but need to use muon to read the file? Seurat has basically every reader one would need. This kind of fractured environment is not going to help us gain ground. > Who manages the sub-packages? Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places? > How does this impact users vs. developers? Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1692,security,access,access,1692,"n ground. > Who manages the sub-packages? Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places? > How does this impact users vs. developers? Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use `scio` and then be on their way (a reality that many people do not feel the need to use scanpy/muon). If there are R converters, this would be a major use case. > What we read in, and how we represent it, is very tightly coupled to the methods we have. Up for discussion, but read the maximal amount of information by default. If necessary (don't see any particular cases at the moment), package devs use the underlying `scio` function and reorganize.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1948,security,compl,completely,1948,"n ground. > Who manages the sub-packages? Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places? > How does this impact users vs. developers? Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use `scio` and then be on their way (a reality that many people do not feel the need to use scanpy/muon). If there are R converters, this would be a major use case. > What we read in, and how we represent it, is very tightly coupled to the methods we have. Up for discussion, but read the maximal amount of information by default. If necessary (don't see any particular cases at the moment), package devs use the underlying `scio` function and reorganize.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:794,testability,depend,dependency,794,"Project specific IO is interesting but IMO makes it even more complicated in some ways. The current biggest problem we face is that no one knows where to go to read certain formats.... scanpy? muon? squidpy? Scanpy has read visium but squidpy is the spatial package? I can analyze atac data in scanpy but need to use muon to read the file? Seurat has basically every reader one would need. This kind of fractured environment is not going to help us gain ground. > Who manages the sub-packages? Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places? > How does this impact users vs. developers? Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:880,testability,depend,dependency,880,"Project specific IO is interesting but IMO makes it even more complicated in some ways. The current biggest problem we face is that no one knows where to go to read certain formats.... scanpy? muon? squidpy? Scanpy has read visium but squidpy is the spatial package? I can analyze atac data in scanpy but need to use muon to read the file? Seurat has basically every reader one would need. This kind of fractured environment is not going to help us gain ground. > Who manages the sub-packages? Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places? > How does this impact users vs. developers? Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1255,testability,depend,dependencies,1255,"kage? I can analyze atac data in scanpy but need to use muon to read the file? Seurat has basically every reader one would need. This kind of fractured environment is not going to help us gain ground. > Who manages the sub-packages? Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places? > How does this impact users vs. developers? Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use `scio` and then be on their way (a reality that many people do not feel the need to use scanpy/muon). If there are R converters, this would be a major use case. > What we read in, and how we represent it, is very tightly coupled to the methods we have. Up for ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1816,testability,depend,depend,1816,"n ground. > Who manages the sub-packages? Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places? > How does this impact users vs. developers? Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use `scio` and then be on their way (a reality that many people do not feel the need to use scanpy/muon). If there are R converters, this would be a major use case. > What we read in, and how we represent it, is very tightly coupled to the methods we have. Up for discussion, but read the maximal amount of information by default. If necessary (don't see any particular cases at the moment), package devs use the underlying `scio` function and reorganize.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:2222,testability,coupl,coupled,2222,"n ground. > Who manages the sub-packages? Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places? > How does this impact users vs. developers? Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use `scio` and then be on their way (a reality that many people do not feel the need to use scanpy/muon). If there are R converters, this would be a major use case. > What we read in, and how we represent it, is very tightly coupled to the methods we have. Up for discussion, but read the maximal amount of information by default. If necessary (don't see any particular cases at the moment), package devs use the underlying `scio` function and reorganize.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:441,usability,help,help,441,"Project specific IO is interesting but IMO makes it even more complicated in some ways. The current biggest problem we face is that no one knows where to go to read certain formats.... scanpy? muon? squidpy? Scanpy has read visium but squidpy is the spatial package? I can analyze atac data in scanpy but need to use muon to read the file? Seurat has basically every reader one would need. This kind of fractured environment is not going to help us gain ground. > Who manages the sub-packages? Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places? > How does this impact users vs. developers? Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:698,usability,person,person,698,"Project specific IO is interesting but IMO makes it even more complicated in some ways. The current biggest problem we face is that no one knows where to go to read certain formats.... scanpy? muon? squidpy? Scanpy has read visium but squidpy is the spatial package? I can analyze atac data in scanpy but need to use muon to read the file? Seurat has basically every reader one would need. This kind of fractured environment is not going to help us gain ground. > Who manages the sub-packages? Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places? > How does this impact users vs. developers? Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1166,usability,user,user,1166,"tain formats.... scanpy? muon? squidpy? Scanpy has read visium but squidpy is the spatial package? I can analyze atac data in scanpy but need to use muon to read the file? Seurat has basically every reader one would need. This kind of fractured environment is not going to help us gain ground. > Who manages the sub-packages? Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places? > How does this impact users vs. developers? Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use `scio` and then be on their way (a reality that many people do not feel the need to use scanpy/muon). If there are R converters, this would be a major use case. > What",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1171,usability,experien,experience,1171,"mats.... scanpy? muon? squidpy? Scanpy has read visium but squidpy is the spatial package? I can analyze atac data in scanpy but need to use muon to read the file? Seurat has basically every reader one would need. This kind of fractured environment is not going to help us gain ground. > Who manages the sub-packages? Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places? > How does this impact users vs. developers? Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use `scio` and then be on their way (a reality that many people do not feel the need to use scanpy/muon). If there are R converters, this would be a major use case. > What we read",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1240,usability,minim,minimal,1240,"s the spatial package? I can analyze atac data in scanpy but need to use muon to read the file? Seurat has basically every reader one would need. This kind of fractured environment is not going to help us gain ground. > Who manages the sub-packages? Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places? > How does this impact users vs. developers? Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use `scio` and then be on their way (a reality that many people do not feel the need to use scanpy/muon). If there are R converters, this would be a major use case. > What we read in, and how we represent it, is very tightly coupled to the methods",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1566,usability,user,users,1566,"n ground. > Who manages the sub-packages? Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places? > How does this impact users vs. developers? Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use `scio` and then be on their way (a reality that many people do not feel the need to use scanpy/muon). If there are R converters, this would be a major use case. > What we read in, and how we represent it, is very tightly coupled to the methods we have. Up for discussion, but read the maximal amount of information by default. If necessary (don't see any particular cases at the moment), package devs use the underlying `scio` function and reorganize.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1755,usability,tool,tools,1755,"n ground. > Who manages the sub-packages? Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places? > How does this impact users vs. developers? Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use `scio` and then be on their way (a reality that many people do not feel the need to use scanpy/muon). If there are R converters, this would be a major use case. > What we read in, and how we represent it, is very tightly coupled to the methods we have. Up for discussion, but read the maximal amount of information by default. If necessary (don't see any particular cases at the moment), package devs use the underlying `scio` function and reorganize.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1852,usability,User,Users,1852,"n ground. > Who manages the sub-packages? Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places? > How does this impact users vs. developers? Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use `scio` and then be on their way (a reality that many people do not feel the need to use scanpy/muon). If there are R converters, this would be a major use case. > What we read in, and how we represent it, is very tightly coupled to the methods we have. Up for discussion, but read the maximal amount of information by default. If necessary (don't see any particular cases at the moment), package devs use the underlying `scio` function and reorganize.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:778,availability,failur,failure,778,"I am more and more convinced about having a single package for the reasons @adamgayoso mentioned. To address a few concerns from above: . ---. > > Who manages the sub-packages? > . > Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). Scverse core developers could take turns (e.g. every 6 months) in being ""lead maintainer"", i.e. in charge of releases and first-responders to issues (delegating them to the most appropriate people). This has the additional advantage that everything needs to be documented to a point that there can't be a single point of failure. . ---. > Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. ```. pip install scio[all]. ```. could be broadly advertised in the README. Packages could still use the slimmer version, e.g. in scirpy, I could depend on . `scio[vdj]`. . ---. > I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult. I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common `scio` package could be a way to achieve that. . > I know squidpy will be changing its representation and I think muon should have changes to the ATAC representation. Also muon and scvi-tools read in different things from 10x atac data. A solution to that would be versioned schemata. E.g. whatever squidpy uses now is the ""spatial schema `v1`"". When we come up with a better way it becomes the ""spatial schema `v2`"". Old schemata will be deprecated but can stick around for a while. If a schema is experimental and subject to active changes it can be `v0.1`. . ```python. scio.spatial.read_vis",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1034,availability,sli,slimmer,1034,"d about having a single package for the reasons @adamgayoso mentioned. To address a few concerns from above: . ---. > > Who manages the sub-packages? > . > Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). Scverse core developers could take turns (e.g. every 6 months) in being ""lead maintainer"", i.e. in charge of releases and first-responders to issues (delegating them to the most appropriate people). This has the additional advantage that everything needs to be documented to a point that there can't be a single point of failure. . ---. > Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. ```. pip install scio[all]. ```. could be broadly advertised in the README. Packages could still use the slimmer version, e.g. in scirpy, I could depend on . `scio[vdj]`. . ---. > I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult. I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common `scio` package could be a way to achieve that. . > I know squidpy will be changing its representation and I think muon should have changes to the ATAC representation. Also muon and scvi-tools read in different things from 10x atac data. A solution to that would be versioned schemata. E.g. whatever squidpy uses now is the ""spatial schema `v1`"". When we come up with a better way it becomes the ""spatial schema `v2`"". Old schemata will be deprecated but can stick around for a while. If a schema is experimental and subject to active changes it can be `v0.1`. . ```python. scio.spatial.read_visium(path, schema=""v1""). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:151,deployability,manag,manages,151,"I am more and more convinced about having a single package for the reasons @adamgayoso mentioned. To address a few concerns from above: . ---. > > Who manages the sub-packages? > . > Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). Scverse core developers could take turns (e.g. every 6 months) in being ""lead maintainer"", i.e. in charge of releases and first-responders to issues (delegating them to the most appropriate people). This has the additional advantage that everything needs to be documented to a point that there can't be a single point of failure. . ---. > Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. ```. pip install scio[all]. ```. could be broadly advertised in the README. Packages could still use the slimmer version, e.g. in scirpy, I could depend on . `scio[vdj]`. . ---. > I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult. I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common `scio` package could be a way to achieve that. . > I know squidpy will be changing its representation and I think muon should have changes to the ATAC representation. Also muon and scvi-tools read in different things from 10x atac data. A solution to that would be versioned schemata. E.g. whatever squidpy uses now is the ""spatial schema `v1`"". When we come up with a better way it becomes the ""spatial schema `v2`"". Old schemata will be deprecated but can stick around for a while. If a schema is experimental and subject to active changes it can be `v0.1`. . ```python. scio.spatial.read_vis",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:420,deployability,releas,releases,420,"I am more and more convinced about having a single package for the reasons @adamgayoso mentioned. To address a few concerns from above: . ---. > > Who manages the sub-packages? > . > Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). Scverse core developers could take turns (e.g. every 6 months) in being ""lead maintainer"", i.e. in charge of releases and first-responders to issues (delegating them to the most appropriate people). This has the additional advantage that everything needs to be documented to a point that there can't be a single point of failure. . ---. > Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. ```. pip install scio[all]. ```. could be broadly advertised in the README. Packages could still use the slimmer version, e.g. in scirpy, I could depend on . `scio[vdj]`. . ---. > I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult. I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common `scio` package could be a way to achieve that. . > I know squidpy will be changing its representation and I think muon should have changes to the ATAC representation. Also muon and scvi-tools read in different things from 10x atac data. A solution to that would be versioned schemata. E.g. whatever squidpy uses now is the ""spatial schema `v1`"". When we come up with a better way it becomes the ""spatial schema `v2`"". Old schemata will be deprecated but can stick around for a while. If a schema is experimental and subject to active changes it can be `v0.1`. . ```python. scio.spatial.read_vis",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:566,deployability,releas,releases,566,"I am more and more convinced about having a single package for the reasons @adamgayoso mentioned. To address a few concerns from above: . ---. > > Who manages the sub-packages? > . > Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). Scverse core developers could take turns (e.g. every 6 months) in being ""lead maintainer"", i.e. in charge of releases and first-responders to issues (delegating them to the most appropriate people). This has the additional advantage that everything needs to be documented to a point that there can't be a single point of failure. . ---. > Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. ```. pip install scio[all]. ```. could be broadly advertised in the README. Packages could still use the slimmer version, e.g. in scirpy, I could depend on . `scio[vdj]`. . ---. > I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult. I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common `scio` package could be a way to achieve that. . > I know squidpy will be changing its representation and I think muon should have changes to the ATAC representation. Also muon and scvi-tools read in different things from 10x atac data. A solution to that would be versioned schemata. E.g. whatever squidpy uses now is the ""spatial schema `v1`"". When we come up with a better way it becomes the ""spatial schema `v2`"". Old schemata will be deprecated but can stick around for a while. If a schema is experimental and subject to active changes it can be `v0.1`. . ```python. scio.spatial.read_vis",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:778,deployability,fail,failure,778,"I am more and more convinced about having a single package for the reasons @adamgayoso mentioned. To address a few concerns from above: . ---. > > Who manages the sub-packages? > . > Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). Scverse core developers could take turns (e.g. every 6 months) in being ""lead maintainer"", i.e. in charge of releases and first-responders to issues (delegating them to the most appropriate people). This has the additional advantage that everything needs to be documented to a point that there can't be a single point of failure. . ---. > Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. ```. pip install scio[all]. ```. could be broadly advertised in the README. Packages could still use the slimmer version, e.g. in scirpy, I could depend on . `scio[vdj]`. . ---. > I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult. I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common `scio` package could be a way to achieve that. . > I know squidpy will be changing its representation and I think muon should have changes to the ATAC representation. Also muon and scvi-tools read in different things from 10x atac data. A solution to that would be versioned schemata. E.g. whatever squidpy uses now is the ""spatial schema `v1`"". When we come up with a better way it becomes the ""spatial schema `v2`"". Old schemata will be deprecated but can stick around for a while. If a schema is experimental and subject to active changes it can be `v0.1`. . ```python. scio.spatial.read_vis",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:820,deployability,instal,install,820,"I am more and more convinced about having a single package for the reasons @adamgayoso mentioned. To address a few concerns from above: . ---. > > Who manages the sub-packages? > . > Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). Scverse core developers could take turns (e.g. every 6 months) in being ""lead maintainer"", i.e. in charge of releases and first-responders to issues (delegating them to the most appropriate people). This has the additional advantage that everything needs to be documented to a point that there can't be a single point of failure. . ---. > Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. ```. pip install scio[all]. ```. could be broadly advertised in the README. Packages could still use the slimmer version, e.g. in scirpy, I could depend on . `scio[vdj]`. . ---. > I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult. I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common `scio` package could be a way to achieve that. . > I know squidpy will be changing its representation and I think muon should have changes to the ATAC representation. Also muon and scvi-tools read in different things from 10x atac data. A solution to that would be versioned schemata. E.g. whatever squidpy uses now is the ""spatial schema `v1`"". When we come up with a better way it becomes the ""spatial schema `v2`"". Old schemata will be deprecated but can stick around for a while. If a schema is experimental and subject to active changes it can be `v0.1`. . ```python. scio.spatial.read_vis",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:915,deployability,depend,dependencies,915,"I am more and more convinced about having a single package for the reasons @adamgayoso mentioned. To address a few concerns from above: . ---. > > Who manages the sub-packages? > . > Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). Scverse core developers could take turns (e.g. every 6 months) in being ""lead maintainer"", i.e. in charge of releases and first-responders to issues (delegating them to the most appropriate people). This has the additional advantage that everything needs to be documented to a point that there can't be a single point of failure. . ---. > Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. ```. pip install scio[all]. ```. could be broadly advertised in the README. Packages could still use the slimmer version, e.g. in scirpy, I could depend on . `scio[vdj]`. . ---. > I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult. I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common `scio` package could be a way to achieve that. . > I know squidpy will be changing its representation and I think muon should have changes to the ATAC representation. Also muon and scvi-tools read in different things from 10x atac data. A solution to that would be versioned schemata. E.g. whatever squidpy uses now is the ""spatial schema `v1`"". When we come up with a better way it becomes the ""spatial schema `v2`"". Old schemata will be deprecated but can stick around for a while. If a schema is experimental and subject to active changes it can be `v0.1`. . ```python. scio.spatial.read_vis",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:938,deployability,instal,install,938,"I am more and more convinced about having a single package for the reasons @adamgayoso mentioned. To address a few concerns from above: . ---. > > Who manages the sub-packages? > . > Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). Scverse core developers could take turns (e.g. every 6 months) in being ""lead maintainer"", i.e. in charge of releases and first-responders to issues (delegating them to the most appropriate people). This has the additional advantage that everything needs to be documented to a point that there can't be a single point of failure. . ---. > Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. ```. pip install scio[all]. ```. could be broadly advertised in the README. Packages could still use the slimmer version, e.g. in scirpy, I could depend on . `scio[vdj]`. . ---. > I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult. I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common `scio` package could be a way to achieve that. . > I know squidpy will be changing its representation and I think muon should have changes to the ATAC representation. Also muon and scvi-tools read in different things from 10x atac data. A solution to that would be versioned schemata. E.g. whatever squidpy uses now is the ""spatial schema `v1`"". When we come up with a better way it becomes the ""spatial schema `v2`"". Old schemata will be deprecated but can stick around for a while. If a schema is experimental and subject to active changes it can be `v0.1`. . ```python. scio.spatial.read_vis",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1042,deployability,version,version,1042,"d about having a single package for the reasons @adamgayoso mentioned. To address a few concerns from above: . ---. > > Who manages the sub-packages? > . > Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). Scverse core developers could take turns (e.g. every 6 months) in being ""lead maintainer"", i.e. in charge of releases and first-responders to issues (delegating them to the most appropriate people). This has the additional advantage that everything needs to be documented to a point that there can't be a single point of failure. . ---. > Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. ```. pip install scio[all]. ```. could be broadly advertised in the README. Packages could still use the slimmer version, e.g. in scirpy, I could depend on . `scio[vdj]`. . ---. > I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult. I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common `scio` package could be a way to achieve that. . > I know squidpy will be changing its representation and I think muon should have changes to the ATAC representation. Also muon and scvi-tools read in different things from 10x atac data. A solution to that would be versioned schemata. E.g. whatever squidpy uses now is the ""spatial schema `v1`"". When we come up with a better way it becomes the ""spatial schema `v2`"". Old schemata will be deprecated but can stick around for a while. If a schema is experimental and subject to active changes it can be `v0.1`. . ```python. scio.spatial.read_visium(path, schema=""v1""). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1075,deployability,depend,depend,1075,"d about having a single package for the reasons @adamgayoso mentioned. To address a few concerns from above: . ---. > > Who manages the sub-packages? > . > Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). Scverse core developers could take turns (e.g. every 6 months) in being ""lead maintainer"", i.e. in charge of releases and first-responders to issues (delegating them to the most appropriate people). This has the additional advantage that everything needs to be documented to a point that there can't be a single point of failure. . ---. > Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. ```. pip install scio[all]. ```. could be broadly advertised in the README. Packages could still use the slimmer version, e.g. in scirpy, I could depend on . `scio[vdj]`. . ---. > I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult. I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common `scio` package could be a way to achieve that. . > I know squidpy will be changing its representation and I think muon should have changes to the ATAC representation. Also muon and scvi-tools read in different things from 10x atac data. A solution to that would be versioned schemata. E.g. whatever squidpy uses now is the ""spatial schema `v1`"". When we come up with a better way it becomes the ""spatial schema `v2`"". Old schemata will be deprecated but can stick around for a while. If a schema is experimental and subject to active changes it can be `v0.1`. . ```python. scio.spatial.read_visium(path, schema=""v1""). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1671,deployability,version,versioned,1671,"d about having a single package for the reasons @adamgayoso mentioned. To address a few concerns from above: . ---. > > Who manages the sub-packages? > . > Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). Scverse core developers could take turns (e.g. every 6 months) in being ""lead maintainer"", i.e. in charge of releases and first-responders to issues (delegating them to the most appropriate people). This has the additional advantage that everything needs to be documented to a point that there can't be a single point of failure. . ---. > Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. ```. pip install scio[all]. ```. could be broadly advertised in the README. Packages could still use the slimmer version, e.g. in scirpy, I could depend on . `scio[vdj]`. . ---. > I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult. I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common `scio` package could be a way to achieve that. . > I know squidpy will be changing its representation and I think muon should have changes to the ATAC representation. Also muon and scvi-tools read in different things from 10x atac data. A solution to that would be versioned schemata. E.g. whatever squidpy uses now is the ""spatial schema `v1`"". When we come up with a better way it becomes the ""spatial schema `v2`"". Old schemata will be deprecated but can stick around for a while. If a schema is experimental and subject to active changes it can be `v0.1`. . ```python. scio.spatial.read_visium(path, schema=""v1""). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:151,energy efficiency,manag,manages,151,"I am more and more convinced about having a single package for the reasons @adamgayoso mentioned. To address a few concerns from above: . ---. > > Who manages the sub-packages? > . > Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). Scverse core developers could take turns (e.g. every 6 months) in being ""lead maintainer"", i.e. in charge of releases and first-responders to issues (delegating them to the most appropriate people). This has the additional advantage that everything needs to be documented to a point that there can't be a single point of failure. . ---. > Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. ```. pip install scio[all]. ```. could be broadly advertised in the README. Packages could still use the slimmer version, e.g. in scirpy, I could depend on . `scio[vdj]`. . ---. > I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult. I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common `scio` package could be a way to achieve that. . > I know squidpy will be changing its representation and I think muon should have changes to the ATAC representation. Also muon and scvi-tools read in different things from 10x atac data. A solution to that would be versioned schemata. E.g. whatever squidpy uses now is the ""spatial schema `v1`"". When we come up with a better way it becomes the ""spatial schema `v2`"". Old schemata will be deprecated but can stick around for a while. If a schema is experimental and subject to active changes it can be `v0.1`. . ```python. scio.spatial.read_vis",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:465,energy efficiency,core,core,465,"I am more and more convinced about having a single package for the reasons @adamgayoso mentioned. To address a few concerns from above: . ---. > > Who manages the sub-packages? > . > Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). Scverse core developers could take turns (e.g. every 6 months) in being ""lead maintainer"", i.e. in charge of releases and first-responders to issues (delegating them to the most appropriate people). This has the additional advantage that everything needs to be documented to a point that there can't be a single point of failure. . ---. > Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. ```. pip install scio[all]. ```. could be broadly advertised in the README. Packages could still use the slimmer version, e.g. in scirpy, I could depend on . `scio[vdj]`. . ---. > I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult. I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common `scio` package could be a way to achieve that. . > I know squidpy will be changing its representation and I think muon should have changes to the ATAC representation. Also muon and scvi-tools read in different things from 10x atac data. A solution to that would be versioned schemata. E.g. whatever squidpy uses now is the ""spatial schema `v1`"". When we come up with a better way it becomes the ""spatial schema `v2`"". Old schemata will be deprecated but can stick around for a while. If a schema is experimental and subject to active changes it can be `v0.1`. . ```python. scio.spatial.read_vis",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:556,energy efficiency,charg,charge,556,"I am more and more convinced about having a single package for the reasons @adamgayoso mentioned. To address a few concerns from above: . ---. > > Who manages the sub-packages? > . > Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). Scverse core developers could take turns (e.g. every 6 months) in being ""lead maintainer"", i.e. in charge of releases and first-responders to issues (delegating them to the most appropriate people). This has the additional advantage that everything needs to be documented to a point that there can't be a single point of failure. . ---. > Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. ```. pip install scio[all]. ```. could be broadly advertised in the README. Packages could still use the slimmer version, e.g. in scirpy, I could depend on . `scio[vdj]`. . ---. > I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult. I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common `scio` package could be a way to achieve that. . > I know squidpy will be changing its representation and I think muon should have changes to the ATAC representation. Also muon and scvi-tools read in different things from 10x atac data. A solution to that would be versioned schemata. E.g. whatever squidpy uses now is the ""spatial schema `v1`"". When we come up with a better way it becomes the ""spatial schema `v2`"". Old schemata will be deprecated but can stick around for a while. If a schema is experimental and subject to active changes it can be `v0.1`. . ```python. scio.spatial.read_vis",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:163,integrability,sub,sub-packages,163,"I am more and more convinced about having a single package for the reasons @adamgayoso mentioned. To address a few concerns from above: . ---. > > Who manages the sub-packages? > . > Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). Scverse core developers could take turns (e.g. every 6 months) in being ""lead maintainer"", i.e. in charge of releases and first-responders to issues (delegating them to the most appropriate people). This has the additional advantage that everything needs to be documented to a point that there can't be a single point of failure. . ---. > Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. ```. pip install scio[all]. ```. could be broadly advertised in the README. Packages could still use the slimmer version, e.g. in scirpy, I could depend on . `scio[vdj]`. . ---. > I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult. I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common `scio` package could be a way to achieve that. . > I know squidpy will be changing its representation and I think muon should have changes to the ATAC representation. Also muon and scvi-tools read in different things from 10x atac data. A solution to that would be versioned schemata. E.g. whatever squidpy uses now is the ""spatial schema `v1`"". When we come up with a better way it becomes the ""spatial schema `v2`"". Old schemata will be deprecated but can stick around for a while. If a schema is experimental and subject to active changes it can be `v0.1`. . ```python. scio.spatial.read_vis",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:915,integrability,depend,dependencies,915,"I am more and more convinced about having a single package for the reasons @adamgayoso mentioned. To address a few concerns from above: . ---. > > Who manages the sub-packages? > . > Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). Scverse core developers could take turns (e.g. every 6 months) in being ""lead maintainer"", i.e. in charge of releases and first-responders to issues (delegating them to the most appropriate people). This has the additional advantage that everything needs to be documented to a point that there can't be a single point of failure. . ---. > Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. ```. pip install scio[all]. ```. could be broadly advertised in the README. Packages could still use the slimmer version, e.g. in scirpy, I could depend on . `scio[vdj]`. . ---. > I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult. I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common `scio` package could be a way to achieve that. . > I know squidpy will be changing its representation and I think muon should have changes to the ATAC representation. Also muon and scvi-tools read in different things from 10x atac data. A solution to that would be versioned schemata. E.g. whatever squidpy uses now is the ""spatial schema `v1`"". When we come up with a better way it becomes the ""spatial schema `v2`"". Old schemata will be deprecated but can stick around for a while. If a schema is experimental and subject to active changes it can be `v0.1`. . ```python. scio.spatial.read_vis",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1042,integrability,version,version,1042,"d about having a single package for the reasons @adamgayoso mentioned. To address a few concerns from above: . ---. > > Who manages the sub-packages? > . > Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). Scverse core developers could take turns (e.g. every 6 months) in being ""lead maintainer"", i.e. in charge of releases and first-responders to issues (delegating them to the most appropriate people). This has the additional advantage that everything needs to be documented to a point that there can't be a single point of failure. . ---. > Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. ```. pip install scio[all]. ```. could be broadly advertised in the README. Packages could still use the slimmer version, e.g. in scirpy, I could depend on . `scio[vdj]`. . ---. > I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult. I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common `scio` package could be a way to achieve that. . > I know squidpy will be changing its representation and I think muon should have changes to the ATAC representation. Also muon and scvi-tools read in different things from 10x atac data. A solution to that would be versioned schemata. E.g. whatever squidpy uses now is the ""spatial schema `v1`"". When we come up with a better way it becomes the ""spatial schema `v2`"". Old schemata will be deprecated but can stick around for a while. If a schema is experimental and subject to active changes it can be `v0.1`. . ```python. scio.spatial.read_visium(path, schema=""v1""). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1075,integrability,depend,depend,1075,"d about having a single package for the reasons @adamgayoso mentioned. To address a few concerns from above: . ---. > > Who manages the sub-packages? > . > Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). Scverse core developers could take turns (e.g. every 6 months) in being ""lead maintainer"", i.e. in charge of releases and first-responders to issues (delegating them to the most appropriate people). This has the additional advantage that everything needs to be documented to a point that there can't be a single point of failure. . ---. > Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. ```. pip install scio[all]. ```. could be broadly advertised in the README. Packages could still use the slimmer version, e.g. in scirpy, I could depend on . `scio[vdj]`. . ---. > I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult. I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common `scio` package could be a way to achieve that. . > I know squidpy will be changing its representation and I think muon should have changes to the ATAC representation. Also muon and scvi-tools read in different things from 10x atac data. A solution to that would be versioned schemata. E.g. whatever squidpy uses now is the ""spatial schema `v1`"". When we come up with a better way it becomes the ""spatial schema `v2`"". Old schemata will be deprecated but can stick around for a while. If a schema is experimental and subject to active changes it can be `v0.1`. . ```python. scio.spatial.read_visium(path, schema=""v1""). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1671,integrability,version,versioned,1671,"d about having a single package for the reasons @adamgayoso mentioned. To address a few concerns from above: . ---. > > Who manages the sub-packages? > . > Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). Scverse core developers could take turns (e.g. every 6 months) in being ""lead maintainer"", i.e. in charge of releases and first-responders to issues (delegating them to the most appropriate people). This has the additional advantage that everything needs to be documented to a point that there can't be a single point of failure. . ---. > Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. ```. pip install scio[all]. ```. could be broadly advertised in the README. Packages could still use the slimmer version, e.g. in scirpy, I could depend on . `scio[vdj]`. . ---. > I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult. I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common `scio` package could be a way to achieve that. . > I know squidpy will be changing its representation and I think muon should have changes to the ATAC representation. Also muon and scvi-tools read in different things from 10x atac data. A solution to that would be versioned schemata. E.g. whatever squidpy uses now is the ""spatial schema `v1`"". When we come up with a better way it becomes the ""spatial schema `v2`"". Old schemata will be deprecated but can stick around for a while. If a schema is experimental and subject to active changes it can be `v0.1`. . ```python. scio.spatial.read_visium(path, schema=""v1""). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1681,integrability,schema,schemata,1681,"d about having a single package for the reasons @adamgayoso mentioned. To address a few concerns from above: . ---. > > Who manages the sub-packages? > . > Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). Scverse core developers could take turns (e.g. every 6 months) in being ""lead maintainer"", i.e. in charge of releases and first-responders to issues (delegating them to the most appropriate people). This has the additional advantage that everything needs to be documented to a point that there can't be a single point of failure. . ---. > Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. ```. pip install scio[all]. ```. could be broadly advertised in the README. Packages could still use the slimmer version, e.g. in scirpy, I could depend on . `scio[vdj]`. . ---. > I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult. I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common `scio` package could be a way to achieve that. . > I know squidpy will be changing its representation and I think muon should have changes to the ATAC representation. Also muon and scvi-tools read in different things from 10x atac data. A solution to that would be versioned schemata. E.g. whatever squidpy uses now is the ""spatial schema `v1`"". When we come up with a better way it becomes the ""spatial schema `v2`"". Old schemata will be deprecated but can stick around for a while. If a schema is experimental and subject to active changes it can be `v0.1`. . ```python. scio.spatial.read_visium(path, schema=""v1""). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1738,integrability,schema,schema,1738,"d about having a single package for the reasons @adamgayoso mentioned. To address a few concerns from above: . ---. > > Who manages the sub-packages? > . > Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). Scverse core developers could take turns (e.g. every 6 months) in being ""lead maintainer"", i.e. in charge of releases and first-responders to issues (delegating them to the most appropriate people). This has the additional advantage that everything needs to be documented to a point that there can't be a single point of failure. . ---. > Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. ```. pip install scio[all]. ```. could be broadly advertised in the README. Packages could still use the slimmer version, e.g. in scirpy, I could depend on . `scio[vdj]`. . ---. > I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult. I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common `scio` package could be a way to achieve that. . > I know squidpy will be changing its representation and I think muon should have changes to the ATAC representation. Also muon and scvi-tools read in different things from 10x atac data. A solution to that would be versioned schemata. E.g. whatever squidpy uses now is the ""spatial schema `v1`"". When we come up with a better way it becomes the ""spatial schema `v2`"". Old schemata will be deprecated but can stick around for a while. If a schema is experimental and subject to active changes it can be `v0.1`. . ```python. scio.spatial.read_visium(path, schema=""v1""). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1810,integrability,schema,schema,1810,"d about having a single package for the reasons @adamgayoso mentioned. To address a few concerns from above: . ---. > > Who manages the sub-packages? > . > Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). Scverse core developers could take turns (e.g. every 6 months) in being ""lead maintainer"", i.e. in charge of releases and first-responders to issues (delegating them to the most appropriate people). This has the additional advantage that everything needs to be documented to a point that there can't be a single point of failure. . ---. > Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. ```. pip install scio[all]. ```. could be broadly advertised in the README. Packages could still use the slimmer version, e.g. in scirpy, I could depend on . `scio[vdj]`. . ---. > I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult. I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common `scio` package could be a way to achieve that. . > I know squidpy will be changing its representation and I think muon should have changes to the ATAC representation. Also muon and scvi-tools read in different things from 10x atac data. A solution to that would be versioned schemata. E.g. whatever squidpy uses now is the ""spatial schema `v1`"". When we come up with a better way it becomes the ""spatial schema `v2`"". Old schemata will be deprecated but can stick around for a while. If a schema is experimental and subject to active changes it can be `v0.1`. . ```python. scio.spatial.read_visium(path, schema=""v1""). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1828,integrability,schema,schemata,1828,"d about having a single package for the reasons @adamgayoso mentioned. To address a few concerns from above: . ---. > > Who manages the sub-packages? > . > Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). Scverse core developers could take turns (e.g. every 6 months) in being ""lead maintainer"", i.e. in charge of releases and first-responders to issues (delegating them to the most appropriate people). This has the additional advantage that everything needs to be documented to a point that there can't be a single point of failure. . ---. > Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. ```. pip install scio[all]. ```. could be broadly advertised in the README. Packages could still use the slimmer version, e.g. in scirpy, I could depend on . `scio[vdj]`. . ---. > I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult. I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common `scio` package could be a way to achieve that. . > I know squidpy will be changing its representation and I think muon should have changes to the ATAC representation. Also muon and scvi-tools read in different things from 10x atac data. A solution to that would be versioned schemata. E.g. whatever squidpy uses now is the ""spatial schema `v1`"". When we come up with a better way it becomes the ""spatial schema `v2`"". Old schemata will be deprecated but can stick around for a while. If a schema is experimental and subject to active changes it can be `v0.1`. . ```python. scio.spatial.read_visium(path, schema=""v1""). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1895,integrability,schema,schema,1895,"d about having a single package for the reasons @adamgayoso mentioned. To address a few concerns from above: . ---. > > Who manages the sub-packages? > . > Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). Scverse core developers could take turns (e.g. every 6 months) in being ""lead maintainer"", i.e. in charge of releases and first-responders to issues (delegating them to the most appropriate people). This has the additional advantage that everything needs to be documented to a point that there can't be a single point of failure. . ---. > Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. ```. pip install scio[all]. ```. could be broadly advertised in the README. Packages could still use the slimmer version, e.g. in scirpy, I could depend on . `scio[vdj]`. . ---. > I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult. I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common `scio` package could be a way to achieve that. . > I know squidpy will be changing its representation and I think muon should have changes to the ATAC representation. Also muon and scvi-tools read in different things from 10x atac data. A solution to that would be versioned schemata. E.g. whatever squidpy uses now is the ""spatial schema `v1`"". When we come up with a better way it becomes the ""spatial schema `v2`"". Old schemata will be deprecated but can stick around for a while. If a schema is experimental and subject to active changes it can be `v0.1`. . ```python. scio.spatial.read_visium(path, schema=""v1""). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1922,integrability,sub,subject,1922,"d about having a single package for the reasons @adamgayoso mentioned. To address a few concerns from above: . ---. > > Who manages the sub-packages? > . > Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). Scverse core developers could take turns (e.g. every 6 months) in being ""lead maintainer"", i.e. in charge of releases and first-responders to issues (delegating them to the most appropriate people). This has the additional advantage that everything needs to be documented to a point that there can't be a single point of failure. . ---. > Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. ```. pip install scio[all]. ```. could be broadly advertised in the README. Packages could still use the slimmer version, e.g. in scirpy, I could depend on . `scio[vdj]`. . ---. > I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult. I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common `scio` package could be a way to achieve that. . > I know squidpy will be changing its representation and I think muon should have changes to the ATAC representation. Also muon and scvi-tools read in different things from 10x atac data. A solution to that would be versioned schemata. E.g. whatever squidpy uses now is the ""spatial schema `v1`"". When we come up with a better way it becomes the ""spatial schema `v2`"". Old schemata will be deprecated but can stick around for a while. If a schema is experimental and subject to active changes it can be `v0.1`. . ```python. scio.spatial.read_visium(path, schema=""v1""). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:2010,integrability,schema,schema,2010,"d about having a single package for the reasons @adamgayoso mentioned. To address a few concerns from above: . ---. > > Who manages the sub-packages? > . > Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). Scverse core developers could take turns (e.g. every 6 months) in being ""lead maintainer"", i.e. in charge of releases and first-responders to issues (delegating them to the most appropriate people). This has the additional advantage that everything needs to be documented to a point that there can't be a single point of failure. . ---. > Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. ```. pip install scio[all]. ```. could be broadly advertised in the README. Packages could still use the slimmer version, e.g. in scirpy, I could depend on . `scio[vdj]`. . ---. > I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult. I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common `scio` package could be a way to achieve that. . > I know squidpy will be changing its representation and I think muon should have changes to the ATAC representation. Also muon and scvi-tools read in different things from 10x atac data. A solution to that would be versioned schemata. E.g. whatever squidpy uses now is the ""spatial schema `v1`"". When we come up with a better way it becomes the ""spatial schema `v2`"". Old schemata will be deprecated but can stick around for a while. If a schema is experimental and subject to active changes it can be `v0.1`. . ```python. scio.spatial.read_visium(path, schema=""v1""). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1127,interoperability,format,formats,1127,"d about having a single package for the reasons @adamgayoso mentioned. To address a few concerns from above: . ---. > > Who manages the sub-packages? > . > Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). Scverse core developers could take turns (e.g. every 6 months) in being ""lead maintainer"", i.e. in charge of releases and first-responders to issues (delegating them to the most appropriate people). This has the additional advantage that everything needs to be documented to a point that there can't be a single point of failure. . ---. > Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. ```. pip install scio[all]. ```. could be broadly advertised in the README. Packages could still use the slimmer version, e.g. in scirpy, I could depend on . `scio[vdj]`. . ---. > I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult. I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common `scio` package could be a way to achieve that. . > I know squidpy will be changing its representation and I think muon should have changes to the ATAC representation. Also muon and scvi-tools read in different things from 10x atac data. A solution to that would be versioned schemata. E.g. whatever squidpy uses now is the ""spatial schema `v1`"". When we come up with a better way it becomes the ""spatial schema `v2`"". Old schemata will be deprecated but can stick around for a while. If a schema is experimental and subject to active changes it can be `v0.1`. . ```python. scio.spatial.read_visium(path, schema=""v1""). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:51,modifiability,pac,package,51,"I am more and more convinced about having a single package for the reasons @adamgayoso mentioned. To address a few concerns from above: . ---. > > Who manages the sub-packages? > . > Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). Scverse core developers could take turns (e.g. every 6 months) in being ""lead maintainer"", i.e. in charge of releases and first-responders to issues (delegating them to the most appropriate people). This has the additional advantage that everything needs to be documented to a point that there can't be a single point of failure. . ---. > Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. ```. pip install scio[all]. ```. could be broadly advertised in the README. Packages could still use the slimmer version, e.g. in scirpy, I could depend on . `scio[vdj]`. . ---. > I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult. I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common `scio` package could be a way to achieve that. . > I know squidpy will be changing its representation and I think muon should have changes to the ATAC representation. Also muon and scvi-tools read in different things from 10x atac data. A solution to that would be versioned schemata. E.g. whatever squidpy uses now is the ""spatial schema `v1`"". When we come up with a better way it becomes the ""spatial schema `v2`"". Old schemata will be deprecated but can stick around for a while. If a schema is experimental and subject to active changes it can be `v0.1`. . ```python. scio.spatial.read_vis",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:115,modifiability,concern,concerns,115,"I am more and more convinced about having a single package for the reasons @adamgayoso mentioned. To address a few concerns from above: . ---. > > Who manages the sub-packages? > . > Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). Scverse core developers could take turns (e.g. every 6 months) in being ""lead maintainer"", i.e. in charge of releases and first-responders to issues (delegating them to the most appropriate people). This has the additional advantage that everything needs to be documented to a point that there can't be a single point of failure. . ---. > Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. ```. pip install scio[all]. ```. could be broadly advertised in the README. Packages could still use the slimmer version, e.g. in scirpy, I could depend on . `scio[vdj]`. . ---. > I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult. I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common `scio` package could be a way to achieve that. . > I know squidpy will be changing its representation and I think muon should have changes to the ATAC representation. Also muon and scvi-tools read in different things from 10x atac data. A solution to that would be versioned schemata. E.g. whatever squidpy uses now is the ""spatial schema `v1`"". When we come up with a better way it becomes the ""spatial schema `v2`"". Old schemata will be deprecated but can stick around for a while. If a schema is experimental and subject to active changes it can be `v0.1`. . ```python. scio.spatial.read_vis",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:167,modifiability,pac,packages,167,"I am more and more convinced about having a single package for the reasons @adamgayoso mentioned. To address a few concerns from above: . ---. > > Who manages the sub-packages? > . > Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). Scverse core developers could take turns (e.g. every 6 months) in being ""lead maintainer"", i.e. in charge of releases and first-responders to issues (delegating them to the most appropriate people). This has the additional advantage that everything needs to be documented to a point that there can't be a single point of failure. . ---. > Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. ```. pip install scio[all]. ```. could be broadly advertised in the README. Packages could still use the slimmer version, e.g. in scirpy, I could depend on . `scio[vdj]`. . ---. > I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult. I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common `scio` package could be a way to achieve that. . > I know squidpy will be changing its representation and I think muon should have changes to the ATAC representation. Also muon and scvi-tools read in different things from 10x atac data. A solution to that would be versioned schemata. E.g. whatever squidpy uses now is the ""spatial schema `v1`"". When we come up with a better way it becomes the ""spatial schema `v2`"". Old schemata will be deprecated but can stick around for a while. If a schema is experimental and subject to active changes it can be `v0.1`. . ```python. scio.spatial.read_vis",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:206,modifiability,pac,package,206,"I am more and more convinced about having a single package for the reasons @adamgayoso mentioned. To address a few concerns from above: . ---. > > Who manages the sub-packages? > . > Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). Scverse core developers could take turns (e.g. every 6 months) in being ""lead maintainer"", i.e. in charge of releases and first-responders to issues (delegating them to the most appropriate people). This has the additional advantage that everything needs to be documented to a point that there can't be a single point of failure. . ---. > Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. ```. pip install scio[all]. ```. could be broadly advertised in the README. Packages could still use the slimmer version, e.g. in scirpy, I could depend on . `scio[vdj]`. . ---. > I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult. I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common `scio` package could be a way to achieve that. . > I know squidpy will be changing its representation and I think muon should have changes to the ATAC representation. Also muon and scvi-tools read in different things from 10x atac data. A solution to that would be versioned schemata. E.g. whatever squidpy uses now is the ""spatial schema `v1`"". When we come up with a better way it becomes the ""spatial schema `v2`"". Old schemata will be deprecated but can stick around for a while. If a schema is experimental and subject to active changes it can be `v0.1`. . ```python. scio.spatial.read_vis",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:351,modifiability,pac,package,351,"I am more and more convinced about having a single package for the reasons @adamgayoso mentioned. To address a few concerns from above: . ---. > > Who manages the sub-packages? > . > Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). Scverse core developers could take turns (e.g. every 6 months) in being ""lead maintainer"", i.e. in charge of releases and first-responders to issues (delegating them to the most appropriate people). This has the additional advantage that everything needs to be documented to a point that there can't be a single point of failure. . ---. > Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. ```. pip install scio[all]. ```. could be broadly advertised in the README. Packages could still use the slimmer version, e.g. in scirpy, I could depend on . `scio[vdj]`. . ---. > I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult. I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common `scio` package could be a way to achieve that. . > I know squidpy will be changing its representation and I think muon should have changes to the ATAC representation. Also muon and scvi-tools read in different things from 10x atac data. A solution to that would be versioned schemata. E.g. whatever squidpy uses now is the ""spatial schema `v1`"". When we come up with a better way it becomes the ""spatial schema `v2`"". Old schemata will be deprecated but can stick around for a while. If a schema is experimental and subject to active changes it can be `v0.1`. . ```python. scio.spatial.read_vis",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:362,modifiability,maintain,maintain,362,"I am more and more convinced about having a single package for the reasons @adamgayoso mentioned. To address a few concerns from above: . ---. > > Who manages the sub-packages? > . > Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). Scverse core developers could take turns (e.g. every 6 months) in being ""lead maintainer"", i.e. in charge of releases and first-responders to issues (delegating them to the most appropriate people). This has the additional advantage that everything needs to be documented to a point that there can't be a single point of failure. . ---. > Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. ```. pip install scio[all]. ```. could be broadly advertised in the README. Packages could still use the slimmer version, e.g. in scirpy, I could depend on . `scio[vdj]`. . ---. > I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult. I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common `scio` package could be a way to achieve that. . > I know squidpy will be changing its representation and I think muon should have changes to the ATAC representation. Also muon and scvi-tools read in different things from 10x atac data. A solution to that would be versioned schemata. E.g. whatever squidpy uses now is the ""spatial schema `v1`"". When we come up with a better way it becomes the ""spatial schema `v2`"". Old schemata will be deprecated but can stick around for a while. If a schema is experimental and subject to active changes it can be `v0.1`. . ```python. scio.spatial.read_vis",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:535,modifiability,maintain,maintainer,535,"I am more and more convinced about having a single package for the reasons @adamgayoso mentioned. To address a few concerns from above: . ---. > > Who manages the sub-packages? > . > Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). Scverse core developers could take turns (e.g. every 6 months) in being ""lead maintainer"", i.e. in charge of releases and first-responders to issues (delegating them to the most appropriate people). This has the additional advantage that everything needs to be documented to a point that there can't be a single point of failure. . ---. > Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. ```. pip install scio[all]. ```. could be broadly advertised in the README. Packages could still use the slimmer version, e.g. in scirpy, I could depend on . `scio[vdj]`. . ---. > I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult. I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common `scio` package could be a way to achieve that. . > I know squidpy will be changing its representation and I think muon should have changes to the ATAC representation. Also muon and scvi-tools read in different things from 10x atac data. A solution to that would be versioned schemata. E.g. whatever squidpy uses now is the ""spatial schema `v1`"". When we come up with a better way it becomes the ""spatial schema `v2`"". Old schemata will be deprecated but can stick around for a while. If a schema is experimental and subject to active changes it can be `v0.1`. . ```python. scio.spatial.read_vis",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:830,modifiability,pac,package,830,"I am more and more convinced about having a single package for the reasons @adamgayoso mentioned. To address a few concerns from above: . ---. > > Who manages the sub-packages? > . > Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). Scverse core developers could take turns (e.g. every 6 months) in being ""lead maintainer"", i.e. in charge of releases and first-responders to issues (delegating them to the most appropriate people). This has the additional advantage that everything needs to be documented to a point that there can't be a single point of failure. . ---. > Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. ```. pip install scio[all]. ```. could be broadly advertised in the README. Packages could still use the slimmer version, e.g. in scirpy, I could depend on . `scio[vdj]`. . ---. > I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult. I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common `scio` package could be a way to achieve that. . > I know squidpy will be changing its representation and I think muon should have changes to the ATAC representation. Also muon and scvi-tools read in different things from 10x atac data. A solution to that would be versioned schemata. E.g. whatever squidpy uses now is the ""spatial schema `v1`"". When we come up with a better way it becomes the ""spatial schema `v2`"". Old schemata will be deprecated but can stick around for a while. If a schema is experimental and subject to active changes it can be `v0.1`. . ```python. scio.spatial.read_vis",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:915,modifiability,depend,dependencies,915,"I am more and more convinced about having a single package for the reasons @adamgayoso mentioned. To address a few concerns from above: . ---. > > Who manages the sub-packages? > . > Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). Scverse core developers could take turns (e.g. every 6 months) in being ""lead maintainer"", i.e. in charge of releases and first-responders to issues (delegating them to the most appropriate people). This has the additional advantage that everything needs to be documented to a point that there can't be a single point of failure. . ---. > Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. ```. pip install scio[all]. ```. could be broadly advertised in the README. Packages could still use the slimmer version, e.g. in scirpy, I could depend on . `scio[vdj]`. . ---. > I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult. I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common `scio` package could be a way to achieve that. . > I know squidpy will be changing its representation and I think muon should have changes to the ATAC representation. Also muon and scvi-tools read in different things from 10x atac data. A solution to that would be versioned schemata. E.g. whatever squidpy uses now is the ""spatial schema `v1`"". When we come up with a better way it becomes the ""spatial schema `v2`"". Old schemata will be deprecated but can stick around for a while. If a schema is experimental and subject to active changes it can be `v0.1`. . ```python. scio.spatial.read_vis",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1005,modifiability,Pac,Packages,1005," and more convinced about having a single package for the reasons @adamgayoso mentioned. To address a few concerns from above: . ---. > > Who manages the sub-packages? > . > Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). Scverse core developers could take turns (e.g. every 6 months) in being ""lead maintainer"", i.e. in charge of releases and first-responders to issues (delegating them to the most appropriate people). This has the additional advantage that everything needs to be documented to a point that there can't be a single point of failure. . ---. > Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. ```. pip install scio[all]. ```. could be broadly advertised in the README. Packages could still use the slimmer version, e.g. in scirpy, I could depend on . `scio[vdj]`. . ---. > I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult. I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common `scio` package could be a way to achieve that. . > I know squidpy will be changing its representation and I think muon should have changes to the ATAC representation. Also muon and scvi-tools read in different things from 10x atac data. A solution to that would be versioned schemata. E.g. whatever squidpy uses now is the ""spatial schema `v1`"". When we come up with a better way it becomes the ""spatial schema `v2`"". Old schemata will be deprecated but can stick around for a while. If a schema is experimental and subject to active changes it can be `v0.1`. . ```python. scio.spatial.read_visium(path,",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1042,modifiability,version,version,1042,"d about having a single package for the reasons @adamgayoso mentioned. To address a few concerns from above: . ---. > > Who manages the sub-packages? > . > Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). Scverse core developers could take turns (e.g. every 6 months) in being ""lead maintainer"", i.e. in charge of releases and first-responders to issues (delegating them to the most appropriate people). This has the additional advantage that everything needs to be documented to a point that there can't be a single point of failure. . ---. > Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. ```. pip install scio[all]. ```. could be broadly advertised in the README. Packages could still use the slimmer version, e.g. in scirpy, I could depend on . `scio[vdj]`. . ---. > I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult. I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common `scio` package could be a way to achieve that. . > I know squidpy will be changing its representation and I think muon should have changes to the ATAC representation. Also muon and scvi-tools read in different things from 10x atac data. A solution to that would be versioned schemata. E.g. whatever squidpy uses now is the ""spatial schema `v1`"". When we come up with a better way it becomes the ""spatial schema `v2`"". Old schemata will be deprecated but can stick around for a while. If a schema is experimental and subject to active changes it can be `v0.1`. . ```python. scio.spatial.read_visium(path, schema=""v1""). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1075,modifiability,depend,depend,1075,"d about having a single package for the reasons @adamgayoso mentioned. To address a few concerns from above: . ---. > > Who manages the sub-packages? > . > Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). Scverse core developers could take turns (e.g. every 6 months) in being ""lead maintainer"", i.e. in charge of releases and first-responders to issues (delegating them to the most appropriate people). This has the additional advantage that everything needs to be documented to a point that there can't be a single point of failure. . ---. > Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. ```. pip install scio[all]. ```. could be broadly advertised in the README. Packages could still use the slimmer version, e.g. in scirpy, I could depend on . `scio[vdj]`. . ---. > I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult. I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common `scio` package could be a way to achieve that. . > I know squidpy will be changing its representation and I think muon should have changes to the ATAC representation. Also muon and scvi-tools read in different things from 10x atac data. A solution to that would be versioned schemata. E.g. whatever squidpy uses now is the ""spatial schema `v1`"". When we come up with a better way it becomes the ""spatial schema `v2`"". Old schemata will be deprecated but can stick around for a while. If a schema is experimental and subject to active changes it can be `v0.1`. . ```python. scio.spatial.read_visium(path, schema=""v1""). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1413,modifiability,pac,package,1413,"d about having a single package for the reasons @adamgayoso mentioned. To address a few concerns from above: . ---. > > Who manages the sub-packages? > . > Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). Scverse core developers could take turns (e.g. every 6 months) in being ""lead maintainer"", i.e. in charge of releases and first-responders to issues (delegating them to the most appropriate people). This has the additional advantage that everything needs to be documented to a point that there can't be a single point of failure. . ---. > Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. ```. pip install scio[all]. ```. could be broadly advertised in the README. Packages could still use the slimmer version, e.g. in scirpy, I could depend on . `scio[vdj]`. . ---. > I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult. I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common `scio` package could be a way to achieve that. . > I know squidpy will be changing its representation and I think muon should have changes to the ATAC representation. Also muon and scvi-tools read in different things from 10x atac data. A solution to that would be versioned schemata. E.g. whatever squidpy uses now is the ""spatial schema `v1`"". When we come up with a better way it becomes the ""spatial schema `v2`"". Old schemata will be deprecated but can stick around for a while. If a schema is experimental and subject to active changes it can be `v0.1`. . ```python. scio.spatial.read_visium(path, schema=""v1""). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1671,modifiability,version,versioned,1671,"d about having a single package for the reasons @adamgayoso mentioned. To address a few concerns from above: . ---. > > Who manages the sub-packages? > . > Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). Scverse core developers could take turns (e.g. every 6 months) in being ""lead maintainer"", i.e. in charge of releases and first-responders to issues (delegating them to the most appropriate people). This has the additional advantage that everything needs to be documented to a point that there can't be a single point of failure. . ---. > Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. ```. pip install scio[all]. ```. could be broadly advertised in the README. Packages could still use the slimmer version, e.g. in scirpy, I could depend on . `scio[vdj]`. . ---. > I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult. I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common `scio` package could be a way to achieve that. . > I know squidpy will be changing its representation and I think muon should have changes to the ATAC representation. Also muon and scvi-tools read in different things from 10x atac data. A solution to that would be versioned schemata. E.g. whatever squidpy uses now is the ""spatial schema `v1`"". When we come up with a better way it becomes the ""spatial schema `v2`"". Old schemata will be deprecated but can stick around for a while. If a schema is experimental and subject to active changes it can be `v0.1`. . ```python. scio.spatial.read_visium(path, schema=""v1""). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:295,performance,time,times,295,"I am more and more convinced about having a single package for the reasons @adamgayoso mentioned. To address a few concerns from above: . ---. > > Who manages the sub-packages? > . > Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). Scverse core developers could take turns (e.g. every 6 months) in being ""lead maintainer"", i.e. in charge of releases and first-responders to issues (delegating them to the most appropriate people). This has the additional advantage that everything needs to be documented to a point that there can't be a single point of failure. . ---. > Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. ```. pip install scio[all]. ```. could be broadly advertised in the README. Packages could still use the slimmer version, e.g. in scirpy, I could depend on . `scio[vdj]`. . ---. > I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult. I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common `scio` package could be a way to achieve that. . > I know squidpy will be changing its representation and I think muon should have changes to the ATAC representation. Also muon and scvi-tools read in different things from 10x atac data. A solution to that would be versioned schemata. E.g. whatever squidpy uses now is the ""spatial schema `v1`"". When we come up with a better way it becomes the ""spatial schema `v2`"". Old schemata will be deprecated but can stick around for a while. If a schema is experimental and subject to active changes it can be `v0.1`. . ```python. scio.spatial.read_vis",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:778,performance,failur,failure,778,"I am more and more convinced about having a single package for the reasons @adamgayoso mentioned. To address a few concerns from above: . ---. > > Who manages the sub-packages? > . > Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). Scverse core developers could take turns (e.g. every 6 months) in being ""lead maintainer"", i.e. in charge of releases and first-responders to issues (delegating them to the most appropriate people). This has the additional advantage that everything needs to be documented to a point that there can't be a single point of failure. . ---. > Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. ```. pip install scio[all]. ```. could be broadly advertised in the README. Packages could still use the slimmer version, e.g. in scirpy, I could depend on . `scio[vdj]`. . ---. > I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult. I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common `scio` package could be a way to achieve that. . > I know squidpy will be changing its representation and I think muon should have changes to the ATAC representation. Also muon and scvi-tools read in different things from 10x atac data. A solution to that would be versioned schemata. E.g. whatever squidpy uses now is the ""spatial schema `v1`"". When we come up with a better way it becomes the ""spatial schema `v2`"". Old schemata will be deprecated but can stick around for a while. If a schema is experimental and subject to active changes it can be `v0.1`. . ```python. scio.spatial.read_vis",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:778,reliability,fail,failure,778,"I am more and more convinced about having a single package for the reasons @adamgayoso mentioned. To address a few concerns from above: . ---. > > Who manages the sub-packages? > . > Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). Scverse core developers could take turns (e.g. every 6 months) in being ""lead maintainer"", i.e. in charge of releases and first-responders to issues (delegating them to the most appropriate people). This has the additional advantage that everything needs to be documented to a point that there can't be a single point of failure. . ---. > Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. ```. pip install scio[all]. ```. could be broadly advertised in the README. Packages could still use the slimmer version, e.g. in scirpy, I could depend on . `scio[vdj]`. . ---. > I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult. I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common `scio` package could be a way to achieve that. . > I know squidpy will be changing its representation and I think muon should have changes to the ATAC representation. Also muon and scvi-tools read in different things from 10x atac data. A solution to that would be versioned schemata. E.g. whatever squidpy uses now is the ""spatial schema `v1`"". When we come up with a better way it becomes the ""spatial schema `v2`"". Old schemata will be deprecated but can stick around for a while. If a schema is experimental and subject to active changes it can be `v0.1`. . ```python. scio.spatial.read_vis",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1034,reliability,sli,slimmer,1034,"d about having a single package for the reasons @adamgayoso mentioned. To address a few concerns from above: . ---. > > Who manages the sub-packages? > . > Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). Scverse core developers could take turns (e.g. every 6 months) in being ""lead maintainer"", i.e. in charge of releases and first-responders to issues (delegating them to the most appropriate people). This has the additional advantage that everything needs to be documented to a point that there can't be a single point of failure. . ---. > Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. ```. pip install scio[all]. ```. could be broadly advertised in the README. Packages could still use the slimmer version, e.g. in scirpy, I could depend on . `scio[vdj]`. . ---. > I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult. I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common `scio` package could be a way to achieve that. . > I know squidpy will be changing its representation and I think muon should have changes to the ATAC representation. Also muon and scvi-tools read in different things from 10x atac data. A solution to that would be versioned schemata. E.g. whatever squidpy uses now is the ""spatial schema `v1`"". When we come up with a better way it becomes the ""spatial schema `v2`"". Old schemata will be deprecated but can stick around for a while. If a schema is experimental and subject to active changes it can be `v0.1`. . ```python. scio.spatial.read_visium(path, schema=""v1""). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:151,safety,manag,manages,151,"I am more and more convinced about having a single package for the reasons @adamgayoso mentioned. To address a few concerns from above: . ---. > > Who manages the sub-packages? > . > Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). Scverse core developers could take turns (e.g. every 6 months) in being ""lead maintainer"", i.e. in charge of releases and first-responders to issues (delegating them to the most appropriate people). This has the additional advantage that everything needs to be documented to a point that there can't be a single point of failure. . ---. > Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. ```. pip install scio[all]. ```. could be broadly advertised in the README. Packages could still use the slimmer version, e.g. in scirpy, I could depend on . `scio[vdj]`. . ---. > I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult. I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common `scio` package could be a way to achieve that. . > I know squidpy will be changing its representation and I think muon should have changes to the ATAC representation. Also muon and scvi-tools read in different things from 10x atac data. A solution to that would be versioned schemata. E.g. whatever squidpy uses now is the ""spatial schema `v1`"". When we come up with a better way it becomes the ""spatial schema `v2`"". Old schemata will be deprecated but can stick around for a while. If a schema is experimental and subject to active changes it can be `v0.1`. . ```python. scio.spatial.read_vis",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:339,safety,compl,complicated,339,"I am more and more convinced about having a single package for the reasons @adamgayoso mentioned. To address a few concerns from above: . ---. > > Who manages the sub-packages? > . > Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). Scverse core developers could take turns (e.g. every 6 months) in being ""lead maintainer"", i.e. in charge of releases and first-responders to issues (delegating them to the most appropriate people). This has the additional advantage that everything needs to be documented to a point that there can't be a single point of failure. . ---. > Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. ```. pip install scio[all]. ```. could be broadly advertised in the README. Packages could still use the slimmer version, e.g. in scirpy, I could depend on . `scio[vdj]`. . ---. > I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult. I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common `scio` package could be a way to achieve that. . > I know squidpy will be changing its representation and I think muon should have changes to the ATAC representation. Also muon and scvi-tools read in different things from 10x atac data. A solution to that would be versioned schemata. E.g. whatever squidpy uses now is the ""spatial schema `v1`"". When we come up with a better way it becomes the ""spatial schema `v2`"". Old schemata will be deprecated but can stick around for a while. If a schema is experimental and subject to active changes it can be `v0.1`. . ```python. scio.spatial.read_vis",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:362,safety,maintain,maintain,362,"I am more and more convinced about having a single package for the reasons @adamgayoso mentioned. To address a few concerns from above: . ---. > > Who manages the sub-packages? > . > Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). Scverse core developers could take turns (e.g. every 6 months) in being ""lead maintainer"", i.e. in charge of releases and first-responders to issues (delegating them to the most appropriate people). This has the additional advantage that everything needs to be documented to a point that there can't be a single point of failure. . ---. > Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. ```. pip install scio[all]. ```. could be broadly advertised in the README. Packages could still use the slimmer version, e.g. in scirpy, I could depend on . `scio[vdj]`. . ---. > I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult. I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common `scio` package could be a way to achieve that. . > I know squidpy will be changing its representation and I think muon should have changes to the ATAC representation. Also muon and scvi-tools read in different things from 10x atac data. A solution to that would be versioned schemata. E.g. whatever squidpy uses now is the ""spatial schema `v1`"". When we come up with a better way it becomes the ""spatial schema `v2`"". Old schemata will be deprecated but can stick around for a while. If a schema is experimental and subject to active changes it can be `v0.1`. . ```python. scio.spatial.read_vis",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:535,safety,maintain,maintainer,535,"I am more and more convinced about having a single package for the reasons @adamgayoso mentioned. To address a few concerns from above: . ---. > > Who manages the sub-packages? > . > Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). Scverse core developers could take turns (e.g. every 6 months) in being ""lead maintainer"", i.e. in charge of releases and first-responders to issues (delegating them to the most appropriate people). This has the additional advantage that everything needs to be documented to a point that there can't be a single point of failure. . ---. > Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. ```. pip install scio[all]. ```. could be broadly advertised in the README. Packages could still use the slimmer version, e.g. in scirpy, I could depend on . `scio[vdj]`. . ---. > I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult. I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common `scio` package could be a way to achieve that. . > I know squidpy will be changing its representation and I think muon should have changes to the ATAC representation. Also muon and scvi-tools read in different things from 10x atac data. A solution to that would be versioned schemata. E.g. whatever squidpy uses now is the ""spatial schema `v1`"". When we come up with a better way it becomes the ""spatial schema `v2`"". Old schemata will be deprecated but can stick around for a while. If a schema is experimental and subject to active changes it can be `v0.1`. . ```python. scio.spatial.read_vis",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:915,safety,depend,dependencies,915,"I am more and more convinced about having a single package for the reasons @adamgayoso mentioned. To address a few concerns from above: . ---. > > Who manages the sub-packages? > . > Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). Scverse core developers could take turns (e.g. every 6 months) in being ""lead maintainer"", i.e. in charge of releases and first-responders to issues (delegating them to the most appropriate people). This has the additional advantage that everything needs to be documented to a point that there can't be a single point of failure. . ---. > Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. ```. pip install scio[all]. ```. could be broadly advertised in the README. Packages could still use the slimmer version, e.g. in scirpy, I could depend on . `scio[vdj]`. . ---. > I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult. I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common `scio` package could be a way to achieve that. . > I know squidpy will be changing its representation and I think muon should have changes to the ATAC representation. Also muon and scvi-tools read in different things from 10x atac data. A solution to that would be versioned schemata. E.g. whatever squidpy uses now is the ""spatial schema `v1`"". When we come up with a better way it becomes the ""spatial schema `v2`"". Old schemata will be deprecated but can stick around for a while. If a schema is experimental and subject to active changes it can be `v0.1`. . ```python. scio.spatial.read_vis",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1075,safety,depend,depend,1075,"d about having a single package for the reasons @adamgayoso mentioned. To address a few concerns from above: . ---. > > Who manages the sub-packages? > . > Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). Scverse core developers could take turns (e.g. every 6 months) in being ""lead maintainer"", i.e. in charge of releases and first-responders to issues (delegating them to the most appropriate people). This has the additional advantage that everything needs to be documented to a point that there can't be a single point of failure. . ---. > Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. ```. pip install scio[all]. ```. could be broadly advertised in the README. Packages could still use the slimmer version, e.g. in scirpy, I could depend on . `scio[vdj]`. . ---. > I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult. I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common `scio` package could be a way to achieve that. . > I know squidpy will be changing its representation and I think muon should have changes to the ATAC representation. Also muon and scvi-tools read in different things from 10x atac data. A solution to that would be versioned schemata. E.g. whatever squidpy uses now is the ""spatial schema `v1`"". When we come up with a better way it becomes the ""spatial schema `v2`"". Old schemata will be deprecated but can stick around for a while. If a schema is experimental and subject to active changes it can be `v0.1`. . ```python. scio.spatial.read_visium(path, schema=""v1""). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:339,security,compl,complicated,339,"I am more and more convinced about having a single package for the reasons @adamgayoso mentioned. To address a few concerns from above: . ---. > > Who manages the sub-packages? > . > Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). Scverse core developers could take turns (e.g. every 6 months) in being ""lead maintainer"", i.e. in charge of releases and first-responders to issues (delegating them to the most appropriate people). This has the additional advantage that everything needs to be documented to a point that there can't be a single point of failure. . ---. > Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. ```. pip install scio[all]. ```. could be broadly advertised in the README. Packages could still use the slimmer version, e.g. in scirpy, I could depend on . `scio[vdj]`. . ---. > I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult. I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common `scio` package could be a way to achieve that. . > I know squidpy will be changing its representation and I think muon should have changes to the ATAC representation. Also muon and scvi-tools read in different things from 10x atac data. A solution to that would be versioned schemata. E.g. whatever squidpy uses now is the ""spatial schema `v1`"". When we come up with a better way it becomes the ""spatial schema `v2`"". Old schemata will be deprecated but can stick around for a while. If a schema is experimental and subject to active changes it can be `v0.1`. . ```python. scio.spatial.read_vis",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:115,testability,concern,concerns,115,"I am more and more convinced about having a single package for the reasons @adamgayoso mentioned. To address a few concerns from above: . ---. > > Who manages the sub-packages? > . > Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). Scverse core developers could take turns (e.g. every 6 months) in being ""lead maintainer"", i.e. in charge of releases and first-responders to issues (delegating them to the most appropriate people). This has the additional advantage that everything needs to be documented to a point that there can't be a single point of failure. . ---. > Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. ```. pip install scio[all]. ```. could be broadly advertised in the README. Packages could still use the slimmer version, e.g. in scirpy, I could depend on . `scio[vdj]`. . ---. > I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult. I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common `scio` package could be a way to achieve that. . > I know squidpy will be changing its representation and I think muon should have changes to the ATAC representation. Also muon and scvi-tools read in different things from 10x atac data. A solution to that would be versioned schemata. E.g. whatever squidpy uses now is the ""spatial schema `v1`"". When we come up with a better way it becomes the ""spatial schema `v2`"". Old schemata will be deprecated but can stick around for a while. If a schema is experimental and subject to active changes it can be `v0.1`. . ```python. scio.spatial.read_vis",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:915,testability,depend,dependencies,915,"I am more and more convinced about having a single package for the reasons @adamgayoso mentioned. To address a few concerns from above: . ---. > > Who manages the sub-packages? > . > Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). Scverse core developers could take turns (e.g. every 6 months) in being ""lead maintainer"", i.e. in charge of releases and first-responders to issues (delegating them to the most appropriate people). This has the additional advantage that everything needs to be documented to a point that there can't be a single point of failure. . ---. > Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. ```. pip install scio[all]. ```. could be broadly advertised in the README. Packages could still use the slimmer version, e.g. in scirpy, I could depend on . `scio[vdj]`. . ---. > I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult. I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common `scio` package could be a way to achieve that. . > I know squidpy will be changing its representation and I think muon should have changes to the ATAC representation. Also muon and scvi-tools read in different things from 10x atac data. A solution to that would be versioned schemata. E.g. whatever squidpy uses now is the ""spatial schema `v1`"". When we come up with a better way it becomes the ""spatial schema `v2`"". Old schemata will be deprecated but can stick around for a while. If a schema is experimental and subject to active changes it can be `v0.1`. . ```python. scio.spatial.read_vis",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1075,testability,depend,depend,1075,"d about having a single package for the reasons @adamgayoso mentioned. To address a few concerns from above: . ---. > > Who manages the sub-packages? > . > Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). Scverse core developers could take turns (e.g. every 6 months) in being ""lead maintainer"", i.e. in charge of releases and first-responders to issues (delegating them to the most appropriate people). This has the additional advantage that everything needs to be documented to a point that there can't be a single point of failure. . ---. > Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. ```. pip install scio[all]. ```. could be broadly advertised in the README. Packages could still use the slimmer version, e.g. in scirpy, I could depend on . `scio[vdj]`. . ---. > I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult. I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common `scio` package could be a way to achieve that. . > I know squidpy will be changing its representation and I think muon should have changes to the ATAC representation. Also muon and scvi-tools read in different things from 10x atac data. A solution to that would be versioned schemata. E.g. whatever squidpy uses now is the ""spatial schema `v1`"". When we come up with a better way it becomes the ""spatial schema `v2`"". Old schemata will be deprecated but can stick around for a while. If a schema is experimental and subject to active changes it can be `v0.1`. . ```python. scio.spatial.read_visium(path, schema=""v1""). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:387,usability,person,person,387,"I am more and more convinced about having a single package for the reasons @adamgayoso mentioned. To address a few concerns from above: . ---. > > Who manages the sub-packages? > . > Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). Scverse core developers could take turns (e.g. every 6 months) in being ""lead maintainer"", i.e. in charge of releases and first-responders to issues (delegating them to the most appropriate people). This has the additional advantage that everything needs to be documented to a point that there can't be a single point of failure. . ---. > Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. ```. pip install scio[all]. ```. could be broadly advertised in the README. Packages could still use the slimmer version, e.g. in scirpy, I could depend on . `scio[vdj]`. . ---. > I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult. I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common `scio` package could be a way to achieve that. . > I know squidpy will be changing its representation and I think muon should have changes to the ATAC representation. Also muon and scvi-tools read in different things from 10x atac data. A solution to that would be versioned schemata. E.g. whatever squidpy uses now is the ""spatial schema `v1`"". When we come up with a better way it becomes the ""spatial schema `v2`"". Old schemata will be deprecated but can stick around for a while. If a schema is experimental and subject to active changes it can be `v0.1`. . ```python. scio.spatial.read_vis",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:718,usability,document,documented,718,"I am more and more convinced about having a single package for the reasons @adamgayoso mentioned. To address a few concerns from above: . ---. > > Who manages the sub-packages? > . > Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). Scverse core developers could take turns (e.g. every 6 months) in being ""lead maintainer"", i.e. in charge of releases and first-responders to issues (delegating them to the most appropriate people). This has the additional advantage that everything needs to be documented to a point that there can't be a single point of failure. . ---. > Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. ```. pip install scio[all]. ```. could be broadly advertised in the README. Packages could still use the slimmer version, e.g. in scirpy, I could depend on . `scio[vdj]`. . ---. > I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult. I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common `scio` package could be a way to achieve that. . > I know squidpy will be changing its representation and I think muon should have changes to the ATAC representation. Also muon and scvi-tools read in different things from 10x atac data. A solution to that would be versioned schemata. E.g. whatever squidpy uses now is the ""spatial schema `v1`"". When we come up with a better way it becomes the ""spatial schema `v2`"". Old schemata will be deprecated but can stick around for a while. If a schema is experimental and subject to active changes it can be `v0.1`. . ```python. scio.spatial.read_vis",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:1592,usability,tool,tools,1592,"d about having a single package for the reasons @adamgayoso mentioned. To address a few concerns from above: . ---. > > Who manages the sub-packages? > . > Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). Scverse core developers could take turns (e.g. every 6 months) in being ""lead maintainer"", i.e. in charge of releases and first-responders to issues (delegating them to the most appropriate people). This has the additional advantage that everything needs to be documented to a point that there can't be a single point of failure. . ---. > Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. ```. pip install scio[all]. ```. could be broadly advertised in the README. Packages could still use the slimmer version, e.g. in scirpy, I could depend on . `scio[vdj]`. . ---. > I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult. I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common `scio` package could be a way to achieve that. . > I know squidpy will be changing its representation and I think muon should have changes to the ATAC representation. Also muon and scvi-tools read in different things from 10x atac data. A solution to that would be versioned schemata. E.g. whatever squidpy uses now is the ""spatial schema `v1`"". When we come up with a better way it becomes the ""spatial schema `v2`"". Old schemata will be deprecated but can stick around for a while. If a schema is experimental and subject to active changes it can be `v0.1`. . ```python. scio.spatial.read_visium(path, schema=""v1""). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:442,deployability,version,versioned,442,"> I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common scio package could be a way to achieve that. Agree, also R packages seem to be doing just fine with Seurat/Bioconductor representation? > Also muon and scvi-tools read in different things from 10x atac data. This is not intentional at all, muon read atac data would work just fine in our package. > A solution to that would be versioned schemata. That could be good, the schema also don't have to be versioned, we can just have a few options and package devs wrap the method with their choice.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:515,deployability,version,versioned,515,"> I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common scio package could be a way to achieve that. Agree, also R packages seem to be doing just fine with Seurat/Bioconductor representation? > Also muon and scvi-tools read in different things from 10x atac data. This is not intentional at all, muon read atac data would work just fine in our package. > A solution to that would be versioned schemata. That could be good, the schema also don't have to be versioned, we can just have a few options and package devs wrap the method with their choice.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:442,integrability,version,versioned,442,"> I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common scio package could be a way to achieve that. Agree, also R packages seem to be doing just fine with Seurat/Bioconductor representation? > Also muon and scvi-tools read in different things from 10x atac data. This is not intentional at all, muon read atac data would work just fine in our package. > A solution to that would be versioned schemata. That could be good, the schema also don't have to be versioned, we can just have a few options and package devs wrap the method with their choice.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:452,integrability,schema,schemata,452,"> I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common scio package could be a way to achieve that. Agree, also R packages seem to be doing just fine with Seurat/Bioconductor representation? > Also muon and scvi-tools read in different things from 10x atac data. This is not intentional at all, muon read atac data would work just fine in our package. > A solution to that would be versioned schemata. That could be good, the schema also don't have to be versioned, we can just have a few options and package devs wrap the method with their choice.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:486,integrability,schema,schema,486,"> I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common scio package could be a way to achieve that. Agree, also R packages seem to be doing just fine with Seurat/Bioconductor representation? > Also muon and scvi-tools read in different things from 10x atac data. This is not intentional at all, muon read atac data would work just fine in our package. > A solution to that would be versioned schemata. That could be good, the schema also don't have to be versioned, we can just have a few options and package devs wrap the method with their choice.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:515,integrability,version,versioned,515,"> I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common scio package could be a way to achieve that. Agree, also R packages seem to be doing just fine with Seurat/Bioconductor representation? > Also muon and scvi-tools read in different things from 10x atac data. This is not intentional at all, muon read atac data would work just fine in our package. > A solution to that would be versioned schemata. That could be good, the schema also don't have to be versioned, we can just have a few options and package devs wrap the method with their choice.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:574,integrability,wrap,wrap,574,"> I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common scio package could be a way to achieve that. Agree, also R packages seem to be doing just fine with Seurat/Bioconductor representation? > Also muon and scvi-tools read in different things from 10x atac data. This is not intentional at all, muon read atac data would work just fine in our package. > A solution to that would be versioned schemata. That could be good, the schema also don't have to be versioned, we can just have a few options and package devs wrap the method with their choice.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:120,modifiability,pac,package,120,"> I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common scio package could be a way to achieve that. Agree, also R packages seem to be doing just fine with Seurat/Bioconductor representation? > Also muon and scvi-tools read in different things from 10x atac data. This is not intentional at all, muon read atac data would work just fine in our package. > A solution to that would be versioned schemata. That could be good, the schema also don't have to be versioned, we can just have a few options and package devs wrap the method with their choice.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:174,modifiability,pac,packages,174,"> I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common scio package could be a way to achieve that. Agree, also R packages seem to be doing just fine with Seurat/Bioconductor representation? > Also muon and scvi-tools read in different things from 10x atac data. This is not intentional at all, muon read atac data would work just fine in our package. > A solution to that would be versioned schemata. That could be good, the schema also don't have to be versioned, we can just have a few options and package devs wrap the method with their choice.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:403,modifiability,pac,package,403,"> I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common scio package could be a way to achieve that. Agree, also R packages seem to be doing just fine with Seurat/Bioconductor representation? > Also muon and scvi-tools read in different things from 10x atac data. This is not intentional at all, muon read atac data would work just fine in our package. > A solution to that would be versioned schemata. That could be good, the schema also don't have to be versioned, we can just have a few options and package devs wrap the method with their choice.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:442,modifiability,version,versioned,442,"> I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common scio package could be a way to achieve that. Agree, also R packages seem to be doing just fine with Seurat/Bioconductor representation? > Also muon and scvi-tools read in different things from 10x atac data. This is not intentional at all, muon read atac data would work just fine in our package. > A solution to that would be versioned schemata. That could be good, the schema also don't have to be versioned, we can just have a few options and package devs wrap the method with their choice.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:515,modifiability,version,versioned,515,"> I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common scio package could be a way to achieve that. Agree, also R packages seem to be doing just fine with Seurat/Bioconductor representation? > Also muon and scvi-tools read in different things from 10x atac data. This is not intentional at all, muon read atac data would work just fine in our package. > A solution to that would be versioned schemata. That could be good, the schema also don't have to be versioned, we can just have a few options and package devs wrap the method with their choice.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:561,modifiability,pac,package,561,"> I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common scio package could be a way to achieve that. Agree, also R packages seem to be doing just fine with Seurat/Bioconductor representation? > Also muon and scvi-tools read in different things from 10x atac data. This is not intentional at all, muon read atac data would work just fine in our package. > A solution to that would be versioned schemata. That could be good, the schema also don't have to be versioned, we can just have a few options and package devs wrap the method with their choice.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:272,usability,tool,tools,272,"> I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common scio package could be a way to achieve that. Agree, also R packages seem to be doing just fine with Seurat/Bioconductor representation? > Also muon and scvi-tools read in different things from 10x atac data. This is not intentional at all, muon read atac data would work just fine in our package. > A solution to that would be versioned schemata. That could be good, the schema also don't have to be versioned, we can just have a few options and package devs wrap the method with their choice.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:238,energy efficiency,current,currently,238,"Been meaning to share this hear, but haven't gotten around writing it up so I'll just write something quick:. I think if we are going to say ""here is the way to represent this kind of data"" we shouldn't just set that to be whatever we do currently and call it a standard. I think it's worth consulting with the people this affects, like those we need to do interchange with and data repositories. [Feature Object Matrix (FOM) schema](https://docs.google.com/document/d/1xr03mkHEBm9elzCbwlE4KvpQh8Er-6t9FgyERM-WSrM/edit) working group is organizing such a group of stakeholders to define these standards. I think it would make a lot of sense to have more scverse participation with this group and to plan to adopt their standards.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:383,integrability,repositor,repositories,383,"Been meaning to share this hear, but haven't gotten around writing it up so I'll just write something quick:. I think if we are going to say ""here is the way to represent this kind of data"" we shouldn't just set that to be whatever we do currently and call it a standard. I think it's worth consulting with the people this affects, like those we need to do interchange with and data repositories. [Feature Object Matrix (FOM) schema](https://docs.google.com/document/d/1xr03mkHEBm9elzCbwlE4KvpQh8Er-6t9FgyERM-WSrM/edit) working group is organizing such a group of stakeholders to define these standards. I think it would make a lot of sense to have more scverse participation with this group and to plan to adopt their standards.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:426,integrability,schema,schema,426,"Been meaning to share this hear, but haven't gotten around writing it up so I'll just write something quick:. I think if we are going to say ""here is the way to represent this kind of data"" we shouldn't just set that to be whatever we do currently and call it a standard. I think it's worth consulting with the people this affects, like those we need to do interchange with and data repositories. [Feature Object Matrix (FOM) schema](https://docs.google.com/document/d/1xr03mkHEBm9elzCbwlE4KvpQh8Er-6t9FgyERM-WSrM/edit) working group is organizing such a group of stakeholders to define these standards. I think it would make a lot of sense to have more scverse participation with this group and to plan to adopt their standards.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:16,interoperability,share,share,16,"Been meaning to share this hear, but haven't gotten around writing it up so I'll just write something quick:. I think if we are going to say ""here is the way to represent this kind of data"" we shouldn't just set that to be whatever we do currently and call it a standard. I think it's worth consulting with the people this affects, like those we need to do interchange with and data repositories. [Feature Object Matrix (FOM) schema](https://docs.google.com/document/d/1xr03mkHEBm9elzCbwlE4KvpQh8Er-6t9FgyERM-WSrM/edit) working group is organizing such a group of stakeholders to define these standards. I think it would make a lot of sense to have more scverse participation with this group and to plan to adopt their standards.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:262,interoperability,standard,standard,262,"Been meaning to share this hear, but haven't gotten around writing it up so I'll just write something quick:. I think if we are going to say ""here is the way to represent this kind of data"" we shouldn't just set that to be whatever we do currently and call it a standard. I think it's worth consulting with the people this affects, like those we need to do interchange with and data repositories. [Feature Object Matrix (FOM) schema](https://docs.google.com/document/d/1xr03mkHEBm9elzCbwlE4KvpQh8Er-6t9FgyERM-WSrM/edit) working group is organizing such a group of stakeholders to define these standards. I think it would make a lot of sense to have more scverse participation with this group and to plan to adopt their standards.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:383,interoperability,repositor,repositories,383,"Been meaning to share this hear, but haven't gotten around writing it up so I'll just write something quick:. I think if we are going to say ""here is the way to represent this kind of data"" we shouldn't just set that to be whatever we do currently and call it a standard. I think it's worth consulting with the people this affects, like those we need to do interchange with and data repositories. [Feature Object Matrix (FOM) schema](https://docs.google.com/document/d/1xr03mkHEBm9elzCbwlE4KvpQh8Er-6t9FgyERM-WSrM/edit) working group is organizing such a group of stakeholders to define these standards. I think it would make a lot of sense to have more scverse participation with this group and to plan to adopt their standards.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:593,interoperability,standard,standards,593,"Been meaning to share this hear, but haven't gotten around writing it up so I'll just write something quick:. I think if we are going to say ""here is the way to represent this kind of data"" we shouldn't just set that to be whatever we do currently and call it a standard. I think it's worth consulting with the people this affects, like those we need to do interchange with and data repositories. [Feature Object Matrix (FOM) schema](https://docs.google.com/document/d/1xr03mkHEBm9elzCbwlE4KvpQh8Er-6t9FgyERM-WSrM/edit) working group is organizing such a group of stakeholders to define these standards. I think it would make a lot of sense to have more scverse participation with this group and to plan to adopt their standards.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:719,interoperability,standard,standards,719,"Been meaning to share this hear, but haven't gotten around writing it up so I'll just write something quick:. I think if we are going to say ""here is the way to represent this kind of data"" we shouldn't just set that to be whatever we do currently and call it a standard. I think it's worth consulting with the people this affects, like those we need to do interchange with and data repositories. [Feature Object Matrix (FOM) schema](https://docs.google.com/document/d/1xr03mkHEBm9elzCbwlE4KvpQh8Er-6t9FgyERM-WSrM/edit) working group is organizing such a group of stakeholders to define these standards. I think it would make a lot of sense to have more scverse participation with this group and to plan to adopt their standards.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:699,testability,plan,plan,699,"Been meaning to share this hear, but haven't gotten around writing it up so I'll just write something quick:. I think if we are going to say ""here is the way to represent this kind of data"" we shouldn't just set that to be whatever we do currently and call it a standard. I think it's worth consulting with the people this affects, like those we need to do interchange with and data repositories. [Feature Object Matrix (FOM) schema](https://docs.google.com/document/d/1xr03mkHEBm9elzCbwlE4KvpQh8Er-6t9FgyERM-WSrM/edit) working group is organizing such a group of stakeholders to define these standards. I think it would make a lot of sense to have more scverse participation with this group and to plan to adopt their standards.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:458,usability,document,document,458,"Been meaning to share this hear, but haven't gotten around writing it up so I'll just write something quick:. I think if we are going to say ""here is the way to represent this kind of data"" we shouldn't just set that to be whatever we do currently and call it a standard. I think it's worth consulting with the people this affects, like those we need to do interchange with and data repositories. [Feature Object Matrix (FOM) schema](https://docs.google.com/document/d/1xr03mkHEBm9elzCbwlE4KvpQh8Er-6t9FgyERM-WSrM/edit) working group is organizing such a group of stakeholders to define these standards. I think it would make a lot of sense to have more scverse participation with this group and to plan to adopt their standards.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:152,energy efficiency,load,load,152,"Sure, don't see how that's mutually exclusive with having a package. We have a huge problem in the ecosystem right now that it's not straightforward to load data from non rna-seq experiments (no clear guidance where to go etc)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:60,modifiability,pac,package,60,"Sure, don't see how that's mutually exclusive with having a package. We have a huge problem in the ecosystem right now that it's not straightforward to load data from non rna-seq experiments (no clear guidance where to go etc)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:152,performance,load,load,152,"Sure, don't see how that's mutually exclusive with having a package. We have a huge problem in the ecosystem right now that it's not straightforward to load data from non rna-seq experiments (no clear guidance where to go etc)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:195,usability,clear,clear,195,"Sure, don't see how that's mutually exclusive with having a package. We have a huge problem in the ecosystem right now that it's not straightforward to load data from non rna-seq experiments (no clear guidance where to go etc)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:201,usability,guidanc,guidance,201,"Sure, don't see how that's mutually exclusive with having a package. We have a huge problem in the ecosystem right now that it's not straightforward to load data from non rna-seq experiments (no clear guidance where to go etc)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:31,integrability,standardiz,standardization,31,"I agree with Adam! I'm all for standardization and joining efforts with the FOM group, but this is an effort that my very well take years. We need a temporary solution in the meanwhile.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:31,interoperability,standard,standardization,31,"I agree with Adam! I'm all for standardization and joining efforts with the FOM group, but this is an effort that my very well take years. We need a temporary solution in the meanwhile.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:418,deployability,API,API,418,"agree with @grst -- also:. > I think if we are going to say ""here is the way to represent this kind of data"" we shouldn't just set that to be whatever we do currently and call it a standard. I mean this is what we are currently doing explicitly, it's just scattered across a few packages. We really need to fill the current gap in accessibility. The first hit below takes me to a package that doesn't have functioning API documentation (while it might work it's not clear if I don't know what I'm doing). <img width=""300"" alt=""Screen Shot 2022-04-28 at 8 29 52 AM"" src=""https://user-images.githubusercontent.com/10859440/165788872-442dff0f-64d4-4893-8a27-61a4a965e2f8.png"">",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:157,energy efficiency,current,currently,157,"agree with @grst -- also:. > I think if we are going to say ""here is the way to represent this kind of data"" we shouldn't just set that to be whatever we do currently and call it a standard. I mean this is what we are currently doing explicitly, it's just scattered across a few packages. We really need to fill the current gap in accessibility. The first hit below takes me to a package that doesn't have functioning API documentation (while it might work it's not clear if I don't know what I'm doing). <img width=""300"" alt=""Screen Shot 2022-04-28 at 8 29 52 AM"" src=""https://user-images.githubusercontent.com/10859440/165788872-442dff0f-64d4-4893-8a27-61a4a965e2f8.png"">",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:218,energy efficiency,current,currently,218,"agree with @grst -- also:. > I think if we are going to say ""here is the way to represent this kind of data"" we shouldn't just set that to be whatever we do currently and call it a standard. I mean this is what we are currently doing explicitly, it's just scattered across a few packages. We really need to fill the current gap in accessibility. The first hit below takes me to a package that doesn't have functioning API documentation (while it might work it's not clear if I don't know what I'm doing). <img width=""300"" alt=""Screen Shot 2022-04-28 at 8 29 52 AM"" src=""https://user-images.githubusercontent.com/10859440/165788872-442dff0f-64d4-4893-8a27-61a4a965e2f8.png"">",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:316,energy efficiency,current,current,316,"agree with @grst -- also:. > I think if we are going to say ""here is the way to represent this kind of data"" we shouldn't just set that to be whatever we do currently and call it a standard. I mean this is what we are currently doing explicitly, it's just scattered across a few packages. We really need to fill the current gap in accessibility. The first hit below takes me to a package that doesn't have functioning API documentation (while it might work it's not clear if I don't know what I'm doing). <img width=""300"" alt=""Screen Shot 2022-04-28 at 8 29 52 AM"" src=""https://user-images.githubusercontent.com/10859440/165788872-442dff0f-64d4-4893-8a27-61a4a965e2f8.png"">",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:418,integrability,API,API,418,"agree with @grst -- also:. > I think if we are going to say ""here is the way to represent this kind of data"" we shouldn't just set that to be whatever we do currently and call it a standard. I mean this is what we are currently doing explicitly, it's just scattered across a few packages. We really need to fill the current gap in accessibility. The first hit below takes me to a package that doesn't have functioning API documentation (while it might work it's not clear if I don't know what I'm doing). <img width=""300"" alt=""Screen Shot 2022-04-28 at 8 29 52 AM"" src=""https://user-images.githubusercontent.com/10859440/165788872-442dff0f-64d4-4893-8a27-61a4a965e2f8.png"">",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:181,interoperability,standard,standard,181,"agree with @grst -- also:. > I think if we are going to say ""here is the way to represent this kind of data"" we shouldn't just set that to be whatever we do currently and call it a standard. I mean this is what we are currently doing explicitly, it's just scattered across a few packages. We really need to fill the current gap in accessibility. The first hit below takes me to a package that doesn't have functioning API documentation (while it might work it's not clear if I don't know what I'm doing). <img width=""300"" alt=""Screen Shot 2022-04-28 at 8 29 52 AM"" src=""https://user-images.githubusercontent.com/10859440/165788872-442dff0f-64d4-4893-8a27-61a4a965e2f8.png"">",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:418,interoperability,API,API,418,"agree with @grst -- also:. > I think if we are going to say ""here is the way to represent this kind of data"" we shouldn't just set that to be whatever we do currently and call it a standard. I mean this is what we are currently doing explicitly, it's just scattered across a few packages. We really need to fill the current gap in accessibility. The first hit below takes me to a package that doesn't have functioning API documentation (while it might work it's not clear if I don't know what I'm doing). <img width=""300"" alt=""Screen Shot 2022-04-28 at 8 29 52 AM"" src=""https://user-images.githubusercontent.com/10859440/165788872-442dff0f-64d4-4893-8a27-61a4a965e2f8.png"">",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:279,modifiability,pac,packages,279,"agree with @grst -- also:. > I think if we are going to say ""here is the way to represent this kind of data"" we shouldn't just set that to be whatever we do currently and call it a standard. I mean this is what we are currently doing explicitly, it's just scattered across a few packages. We really need to fill the current gap in accessibility. The first hit below takes me to a package that doesn't have functioning API documentation (while it might work it's not clear if I don't know what I'm doing). <img width=""300"" alt=""Screen Shot 2022-04-28 at 8 29 52 AM"" src=""https://user-images.githubusercontent.com/10859440/165788872-442dff0f-64d4-4893-8a27-61a4a965e2f8.png"">",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:380,modifiability,pac,package,380,"agree with @grst -- also:. > I think if we are going to say ""here is the way to represent this kind of data"" we shouldn't just set that to be whatever we do currently and call it a standard. I mean this is what we are currently doing explicitly, it's just scattered across a few packages. We really need to fill the current gap in accessibility. The first hit below takes me to a package that doesn't have functioning API documentation (while it might work it's not clear if I don't know what I'm doing). <img width=""300"" alt=""Screen Shot 2022-04-28 at 8 29 52 AM"" src=""https://user-images.githubusercontent.com/10859440/165788872-442dff0f-64d4-4893-8a27-61a4a965e2f8.png"">",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:393,reliability,doe,doesn,393,"agree with @grst -- also:. > I think if we are going to say ""here is the way to represent this kind of data"" we shouldn't just set that to be whatever we do currently and call it a standard. I mean this is what we are currently doing explicitly, it's just scattered across a few packages. We really need to fill the current gap in accessibility. The first hit below takes me to a package that doesn't have functioning API documentation (while it might work it's not clear if I don't know what I'm doing). <img width=""300"" alt=""Screen Shot 2022-04-28 at 8 29 52 AM"" src=""https://user-images.githubusercontent.com/10859440/165788872-442dff0f-64d4-4893-8a27-61a4a965e2f8.png"">",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:331,security,access,accessibility,331,"agree with @grst -- also:. > I think if we are going to say ""here is the way to represent this kind of data"" we shouldn't just set that to be whatever we do currently and call it a standard. I mean this is what we are currently doing explicitly, it's just scattered across a few packages. We really need to fill the current gap in accessibility. The first hit below takes me to a package that doesn't have functioning API documentation (while it might work it's not clear if I don't know what I'm doing). <img width=""300"" alt=""Screen Shot 2022-04-28 at 8 29 52 AM"" src=""https://user-images.githubusercontent.com/10859440/165788872-442dff0f-64d4-4893-8a27-61a4a965e2f8.png"">",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:422,usability,document,documentation,422,"agree with @grst -- also:. > I think if we are going to say ""here is the way to represent this kind of data"" we shouldn't just set that to be whatever we do currently and call it a standard. I mean this is what we are currently doing explicitly, it's just scattered across a few packages. We really need to fill the current gap in accessibility. The first hit below takes me to a package that doesn't have functioning API documentation (while it might work it's not clear if I don't know what I'm doing). <img width=""300"" alt=""Screen Shot 2022-04-28 at 8 29 52 AM"" src=""https://user-images.githubusercontent.com/10859440/165788872-442dff0f-64d4-4893-8a27-61a4a965e2f8.png"">",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:466,usability,clear,clear,466,"agree with @grst -- also:. > I think if we are going to say ""here is the way to represent this kind of data"" we shouldn't just set that to be whatever we do currently and call it a standard. I mean this is what we are currently doing explicitly, it's just scattered across a few packages. We really need to fill the current gap in accessibility. The first hit below takes me to a package that doesn't have functioning API documentation (while it might work it's not clear if I don't know what I'm doing). <img width=""300"" alt=""Screen Shot 2022-04-28 at 8 29 52 AM"" src=""https://user-images.githubusercontent.com/10859440/165788872-442dff0f-64d4-4893-8a27-61a4a965e2f8.png"">",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/issues/1387:578,usability,user,user-images,578,"agree with @grst -- also:. > I think if we are going to say ""here is the way to represent this kind of data"" we shouldn't just set that to be whatever we do currently and call it a standard. I mean this is what we are currently doing explicitly, it's just scattered across a few packages. We really need to fill the current gap in accessibility. The first hit below takes me to a package that doesn't have functioning API documentation (while it might work it's not clear if I don't know what I'm doing). <img width=""300"" alt=""Screen Shot 2022-04-28 at 8 29 52 AM"" src=""https://user-images.githubusercontent.com/10859440/165788872-442dff0f-64d4-4893-8a27-61a4a965e2f8.png"">",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387
https://github.com/scverse/scanpy/pull/1388:147,energy efficiency,measur,measurement,147,"I had to look up what `""pts""` was for this (its the proportion of samples with non-zero expression). Maybe it could have a better name? Also, this measurement is a lot like the mean or variance. Should those be included here too?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1388
https://github.com/scverse/scanpy/pull/1388:386,energy efficiency,measur,measurement,386,"> I had to look up what `""pts""` was for this (its the proportion of samples with non-zero expression). Maybe it could have a better name? . I agree. I use `fraction_expressed` in my custom code, we can use it also here. But this should be changed in sc.tl.rank_genes_groups, I guess. This is also the name of the option now rank_genes_groups(..., pts=True). @Koncopd wdyt? > Also, this measurement is a lot like the mean or variance. Should those be included here too? Yes, I fully agree. I have ugly scripts to add these actually, but it'd be great to have them here too. Again, maybe we should do this also in rank_genes_groups? I'd also love to have something like groupby for anndata to easily calculate mean, var etc. of X.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1388
https://github.com/scverse/scanpy/pull/1388:182,usability,custom,custom,182,"> I had to look up what `""pts""` was for this (its the proportion of samples with non-zero expression). Maybe it could have a better name? . I agree. I use `fraction_expressed` in my custom code, we can use it also here. But this should be changed in sc.tl.rank_genes_groups, I guess. This is also the name of the option now rank_genes_groups(..., pts=True). @Koncopd wdyt? > Also, this measurement is a lot like the mean or variance. Should those be included here too? Yes, I fully agree. I have ugly scripts to add these actually, but it'd be great to have them here too. Again, maybe we should do this also in rank_genes_groups? I'd also love to have something like groupby for anndata to easily calculate mean, var etc. of X.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1388
https://github.com/scverse/scanpy/pull/1388:115,usability,feedback,feedback,115,I renamed pts and pts_rest to `fraction_group` and `fraction_rest`. I'd like to merge this PR if there is no other feedback. We can think about how to provide aggregate statistics somewhere else. Then we can revisit this function and merge this info too.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1388
https://github.com/scverse/scanpy/pull/1388:168,deployability,api,api,168,"> ### `group` argument. > . > I'm not sure I like the group argument accepting multiple groups. Could that get removed? If you'd like to keep it, why is this the right api? > . I wanna keep it this way, if possible. There are two reasons I can think of:. 1) When I want to save the results of DE (e.g. as a table for a publication or to send to collaborators), I always need to loop over all categories which is not so hard but gets super annoying since I need it all the time. This is something almost all scanpy users I know also need, because they too need a table for DE results with all groups in it in almost every project. . 2) We don't have such an argument limiting the function to a specific category in a _mandatory_ way in any DE-related function e.g. we don't have a mandatory group argument in sc.tl.rank_genes_groups, right? Why? Because it is more common to do with all groups. We could have made sc.tl.rank_genes_groups also like existing API of sc.get.rank_genes_groups_df, but we didn't. Another example is the groups argument in sc.pl.umap, it's more common to look at all groups in UMAPs rather than a specific group, so not mandatory. So maybe it makes sense to ask the question other way around: why do we limit it to single group here in sc.get.rank_genes_groups_df and why is it the right API? > ### New column name. > . > I don't think `fraction_group` and `fraction_rest` are obvious either. `fraction_expressed_group` and `fraction_expressed_rest`? Those are a bit wordy. Also, is `rest` the right term? > . > The idea behind this function was to get the basic summary of differential expression that you'd expect to get from a DE package. Do other packages provide this value? If so, what do they call this column? Again, mean expression and fraction of cells ""expressing"" a gene is expected by A LOT OF people running DE in any single cell tool. I am not asking for these features randomly or only bcs I personally need them, it is indeed needed by many people. Seurat u",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1388
https://github.com/scverse/scanpy/pull/1388:956,deployability,API,API,956,"> ### `group` argument. > . > I'm not sure I like the group argument accepting multiple groups. Could that get removed? If you'd like to keep it, why is this the right api? > . I wanna keep it this way, if possible. There are two reasons I can think of:. 1) When I want to save the results of DE (e.g. as a table for a publication or to send to collaborators), I always need to loop over all categories which is not so hard but gets super annoying since I need it all the time. This is something almost all scanpy users I know also need, because they too need a table for DE results with all groups in it in almost every project. . 2) We don't have such an argument limiting the function to a specific category in a _mandatory_ way in any DE-related function e.g. we don't have a mandatory group argument in sc.tl.rank_genes_groups, right? Why? Because it is more common to do with all groups. We could have made sc.tl.rank_genes_groups also like existing API of sc.get.rank_genes_groups_df, but we didn't. Another example is the groups argument in sc.pl.umap, it's more common to look at all groups in UMAPs rather than a specific group, so not mandatory. So maybe it makes sense to ask the question other way around: why do we limit it to single group here in sc.get.rank_genes_groups_df and why is it the right API? > ### New column name. > . > I don't think `fraction_group` and `fraction_rest` are obvious either. `fraction_expressed_group` and `fraction_expressed_rest`? Those are a bit wordy. Also, is `rest` the right term? > . > The idea behind this function was to get the basic summary of differential expression that you'd expect to get from a DE package. Do other packages provide this value? If so, what do they call this column? Again, mean expression and fraction of cells ""expressing"" a gene is expected by A LOT OF people running DE in any single cell tool. I am not asking for these features randomly or only bcs I personally need them, it is indeed needed by many people. Seurat u",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1388
https://github.com/scverse/scanpy/pull/1388:1314,deployability,API,API,1314," a publication or to send to collaborators), I always need to loop over all categories which is not so hard but gets super annoying since I need it all the time. This is something almost all scanpy users I know also need, because they too need a table for DE results with all groups in it in almost every project. . 2) We don't have such an argument limiting the function to a specific category in a _mandatory_ way in any DE-related function e.g. we don't have a mandatory group argument in sc.tl.rank_genes_groups, right? Why? Because it is more common to do with all groups. We could have made sc.tl.rank_genes_groups also like existing API of sc.get.rank_genes_groups_df, but we didn't. Another example is the groups argument in sc.pl.umap, it's more common to look at all groups in UMAPs rather than a specific group, so not mandatory. So maybe it makes sense to ask the question other way around: why do we limit it to single group here in sc.get.rank_genes_groups_df and why is it the right API? > ### New column name. > . > I don't think `fraction_group` and `fraction_rest` are obvious either. `fraction_expressed_group` and `fraction_expressed_rest`? Those are a bit wordy. Also, is `rest` the right term? > . > The idea behind this function was to get the basic summary of differential expression that you'd expect to get from a DE package. Do other packages provide this value? If so, what do they call this column? Again, mean expression and fraction of cells ""expressing"" a gene is expected by A LOT OF people running DE in any single cell tool. I am not asking for these features randomly or only bcs I personally need them, it is indeed needed by many people. Seurat users expect them too. scanpy API should prioritize the users a little bit more, in my opinion. If it sounds too subjective let's make an experiment: let's make a poll on Twitter using the scanpy account ask people if they think fractions are needed or not and how they feel about these names :). Main suggestion was ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1388
https://github.com/scverse/scanpy/pull/1388:2029,deployability,API,API,2029,"roups argument in sc.pl.umap, it's more common to look at all groups in UMAPs rather than a specific group, so not mandatory. So maybe it makes sense to ask the question other way around: why do we limit it to single group here in sc.get.rank_genes_groups_df and why is it the right API? > ### New column name. > . > I don't think `fraction_group` and `fraction_rest` are obvious either. `fraction_expressed_group` and `fraction_expressed_rest`? Those are a bit wordy. Also, is `rest` the right term? > . > The idea behind this function was to get the basic summary of differential expression that you'd expect to get from a DE package. Do other packages provide this value? If so, what do they call this column? Again, mean expression and fraction of cells ""expressing"" a gene is expected by A LOT OF people running DE in any single cell tool. I am not asking for these features randomly or only bcs I personally need them, it is indeed needed by many people. Seurat users expect them too. scanpy API should prioritize the users a little bit more, in my opinion. If it sounds too subjective let's make an experiment: let's make a poll on Twitter using the scanpy account ask people if they think fractions are needed or not and how they feel about these names :). Main suggestion was indeed influenced by Seurat (see https://github.com/theislab/scanpy/pull/1081#issuecomment-595789816). But I don't really mind what the name will be. I think most people in the field are familiar with these names (even mu and alpha are used a lot for mean expression and fraction of cells expressing a gene, even though it is even more cryptic). . I do not think there is a super easy way to find a short and obvious name for the column, but I think using fraction_group and fraction_rest (or fraction_ref) are good enough. Reason I suggest fraction_rest is because ""rest"" is the default value of the reference argument in rank_genes_groups. We can also make it `f""fraction_{reference}""`, if this is the way it is i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1388
https://github.com/scverse/scanpy/pull/1388:3394,deployability,log,logistic,3394,"o other packages provide this value? If so, what do they call this column? Again, mean expression and fraction of cells ""expressing"" a gene is expected by A LOT OF people running DE in any single cell tool. I am not asking for these features randomly or only bcs I personally need them, it is indeed needed by many people. Seurat users expect them too. scanpy API should prioritize the users a little bit more, in my opinion. If it sounds too subjective let's make an experiment: let's make a poll on Twitter using the scanpy account ask people if they think fractions are needed or not and how they feel about these names :). Main suggestion was indeed influenced by Seurat (see https://github.com/theislab/scanpy/pull/1081#issuecomment-595789816). But I don't really mind what the name will be. I think most people in the field are familiar with these names (even mu and alpha are used a lot for mean expression and fraction of cells expressing a gene, even though it is even more cryptic). . I do not think there is a super easy way to find a short and obvious name for the column, but I think using fraction_group and fraction_rest (or fraction_ref) are good enough. Reason I suggest fraction_rest is because ""rest"" is the default value of the reference argument in rank_genes_groups. We can also make it `f""fraction_{reference}""`, if this is the way it is implemented. Speaking of the column names, for example, do you think `score` is really obvious :) Try to ask a few regular scanpy users what `score` means in the DE results we generate. Even our documentation is wrong: `Structured array to be indexed by group id storing the z-score underlying the computation of a p-value for each gene for each group.` It is the logistic regression beta coef for logreg and t-statistic for t-test, it's not z-score at all... . So, even if the column name is not obvious, I think it's ok to explain it properly in the documentation and for `fraction_group`, it is easy (also easier than score) to explain.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1388
https://github.com/scverse/scanpy/pull/1388:3428,deployability,log,logreg,3428,"o other packages provide this value? If so, what do they call this column? Again, mean expression and fraction of cells ""expressing"" a gene is expected by A LOT OF people running DE in any single cell tool. I am not asking for these features randomly or only bcs I personally need them, it is indeed needed by many people. Seurat users expect them too. scanpy API should prioritize the users a little bit more, in my opinion. If it sounds too subjective let's make an experiment: let's make a poll on Twitter using the scanpy account ask people if they think fractions are needed or not and how they feel about these names :). Main suggestion was indeed influenced by Seurat (see https://github.com/theislab/scanpy/pull/1081#issuecomment-595789816). But I don't really mind what the name will be. I think most people in the field are familiar with these names (even mu and alpha are used a lot for mean expression and fraction of cells expressing a gene, even though it is even more cryptic). . I do not think there is a super easy way to find a short and obvious name for the column, but I think using fraction_group and fraction_rest (or fraction_ref) are good enough. Reason I suggest fraction_rest is because ""rest"" is the default value of the reference argument in rank_genes_groups. We can also make it `f""fraction_{reference}""`, if this is the way it is implemented. Speaking of the column names, for example, do you think `score` is really obvious :) Try to ask a few regular scanpy users what `score` means in the DE results we generate. Even our documentation is wrong: `Structured array to be indexed by group id storing the z-score underlying the computation of a p-value for each gene for each group.` It is the logistic regression beta coef for logreg and t-statistic for t-test, it's not z-score at all... . So, even if the column name is not obvious, I think it's ok to explain it properly in the documentation and for `fraction_group`, it is easy (also easier than score) to explain.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1388
https://github.com/scverse/scanpy/pull/1388:168,integrability,api,api,168,"> ### `group` argument. > . > I'm not sure I like the group argument accepting multiple groups. Could that get removed? If you'd like to keep it, why is this the right api? > . I wanna keep it this way, if possible. There are two reasons I can think of:. 1) When I want to save the results of DE (e.g. as a table for a publication or to send to collaborators), I always need to loop over all categories which is not so hard but gets super annoying since I need it all the time. This is something almost all scanpy users I know also need, because they too need a table for DE results with all groups in it in almost every project. . 2) We don't have such an argument limiting the function to a specific category in a _mandatory_ way in any DE-related function e.g. we don't have a mandatory group argument in sc.tl.rank_genes_groups, right? Why? Because it is more common to do with all groups. We could have made sc.tl.rank_genes_groups also like existing API of sc.get.rank_genes_groups_df, but we didn't. Another example is the groups argument in sc.pl.umap, it's more common to look at all groups in UMAPs rather than a specific group, so not mandatory. So maybe it makes sense to ask the question other way around: why do we limit it to single group here in sc.get.rank_genes_groups_df and why is it the right API? > ### New column name. > . > I don't think `fraction_group` and `fraction_rest` are obvious either. `fraction_expressed_group` and `fraction_expressed_rest`? Those are a bit wordy. Also, is `rest` the right term? > . > The idea behind this function was to get the basic summary of differential expression that you'd expect to get from a DE package. Do other packages provide this value? If so, what do they call this column? Again, mean expression and fraction of cells ""expressing"" a gene is expected by A LOT OF people running DE in any single cell tool. I am not asking for these features randomly or only bcs I personally need them, it is indeed needed by many people. Seurat u",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1388
https://github.com/scverse/scanpy/pull/1388:319,integrability,pub,publication,319,"> ### `group` argument. > . > I'm not sure I like the group argument accepting multiple groups. Could that get removed? If you'd like to keep it, why is this the right api? > . I wanna keep it this way, if possible. There are two reasons I can think of:. 1) When I want to save the results of DE (e.g. as a table for a publication or to send to collaborators), I always need to loop over all categories which is not so hard but gets super annoying since I need it all the time. This is something almost all scanpy users I know also need, because they too need a table for DE results with all groups in it in almost every project. . 2) We don't have such an argument limiting the function to a specific category in a _mandatory_ way in any DE-related function e.g. we don't have a mandatory group argument in sc.tl.rank_genes_groups, right? Why? Because it is more common to do with all groups. We could have made sc.tl.rank_genes_groups also like existing API of sc.get.rank_genes_groups_df, but we didn't. Another example is the groups argument in sc.pl.umap, it's more common to look at all groups in UMAPs rather than a specific group, so not mandatory. So maybe it makes sense to ask the question other way around: why do we limit it to single group here in sc.get.rank_genes_groups_df and why is it the right API? > ### New column name. > . > I don't think `fraction_group` and `fraction_rest` are obvious either. `fraction_expressed_group` and `fraction_expressed_rest`? Those are a bit wordy. Also, is `rest` the right term? > . > The idea behind this function was to get the basic summary of differential expression that you'd expect to get from a DE package. Do other packages provide this value? If so, what do they call this column? Again, mean expression and fraction of cells ""expressing"" a gene is expected by A LOT OF people running DE in any single cell tool. I am not asking for these features randomly or only bcs I personally need them, it is indeed needed by many people. Seurat u",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1388
https://github.com/scverse/scanpy/pull/1388:956,integrability,API,API,956,"> ### `group` argument. > . > I'm not sure I like the group argument accepting multiple groups. Could that get removed? If you'd like to keep it, why is this the right api? > . I wanna keep it this way, if possible. There are two reasons I can think of:. 1) When I want to save the results of DE (e.g. as a table for a publication or to send to collaborators), I always need to loop over all categories which is not so hard but gets super annoying since I need it all the time. This is something almost all scanpy users I know also need, because they too need a table for DE results with all groups in it in almost every project. . 2) We don't have such an argument limiting the function to a specific category in a _mandatory_ way in any DE-related function e.g. we don't have a mandatory group argument in sc.tl.rank_genes_groups, right? Why? Because it is more common to do with all groups. We could have made sc.tl.rank_genes_groups also like existing API of sc.get.rank_genes_groups_df, but we didn't. Another example is the groups argument in sc.pl.umap, it's more common to look at all groups in UMAPs rather than a specific group, so not mandatory. So maybe it makes sense to ask the question other way around: why do we limit it to single group here in sc.get.rank_genes_groups_df and why is it the right API? > ### New column name. > . > I don't think `fraction_group` and `fraction_rest` are obvious either. `fraction_expressed_group` and `fraction_expressed_rest`? Those are a bit wordy. Also, is `rest` the right term? > . > The idea behind this function was to get the basic summary of differential expression that you'd expect to get from a DE package. Do other packages provide this value? If so, what do they call this column? Again, mean expression and fraction of cells ""expressing"" a gene is expected by A LOT OF people running DE in any single cell tool. I am not asking for these features randomly or only bcs I personally need them, it is indeed needed by many people. Seurat u",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1388
https://github.com/scverse/scanpy/pull/1388:1314,integrability,API,API,1314," a publication or to send to collaborators), I always need to loop over all categories which is not so hard but gets super annoying since I need it all the time. This is something almost all scanpy users I know also need, because they too need a table for DE results with all groups in it in almost every project. . 2) We don't have such an argument limiting the function to a specific category in a _mandatory_ way in any DE-related function e.g. we don't have a mandatory group argument in sc.tl.rank_genes_groups, right? Why? Because it is more common to do with all groups. We could have made sc.tl.rank_genes_groups also like existing API of sc.get.rank_genes_groups_df, but we didn't. Another example is the groups argument in sc.pl.umap, it's more common to look at all groups in UMAPs rather than a specific group, so not mandatory. So maybe it makes sense to ask the question other way around: why do we limit it to single group here in sc.get.rank_genes_groups_df and why is it the right API? > ### New column name. > . > I don't think `fraction_group` and `fraction_rest` are obvious either. `fraction_expressed_group` and `fraction_expressed_rest`? Those are a bit wordy. Also, is `rest` the right term? > . > The idea behind this function was to get the basic summary of differential expression that you'd expect to get from a DE package. Do other packages provide this value? If so, what do they call this column? Again, mean expression and fraction of cells ""expressing"" a gene is expected by A LOT OF people running DE in any single cell tool. I am not asking for these features randomly or only bcs I personally need them, it is indeed needed by many people. Seurat users expect them too. scanpy API should prioritize the users a little bit more, in my opinion. If it sounds too subjective let's make an experiment: let's make a poll on Twitter using the scanpy account ask people if they think fractions are needed or not and how they feel about these names :). Main suggestion was ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1388
https://github.com/scverse/scanpy/pull/1388:2029,integrability,API,API,2029,"roups argument in sc.pl.umap, it's more common to look at all groups in UMAPs rather than a specific group, so not mandatory. So maybe it makes sense to ask the question other way around: why do we limit it to single group here in sc.get.rank_genes_groups_df and why is it the right API? > ### New column name. > . > I don't think `fraction_group` and `fraction_rest` are obvious either. `fraction_expressed_group` and `fraction_expressed_rest`? Those are a bit wordy. Also, is `rest` the right term? > . > The idea behind this function was to get the basic summary of differential expression that you'd expect to get from a DE package. Do other packages provide this value? If so, what do they call this column? Again, mean expression and fraction of cells ""expressing"" a gene is expected by A LOT OF people running DE in any single cell tool. I am not asking for these features randomly or only bcs I personally need them, it is indeed needed by many people. Seurat users expect them too. scanpy API should prioritize the users a little bit more, in my opinion. If it sounds too subjective let's make an experiment: let's make a poll on Twitter using the scanpy account ask people if they think fractions are needed or not and how they feel about these names :). Main suggestion was indeed influenced by Seurat (see https://github.com/theislab/scanpy/pull/1081#issuecomment-595789816). But I don't really mind what the name will be. I think most people in the field are familiar with these names (even mu and alpha are used a lot for mean expression and fraction of cells expressing a gene, even though it is even more cryptic). . I do not think there is a super easy way to find a short and obvious name for the column, but I think using fraction_group and fraction_rest (or fraction_ref) are good enough. Reason I suggest fraction_rest is because ""rest"" is the default value of the reference argument in rank_genes_groups. We can also make it `f""fraction_{reference}""`, if this is the way it is i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1388
https://github.com/scverse/scanpy/pull/1388:2112,integrability,sub,subjective,2112,"han a specific group, so not mandatory. So maybe it makes sense to ask the question other way around: why do we limit it to single group here in sc.get.rank_genes_groups_df and why is it the right API? > ### New column name. > . > I don't think `fraction_group` and `fraction_rest` are obvious either. `fraction_expressed_group` and `fraction_expressed_rest`? Those are a bit wordy. Also, is `rest` the right term? > . > The idea behind this function was to get the basic summary of differential expression that you'd expect to get from a DE package. Do other packages provide this value? If so, what do they call this column? Again, mean expression and fraction of cells ""expressing"" a gene is expected by A LOT OF people running DE in any single cell tool. I am not asking for these features randomly or only bcs I personally need them, it is indeed needed by many people. Seurat users expect them too. scanpy API should prioritize the users a little bit more, in my opinion. If it sounds too subjective let's make an experiment: let's make a poll on Twitter using the scanpy account ask people if they think fractions are needed or not and how they feel about these names :). Main suggestion was indeed influenced by Seurat (see https://github.com/theislab/scanpy/pull/1081#issuecomment-595789816). But I don't really mind what the name will be. I think most people in the field are familiar with these names (even mu and alpha are used a lot for mean expression and fraction of cells expressing a gene, even though it is even more cryptic). . I do not think there is a super easy way to find a short and obvious name for the column, but I think using fraction_group and fraction_rest (or fraction_ref) are good enough. Reason I suggest fraction_rest is because ""rest"" is the default value of the reference argument in rank_genes_groups. We can also make it `f""fraction_{reference}""`, if this is the way it is implemented. Speaking of the column names, for example, do you think `score` is really ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1388
https://github.com/scverse/scanpy/pull/1388:168,interoperability,api,api,168,"> ### `group` argument. > . > I'm not sure I like the group argument accepting multiple groups. Could that get removed? If you'd like to keep it, why is this the right api? > . I wanna keep it this way, if possible. There are two reasons I can think of:. 1) When I want to save the results of DE (e.g. as a table for a publication or to send to collaborators), I always need to loop over all categories which is not so hard but gets super annoying since I need it all the time. This is something almost all scanpy users I know also need, because they too need a table for DE results with all groups in it in almost every project. . 2) We don't have such an argument limiting the function to a specific category in a _mandatory_ way in any DE-related function e.g. we don't have a mandatory group argument in sc.tl.rank_genes_groups, right? Why? Because it is more common to do with all groups. We could have made sc.tl.rank_genes_groups also like existing API of sc.get.rank_genes_groups_df, but we didn't. Another example is the groups argument in sc.pl.umap, it's more common to look at all groups in UMAPs rather than a specific group, so not mandatory. So maybe it makes sense to ask the question other way around: why do we limit it to single group here in sc.get.rank_genes_groups_df and why is it the right API? > ### New column name. > . > I don't think `fraction_group` and `fraction_rest` are obvious either. `fraction_expressed_group` and `fraction_expressed_rest`? Those are a bit wordy. Also, is `rest` the right term? > . > The idea behind this function was to get the basic summary of differential expression that you'd expect to get from a DE package. Do other packages provide this value? If so, what do they call this column? Again, mean expression and fraction of cells ""expressing"" a gene is expected by A LOT OF people running DE in any single cell tool. I am not asking for these features randomly or only bcs I personally need them, it is indeed needed by many people. Seurat u",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1388
https://github.com/scverse/scanpy/pull/1388:693,interoperability,specif,specific,693,"> ### `group` argument. > . > I'm not sure I like the group argument accepting multiple groups. Could that get removed? If you'd like to keep it, why is this the right api? > . I wanna keep it this way, if possible. There are two reasons I can think of:. 1) When I want to save the results of DE (e.g. as a table for a publication or to send to collaborators), I always need to loop over all categories which is not so hard but gets super annoying since I need it all the time. This is something almost all scanpy users I know also need, because they too need a table for DE results with all groups in it in almost every project. . 2) We don't have such an argument limiting the function to a specific category in a _mandatory_ way in any DE-related function e.g. we don't have a mandatory group argument in sc.tl.rank_genes_groups, right? Why? Because it is more common to do with all groups. We could have made sc.tl.rank_genes_groups also like existing API of sc.get.rank_genes_groups_df, but we didn't. Another example is the groups argument in sc.pl.umap, it's more common to look at all groups in UMAPs rather than a specific group, so not mandatory. So maybe it makes sense to ask the question other way around: why do we limit it to single group here in sc.get.rank_genes_groups_df and why is it the right API? > ### New column name. > . > I don't think `fraction_group` and `fraction_rest` are obvious either. `fraction_expressed_group` and `fraction_expressed_rest`? Those are a bit wordy. Also, is `rest` the right term? > . > The idea behind this function was to get the basic summary of differential expression that you'd expect to get from a DE package. Do other packages provide this value? If so, what do they call this column? Again, mean expression and fraction of cells ""expressing"" a gene is expected by A LOT OF people running DE in any single cell tool. I am not asking for these features randomly or only bcs I personally need them, it is indeed needed by many people. Seurat u",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1388
https://github.com/scverse/scanpy/pull/1388:956,interoperability,API,API,956,"> ### `group` argument. > . > I'm not sure I like the group argument accepting multiple groups. Could that get removed? If you'd like to keep it, why is this the right api? > . I wanna keep it this way, if possible. There are two reasons I can think of:. 1) When I want to save the results of DE (e.g. as a table for a publication or to send to collaborators), I always need to loop over all categories which is not so hard but gets super annoying since I need it all the time. This is something almost all scanpy users I know also need, because they too need a table for DE results with all groups in it in almost every project. . 2) We don't have such an argument limiting the function to a specific category in a _mandatory_ way in any DE-related function e.g. we don't have a mandatory group argument in sc.tl.rank_genes_groups, right? Why? Because it is more common to do with all groups. We could have made sc.tl.rank_genes_groups also like existing API of sc.get.rank_genes_groups_df, but we didn't. Another example is the groups argument in sc.pl.umap, it's more common to look at all groups in UMAPs rather than a specific group, so not mandatory. So maybe it makes sense to ask the question other way around: why do we limit it to single group here in sc.get.rank_genes_groups_df and why is it the right API? > ### New column name. > . > I don't think `fraction_group` and `fraction_rest` are obvious either. `fraction_expressed_group` and `fraction_expressed_rest`? Those are a bit wordy. Also, is `rest` the right term? > . > The idea behind this function was to get the basic summary of differential expression that you'd expect to get from a DE package. Do other packages provide this value? If so, what do they call this column? Again, mean expression and fraction of cells ""expressing"" a gene is expected by A LOT OF people running DE in any single cell tool. I am not asking for these features randomly or only bcs I personally need them, it is indeed needed by many people. Seurat u",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1388
https://github.com/scverse/scanpy/pull/1388:1123,interoperability,specif,specific,1123,"d like to keep it, why is this the right api? > . I wanna keep it this way, if possible. There are two reasons I can think of:. 1) When I want to save the results of DE (e.g. as a table for a publication or to send to collaborators), I always need to loop over all categories which is not so hard but gets super annoying since I need it all the time. This is something almost all scanpy users I know also need, because they too need a table for DE results with all groups in it in almost every project. . 2) We don't have such an argument limiting the function to a specific category in a _mandatory_ way in any DE-related function e.g. we don't have a mandatory group argument in sc.tl.rank_genes_groups, right? Why? Because it is more common to do with all groups. We could have made sc.tl.rank_genes_groups also like existing API of sc.get.rank_genes_groups_df, but we didn't. Another example is the groups argument in sc.pl.umap, it's more common to look at all groups in UMAPs rather than a specific group, so not mandatory. So maybe it makes sense to ask the question other way around: why do we limit it to single group here in sc.get.rank_genes_groups_df and why is it the right API? > ### New column name. > . > I don't think `fraction_group` and `fraction_rest` are obvious either. `fraction_expressed_group` and `fraction_expressed_rest`? Those are a bit wordy. Also, is `rest` the right term? > . > The idea behind this function was to get the basic summary of differential expression that you'd expect to get from a DE package. Do other packages provide this value? If so, what do they call this column? Again, mean expression and fraction of cells ""expressing"" a gene is expected by A LOT OF people running DE in any single cell tool. I am not asking for these features randomly or only bcs I personally need them, it is indeed needed by many people. Seurat users expect them too. scanpy API should prioritize the users a little bit more, in my opinion. If it sounds too subjective let'",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1388
https://github.com/scverse/scanpy/pull/1388:1314,interoperability,API,API,1314," a publication or to send to collaborators), I always need to loop over all categories which is not so hard but gets super annoying since I need it all the time. This is something almost all scanpy users I know also need, because they too need a table for DE results with all groups in it in almost every project. . 2) We don't have such an argument limiting the function to a specific category in a _mandatory_ way in any DE-related function e.g. we don't have a mandatory group argument in sc.tl.rank_genes_groups, right? Why? Because it is more common to do with all groups. We could have made sc.tl.rank_genes_groups also like existing API of sc.get.rank_genes_groups_df, but we didn't. Another example is the groups argument in sc.pl.umap, it's more common to look at all groups in UMAPs rather than a specific group, so not mandatory. So maybe it makes sense to ask the question other way around: why do we limit it to single group here in sc.get.rank_genes_groups_df and why is it the right API? > ### New column name. > . > I don't think `fraction_group` and `fraction_rest` are obvious either. `fraction_expressed_group` and `fraction_expressed_rest`? Those are a bit wordy. Also, is `rest` the right term? > . > The idea behind this function was to get the basic summary of differential expression that you'd expect to get from a DE package. Do other packages provide this value? If so, what do they call this column? Again, mean expression and fraction of cells ""expressing"" a gene is expected by A LOT OF people running DE in any single cell tool. I am not asking for these features randomly or only bcs I personally need them, it is indeed needed by many people. Seurat users expect them too. scanpy API should prioritize the users a little bit more, in my opinion. If it sounds too subjective let's make an experiment: let's make a poll on Twitter using the scanpy account ask people if they think fractions are needed or not and how they feel about these names :). Main suggestion was ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1388
https://github.com/scverse/scanpy/pull/1388:2029,interoperability,API,API,2029,"roups argument in sc.pl.umap, it's more common to look at all groups in UMAPs rather than a specific group, so not mandatory. So maybe it makes sense to ask the question other way around: why do we limit it to single group here in sc.get.rank_genes_groups_df and why is it the right API? > ### New column name. > . > I don't think `fraction_group` and `fraction_rest` are obvious either. `fraction_expressed_group` and `fraction_expressed_rest`? Those are a bit wordy. Also, is `rest` the right term? > . > The idea behind this function was to get the basic summary of differential expression that you'd expect to get from a DE package. Do other packages provide this value? If so, what do they call this column? Again, mean expression and fraction of cells ""expressing"" a gene is expected by A LOT OF people running DE in any single cell tool. I am not asking for these features randomly or only bcs I personally need them, it is indeed needed by many people. Seurat users expect them too. scanpy API should prioritize the users a little bit more, in my opinion. If it sounds too subjective let's make an experiment: let's make a poll on Twitter using the scanpy account ask people if they think fractions are needed or not and how they feel about these names :). Main suggestion was indeed influenced by Seurat (see https://github.com/theislab/scanpy/pull/1081#issuecomment-595789816). But I don't really mind what the name will be. I think most people in the field are familiar with these names (even mu and alpha are used a lot for mean expression and fraction of cells expressing a gene, even though it is even more cryptic). . I do not think there is a super easy way to find a short and obvious name for the column, but I think using fraction_group and fraction_rest (or fraction_ref) are good enough. Reason I suggest fraction_rest is because ""rest"" is the default value of the reference argument in rank_genes_groups. We can also make it `f""fraction_{reference}""`, if this is the way it is i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1388
https://github.com/scverse/scanpy/pull/1388:1659,modifiability,pac,package,1659,"nt limiting the function to a specific category in a _mandatory_ way in any DE-related function e.g. we don't have a mandatory group argument in sc.tl.rank_genes_groups, right? Why? Because it is more common to do with all groups. We could have made sc.tl.rank_genes_groups also like existing API of sc.get.rank_genes_groups_df, but we didn't. Another example is the groups argument in sc.pl.umap, it's more common to look at all groups in UMAPs rather than a specific group, so not mandatory. So maybe it makes sense to ask the question other way around: why do we limit it to single group here in sc.get.rank_genes_groups_df and why is it the right API? > ### New column name. > . > I don't think `fraction_group` and `fraction_rest` are obvious either. `fraction_expressed_group` and `fraction_expressed_rest`? Those are a bit wordy. Also, is `rest` the right term? > . > The idea behind this function was to get the basic summary of differential expression that you'd expect to get from a DE package. Do other packages provide this value? If so, what do they call this column? Again, mean expression and fraction of cells ""expressing"" a gene is expected by A LOT OF people running DE in any single cell tool. I am not asking for these features randomly or only bcs I personally need them, it is indeed needed by many people. Seurat users expect them too. scanpy API should prioritize the users a little bit more, in my opinion. If it sounds too subjective let's make an experiment: let's make a poll on Twitter using the scanpy account ask people if they think fractions are needed or not and how they feel about these names :). Main suggestion was indeed influenced by Seurat (see https://github.com/theislab/scanpy/pull/1081#issuecomment-595789816). But I don't really mind what the name will be. I think most people in the field are familiar with these names (even mu and alpha are used a lot for mean expression and fraction of cells expressing a gene, even though it is even more cryptic). .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1388
https://github.com/scverse/scanpy/pull/1388:1677,modifiability,pac,packages,1677,"nction to a specific category in a _mandatory_ way in any DE-related function e.g. we don't have a mandatory group argument in sc.tl.rank_genes_groups, right? Why? Because it is more common to do with all groups. We could have made sc.tl.rank_genes_groups also like existing API of sc.get.rank_genes_groups_df, but we didn't. Another example is the groups argument in sc.pl.umap, it's more common to look at all groups in UMAPs rather than a specific group, so not mandatory. So maybe it makes sense to ask the question other way around: why do we limit it to single group here in sc.get.rank_genes_groups_df and why is it the right API? > ### New column name. > . > I don't think `fraction_group` and `fraction_rest` are obvious either. `fraction_expressed_group` and `fraction_expressed_rest`? Those are a bit wordy. Also, is `rest` the right term? > . > The idea behind this function was to get the basic summary of differential expression that you'd expect to get from a DE package. Do other packages provide this value? If so, what do they call this column? Again, mean expression and fraction of cells ""expressing"" a gene is expected by A LOT OF people running DE in any single cell tool. I am not asking for these features randomly or only bcs I personally need them, it is indeed needed by many people. Seurat users expect them too. scanpy API should prioritize the users a little bit more, in my opinion. If it sounds too subjective let's make an experiment: let's make a poll on Twitter using the scanpy account ask people if they think fractions are needed or not and how they feel about these names :). Main suggestion was indeed influenced by Seurat (see https://github.com/theislab/scanpy/pull/1081#issuecomment-595789816). But I don't really mind what the name will be. I think most people in the field are familiar with these names (even mu and alpha are used a lot for mean expression and fraction of cells expressing a gene, even though it is even more cryptic). . I do not think th",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1388
https://github.com/scverse/scanpy/pull/1388:472,performance,time,time,472,"> ### `group` argument. > . > I'm not sure I like the group argument accepting multiple groups. Could that get removed? If you'd like to keep it, why is this the right api? > . I wanna keep it this way, if possible. There are two reasons I can think of:. 1) When I want to save the results of DE (e.g. as a table for a publication or to send to collaborators), I always need to loop over all categories which is not so hard but gets super annoying since I need it all the time. This is something almost all scanpy users I know also need, because they too need a table for DE results with all groups in it in almost every project. . 2) We don't have such an argument limiting the function to a specific category in a _mandatory_ way in any DE-related function e.g. we don't have a mandatory group argument in sc.tl.rank_genes_groups, right? Why? Because it is more common to do with all groups. We could have made sc.tl.rank_genes_groups also like existing API of sc.get.rank_genes_groups_df, but we didn't. Another example is the groups argument in sc.pl.umap, it's more common to look at all groups in UMAPs rather than a specific group, so not mandatory. So maybe it makes sense to ask the question other way around: why do we limit it to single group here in sc.get.rank_genes_groups_df and why is it the right API? > ### New column name. > . > I don't think `fraction_group` and `fraction_rest` are obvious either. `fraction_expressed_group` and `fraction_expressed_rest`? Those are a bit wordy. Also, is `rest` the right term? > . > The idea behind this function was to get the basic summary of differential expression that you'd expect to get from a DE package. Do other packages provide this value? If so, what do they call this column? Again, mean expression and fraction of cells ""expressing"" a gene is expected by A LOT OF people running DE in any single cell tool. I am not asking for these features randomly or only bcs I personally need them, it is indeed needed by many people. Seurat u",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1388
https://github.com/scverse/scanpy/pull/1388:3394,safety,log,logistic,3394,"o other packages provide this value? If so, what do they call this column? Again, mean expression and fraction of cells ""expressing"" a gene is expected by A LOT OF people running DE in any single cell tool. I am not asking for these features randomly or only bcs I personally need them, it is indeed needed by many people. Seurat users expect them too. scanpy API should prioritize the users a little bit more, in my opinion. If it sounds too subjective let's make an experiment: let's make a poll on Twitter using the scanpy account ask people if they think fractions are needed or not and how they feel about these names :). Main suggestion was indeed influenced by Seurat (see https://github.com/theislab/scanpy/pull/1081#issuecomment-595789816). But I don't really mind what the name will be. I think most people in the field are familiar with these names (even mu and alpha are used a lot for mean expression and fraction of cells expressing a gene, even though it is even more cryptic). . I do not think there is a super easy way to find a short and obvious name for the column, but I think using fraction_group and fraction_rest (or fraction_ref) are good enough. Reason I suggest fraction_rest is because ""rest"" is the default value of the reference argument in rank_genes_groups. We can also make it `f""fraction_{reference}""`, if this is the way it is implemented. Speaking of the column names, for example, do you think `score` is really obvious :) Try to ask a few regular scanpy users what `score` means in the DE results we generate. Even our documentation is wrong: `Structured array to be indexed by group id storing the z-score underlying the computation of a p-value for each gene for each group.` It is the logistic regression beta coef for logreg and t-statistic for t-test, it's not z-score at all... . So, even if the column name is not obvious, I think it's ok to explain it properly in the documentation and for `fraction_group`, it is easy (also easier than score) to explain.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1388
https://github.com/scverse/scanpy/pull/1388:3428,safety,log,logreg,3428,"o other packages provide this value? If so, what do they call this column? Again, mean expression and fraction of cells ""expressing"" a gene is expected by A LOT OF people running DE in any single cell tool. I am not asking for these features randomly or only bcs I personally need them, it is indeed needed by many people. Seurat users expect them too. scanpy API should prioritize the users a little bit more, in my opinion. If it sounds too subjective let's make an experiment: let's make a poll on Twitter using the scanpy account ask people if they think fractions are needed or not and how they feel about these names :). Main suggestion was indeed influenced by Seurat (see https://github.com/theislab/scanpy/pull/1081#issuecomment-595789816). But I don't really mind what the name will be. I think most people in the field are familiar with these names (even mu and alpha are used a lot for mean expression and fraction of cells expressing a gene, even though it is even more cryptic). . I do not think there is a super easy way to find a short and obvious name for the column, but I think using fraction_group and fraction_rest (or fraction_ref) are good enough. Reason I suggest fraction_rest is because ""rest"" is the default value of the reference argument in rank_genes_groups. We can also make it `f""fraction_{reference}""`, if this is the way it is implemented. Speaking of the column names, for example, do you think `score` is really obvious :) Try to ask a few regular scanpy users what `score` means in the DE results we generate. Even our documentation is wrong: `Structured array to be indexed by group id storing the z-score underlying the computation of a p-value for each gene for each group.` It is the logistic regression beta coef for logreg and t-statistic for t-test, it's not z-score at all... . So, even if the column name is not obvious, I think it's ok to explain it properly in the documentation and for `fraction_group`, it is easy (also easier than score) to explain.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1388
https://github.com/scverse/scanpy/pull/1388:3457,safety,test,test,3457,"o other packages provide this value? If so, what do they call this column? Again, mean expression and fraction of cells ""expressing"" a gene is expected by A LOT OF people running DE in any single cell tool. I am not asking for these features randomly or only bcs I personally need them, it is indeed needed by many people. Seurat users expect them too. scanpy API should prioritize the users a little bit more, in my opinion. If it sounds too subjective let's make an experiment: let's make a poll on Twitter using the scanpy account ask people if they think fractions are needed or not and how they feel about these names :). Main suggestion was indeed influenced by Seurat (see https://github.com/theislab/scanpy/pull/1081#issuecomment-595789816). But I don't really mind what the name will be. I think most people in the field are familiar with these names (even mu and alpha are used a lot for mean expression and fraction of cells expressing a gene, even though it is even more cryptic). . I do not think there is a super easy way to find a short and obvious name for the column, but I think using fraction_group and fraction_rest (or fraction_ref) are good enough. Reason I suggest fraction_rest is because ""rest"" is the default value of the reference argument in rank_genes_groups. We can also make it `f""fraction_{reference}""`, if this is the way it is implemented. Speaking of the column names, for example, do you think `score` is really obvious :) Try to ask a few regular scanpy users what `score` means in the DE results we generate. Even our documentation is wrong: `Structured array to be indexed by group id storing the z-score underlying the computation of a p-value for each gene for each group.` It is the logistic regression beta coef for logreg and t-statistic for t-test, it's not z-score at all... . So, even if the column name is not obvious, I think it's ok to explain it properly in the documentation and for `fraction_group`, it is easy (also easier than score) to explain.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1388
https://github.com/scverse/scanpy/pull/1388:3394,security,log,logistic,3394,"o other packages provide this value? If so, what do they call this column? Again, mean expression and fraction of cells ""expressing"" a gene is expected by A LOT OF people running DE in any single cell tool. I am not asking for these features randomly or only bcs I personally need them, it is indeed needed by many people. Seurat users expect them too. scanpy API should prioritize the users a little bit more, in my opinion. If it sounds too subjective let's make an experiment: let's make a poll on Twitter using the scanpy account ask people if they think fractions are needed or not and how they feel about these names :). Main suggestion was indeed influenced by Seurat (see https://github.com/theislab/scanpy/pull/1081#issuecomment-595789816). But I don't really mind what the name will be. I think most people in the field are familiar with these names (even mu and alpha are used a lot for mean expression and fraction of cells expressing a gene, even though it is even more cryptic). . I do not think there is a super easy way to find a short and obvious name for the column, but I think using fraction_group and fraction_rest (or fraction_ref) are good enough. Reason I suggest fraction_rest is because ""rest"" is the default value of the reference argument in rank_genes_groups. We can also make it `f""fraction_{reference}""`, if this is the way it is implemented. Speaking of the column names, for example, do you think `score` is really obvious :) Try to ask a few regular scanpy users what `score` means in the DE results we generate. Even our documentation is wrong: `Structured array to be indexed by group id storing the z-score underlying the computation of a p-value for each gene for each group.` It is the logistic regression beta coef for logreg and t-statistic for t-test, it's not z-score at all... . So, even if the column name is not obvious, I think it's ok to explain it properly in the documentation and for `fraction_group`, it is easy (also easier than score) to explain.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1388
https://github.com/scverse/scanpy/pull/1388:3428,security,log,logreg,3428,"o other packages provide this value? If so, what do they call this column? Again, mean expression and fraction of cells ""expressing"" a gene is expected by A LOT OF people running DE in any single cell tool. I am not asking for these features randomly or only bcs I personally need them, it is indeed needed by many people. Seurat users expect them too. scanpy API should prioritize the users a little bit more, in my opinion. If it sounds too subjective let's make an experiment: let's make a poll on Twitter using the scanpy account ask people if they think fractions are needed or not and how they feel about these names :). Main suggestion was indeed influenced by Seurat (see https://github.com/theislab/scanpy/pull/1081#issuecomment-595789816). But I don't really mind what the name will be. I think most people in the field are familiar with these names (even mu and alpha are used a lot for mean expression and fraction of cells expressing a gene, even though it is even more cryptic). . I do not think there is a super easy way to find a short and obvious name for the column, but I think using fraction_group and fraction_rest (or fraction_ref) are good enough. Reason I suggest fraction_rest is because ""rest"" is the default value of the reference argument in rank_genes_groups. We can also make it `f""fraction_{reference}""`, if this is the way it is implemented. Speaking of the column names, for example, do you think `score` is really obvious :) Try to ask a few regular scanpy users what `score` means in the DE results we generate. Even our documentation is wrong: `Structured array to be indexed by group id storing the z-score underlying the computation of a p-value for each gene for each group.` It is the logistic regression beta coef for logreg and t-statistic for t-test, it's not z-score at all... . So, even if the column name is not obvious, I think it's ok to explain it properly in the documentation and for `fraction_group`, it is easy (also easier than score) to explain.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1388
https://github.com/scverse/scanpy/pull/1388:3394,testability,log,logistic,3394,"o other packages provide this value? If so, what do they call this column? Again, mean expression and fraction of cells ""expressing"" a gene is expected by A LOT OF people running DE in any single cell tool. I am not asking for these features randomly or only bcs I personally need them, it is indeed needed by many people. Seurat users expect them too. scanpy API should prioritize the users a little bit more, in my opinion. If it sounds too subjective let's make an experiment: let's make a poll on Twitter using the scanpy account ask people if they think fractions are needed or not and how they feel about these names :). Main suggestion was indeed influenced by Seurat (see https://github.com/theislab/scanpy/pull/1081#issuecomment-595789816). But I don't really mind what the name will be. I think most people in the field are familiar with these names (even mu and alpha are used a lot for mean expression and fraction of cells expressing a gene, even though it is even more cryptic). . I do not think there is a super easy way to find a short and obvious name for the column, but I think using fraction_group and fraction_rest (or fraction_ref) are good enough. Reason I suggest fraction_rest is because ""rest"" is the default value of the reference argument in rank_genes_groups. We can also make it `f""fraction_{reference}""`, if this is the way it is implemented. Speaking of the column names, for example, do you think `score` is really obvious :) Try to ask a few regular scanpy users what `score` means in the DE results we generate. Even our documentation is wrong: `Structured array to be indexed by group id storing the z-score underlying the computation of a p-value for each gene for each group.` It is the logistic regression beta coef for logreg and t-statistic for t-test, it's not z-score at all... . So, even if the column name is not obvious, I think it's ok to explain it properly in the documentation and for `fraction_group`, it is easy (also easier than score) to explain.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1388
https://github.com/scverse/scanpy/pull/1388:3403,testability,regress,regression,3403,"o other packages provide this value? If so, what do they call this column? Again, mean expression and fraction of cells ""expressing"" a gene is expected by A LOT OF people running DE in any single cell tool. I am not asking for these features randomly or only bcs I personally need them, it is indeed needed by many people. Seurat users expect them too. scanpy API should prioritize the users a little bit more, in my opinion. If it sounds too subjective let's make an experiment: let's make a poll on Twitter using the scanpy account ask people if they think fractions are needed or not and how they feel about these names :). Main suggestion was indeed influenced by Seurat (see https://github.com/theislab/scanpy/pull/1081#issuecomment-595789816). But I don't really mind what the name will be. I think most people in the field are familiar with these names (even mu and alpha are used a lot for mean expression and fraction of cells expressing a gene, even though it is even more cryptic). . I do not think there is a super easy way to find a short and obvious name for the column, but I think using fraction_group and fraction_rest (or fraction_ref) are good enough. Reason I suggest fraction_rest is because ""rest"" is the default value of the reference argument in rank_genes_groups. We can also make it `f""fraction_{reference}""`, if this is the way it is implemented. Speaking of the column names, for example, do you think `score` is really obvious :) Try to ask a few regular scanpy users what `score` means in the DE results we generate. Even our documentation is wrong: `Structured array to be indexed by group id storing the z-score underlying the computation of a p-value for each gene for each group.` It is the logistic regression beta coef for logreg and t-statistic for t-test, it's not z-score at all... . So, even if the column name is not obvious, I think it's ok to explain it properly in the documentation and for `fraction_group`, it is easy (also easier than score) to explain.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1388
https://github.com/scverse/scanpy/pull/1388:3428,testability,log,logreg,3428,"o other packages provide this value? If so, what do they call this column? Again, mean expression and fraction of cells ""expressing"" a gene is expected by A LOT OF people running DE in any single cell tool. I am not asking for these features randomly or only bcs I personally need them, it is indeed needed by many people. Seurat users expect them too. scanpy API should prioritize the users a little bit more, in my opinion. If it sounds too subjective let's make an experiment: let's make a poll on Twitter using the scanpy account ask people if they think fractions are needed or not and how they feel about these names :). Main suggestion was indeed influenced by Seurat (see https://github.com/theislab/scanpy/pull/1081#issuecomment-595789816). But I don't really mind what the name will be. I think most people in the field are familiar with these names (even mu and alpha are used a lot for mean expression and fraction of cells expressing a gene, even though it is even more cryptic). . I do not think there is a super easy way to find a short and obvious name for the column, but I think using fraction_group and fraction_rest (or fraction_ref) are good enough. Reason I suggest fraction_rest is because ""rest"" is the default value of the reference argument in rank_genes_groups. We can also make it `f""fraction_{reference}""`, if this is the way it is implemented. Speaking of the column names, for example, do you think `score` is really obvious :) Try to ask a few regular scanpy users what `score` means in the DE results we generate. Even our documentation is wrong: `Structured array to be indexed by group id storing the z-score underlying the computation of a p-value for each gene for each group.` It is the logistic regression beta coef for logreg and t-statistic for t-test, it's not z-score at all... . So, even if the column name is not obvious, I think it's ok to explain it properly in the documentation and for `fraction_group`, it is easy (also easier than score) to explain.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1388
https://github.com/scverse/scanpy/pull/1388:3457,testability,test,test,3457,"o other packages provide this value? If so, what do they call this column? Again, mean expression and fraction of cells ""expressing"" a gene is expected by A LOT OF people running DE in any single cell tool. I am not asking for these features randomly or only bcs I personally need them, it is indeed needed by many people. Seurat users expect them too. scanpy API should prioritize the users a little bit more, in my opinion. If it sounds too subjective let's make an experiment: let's make a poll on Twitter using the scanpy account ask people if they think fractions are needed or not and how they feel about these names :). Main suggestion was indeed influenced by Seurat (see https://github.com/theislab/scanpy/pull/1081#issuecomment-595789816). But I don't really mind what the name will be. I think most people in the field are familiar with these names (even mu and alpha are used a lot for mean expression and fraction of cells expressing a gene, even though it is even more cryptic). . I do not think there is a super easy way to find a short and obvious name for the column, but I think using fraction_group and fraction_rest (or fraction_ref) are good enough. Reason I suggest fraction_rest is because ""rest"" is the default value of the reference argument in rank_genes_groups. We can also make it `f""fraction_{reference}""`, if this is the way it is implemented. Speaking of the column names, for example, do you think `score` is really obvious :) Try to ask a few regular scanpy users what `score` means in the DE results we generate. Even our documentation is wrong: `Structured array to be indexed by group id storing the z-score underlying the computation of a p-value for each gene for each group.` It is the logistic regression beta coef for logreg and t-statistic for t-test, it's not z-score at all... . So, even if the column name is not obvious, I think it's ok to explain it properly in the documentation and for `fraction_group`, it is easy (also easier than score) to explain.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1388
https://github.com/scverse/scanpy/pull/1388:514,usability,user,users,514,"> ### `group` argument. > . > I'm not sure I like the group argument accepting multiple groups. Could that get removed? If you'd like to keep it, why is this the right api? > . I wanna keep it this way, if possible. There are two reasons I can think of:. 1) When I want to save the results of DE (e.g. as a table for a publication or to send to collaborators), I always need to loop over all categories which is not so hard but gets super annoying since I need it all the time. This is something almost all scanpy users I know also need, because they too need a table for DE results with all groups in it in almost every project. . 2) We don't have such an argument limiting the function to a specific category in a _mandatory_ way in any DE-related function e.g. we don't have a mandatory group argument in sc.tl.rank_genes_groups, right? Why? Because it is more common to do with all groups. We could have made sc.tl.rank_genes_groups also like existing API of sc.get.rank_genes_groups_df, but we didn't. Another example is the groups argument in sc.pl.umap, it's more common to look at all groups in UMAPs rather than a specific group, so not mandatory. So maybe it makes sense to ask the question other way around: why do we limit it to single group here in sc.get.rank_genes_groups_df and why is it the right API? > ### New column name. > . > I don't think `fraction_group` and `fraction_rest` are obvious either. `fraction_expressed_group` and `fraction_expressed_rest`? Those are a bit wordy. Also, is `rest` the right term? > . > The idea behind this function was to get the basic summary of differential expression that you'd expect to get from a DE package. Do other packages provide this value? If so, what do they call this column? Again, mean expression and fraction of cells ""expressing"" a gene is expected by A LOT OF people running DE in any single cell tool. I am not asking for these features randomly or only bcs I personally need them, it is indeed needed by many people. Seurat u",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1388
https://github.com/scverse/scanpy/pull/1388:1870,usability,tool,tool,1870,"o do with all groups. We could have made sc.tl.rank_genes_groups also like existing API of sc.get.rank_genes_groups_df, but we didn't. Another example is the groups argument in sc.pl.umap, it's more common to look at all groups in UMAPs rather than a specific group, so not mandatory. So maybe it makes sense to ask the question other way around: why do we limit it to single group here in sc.get.rank_genes_groups_df and why is it the right API? > ### New column name. > . > I don't think `fraction_group` and `fraction_rest` are obvious either. `fraction_expressed_group` and `fraction_expressed_rest`? Those are a bit wordy. Also, is `rest` the right term? > . > The idea behind this function was to get the basic summary of differential expression that you'd expect to get from a DE package. Do other packages provide this value? If so, what do they call this column? Again, mean expression and fraction of cells ""expressing"" a gene is expected by A LOT OF people running DE in any single cell tool. I am not asking for these features randomly or only bcs I personally need them, it is indeed needed by many people. Seurat users expect them too. scanpy API should prioritize the users a little bit more, in my opinion. If it sounds too subjective let's make an experiment: let's make a poll on Twitter using the scanpy account ask people if they think fractions are needed or not and how they feel about these names :). Main suggestion was indeed influenced by Seurat (see https://github.com/theislab/scanpy/pull/1081#issuecomment-595789816). But I don't really mind what the name will be. I think most people in the field are familiar with these names (even mu and alpha are used a lot for mean expression and fraction of cells expressing a gene, even though it is even more cryptic). . I do not think there is a super easy way to find a short and obvious name for the column, but I think using fraction_group and fraction_rest (or fraction_ref) are good enough. Reason I suggest fraction_rest i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1388
https://github.com/scverse/scanpy/pull/1388:1934,usability,person,personally,1934,"so like existing API of sc.get.rank_genes_groups_df, but we didn't. Another example is the groups argument in sc.pl.umap, it's more common to look at all groups in UMAPs rather than a specific group, so not mandatory. So maybe it makes sense to ask the question other way around: why do we limit it to single group here in sc.get.rank_genes_groups_df and why is it the right API? > ### New column name. > . > I don't think `fraction_group` and `fraction_rest` are obvious either. `fraction_expressed_group` and `fraction_expressed_rest`? Those are a bit wordy. Also, is `rest` the right term? > . > The idea behind this function was to get the basic summary of differential expression that you'd expect to get from a DE package. Do other packages provide this value? If so, what do they call this column? Again, mean expression and fraction of cells ""expressing"" a gene is expected by A LOT OF people running DE in any single cell tool. I am not asking for these features randomly or only bcs I personally need them, it is indeed needed by many people. Seurat users expect them too. scanpy API should prioritize the users a little bit more, in my opinion. If it sounds too subjective let's make an experiment: let's make a poll on Twitter using the scanpy account ask people if they think fractions are needed or not and how they feel about these names :). Main suggestion was indeed influenced by Seurat (see https://github.com/theislab/scanpy/pull/1081#issuecomment-595789816). But I don't really mind what the name will be. I think most people in the field are familiar with these names (even mu and alpha are used a lot for mean expression and fraction of cells expressing a gene, even though it is even more cryptic). . I do not think there is a super easy way to find a short and obvious name for the column, but I think using fraction_group and fraction_rest (or fraction_ref) are good enough. Reason I suggest fraction_rest is because ""rest"" is the default value of the reference argument in ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1388
https://github.com/scverse/scanpy/pull/1388:1999,usability,user,users,1999,"n't. Another example is the groups argument in sc.pl.umap, it's more common to look at all groups in UMAPs rather than a specific group, so not mandatory. So maybe it makes sense to ask the question other way around: why do we limit it to single group here in sc.get.rank_genes_groups_df and why is it the right API? > ### New column name. > . > I don't think `fraction_group` and `fraction_rest` are obvious either. `fraction_expressed_group` and `fraction_expressed_rest`? Those are a bit wordy. Also, is `rest` the right term? > . > The idea behind this function was to get the basic summary of differential expression that you'd expect to get from a DE package. Do other packages provide this value? If so, what do they call this column? Again, mean expression and fraction of cells ""expressing"" a gene is expected by A LOT OF people running DE in any single cell tool. I am not asking for these features randomly or only bcs I personally need them, it is indeed needed by many people. Seurat users expect them too. scanpy API should prioritize the users a little bit more, in my opinion. If it sounds too subjective let's make an experiment: let's make a poll on Twitter using the scanpy account ask people if they think fractions are needed or not and how they feel about these names :). Main suggestion was indeed influenced by Seurat (see https://github.com/theislab/scanpy/pull/1081#issuecomment-595789816). But I don't really mind what the name will be. I think most people in the field are familiar with these names (even mu and alpha are used a lot for mean expression and fraction of cells expressing a gene, even though it is even more cryptic). . I do not think there is a super easy way to find a short and obvious name for the column, but I think using fraction_group and fraction_rest (or fraction_ref) are good enough. Reason I suggest fraction_rest is because ""rest"" is the default value of the reference argument in rank_genes_groups. We can also make it `f""fraction_{reference}""",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1388
https://github.com/scverse/scanpy/pull/1388:2055,usability,user,users,2055,"p, it's more common to look at all groups in UMAPs rather than a specific group, so not mandatory. So maybe it makes sense to ask the question other way around: why do we limit it to single group here in sc.get.rank_genes_groups_df and why is it the right API? > ### New column name. > . > I don't think `fraction_group` and `fraction_rest` are obvious either. `fraction_expressed_group` and `fraction_expressed_rest`? Those are a bit wordy. Also, is `rest` the right term? > . > The idea behind this function was to get the basic summary of differential expression that you'd expect to get from a DE package. Do other packages provide this value? If so, what do they call this column? Again, mean expression and fraction of cells ""expressing"" a gene is expected by A LOT OF people running DE in any single cell tool. I am not asking for these features randomly or only bcs I personally need them, it is indeed needed by many people. Seurat users expect them too. scanpy API should prioritize the users a little bit more, in my opinion. If it sounds too subjective let's make an experiment: let's make a poll on Twitter using the scanpy account ask people if they think fractions are needed or not and how they feel about these names :). Main suggestion was indeed influenced by Seurat (see https://github.com/theislab/scanpy/pull/1081#issuecomment-595789816). But I don't really mind what the name will be. I think most people in the field are familiar with these names (even mu and alpha are used a lot for mean expression and fraction of cells expressing a gene, even though it is even more cryptic). . I do not think there is a super easy way to find a short and obvious name for the column, but I think using fraction_group and fraction_rest (or fraction_ref) are good enough. Reason I suggest fraction_rest is because ""rest"" is the default value of the reference argument in rank_genes_groups. We can also make it `f""fraction_{reference}""`, if this is the way it is implemented. Speaking of the",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1388
https://github.com/scverse/scanpy/pull/1388:3160,usability,user,users,3160,"o other packages provide this value? If so, what do they call this column? Again, mean expression and fraction of cells ""expressing"" a gene is expected by A LOT OF people running DE in any single cell tool. I am not asking for these features randomly or only bcs I personally need them, it is indeed needed by many people. Seurat users expect them too. scanpy API should prioritize the users a little bit more, in my opinion. If it sounds too subjective let's make an experiment: let's make a poll on Twitter using the scanpy account ask people if they think fractions are needed or not and how they feel about these names :). Main suggestion was indeed influenced by Seurat (see https://github.com/theislab/scanpy/pull/1081#issuecomment-595789816). But I don't really mind what the name will be. I think most people in the field are familiar with these names (even mu and alpha are used a lot for mean expression and fraction of cells expressing a gene, even though it is even more cryptic). . I do not think there is a super easy way to find a short and obvious name for the column, but I think using fraction_group and fraction_rest (or fraction_ref) are good enough. Reason I suggest fraction_rest is because ""rest"" is the default value of the reference argument in rank_genes_groups. We can also make it `f""fraction_{reference}""`, if this is the way it is implemented. Speaking of the column names, for example, do you think `score` is really obvious :) Try to ask a few regular scanpy users what `score` means in the DE results we generate. Even our documentation is wrong: `Structured array to be indexed by group id storing the z-score underlying the computation of a p-value for each gene for each group.` It is the logistic regression beta coef for logreg and t-statistic for t-test, it's not z-score at all... . So, even if the column name is not obvious, I think it's ok to explain it properly in the documentation and for `fraction_group`, it is easy (also easier than score) to explain.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1388
https://github.com/scverse/scanpy/pull/1388:3225,usability,document,documentation,3225,"o other packages provide this value? If so, what do they call this column? Again, mean expression and fraction of cells ""expressing"" a gene is expected by A LOT OF people running DE in any single cell tool. I am not asking for these features randomly or only bcs I personally need them, it is indeed needed by many people. Seurat users expect them too. scanpy API should prioritize the users a little bit more, in my opinion. If it sounds too subjective let's make an experiment: let's make a poll on Twitter using the scanpy account ask people if they think fractions are needed or not and how they feel about these names :). Main suggestion was indeed influenced by Seurat (see https://github.com/theislab/scanpy/pull/1081#issuecomment-595789816). But I don't really mind what the name will be. I think most people in the field are familiar with these names (even mu and alpha are used a lot for mean expression and fraction of cells expressing a gene, even though it is even more cryptic). . I do not think there is a super easy way to find a short and obvious name for the column, but I think using fraction_group and fraction_rest (or fraction_ref) are good enough. Reason I suggest fraction_rest is because ""rest"" is the default value of the reference argument in rank_genes_groups. We can also make it `f""fraction_{reference}""`, if this is the way it is implemented. Speaking of the column names, for example, do you think `score` is really obvious :) Try to ask a few regular scanpy users what `score` means in the DE results we generate. Even our documentation is wrong: `Structured array to be indexed by group id storing the z-score underlying the computation of a p-value for each gene for each group.` It is the logistic regression beta coef for logreg and t-statistic for t-test, it's not z-score at all... . So, even if the column name is not obvious, I think it's ok to explain it properly in the documentation and for `fraction_group`, it is easy (also easier than score) to explain.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1388
https://github.com/scverse/scanpy/pull/1388:3582,usability,document,documentation,3582,"o other packages provide this value? If so, what do they call this column? Again, mean expression and fraction of cells ""expressing"" a gene is expected by A LOT OF people running DE in any single cell tool. I am not asking for these features randomly or only bcs I personally need them, it is indeed needed by many people. Seurat users expect them too. scanpy API should prioritize the users a little bit more, in my opinion. If it sounds too subjective let's make an experiment: let's make a poll on Twitter using the scanpy account ask people if they think fractions are needed or not and how they feel about these names :). Main suggestion was indeed influenced by Seurat (see https://github.com/theislab/scanpy/pull/1081#issuecomment-595789816). But I don't really mind what the name will be. I think most people in the field are familiar with these names (even mu and alpha are used a lot for mean expression and fraction of cells expressing a gene, even though it is even more cryptic). . I do not think there is a super easy way to find a short and obvious name for the column, but I think using fraction_group and fraction_rest (or fraction_ref) are good enough. Reason I suggest fraction_rest is because ""rest"" is the default value of the reference argument in rank_genes_groups. We can also make it `f""fraction_{reference}""`, if this is the way it is implemented. Speaking of the column names, for example, do you think `score` is really obvious :) Try to ask a few regular scanpy users what `score` means in the DE results we generate. Even our documentation is wrong: `Structured array to be indexed by group id storing the z-score underlying the computation of a p-value for each gene for each group.` It is the logistic regression beta coef for logreg and t-statistic for t-test, it's not z-score at all... . So, even if the column name is not obvious, I think it's ok to explain it properly in the documentation and for `fraction_group`, it is easy (also easier than score) to explain.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1388
https://github.com/scverse/scanpy/pull/1388:0,availability,Ping,Ping,0,"Ping @ivirshup. I wanna merge this if possible, it'd be great if you can have a look at the reply above. Together with this PR and https://github.com/theislab/scanpy/pull/1488, it would be great to do gene-set enrichment of all cell types at once without loops \o/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1388
https://github.com/scverse/scanpy/pull/1388:1459,availability,consist,consistent,1459,"groups will be used. But you have to pass something. This means you can't just forget to pass a parameter and then open a bug report about how genes are showing up multiple times in your DE results. You had to opt in to either behavior. OK, sounds good. Done. > . > ## New column name. > I wasn't clear here. We should definitely include these values. I just think the names could be better and was wondering what other packages use as column names for these values. > . > AFAICT there is no agreed upon way to name these. Seems weird, since you'd think there'd be a technical name for ""when logFC is positive the xxxx group had higher expression"". > . > I would go for `f""fraction_{reference}""`, but then you can't pass the output directly to a plotting function without also passing the value for `reference`. > . > How about:. > . > `pct_nz_group` and `pct_nz_reference`/ `pct_nz_ref`? I could also go for `lhs`/ `rhs` instead of `group`/ `reference`, and `fraction` instead of `pct`. But `group`/`reference` is consistent with `rank_genes_groups` and `pct` is consistent with `calculate_qc_metrics`. I like having `nz` in there since otherwise it's not super clear what fraction we're talking about. Could be fraction of total expression, or something about proportion of the dataset? This way it's more clear in the table you show to a collaborator. Sounds good, done. > . > I agree `score` is a bit weird. Maybe `statistic` is a better choice? @davidsebfischer could probably be more authoritative on this. And yeah, we should change those `z-score` docs. Shall we change this in this function or in sc.tl.rank_genes_groups? I feel like renaming it here is not the best way. > . > ### Performance. > General question about performance. Is this faster than calling the previous function separately on each group, then concatenating the results? I think so:. <img width=""737"" alt=""image"" src=""https://user-images.githubusercontent.com/1140359/101388354-d29d4b00-388d-11eb-98b3-f91cf03ac84c.png"">.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1388
https://github.com/scverse/scanpy/pull/1388:1508,availability,consist,consistent,1508,"groups will be used. But you have to pass something. This means you can't just forget to pass a parameter and then open a bug report about how genes are showing up multiple times in your DE results. You had to opt in to either behavior. OK, sounds good. Done. > . > ## New column name. > I wasn't clear here. We should definitely include these values. I just think the names could be better and was wondering what other packages use as column names for these values. > . > AFAICT there is no agreed upon way to name these. Seems weird, since you'd think there'd be a technical name for ""when logFC is positive the xxxx group had higher expression"". > . > I would go for `f""fraction_{reference}""`, but then you can't pass the output directly to a plotting function without also passing the value for `reference`. > . > How about:. > . > `pct_nz_group` and `pct_nz_reference`/ `pct_nz_ref`? I could also go for `lhs`/ `rhs` instead of `group`/ `reference`, and `fraction` instead of `pct`. But `group`/`reference` is consistent with `rank_genes_groups` and `pct` is consistent with `calculate_qc_metrics`. I like having `nz` in there since otherwise it's not super clear what fraction we're talking about. Could be fraction of total expression, or something about proportion of the dataset? This way it's more clear in the table you show to a collaborator. Sounds good, done. > . > I agree `score` is a bit weird. Maybe `statistic` is a better choice? @davidsebfischer could probably be more authoritative on this. And yeah, we should change those `z-score` docs. Shall we change this in this function or in sc.tl.rank_genes_groups? I feel like renaming it here is not the best way. > . > ### Performance. > General question about performance. Is this faster than calling the previous function separately on each group, then concatenating the results? I think so:. <img width=""737"" alt=""image"" src=""https://user-images.githubusercontent.com/1140359/101388354-d29d4b00-388d-11eb-98b3-f91cf03ac84c.png"">.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1388
https://github.com/scverse/scanpy/pull/1388:125,deployability,stack,stacked,125,"> ## `groups`. > I still don't think this is quite right. When I'm sharing DE results, it's not going to be every comparison stacked together in one table. It would be a table per comparison, either in separate files or in a spreadsheet with a page per comparison. > . > But how about this for a compromise, `groups` stays a required argument. You can pass a list of groups, and a groups column will be added. You can also pass `None`, and all groups will be used. But you have to pass something. This means you can't just forget to pass a parameter and then open a bug report about how genes are showing up multiple times in your DE results. You had to opt in to either behavior. OK, sounds good. Done. > . > ## New column name. > I wasn't clear here. We should definitely include these values. I just think the names could be better and was wondering what other packages use as column names for these values. > . > AFAICT there is no agreed upon way to name these. Seems weird, since you'd think there'd be a technical name for ""when logFC is positive the xxxx group had higher expression"". > . > I would go for `f""fraction_{reference}""`, but then you can't pass the output directly to a plotting function without also passing the value for `reference`. > . > How about:. > . > `pct_nz_group` and `pct_nz_reference`/ `pct_nz_ref`? I could also go for `lhs`/ `rhs` instead of `group`/ `reference`, and `fraction` instead of `pct`. But `group`/`reference` is consistent with `rank_genes_groups` and `pct` is consistent with `calculate_qc_metrics`. I like having `nz` in there since otherwise it's not super clear what fraction we're talking about. Could be fraction of total expression, or something about proportion of the dataset? This way it's more clear in the table you show to a collaborator. Sounds good, done. > . > I agree `score` is a bit weird. Maybe `statistic` is a better choice? @davidsebfischer could probably be more authoritative on this. And yeah, we should change those `z-score` ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1388
https://github.com/scverse/scanpy/pull/1388:1036,deployability,log,logFC,1036,"is is quite right. When I'm sharing DE results, it's not going to be every comparison stacked together in one table. It would be a table per comparison, either in separate files or in a spreadsheet with a page per comparison. > . > But how about this for a compromise, `groups` stays a required argument. You can pass a list of groups, and a groups column will be added. You can also pass `None`, and all groups will be used. But you have to pass something. This means you can't just forget to pass a parameter and then open a bug report about how genes are showing up multiple times in your DE results. You had to opt in to either behavior. OK, sounds good. Done. > . > ## New column name. > I wasn't clear here. We should definitely include these values. I just think the names could be better and was wondering what other packages use as column names for these values. > . > AFAICT there is no agreed upon way to name these. Seems weird, since you'd think there'd be a technical name for ""when logFC is positive the xxxx group had higher expression"". > . > I would go for `f""fraction_{reference}""`, but then you can't pass the output directly to a plotting function without also passing the value for `reference`. > . > How about:. > . > `pct_nz_group` and `pct_nz_reference`/ `pct_nz_ref`? I could also go for `lhs`/ `rhs` instead of `group`/ `reference`, and `fraction` instead of `pct`. But `group`/`reference` is consistent with `rank_genes_groups` and `pct` is consistent with `calculate_qc_metrics`. I like having `nz` in there since otherwise it's not super clear what fraction we're talking about. Could be fraction of total expression, or something about proportion of the dataset? This way it's more clear in the table you show to a collaborator. Sounds good, done. > . > I agree `score` is a bit weird. Maybe `statistic` is a better choice? @davidsebfischer could probably be more authoritative on this. And yeah, we should change those `z-score` docs. Shall we change this in this func",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1388
https://github.com/scverse/scanpy/pull/1388:540,modifiability,paramet,parameter,540,"> ## `groups`. > I still don't think this is quite right. When I'm sharing DE results, it's not going to be every comparison stacked together in one table. It would be a table per comparison, either in separate files or in a spreadsheet with a page per comparison. > . > But how about this for a compromise, `groups` stays a required argument. You can pass a list of groups, and a groups column will be added. You can also pass `None`, and all groups will be used. But you have to pass something. This means you can't just forget to pass a parameter and then open a bug report about how genes are showing up multiple times in your DE results. You had to opt in to either behavior. OK, sounds good. Done. > . > ## New column name. > I wasn't clear here. We should definitely include these values. I just think the names could be better and was wondering what other packages use as column names for these values. > . > AFAICT there is no agreed upon way to name these. Seems weird, since you'd think there'd be a technical name for ""when logFC is positive the xxxx group had higher expression"". > . > I would go for `f""fraction_{reference}""`, but then you can't pass the output directly to a plotting function without also passing the value for `reference`. > . > How about:. > . > `pct_nz_group` and `pct_nz_reference`/ `pct_nz_ref`? I could also go for `lhs`/ `rhs` instead of `group`/ `reference`, and `fraction` instead of `pct`. But `group`/`reference` is consistent with `rank_genes_groups` and `pct` is consistent with `calculate_qc_metrics`. I like having `nz` in there since otherwise it's not super clear what fraction we're talking about. Could be fraction of total expression, or something about proportion of the dataset? This way it's more clear in the table you show to a collaborator. Sounds good, done. > . > I agree `score` is a bit weird. Maybe `statistic` is a better choice? @davidsebfischer could probably be more authoritative on this. And yeah, we should change those `z-score` ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1388
https://github.com/scverse/scanpy/pull/1388:864,modifiability,pac,packages,864,"> ## `groups`. > I still don't think this is quite right. When I'm sharing DE results, it's not going to be every comparison stacked together in one table. It would be a table per comparison, either in separate files or in a spreadsheet with a page per comparison. > . > But how about this for a compromise, `groups` stays a required argument. You can pass a list of groups, and a groups column will be added. You can also pass `None`, and all groups will be used. But you have to pass something. This means you can't just forget to pass a parameter and then open a bug report about how genes are showing up multiple times in your DE results. You had to opt in to either behavior. OK, sounds good. Done. > . > ## New column name. > I wasn't clear here. We should definitely include these values. I just think the names could be better and was wondering what other packages use as column names for these values. > . > AFAICT there is no agreed upon way to name these. Seems weird, since you'd think there'd be a technical name for ""when logFC is positive the xxxx group had higher expression"". > . > I would go for `f""fraction_{reference}""`, but then you can't pass the output directly to a plotting function without also passing the value for `reference`. > . > How about:. > . > `pct_nz_group` and `pct_nz_reference`/ `pct_nz_ref`? I could also go for `lhs`/ `rhs` instead of `group`/ `reference`, and `fraction` instead of `pct`. But `group`/`reference` is consistent with `rank_genes_groups` and `pct` is consistent with `calculate_qc_metrics`. I like having `nz` in there since otherwise it's not super clear what fraction we're talking about. Could be fraction of total expression, or something about proportion of the dataset? This way it's more clear in the table you show to a collaborator. Sounds good, done. > . > I agree `score` is a bit weird. Maybe `statistic` is a better choice? @davidsebfischer could probably be more authoritative on this. And yeah, we should change those `z-score` ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1388
https://github.com/scverse/scanpy/pull/1388:617,performance,time,times,617,"> ## `groups`. > I still don't think this is quite right. When I'm sharing DE results, it's not going to be every comparison stacked together in one table. It would be a table per comparison, either in separate files or in a spreadsheet with a page per comparison. > . > But how about this for a compromise, `groups` stays a required argument. You can pass a list of groups, and a groups column will be added. You can also pass `None`, and all groups will be used. But you have to pass something. This means you can't just forget to pass a parameter and then open a bug report about how genes are showing up multiple times in your DE results. You had to opt in to either behavior. OK, sounds good. Done. > . > ## New column name. > I wasn't clear here. We should definitely include these values. I just think the names could be better and was wondering what other packages use as column names for these values. > . > AFAICT there is no agreed upon way to name these. Seems weird, since you'd think there'd be a technical name for ""when logFC is positive the xxxx group had higher expression"". > . > I would go for `f""fraction_{reference}""`, but then you can't pass the output directly to a plotting function without also passing the value for `reference`. > . > How about:. > . > `pct_nz_group` and `pct_nz_reference`/ `pct_nz_ref`? I could also go for `lhs`/ `rhs` instead of `group`/ `reference`, and `fraction` instead of `pct`. But `group`/`reference` is consistent with `rank_genes_groups` and `pct` is consistent with `calculate_qc_metrics`. I like having `nz` in there since otherwise it's not super clear what fraction we're talking about. Could be fraction of total expression, or something about proportion of the dataset? This way it's more clear in the table you show to a collaborator. Sounds good, done. > . > I agree `score` is a bit weird. Maybe `statistic` is a better choice? @davidsebfischer could probably be more authoritative on this. And yeah, we should change those `z-score` ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1388
https://github.com/scverse/scanpy/pull/1388:2135,performance,Perform,Performance,2135,"groups will be used. But you have to pass something. This means you can't just forget to pass a parameter and then open a bug report about how genes are showing up multiple times in your DE results. You had to opt in to either behavior. OK, sounds good. Done. > . > ## New column name. > I wasn't clear here. We should definitely include these values. I just think the names could be better and was wondering what other packages use as column names for these values. > . > AFAICT there is no agreed upon way to name these. Seems weird, since you'd think there'd be a technical name for ""when logFC is positive the xxxx group had higher expression"". > . > I would go for `f""fraction_{reference}""`, but then you can't pass the output directly to a plotting function without also passing the value for `reference`. > . > How about:. > . > `pct_nz_group` and `pct_nz_reference`/ `pct_nz_ref`? I could also go for `lhs`/ `rhs` instead of `group`/ `reference`, and `fraction` instead of `pct`. But `group`/`reference` is consistent with `rank_genes_groups` and `pct` is consistent with `calculate_qc_metrics`. I like having `nz` in there since otherwise it's not super clear what fraction we're talking about. Could be fraction of total expression, or something about proportion of the dataset? This way it's more clear in the table you show to a collaborator. Sounds good, done. > . > I agree `score` is a bit weird. Maybe `statistic` is a better choice? @davidsebfischer could probably be more authoritative on this. And yeah, we should change those `z-score` docs. Shall we change this in this function or in sc.tl.rank_genes_groups? I feel like renaming it here is not the best way. > . > ### Performance. > General question about performance. Is this faster than calling the previous function separately on each group, then concatenating the results? I think so:. <img width=""737"" alt=""image"" src=""https://user-images.githubusercontent.com/1140359/101388354-d29d4b00-388d-11eb-98b3-f91cf03ac84c.png"">.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1388
https://github.com/scverse/scanpy/pull/1388:2173,performance,perform,performance,2173,"groups will be used. But you have to pass something. This means you can't just forget to pass a parameter and then open a bug report about how genes are showing up multiple times in your DE results. You had to opt in to either behavior. OK, sounds good. Done. > . > ## New column name. > I wasn't clear here. We should definitely include these values. I just think the names could be better and was wondering what other packages use as column names for these values. > . > AFAICT there is no agreed upon way to name these. Seems weird, since you'd think there'd be a technical name for ""when logFC is positive the xxxx group had higher expression"". > . > I would go for `f""fraction_{reference}""`, but then you can't pass the output directly to a plotting function without also passing the value for `reference`. > . > How about:. > . > `pct_nz_group` and `pct_nz_reference`/ `pct_nz_ref`? I could also go for `lhs`/ `rhs` instead of `group`/ `reference`, and `fraction` instead of `pct`. But `group`/`reference` is consistent with `rank_genes_groups` and `pct` is consistent with `calculate_qc_metrics`. I like having `nz` in there since otherwise it's not super clear what fraction we're talking about. Could be fraction of total expression, or something about proportion of the dataset? This way it's more clear in the table you show to a collaborator. Sounds good, done. > . > I agree `score` is a bit weird. Maybe `statistic` is a better choice? @davidsebfischer could probably be more authoritative on this. And yeah, we should change those `z-score` docs. Shall we change this in this function or in sc.tl.rank_genes_groups? I feel like renaming it here is not the best way. > . > ### Performance. > General question about performance. Is this faster than calling the previous function separately on each group, then concatenating the results? I think so:. <img width=""737"" alt=""image"" src=""https://user-images.githubusercontent.com/1140359/101388354-d29d4b00-388d-11eb-98b3-f91cf03ac84c.png"">.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1388
https://github.com/scverse/scanpy/pull/1388:1036,safety,log,logFC,1036,"is is quite right. When I'm sharing DE results, it's not going to be every comparison stacked together in one table. It would be a table per comparison, either in separate files or in a spreadsheet with a page per comparison. > . > But how about this for a compromise, `groups` stays a required argument. You can pass a list of groups, and a groups column will be added. You can also pass `None`, and all groups will be used. But you have to pass something. This means you can't just forget to pass a parameter and then open a bug report about how genes are showing up multiple times in your DE results. You had to opt in to either behavior. OK, sounds good. Done. > . > ## New column name. > I wasn't clear here. We should definitely include these values. I just think the names could be better and was wondering what other packages use as column names for these values. > . > AFAICT there is no agreed upon way to name these. Seems weird, since you'd think there'd be a technical name for ""when logFC is positive the xxxx group had higher expression"". > . > I would go for `f""fraction_{reference}""`, but then you can't pass the output directly to a plotting function without also passing the value for `reference`. > . > How about:. > . > `pct_nz_group` and `pct_nz_reference`/ `pct_nz_ref`? I could also go for `lhs`/ `rhs` instead of `group`/ `reference`, and `fraction` instead of `pct`. But `group`/`reference` is consistent with `rank_genes_groups` and `pct` is consistent with `calculate_qc_metrics`. I like having `nz` in there since otherwise it's not super clear what fraction we're talking about. Could be fraction of total expression, or something about proportion of the dataset? This way it's more clear in the table you show to a collaborator. Sounds good, done. > . > I agree `score` is a bit weird. Maybe `statistic` is a better choice? @davidsebfischer could probably be more authoritative on this. And yeah, we should change those `z-score` docs. Shall we change this in this func",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1388
https://github.com/scverse/scanpy/pull/1388:1036,security,log,logFC,1036,"is is quite right. When I'm sharing DE results, it's not going to be every comparison stacked together in one table. It would be a table per comparison, either in separate files or in a spreadsheet with a page per comparison. > . > But how about this for a compromise, `groups` stays a required argument. You can pass a list of groups, and a groups column will be added. You can also pass `None`, and all groups will be used. But you have to pass something. This means you can't just forget to pass a parameter and then open a bug report about how genes are showing up multiple times in your DE results. You had to opt in to either behavior. OK, sounds good. Done. > . > ## New column name. > I wasn't clear here. We should definitely include these values. I just think the names could be better and was wondering what other packages use as column names for these values. > . > AFAICT there is no agreed upon way to name these. Seems weird, since you'd think there'd be a technical name for ""when logFC is positive the xxxx group had higher expression"". > . > I would go for `f""fraction_{reference}""`, but then you can't pass the output directly to a plotting function without also passing the value for `reference`. > . > How about:. > . > `pct_nz_group` and `pct_nz_reference`/ `pct_nz_ref`? I could also go for `lhs`/ `rhs` instead of `group`/ `reference`, and `fraction` instead of `pct`. But `group`/`reference` is consistent with `rank_genes_groups` and `pct` is consistent with `calculate_qc_metrics`. I like having `nz` in there since otherwise it's not super clear what fraction we're talking about. Could be fraction of total expression, or something about proportion of the dataset? This way it's more clear in the table you show to a collaborator. Sounds good, done. > . > I agree `score` is a bit weird. Maybe `statistic` is a better choice? @davidsebfischer could probably be more authoritative on this. And yeah, we should change those `z-score` docs. Shall we change this in this func",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1388
https://github.com/scverse/scanpy/pull/1388:1934,security,auth,authoritative,1934,"groups will be used. But you have to pass something. This means you can't just forget to pass a parameter and then open a bug report about how genes are showing up multiple times in your DE results. You had to opt in to either behavior. OK, sounds good. Done. > . > ## New column name. > I wasn't clear here. We should definitely include these values. I just think the names could be better and was wondering what other packages use as column names for these values. > . > AFAICT there is no agreed upon way to name these. Seems weird, since you'd think there'd be a technical name for ""when logFC is positive the xxxx group had higher expression"". > . > I would go for `f""fraction_{reference}""`, but then you can't pass the output directly to a plotting function without also passing the value for `reference`. > . > How about:. > . > `pct_nz_group` and `pct_nz_reference`/ `pct_nz_ref`? I could also go for `lhs`/ `rhs` instead of `group`/ `reference`, and `fraction` instead of `pct`. But `group`/`reference` is consistent with `rank_genes_groups` and `pct` is consistent with `calculate_qc_metrics`. I like having `nz` in there since otherwise it's not super clear what fraction we're talking about. Could be fraction of total expression, or something about proportion of the dataset? This way it's more clear in the table you show to a collaborator. Sounds good, done. > . > I agree `score` is a bit weird. Maybe `statistic` is a better choice? @davidsebfischer could probably be more authoritative on this. And yeah, we should change those `z-score` docs. Shall we change this in this function or in sc.tl.rank_genes_groups? I feel like renaming it here is not the best way. > . > ### Performance. > General question about performance. Is this faster than calling the previous function separately on each group, then concatenating the results? I think so:. <img width=""737"" alt=""image"" src=""https://user-images.githubusercontent.com/1140359/101388354-d29d4b00-388d-11eb-98b3-f91cf03ac84c.png"">.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1388
https://github.com/scverse/scanpy/pull/1388:1036,testability,log,logFC,1036,"is is quite right. When I'm sharing DE results, it's not going to be every comparison stacked together in one table. It would be a table per comparison, either in separate files or in a spreadsheet with a page per comparison. > . > But how about this for a compromise, `groups` stays a required argument. You can pass a list of groups, and a groups column will be added. You can also pass `None`, and all groups will be used. But you have to pass something. This means you can't just forget to pass a parameter and then open a bug report about how genes are showing up multiple times in your DE results. You had to opt in to either behavior. OK, sounds good. Done. > . > ## New column name. > I wasn't clear here. We should definitely include these values. I just think the names could be better and was wondering what other packages use as column names for these values. > . > AFAICT there is no agreed upon way to name these. Seems weird, since you'd think there'd be a technical name for ""when logFC is positive the xxxx group had higher expression"". > . > I would go for `f""fraction_{reference}""`, but then you can't pass the output directly to a plotting function without also passing the value for `reference`. > . > How about:. > . > `pct_nz_group` and `pct_nz_reference`/ `pct_nz_ref`? I could also go for `lhs`/ `rhs` instead of `group`/ `reference`, and `fraction` instead of `pct`. But `group`/`reference` is consistent with `rank_genes_groups` and `pct` is consistent with `calculate_qc_metrics`. I like having `nz` in there since otherwise it's not super clear what fraction we're talking about. Could be fraction of total expression, or something about proportion of the dataset? This way it's more clear in the table you show to a collaborator. Sounds good, done. > . > I agree `score` is a bit weird. Maybe `statistic` is a better choice? @davidsebfischer could probably be more authoritative on this. And yeah, we should change those `z-score` docs. Shall we change this in this func",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1388
https://github.com/scverse/scanpy/pull/1388:671,usability,behavi,behavior,671,"> ## `groups`. > I still don't think this is quite right. When I'm sharing DE results, it's not going to be every comparison stacked together in one table. It would be a table per comparison, either in separate files or in a spreadsheet with a page per comparison. > . > But how about this for a compromise, `groups` stays a required argument. You can pass a list of groups, and a groups column will be added. You can also pass `None`, and all groups will be used. But you have to pass something. This means you can't just forget to pass a parameter and then open a bug report about how genes are showing up multiple times in your DE results. You had to opt in to either behavior. OK, sounds good. Done. > . > ## New column name. > I wasn't clear here. We should definitely include these values. I just think the names could be better and was wondering what other packages use as column names for these values. > . > AFAICT there is no agreed upon way to name these. Seems weird, since you'd think there'd be a technical name for ""when logFC is positive the xxxx group had higher expression"". > . > I would go for `f""fraction_{reference}""`, but then you can't pass the output directly to a plotting function without also passing the value for `reference`. > . > How about:. > . > `pct_nz_group` and `pct_nz_reference`/ `pct_nz_ref`? I could also go for `lhs`/ `rhs` instead of `group`/ `reference`, and `fraction` instead of `pct`. But `group`/`reference` is consistent with `rank_genes_groups` and `pct` is consistent with `calculate_qc_metrics`. I like having `nz` in there since otherwise it's not super clear what fraction we're talking about. Could be fraction of total expression, or something about proportion of the dataset? This way it's more clear in the table you show to a collaborator. Sounds good, done. > . > I agree `score` is a bit weird. Maybe `statistic` is a better choice? @davidsebfischer could probably be more authoritative on this. And yeah, we should change those `z-score` ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1388
https://github.com/scverse/scanpy/pull/1388:741,usability,clear,clear,741,"> ## `groups`. > I still don't think this is quite right. When I'm sharing DE results, it's not going to be every comparison stacked together in one table. It would be a table per comparison, either in separate files or in a spreadsheet with a page per comparison. > . > But how about this for a compromise, `groups` stays a required argument. You can pass a list of groups, and a groups column will be added. You can also pass `None`, and all groups will be used. But you have to pass something. This means you can't just forget to pass a parameter and then open a bug report about how genes are showing up multiple times in your DE results. You had to opt in to either behavior. OK, sounds good. Done. > . > ## New column name. > I wasn't clear here. We should definitely include these values. I just think the names could be better and was wondering what other packages use as column names for these values. > . > AFAICT there is no agreed upon way to name these. Seems weird, since you'd think there'd be a technical name for ""when logFC is positive the xxxx group had higher expression"". > . > I would go for `f""fraction_{reference}""`, but then you can't pass the output directly to a plotting function without also passing the value for `reference`. > . > How about:. > . > `pct_nz_group` and `pct_nz_reference`/ `pct_nz_ref`? I could also go for `lhs`/ `rhs` instead of `group`/ `reference`, and `fraction` instead of `pct`. But `group`/`reference` is consistent with `rank_genes_groups` and `pct` is consistent with `calculate_qc_metrics`. I like having `nz` in there since otherwise it's not super clear what fraction we're talking about. Could be fraction of total expression, or something about proportion of the dataset? This way it's more clear in the table you show to a collaborator. Sounds good, done. > . > I agree `score` is a bit weird. Maybe `statistic` is a better choice? @davidsebfischer could probably be more authoritative on this. And yeah, we should change those `z-score` ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1388
https://github.com/scverse/scanpy/pull/1388:1459,usability,consist,consistent,1459,"groups will be used. But you have to pass something. This means you can't just forget to pass a parameter and then open a bug report about how genes are showing up multiple times in your DE results. You had to opt in to either behavior. OK, sounds good. Done. > . > ## New column name. > I wasn't clear here. We should definitely include these values. I just think the names could be better and was wondering what other packages use as column names for these values. > . > AFAICT there is no agreed upon way to name these. Seems weird, since you'd think there'd be a technical name for ""when logFC is positive the xxxx group had higher expression"". > . > I would go for `f""fraction_{reference}""`, but then you can't pass the output directly to a plotting function without also passing the value for `reference`. > . > How about:. > . > `pct_nz_group` and `pct_nz_reference`/ `pct_nz_ref`? I could also go for `lhs`/ `rhs` instead of `group`/ `reference`, and `fraction` instead of `pct`. But `group`/`reference` is consistent with `rank_genes_groups` and `pct` is consistent with `calculate_qc_metrics`. I like having `nz` in there since otherwise it's not super clear what fraction we're talking about. Could be fraction of total expression, or something about proportion of the dataset? This way it's more clear in the table you show to a collaborator. Sounds good, done. > . > I agree `score` is a bit weird. Maybe `statistic` is a better choice? @davidsebfischer could probably be more authoritative on this. And yeah, we should change those `z-score` docs. Shall we change this in this function or in sc.tl.rank_genes_groups? I feel like renaming it here is not the best way. > . > ### Performance. > General question about performance. Is this faster than calling the previous function separately on each group, then concatenating the results? I think so:. <img width=""737"" alt=""image"" src=""https://user-images.githubusercontent.com/1140359/101388354-d29d4b00-388d-11eb-98b3-f91cf03ac84c.png"">.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1388
https://github.com/scverse/scanpy/pull/1388:1508,usability,consist,consistent,1508,"groups will be used. But you have to pass something. This means you can't just forget to pass a parameter and then open a bug report about how genes are showing up multiple times in your DE results. You had to opt in to either behavior. OK, sounds good. Done. > . > ## New column name. > I wasn't clear here. We should definitely include these values. I just think the names could be better and was wondering what other packages use as column names for these values. > . > AFAICT there is no agreed upon way to name these. Seems weird, since you'd think there'd be a technical name for ""when logFC is positive the xxxx group had higher expression"". > . > I would go for `f""fraction_{reference}""`, but then you can't pass the output directly to a plotting function without also passing the value for `reference`. > . > How about:. > . > `pct_nz_group` and `pct_nz_reference`/ `pct_nz_ref`? I could also go for `lhs`/ `rhs` instead of `group`/ `reference`, and `fraction` instead of `pct`. But `group`/`reference` is consistent with `rank_genes_groups` and `pct` is consistent with `calculate_qc_metrics`. I like having `nz` in there since otherwise it's not super clear what fraction we're talking about. Could be fraction of total expression, or something about proportion of the dataset? This way it's more clear in the table you show to a collaborator. Sounds good, done. > . > I agree `score` is a bit weird. Maybe `statistic` is a better choice? @davidsebfischer could probably be more authoritative on this. And yeah, we should change those `z-score` docs. Shall we change this in this function or in sc.tl.rank_genes_groups? I feel like renaming it here is not the best way. > . > ### Performance. > General question about performance. Is this faster than calling the previous function separately on each group, then concatenating the results? I think so:. <img width=""737"" alt=""image"" src=""https://user-images.githubusercontent.com/1140359/101388354-d29d4b00-388d-11eb-98b3-f91cf03ac84c.png"">.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1388
https://github.com/scverse/scanpy/pull/1388:1607,usability,clear,clear,1607,"groups will be used. But you have to pass something. This means you can't just forget to pass a parameter and then open a bug report about how genes are showing up multiple times in your DE results. You had to opt in to either behavior. OK, sounds good. Done. > . > ## New column name. > I wasn't clear here. We should definitely include these values. I just think the names could be better and was wondering what other packages use as column names for these values. > . > AFAICT there is no agreed upon way to name these. Seems weird, since you'd think there'd be a technical name for ""when logFC is positive the xxxx group had higher expression"". > . > I would go for `f""fraction_{reference}""`, but then you can't pass the output directly to a plotting function without also passing the value for `reference`. > . > How about:. > . > `pct_nz_group` and `pct_nz_reference`/ `pct_nz_ref`? I could also go for `lhs`/ `rhs` instead of `group`/ `reference`, and `fraction` instead of `pct`. But `group`/`reference` is consistent with `rank_genes_groups` and `pct` is consistent with `calculate_qc_metrics`. I like having `nz` in there since otherwise it's not super clear what fraction we're talking about. Could be fraction of total expression, or something about proportion of the dataset? This way it's more clear in the table you show to a collaborator. Sounds good, done. > . > I agree `score` is a bit weird. Maybe `statistic` is a better choice? @davidsebfischer could probably be more authoritative on this. And yeah, we should change those `z-score` docs. Shall we change this in this function or in sc.tl.rank_genes_groups? I feel like renaming it here is not the best way. > . > ### Performance. > General question about performance. Is this faster than calling the previous function separately on each group, then concatenating the results? I think so:. <img width=""737"" alt=""image"" src=""https://user-images.githubusercontent.com/1140359/101388354-d29d4b00-388d-11eb-98b3-f91cf03ac84c.png"">.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1388
https://github.com/scverse/scanpy/pull/1388:1752,usability,clear,clear,1752,"groups will be used. But you have to pass something. This means you can't just forget to pass a parameter and then open a bug report about how genes are showing up multiple times in your DE results. You had to opt in to either behavior. OK, sounds good. Done. > . > ## New column name. > I wasn't clear here. We should definitely include these values. I just think the names could be better and was wondering what other packages use as column names for these values. > . > AFAICT there is no agreed upon way to name these. Seems weird, since you'd think there'd be a technical name for ""when logFC is positive the xxxx group had higher expression"". > . > I would go for `f""fraction_{reference}""`, but then you can't pass the output directly to a plotting function without also passing the value for `reference`. > . > How about:. > . > `pct_nz_group` and `pct_nz_reference`/ `pct_nz_ref`? I could also go for `lhs`/ `rhs` instead of `group`/ `reference`, and `fraction` instead of `pct`. But `group`/`reference` is consistent with `rank_genes_groups` and `pct` is consistent with `calculate_qc_metrics`. I like having `nz` in there since otherwise it's not super clear what fraction we're talking about. Could be fraction of total expression, or something about proportion of the dataset? This way it's more clear in the table you show to a collaborator. Sounds good, done. > . > I agree `score` is a bit weird. Maybe `statistic` is a better choice? @davidsebfischer could probably be more authoritative on this. And yeah, we should change those `z-score` docs. Shall we change this in this function or in sc.tl.rank_genes_groups? I feel like renaming it here is not the best way. > . > ### Performance. > General question about performance. Is this faster than calling the previous function separately on each group, then concatenating the results? I think so:. <img width=""737"" alt=""image"" src=""https://user-images.githubusercontent.com/1140359/101388354-d29d4b00-388d-11eb-98b3-f91cf03ac84c.png"">.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1388
https://github.com/scverse/scanpy/pull/1388:2135,usability,Perform,Performance,2135,"groups will be used. But you have to pass something. This means you can't just forget to pass a parameter and then open a bug report about how genes are showing up multiple times in your DE results. You had to opt in to either behavior. OK, sounds good. Done. > . > ## New column name. > I wasn't clear here. We should definitely include these values. I just think the names could be better and was wondering what other packages use as column names for these values. > . > AFAICT there is no agreed upon way to name these. Seems weird, since you'd think there'd be a technical name for ""when logFC is positive the xxxx group had higher expression"". > . > I would go for `f""fraction_{reference}""`, but then you can't pass the output directly to a plotting function without also passing the value for `reference`. > . > How about:. > . > `pct_nz_group` and `pct_nz_reference`/ `pct_nz_ref`? I could also go for `lhs`/ `rhs` instead of `group`/ `reference`, and `fraction` instead of `pct`. But `group`/`reference` is consistent with `rank_genes_groups` and `pct` is consistent with `calculate_qc_metrics`. I like having `nz` in there since otherwise it's not super clear what fraction we're talking about. Could be fraction of total expression, or something about proportion of the dataset? This way it's more clear in the table you show to a collaborator. Sounds good, done. > . > I agree `score` is a bit weird. Maybe `statistic` is a better choice? @davidsebfischer could probably be more authoritative on this. And yeah, we should change those `z-score` docs. Shall we change this in this function or in sc.tl.rank_genes_groups? I feel like renaming it here is not the best way. > . > ### Performance. > General question about performance. Is this faster than calling the previous function separately on each group, then concatenating the results? I think so:. <img width=""737"" alt=""image"" src=""https://user-images.githubusercontent.com/1140359/101388354-d29d4b00-388d-11eb-98b3-f91cf03ac84c.png"">.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1388
https://github.com/scverse/scanpy/pull/1388:2173,usability,perform,performance,2173,"groups will be used. But you have to pass something. This means you can't just forget to pass a parameter and then open a bug report about how genes are showing up multiple times in your DE results. You had to opt in to either behavior. OK, sounds good. Done. > . > ## New column name. > I wasn't clear here. We should definitely include these values. I just think the names could be better and was wondering what other packages use as column names for these values. > . > AFAICT there is no agreed upon way to name these. Seems weird, since you'd think there'd be a technical name for ""when logFC is positive the xxxx group had higher expression"". > . > I would go for `f""fraction_{reference}""`, but then you can't pass the output directly to a plotting function without also passing the value for `reference`. > . > How about:. > . > `pct_nz_group` and `pct_nz_reference`/ `pct_nz_ref`? I could also go for `lhs`/ `rhs` instead of `group`/ `reference`, and `fraction` instead of `pct`. But `group`/`reference` is consistent with `rank_genes_groups` and `pct` is consistent with `calculate_qc_metrics`. I like having `nz` in there since otherwise it's not super clear what fraction we're talking about. Could be fraction of total expression, or something about proportion of the dataset? This way it's more clear in the table you show to a collaborator. Sounds good, done. > . > I agree `score` is a bit weird. Maybe `statistic` is a better choice? @davidsebfischer could probably be more authoritative on this. And yeah, we should change those `z-score` docs. Shall we change this in this function or in sc.tl.rank_genes_groups? I feel like renaming it here is not the best way. > . > ### Performance. > General question about performance. Is this faster than calling the previous function separately on each group, then concatenating the results? I think so:. <img width=""737"" alt=""image"" src=""https://user-images.githubusercontent.com/1140359/101388354-d29d4b00-388d-11eb-98b3-f91cf03ac84c.png"">.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1388
https://github.com/scverse/scanpy/pull/1388:2349,usability,user,user-images,2349,"groups will be used. But you have to pass something. This means you can't just forget to pass a parameter and then open a bug report about how genes are showing up multiple times in your DE results. You had to opt in to either behavior. OK, sounds good. Done. > . > ## New column name. > I wasn't clear here. We should definitely include these values. I just think the names could be better and was wondering what other packages use as column names for these values. > . > AFAICT there is no agreed upon way to name these. Seems weird, since you'd think there'd be a technical name for ""when logFC is positive the xxxx group had higher expression"". > . > I would go for `f""fraction_{reference}""`, but then you can't pass the output directly to a plotting function without also passing the value for `reference`. > . > How about:. > . > `pct_nz_group` and `pct_nz_reference`/ `pct_nz_ref`? I could also go for `lhs`/ `rhs` instead of `group`/ `reference`, and `fraction` instead of `pct`. But `group`/`reference` is consistent with `rank_genes_groups` and `pct` is consistent with `calculate_qc_metrics`. I like having `nz` in there since otherwise it's not super clear what fraction we're talking about. Could be fraction of total expression, or something about proportion of the dataset? This way it's more clear in the table you show to a collaborator. Sounds good, done. > . > I agree `score` is a bit weird. Maybe `statistic` is a better choice? @davidsebfischer could probably be more authoritative on this. And yeah, we should change those `z-score` docs. Shall we change this in this function or in sc.tl.rank_genes_groups? I feel like renaming it here is not the best way. > . > ### Performance. > General question about performance. Is this faster than calling the previous function separately on each group, then concatenating the results? I think so:. <img width=""737"" alt=""image"" src=""https://user-images.githubusercontent.com/1140359/101388354-d29d4b00-388d-11eb-98b3-f91cf03ac84c.png"">.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1388
https://github.com/scverse/scanpy/pull/1391:190,deployability,log,logarithmized,190,"Yes, i'll add a test for this. . Yes, it uses the right formula for fold changes now, as we need to use the formula from `rank_genes_groups` which includes exponentiation of means, implying logarithmized data (we have this in the [tutorial](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html), for example).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1391
https://github.com/scverse/scanpy/pull/1391:16,safety,test,test,16,"Yes, i'll add a test for this. . Yes, it uses the right formula for fold changes now, as we need to use the formula from `rank_genes_groups` which includes exponentiation of means, implying logarithmized data (we have this in the [tutorial](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html), for example).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1391
https://github.com/scverse/scanpy/pull/1391:190,safety,log,logarithmized,190,"Yes, i'll add a test for this. . Yes, it uses the right formula for fold changes now, as we need to use the formula from `rank_genes_groups` which includes exponentiation of means, implying logarithmized data (we have this in the [tutorial](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html), for example).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1391
https://github.com/scverse/scanpy/pull/1391:190,security,log,logarithmized,190,"Yes, i'll add a test for this. . Yes, it uses the right formula for fold changes now, as we need to use the formula from `rank_genes_groups` which includes exponentiation of means, implying logarithmized data (we have this in the [tutorial](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html), for example).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1391
https://github.com/scverse/scanpy/pull/1391:16,testability,test,test,16,"Yes, i'll add a test for this. . Yes, it uses the right formula for fold changes now, as we need to use the formula from `rank_genes_groups` which includes exponentiation of means, implying logarithmized data (we have this in the [tutorial](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html), for example).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1391
https://github.com/scverse/scanpy/pull/1391:190,testability,log,logarithmized,190,"Yes, i'll add a test for this. . Yes, it uses the right formula for fold changes now, as we need to use the formula from `rank_genes_groups` which includes exponentiation of means, implying logarithmized data (we have this in the [tutorial](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html), for example).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1391
https://github.com/scverse/scanpy/pull/1391:13,deployability,updat,update,13,@Koncopd Any update on this?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1391
https://github.com/scverse/scanpy/pull/1391:13,safety,updat,update,13,@Koncopd Any update on this?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1391
https://github.com/scverse/scanpy/pull/1391:13,security,updat,update,13,@Koncopd Any update on this?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1391
https://github.com/scverse/scanpy/pull/1391:15,deployability,updat,update,15,@fidelram i'll update this soon.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1391
https://github.com/scverse/scanpy/pull/1391:15,safety,updat,update,15,@fidelram i'll update this soon.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1391
https://github.com/scverse/scanpy/pull/1391:15,security,updat,update,15,@fidelram i'll update this soon.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1391
https://github.com/scverse/scanpy/pull/1391:22,deployability,updat,updated,22,"@ivirshup @fidelram . updated, this should be ready for merge.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1391
https://github.com/scverse/scanpy/pull/1391:22,safety,updat,updated,22,"@ivirshup @fidelram . updated, this should be ready for merge.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1391
https://github.com/scverse/scanpy/pull/1391:22,security,updat,updated,22,"@ivirshup @fidelram . updated, this should be ready for merge.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1391
https://github.com/scverse/scanpy/pull/1392:112,interoperability,specif,specificlly,112,"I'm not familiar with the code, does it also mean that colors in adata.uns['field_colors'] can be dict or is it specificlly for the palette argument?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1392
https://github.com/scverse/scanpy/pull/1392:32,reliability,doe,does,32,"I'm not familiar with the code, does it also mean that colors in adata.uns['field_colors'] can be dict or is it specificlly for the palette argument?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1392
https://github.com/scverse/scanpy/pull/1392:30,testability,simpl,simply,30,Underlying wish: Why don't we simply ditch list-based coloring everywhere :D,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1392
https://github.com/scverse/scanpy/pull/1392:30,usability,simpl,simply,30,Underlying wish: Why don't we simply ditch list-based coloring everywhere :D,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1392
https://github.com/scverse/scanpy/issues/1395:13,usability,help,help,13,I'm happy to help with a PR if you tell me how to tackle this.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1395
https://github.com/scverse/scanpy/issues/1395:101,deployability,version,versions,101,Thanks for the bug report! The issue was that `score_genes` had some code for working around how old versions of anndata would automatically remove dimensions from `X` if a dimension had only one entry.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1395
https://github.com/scverse/scanpy/issues/1395:127,deployability,automat,automatically,127,Thanks for the bug report! The issue was that `score_genes` had some code for working around how old versions of anndata would automatically remove dimensions from `X` if a dimension had only one entry.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1395
https://github.com/scverse/scanpy/issues/1395:101,integrability,version,versions,101,Thanks for the bug report! The issue was that `score_genes` had some code for working around how old versions of anndata would automatically remove dimensions from `X` if a dimension had only one entry.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1395
https://github.com/scverse/scanpy/issues/1395:101,modifiability,version,versions,101,Thanks for the bug report! The issue was that `score_genes` had some code for working around how old versions of anndata would automatically remove dimensions from `X` if a dimension had only one entry.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1395
https://github.com/scverse/scanpy/issues/1395:127,testability,automat,automatically,127,Thanks for the bug report! The issue was that `score_genes` had some code for working around how old versions of anndata would automatically remove dimensions from `X` if a dimension had only one entry.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1395
https://github.com/scverse/scanpy/issues/1396:114,availability,down,down,114,"AFAICT, I think the parallelization you're seeing will be due to the underlying calls in statsmodels. If you turn down the number of threads blas can use, do you see the same utilization?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1396
https://github.com/scverse/scanpy/issues/1396:20,performance,parallel,parallelization,20,"AFAICT, I think the parallelization you're seeing will be due to the underlying calls in statsmodels. If you turn down the number of threads blas can use, do you see the same utilization?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1396
https://github.com/scverse/scanpy/issues/1396:116,availability,down,down,116,"> AFAICT, I think the parallelization you're seeing will be due to the underlying calls in statsmodels. If you turn down the number of threads blas can use, do you see the same utilization? FYI more n_jobs seems to be slower for regress_out if I don't disable the BLAS multi threading:. ```. sc.pp.regress_out(adata, ['percent_mito'], n_jobs=1). ```. ```. regressing out ['percent_mito']. sparse input is densified and may lead to high memory use. finished (0:04:05). ```. ```. sc.pp.regress_out(adata, ['percent_mito'], n_jobs=24). ```. ```. regressing out ['percent_mito']. sparse input is densified and may lead to high memory use. finished (0:07:41). ```. I'm using scanpy 1.5.2.dev104+g8611dba1. Indeed after disabling BLAS multi threading sc.pp.regress_out will only use one core if setting n_jobs = 1. But it has to be disable by exporting these environmental variables before starting python ..., but I guess it is not a good thing because other scanpy functions may be affected? ```. export MKL_NUM_THREADS=1. export NUMEXPR_NUM_THREADS=1. export OMP_NUM_THREADS=1. ```. More interesting is that regress_out becomes lightning fast when n_jobs = 24 and with BLAS multi threading disabled:. ```. sc.pp.regress_out(adata, ['percent_mito'], n_jobs=24). ```. ```. regressing out ['percent_mito']. sparse input is densified and may lead to high memory use. finished (0:00:23). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1396
https://github.com/scverse/scanpy/issues/1396:218,availability,slo,slower,218,"> AFAICT, I think the parallelization you're seeing will be due to the underlying calls in statsmodels. If you turn down the number of threads blas can use, do you see the same utilization? FYI more n_jobs seems to be slower for regress_out if I don't disable the BLAS multi threading:. ```. sc.pp.regress_out(adata, ['percent_mito'], n_jobs=1). ```. ```. regressing out ['percent_mito']. sparse input is densified and may lead to high memory use. finished (0:04:05). ```. ```. sc.pp.regress_out(adata, ['percent_mito'], n_jobs=24). ```. ```. regressing out ['percent_mito']. sparse input is densified and may lead to high memory use. finished (0:07:41). ```. I'm using scanpy 1.5.2.dev104+g8611dba1. Indeed after disabling BLAS multi threading sc.pp.regress_out will only use one core if setting n_jobs = 1. But it has to be disable by exporting these environmental variables before starting python ..., but I guess it is not a good thing because other scanpy functions may be affected? ```. export MKL_NUM_THREADS=1. export NUMEXPR_NUM_THREADS=1. export OMP_NUM_THREADS=1. ```. More interesting is that regress_out becomes lightning fast when n_jobs = 24 and with BLAS multi threading disabled:. ```. sc.pp.regress_out(adata, ['percent_mito'], n_jobs=24). ```. ```. regressing out ['percent_mito']. sparse input is densified and may lead to high memory use. finished (0:00:23). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1396
https://github.com/scverse/scanpy/issues/1396:781,energy efficiency,core,core,781,"> AFAICT, I think the parallelization you're seeing will be due to the underlying calls in statsmodels. If you turn down the number of threads blas can use, do you see the same utilization? FYI more n_jobs seems to be slower for regress_out if I don't disable the BLAS multi threading:. ```. sc.pp.regress_out(adata, ['percent_mito'], n_jobs=1). ```. ```. regressing out ['percent_mito']. sparse input is densified and may lead to high memory use. finished (0:04:05). ```. ```. sc.pp.regress_out(adata, ['percent_mito'], n_jobs=24). ```. ```. regressing out ['percent_mito']. sparse input is densified and may lead to high memory use. finished (0:07:41). ```. I'm using scanpy 1.5.2.dev104+g8611dba1. Indeed after disabling BLAS multi threading sc.pp.regress_out will only use one core if setting n_jobs = 1. But it has to be disable by exporting these environmental variables before starting python ..., but I guess it is not a good thing because other scanpy functions may be affected? ```. export MKL_NUM_THREADS=1. export NUMEXPR_NUM_THREADS=1. export OMP_NUM_THREADS=1. ```. More interesting is that regress_out becomes lightning fast when n_jobs = 24 and with BLAS multi threading disabled:. ```. sc.pp.regress_out(adata, ['percent_mito'], n_jobs=24). ```. ```. regressing out ['percent_mito']. sparse input is densified and may lead to high memory use. finished (0:00:23). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1396
https://github.com/scverse/scanpy/issues/1396:867,modifiability,variab,variables,867,"> AFAICT, I think the parallelization you're seeing will be due to the underlying calls in statsmodels. If you turn down the number of threads blas can use, do you see the same utilization? FYI more n_jobs seems to be slower for regress_out if I don't disable the BLAS multi threading:. ```. sc.pp.regress_out(adata, ['percent_mito'], n_jobs=1). ```. ```. regressing out ['percent_mito']. sparse input is densified and may lead to high memory use. finished (0:04:05). ```. ```. sc.pp.regress_out(adata, ['percent_mito'], n_jobs=24). ```. ```. regressing out ['percent_mito']. sparse input is densified and may lead to high memory use. finished (0:07:41). ```. I'm using scanpy 1.5.2.dev104+g8611dba1. Indeed after disabling BLAS multi threading sc.pp.regress_out will only use one core if setting n_jobs = 1. But it has to be disable by exporting these environmental variables before starting python ..., but I guess it is not a good thing because other scanpy functions may be affected? ```. export MKL_NUM_THREADS=1. export NUMEXPR_NUM_THREADS=1. export OMP_NUM_THREADS=1. ```. More interesting is that regress_out becomes lightning fast when n_jobs = 24 and with BLAS multi threading disabled:. ```. sc.pp.regress_out(adata, ['percent_mito'], n_jobs=24). ```. ```. regressing out ['percent_mito']. sparse input is densified and may lead to high memory use. finished (0:00:23). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1396
https://github.com/scverse/scanpy/issues/1396:22,performance,parallel,parallelization,22,"> AFAICT, I think the parallelization you're seeing will be due to the underlying calls in statsmodels. If you turn down the number of threads blas can use, do you see the same utilization? FYI more n_jobs seems to be slower for regress_out if I don't disable the BLAS multi threading:. ```. sc.pp.regress_out(adata, ['percent_mito'], n_jobs=1). ```. ```. regressing out ['percent_mito']. sparse input is densified and may lead to high memory use. finished (0:04:05). ```. ```. sc.pp.regress_out(adata, ['percent_mito'], n_jobs=24). ```. ```. regressing out ['percent_mito']. sparse input is densified and may lead to high memory use. finished (0:07:41). ```. I'm using scanpy 1.5.2.dev104+g8611dba1. Indeed after disabling BLAS multi threading sc.pp.regress_out will only use one core if setting n_jobs = 1. But it has to be disable by exporting these environmental variables before starting python ..., but I guess it is not a good thing because other scanpy functions may be affected? ```. export MKL_NUM_THREADS=1. export NUMEXPR_NUM_THREADS=1. export OMP_NUM_THREADS=1. ```. More interesting is that regress_out becomes lightning fast when n_jobs = 24 and with BLAS multi threading disabled:. ```. sc.pp.regress_out(adata, ['percent_mito'], n_jobs=24). ```. ```. regressing out ['percent_mito']. sparse input is densified and may lead to high memory use. finished (0:00:23). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1396
https://github.com/scverse/scanpy/issues/1396:436,performance,memor,memory,436,"> AFAICT, I think the parallelization you're seeing will be due to the underlying calls in statsmodels. If you turn down the number of threads blas can use, do you see the same utilization? FYI more n_jobs seems to be slower for regress_out if I don't disable the BLAS multi threading:. ```. sc.pp.regress_out(adata, ['percent_mito'], n_jobs=1). ```. ```. regressing out ['percent_mito']. sparse input is densified and may lead to high memory use. finished (0:04:05). ```. ```. sc.pp.regress_out(adata, ['percent_mito'], n_jobs=24). ```. ```. regressing out ['percent_mito']. sparse input is densified and may lead to high memory use. finished (0:07:41). ```. I'm using scanpy 1.5.2.dev104+g8611dba1. Indeed after disabling BLAS multi threading sc.pp.regress_out will only use one core if setting n_jobs = 1. But it has to be disable by exporting these environmental variables before starting python ..., but I guess it is not a good thing because other scanpy functions may be affected? ```. export MKL_NUM_THREADS=1. export NUMEXPR_NUM_THREADS=1. export OMP_NUM_THREADS=1. ```. More interesting is that regress_out becomes lightning fast when n_jobs = 24 and with BLAS multi threading disabled:. ```. sc.pp.regress_out(adata, ['percent_mito'], n_jobs=24). ```. ```. regressing out ['percent_mito']. sparse input is densified and may lead to high memory use. finished (0:00:23). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1396
https://github.com/scverse/scanpy/issues/1396:623,performance,memor,memory,623,"> AFAICT, I think the parallelization you're seeing will be due to the underlying calls in statsmodels. If you turn down the number of threads blas can use, do you see the same utilization? FYI more n_jobs seems to be slower for regress_out if I don't disable the BLAS multi threading:. ```. sc.pp.regress_out(adata, ['percent_mito'], n_jobs=1). ```. ```. regressing out ['percent_mito']. sparse input is densified and may lead to high memory use. finished (0:04:05). ```. ```. sc.pp.regress_out(adata, ['percent_mito'], n_jobs=24). ```. ```. regressing out ['percent_mito']. sparse input is densified and may lead to high memory use. finished (0:07:41). ```. I'm using scanpy 1.5.2.dev104+g8611dba1. Indeed after disabling BLAS multi threading sc.pp.regress_out will only use one core if setting n_jobs = 1. But it has to be disable by exporting these environmental variables before starting python ..., but I guess it is not a good thing because other scanpy functions may be affected? ```. export MKL_NUM_THREADS=1. export NUMEXPR_NUM_THREADS=1. export OMP_NUM_THREADS=1. ```. More interesting is that regress_out becomes lightning fast when n_jobs = 24 and with BLAS multi threading disabled:. ```. sc.pp.regress_out(adata, ['percent_mito'], n_jobs=24). ```. ```. regressing out ['percent_mito']. sparse input is densified and may lead to high memory use. finished (0:00:23). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1396
https://github.com/scverse/scanpy/issues/1396:1348,performance,memor,memory,1348,"> AFAICT, I think the parallelization you're seeing will be due to the underlying calls in statsmodels. If you turn down the number of threads blas can use, do you see the same utilization? FYI more n_jobs seems to be slower for regress_out if I don't disable the BLAS multi threading:. ```. sc.pp.regress_out(adata, ['percent_mito'], n_jobs=1). ```. ```. regressing out ['percent_mito']. sparse input is densified and may lead to high memory use. finished (0:04:05). ```. ```. sc.pp.regress_out(adata, ['percent_mito'], n_jobs=24). ```. ```. regressing out ['percent_mito']. sparse input is densified and may lead to high memory use. finished (0:07:41). ```. I'm using scanpy 1.5.2.dev104+g8611dba1. Indeed after disabling BLAS multi threading sc.pp.regress_out will only use one core if setting n_jobs = 1. But it has to be disable by exporting these environmental variables before starting python ..., but I guess it is not a good thing because other scanpy functions may be affected? ```. export MKL_NUM_THREADS=1. export NUMEXPR_NUM_THREADS=1. export OMP_NUM_THREADS=1. ```. More interesting is that regress_out becomes lightning fast when n_jobs = 24 and with BLAS multi threading disabled:. ```. sc.pp.regress_out(adata, ['percent_mito'], n_jobs=24). ```. ```. regressing out ['percent_mito']. sparse input is densified and may lead to high memory use. finished (0:00:23). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1396
https://github.com/scverse/scanpy/issues/1396:218,reliability,slo,slower,218,"> AFAICT, I think the parallelization you're seeing will be due to the underlying calls in statsmodels. If you turn down the number of threads blas can use, do you see the same utilization? FYI more n_jobs seems to be slower for regress_out if I don't disable the BLAS multi threading:. ```. sc.pp.regress_out(adata, ['percent_mito'], n_jobs=1). ```. ```. regressing out ['percent_mito']. sparse input is densified and may lead to high memory use. finished (0:04:05). ```. ```. sc.pp.regress_out(adata, ['percent_mito'], n_jobs=24). ```. ```. regressing out ['percent_mito']. sparse input is densified and may lead to high memory use. finished (0:07:41). ```. I'm using scanpy 1.5.2.dev104+g8611dba1. Indeed after disabling BLAS multi threading sc.pp.regress_out will only use one core if setting n_jobs = 1. But it has to be disable by exporting these environmental variables before starting python ..., but I guess it is not a good thing because other scanpy functions may be affected? ```. export MKL_NUM_THREADS=1. export NUMEXPR_NUM_THREADS=1. export OMP_NUM_THREADS=1. ```. More interesting is that regress_out becomes lightning fast when n_jobs = 24 and with BLAS multi threading disabled:. ```. sc.pp.regress_out(adata, ['percent_mito'], n_jobs=24). ```. ```. regressing out ['percent_mito']. sparse input is densified and may lead to high memory use. finished (0:00:23). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1396
https://github.com/scverse/scanpy/issues/1396:396,safety,input,input,396,"> AFAICT, I think the parallelization you're seeing will be due to the underlying calls in statsmodels. If you turn down the number of threads blas can use, do you see the same utilization? FYI more n_jobs seems to be slower for regress_out if I don't disable the BLAS multi threading:. ```. sc.pp.regress_out(adata, ['percent_mito'], n_jobs=1). ```. ```. regressing out ['percent_mito']. sparse input is densified and may lead to high memory use. finished (0:04:05). ```. ```. sc.pp.regress_out(adata, ['percent_mito'], n_jobs=24). ```. ```. regressing out ['percent_mito']. sparse input is densified and may lead to high memory use. finished (0:07:41). ```. I'm using scanpy 1.5.2.dev104+g8611dba1. Indeed after disabling BLAS multi threading sc.pp.regress_out will only use one core if setting n_jobs = 1. But it has to be disable by exporting these environmental variables before starting python ..., but I guess it is not a good thing because other scanpy functions may be affected? ```. export MKL_NUM_THREADS=1. export NUMEXPR_NUM_THREADS=1. export OMP_NUM_THREADS=1. ```. More interesting is that regress_out becomes lightning fast when n_jobs = 24 and with BLAS multi threading disabled:. ```. sc.pp.regress_out(adata, ['percent_mito'], n_jobs=24). ```. ```. regressing out ['percent_mito']. sparse input is densified and may lead to high memory use. finished (0:00:23). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1396
https://github.com/scverse/scanpy/issues/1396:583,safety,input,input,583,"> AFAICT, I think the parallelization you're seeing will be due to the underlying calls in statsmodels. If you turn down the number of threads blas can use, do you see the same utilization? FYI more n_jobs seems to be slower for regress_out if I don't disable the BLAS multi threading:. ```. sc.pp.regress_out(adata, ['percent_mito'], n_jobs=1). ```. ```. regressing out ['percent_mito']. sparse input is densified and may lead to high memory use. finished (0:04:05). ```. ```. sc.pp.regress_out(adata, ['percent_mito'], n_jobs=24). ```. ```. regressing out ['percent_mito']. sparse input is densified and may lead to high memory use. finished (0:07:41). ```. I'm using scanpy 1.5.2.dev104+g8611dba1. Indeed after disabling BLAS multi threading sc.pp.regress_out will only use one core if setting n_jobs = 1. But it has to be disable by exporting these environmental variables before starting python ..., but I guess it is not a good thing because other scanpy functions may be affected? ```. export MKL_NUM_THREADS=1. export NUMEXPR_NUM_THREADS=1. export OMP_NUM_THREADS=1. ```. More interesting is that regress_out becomes lightning fast when n_jobs = 24 and with BLAS multi threading disabled:. ```. sc.pp.regress_out(adata, ['percent_mito'], n_jobs=24). ```. ```. regressing out ['percent_mito']. sparse input is densified and may lead to high memory use. finished (0:00:23). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1396
https://github.com/scverse/scanpy/issues/1396:1308,safety,input,input,1308,"> AFAICT, I think the parallelization you're seeing will be due to the underlying calls in statsmodels. If you turn down the number of threads blas can use, do you see the same utilization? FYI more n_jobs seems to be slower for regress_out if I don't disable the BLAS multi threading:. ```. sc.pp.regress_out(adata, ['percent_mito'], n_jobs=1). ```. ```. regressing out ['percent_mito']. sparse input is densified and may lead to high memory use. finished (0:04:05). ```. ```. sc.pp.regress_out(adata, ['percent_mito'], n_jobs=24). ```. ```. regressing out ['percent_mito']. sparse input is densified and may lead to high memory use. finished (0:07:41). ```. I'm using scanpy 1.5.2.dev104+g8611dba1. Indeed after disabling BLAS multi threading sc.pp.regress_out will only use one core if setting n_jobs = 1. But it has to be disable by exporting these environmental variables before starting python ..., but I guess it is not a good thing because other scanpy functions may be affected? ```. export MKL_NUM_THREADS=1. export NUMEXPR_NUM_THREADS=1. export OMP_NUM_THREADS=1. ```. More interesting is that regress_out becomes lightning fast when n_jobs = 24 and with BLAS multi threading disabled:. ```. sc.pp.regress_out(adata, ['percent_mito'], n_jobs=24). ```. ```. regressing out ['percent_mito']. sparse input is densified and may lead to high memory use. finished (0:00:23). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1396
https://github.com/scverse/scanpy/issues/1396:356,testability,regress,regressing,356,"> AFAICT, I think the parallelization you're seeing will be due to the underlying calls in statsmodels. If you turn down the number of threads blas can use, do you see the same utilization? FYI more n_jobs seems to be slower for regress_out if I don't disable the BLAS multi threading:. ```. sc.pp.regress_out(adata, ['percent_mito'], n_jobs=1). ```. ```. regressing out ['percent_mito']. sparse input is densified and may lead to high memory use. finished (0:04:05). ```. ```. sc.pp.regress_out(adata, ['percent_mito'], n_jobs=24). ```. ```. regressing out ['percent_mito']. sparse input is densified and may lead to high memory use. finished (0:07:41). ```. I'm using scanpy 1.5.2.dev104+g8611dba1. Indeed after disabling BLAS multi threading sc.pp.regress_out will only use one core if setting n_jobs = 1. But it has to be disable by exporting these environmental variables before starting python ..., but I guess it is not a good thing because other scanpy functions may be affected? ```. export MKL_NUM_THREADS=1. export NUMEXPR_NUM_THREADS=1. export OMP_NUM_THREADS=1. ```. More interesting is that regress_out becomes lightning fast when n_jobs = 24 and with BLAS multi threading disabled:. ```. sc.pp.regress_out(adata, ['percent_mito'], n_jobs=24). ```. ```. regressing out ['percent_mito']. sparse input is densified and may lead to high memory use. finished (0:00:23). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1396
https://github.com/scverse/scanpy/issues/1396:543,testability,regress,regressing,543,"> AFAICT, I think the parallelization you're seeing will be due to the underlying calls in statsmodels. If you turn down the number of threads blas can use, do you see the same utilization? FYI more n_jobs seems to be slower for regress_out if I don't disable the BLAS multi threading:. ```. sc.pp.regress_out(adata, ['percent_mito'], n_jobs=1). ```. ```. regressing out ['percent_mito']. sparse input is densified and may lead to high memory use. finished (0:04:05). ```. ```. sc.pp.regress_out(adata, ['percent_mito'], n_jobs=24). ```. ```. regressing out ['percent_mito']. sparse input is densified and may lead to high memory use. finished (0:07:41). ```. I'm using scanpy 1.5.2.dev104+g8611dba1. Indeed after disabling BLAS multi threading sc.pp.regress_out will only use one core if setting n_jobs = 1. But it has to be disable by exporting these environmental variables before starting python ..., but I guess it is not a good thing because other scanpy functions may be affected? ```. export MKL_NUM_THREADS=1. export NUMEXPR_NUM_THREADS=1. export OMP_NUM_THREADS=1. ```. More interesting is that regress_out becomes lightning fast when n_jobs = 24 and with BLAS multi threading disabled:. ```. sc.pp.regress_out(adata, ['percent_mito'], n_jobs=24). ```. ```. regressing out ['percent_mito']. sparse input is densified and may lead to high memory use. finished (0:00:23). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1396
https://github.com/scverse/scanpy/issues/1396:1268,testability,regress,regressing,1268,"> AFAICT, I think the parallelization you're seeing will be due to the underlying calls in statsmodels. If you turn down the number of threads blas can use, do you see the same utilization? FYI more n_jobs seems to be slower for regress_out if I don't disable the BLAS multi threading:. ```. sc.pp.regress_out(adata, ['percent_mito'], n_jobs=1). ```. ```. regressing out ['percent_mito']. sparse input is densified and may lead to high memory use. finished (0:04:05). ```. ```. sc.pp.regress_out(adata, ['percent_mito'], n_jobs=24). ```. ```. regressing out ['percent_mito']. sparse input is densified and may lead to high memory use. finished (0:07:41). ```. I'm using scanpy 1.5.2.dev104+g8611dba1. Indeed after disabling BLAS multi threading sc.pp.regress_out will only use one core if setting n_jobs = 1. But it has to be disable by exporting these environmental variables before starting python ..., but I guess it is not a good thing because other scanpy functions may be affected? ```. export MKL_NUM_THREADS=1. export NUMEXPR_NUM_THREADS=1. export OMP_NUM_THREADS=1. ```. More interesting is that regress_out becomes lightning fast when n_jobs = 24 and with BLAS multi threading disabled:. ```. sc.pp.regress_out(adata, ['percent_mito'], n_jobs=24). ```. ```. regressing out ['percent_mito']. sparse input is densified and may lead to high memory use. finished (0:00:23). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1396
https://github.com/scverse/scanpy/issues/1396:396,usability,input,input,396,"> AFAICT, I think the parallelization you're seeing will be due to the underlying calls in statsmodels. If you turn down the number of threads blas can use, do you see the same utilization? FYI more n_jobs seems to be slower for regress_out if I don't disable the BLAS multi threading:. ```. sc.pp.regress_out(adata, ['percent_mito'], n_jobs=1). ```. ```. regressing out ['percent_mito']. sparse input is densified and may lead to high memory use. finished (0:04:05). ```. ```. sc.pp.regress_out(adata, ['percent_mito'], n_jobs=24). ```. ```. regressing out ['percent_mito']. sparse input is densified and may lead to high memory use. finished (0:07:41). ```. I'm using scanpy 1.5.2.dev104+g8611dba1. Indeed after disabling BLAS multi threading sc.pp.regress_out will only use one core if setting n_jobs = 1. But it has to be disable by exporting these environmental variables before starting python ..., but I guess it is not a good thing because other scanpy functions may be affected? ```. export MKL_NUM_THREADS=1. export NUMEXPR_NUM_THREADS=1. export OMP_NUM_THREADS=1. ```. More interesting is that regress_out becomes lightning fast when n_jobs = 24 and with BLAS multi threading disabled:. ```. sc.pp.regress_out(adata, ['percent_mito'], n_jobs=24). ```. ```. regressing out ['percent_mito']. sparse input is densified and may lead to high memory use. finished (0:00:23). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1396
https://github.com/scverse/scanpy/issues/1396:436,usability,memor,memory,436,"> AFAICT, I think the parallelization you're seeing will be due to the underlying calls in statsmodels. If you turn down the number of threads blas can use, do you see the same utilization? FYI more n_jobs seems to be slower for regress_out if I don't disable the BLAS multi threading:. ```. sc.pp.regress_out(adata, ['percent_mito'], n_jobs=1). ```. ```. regressing out ['percent_mito']. sparse input is densified and may lead to high memory use. finished (0:04:05). ```. ```. sc.pp.regress_out(adata, ['percent_mito'], n_jobs=24). ```. ```. regressing out ['percent_mito']. sparse input is densified and may lead to high memory use. finished (0:07:41). ```. I'm using scanpy 1.5.2.dev104+g8611dba1. Indeed after disabling BLAS multi threading sc.pp.regress_out will only use one core if setting n_jobs = 1. But it has to be disable by exporting these environmental variables before starting python ..., but I guess it is not a good thing because other scanpy functions may be affected? ```. export MKL_NUM_THREADS=1. export NUMEXPR_NUM_THREADS=1. export OMP_NUM_THREADS=1. ```. More interesting is that regress_out becomes lightning fast when n_jobs = 24 and with BLAS multi threading disabled:. ```. sc.pp.regress_out(adata, ['percent_mito'], n_jobs=24). ```. ```. regressing out ['percent_mito']. sparse input is densified and may lead to high memory use. finished (0:00:23). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1396
https://github.com/scverse/scanpy/issues/1396:583,usability,input,input,583,"> AFAICT, I think the parallelization you're seeing will be due to the underlying calls in statsmodels. If you turn down the number of threads blas can use, do you see the same utilization? FYI more n_jobs seems to be slower for regress_out if I don't disable the BLAS multi threading:. ```. sc.pp.regress_out(adata, ['percent_mito'], n_jobs=1). ```. ```. regressing out ['percent_mito']. sparse input is densified and may lead to high memory use. finished (0:04:05). ```. ```. sc.pp.regress_out(adata, ['percent_mito'], n_jobs=24). ```. ```. regressing out ['percent_mito']. sparse input is densified and may lead to high memory use. finished (0:07:41). ```. I'm using scanpy 1.5.2.dev104+g8611dba1. Indeed after disabling BLAS multi threading sc.pp.regress_out will only use one core if setting n_jobs = 1. But it has to be disable by exporting these environmental variables before starting python ..., but I guess it is not a good thing because other scanpy functions may be affected? ```. export MKL_NUM_THREADS=1. export NUMEXPR_NUM_THREADS=1. export OMP_NUM_THREADS=1. ```. More interesting is that regress_out becomes lightning fast when n_jobs = 24 and with BLAS multi threading disabled:. ```. sc.pp.regress_out(adata, ['percent_mito'], n_jobs=24). ```. ```. regressing out ['percent_mito']. sparse input is densified and may lead to high memory use. finished (0:00:23). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1396
https://github.com/scverse/scanpy/issues/1396:623,usability,memor,memory,623,"> AFAICT, I think the parallelization you're seeing will be due to the underlying calls in statsmodels. If you turn down the number of threads blas can use, do you see the same utilization? FYI more n_jobs seems to be slower for regress_out if I don't disable the BLAS multi threading:. ```. sc.pp.regress_out(adata, ['percent_mito'], n_jobs=1). ```. ```. regressing out ['percent_mito']. sparse input is densified and may lead to high memory use. finished (0:04:05). ```. ```. sc.pp.regress_out(adata, ['percent_mito'], n_jobs=24). ```. ```. regressing out ['percent_mito']. sparse input is densified and may lead to high memory use. finished (0:07:41). ```. I'm using scanpy 1.5.2.dev104+g8611dba1. Indeed after disabling BLAS multi threading sc.pp.regress_out will only use one core if setting n_jobs = 1. But it has to be disable by exporting these environmental variables before starting python ..., but I guess it is not a good thing because other scanpy functions may be affected? ```. export MKL_NUM_THREADS=1. export NUMEXPR_NUM_THREADS=1. export OMP_NUM_THREADS=1. ```. More interesting is that regress_out becomes lightning fast when n_jobs = 24 and with BLAS multi threading disabled:. ```. sc.pp.regress_out(adata, ['percent_mito'], n_jobs=24). ```. ```. regressing out ['percent_mito']. sparse input is densified and may lead to high memory use. finished (0:00:23). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1396
https://github.com/scverse/scanpy/issues/1396:1308,usability,input,input,1308,"> AFAICT, I think the parallelization you're seeing will be due to the underlying calls in statsmodels. If you turn down the number of threads blas can use, do you see the same utilization? FYI more n_jobs seems to be slower for regress_out if I don't disable the BLAS multi threading:. ```. sc.pp.regress_out(adata, ['percent_mito'], n_jobs=1). ```. ```. regressing out ['percent_mito']. sparse input is densified and may lead to high memory use. finished (0:04:05). ```. ```. sc.pp.regress_out(adata, ['percent_mito'], n_jobs=24). ```. ```. regressing out ['percent_mito']. sparse input is densified and may lead to high memory use. finished (0:07:41). ```. I'm using scanpy 1.5.2.dev104+g8611dba1. Indeed after disabling BLAS multi threading sc.pp.regress_out will only use one core if setting n_jobs = 1. But it has to be disable by exporting these environmental variables before starting python ..., but I guess it is not a good thing because other scanpy functions may be affected? ```. export MKL_NUM_THREADS=1. export NUMEXPR_NUM_THREADS=1. export OMP_NUM_THREADS=1. ```. More interesting is that regress_out becomes lightning fast when n_jobs = 24 and with BLAS multi threading disabled:. ```. sc.pp.regress_out(adata, ['percent_mito'], n_jobs=24). ```. ```. regressing out ['percent_mito']. sparse input is densified and may lead to high memory use. finished (0:00:23). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1396
https://github.com/scverse/scanpy/issues/1396:1348,usability,memor,memory,1348,"> AFAICT, I think the parallelization you're seeing will be due to the underlying calls in statsmodels. If you turn down the number of threads blas can use, do you see the same utilization? FYI more n_jobs seems to be slower for regress_out if I don't disable the BLAS multi threading:. ```. sc.pp.regress_out(adata, ['percent_mito'], n_jobs=1). ```. ```. regressing out ['percent_mito']. sparse input is densified and may lead to high memory use. finished (0:04:05). ```. ```. sc.pp.regress_out(adata, ['percent_mito'], n_jobs=24). ```. ```. regressing out ['percent_mito']. sparse input is densified and may lead to high memory use. finished (0:07:41). ```. I'm using scanpy 1.5.2.dev104+g8611dba1. Indeed after disabling BLAS multi threading sc.pp.regress_out will only use one core if setting n_jobs = 1. But it has to be disable by exporting these environmental variables before starting python ..., but I guess it is not a good thing because other scanpy functions may be affected? ```. export MKL_NUM_THREADS=1. export NUMEXPR_NUM_THREADS=1. export OMP_NUM_THREADS=1. ```. More interesting is that regress_out becomes lightning fast when n_jobs = 24 and with BLAS multi threading disabled:. ```. sc.pp.regress_out(adata, ['percent_mito'], n_jobs=24). ```. ```. regressing out ['percent_mito']. sparse input is densified and may lead to high memory use. finished (0:00:23). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1396
https://github.com/scverse/scanpy/issues/1396:431,deployability,manag,manage,431,"> More interesting is that regress_out becomes lightning fast when n_jobs = 24 and with BLAS multi threading disabled:. Thats not too surprising to me. This must be significantly over scheduling your machine. --------------------------------. This got me doing a little more digging into this, and it look's like there's actually a solution now! We can use [`threadpoolctl`](https://github.com/joblib/threadpoolctl) to dynamically manage the number of threads BLAS uses via the `threadpool_limits` context manager. . I'm definitely interested in using this inside scanpy to manage the number of threads used here. Not quite sure yet what the right behaviour/ api is. Some options:. * Should all calls use 1 blas thread by default, so parallelization will only happen through the number of jobs? * Do we only limit the number of threads if `n_jobs` is specified? . * Do we try and be fancy, with something like `n_threads = n_cpus // n_jobs`? *Minor update*. [If we use `joblib` (with the `loky` backend) instead of `multiprocessing`, the fancy solution will be used by default.](https://joblib.readthedocs.io/en/latest/parallel.html#avoiding-over-subscription-of-cpu-resources). I think this is what the code would look like inside of `regress_out`:. ```python. from joblib import Parallel, delayed. res = Parallel(n_jobs=n_jobs)(delayed(_regress_out_chunk)(task) for task in tasks). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1396
https://github.com/scverse/scanpy/issues/1396:506,deployability,manag,manager,506,"> More interesting is that regress_out becomes lightning fast when n_jobs = 24 and with BLAS multi threading disabled:. Thats not too surprising to me. This must be significantly over scheduling your machine. --------------------------------. This got me doing a little more digging into this, and it look's like there's actually a solution now! We can use [`threadpoolctl`](https://github.com/joblib/threadpoolctl) to dynamically manage the number of threads BLAS uses via the `threadpool_limits` context manager. . I'm definitely interested in using this inside scanpy to manage the number of threads used here. Not quite sure yet what the right behaviour/ api is. Some options:. * Should all calls use 1 blas thread by default, so parallelization will only happen through the number of jobs? * Do we only limit the number of threads if `n_jobs` is specified? . * Do we try and be fancy, with something like `n_threads = n_cpus // n_jobs`? *Minor update*. [If we use `joblib` (with the `loky` backend) instead of `multiprocessing`, the fancy solution will be used by default.](https://joblib.readthedocs.io/en/latest/parallel.html#avoiding-over-subscription-of-cpu-resources). I think this is what the code would look like inside of `regress_out`:. ```python. from joblib import Parallel, delayed. res = Parallel(n_jobs=n_jobs)(delayed(_regress_out_chunk)(task) for task in tasks). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1396
https://github.com/scverse/scanpy/issues/1396:574,deployability,manag,manage,574,"> More interesting is that regress_out becomes lightning fast when n_jobs = 24 and with BLAS multi threading disabled:. Thats not too surprising to me. This must be significantly over scheduling your machine. --------------------------------. This got me doing a little more digging into this, and it look's like there's actually a solution now! We can use [`threadpoolctl`](https://github.com/joblib/threadpoolctl) to dynamically manage the number of threads BLAS uses via the `threadpool_limits` context manager. . I'm definitely interested in using this inside scanpy to manage the number of threads used here. Not quite sure yet what the right behaviour/ api is. Some options:. * Should all calls use 1 blas thread by default, so parallelization will only happen through the number of jobs? * Do we only limit the number of threads if `n_jobs` is specified? . * Do we try and be fancy, with something like `n_threads = n_cpus // n_jobs`? *Minor update*. [If we use `joblib` (with the `loky` backend) instead of `multiprocessing`, the fancy solution will be used by default.](https://joblib.readthedocs.io/en/latest/parallel.html#avoiding-over-subscription-of-cpu-resources). I think this is what the code would look like inside of `regress_out`:. ```python. from joblib import Parallel, delayed. res = Parallel(n_jobs=n_jobs)(delayed(_regress_out_chunk)(task) for task in tasks). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1396
https://github.com/scverse/scanpy/issues/1396:659,deployability,api,api,659,"> More interesting is that regress_out becomes lightning fast when n_jobs = 24 and with BLAS multi threading disabled:. Thats not too surprising to me. This must be significantly over scheduling your machine. --------------------------------. This got me doing a little more digging into this, and it look's like there's actually a solution now! We can use [`threadpoolctl`](https://github.com/joblib/threadpoolctl) to dynamically manage the number of threads BLAS uses via the `threadpool_limits` context manager. . I'm definitely interested in using this inside scanpy to manage the number of threads used here. Not quite sure yet what the right behaviour/ api is. Some options:. * Should all calls use 1 blas thread by default, so parallelization will only happen through the number of jobs? * Do we only limit the number of threads if `n_jobs` is specified? . * Do we try and be fancy, with something like `n_threads = n_cpus // n_jobs`? *Minor update*. [If we use `joblib` (with the `loky` backend) instead of `multiprocessing`, the fancy solution will be used by default.](https://joblib.readthedocs.io/en/latest/parallel.html#avoiding-over-subscription-of-cpu-resources). I think this is what the code would look like inside of `regress_out`:. ```python. from joblib import Parallel, delayed. res = Parallel(n_jobs=n_jobs)(delayed(_regress_out_chunk)(task) for task in tasks). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1396
https://github.com/scverse/scanpy/issues/1396:949,deployability,updat,update,949,"> More interesting is that regress_out becomes lightning fast when n_jobs = 24 and with BLAS multi threading disabled:. Thats not too surprising to me. This must be significantly over scheduling your machine. --------------------------------. This got me doing a little more digging into this, and it look's like there's actually a solution now! We can use [`threadpoolctl`](https://github.com/joblib/threadpoolctl) to dynamically manage the number of threads BLAS uses via the `threadpool_limits` context manager. . I'm definitely interested in using this inside scanpy to manage the number of threads used here. Not quite sure yet what the right behaviour/ api is. Some options:. * Should all calls use 1 blas thread by default, so parallelization will only happen through the number of jobs? * Do we only limit the number of threads if `n_jobs` is specified? . * Do we try and be fancy, with something like `n_threads = n_cpus // n_jobs`? *Minor update*. [If we use `joblib` (with the `loky` backend) instead of `multiprocessing`, the fancy solution will be used by default.](https://joblib.readthedocs.io/en/latest/parallel.html#avoiding-over-subscription-of-cpu-resources). I think this is what the code would look like inside of `regress_out`:. ```python. from joblib import Parallel, delayed. res = Parallel(n_jobs=n_jobs)(delayed(_regress_out_chunk)(task) for task in tasks). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1396
https://github.com/scverse/scanpy/issues/1396:1167,deployability,resourc,resources,1167,"> More interesting is that regress_out becomes lightning fast when n_jobs = 24 and with BLAS multi threading disabled:. Thats not too surprising to me. This must be significantly over scheduling your machine. --------------------------------. This got me doing a little more digging into this, and it look's like there's actually a solution now! We can use [`threadpoolctl`](https://github.com/joblib/threadpoolctl) to dynamically manage the number of threads BLAS uses via the `threadpool_limits` context manager. . I'm definitely interested in using this inside scanpy to manage the number of threads used here. Not quite sure yet what the right behaviour/ api is. Some options:. * Should all calls use 1 blas thread by default, so parallelization will only happen through the number of jobs? * Do we only limit the number of threads if `n_jobs` is specified? . * Do we try and be fancy, with something like `n_threads = n_cpus // n_jobs`? *Minor update*. [If we use `joblib` (with the `loky` backend) instead of `multiprocessing`, the fancy solution will be used by default.](https://joblib.readthedocs.io/en/latest/parallel.html#avoiding-over-subscription-of-cpu-resources). I think this is what the code would look like inside of `regress_out`:. ```python. from joblib import Parallel, delayed. res = Parallel(n_jobs=n_jobs)(delayed(_regress_out_chunk)(task) for task in tasks). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1396
https://github.com/scverse/scanpy/issues/1396:184,energy efficiency,schedul,scheduling,184,"> More interesting is that regress_out becomes lightning fast when n_jobs = 24 and with BLAS multi threading disabled:. Thats not too surprising to me. This must be significantly over scheduling your machine. --------------------------------. This got me doing a little more digging into this, and it look's like there's actually a solution now! We can use [`threadpoolctl`](https://github.com/joblib/threadpoolctl) to dynamically manage the number of threads BLAS uses via the `threadpool_limits` context manager. . I'm definitely interested in using this inside scanpy to manage the number of threads used here. Not quite sure yet what the right behaviour/ api is. Some options:. * Should all calls use 1 blas thread by default, so parallelization will only happen through the number of jobs? * Do we only limit the number of threads if `n_jobs` is specified? . * Do we try and be fancy, with something like `n_threads = n_cpus // n_jobs`? *Minor update*. [If we use `joblib` (with the `loky` backend) instead of `multiprocessing`, the fancy solution will be used by default.](https://joblib.readthedocs.io/en/latest/parallel.html#avoiding-over-subscription-of-cpu-resources). I think this is what the code would look like inside of `regress_out`:. ```python. from joblib import Parallel, delayed. res = Parallel(n_jobs=n_jobs)(delayed(_regress_out_chunk)(task) for task in tasks). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1396
https://github.com/scverse/scanpy/issues/1396:431,energy efficiency,manag,manage,431,"> More interesting is that regress_out becomes lightning fast when n_jobs = 24 and with BLAS multi threading disabled:. Thats not too surprising to me. This must be significantly over scheduling your machine. --------------------------------. This got me doing a little more digging into this, and it look's like there's actually a solution now! We can use [`threadpoolctl`](https://github.com/joblib/threadpoolctl) to dynamically manage the number of threads BLAS uses via the `threadpool_limits` context manager. . I'm definitely interested in using this inside scanpy to manage the number of threads used here. Not quite sure yet what the right behaviour/ api is. Some options:. * Should all calls use 1 blas thread by default, so parallelization will only happen through the number of jobs? * Do we only limit the number of threads if `n_jobs` is specified? . * Do we try and be fancy, with something like `n_threads = n_cpus // n_jobs`? *Minor update*. [If we use `joblib` (with the `loky` backend) instead of `multiprocessing`, the fancy solution will be used by default.](https://joblib.readthedocs.io/en/latest/parallel.html#avoiding-over-subscription-of-cpu-resources). I think this is what the code would look like inside of `regress_out`:. ```python. from joblib import Parallel, delayed. res = Parallel(n_jobs=n_jobs)(delayed(_regress_out_chunk)(task) for task in tasks). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1396
https://github.com/scverse/scanpy/issues/1396:506,energy efficiency,manag,manager,506,"> More interesting is that regress_out becomes lightning fast when n_jobs = 24 and with BLAS multi threading disabled:. Thats not too surprising to me. This must be significantly over scheduling your machine. --------------------------------. This got me doing a little more digging into this, and it look's like there's actually a solution now! We can use [`threadpoolctl`](https://github.com/joblib/threadpoolctl) to dynamically manage the number of threads BLAS uses via the `threadpool_limits` context manager. . I'm definitely interested in using this inside scanpy to manage the number of threads used here. Not quite sure yet what the right behaviour/ api is. Some options:. * Should all calls use 1 blas thread by default, so parallelization will only happen through the number of jobs? * Do we only limit the number of threads if `n_jobs` is specified? . * Do we try and be fancy, with something like `n_threads = n_cpus // n_jobs`? *Minor update*. [If we use `joblib` (with the `loky` backend) instead of `multiprocessing`, the fancy solution will be used by default.](https://joblib.readthedocs.io/en/latest/parallel.html#avoiding-over-subscription-of-cpu-resources). I think this is what the code would look like inside of `regress_out`:. ```python. from joblib import Parallel, delayed. res = Parallel(n_jobs=n_jobs)(delayed(_regress_out_chunk)(task) for task in tasks). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1396
https://github.com/scverse/scanpy/issues/1396:574,energy efficiency,manag,manage,574,"> More interesting is that regress_out becomes lightning fast when n_jobs = 24 and with BLAS multi threading disabled:. Thats not too surprising to me. This must be significantly over scheduling your machine. --------------------------------. This got me doing a little more digging into this, and it look's like there's actually a solution now! We can use [`threadpoolctl`](https://github.com/joblib/threadpoolctl) to dynamically manage the number of threads BLAS uses via the `threadpool_limits` context manager. . I'm definitely interested in using this inside scanpy to manage the number of threads used here. Not quite sure yet what the right behaviour/ api is. Some options:. * Should all calls use 1 blas thread by default, so parallelization will only happen through the number of jobs? * Do we only limit the number of threads if `n_jobs` is specified? . * Do we try and be fancy, with something like `n_threads = n_cpus // n_jobs`? *Minor update*. [If we use `joblib` (with the `loky` backend) instead of `multiprocessing`, the fancy solution will be used by default.](https://joblib.readthedocs.io/en/latest/parallel.html#avoiding-over-subscription-of-cpu-resources). I think this is what the code would look like inside of `regress_out`:. ```python. from joblib import Parallel, delayed. res = Parallel(n_jobs=n_jobs)(delayed(_regress_out_chunk)(task) for task in tasks). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1396
https://github.com/scverse/scanpy/issues/1396:1163,energy efficiency,cpu,cpu-resources,1163,"> More interesting is that regress_out becomes lightning fast when n_jobs = 24 and with BLAS multi threading disabled:. Thats not too surprising to me. This must be significantly over scheduling your machine. --------------------------------. This got me doing a little more digging into this, and it look's like there's actually a solution now! We can use [`threadpoolctl`](https://github.com/joblib/threadpoolctl) to dynamically manage the number of threads BLAS uses via the `threadpool_limits` context manager. . I'm definitely interested in using this inside scanpy to manage the number of threads used here. Not quite sure yet what the right behaviour/ api is. Some options:. * Should all calls use 1 blas thread by default, so parallelization will only happen through the number of jobs? * Do we only limit the number of threads if `n_jobs` is specified? . * Do we try and be fancy, with something like `n_threads = n_cpus // n_jobs`? *Minor update*. [If we use `joblib` (with the `loky` backend) instead of `multiprocessing`, the fancy solution will be used by default.](https://joblib.readthedocs.io/en/latest/parallel.html#avoiding-over-subscription-of-cpu-resources). I think this is what the code would look like inside of `regress_out`:. ```python. from joblib import Parallel, delayed. res = Parallel(n_jobs=n_jobs)(delayed(_regress_out_chunk)(task) for task in tasks). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1396
https://github.com/scverse/scanpy/issues/1396:659,integrability,api,api,659,"> More interesting is that regress_out becomes lightning fast when n_jobs = 24 and with BLAS multi threading disabled:. Thats not too surprising to me. This must be significantly over scheduling your machine. --------------------------------. This got me doing a little more digging into this, and it look's like there's actually a solution now! We can use [`threadpoolctl`](https://github.com/joblib/threadpoolctl) to dynamically manage the number of threads BLAS uses via the `threadpool_limits` context manager. . I'm definitely interested in using this inside scanpy to manage the number of threads used here. Not quite sure yet what the right behaviour/ api is. Some options:. * Should all calls use 1 blas thread by default, so parallelization will only happen through the number of jobs? * Do we only limit the number of threads if `n_jobs` is specified? . * Do we try and be fancy, with something like `n_threads = n_cpus // n_jobs`? *Minor update*. [If we use `joblib` (with the `loky` backend) instead of `multiprocessing`, the fancy solution will be used by default.](https://joblib.readthedocs.io/en/latest/parallel.html#avoiding-over-subscription-of-cpu-resources). I think this is what the code would look like inside of `regress_out`:. ```python. from joblib import Parallel, delayed. res = Parallel(n_jobs=n_jobs)(delayed(_regress_out_chunk)(task) for task in tasks). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1396
https://github.com/scverse/scanpy/issues/1396:1147,integrability,sub,subscription-of-cpu-resources,1147,"> More interesting is that regress_out becomes lightning fast when n_jobs = 24 and with BLAS multi threading disabled:. Thats not too surprising to me. This must be significantly over scheduling your machine. --------------------------------. This got me doing a little more digging into this, and it look's like there's actually a solution now! We can use [`threadpoolctl`](https://github.com/joblib/threadpoolctl) to dynamically manage the number of threads BLAS uses via the `threadpool_limits` context manager. . I'm definitely interested in using this inside scanpy to manage the number of threads used here. Not quite sure yet what the right behaviour/ api is. Some options:. * Should all calls use 1 blas thread by default, so parallelization will only happen through the number of jobs? * Do we only limit the number of threads if `n_jobs` is specified? . * Do we try and be fancy, with something like `n_threads = n_cpus // n_jobs`? *Minor update*. [If we use `joblib` (with the `loky` backend) instead of `multiprocessing`, the fancy solution will be used by default.](https://joblib.readthedocs.io/en/latest/parallel.html#avoiding-over-subscription-of-cpu-resources). I think this is what the code would look like inside of `regress_out`:. ```python. from joblib import Parallel, delayed. res = Parallel(n_jobs=n_jobs)(delayed(_regress_out_chunk)(task) for task in tasks). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1396
https://github.com/scverse/scanpy/issues/1396:659,interoperability,api,api,659,"> More interesting is that regress_out becomes lightning fast when n_jobs = 24 and with BLAS multi threading disabled:. Thats not too surprising to me. This must be significantly over scheduling your machine. --------------------------------. This got me doing a little more digging into this, and it look's like there's actually a solution now! We can use [`threadpoolctl`](https://github.com/joblib/threadpoolctl) to dynamically manage the number of threads BLAS uses via the `threadpool_limits` context manager. . I'm definitely interested in using this inside scanpy to manage the number of threads used here. Not quite sure yet what the right behaviour/ api is. Some options:. * Should all calls use 1 blas thread by default, so parallelization will only happen through the number of jobs? * Do we only limit the number of threads if `n_jobs` is specified? . * Do we try and be fancy, with something like `n_threads = n_cpus // n_jobs`? *Minor update*. [If we use `joblib` (with the `loky` backend) instead of `multiprocessing`, the fancy solution will be used by default.](https://joblib.readthedocs.io/en/latest/parallel.html#avoiding-over-subscription-of-cpu-resources). I think this is what the code would look like inside of `regress_out`:. ```python. from joblib import Parallel, delayed. res = Parallel(n_jobs=n_jobs)(delayed(_regress_out_chunk)(task) for task in tasks). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1396
https://github.com/scverse/scanpy/issues/1396:851,interoperability,specif,specified,851,"> More interesting is that regress_out becomes lightning fast when n_jobs = 24 and with BLAS multi threading disabled:. Thats not too surprising to me. This must be significantly over scheduling your machine. --------------------------------. This got me doing a little more digging into this, and it look's like there's actually a solution now! We can use [`threadpoolctl`](https://github.com/joblib/threadpoolctl) to dynamically manage the number of threads BLAS uses via the `threadpool_limits` context manager. . I'm definitely interested in using this inside scanpy to manage the number of threads used here. Not quite sure yet what the right behaviour/ api is. Some options:. * Should all calls use 1 blas thread by default, so parallelization will only happen through the number of jobs? * Do we only limit the number of threads if `n_jobs` is specified? . * Do we try and be fancy, with something like `n_threads = n_cpus // n_jobs`? *Minor update*. [If we use `joblib` (with the `loky` backend) instead of `multiprocessing`, the fancy solution will be used by default.](https://joblib.readthedocs.io/en/latest/parallel.html#avoiding-over-subscription-of-cpu-resources). I think this is what the code would look like inside of `regress_out`:. ```python. from joblib import Parallel, delayed. res = Parallel(n_jobs=n_jobs)(delayed(_regress_out_chunk)(task) for task in tasks). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1396
https://github.com/scverse/scanpy/issues/1396:184,performance,schedul,scheduling,184,"> More interesting is that regress_out becomes lightning fast when n_jobs = 24 and with BLAS multi threading disabled:. Thats not too surprising to me. This must be significantly over scheduling your machine. --------------------------------. This got me doing a little more digging into this, and it look's like there's actually a solution now! We can use [`threadpoolctl`](https://github.com/joblib/threadpoolctl) to dynamically manage the number of threads BLAS uses via the `threadpool_limits` context manager. . I'm definitely interested in using this inside scanpy to manage the number of threads used here. Not quite sure yet what the right behaviour/ api is. Some options:. * Should all calls use 1 blas thread by default, so parallelization will only happen through the number of jobs? * Do we only limit the number of threads if `n_jobs` is specified? . * Do we try and be fancy, with something like `n_threads = n_cpus // n_jobs`? *Minor update*. [If we use `joblib` (with the `loky` backend) instead of `multiprocessing`, the fancy solution will be used by default.](https://joblib.readthedocs.io/en/latest/parallel.html#avoiding-over-subscription-of-cpu-resources). I think this is what the code would look like inside of `regress_out`:. ```python. from joblib import Parallel, delayed. res = Parallel(n_jobs=n_jobs)(delayed(_regress_out_chunk)(task) for task in tasks). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1396
https://github.com/scverse/scanpy/issues/1396:734,performance,parallel,parallelization,734,"> More interesting is that regress_out becomes lightning fast when n_jobs = 24 and with BLAS multi threading disabled:. Thats not too surprising to me. This must be significantly over scheduling your machine. --------------------------------. This got me doing a little more digging into this, and it look's like there's actually a solution now! We can use [`threadpoolctl`](https://github.com/joblib/threadpoolctl) to dynamically manage the number of threads BLAS uses via the `threadpool_limits` context manager. . I'm definitely interested in using this inside scanpy to manage the number of threads used here. Not quite sure yet what the right behaviour/ api is. Some options:. * Should all calls use 1 blas thread by default, so parallelization will only happen through the number of jobs? * Do we only limit the number of threads if `n_jobs` is specified? . * Do we try and be fancy, with something like `n_threads = n_cpus // n_jobs`? *Minor update*. [If we use `joblib` (with the `loky` backend) instead of `multiprocessing`, the fancy solution will be used by default.](https://joblib.readthedocs.io/en/latest/parallel.html#avoiding-over-subscription-of-cpu-resources). I think this is what the code would look like inside of `regress_out`:. ```python. from joblib import Parallel, delayed. res = Parallel(n_jobs=n_jobs)(delayed(_regress_out_chunk)(task) for task in tasks). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1396
https://github.com/scverse/scanpy/issues/1396:1119,performance,parallel,parallel,1119,"> More interesting is that regress_out becomes lightning fast when n_jobs = 24 and with BLAS multi threading disabled:. Thats not too surprising to me. This must be significantly over scheduling your machine. --------------------------------. This got me doing a little more digging into this, and it look's like there's actually a solution now! We can use [`threadpoolctl`](https://github.com/joblib/threadpoolctl) to dynamically manage the number of threads BLAS uses via the `threadpool_limits` context manager. . I'm definitely interested in using this inside scanpy to manage the number of threads used here. Not quite sure yet what the right behaviour/ api is. Some options:. * Should all calls use 1 blas thread by default, so parallelization will only happen through the number of jobs? * Do we only limit the number of threads if `n_jobs` is specified? . * Do we try and be fancy, with something like `n_threads = n_cpus // n_jobs`? *Minor update*. [If we use `joblib` (with the `loky` backend) instead of `multiprocessing`, the fancy solution will be used by default.](https://joblib.readthedocs.io/en/latest/parallel.html#avoiding-over-subscription-of-cpu-resources). I think this is what the code would look like inside of `regress_out`:. ```python. from joblib import Parallel, delayed. res = Parallel(n_jobs=n_jobs)(delayed(_regress_out_chunk)(task) for task in tasks). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1396
https://github.com/scverse/scanpy/issues/1396:1163,performance,cpu,cpu-resources,1163,"> More interesting is that regress_out becomes lightning fast when n_jobs = 24 and with BLAS multi threading disabled:. Thats not too surprising to me. This must be significantly over scheduling your machine. --------------------------------. This got me doing a little more digging into this, and it look's like there's actually a solution now! We can use [`threadpoolctl`](https://github.com/joblib/threadpoolctl) to dynamically manage the number of threads BLAS uses via the `threadpool_limits` context manager. . I'm definitely interested in using this inside scanpy to manage the number of threads used here. Not quite sure yet what the right behaviour/ api is. Some options:. * Should all calls use 1 blas thread by default, so parallelization will only happen through the number of jobs? * Do we only limit the number of threads if `n_jobs` is specified? . * Do we try and be fancy, with something like `n_threads = n_cpus // n_jobs`? *Minor update*. [If we use `joblib` (with the `loky` backend) instead of `multiprocessing`, the fancy solution will be used by default.](https://joblib.readthedocs.io/en/latest/parallel.html#avoiding-over-subscription-of-cpu-resources). I think this is what the code would look like inside of `regress_out`:. ```python. from joblib import Parallel, delayed. res = Parallel(n_jobs=n_jobs)(delayed(_regress_out_chunk)(task) for task in tasks). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1396
https://github.com/scverse/scanpy/issues/1396:1281,performance,Parallel,Parallel,1281,"> More interesting is that regress_out becomes lightning fast when n_jobs = 24 and with BLAS multi threading disabled:. Thats not too surprising to me. This must be significantly over scheduling your machine. --------------------------------. This got me doing a little more digging into this, and it look's like there's actually a solution now! We can use [`threadpoolctl`](https://github.com/joblib/threadpoolctl) to dynamically manage the number of threads BLAS uses via the `threadpool_limits` context manager. . I'm definitely interested in using this inside scanpy to manage the number of threads used here. Not quite sure yet what the right behaviour/ api is. Some options:. * Should all calls use 1 blas thread by default, so parallelization will only happen through the number of jobs? * Do we only limit the number of threads if `n_jobs` is specified? . * Do we try and be fancy, with something like `n_threads = n_cpus // n_jobs`? *Minor update*. [If we use `joblib` (with the `loky` backend) instead of `multiprocessing`, the fancy solution will be used by default.](https://joblib.readthedocs.io/en/latest/parallel.html#avoiding-over-subscription-of-cpu-resources). I think this is what the code would look like inside of `regress_out`:. ```python. from joblib import Parallel, delayed. res = Parallel(n_jobs=n_jobs)(delayed(_regress_out_chunk)(task) for task in tasks). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1396
https://github.com/scverse/scanpy/issues/1396:1306,performance,Parallel,Parallel,1306,"> More interesting is that regress_out becomes lightning fast when n_jobs = 24 and with BLAS multi threading disabled:. Thats not too surprising to me. This must be significantly over scheduling your machine. --------------------------------. This got me doing a little more digging into this, and it look's like there's actually a solution now! We can use [`threadpoolctl`](https://github.com/joblib/threadpoolctl) to dynamically manage the number of threads BLAS uses via the `threadpool_limits` context manager. . I'm definitely interested in using this inside scanpy to manage the number of threads used here. Not quite sure yet what the right behaviour/ api is. Some options:. * Should all calls use 1 blas thread by default, so parallelization will only happen through the number of jobs? * Do we only limit the number of threads if `n_jobs` is specified? . * Do we try and be fancy, with something like `n_threads = n_cpus // n_jobs`? *Minor update*. [If we use `joblib` (with the `loky` backend) instead of `multiprocessing`, the fancy solution will be used by default.](https://joblib.readthedocs.io/en/latest/parallel.html#avoiding-over-subscription-of-cpu-resources). I think this is what the code would look like inside of `regress_out`:. ```python. from joblib import Parallel, delayed. res = Parallel(n_jobs=n_jobs)(delayed(_regress_out_chunk)(task) for task in tasks). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1396
https://github.com/scverse/scanpy/issues/1396:431,safety,manag,manage,431,"> More interesting is that regress_out becomes lightning fast when n_jobs = 24 and with BLAS multi threading disabled:. Thats not too surprising to me. This must be significantly over scheduling your machine. --------------------------------. This got me doing a little more digging into this, and it look's like there's actually a solution now! We can use [`threadpoolctl`](https://github.com/joblib/threadpoolctl) to dynamically manage the number of threads BLAS uses via the `threadpool_limits` context manager. . I'm definitely interested in using this inside scanpy to manage the number of threads used here. Not quite sure yet what the right behaviour/ api is. Some options:. * Should all calls use 1 blas thread by default, so parallelization will only happen through the number of jobs? * Do we only limit the number of threads if `n_jobs` is specified? . * Do we try and be fancy, with something like `n_threads = n_cpus // n_jobs`? *Minor update*. [If we use `joblib` (with the `loky` backend) instead of `multiprocessing`, the fancy solution will be used by default.](https://joblib.readthedocs.io/en/latest/parallel.html#avoiding-over-subscription-of-cpu-resources). I think this is what the code would look like inside of `regress_out`:. ```python. from joblib import Parallel, delayed. res = Parallel(n_jobs=n_jobs)(delayed(_regress_out_chunk)(task) for task in tasks). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1396
https://github.com/scverse/scanpy/issues/1396:506,safety,manag,manager,506,"> More interesting is that regress_out becomes lightning fast when n_jobs = 24 and with BLAS multi threading disabled:. Thats not too surprising to me. This must be significantly over scheduling your machine. --------------------------------. This got me doing a little more digging into this, and it look's like there's actually a solution now! We can use [`threadpoolctl`](https://github.com/joblib/threadpoolctl) to dynamically manage the number of threads BLAS uses via the `threadpool_limits` context manager. . I'm definitely interested in using this inside scanpy to manage the number of threads used here. Not quite sure yet what the right behaviour/ api is. Some options:. * Should all calls use 1 blas thread by default, so parallelization will only happen through the number of jobs? * Do we only limit the number of threads if `n_jobs` is specified? . * Do we try and be fancy, with something like `n_threads = n_cpus // n_jobs`? *Minor update*. [If we use `joblib` (with the `loky` backend) instead of `multiprocessing`, the fancy solution will be used by default.](https://joblib.readthedocs.io/en/latest/parallel.html#avoiding-over-subscription-of-cpu-resources). I think this is what the code would look like inside of `regress_out`:. ```python. from joblib import Parallel, delayed. res = Parallel(n_jobs=n_jobs)(delayed(_regress_out_chunk)(task) for task in tasks). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1396
https://github.com/scverse/scanpy/issues/1396:574,safety,manag,manage,574,"> More interesting is that regress_out becomes lightning fast when n_jobs = 24 and with BLAS multi threading disabled:. Thats not too surprising to me. This must be significantly over scheduling your machine. --------------------------------. This got me doing a little more digging into this, and it look's like there's actually a solution now! We can use [`threadpoolctl`](https://github.com/joblib/threadpoolctl) to dynamically manage the number of threads BLAS uses via the `threadpool_limits` context manager. . I'm definitely interested in using this inside scanpy to manage the number of threads used here. Not quite sure yet what the right behaviour/ api is. Some options:. * Should all calls use 1 blas thread by default, so parallelization will only happen through the number of jobs? * Do we only limit the number of threads if `n_jobs` is specified? . * Do we try and be fancy, with something like `n_threads = n_cpus // n_jobs`? *Minor update*. [If we use `joblib` (with the `loky` backend) instead of `multiprocessing`, the fancy solution will be used by default.](https://joblib.readthedocs.io/en/latest/parallel.html#avoiding-over-subscription-of-cpu-resources). I think this is what the code would look like inside of `regress_out`:. ```python. from joblib import Parallel, delayed. res = Parallel(n_jobs=n_jobs)(delayed(_regress_out_chunk)(task) for task in tasks). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1396
https://github.com/scverse/scanpy/issues/1396:949,safety,updat,update,949,"> More interesting is that regress_out becomes lightning fast when n_jobs = 24 and with BLAS multi threading disabled:. Thats not too surprising to me. This must be significantly over scheduling your machine. --------------------------------. This got me doing a little more digging into this, and it look's like there's actually a solution now! We can use [`threadpoolctl`](https://github.com/joblib/threadpoolctl) to dynamically manage the number of threads BLAS uses via the `threadpool_limits` context manager. . I'm definitely interested in using this inside scanpy to manage the number of threads used here. Not quite sure yet what the right behaviour/ api is. Some options:. * Should all calls use 1 blas thread by default, so parallelization will only happen through the number of jobs? * Do we only limit the number of threads if `n_jobs` is specified? . * Do we try and be fancy, with something like `n_threads = n_cpus // n_jobs`? *Minor update*. [If we use `joblib` (with the `loky` backend) instead of `multiprocessing`, the fancy solution will be used by default.](https://joblib.readthedocs.io/en/latest/parallel.html#avoiding-over-subscription-of-cpu-resources). I think this is what the code would look like inside of `regress_out`:. ```python. from joblib import Parallel, delayed. res = Parallel(n_jobs=n_jobs)(delayed(_regress_out_chunk)(task) for task in tasks). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1396
https://github.com/scverse/scanpy/issues/1396:1133,safety,avoid,avoiding-over-subscription-of-cpu-resources,1133,"> More interesting is that regress_out becomes lightning fast when n_jobs = 24 and with BLAS multi threading disabled:. Thats not too surprising to me. This must be significantly over scheduling your machine. --------------------------------. This got me doing a little more digging into this, and it look's like there's actually a solution now! We can use [`threadpoolctl`](https://github.com/joblib/threadpoolctl) to dynamically manage the number of threads BLAS uses via the `threadpool_limits` context manager. . I'm definitely interested in using this inside scanpy to manage the number of threads used here. Not quite sure yet what the right behaviour/ api is. Some options:. * Should all calls use 1 blas thread by default, so parallelization will only happen through the number of jobs? * Do we only limit the number of threads if `n_jobs` is specified? . * Do we try and be fancy, with something like `n_threads = n_cpus // n_jobs`? *Minor update*. [If we use `joblib` (with the `loky` backend) instead of `multiprocessing`, the fancy solution will be used by default.](https://joblib.readthedocs.io/en/latest/parallel.html#avoiding-over-subscription-of-cpu-resources). I think this is what the code would look like inside of `regress_out`:. ```python. from joblib import Parallel, delayed. res = Parallel(n_jobs=n_jobs)(delayed(_regress_out_chunk)(task) for task in tasks). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1396
https://github.com/scverse/scanpy/issues/1396:165,security,sign,significantly,165,"> More interesting is that regress_out becomes lightning fast when n_jobs = 24 and with BLAS multi threading disabled:. Thats not too surprising to me. This must be significantly over scheduling your machine. --------------------------------. This got me doing a little more digging into this, and it look's like there's actually a solution now! We can use [`threadpoolctl`](https://github.com/joblib/threadpoolctl) to dynamically manage the number of threads BLAS uses via the `threadpool_limits` context manager. . I'm definitely interested in using this inside scanpy to manage the number of threads used here. Not quite sure yet what the right behaviour/ api is. Some options:. * Should all calls use 1 blas thread by default, so parallelization will only happen through the number of jobs? * Do we only limit the number of threads if `n_jobs` is specified? . * Do we try and be fancy, with something like `n_threads = n_cpus // n_jobs`? *Minor update*. [If we use `joblib` (with the `loky` backend) instead of `multiprocessing`, the fancy solution will be used by default.](https://joblib.readthedocs.io/en/latest/parallel.html#avoiding-over-subscription-of-cpu-resources). I think this is what the code would look like inside of `regress_out`:. ```python. from joblib import Parallel, delayed. res = Parallel(n_jobs=n_jobs)(delayed(_regress_out_chunk)(task) for task in tasks). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1396
https://github.com/scverse/scanpy/issues/1396:949,security,updat,update,949,"> More interesting is that regress_out becomes lightning fast when n_jobs = 24 and with BLAS multi threading disabled:. Thats not too surprising to me. This must be significantly over scheduling your machine. --------------------------------. This got me doing a little more digging into this, and it look's like there's actually a solution now! We can use [`threadpoolctl`](https://github.com/joblib/threadpoolctl) to dynamically manage the number of threads BLAS uses via the `threadpool_limits` context manager. . I'm definitely interested in using this inside scanpy to manage the number of threads used here. Not quite sure yet what the right behaviour/ api is. Some options:. * Should all calls use 1 blas thread by default, so parallelization will only happen through the number of jobs? * Do we only limit the number of threads if `n_jobs` is specified? . * Do we try and be fancy, with something like `n_threads = n_cpus // n_jobs`? *Minor update*. [If we use `joblib` (with the `loky` backend) instead of `multiprocessing`, the fancy solution will be used by default.](https://joblib.readthedocs.io/en/latest/parallel.html#avoiding-over-subscription-of-cpu-resources). I think this is what the code would look like inside of `regress_out`:. ```python. from joblib import Parallel, delayed. res = Parallel(n_jobs=n_jobs)(delayed(_regress_out_chunk)(task) for task in tasks). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1396
https://github.com/scverse/scanpy/issues/1396:498,testability,context,context,498,"> More interesting is that regress_out becomes lightning fast when n_jobs = 24 and with BLAS multi threading disabled:. Thats not too surprising to me. This must be significantly over scheduling your machine. --------------------------------. This got me doing a little more digging into this, and it look's like there's actually a solution now! We can use [`threadpoolctl`](https://github.com/joblib/threadpoolctl) to dynamically manage the number of threads BLAS uses via the `threadpool_limits` context manager. . I'm definitely interested in using this inside scanpy to manage the number of threads used here. Not quite sure yet what the right behaviour/ api is. Some options:. * Should all calls use 1 blas thread by default, so parallelization will only happen through the number of jobs? * Do we only limit the number of threads if `n_jobs` is specified? . * Do we try and be fancy, with something like `n_threads = n_cpus // n_jobs`? *Minor update*. [If we use `joblib` (with the `loky` backend) instead of `multiprocessing`, the fancy solution will be used by default.](https://joblib.readthedocs.io/en/latest/parallel.html#avoiding-over-subscription-of-cpu-resources). I think this is what the code would look like inside of `regress_out`:. ```python. from joblib import Parallel, delayed. res = Parallel(n_jobs=n_jobs)(delayed(_regress_out_chunk)(task) for task in tasks). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1396
https://github.com/scverse/scanpy/issues/1396:1167,testability,resourc,resources,1167,"> More interesting is that regress_out becomes lightning fast when n_jobs = 24 and with BLAS multi threading disabled:. Thats not too surprising to me. This must be significantly over scheduling your machine. --------------------------------. This got me doing a little more digging into this, and it look's like there's actually a solution now! We can use [`threadpoolctl`](https://github.com/joblib/threadpoolctl) to dynamically manage the number of threads BLAS uses via the `threadpool_limits` context manager. . I'm definitely interested in using this inside scanpy to manage the number of threads used here. Not quite sure yet what the right behaviour/ api is. Some options:. * Should all calls use 1 blas thread by default, so parallelization will only happen through the number of jobs? * Do we only limit the number of threads if `n_jobs` is specified? . * Do we try and be fancy, with something like `n_threads = n_cpus // n_jobs`? *Minor update*. [If we use `joblib` (with the `loky` backend) instead of `multiprocessing`, the fancy solution will be used by default.](https://joblib.readthedocs.io/en/latest/parallel.html#avoiding-over-subscription-of-cpu-resources). I think this is what the code would look like inside of `regress_out`:. ```python. from joblib import Parallel, delayed. res = Parallel(n_jobs=n_jobs)(delayed(_regress_out_chunk)(task) for task in tasks). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1396
https://github.com/scverse/scanpy/issues/1396:648,usability,behavi,behaviour,648,"> More interesting is that regress_out becomes lightning fast when n_jobs = 24 and with BLAS multi threading disabled:. Thats not too surprising to me. This must be significantly over scheduling your machine. --------------------------------. This got me doing a little more digging into this, and it look's like there's actually a solution now! We can use [`threadpoolctl`](https://github.com/joblib/threadpoolctl) to dynamically manage the number of threads BLAS uses via the `threadpool_limits` context manager. . I'm definitely interested in using this inside scanpy to manage the number of threads used here. Not quite sure yet what the right behaviour/ api is. Some options:. * Should all calls use 1 blas thread by default, so parallelization will only happen through the number of jobs? * Do we only limit the number of threads if `n_jobs` is specified? . * Do we try and be fancy, with something like `n_threads = n_cpus // n_jobs`? *Minor update*. [If we use `joblib` (with the `loky` backend) instead of `multiprocessing`, the fancy solution will be used by default.](https://joblib.readthedocs.io/en/latest/parallel.html#avoiding-over-subscription-of-cpu-resources). I think this is what the code would look like inside of `regress_out`:. ```python. from joblib import Parallel, delayed. res = Parallel(n_jobs=n_jobs)(delayed(_regress_out_chunk)(task) for task in tasks). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1396
https://github.com/scverse/scanpy/issues/1396:90,availability,state,stated,90,"Is there a general fix for this besides zhangguy's great suggestion? (It works but as was stated, but I'm not sure how this alters other functions in the scanpy). I continue to get the putative over scheduling and sometimes a crash (on big datasets) when using all cores versus the super fast completion when using 1/2 cores) on a 32 core/64 thread threadripper. (RAM doesn't seem to be a problem as I'm barely touching 10% of the 256GB.)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1396
https://github.com/scverse/scanpy/issues/1396:165,deployability,continu,continue,165,"Is there a general fix for this besides zhangguy's great suggestion? (It works but as was stated, but I'm not sure how this alters other functions in the scanpy). I continue to get the putative over scheduling and sometimes a crash (on big datasets) when using all cores versus the super fast completion when using 1/2 cores) on a 32 core/64 thread threadripper. (RAM doesn't seem to be a problem as I'm barely touching 10% of the 256GB.)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1396
https://github.com/scverse/scanpy/issues/1396:199,energy efficiency,schedul,scheduling,199,"Is there a general fix for this besides zhangguy's great suggestion? (It works but as was stated, but I'm not sure how this alters other functions in the scanpy). I continue to get the putative over scheduling and sometimes a crash (on big datasets) when using all cores versus the super fast completion when using 1/2 cores) on a 32 core/64 thread threadripper. (RAM doesn't seem to be a problem as I'm barely touching 10% of the 256GB.)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1396
https://github.com/scverse/scanpy/issues/1396:265,energy efficiency,core,cores,265,"Is there a general fix for this besides zhangguy's great suggestion? (It works but as was stated, but I'm not sure how this alters other functions in the scanpy). I continue to get the putative over scheduling and sometimes a crash (on big datasets) when using all cores versus the super fast completion when using 1/2 cores) on a 32 core/64 thread threadripper. (RAM doesn't seem to be a problem as I'm barely touching 10% of the 256GB.)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1396
https://github.com/scverse/scanpy/issues/1396:319,energy efficiency,core,cores,319,"Is there a general fix for this besides zhangguy's great suggestion? (It works but as was stated, but I'm not sure how this alters other functions in the scanpy). I continue to get the putative over scheduling and sometimes a crash (on big datasets) when using all cores versus the super fast completion when using 1/2 cores) on a 32 core/64 thread threadripper. (RAM doesn't seem to be a problem as I'm barely touching 10% of the 256GB.)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1396
https://github.com/scverse/scanpy/issues/1396:334,energy efficiency,core,core,334,"Is there a general fix for this besides zhangguy's great suggestion? (It works but as was stated, but I'm not sure how this alters other functions in the scanpy). I continue to get the putative over scheduling and sometimes a crash (on big datasets) when using all cores versus the super fast completion when using 1/2 cores) on a 32 core/64 thread threadripper. (RAM doesn't seem to be a problem as I'm barely touching 10% of the 256GB.)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1396
https://github.com/scverse/scanpy/issues/1396:90,integrability,state,stated,90,"Is there a general fix for this besides zhangguy's great suggestion? (It works but as was stated, but I'm not sure how this alters other functions in the scanpy). I continue to get the putative over scheduling and sometimes a crash (on big datasets) when using all cores versus the super fast completion when using 1/2 cores) on a 32 core/64 thread threadripper. (RAM doesn't seem to be a problem as I'm barely touching 10% of the 256GB.)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1396
https://github.com/scverse/scanpy/issues/1396:199,performance,schedul,scheduling,199,"Is there a general fix for this besides zhangguy's great suggestion? (It works but as was stated, but I'm not sure how this alters other functions in the scanpy). I continue to get the putative over scheduling and sometimes a crash (on big datasets) when using all cores versus the super fast completion when using 1/2 cores) on a 32 core/64 thread threadripper. (RAM doesn't seem to be a problem as I'm barely touching 10% of the 256GB.)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1396
https://github.com/scverse/scanpy/issues/1396:368,reliability,doe,doesn,368,"Is there a general fix for this besides zhangguy's great suggestion? (It works but as was stated, but I'm not sure how this alters other functions in the scanpy). I continue to get the putative over scheduling and sometimes a crash (on big datasets) when using all cores versus the super fast completion when using 1/2 cores) on a 32 core/64 thread threadripper. (RAM doesn't seem to be a problem as I'm barely touching 10% of the 256GB.)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1396
https://github.com/scverse/scanpy/issues/1396:293,safety,compl,completion,293,"Is there a general fix for this besides zhangguy's great suggestion? (It works but as was stated, but I'm not sure how this alters other functions in the scanpy). I continue to get the putative over scheduling and sometimes a crash (on big datasets) when using all cores versus the super fast completion when using 1/2 cores) on a 32 core/64 thread threadripper. (RAM doesn't seem to be a problem as I'm barely touching 10% of the 256GB.)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1396
https://github.com/scverse/scanpy/issues/1396:293,security,compl,completion,293,"Is there a general fix for this besides zhangguy's great suggestion? (It works but as was stated, but I'm not sure how this alters other functions in the scanpy). I continue to get the putative over scheduling and sometimes a crash (on big datasets) when using all cores versus the super fast completion when using 1/2 cores) on a 32 core/64 thread threadripper. (RAM doesn't seem to be a problem as I'm barely touching 10% of the 256GB.)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1396
https://github.com/scverse/scanpy/issues/1397:260,availability,state,statements,260,"Thanks for the bug report! I think you should use:. ```python. import scanpy as sc. ```. instead of. ```python. import scanpy.api as sc. ```. Is there any reason you're using `scanpy.api`? @flying-sheep @falexwolf, was there a reason we didn't just make those statements equivalent?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:126,deployability,api,api,126,"Thanks for the bug report! I think you should use:. ```python. import scanpy as sc. ```. instead of. ```python. import scanpy.api as sc. ```. Is there any reason you're using `scanpy.api`? @flying-sheep @falexwolf, was there a reason we didn't just make those statements equivalent?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:183,deployability,api,api,183,"Thanks for the bug report! I think you should use:. ```python. import scanpy as sc. ```. instead of. ```python. import scanpy.api as sc. ```. Is there any reason you're using `scanpy.api`? @flying-sheep @falexwolf, was there a reason we didn't just make those statements equivalent?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:126,integrability,api,api,126,"Thanks for the bug report! I think you should use:. ```python. import scanpy as sc. ```. instead of. ```python. import scanpy.api as sc. ```. Is there any reason you're using `scanpy.api`? @flying-sheep @falexwolf, was there a reason we didn't just make those statements equivalent?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:183,integrability,api,api,183,"Thanks for the bug report! I think you should use:. ```python. import scanpy as sc. ```. instead of. ```python. import scanpy.api as sc. ```. Is there any reason you're using `scanpy.api`? @flying-sheep @falexwolf, was there a reason we didn't just make those statements equivalent?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:260,integrability,state,statements,260,"Thanks for the bug report! I think you should use:. ```python. import scanpy as sc. ```. instead of. ```python. import scanpy.api as sc. ```. Is there any reason you're using `scanpy.api`? @flying-sheep @falexwolf, was there a reason we didn't just make those statements equivalent?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:126,interoperability,api,api,126,"Thanks for the bug report! I think you should use:. ```python. import scanpy as sc. ```. instead of. ```python. import scanpy.api as sc. ```. Is there any reason you're using `scanpy.api`? @flying-sheep @falexwolf, was there a reason we didn't just make those statements equivalent?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:183,interoperability,api,api,183,"Thanks for the bug report! I think you should use:. ```python. import scanpy as sc. ```. instead of. ```python. import scanpy.api as sc. ```. Is there any reason you're using `scanpy.api`? @flying-sheep @falexwolf, was there a reason we didn't just make those statements equivalent?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:197,availability,down,downgrading,197,"I'm actually testing and tweaking someone else's code that was written a while ago. I assume they used. `import scanpy.api as sc` because it was appropriate then. I personally resolved my issue by downgrading versions, I just wanted to bring this up!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:119,deployability,api,api,119,"I'm actually testing and tweaking someone else's code that was written a while ago. I assume they used. `import scanpy.api as sc` because it was appropriate then. I personally resolved my issue by downgrading versions, I just wanted to bring this up!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:209,deployability,version,versions,209,"I'm actually testing and tweaking someone else's code that was written a while ago. I assume they used. `import scanpy.api as sc` because it was appropriate then. I personally resolved my issue by downgrading versions, I just wanted to bring this up!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:119,integrability,api,api,119,"I'm actually testing and tweaking someone else's code that was written a while ago. I assume they used. `import scanpy.api as sc` because it was appropriate then. I personally resolved my issue by downgrading versions, I just wanted to bring this up!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:209,integrability,version,versions,209,"I'm actually testing and tweaking someone else's code that was written a while ago. I assume they used. `import scanpy.api as sc` because it was appropriate then. I personally resolved my issue by downgrading versions, I just wanted to bring this up!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:119,interoperability,api,api,119,"I'm actually testing and tweaking someone else's code that was written a while ago. I assume they used. `import scanpy.api as sc` because it was appropriate then. I personally resolved my issue by downgrading versions, I just wanted to bring this up!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:209,modifiability,version,versions,209,"I'm actually testing and tweaking someone else's code that was written a while ago. I assume they used. `import scanpy.api as sc` because it was appropriate then. I personally resolved my issue by downgrading versions, I just wanted to bring this up!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:13,safety,test,testing,13,"I'm actually testing and tweaking someone else's code that was written a while ago. I assume they used. `import scanpy.api as sc` because it was appropriate then. I personally resolved my issue by downgrading versions, I just wanted to bring this up!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:13,testability,test,testing,13,"I'm actually testing and tweaking someone else's code that was written a while ago. I assume they used. `import scanpy.api as sc` because it was appropriate then. I personally resolved my issue by downgrading versions, I just wanted to bring this up!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:165,usability,person,personally,165,"I'm actually testing and tweaking someone else's code that was written a while ago. I assume they used. `import scanpy.api as sc` because it was appropriate then. I personally resolved my issue by downgrading versions, I just wanted to bring this up!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:201,availability,down,downgrading,201,"> I'm actually testing and tweaking someone else's code that was written a while ago. I assume they used. > `import scanpy.api as sc` because it was appropriate then. I personally resolved my issue by downgrading versions, I just wanted to bring this up! I encountered the same issue. Which version are you using to fix this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:123,deployability,api,api,123,"> I'm actually testing and tweaking someone else's code that was written a while ago. I assume they used. > `import scanpy.api as sc` because it was appropriate then. I personally resolved my issue by downgrading versions, I just wanted to bring this up! I encountered the same issue. Which version are you using to fix this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:213,deployability,version,versions,213,"> I'm actually testing and tweaking someone else's code that was written a while ago. I assume they used. > `import scanpy.api as sc` because it was appropriate then. I personally resolved my issue by downgrading versions, I just wanted to bring this up! I encountered the same issue. Which version are you using to fix this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:291,deployability,version,version,291,"> I'm actually testing and tweaking someone else's code that was written a while ago. I assume they used. > `import scanpy.api as sc` because it was appropriate then. I personally resolved my issue by downgrading versions, I just wanted to bring this up! I encountered the same issue. Which version are you using to fix this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:123,integrability,api,api,123,"> I'm actually testing and tweaking someone else's code that was written a while ago. I assume they used. > `import scanpy.api as sc` because it was appropriate then. I personally resolved my issue by downgrading versions, I just wanted to bring this up! I encountered the same issue. Which version are you using to fix this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:213,integrability,version,versions,213,"> I'm actually testing and tweaking someone else's code that was written a while ago. I assume they used. > `import scanpy.api as sc` because it was appropriate then. I personally resolved my issue by downgrading versions, I just wanted to bring this up! I encountered the same issue. Which version are you using to fix this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:291,integrability,version,version,291,"> I'm actually testing and tweaking someone else's code that was written a while ago. I assume they used. > `import scanpy.api as sc` because it was appropriate then. I personally resolved my issue by downgrading versions, I just wanted to bring this up! I encountered the same issue. Which version are you using to fix this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:123,interoperability,api,api,123,"> I'm actually testing and tweaking someone else's code that was written a while ago. I assume they used. > `import scanpy.api as sc` because it was appropriate then. I personally resolved my issue by downgrading versions, I just wanted to bring this up! I encountered the same issue. Which version are you using to fix this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:213,modifiability,version,versions,213,"> I'm actually testing and tweaking someone else's code that was written a while ago. I assume they used. > `import scanpy.api as sc` because it was appropriate then. I personally resolved my issue by downgrading versions, I just wanted to bring this up! I encountered the same issue. Which version are you using to fix this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:291,modifiability,version,version,291,"> I'm actually testing and tweaking someone else's code that was written a while ago. I assume they used. > `import scanpy.api as sc` because it was appropriate then. I personally resolved my issue by downgrading versions, I just wanted to bring this up! I encountered the same issue. Which version are you using to fix this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:15,safety,test,testing,15,"> I'm actually testing and tweaking someone else's code that was written a while ago. I assume they used. > `import scanpy.api as sc` because it was appropriate then. I personally resolved my issue by downgrading versions, I just wanted to bring this up! I encountered the same issue. Which version are you using to fix this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:15,testability,test,testing,15,"> I'm actually testing and tweaking someone else's code that was written a while ago. I assume they used. > `import scanpy.api as sc` because it was appropriate then. I personally resolved my issue by downgrading versions, I just wanted to bring this up! I encountered the same issue. Which version are you using to fix this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:169,usability,person,personally,169,"> I'm actually testing and tweaking someone else's code that was written a while ago. I assume they used. > `import scanpy.api as sc` because it was appropriate then. I personally resolved my issue by downgrading versions, I just wanted to bring this up! I encountered the same issue. Which version are you using to fix this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:205,availability,down,downgrading,205,"> > I'm actually testing and tweaking someone else's code that was written a while ago. I assume they used. > > `import scanpy.api as sc` because it was appropriate then. I personally resolved my issue by downgrading versions, I just wanted to bring this up! > . > I encountered the same issue. Which version are you using to fix this? nm, downgrading to 1.5.1 fixed my problem. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:340,availability,down,downgrading,340,"> > I'm actually testing and tweaking someone else's code that was written a while ago. I assume they used. > > `import scanpy.api as sc` because it was appropriate then. I personally resolved my issue by downgrading versions, I just wanted to bring this up! > . > I encountered the same issue. Which version are you using to fix this? nm, downgrading to 1.5.1 fixed my problem. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:127,deployability,api,api,127,"> > I'm actually testing and tweaking someone else's code that was written a while ago. I assume they used. > > `import scanpy.api as sc` because it was appropriate then. I personally resolved my issue by downgrading versions, I just wanted to bring this up! > . > I encountered the same issue. Which version are you using to fix this? nm, downgrading to 1.5.1 fixed my problem. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:217,deployability,version,versions,217,"> > I'm actually testing and tweaking someone else's code that was written a while ago. I assume they used. > > `import scanpy.api as sc` because it was appropriate then. I personally resolved my issue by downgrading versions, I just wanted to bring this up! > . > I encountered the same issue. Which version are you using to fix this? nm, downgrading to 1.5.1 fixed my problem. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:301,deployability,version,version,301,"> > I'm actually testing and tweaking someone else's code that was written a while ago. I assume they used. > > `import scanpy.api as sc` because it was appropriate then. I personally resolved my issue by downgrading versions, I just wanted to bring this up! > . > I encountered the same issue. Which version are you using to fix this? nm, downgrading to 1.5.1 fixed my problem. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:127,integrability,api,api,127,"> > I'm actually testing and tweaking someone else's code that was written a while ago. I assume they used. > > `import scanpy.api as sc` because it was appropriate then. I personally resolved my issue by downgrading versions, I just wanted to bring this up! > . > I encountered the same issue. Which version are you using to fix this? nm, downgrading to 1.5.1 fixed my problem. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:217,integrability,version,versions,217,"> > I'm actually testing and tweaking someone else's code that was written a while ago. I assume they used. > > `import scanpy.api as sc` because it was appropriate then. I personally resolved my issue by downgrading versions, I just wanted to bring this up! > . > I encountered the same issue. Which version are you using to fix this? nm, downgrading to 1.5.1 fixed my problem. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:301,integrability,version,version,301,"> > I'm actually testing and tweaking someone else's code that was written a while ago. I assume they used. > > `import scanpy.api as sc` because it was appropriate then. I personally resolved my issue by downgrading versions, I just wanted to bring this up! > . > I encountered the same issue. Which version are you using to fix this? nm, downgrading to 1.5.1 fixed my problem. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:127,interoperability,api,api,127,"> > I'm actually testing and tweaking someone else's code that was written a while ago. I assume they used. > > `import scanpy.api as sc` because it was appropriate then. I personally resolved my issue by downgrading versions, I just wanted to bring this up! > . > I encountered the same issue. Which version are you using to fix this? nm, downgrading to 1.5.1 fixed my problem. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:217,modifiability,version,versions,217,"> > I'm actually testing and tweaking someone else's code that was written a while ago. I assume they used. > > `import scanpy.api as sc` because it was appropriate then. I personally resolved my issue by downgrading versions, I just wanted to bring this up! > . > I encountered the same issue. Which version are you using to fix this? nm, downgrading to 1.5.1 fixed my problem. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:301,modifiability,version,version,301,"> > I'm actually testing and tweaking someone else's code that was written a while ago. I assume they used. > > `import scanpy.api as sc` because it was appropriate then. I personally resolved my issue by downgrading versions, I just wanted to bring this up! > . > I encountered the same issue. Which version are you using to fix this? nm, downgrading to 1.5.1 fixed my problem. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:17,safety,test,testing,17,"> > I'm actually testing and tweaking someone else's code that was written a while ago. I assume they used. > > `import scanpy.api as sc` because it was appropriate then. I personally resolved my issue by downgrading versions, I just wanted to bring this up! > . > I encountered the same issue. Which version are you using to fix this? nm, downgrading to 1.5.1 fixed my problem. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:17,testability,test,testing,17,"> > I'm actually testing and tweaking someone else's code that was written a while ago. I assume they used. > > `import scanpy.api as sc` because it was appropriate then. I personally resolved my issue by downgrading versions, I just wanted to bring this up! > . > I encountered the same issue. Which version are you using to fix this? nm, downgrading to 1.5.1 fixed my problem. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:173,usability,person,personally,173,"> > I'm actually testing and tweaking someone else's code that was written a while ago. I assume they used. > > `import scanpy.api as sc` because it was appropriate then. I personally resolved my issue by downgrading versions, I just wanted to bring this up! > . > I encountered the same issue. Which version are you using to fix this? nm, downgrading to 1.5.1 fixed my problem. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:131,availability,ERROR,ERROR,131,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```. scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================. ERROR: scanpy.api (unittest.loader._FailedTest). ----------------------------------------------------------------------. ImportError: Failed to import test module: scanpy.api. Traceback (most recent call last):. File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path. package = self._get_module_from_name(name). File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name. __import__(name). File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>. from . import pl. File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>. from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------. Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully. ```. --- a/scanpy/api/pl.py. +++ b/scanpy/api/pl.py. @@ -1,4 +1,7 @@. -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot. +from ..plotting._stacked_violin import stacked_violin. +from ..plotting._dotplot import dotplot. +from ..plotting._matrixplot import matrixplot. . from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes. . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:210,availability,ERROR,ERROR,210,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```. scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================. ERROR: scanpy.api (unittest.loader._FailedTest). ----------------------------------------------------------------------. ImportError: Failed to import test module: scanpy.api. Traceback (most recent call last):. File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path. package = self._get_module_from_name(name). File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name. __import__(name). File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>. from . import pl. File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>. from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------. Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully. ```. --- a/scanpy/api/pl.py. +++ b/scanpy/api/pl.py. @@ -1,4 +1,7 @@. -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot. +from ..plotting._stacked_violin import stacked_violin. +from ..plotting._dotplot import dotplot. +from ..plotting._matrixplot import matrixplot. . from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes. . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:921,availability,cluster,clustermap,921,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```. scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================. ERROR: scanpy.api (unittest.loader._FailedTest). ----------------------------------------------------------------------. ImportError: Failed to import test module: scanpy.api. Traceback (most recent call last):. File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path. package = self._get_module_from_name(name). File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name. __import__(name). File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>. from . import pl. File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>. from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------. Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully. ```. --- a/scanpy/api/pl.py. +++ b/scanpy/api/pl.py. @@ -1,4 +1,7 @@. -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot. +from ..plotting._stacked_violin import stacked_violin. +from ..plotting._dotplot import dotplot. +from ..plotting._matrixplot import matrixplot. . from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes. . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:1448,availability,cluster,clustermap,1448,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```. scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================. ERROR: scanpy.api (unittest.loader._FailedTest). ----------------------------------------------------------------------. ImportError: Failed to import test module: scanpy.api. Traceback (most recent call last):. File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path. package = self._get_module_from_name(name). File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name. __import__(name). File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>. from . import pl. File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>. from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------. Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully. ```. --- a/scanpy/api/pl.py. +++ b/scanpy/api/pl.py. @@ -1,4 +1,7 @@. -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot. +from ..plotting._stacked_violin import stacked_violin. +from ..plotting._dotplot import dotplot. +from ..plotting._matrixplot import matrixplot. . from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes. . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:1577,availability,cluster,clustermap,1577,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```. scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================. ERROR: scanpy.api (unittest.loader._FailedTest). ----------------------------------------------------------------------. ImportError: Failed to import test module: scanpy.api. Traceback (most recent call last):. File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path. package = self._get_module_from_name(name). File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name. __import__(name). File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>. from . import pl. File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>. from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------. Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully. ```. --- a/scanpy/api/pl.py. +++ b/scanpy/api/pl.py. @@ -1,4 +1,7 @@. -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot. +from ..plotting._stacked_violin import stacked_violin. +from ..plotting._dotplot import dotplot. +from ..plotting._matrixplot import matrixplot. . from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes. . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:93,deployability,api,api,93,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```. scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================. ERROR: scanpy.api (unittest.loader._FailedTest). ----------------------------------------------------------------------. ImportError: Failed to import test module: scanpy.api. Traceback (most recent call last):. File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path. package = self._get_module_from_name(name). File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name. __import__(name). File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>. from . import pl. File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>. from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------. Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully. ```. --- a/scanpy/api/pl.py. +++ b/scanpy/api/pl.py. @@ -1,4 +1,7 @@. -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot. +from ..plotting._stacked_violin import stacked_violin. +from ..plotting._dotplot import dotplot. +from ..plotting._matrixplot import matrixplot. . from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes. . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:107,deployability,loader,loader,107,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```. scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================. ERROR: scanpy.api (unittest.loader._FailedTest). ----------------------------------------------------------------------. ImportError: Failed to import test module: scanpy.api. Traceback (most recent call last):. File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path. package = self._get_module_from_name(name). File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name. __import__(name). File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>. from . import pl. File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>. from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------. Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully. ```. --- a/scanpy/api/pl.py. +++ b/scanpy/api/pl.py. @@ -1,4 +1,7 @@. -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot. +from ..plotting._stacked_violin import stacked_violin. +from ..plotting._dotplot import dotplot. +from ..plotting._matrixplot import matrixplot. . from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes. . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:224,deployability,api,api,224,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```. scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================. ERROR: scanpy.api (unittest.loader._FailedTest). ----------------------------------------------------------------------. ImportError: Failed to import test module: scanpy.api. Traceback (most recent call last):. File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path. package = self._get_module_from_name(name). File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name. __import__(name). File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>. from . import pl. File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>. from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------. Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully. ```. --- a/scanpy/api/pl.py. +++ b/scanpy/api/pl.py. @@ -1,4 +1,7 @@. -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot. +from ..plotting._stacked_violin import stacked_violin. +from ..plotting._dotplot import dotplot. +from ..plotting._matrixplot import matrixplot. . from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes. . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:238,deployability,loader,loader,238,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```. scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================. ERROR: scanpy.api (unittest.loader._FailedTest). ----------------------------------------------------------------------. ImportError: Failed to import test module: scanpy.api. Traceback (most recent call last):. File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path. package = self._get_module_from_name(name). File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name. __import__(name). File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>. from . import pl. File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>. from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------. Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully. ```. --- a/scanpy/api/pl.py. +++ b/scanpy/api/pl.py. @@ -1,4 +1,7 @@. -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot. +from ..plotting._stacked_violin import stacked_violin. +from ..plotting._dotplot import dotplot. +from ..plotting._matrixplot import matrixplot. . from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes. . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:344,deployability,Fail,Failed,344,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```. scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================. ERROR: scanpy.api (unittest.loader._FailedTest). ----------------------------------------------------------------------. ImportError: Failed to import test module: scanpy.api. Traceback (most recent call last):. File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path. package = self._get_module_from_name(name). File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name. __import__(name). File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>. from . import pl. File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>. from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------. Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully. ```. --- a/scanpy/api/pl.py. +++ b/scanpy/api/pl.py. @@ -1,4 +1,7 @@. -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot. +from ..plotting._stacked_violin import stacked_violin. +from ..plotting._dotplot import dotplot. +from ..plotting._matrixplot import matrixplot. . from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes. . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:366,deployability,modul,module,366,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```. scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================. ERROR: scanpy.api (unittest.loader._FailedTest). ----------------------------------------------------------------------. ImportError: Failed to import test module: scanpy.api. Traceback (most recent call last):. File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path. package = self._get_module_from_name(name). File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name. __import__(name). File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>. from . import pl. File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>. from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------. Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully. ```. --- a/scanpy/api/pl.py. +++ b/scanpy/api/pl.py. @@ -1,4 +1,7 @@. -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot. +from ..plotting._stacked_violin import stacked_violin. +from ..plotting._dotplot import dotplot. +from ..plotting._matrixplot import matrixplot. . from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes. . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:381,deployability,api,api,381,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```. scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================. ERROR: scanpy.api (unittest.loader._FailedTest). ----------------------------------------------------------------------. ImportError: Failed to import test module: scanpy.api. Traceback (most recent call last):. File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path. package = self._get_module_from_name(name). File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name. __import__(name). File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>. from . import pl. File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>. from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------. Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully. ```. --- a/scanpy/api/pl.py. +++ b/scanpy/api/pl.py. @@ -1,4 +1,7 @@. -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot. +from ..plotting._stacked_violin import stacked_violin. +from ..plotting._dotplot import dotplot. +from ..plotting._matrixplot import matrixplot. . from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes. . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:456,deployability,loader,loader,456,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```. scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================. ERROR: scanpy.api (unittest.loader._FailedTest). ----------------------------------------------------------------------. ImportError: Failed to import test module: scanpy.api. Traceback (most recent call last):. File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path. package = self._get_module_from_name(name). File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name. __import__(name). File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>. from . import pl. File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>. from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------. Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully. ```. --- a/scanpy/api/pl.py. +++ b/scanpy/api/pl.py. @@ -1,4 +1,7 @@. -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot. +from ..plotting._stacked_violin import stacked_violin. +from ..plotting._dotplot import dotplot. +from ..plotting._matrixplot import matrixplot. . from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes. . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:576,deployability,loader,loader,576,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```. scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================. ERROR: scanpy.api (unittest.loader._FailedTest). ----------------------------------------------------------------------. ImportError: Failed to import test module: scanpy.api. Traceback (most recent call last):. File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path. package = self._get_module_from_name(name). File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name. __import__(name). File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>. from . import pl. File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>. from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------. Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully. ```. --- a/scanpy/api/pl.py. +++ b/scanpy/api/pl.py. @@ -1,4 +1,7 @@. -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot. +from ..plotting._stacked_violin import stacked_violin. +from ..plotting._dotplot import dotplot. +from ..plotting._matrixplot import matrixplot. . from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes. . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:694,deployability,build,build,694,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```. scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================. ERROR: scanpy.api (unittest.loader._FailedTest). ----------------------------------------------------------------------. ImportError: Failed to import test module: scanpy.api. Traceback (most recent call last):. File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path. package = self._get_module_from_name(name). File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name. __import__(name). File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>. from . import pl. File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>. from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------. Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully. ```. --- a/scanpy/api/pl.py. +++ b/scanpy/api/pl.py. @@ -1,4 +1,7 @@. -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot. +from ..plotting._stacked_violin import stacked_violin. +from ..plotting._dotplot import dotplot. +from ..plotting._matrixplot import matrixplot. . from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes. . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:707,deployability,api,api,707,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```. scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================. ERROR: scanpy.api (unittest.loader._FailedTest). ----------------------------------------------------------------------. ImportError: Failed to import test module: scanpy.api. Traceback (most recent call last):. File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path. package = self._get_module_from_name(name). File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name. __import__(name). File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>. from . import pl. File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>. from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------. Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully. ```. --- a/scanpy/api/pl.py. +++ b/scanpy/api/pl.py. @@ -1,4 +1,7 @@. -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot. +from ..plotting._stacked_violin import stacked_violin. +from ..plotting._dotplot import dotplot. +from ..plotting._matrixplot import matrixplot. . from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes. . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:738,deployability,modul,module,738,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```. scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================. ERROR: scanpy.api (unittest.loader._FailedTest). ----------------------------------------------------------------------. ImportError: Failed to import test module: scanpy.api. Traceback (most recent call last):. File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path. package = self._get_module_from_name(name). File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name. __import__(name). File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>. from . import pl. File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>. from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------. Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully. ```. --- a/scanpy/api/pl.py. +++ b/scanpy/api/pl.py. @@ -1,4 +1,7 @@. -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot. +from ..plotting._stacked_violin import stacked_violin. +from ..plotting._dotplot import dotplot. +from ..plotting._matrixplot import matrixplot. . from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes. . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:817,deployability,build,build,817,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```. scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================. ERROR: scanpy.api (unittest.loader._FailedTest). ----------------------------------------------------------------------. ImportError: Failed to import test module: scanpy.api. Traceback (most recent call last):. File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path. package = self._get_module_from_name(name). File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name. __import__(name). File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>. from . import pl. File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>. from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------. Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully. ```. --- a/scanpy/api/pl.py. +++ b/scanpy/api/pl.py. @@ -1,4 +1,7 @@. -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot. +from ..plotting._stacked_violin import stacked_violin. +from ..plotting._dotplot import dotplot. +from ..plotting._matrixplot import matrixplot. . from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes. . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:830,deployability,api,api,830,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```. scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================. ERROR: scanpy.api (unittest.loader._FailedTest). ----------------------------------------------------------------------. ImportError: Failed to import test module: scanpy.api. Traceback (most recent call last):. File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path. package = self._get_module_from_name(name). File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name. __import__(name). File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>. from . import pl. File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>. from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------. Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully. ```. --- a/scanpy/api/pl.py. +++ b/scanpy/api/pl.py. @@ -1,4 +1,7 @@. -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot. +from ..plotting._stacked_violin import stacked_violin. +from ..plotting._dotplot import dotplot. +from ..plotting._matrixplot import matrixplot. . from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes. . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:854,deployability,modul,module,854,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```. scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================. ERROR: scanpy.api (unittest.loader._FailedTest). ----------------------------------------------------------------------. ImportError: Failed to import test module: scanpy.api. Traceback (most recent call last):. File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path. package = self._get_module_from_name(name). File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name. __import__(name). File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>. from . import pl. File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>. from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------. Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully. ```. --- a/scanpy/api/pl.py. +++ b/scanpy/api/pl.py. @@ -1,4 +1,7 @@. -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot. +from ..plotting._stacked_violin import stacked_violin. +from ..plotting._dotplot import dotplot. +from ..plotting._matrixplot import matrixplot. . from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes. . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:921,deployability,cluster,clustermap,921,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```. scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================. ERROR: scanpy.api (unittest.loader._FailedTest). ----------------------------------------------------------------------. ImportError: Failed to import test module: scanpy.api. Traceback (most recent call last):. File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path. package = self._get_module_from_name(name). File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name. __import__(name). File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>. from . import pl. File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>. from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------. Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully. ```. --- a/scanpy/api/pl.py. +++ b/scanpy/api/pl.py. @@ -1,4 +1,7 @@. -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot. +from ..plotting._stacked_violin import stacked_violin. +from ..plotting._dotplot import dotplot. +from ..plotting._matrixplot import matrixplot. . from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes. . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:1119,deployability,build,build,1119,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```. scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================. ERROR: scanpy.api (unittest.loader._FailedTest). ----------------------------------------------------------------------. ImportError: Failed to import test module: scanpy.api. Traceback (most recent call last):. File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path. package = self._get_module_from_name(name). File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name. __import__(name). File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>. from . import pl. File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>. from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------. Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully. ```. --- a/scanpy/api/pl.py. +++ b/scanpy/api/pl.py. @@ -1,4 +1,7 @@. -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot. +from ..plotting._stacked_violin import stacked_violin. +from ..plotting._dotplot import dotplot. +from ..plotting._matrixplot import matrixplot. . from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes. . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:1275,deployability,patch,patch,1275,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```. scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================. ERROR: scanpy.api (unittest.loader._FailedTest). ----------------------------------------------------------------------. ImportError: Failed to import test module: scanpy.api. Traceback (most recent call last):. File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path. package = self._get_module_from_name(name). File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name. __import__(name). File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>. from . import pl. File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>. from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------. Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully. ```. --- a/scanpy/api/pl.py. +++ b/scanpy/api/pl.py. @@ -1,4 +1,7 @@. -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot. +from ..plotting._stacked_violin import stacked_violin. +from ..plotting._dotplot import dotplot. +from ..plotting._matrixplot import matrixplot. . from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes. . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:1337,deployability,api,api,1337,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```. scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================. ERROR: scanpy.api (unittest.loader._FailedTest). ----------------------------------------------------------------------. ImportError: Failed to import test module: scanpy.api. Traceback (most recent call last):. File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path. package = self._get_module_from_name(name). File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name. __import__(name). File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>. from . import pl. File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>. from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------. Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully. ```. --- a/scanpy/api/pl.py. +++ b/scanpy/api/pl.py. @@ -1,4 +1,7 @@. -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot. +from ..plotting._stacked_violin import stacked_violin. +from ..plotting._dotplot import dotplot. +from ..plotting._matrixplot import matrixplot. . from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes. . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:1361,deployability,api,api,1361,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```. scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================. ERROR: scanpy.api (unittest.loader._FailedTest). ----------------------------------------------------------------------. ImportError: Failed to import test module: scanpy.api. Traceback (most recent call last):. File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path. package = self._get_module_from_name(name). File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name. __import__(name). File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>. from . import pl. File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>. from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------. Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully. ```. --- a/scanpy/api/pl.py. +++ b/scanpy/api/pl.py. @@ -1,4 +1,7 @@. -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot. +from ..plotting._stacked_violin import stacked_violin. +from ..plotting._dotplot import dotplot. +from ..plotting._matrixplot import matrixplot. . from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes. . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:1448,deployability,cluster,clustermap,1448,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```. scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================. ERROR: scanpy.api (unittest.loader._FailedTest). ----------------------------------------------------------------------. ImportError: Failed to import test module: scanpy.api. Traceback (most recent call last):. File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path. package = self._get_module_from_name(name). File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name. __import__(name). File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>. from . import pl. File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>. from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------. Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully. ```. --- a/scanpy/api/pl.py. +++ b/scanpy/api/pl.py. @@ -1,4 +1,7 @@. -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot. +from ..plotting._stacked_violin import stacked_violin. +from ..plotting._dotplot import dotplot. +from ..plotting._matrixplot import matrixplot. . from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes. . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:1577,deployability,cluster,clustermap,1577,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```. scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================. ERROR: scanpy.api (unittest.loader._FailedTest). ----------------------------------------------------------------------. ImportError: Failed to import test module: scanpy.api. Traceback (most recent call last):. File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path. package = self._get_module_from_name(name). File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name. __import__(name). File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>. from . import pl. File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>. from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------. Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully. ```. --- a/scanpy/api/pl.py. +++ b/scanpy/api/pl.py. @@ -1,4 +1,7 @@. -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot. +from ..plotting._stacked_violin import stacked_violin. +from ..plotting._dotplot import dotplot. +from ..plotting._matrixplot import matrixplot. . from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes. . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:107,energy efficiency,load,loader,107,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```. scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================. ERROR: scanpy.api (unittest.loader._FailedTest). ----------------------------------------------------------------------. ImportError: Failed to import test module: scanpy.api. Traceback (most recent call last):. File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path. package = self._get_module_from_name(name). File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name. __import__(name). File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>. from . import pl. File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>. from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------. Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully. ```. --- a/scanpy/api/pl.py. +++ b/scanpy/api/pl.py. @@ -1,4 +1,7 @@. -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot. +from ..plotting._stacked_violin import stacked_violin. +from ..plotting._dotplot import dotplot. +from ..plotting._matrixplot import matrixplot. . from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes. . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:238,energy efficiency,load,loader,238,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```. scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================. ERROR: scanpy.api (unittest.loader._FailedTest). ----------------------------------------------------------------------. ImportError: Failed to import test module: scanpy.api. Traceback (most recent call last):. File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path. package = self._get_module_from_name(name). File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name. __import__(name). File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>. from . import pl. File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>. from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------. Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully. ```. --- a/scanpy/api/pl.py. +++ b/scanpy/api/pl.py. @@ -1,4 +1,7 @@. -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot. +from ..plotting._stacked_violin import stacked_violin. +from ..plotting._dotplot import dotplot. +from ..plotting._matrixplot import matrixplot. . from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes. . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:456,energy efficiency,load,loader,456,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```. scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================. ERROR: scanpy.api (unittest.loader._FailedTest). ----------------------------------------------------------------------. ImportError: Failed to import test module: scanpy.api. Traceback (most recent call last):. File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path. package = self._get_module_from_name(name). File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name. __import__(name). File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>. from . import pl. File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>. from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------. Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully. ```. --- a/scanpy/api/pl.py. +++ b/scanpy/api/pl.py. @@ -1,4 +1,7 @@. -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot. +from ..plotting._stacked_violin import stacked_violin. +from ..plotting._dotplot import dotplot. +from ..plotting._matrixplot import matrixplot. . from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes. . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:576,energy efficiency,load,loader,576,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```. scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================. ERROR: scanpy.api (unittest.loader._FailedTest). ----------------------------------------------------------------------. ImportError: Failed to import test module: scanpy.api. Traceback (most recent call last):. File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path. package = self._get_module_from_name(name). File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name. __import__(name). File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>. from . import pl. File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>. from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------. Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully. ```. --- a/scanpy/api/pl.py. +++ b/scanpy/api/pl.py. @@ -1,4 +1,7 @@. -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot. +from ..plotting._stacked_violin import stacked_violin. +from ..plotting._dotplot import dotplot. +from ..plotting._matrixplot import matrixplot. . from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes. . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:949,energy efficiency,heat,heatmap,949,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```. scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================. ERROR: scanpy.api (unittest.loader._FailedTest). ----------------------------------------------------------------------. ImportError: Failed to import test module: scanpy.api. Traceback (most recent call last):. File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path. package = self._get_module_from_name(name). File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name. __import__(name). File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>. from . import pl. File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>. from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------. Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully. ```. --- a/scanpy/api/pl.py. +++ b/scanpy/api/pl.py. @@ -1,4 +1,7 @@. -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot. +from ..plotting._stacked_violin import stacked_violin. +from ..plotting._dotplot import dotplot. +from ..plotting._matrixplot import matrixplot. . from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes. . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:1476,energy efficiency,heat,heatmap,1476,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```. scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================. ERROR: scanpy.api (unittest.loader._FailedTest). ----------------------------------------------------------------------. ImportError: Failed to import test module: scanpy.api. Traceback (most recent call last):. File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path. package = self._get_module_from_name(name). File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name. __import__(name). File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>. from . import pl. File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>. from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------. Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully. ```. --- a/scanpy/api/pl.py. +++ b/scanpy/api/pl.py. @@ -1,4 +1,7 @@. -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot. +from ..plotting._stacked_violin import stacked_violin. +from ..plotting._dotplot import dotplot. +from ..plotting._matrixplot import matrixplot. . from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes. . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:1589,energy efficiency,heat,heatmap,1589,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```. scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================. ERROR: scanpy.api (unittest.loader._FailedTest). ----------------------------------------------------------------------. ImportError: Failed to import test module: scanpy.api. Traceback (most recent call last):. File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path. package = self._get_module_from_name(name). File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name. __import__(name). File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>. from . import pl. File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>. from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------. Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully. ```. --- a/scanpy/api/pl.py. +++ b/scanpy/api/pl.py. @@ -1,4 +1,7 @@. -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot. +from ..plotting._stacked_violin import stacked_violin. +from ..plotting._dotplot import dotplot. +from ..plotting._matrixplot import matrixplot. . from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes. . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:93,integrability,api,api,93,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```. scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================. ERROR: scanpy.api (unittest.loader._FailedTest). ----------------------------------------------------------------------. ImportError: Failed to import test module: scanpy.api. Traceback (most recent call last):. File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path. package = self._get_module_from_name(name). File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name. __import__(name). File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>. from . import pl. File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>. from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------. Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully. ```. --- a/scanpy/api/pl.py. +++ b/scanpy/api/pl.py. @@ -1,4 +1,7 @@. -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot. +from ..plotting._stacked_violin import stacked_violin. +from ..plotting._dotplot import dotplot. +from ..plotting._matrixplot import matrixplot. . from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes. . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:224,integrability,api,api,224,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```. scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================. ERROR: scanpy.api (unittest.loader._FailedTest). ----------------------------------------------------------------------. ImportError: Failed to import test module: scanpy.api. Traceback (most recent call last):. File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path. package = self._get_module_from_name(name). File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name. __import__(name). File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>. from . import pl. File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>. from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------. Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully. ```. --- a/scanpy/api/pl.py. +++ b/scanpy/api/pl.py. @@ -1,4 +1,7 @@. -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot. +from ..plotting._stacked_violin import stacked_violin. +from ..plotting._dotplot import dotplot. +from ..plotting._matrixplot import matrixplot. . from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes. . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:381,integrability,api,api,381,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```. scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================. ERROR: scanpy.api (unittest.loader._FailedTest). ----------------------------------------------------------------------. ImportError: Failed to import test module: scanpy.api. Traceback (most recent call last):. File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path. package = self._get_module_from_name(name). File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name. __import__(name). File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>. from . import pl. File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>. from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------. Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully. ```. --- a/scanpy/api/pl.py. +++ b/scanpy/api/pl.py. @@ -1,4 +1,7 @@. -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot. +from ..plotting._stacked_violin import stacked_violin. +from ..plotting._dotplot import dotplot. +from ..plotting._matrixplot import matrixplot. . from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes. . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:707,integrability,api,api,707,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```. scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================. ERROR: scanpy.api (unittest.loader._FailedTest). ----------------------------------------------------------------------. ImportError: Failed to import test module: scanpy.api. Traceback (most recent call last):. File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path. package = self._get_module_from_name(name). File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name. __import__(name). File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>. from . import pl. File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>. from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------. Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully. ```. --- a/scanpy/api/pl.py. +++ b/scanpy/api/pl.py. @@ -1,4 +1,7 @@. -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot. +from ..plotting._stacked_violin import stacked_violin. +from ..plotting._dotplot import dotplot. +from ..plotting._matrixplot import matrixplot. . from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes. . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:830,integrability,api,api,830,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```. scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================. ERROR: scanpy.api (unittest.loader._FailedTest). ----------------------------------------------------------------------. ImportError: Failed to import test module: scanpy.api. Traceback (most recent call last):. File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path. package = self._get_module_from_name(name). File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name. __import__(name). File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>. from . import pl. File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>. from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------. Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully. ```. --- a/scanpy/api/pl.py. +++ b/scanpy/api/pl.py. @@ -1,4 +1,7 @@. -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot. +from ..plotting._stacked_violin import stacked_violin. +from ..plotting._dotplot import dotplot. +from ..plotting._matrixplot import matrixplot. . from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes. . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:1337,integrability,api,api,1337,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```. scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================. ERROR: scanpy.api (unittest.loader._FailedTest). ----------------------------------------------------------------------. ImportError: Failed to import test module: scanpy.api. Traceback (most recent call last):. File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path. package = self._get_module_from_name(name). File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name. __import__(name). File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>. from . import pl. File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>. from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------. Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully. ```. --- a/scanpy/api/pl.py. +++ b/scanpy/api/pl.py. @@ -1,4 +1,7 @@. -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot. +from ..plotting._stacked_violin import stacked_violin. +from ..plotting._dotplot import dotplot. +from ..plotting._matrixplot import matrixplot. . from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes. . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:1361,integrability,api,api,1361,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```. scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================. ERROR: scanpy.api (unittest.loader._FailedTest). ----------------------------------------------------------------------. ImportError: Failed to import test module: scanpy.api. Traceback (most recent call last):. File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path. package = self._get_module_from_name(name). File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name. __import__(name). File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>. from . import pl. File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>. from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------. Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully. ```. --- a/scanpy/api/pl.py. +++ b/scanpy/api/pl.py. @@ -1,4 +1,7 @@. -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot. +from ..plotting._stacked_violin import stacked_violin. +from ..plotting._dotplot import dotplot. +from ..plotting._matrixplot import matrixplot. . from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes. . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:93,interoperability,api,api,93,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```. scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================. ERROR: scanpy.api (unittest.loader._FailedTest). ----------------------------------------------------------------------. ImportError: Failed to import test module: scanpy.api. Traceback (most recent call last):. File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path. package = self._get_module_from_name(name). File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name. __import__(name). File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>. from . import pl. File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>. from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------. Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully. ```. --- a/scanpy/api/pl.py. +++ b/scanpy/api/pl.py. @@ -1,4 +1,7 @@. -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot. +from ..plotting._stacked_violin import stacked_violin. +from ..plotting._dotplot import dotplot. +from ..plotting._matrixplot import matrixplot. . from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes. . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:224,interoperability,api,api,224,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```. scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================. ERROR: scanpy.api (unittest.loader._FailedTest). ----------------------------------------------------------------------. ImportError: Failed to import test module: scanpy.api. Traceback (most recent call last):. File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path. package = self._get_module_from_name(name). File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name. __import__(name). File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>. from . import pl. File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>. from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------. Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully. ```. --- a/scanpy/api/pl.py. +++ b/scanpy/api/pl.py. @@ -1,4 +1,7 @@. -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot. +from ..plotting._stacked_violin import stacked_violin. +from ..plotting._dotplot import dotplot. +from ..plotting._matrixplot import matrixplot. . from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes. . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:381,interoperability,api,api,381,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```. scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================. ERROR: scanpy.api (unittest.loader._FailedTest). ----------------------------------------------------------------------. ImportError: Failed to import test module: scanpy.api. Traceback (most recent call last):. File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path. package = self._get_module_from_name(name). File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name. __import__(name). File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>. from . import pl. File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>. from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------. Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully. ```. --- a/scanpy/api/pl.py. +++ b/scanpy/api/pl.py. @@ -1,4 +1,7 @@. -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot. +from ..plotting._stacked_violin import stacked_violin. +from ..plotting._dotplot import dotplot. +from ..plotting._matrixplot import matrixplot. . from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes. . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:707,interoperability,api,api,707,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```. scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================. ERROR: scanpy.api (unittest.loader._FailedTest). ----------------------------------------------------------------------. ImportError: Failed to import test module: scanpy.api. Traceback (most recent call last):. File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path. package = self._get_module_from_name(name). File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name. __import__(name). File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>. from . import pl. File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>. from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------. Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully. ```. --- a/scanpy/api/pl.py. +++ b/scanpy/api/pl.py. @@ -1,4 +1,7 @@. -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot. +from ..plotting._stacked_violin import stacked_violin. +from ..plotting._dotplot import dotplot. +from ..plotting._matrixplot import matrixplot. . from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes. . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:830,interoperability,api,api,830,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```. scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================. ERROR: scanpy.api (unittest.loader._FailedTest). ----------------------------------------------------------------------. ImportError: Failed to import test module: scanpy.api. Traceback (most recent call last):. File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path. package = self._get_module_from_name(name). File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name. __import__(name). File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>. from . import pl. File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>. from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------. Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully. ```. --- a/scanpy/api/pl.py. +++ b/scanpy/api/pl.py. @@ -1,4 +1,7 @@. -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot. +from ..plotting._stacked_violin import stacked_violin. +from ..plotting._dotplot import dotplot. +from ..plotting._matrixplot import matrixplot. . from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes. . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:1337,interoperability,api,api,1337,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```. scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================. ERROR: scanpy.api (unittest.loader._FailedTest). ----------------------------------------------------------------------. ImportError: Failed to import test module: scanpy.api. Traceback (most recent call last):. File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path. package = self._get_module_from_name(name). File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name. __import__(name). File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>. from . import pl. File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>. from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------. Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully. ```. --- a/scanpy/api/pl.py. +++ b/scanpy/api/pl.py. @@ -1,4 +1,7 @@. -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot. +from ..plotting._stacked_violin import stacked_violin. +from ..plotting._dotplot import dotplot. +from ..plotting._matrixplot import matrixplot. . from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes. . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:1361,interoperability,api,api,1361,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```. scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================. ERROR: scanpy.api (unittest.loader._FailedTest). ----------------------------------------------------------------------. ImportError: Failed to import test module: scanpy.api. Traceback (most recent call last):. File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path. package = self._get_module_from_name(name). File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name. __import__(name). File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>. from . import pl. File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>. from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------. Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully. ```. --- a/scanpy/api/pl.py. +++ b/scanpy/api/pl.py. @@ -1,4 +1,7 @@. -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot. +from ..plotting._stacked_violin import stacked_violin. +from ..plotting._dotplot import dotplot. +from ..plotting._matrixplot import matrixplot. . from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes. . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:366,modifiability,modul,module,366,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```. scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================. ERROR: scanpy.api (unittest.loader._FailedTest). ----------------------------------------------------------------------. ImportError: Failed to import test module: scanpy.api. Traceback (most recent call last):. File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path. package = self._get_module_from_name(name). File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name. __import__(name). File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>. from . import pl. File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>. from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------. Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully. ```. --- a/scanpy/api/pl.py. +++ b/scanpy/api/pl.py. @@ -1,4 +1,7 @@. -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot. +from ..plotting._stacked_violin import stacked_violin. +from ..plotting._dotplot import dotplot. +from ..plotting._matrixplot import matrixplot. . from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes. . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:498,modifiability,pac,package,498,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```. scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================. ERROR: scanpy.api (unittest.loader._FailedTest). ----------------------------------------------------------------------. ImportError: Failed to import test module: scanpy.api. Traceback (most recent call last):. File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path. package = self._get_module_from_name(name). File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name. __import__(name). File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>. from . import pl. File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>. from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------. Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully. ```. --- a/scanpy/api/pl.py. +++ b/scanpy/api/pl.py. @@ -1,4 +1,7 @@. -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot. +from ..plotting._stacked_violin import stacked_violin. +from ..plotting._dotplot import dotplot. +from ..plotting._matrixplot import matrixplot. . from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes. . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:738,modifiability,modul,module,738,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```. scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================. ERROR: scanpy.api (unittest.loader._FailedTest). ----------------------------------------------------------------------. ImportError: Failed to import test module: scanpy.api. Traceback (most recent call last):. File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path. package = self._get_module_from_name(name). File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name. __import__(name). File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>. from . import pl. File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>. from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------. Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully. ```. --- a/scanpy/api/pl.py. +++ b/scanpy/api/pl.py. @@ -1,4 +1,7 @@. -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot. +from ..plotting._stacked_violin import stacked_violin. +from ..plotting._dotplot import dotplot. +from ..plotting._matrixplot import matrixplot. . from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes. . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:854,modifiability,modul,module,854,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```. scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================. ERROR: scanpy.api (unittest.loader._FailedTest). ----------------------------------------------------------------------. ImportError: Failed to import test module: scanpy.api. Traceback (most recent call last):. File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path. package = self._get_module_from_name(name). File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name. __import__(name). File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>. from . import pl. File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>. from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------. Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully. ```. --- a/scanpy/api/pl.py. +++ b/scanpy/api/pl.py. @@ -1,4 +1,7 @@. -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot. +from ..plotting._stacked_violin import stacked_violin. +from ..plotting._dotplot import dotplot. +from ..plotting._matrixplot import matrixplot. . from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes. . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:107,performance,load,loader,107,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```. scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================. ERROR: scanpy.api (unittest.loader._FailedTest). ----------------------------------------------------------------------. ImportError: Failed to import test module: scanpy.api. Traceback (most recent call last):. File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path. package = self._get_module_from_name(name). File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name. __import__(name). File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>. from . import pl. File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>. from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------. Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully. ```. --- a/scanpy/api/pl.py. +++ b/scanpy/api/pl.py. @@ -1,4 +1,7 @@. -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot. +from ..plotting._stacked_violin import stacked_violin. +from ..plotting._dotplot import dotplot. +from ..plotting._matrixplot import matrixplot. . from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes. . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:131,performance,ERROR,ERROR,131,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```. scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================. ERROR: scanpy.api (unittest.loader._FailedTest). ----------------------------------------------------------------------. ImportError: Failed to import test module: scanpy.api. Traceback (most recent call last):. File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path. package = self._get_module_from_name(name). File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name. __import__(name). File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>. from . import pl. File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>. from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------. Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully. ```. --- a/scanpy/api/pl.py. +++ b/scanpy/api/pl.py. @@ -1,4 +1,7 @@. -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot. +from ..plotting._stacked_violin import stacked_violin. +from ..plotting._dotplot import dotplot. +from ..plotting._matrixplot import matrixplot. . from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes. . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:210,performance,ERROR,ERROR,210,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```. scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================. ERROR: scanpy.api (unittest.loader._FailedTest). ----------------------------------------------------------------------. ImportError: Failed to import test module: scanpy.api. Traceback (most recent call last):. File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path. package = self._get_module_from_name(name). File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name. __import__(name). File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>. from . import pl. File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>. from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------. Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully. ```. --- a/scanpy/api/pl.py. +++ b/scanpy/api/pl.py. @@ -1,4 +1,7 @@. -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot. +from ..plotting._stacked_violin import stacked_violin. +from ..plotting._dotplot import dotplot. +from ..plotting._matrixplot import matrixplot. . from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes. . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:238,performance,load,loader,238,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```. scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================. ERROR: scanpy.api (unittest.loader._FailedTest). ----------------------------------------------------------------------. ImportError: Failed to import test module: scanpy.api. Traceback (most recent call last):. File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path. package = self._get_module_from_name(name). File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name. __import__(name). File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>. from . import pl. File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>. from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------. Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully. ```. --- a/scanpy/api/pl.py. +++ b/scanpy/api/pl.py. @@ -1,4 +1,7 @@. -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot. +from ..plotting._stacked_violin import stacked_violin. +from ..plotting._dotplot import dotplot. +from ..plotting._matrixplot import matrixplot. . from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes. . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:456,performance,load,loader,456,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```. scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================. ERROR: scanpy.api (unittest.loader._FailedTest). ----------------------------------------------------------------------. ImportError: Failed to import test module: scanpy.api. Traceback (most recent call last):. File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path. package = self._get_module_from_name(name). File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name. __import__(name). File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>. from . import pl. File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>. from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------. Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully. ```. --- a/scanpy/api/pl.py. +++ b/scanpy/api/pl.py. @@ -1,4 +1,7 @@. -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot. +from ..plotting._stacked_violin import stacked_violin. +from ..plotting._dotplot import dotplot. +from ..plotting._matrixplot import matrixplot. . from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes. . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:576,performance,load,loader,576,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```. scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================. ERROR: scanpy.api (unittest.loader._FailedTest). ----------------------------------------------------------------------. ImportError: Failed to import test module: scanpy.api. Traceback (most recent call last):. File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path. package = self._get_module_from_name(name). File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name. __import__(name). File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>. from . import pl. File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>. from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------. Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully. ```. --- a/scanpy/api/pl.py. +++ b/scanpy/api/pl.py. @@ -1,4 +1,7 @@. -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot. +from ..plotting._stacked_violin import stacked_violin. +from ..plotting._dotplot import dotplot. +from ..plotting._matrixplot import matrixplot. . from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes. . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:344,reliability,Fail,Failed,344,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```. scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================. ERROR: scanpy.api (unittest.loader._FailedTest). ----------------------------------------------------------------------. ImportError: Failed to import test module: scanpy.api. Traceback (most recent call last):. File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path. package = self._get_module_from_name(name). File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name. __import__(name). File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>. from . import pl. File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>. from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------. Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully. ```. --- a/scanpy/api/pl.py. +++ b/scanpy/api/pl.py. @@ -1,4 +1,7 @@. -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot. +from ..plotting._stacked_violin import stacked_violin. +from ..plotting._dotplot import dotplot. +from ..plotting._matrixplot import matrixplot. . from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes. . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:70,safety,test,test,70,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```. scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================. ERROR: scanpy.api (unittest.loader._FailedTest). ----------------------------------------------------------------------. ImportError: Failed to import test module: scanpy.api. Traceback (most recent call last):. File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path. package = self._get_module_from_name(name). File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name. __import__(name). File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>. from . import pl. File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>. from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------. Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully. ```. --- a/scanpy/api/pl.py. +++ b/scanpy/api/pl.py. @@ -1,4 +1,7 @@. -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot. +from ..plotting._stacked_violin import stacked_violin. +from ..plotting._dotplot import dotplot. +from ..plotting._matrixplot import matrixplot. . from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes. . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:131,safety,ERROR,ERROR,131,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```. scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================. ERROR: scanpy.api (unittest.loader._FailedTest). ----------------------------------------------------------------------. ImportError: Failed to import test module: scanpy.api. Traceback (most recent call last):. File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path. package = self._get_module_from_name(name). File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name. __import__(name). File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>. from . import pl. File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>. from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------. Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully. ```. --- a/scanpy/api/pl.py. +++ b/scanpy/api/pl.py. @@ -1,4 +1,7 @@. -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot. +from ..plotting._stacked_violin import stacked_violin. +from ..plotting._dotplot import dotplot. +from ..plotting._matrixplot import matrixplot. . from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes. . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:210,safety,ERROR,ERROR,210,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```. scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================. ERROR: scanpy.api (unittest.loader._FailedTest). ----------------------------------------------------------------------. ImportError: Failed to import test module: scanpy.api. Traceback (most recent call last):. File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path. package = self._get_module_from_name(name). File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name. __import__(name). File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>. from . import pl. File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>. from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------. Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully. ```. --- a/scanpy/api/pl.py. +++ b/scanpy/api/pl.py. @@ -1,4 +1,7 @@. -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot. +from ..plotting._stacked_violin import stacked_violin. +from ..plotting._dotplot import dotplot. +from ..plotting._matrixplot import matrixplot. . from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes. . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:361,safety,test,test,361,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```. scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================. ERROR: scanpy.api (unittest.loader._FailedTest). ----------------------------------------------------------------------. ImportError: Failed to import test module: scanpy.api. Traceback (most recent call last):. File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path. package = self._get_module_from_name(name). File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name. __import__(name). File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>. from . import pl. File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>. from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------. Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully. ```. --- a/scanpy/api/pl.py. +++ b/scanpy/api/pl.py. @@ -1,4 +1,7 @@. -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot. +from ..plotting._stacked_violin import stacked_violin. +from ..plotting._dotplot import dotplot. +from ..plotting._matrixplot import matrixplot. . from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes. . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:366,safety,modul,module,366,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```. scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================. ERROR: scanpy.api (unittest.loader._FailedTest). ----------------------------------------------------------------------. ImportError: Failed to import test module: scanpy.api. Traceback (most recent call last):. File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path. package = self._get_module_from_name(name). File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name. __import__(name). File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>. from . import pl. File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>. from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------. Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully. ```. --- a/scanpy/api/pl.py. +++ b/scanpy/api/pl.py. @@ -1,4 +1,7 @@. -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot. +from ..plotting._stacked_violin import stacked_violin. +from ..plotting._dotplot import dotplot. +from ..plotting._matrixplot import matrixplot. . from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes. . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:738,safety,modul,module,738,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```. scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================. ERROR: scanpy.api (unittest.loader._FailedTest). ----------------------------------------------------------------------. ImportError: Failed to import test module: scanpy.api. Traceback (most recent call last):. File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path. package = self._get_module_from_name(name). File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name. __import__(name). File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>. from . import pl. File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>. from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------. Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully. ```. --- a/scanpy/api/pl.py. +++ b/scanpy/api/pl.py. @@ -1,4 +1,7 @@. -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot. +from ..plotting._stacked_violin import stacked_violin. +from ..plotting._dotplot import dotplot. +from ..plotting._matrixplot import matrixplot. . from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes. . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:854,safety,modul,module,854,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```. scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================. ERROR: scanpy.api (unittest.loader._FailedTest). ----------------------------------------------------------------------. ImportError: Failed to import test module: scanpy.api. Traceback (most recent call last):. File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path. package = self._get_module_from_name(name). File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name. __import__(name). File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>. from . import pl. File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>. from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------. Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully. ```. --- a/scanpy/api/pl.py. +++ b/scanpy/api/pl.py. @@ -1,4 +1,7 @@. -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot. +from ..plotting._stacked_violin import stacked_violin. +from ..plotting._dotplot import dotplot. +from ..plotting._matrixplot import matrixplot. . from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes. . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:1233,safety,test,test,1233,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```. scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================. ERROR: scanpy.api (unittest.loader._FailedTest). ----------------------------------------------------------------------. ImportError: Failed to import test module: scanpy.api. Traceback (most recent call last):. File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path. package = self._get_module_from_name(name). File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name. __import__(name). File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>. from . import pl. File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>. from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------. Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully. ```. --- a/scanpy/api/pl.py. +++ b/scanpy/api/pl.py. @@ -1,4 +1,7 @@. -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot. +from ..plotting._stacked_violin import stacked_violin. +from ..plotting._dotplot import dotplot. +from ..plotting._matrixplot import matrixplot. . from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes. . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:1275,safety,patch,patch,1275,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```. scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================. ERROR: scanpy.api (unittest.loader._FailedTest). ----------------------------------------------------------------------. ImportError: Failed to import test module: scanpy.api. Traceback (most recent call last):. File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path. package = self._get_module_from_name(name). File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name. __import__(name). File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>. from . import pl. File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>. from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------. Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully. ```. --- a/scanpy/api/pl.py. +++ b/scanpy/api/pl.py. @@ -1,4 +1,7 @@. -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot. +from ..plotting._stacked_violin import stacked_violin. +from ..plotting._dotplot import dotplot. +from ..plotting._matrixplot import matrixplot. . from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes. . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:1292,safety,test,tests,1292,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```. scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================. ERROR: scanpy.api (unittest.loader._FailedTest). ----------------------------------------------------------------------. ImportError: Failed to import test module: scanpy.api. Traceback (most recent call last):. File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path. package = self._get_module_from_name(name). File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name. __import__(name). File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>. from . import pl. File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>. from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------. Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully. ```. --- a/scanpy/api/pl.py. +++ b/scanpy/api/pl.py. @@ -1,4 +1,7 @@. -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot. +from ..plotting._stacked_violin import stacked_violin. +from ..plotting._dotplot import dotplot. +from ..plotting._matrixplot import matrixplot. . from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes. . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:1275,security,patch,patch,1275,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```. scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================. ERROR: scanpy.api (unittest.loader._FailedTest). ----------------------------------------------------------------------. ImportError: Failed to import test module: scanpy.api. Traceback (most recent call last):. File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path. package = self._get_module_from_name(name). File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name. __import__(name). File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>. from . import pl. File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>. from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------. Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully. ```. --- a/scanpy/api/pl.py. +++ b/scanpy/api/pl.py. @@ -1,4 +1,7 @@. -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot. +from ..plotting._stacked_violin import stacked_violin. +from ..plotting._dotplot import dotplot. +from ..plotting._matrixplot import matrixplot. . from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes. . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:70,testability,test,test,70,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```. scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================. ERROR: scanpy.api (unittest.loader._FailedTest). ----------------------------------------------------------------------. ImportError: Failed to import test module: scanpy.api. Traceback (most recent call last):. File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path. package = self._get_module_from_name(name). File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name. __import__(name). File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>. from . import pl. File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>. from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------. Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully. ```. --- a/scanpy/api/pl.py. +++ b/scanpy/api/pl.py. @@ -1,4 +1,7 @@. -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot. +from ..plotting._stacked_violin import stacked_violin. +from ..plotting._dotplot import dotplot. +from ..plotting._matrixplot import matrixplot. . from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes. . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:98,testability,unit,unittest,98,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```. scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================. ERROR: scanpy.api (unittest.loader._FailedTest). ----------------------------------------------------------------------. ImportError: Failed to import test module: scanpy.api. Traceback (most recent call last):. File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path. package = self._get_module_from_name(name). File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name. __import__(name). File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>. from . import pl. File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>. from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------. Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully. ```. --- a/scanpy/api/pl.py. +++ b/scanpy/api/pl.py. @@ -1,4 +1,7 @@. -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot. +from ..plotting._stacked_violin import stacked_violin. +from ..plotting._dotplot import dotplot. +from ..plotting._matrixplot import matrixplot. . from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes. . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:229,testability,unit,unittest,229,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```. scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================. ERROR: scanpy.api (unittest.loader._FailedTest). ----------------------------------------------------------------------. ImportError: Failed to import test module: scanpy.api. Traceback (most recent call last):. File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path. package = self._get_module_from_name(name). File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name. __import__(name). File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>. from . import pl. File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>. from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------. Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully. ```. --- a/scanpy/api/pl.py. +++ b/scanpy/api/pl.py. @@ -1,4 +1,7 @@. -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot. +from ..plotting._stacked_violin import stacked_violin. +from ..plotting._dotplot import dotplot. +from ..plotting._matrixplot import matrixplot. . from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes. . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:361,testability,test,test,361,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```. scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================. ERROR: scanpy.api (unittest.loader._FailedTest). ----------------------------------------------------------------------. ImportError: Failed to import test module: scanpy.api. Traceback (most recent call last):. File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path. package = self._get_module_from_name(name). File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name. __import__(name). File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>. from . import pl. File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>. from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------. Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully. ```. --- a/scanpy/api/pl.py. +++ b/scanpy/api/pl.py. @@ -1,4 +1,7 @@. -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot. +from ..plotting._stacked_violin import stacked_violin. +from ..plotting._dotplot import dotplot. +from ..plotting._matrixplot import matrixplot. . from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes. . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:386,testability,Trace,Traceback,386,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```. scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================. ERROR: scanpy.api (unittest.loader._FailedTest). ----------------------------------------------------------------------. ImportError: Failed to import test module: scanpy.api. Traceback (most recent call last):. File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path. package = self._get_module_from_name(name). File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name. __import__(name). File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>. from . import pl. File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>. from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------. Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully. ```. --- a/scanpy/api/pl.py. +++ b/scanpy/api/pl.py. @@ -1,4 +1,7 @@. -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot. +from ..plotting._stacked_violin import stacked_violin. +from ..plotting._dotplot import dotplot. +from ..plotting._matrixplot import matrixplot. . from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes. . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:447,testability,unit,unittest,447,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```. scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================. ERROR: scanpy.api (unittest.loader._FailedTest). ----------------------------------------------------------------------. ImportError: Failed to import test module: scanpy.api. Traceback (most recent call last):. File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path. package = self._get_module_from_name(name). File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name. __import__(name). File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>. from . import pl. File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>. from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------. Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully. ```. --- a/scanpy/api/pl.py. +++ b/scanpy/api/pl.py. @@ -1,4 +1,7 @@. -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot. +from ..plotting._stacked_violin import stacked_violin. +from ..plotting._dotplot import dotplot. +from ..plotting._matrixplot import matrixplot. . from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes. . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:567,testability,unit,unittest,567,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```. scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================. ERROR: scanpy.api (unittest.loader._FailedTest). ----------------------------------------------------------------------. ImportError: Failed to import test module: scanpy.api. Traceback (most recent call last):. File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path. package = self._get_module_from_name(name). File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name. __import__(name). File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>. from . import pl. File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>. from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------. Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully. ```. --- a/scanpy/api/pl.py. +++ b/scanpy/api/pl.py. @@ -1,4 +1,7 @@. -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot. +from ..plotting._stacked_violin import stacked_violin. +from ..plotting._dotplot import dotplot. +from ..plotting._matrixplot import matrixplot. . from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes. . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:1233,testability,test,test,1233,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```. scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================. ERROR: scanpy.api (unittest.loader._FailedTest). ----------------------------------------------------------------------. ImportError: Failed to import test module: scanpy.api. Traceback (most recent call last):. File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path. package = self._get_module_from_name(name). File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name. __import__(name). File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>. from . import pl. File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>. from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------. Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully. ```. --- a/scanpy/api/pl.py. +++ b/scanpy/api/pl.py. @@ -1,4 +1,7 @@. -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot. +from ..plotting._stacked_violin import stacked_violin. +from ..plotting._dotplot import dotplot. +from ..plotting._matrixplot import matrixplot. . from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes. . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:1292,testability,test,tests,1292,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```. scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================. ERROR: scanpy.api (unittest.loader._FailedTest). ----------------------------------------------------------------------. ImportError: Failed to import test module: scanpy.api. Traceback (most recent call last):. File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path. package = self._get_module_from_name(name). File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name. __import__(name). File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>. from . import pl. File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>. from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------. Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully. ```. --- a/scanpy/api/pl.py. +++ b/scanpy/api/pl.py. @@ -1,4 +1,7 @@. -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot. +from ..plotting._stacked_violin import stacked_violin. +from ..plotting._dotplot import dotplot. +from ..plotting._matrixplot import matrixplot. . from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes. . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:131,usability,ERROR,ERROR,131,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```. scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================. ERROR: scanpy.api (unittest.loader._FailedTest). ----------------------------------------------------------------------. ImportError: Failed to import test module: scanpy.api. Traceback (most recent call last):. File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path. package = self._get_module_from_name(name). File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name. __import__(name). File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>. from . import pl. File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>. from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------. Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully. ```. --- a/scanpy/api/pl.py. +++ b/scanpy/api/pl.py. @@ -1,4 +1,7 @@. -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot. +from ..plotting._stacked_violin import stacked_violin. +from ..plotting._dotplot import dotplot. +from ..plotting._matrixplot import matrixplot. . from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes. . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/issues/1397:210,usability,ERROR,ERROR,210,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```. scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================. ERROR: scanpy.api (unittest.loader._FailedTest). ----------------------------------------------------------------------. ImportError: Failed to import test module: scanpy.api. Traceback (most recent call last):. File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path. package = self._get_module_from_name(name). File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name. __import__(name). File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>. from . import pl. File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>. from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------. Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully. ```. --- a/scanpy/api/pl.py. +++ b/scanpy/api/pl.py. @@ -1,4 +1,7 @@. -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot. +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot. +from ..plotting._stacked_violin import stacked_violin. +from ..plotting._dotplot import dotplot. +from ..plotting._matrixplot import matrixplot. . from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes. . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397
https://github.com/scverse/scanpy/pull/1398:16,usability,help,help,16,/AzurePipelines help,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1398
https://github.com/scverse/scanpy/issues/1399:49,deployability,version,version,49,"How pandas does this: there's branch for a minor version, e.g. `1.1.x`. Bugfixes get back ported to this branch, and releases are tagged here. There is automation of back porting through [meeseeksbox](https://meeseeksbox.github.io). We might have to request access for this? Julia does something pretty similar, except it looks like all back ports are done at once in a PR, instead of continuously. This is a bit more manual, but requires less setup. Both of these systems use tags to mark which PRs need back porting.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1399
https://github.com/scverse/scanpy/issues/1399:117,deployability,releas,releases,117,"How pandas does this: there's branch for a minor version, e.g. `1.1.x`. Bugfixes get back ported to this branch, and releases are tagged here. There is automation of back porting through [meeseeksbox](https://meeseeksbox.github.io). We might have to request access for this? Julia does something pretty similar, except it looks like all back ports are done at once in a PR, instead of continuously. This is a bit more manual, but requires less setup. Both of these systems use tags to mark which PRs need back porting.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1399
https://github.com/scverse/scanpy/issues/1399:152,deployability,automat,automation,152,"How pandas does this: there's branch for a minor version, e.g. `1.1.x`. Bugfixes get back ported to this branch, and releases are tagged here. There is automation of back porting through [meeseeksbox](https://meeseeksbox.github.io). We might have to request access for this? Julia does something pretty similar, except it looks like all back ports are done at once in a PR, instead of continuously. This is a bit more manual, but requires less setup. Both of these systems use tags to mark which PRs need back porting.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1399
https://github.com/scverse/scanpy/issues/1399:385,deployability,continu,continuously,385,"How pandas does this: there's branch for a minor version, e.g. `1.1.x`. Bugfixes get back ported to this branch, and releases are tagged here. There is automation of back porting through [meeseeksbox](https://meeseeksbox.github.io). We might have to request access for this? Julia does something pretty similar, except it looks like all back ports are done at once in a PR, instead of continuously. This is a bit more manual, but requires less setup. Both of these systems use tags to mark which PRs need back porting.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1399
https://github.com/scverse/scanpy/issues/1399:49,integrability,version,version,49,"How pandas does this: there's branch for a minor version, e.g. `1.1.x`. Bugfixes get back ported to this branch, and releases are tagged here. There is automation of back porting through [meeseeksbox](https://meeseeksbox.github.io). We might have to request access for this? Julia does something pretty similar, except it looks like all back ports are done at once in a PR, instead of continuously. This is a bit more manual, but requires less setup. Both of these systems use tags to mark which PRs need back porting.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1399
https://github.com/scverse/scanpy/issues/1399:49,modifiability,version,version,49,"How pandas does this: there's branch for a minor version, e.g. `1.1.x`. Bugfixes get back ported to this branch, and releases are tagged here. There is automation of back porting through [meeseeksbox](https://meeseeksbox.github.io). We might have to request access for this? Julia does something pretty similar, except it looks like all back ports are done at once in a PR, instead of continuously. This is a bit more manual, but requires less setup. Both of these systems use tags to mark which PRs need back porting.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1399
https://github.com/scverse/scanpy/issues/1399:11,reliability,doe,does,11,"How pandas does this: there's branch for a minor version, e.g. `1.1.x`. Bugfixes get back ported to this branch, and releases are tagged here. There is automation of back porting through [meeseeksbox](https://meeseeksbox.github.io). We might have to request access for this? Julia does something pretty similar, except it looks like all back ports are done at once in a PR, instead of continuously. This is a bit more manual, but requires less setup. Both of these systems use tags to mark which PRs need back porting.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1399
https://github.com/scverse/scanpy/issues/1399:281,reliability,doe,does,281,"How pandas does this: there's branch for a minor version, e.g. `1.1.x`. Bugfixes get back ported to this branch, and releases are tagged here. There is automation of back porting through [meeseeksbox](https://meeseeksbox.github.io). We might have to request access for this? Julia does something pretty similar, except it looks like all back ports are done at once in a PR, instead of continuously. This is a bit more manual, but requires less setup. Both of these systems use tags to mark which PRs need back porting.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1399
https://github.com/scverse/scanpy/issues/1399:312,safety,except,except,312,"How pandas does this: there's branch for a minor version, e.g. `1.1.x`. Bugfixes get back ported to this branch, and releases are tagged here. There is automation of back porting through [meeseeksbox](https://meeseeksbox.github.io). We might have to request access for this? Julia does something pretty similar, except it looks like all back ports are done at once in a PR, instead of continuously. This is a bit more manual, but requires less setup. Both of these systems use tags to mark which PRs need back porting.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1399
https://github.com/scverse/scanpy/issues/1399:258,security,access,access,258,"How pandas does this: there's branch for a minor version, e.g. `1.1.x`. Bugfixes get back ported to this branch, and releases are tagged here. There is automation of back porting through [meeseeksbox](https://meeseeksbox.github.io). We might have to request access for this? Julia does something pretty similar, except it looks like all back ports are done at once in a PR, instead of continuously. This is a bit more manual, but requires less setup. Both of these systems use tags to mark which PRs need back porting.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1399
https://github.com/scverse/scanpy/issues/1399:152,testability,automat,automation,152,"How pandas does this: there's branch for a minor version, e.g. `1.1.x`. Bugfixes get back ported to this branch, and releases are tagged here. There is automation of back porting through [meeseeksbox](https://meeseeksbox.github.io). We might have to request access for this? Julia does something pretty similar, except it looks like all back ports are done at once in a PR, instead of continuously. This is a bit more manual, but requires less setup. Both of these systems use tags to mark which PRs need back porting.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1399
https://github.com/scverse/scanpy/pull/1402:11,deployability,build,building,11,"@ivirshup, building the docs failed even though I did not change any documentation. Do you have an idea what the problem may be?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1402
https://github.com/scverse/scanpy/pull/1402:29,deployability,fail,failed,29,"@ivirshup, building the docs failed even though I did not change any documentation. Do you have an idea what the problem may be?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1402
https://github.com/scverse/scanpy/pull/1402:29,reliability,fail,failed,29,"@ivirshup, building the docs failed even though I did not change any documentation. Do you have an idea what the problem may be?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1402
https://github.com/scverse/scanpy/pull/1402:69,usability,document,documentation,69,"@ivirshup, building the docs failed even though I did not change any documentation. Do you have an idea what the problem may be?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1402
https://github.com/scverse/scanpy/issues/1403:61,usability,close,close,61,"This is related to #1301 , let's keep discussion there, I'll close this",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1403
https://github.com/scverse/scanpy/issues/1405:49,availability,error,error,49,Thanks. Can you share the code that produces the error? Maybe also the image as well?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1405
https://github.com/scverse/scanpy/issues/1405:16,interoperability,share,share,16,Thanks. Can you share the code that produces the error? Maybe also the image as well?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1405
https://github.com/scverse/scanpy/issues/1405:49,performance,error,error,49,Thanks. Can you share the code that produces the error? Maybe also the image as well?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1405
https://github.com/scverse/scanpy/issues/1405:49,safety,error,error,49,Thanks. Can you share the code that produces the error? Maybe also the image as well?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1405
https://github.com/scverse/scanpy/issues/1405:49,usability,error,error,49,Thanks. Can you share the code that produces the error? Maybe also the image as well?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1405
https://github.com/scverse/scanpy/issues/1405:41,safety,test,test,41,"`sc.pl.umap(ad, color=['mt.count'],save='test.pdf') `. [umaptest.pdf](https://github.com/theislab/scanpy/files/5171583/umaptest.pdf).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1405
https://github.com/scverse/scanpy/issues/1405:41,testability,test,test,41,"`sc.pl.umap(ad, color=['mt.count'],save='test.pdf') `. [umaptest.pdf](https://github.com/theislab/scanpy/files/5171583/umaptest.pdf).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1405
https://github.com/scverse/scanpy/issues/1405:228,deployability,api,api,228,I think that we rasterize the scatter part because is usually impractical to have a pdf with thousands of dots. I sugggest that you increase the DPI to get a better resolution image:. See https://scanpy.readthedocs.io/en/stable/api/scanpy.set_figure_params.html.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1405
https://github.com/scverse/scanpy/issues/1405:228,integrability,api,api,228,I think that we rasterize the scatter part because is usually impractical to have a pdf with thousands of dots. I sugggest that you increase the DPI to get a better resolution image:. See https://scanpy.readthedocs.io/en/stable/api/scanpy.set_figure_params.html.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1405
https://github.com/scverse/scanpy/issues/1405:228,interoperability,api,api,228,I think that we rasterize the scatter part because is usually impractical to have a pdf with thousands of dots. I sugggest that you increase the DPI to get a better resolution image:. See https://scanpy.readthedocs.io/en/stable/api/scanpy.set_figure_params.html.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1405
https://github.com/scverse/scanpy/issues/1405:65,usability,user,user,65,Seems like there are some approaches on how to solve this on the user side. Feel free to suggest improvements to the docs!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1405
https://github.com/scverse/scanpy/issues/1406:66,usability,command,command,66,"you can check if `adata.var.index`has that gene. Also, what's the command you are trying to run?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:577,availability,cluster,clusters,577,"Hi! Same issue here, but for me no plotting function works for any gene. I have two Seurat object which I converted to .h5ad files using Seurat-disk, I have read them into scanpy and want to plot a few markers using a dotplot. For one dataset, I can plot any gene I want and it succeeds. For the other dataset, I can't plot **any** gene, in any type of plot (tsne, violin, dotplot, matrixplot, etc.). ```. adata_1 = sc.read_h5ad(""path/to/file1.h5ad""). adata_2 = sc.read_h5ad(""path/to/file2.h5ad""). ```. ```. #this works. sc.pl.dotplot(adata_1, adata_1.var_names[0:4], groupby='clusters', color_map = 'Reds'). #these do not. sc.pl.dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds'). ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-72-4fc81df5ca3f> in <module>. ----> 1 sc.pl.dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds'). ~\Anaconda3\envs\UMCU\lib\site-packages\scanpy\plotting\_anndata.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, expression_cutoff, mean_only_expressed, color_map, dot_max, dot_min, standard_scale, smallest_dot, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1809 num_categories,. 1810 layer=layer,. -> 1811 gene_symbols=gene_symbols,. 1812 ). 1813 . ~\Anaconda3\envs\UMCU\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols). 2911 matrix = adata[:, var_names].layers[layer]. 2912 elif use_raw:. -> 2913 matrix = adata.raw[:, var_names].X. 2914 else:. 2915 matrix = adata[:, var_names].X. ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\raw.py in __getitem__(self, index). 94 . 95 def __getitem__(self, index):. ---> 96 oidx, vidx = self._normalize_indices(index). 97 . 98 # To preserve two dimensional shape. ~\Anaconda3\envs\UM",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:2757,availability,error,error,2757,">. ----> 1 sc.pl.dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds'). ~\Anaconda3\envs\UMCU\lib\site-packages\scanpy\plotting\_anndata.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, expression_cutoff, mean_only_expressed, color_map, dot_max, dot_min, standard_scale, smallest_dot, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1809 num_categories,. 1810 layer=layer,. -> 1811 gene_symbols=gene_symbols,. 1812 ). 1813 . ~\Anaconda3\envs\UMCU\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols). 2911 matrix = adata[:, var_names].layers[layer]. 2912 elif use_raw:. -> 2913 matrix = adata.raw[:, var_names].X. 2914 else:. 2915 matrix = adata[:, var_names].X. ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\raw.py in __getitem__(self, index). 94 . 95 def __getitem__(self, index):. ---> 96 oidx, vidx = self._normalize_indices(index). 97 . 98 # To preserve two dimensional shape. ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\raw.py in _normalize_indices(self, packed_index). 154 obs, var = unpack_index(packed_index). 155 obs = _normalize_index(obs, self._adata.obs_names). --> 156 var = _normalize_index(var, self.var_names). 157 return obs, var. 158 . ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\index.py in _normalize_index(indexer, index). 91 not_found = indexer[positions < 0]. 92 raise KeyError(. ---> 93 f""Values {list(not_found)}, from {list(indexer)}, "". 94 ""are not valid obs/ var names or indices."". 95 ). KeyError: ""Values ['Rgs20', 'Oprk1', 'St18', 'Gm26901'], from ['Rgs20', 'Oprk1', 'St18', 'Gm26901'], are not valid obs/ var names or indices."". sc.pl.matrixplot(adata_2, adata_2.var_names[0:4], groupby='celltype') #same error. sc.pl.tsne(adata_2, color=adata_2.var_names[0:4]) #KeyError: 'Rgs20' . ```. Any help would be much appreciated!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:577,deployability,cluster,clusters,577,"Hi! Same issue here, but for me no plotting function works for any gene. I have two Seurat object which I converted to .h5ad files using Seurat-disk, I have read them into scanpy and want to plot a few markers using a dotplot. For one dataset, I can plot any gene I want and it succeeds. For the other dataset, I can't plot **any** gene, in any type of plot (tsne, violin, dotplot, matrixplot, etc.). ```. adata_1 = sc.read_h5ad(""path/to/file1.h5ad""). adata_2 = sc.read_h5ad(""path/to/file2.h5ad""). ```. ```. #this works. sc.pl.dotplot(adata_1, adata_1.var_names[0:4], groupby='clusters', color_map = 'Reds'). #these do not. sc.pl.dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds'). ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-72-4fc81df5ca3f> in <module>. ----> 1 sc.pl.dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds'). ~\Anaconda3\envs\UMCU\lib\site-packages\scanpy\plotting\_anndata.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, expression_cutoff, mean_only_expressed, color_map, dot_max, dot_min, standard_scale, smallest_dot, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1809 num_categories,. 1810 layer=layer,. -> 1811 gene_symbols=gene_symbols,. 1812 ). 1813 . ~\Anaconda3\envs\UMCU\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols). 2911 matrix = adata[:, var_names].layers[layer]. 2912 elif use_raw:. -> 2913 matrix = adata.raw[:, var_names].X. 2914 else:. 2915 matrix = adata[:, var_names].X. ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\raw.py in __getitem__(self, index). 94 . 95 def __getitem__(self, index):. ---> 96 oidx, vidx = self._normalize_indices(index). 97 . 98 # To preserve two dimensional shape. ~\Anaconda3\envs\UM",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:869,deployability,modul,module,869,"Hi! Same issue here, but for me no plotting function works for any gene. I have two Seurat object which I converted to .h5ad files using Seurat-disk, I have read them into scanpy and want to plot a few markers using a dotplot. For one dataset, I can plot any gene I want and it succeeds. For the other dataset, I can't plot **any** gene, in any type of plot (tsne, violin, dotplot, matrixplot, etc.). ```. adata_1 = sc.read_h5ad(""path/to/file1.h5ad""). adata_2 = sc.read_h5ad(""path/to/file2.h5ad""). ```. ```. #this works. sc.pl.dotplot(adata_1, adata_1.var_names[0:4], groupby='clusters', color_map = 'Reds'). #these do not. sc.pl.dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds'). ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-72-4fc81df5ca3f> in <module>. ----> 1 sc.pl.dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds'). ~\Anaconda3\envs\UMCU\lib\site-packages\scanpy\plotting\_anndata.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, expression_cutoff, mean_only_expressed, color_map, dot_max, dot_min, standard_scale, smallest_dot, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1809 num_categories,. 1810 layer=layer,. -> 1811 gene_symbols=gene_symbols,. 1812 ). 1813 . ~\Anaconda3\envs\UMCU\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols). 2911 matrix = adata[:, var_names].layers[layer]. 2912 elif use_raw:. -> 2913 matrix = adata.raw[:, var_names].X. 2914 else:. 2915 matrix = adata[:, var_names].X. ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\raw.py in __getitem__(self, index). 94 . 95 def __getitem__(self, index):. ---> 96 oidx, vidx = self._normalize_indices(index). 97 . 98 # To preserve two dimensional shape. ~\Anaconda3\envs\UM",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:1089,deployability,log,log,1089,"object which I converted to .h5ad files using Seurat-disk, I have read them into scanpy and want to plot a few markers using a dotplot. For one dataset, I can plot any gene I want and it succeeds. For the other dataset, I can't plot **any** gene, in any type of plot (tsne, violin, dotplot, matrixplot, etc.). ```. adata_1 = sc.read_h5ad(""path/to/file1.h5ad""). adata_2 = sc.read_h5ad(""path/to/file2.h5ad""). ```. ```. #this works. sc.pl.dotplot(adata_1, adata_1.var_names[0:4], groupby='clusters', color_map = 'Reds'). #these do not. sc.pl.dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds'). ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-72-4fc81df5ca3f> in <module>. ----> 1 sc.pl.dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds'). ~\Anaconda3\envs\UMCU\lib\site-packages\scanpy\plotting\_anndata.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, expression_cutoff, mean_only_expressed, color_map, dot_max, dot_min, standard_scale, smallest_dot, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1809 num_categories,. 1810 layer=layer,. -> 1811 gene_symbols=gene_symbols,. 1812 ). 1813 . ~\Anaconda3\envs\UMCU\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols). 2911 matrix = adata[:, var_names].layers[layer]. 2912 elif use_raw:. -> 2913 matrix = adata.raw[:, var_names].X. 2914 else:. 2915 matrix = adata[:, var_names].X. ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\raw.py in __getitem__(self, index). 94 . 95 def __getitem__(self, index):. ---> 96 oidx, vidx = self._normalize_indices(index). 97 . 98 # To preserve two dimensional shape. ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\raw.py in _normalize_indices(self, packed_index). 154 ob",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:1549,deployability,log,log,1549,".var_names[0:4], groupby='clusters', color_map = 'Reds'). #these do not. sc.pl.dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds'). ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-72-4fc81df5ca3f> in <module>. ----> 1 sc.pl.dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds'). ~\Anaconda3\envs\UMCU\lib\site-packages\scanpy\plotting\_anndata.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, expression_cutoff, mean_only_expressed, color_map, dot_max, dot_min, standard_scale, smallest_dot, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1809 num_categories,. 1810 layer=layer,. -> 1811 gene_symbols=gene_symbols,. 1812 ). 1813 . ~\Anaconda3\envs\UMCU\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols). 2911 matrix = adata[:, var_names].layers[layer]. 2912 elif use_raw:. -> 2913 matrix = adata.raw[:, var_names].X. 2914 else:. 2915 matrix = adata[:, var_names].X. ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\raw.py in __getitem__(self, index). 94 . 95 def __getitem__(self, index):. ---> 96 oidx, vidx = self._normalize_indices(index). 97 . 98 # To preserve two dimensional shape. ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\raw.py in _normalize_indices(self, packed_index). 154 obs, var = unpack_index(packed_index). 155 obs = _normalize_index(obs, self._adata.obs_names). --> 156 var = _normalize_index(var, self.var_names). 157 return obs, var. 158 . ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\index.py in _normalize_index(indexer, index). 91 not_found = indexer[positions < 0]. 92 raise KeyError(. ---> 93 f""Values {list(not_found)}, from {list(indexer)}, "". 94 ""are not valid obs/ var names or indices."". 95 ). KeyError: ""Val",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:869,modifiability,modul,module,869,"Hi! Same issue here, but for me no plotting function works for any gene. I have two Seurat object which I converted to .h5ad files using Seurat-disk, I have read them into scanpy and want to plot a few markers using a dotplot. For one dataset, I can plot any gene I want and it succeeds. For the other dataset, I can't plot **any** gene, in any type of plot (tsne, violin, dotplot, matrixplot, etc.). ```. adata_1 = sc.read_h5ad(""path/to/file1.h5ad""). adata_2 = sc.read_h5ad(""path/to/file2.h5ad""). ```. ```. #this works. sc.pl.dotplot(adata_1, adata_1.var_names[0:4], groupby='clusters', color_map = 'Reds'). #these do not. sc.pl.dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds'). ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-72-4fc81df5ca3f> in <module>. ----> 1 sc.pl.dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds'). ~\Anaconda3\envs\UMCU\lib\site-packages\scanpy\plotting\_anndata.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, expression_cutoff, mean_only_expressed, color_map, dot_max, dot_min, standard_scale, smallest_dot, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1809 num_categories,. 1810 layer=layer,. -> 1811 gene_symbols=gene_symbols,. 1812 ). 1813 . ~\Anaconda3\envs\UMCU\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols). 2911 matrix = adata[:, var_names].layers[layer]. 2912 elif use_raw:. -> 2913 matrix = adata.raw[:, var_names].X. 2914 else:. 2915 matrix = adata[:, var_names].X. ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\raw.py in __getitem__(self, index). 94 . 95 def __getitem__(self, index):. ---> 96 oidx, vidx = self._normalize_indices(index). 97 . 98 # To preserve two dimensional shape. ~\Anaconda3\envs\UM",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:1005,modifiability,pac,packages,1005,"issue here, but for me no plotting function works for any gene. I have two Seurat object which I converted to .h5ad files using Seurat-disk, I have read them into scanpy and want to plot a few markers using a dotplot. For one dataset, I can plot any gene I want and it succeeds. For the other dataset, I can't plot **any** gene, in any type of plot (tsne, violin, dotplot, matrixplot, etc.). ```. adata_1 = sc.read_h5ad(""path/to/file1.h5ad""). adata_2 = sc.read_h5ad(""path/to/file2.h5ad""). ```. ```. #this works. sc.pl.dotplot(adata_1, adata_1.var_names[0:4], groupby='clusters', color_map = 'Reds'). #these do not. sc.pl.dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds'). ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-72-4fc81df5ca3f> in <module>. ----> 1 sc.pl.dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds'). ~\Anaconda3\envs\UMCU\lib\site-packages\scanpy\plotting\_anndata.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, expression_cutoff, mean_only_expressed, color_map, dot_max, dot_min, standard_scale, smallest_dot, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1809 num_categories,. 1810 layer=layer,. -> 1811 gene_symbols=gene_symbols,. 1812 ). 1813 . ~\Anaconda3\envs\UMCU\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols). 2911 matrix = adata[:, var_names].layers[layer]. 2912 elif use_raw:. -> 2913 matrix = adata.raw[:, var_names].X. 2914 else:. 2915 matrix = adata[:, var_names].X. ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\raw.py in __getitem__(self, index). 94 . 95 def __getitem__(self, index):. ---> 96 oidx, vidx = self._normalize_indices(index). 97 . 98 # To preserve two dimensional shape. ~\Anaconda3\envs\UMCU\lib\si",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:1303,modifiability,layer,layer,1303,"set, I can't plot **any** gene, in any type of plot (tsne, violin, dotplot, matrixplot, etc.). ```. adata_1 = sc.read_h5ad(""path/to/file1.h5ad""). adata_2 = sc.read_h5ad(""path/to/file2.h5ad""). ```. ```. #this works. sc.pl.dotplot(adata_1, adata_1.var_names[0:4], groupby='clusters', color_map = 'Reds'). #these do not. sc.pl.dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds'). ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-72-4fc81df5ca3f> in <module>. ----> 1 sc.pl.dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds'). ~\Anaconda3\envs\UMCU\lib\site-packages\scanpy\plotting\_anndata.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, expression_cutoff, mean_only_expressed, color_map, dot_max, dot_min, standard_scale, smallest_dot, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1809 num_categories,. 1810 layer=layer,. -> 1811 gene_symbols=gene_symbols,. 1812 ). 1813 . ~\Anaconda3\envs\UMCU\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols). 2911 matrix = adata[:, var_names].layers[layer]. 2912 elif use_raw:. -> 2913 matrix = adata.raw[:, var_names].X. 2914 else:. 2915 matrix = adata[:, var_names].X. ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\raw.py in __getitem__(self, index). 94 . 95 def __getitem__(self, index):. ---> 96 oidx, vidx = self._normalize_indices(index). 97 . 98 # To preserve two dimensional shape. ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\raw.py in _normalize_indices(self, packed_index). 154 obs, var = unpack_index(packed_index). 155 obs = _normalize_index(obs, self._adata.obs_names). --> 156 var = _normalize_index(var, self.var_names). 157 return obs, var. 158 . ~\Anaconda3\envs\UMCU\lib\site-packages\an",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:1358,modifiability,layer,layer,1358,"ne, violin, dotplot, matrixplot, etc.). ```. adata_1 = sc.read_h5ad(""path/to/file1.h5ad""). adata_2 = sc.read_h5ad(""path/to/file2.h5ad""). ```. ```. #this works. sc.pl.dotplot(adata_1, adata_1.var_names[0:4], groupby='clusters', color_map = 'Reds'). #these do not. sc.pl.dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds'). ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-72-4fc81df5ca3f> in <module>. ----> 1 sc.pl.dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds'). ~\Anaconda3\envs\UMCU\lib\site-packages\scanpy\plotting\_anndata.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, expression_cutoff, mean_only_expressed, color_map, dot_max, dot_min, standard_scale, smallest_dot, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1809 num_categories,. 1810 layer=layer,. -> 1811 gene_symbols=gene_symbols,. 1812 ). 1813 . ~\Anaconda3\envs\UMCU\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols). 2911 matrix = adata[:, var_names].layers[layer]. 2912 elif use_raw:. -> 2913 matrix = adata.raw[:, var_names].X. 2914 else:. 2915 matrix = adata[:, var_names].X. ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\raw.py in __getitem__(self, index). 94 . 95 def __getitem__(self, index):. ---> 96 oidx, vidx = self._normalize_indices(index). 97 . 98 # To preserve two dimensional shape. ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\raw.py in _normalize_indices(self, packed_index). 154 obs, var = unpack_index(packed_index). 155 obs = _normalize_index(obs, self._adata.obs_names). --> 156 var = _normalize_index(var, self.var_names). 157 return obs, var. 158 . ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\index.py in _normalize_index(indexer, index",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:1364,modifiability,layer,layer,1364,"olin, dotplot, matrixplot, etc.). ```. adata_1 = sc.read_h5ad(""path/to/file1.h5ad""). adata_2 = sc.read_h5ad(""path/to/file2.h5ad""). ```. ```. #this works. sc.pl.dotplot(adata_1, adata_1.var_names[0:4], groupby='clusters', color_map = 'Reds'). #these do not. sc.pl.dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds'). ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-72-4fc81df5ca3f> in <module>. ----> 1 sc.pl.dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds'). ~\Anaconda3\envs\UMCU\lib\site-packages\scanpy\plotting\_anndata.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, expression_cutoff, mean_only_expressed, color_map, dot_max, dot_min, standard_scale, smallest_dot, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1809 num_categories,. 1810 layer=layer,. -> 1811 gene_symbols=gene_symbols,. 1812 ). 1813 . ~\Anaconda3\envs\UMCU\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols). 2911 matrix = adata[:, var_names].layers[layer]. 2912 elif use_raw:. -> 2913 matrix = adata.raw[:, var_names].X. 2914 else:. 2915 matrix = adata[:, var_names].X. ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\raw.py in __getitem__(self, index). 94 . 95 def __getitem__(self, index):. ---> 96 oidx, vidx = self._normalize_indices(index). 97 . 98 # To preserve two dimensional shape. ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\raw.py in _normalize_indices(self, packed_index). 154 obs, var = unpack_index(packed_index). 155 obs = _normalize_index(obs, self._adata.obs_names). --> 156 var = _normalize_index(var, self.var_names). 157 return obs, var. 158 . ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\index.py in _normalize_index(indexer, index). 91 ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:1454,modifiability,pac,packages,1454,"2 = sc.read_h5ad(""path/to/file2.h5ad""). ```. ```. #this works. sc.pl.dotplot(adata_1, adata_1.var_names[0:4], groupby='clusters', color_map = 'Reds'). #these do not. sc.pl.dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds'). ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-72-4fc81df5ca3f> in <module>. ----> 1 sc.pl.dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds'). ~\Anaconda3\envs\UMCU\lib\site-packages\scanpy\plotting\_anndata.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, expression_cutoff, mean_only_expressed, color_map, dot_max, dot_min, standard_scale, smallest_dot, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1809 num_categories,. 1810 layer=layer,. -> 1811 gene_symbols=gene_symbols,. 1812 ). 1813 . ~\Anaconda3\envs\UMCU\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols). 2911 matrix = adata[:, var_names].layers[layer]. 2912 elif use_raw:. -> 2913 matrix = adata.raw[:, var_names].X. 2914 else:. 2915 matrix = adata[:, var_names].X. ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\raw.py in __getitem__(self, index). 94 . 95 def __getitem__(self, index):. ---> 96 oidx, vidx = self._normalize_indices(index). 97 . 98 # To preserve two dimensional shape. ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\raw.py in _normalize_indices(self, packed_index). 154 obs, var = unpack_index(packed_index). 155 obs = _normalize_index(obs, self._adata.obs_names). --> 156 var = _normalize_index(var, self.var_names). 157 return obs, var. 158 . ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\index.py in _normalize_index(indexer, index). 91 not_found = indexer[positions < 0]. 92 raise KeyError(. ---> 93 f""Values {list(not_found)},",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:1570,modifiability,layer,layer,1570,"by='clusters', color_map = 'Reds'). #these do not. sc.pl.dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds'). ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-72-4fc81df5ca3f> in <module>. ----> 1 sc.pl.dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds'). ~\Anaconda3\envs\UMCU\lib\site-packages\scanpy\plotting\_anndata.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, expression_cutoff, mean_only_expressed, color_map, dot_max, dot_min, standard_scale, smallest_dot, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1809 num_categories,. 1810 layer=layer,. -> 1811 gene_symbols=gene_symbols,. 1812 ). 1813 . ~\Anaconda3\envs\UMCU\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols). 2911 matrix = adata[:, var_names].layers[layer]. 2912 elif use_raw:. -> 2913 matrix = adata.raw[:, var_names].X. 2914 else:. 2915 matrix = adata[:, var_names].X. ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\raw.py in __getitem__(self, index). 94 . 95 def __getitem__(self, index):. ---> 96 oidx, vidx = self._normalize_indices(index). 97 . 98 # To preserve two dimensional shape. ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\raw.py in _normalize_indices(self, packed_index). 154 obs, var = unpack_index(packed_index). 155 obs = _normalize_index(obs, self._adata.obs_names). --> 156 var = _normalize_index(var, self.var_names). 157 return obs, var. 158 . ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\index.py in _normalize_index(indexer, index). 91 not_found = indexer[positions < 0]. 92 raise KeyError(. ---> 93 f""Values {list(not_found)}, from {list(indexer)}, "". 94 ""are not valid obs/ var names or indices."". 95 ). KeyError: ""Values ['Rgs20', 'Oprk1',",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:1626,modifiability,layer,layers,1626,".dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds'). ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-72-4fc81df5ca3f> in <module>. ----> 1 sc.pl.dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds'). ~\Anaconda3\envs\UMCU\lib\site-packages\scanpy\plotting\_anndata.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, expression_cutoff, mean_only_expressed, color_map, dot_max, dot_min, standard_scale, smallest_dot, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1809 num_categories,. 1810 layer=layer,. -> 1811 gene_symbols=gene_symbols,. 1812 ). 1813 . ~\Anaconda3\envs\UMCU\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols). 2911 matrix = adata[:, var_names].layers[layer]. 2912 elif use_raw:. -> 2913 matrix = adata.raw[:, var_names].X. 2914 else:. 2915 matrix = adata[:, var_names].X. ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\raw.py in __getitem__(self, index). 94 . 95 def __getitem__(self, index):. ---> 96 oidx, vidx = self._normalize_indices(index). 97 . 98 # To preserve two dimensional shape. ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\raw.py in _normalize_indices(self, packed_index). 154 obs, var = unpack_index(packed_index). 155 obs = _normalize_index(obs, self._adata.obs_names). --> 156 var = _normalize_index(var, self.var_names). 157 return obs, var. 158 . ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\index.py in _normalize_index(indexer, index). 91 not_found = indexer[positions < 0]. 92 raise KeyError(. ---> 93 f""Values {list(not_found)}, from {list(indexer)}, "". 94 ""are not valid obs/ var names or indices."". 95 ). KeyError: ""Values ['Rgs20', 'Oprk1', 'St18', 'Gm26901'], from ['Rgs20', 'Oprk1', 'St18', 'Gm",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:1633,modifiability,layer,layer,1633,"t(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds'). ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-72-4fc81df5ca3f> in <module>. ----> 1 sc.pl.dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds'). ~\Anaconda3\envs\UMCU\lib\site-packages\scanpy\plotting\_anndata.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, expression_cutoff, mean_only_expressed, color_map, dot_max, dot_min, standard_scale, smallest_dot, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1809 num_categories,. 1810 layer=layer,. -> 1811 gene_symbols=gene_symbols,. 1812 ). 1813 . ~\Anaconda3\envs\UMCU\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols). 2911 matrix = adata[:, var_names].layers[layer]. 2912 elif use_raw:. -> 2913 matrix = adata.raw[:, var_names].X. 2914 else:. 2915 matrix = adata[:, var_names].X. ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\raw.py in __getitem__(self, index). 94 . 95 def __getitem__(self, index):. ---> 96 oidx, vidx = self._normalize_indices(index). 97 . 98 # To preserve two dimensional shape. ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\raw.py in _normalize_indices(self, packed_index). 154 obs, var = unpack_index(packed_index). 155 obs = _normalize_index(obs, self._adata.obs_names). --> 156 var = _normalize_index(var, self.var_names). 157 return obs, var. 158 . ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\index.py in _normalize_index(indexer, index). 91 not_found = indexer[positions < 0]. 92 raise KeyError(. ---> 93 f""Values {list(not_found)}, from {list(indexer)}, "". 94 ""are not valid obs/ var names or indices."". 95 ). KeyError: ""Values ['Rgs20', 'Oprk1', 'St18', 'Gm26901'], from ['Rgs20', 'Oprk1', 'St18', 'Gm26901']",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:1785,modifiability,pac,packages,1785,"KeyError Traceback (most recent call last). <ipython-input-72-4fc81df5ca3f> in <module>. ----> 1 sc.pl.dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds'). ~\Anaconda3\envs\UMCU\lib\site-packages\scanpy\plotting\_anndata.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, expression_cutoff, mean_only_expressed, color_map, dot_max, dot_min, standard_scale, smallest_dot, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1809 num_categories,. 1810 layer=layer,. -> 1811 gene_symbols=gene_symbols,. 1812 ). 1813 . ~\Anaconda3\envs\UMCU\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols). 2911 matrix = adata[:, var_names].layers[layer]. 2912 elif use_raw:. -> 2913 matrix = adata.raw[:, var_names].X. 2914 else:. 2915 matrix = adata[:, var_names].X. ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\raw.py in __getitem__(self, index). 94 . 95 def __getitem__(self, index):. ---> 96 oidx, vidx = self._normalize_indices(index). 97 . 98 # To preserve two dimensional shape. ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\raw.py in _normalize_indices(self, packed_index). 154 obs, var = unpack_index(packed_index). 155 obs = _normalize_index(obs, self._adata.obs_names). --> 156 var = _normalize_index(var, self.var_names). 157 return obs, var. 158 . ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\index.py in _normalize_index(indexer, index). 91 not_found = indexer[positions < 0]. 92 raise KeyError(. ---> 93 f""Values {list(not_found)}, from {list(indexer)}, "". 94 ""are not valid obs/ var names or indices."". 95 ). KeyError: ""Values ['Rgs20', 'Oprk1', 'St18', 'Gm26901'], from ['Rgs20', 'Oprk1', 'St18', 'Gm26901'], are not valid obs/ var names or indices."". sc.pl.matrixplot(adata_2, adata_2.var_names[0:4], groupby='celltype') #same error. sc.pl.tsne(adata_2, color",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:2012,modifiability,pac,packages,2012,">. ----> 1 sc.pl.dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds'). ~\Anaconda3\envs\UMCU\lib\site-packages\scanpy\plotting\_anndata.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, expression_cutoff, mean_only_expressed, color_map, dot_max, dot_min, standard_scale, smallest_dot, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1809 num_categories,. 1810 layer=layer,. -> 1811 gene_symbols=gene_symbols,. 1812 ). 1813 . ~\Anaconda3\envs\UMCU\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols). 2911 matrix = adata[:, var_names].layers[layer]. 2912 elif use_raw:. -> 2913 matrix = adata.raw[:, var_names].X. 2914 else:. 2915 matrix = adata[:, var_names].X. ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\raw.py in __getitem__(self, index). 94 . 95 def __getitem__(self, index):. ---> 96 oidx, vidx = self._normalize_indices(index). 97 . 98 # To preserve two dimensional shape. ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\raw.py in _normalize_indices(self, packed_index). 154 obs, var = unpack_index(packed_index). 155 obs = _normalize_index(obs, self._adata.obs_names). --> 156 var = _normalize_index(var, self.var_names). 157 return obs, var. 158 . ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\index.py in _normalize_index(indexer, index). 91 not_found = indexer[positions < 0]. 92 raise KeyError(. ---> 93 f""Values {list(not_found)}, from {list(indexer)}, "". 94 ""are not valid obs/ var names or indices."". 95 ). KeyError: ""Values ['Rgs20', 'Oprk1', 'St18', 'Gm26901'], from ['Rgs20', 'Oprk1', 'St18', 'Gm26901'], are not valid obs/ var names or indices."". sc.pl.matrixplot(adata_2, adata_2.var_names[0:4], groupby='celltype') #same error. sc.pl.tsne(adata_2, color=adata_2.var_names[0:4]) #KeyError: 'Rgs20' . ```. Any help would be much appreciated!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:2295,modifiability,pac,packages,2295,">. ----> 1 sc.pl.dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds'). ~\Anaconda3\envs\UMCU\lib\site-packages\scanpy\plotting\_anndata.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, expression_cutoff, mean_only_expressed, color_map, dot_max, dot_min, standard_scale, smallest_dot, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1809 num_categories,. 1810 layer=layer,. -> 1811 gene_symbols=gene_symbols,. 1812 ). 1813 . ~\Anaconda3\envs\UMCU\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols). 2911 matrix = adata[:, var_names].layers[layer]. 2912 elif use_raw:. -> 2913 matrix = adata.raw[:, var_names].X. 2914 else:. 2915 matrix = adata[:, var_names].X. ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\raw.py in __getitem__(self, index). 94 . 95 def __getitem__(self, index):. ---> 96 oidx, vidx = self._normalize_indices(index). 97 . 98 # To preserve two dimensional shape. ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\raw.py in _normalize_indices(self, packed_index). 154 obs, var = unpack_index(packed_index). 155 obs = _normalize_index(obs, self._adata.obs_names). --> 156 var = _normalize_index(var, self.var_names). 157 return obs, var. 158 . ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\index.py in _normalize_index(indexer, index). 91 not_found = indexer[positions < 0]. 92 raise KeyError(. ---> 93 f""Values {list(not_found)}, from {list(indexer)}, "". 94 ""are not valid obs/ var names or indices."". 95 ). KeyError: ""Values ['Rgs20', 'Oprk1', 'St18', 'Gm26901'], from ['Rgs20', 'Oprk1', 'St18', 'Gm26901'], are not valid obs/ var names or indices."". sc.pl.matrixplot(adata_2, adata_2.var_names[0:4], groupby='celltype') #same error. sc.pl.tsne(adata_2, color=adata_2.var_names[0:4]) #KeyError: 'Rgs20' . ```. Any help would be much appreciated!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:144,performance,disk,disk,144,"Hi! Same issue here, but for me no plotting function works for any gene. I have two Seurat object which I converted to .h5ad files using Seurat-disk, I have read them into scanpy and want to plot a few markers using a dotplot. For one dataset, I can plot any gene I want and it succeeds. For the other dataset, I can't plot **any** gene, in any type of plot (tsne, violin, dotplot, matrixplot, etc.). ```. adata_1 = sc.read_h5ad(""path/to/file1.h5ad""). adata_2 = sc.read_h5ad(""path/to/file2.h5ad""). ```. ```. #this works. sc.pl.dotplot(adata_1, adata_1.var_names[0:4], groupby='clusters', color_map = 'Reds'). #these do not. sc.pl.dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds'). ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-72-4fc81df5ca3f> in <module>. ----> 1 sc.pl.dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds'). ~\Anaconda3\envs\UMCU\lib\site-packages\scanpy\plotting\_anndata.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, expression_cutoff, mean_only_expressed, color_map, dot_max, dot_min, standard_scale, smallest_dot, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1809 num_categories,. 1810 layer=layer,. -> 1811 gene_symbols=gene_symbols,. 1812 ). 1813 . ~\Anaconda3\envs\UMCU\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols). 2911 matrix = adata[:, var_names].layers[layer]. 2912 elif use_raw:. -> 2913 matrix = adata.raw[:, var_names].X. 2914 else:. 2915 matrix = adata[:, var_names].X. ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\raw.py in __getitem__(self, index). 94 . 95 def __getitem__(self, index):. ---> 96 oidx, vidx = self._normalize_indices(index). 97 . 98 # To preserve two dimensional shape. ~\Anaconda3\envs\UM",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:2757,performance,error,error,2757,">. ----> 1 sc.pl.dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds'). ~\Anaconda3\envs\UMCU\lib\site-packages\scanpy\plotting\_anndata.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, expression_cutoff, mean_only_expressed, color_map, dot_max, dot_min, standard_scale, smallest_dot, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1809 num_categories,. 1810 layer=layer,. -> 1811 gene_symbols=gene_symbols,. 1812 ). 1813 . ~\Anaconda3\envs\UMCU\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols). 2911 matrix = adata[:, var_names].layers[layer]. 2912 elif use_raw:. -> 2913 matrix = adata.raw[:, var_names].X. 2914 else:. 2915 matrix = adata[:, var_names].X. ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\raw.py in __getitem__(self, index). 94 . 95 def __getitem__(self, index):. ---> 96 oidx, vidx = self._normalize_indices(index). 97 . 98 # To preserve two dimensional shape. ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\raw.py in _normalize_indices(self, packed_index). 154 obs, var = unpack_index(packed_index). 155 obs = _normalize_index(obs, self._adata.obs_names). --> 156 var = _normalize_index(var, self.var_names). 157 return obs, var. 158 . ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\index.py in _normalize_index(indexer, index). 91 not_found = indexer[positions < 0]. 92 raise KeyError(. ---> 93 f""Values {list(not_found)}, from {list(indexer)}, "". 94 ""are not valid obs/ var names or indices."". 95 ). KeyError: ""Values ['Rgs20', 'Oprk1', 'St18', 'Gm26901'], from ['Rgs20', 'Oprk1', 'St18', 'Gm26901'], are not valid obs/ var names or indices."". sc.pl.matrixplot(adata_2, adata_2.var_names[0:4], groupby='celltype') #same error. sc.pl.tsne(adata_2, color=adata_2.var_names[0:4]) #KeyError: 'Rgs20' . ```. Any help would be much appreciated!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:842,safety,input,input-,842,"Hi! Same issue here, but for me no plotting function works for any gene. I have two Seurat object which I converted to .h5ad files using Seurat-disk, I have read them into scanpy and want to plot a few markers using a dotplot. For one dataset, I can plot any gene I want and it succeeds. For the other dataset, I can't plot **any** gene, in any type of plot (tsne, violin, dotplot, matrixplot, etc.). ```. adata_1 = sc.read_h5ad(""path/to/file1.h5ad""). adata_2 = sc.read_h5ad(""path/to/file2.h5ad""). ```. ```. #this works. sc.pl.dotplot(adata_1, adata_1.var_names[0:4], groupby='clusters', color_map = 'Reds'). #these do not. sc.pl.dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds'). ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-72-4fc81df5ca3f> in <module>. ----> 1 sc.pl.dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds'). ~\Anaconda3\envs\UMCU\lib\site-packages\scanpy\plotting\_anndata.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, expression_cutoff, mean_only_expressed, color_map, dot_max, dot_min, standard_scale, smallest_dot, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1809 num_categories,. 1810 layer=layer,. -> 1811 gene_symbols=gene_symbols,. 1812 ). 1813 . ~\Anaconda3\envs\UMCU\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols). 2911 matrix = adata[:, var_names].layers[layer]. 2912 elif use_raw:. -> 2913 matrix = adata.raw[:, var_names].X. 2914 else:. 2915 matrix = adata[:, var_names].X. ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\raw.py in __getitem__(self, index). 94 . 95 def __getitem__(self, index):. ---> 96 oidx, vidx = self._normalize_indices(index). 97 . 98 # To preserve two dimensional shape. ~\Anaconda3\envs\UM",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:869,safety,modul,module,869,"Hi! Same issue here, but for me no plotting function works for any gene. I have two Seurat object which I converted to .h5ad files using Seurat-disk, I have read them into scanpy and want to plot a few markers using a dotplot. For one dataset, I can plot any gene I want and it succeeds. For the other dataset, I can't plot **any** gene, in any type of plot (tsne, violin, dotplot, matrixplot, etc.). ```. adata_1 = sc.read_h5ad(""path/to/file1.h5ad""). adata_2 = sc.read_h5ad(""path/to/file2.h5ad""). ```. ```. #this works. sc.pl.dotplot(adata_1, adata_1.var_names[0:4], groupby='clusters', color_map = 'Reds'). #these do not. sc.pl.dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds'). ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-72-4fc81df5ca3f> in <module>. ----> 1 sc.pl.dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds'). ~\Anaconda3\envs\UMCU\lib\site-packages\scanpy\plotting\_anndata.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, expression_cutoff, mean_only_expressed, color_map, dot_max, dot_min, standard_scale, smallest_dot, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1809 num_categories,. 1810 layer=layer,. -> 1811 gene_symbols=gene_symbols,. 1812 ). 1813 . ~\Anaconda3\envs\UMCU\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols). 2911 matrix = adata[:, var_names].layers[layer]. 2912 elif use_raw:. -> 2913 matrix = adata.raw[:, var_names].X. 2914 else:. 2915 matrix = adata[:, var_names].X. ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\raw.py in __getitem__(self, index). 94 . 95 def __getitem__(self, index):. ---> 96 oidx, vidx = self._normalize_indices(index). 97 . 98 # To preserve two dimensional shape. ~\Anaconda3\envs\UM",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:1089,safety,log,log,1089,"object which I converted to .h5ad files using Seurat-disk, I have read them into scanpy and want to plot a few markers using a dotplot. For one dataset, I can plot any gene I want and it succeeds. For the other dataset, I can't plot **any** gene, in any type of plot (tsne, violin, dotplot, matrixplot, etc.). ```. adata_1 = sc.read_h5ad(""path/to/file1.h5ad""). adata_2 = sc.read_h5ad(""path/to/file2.h5ad""). ```. ```. #this works. sc.pl.dotplot(adata_1, adata_1.var_names[0:4], groupby='clusters', color_map = 'Reds'). #these do not. sc.pl.dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds'). ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-72-4fc81df5ca3f> in <module>. ----> 1 sc.pl.dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds'). ~\Anaconda3\envs\UMCU\lib\site-packages\scanpy\plotting\_anndata.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, expression_cutoff, mean_only_expressed, color_map, dot_max, dot_min, standard_scale, smallest_dot, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1809 num_categories,. 1810 layer=layer,. -> 1811 gene_symbols=gene_symbols,. 1812 ). 1813 . ~\Anaconda3\envs\UMCU\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols). 2911 matrix = adata[:, var_names].layers[layer]. 2912 elif use_raw:. -> 2913 matrix = adata.raw[:, var_names].X. 2914 else:. 2915 matrix = adata[:, var_names].X. ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\raw.py in __getitem__(self, index). 94 . 95 def __getitem__(self, index):. ---> 96 oidx, vidx = self._normalize_indices(index). 97 . 98 # To preserve two dimensional shape. ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\raw.py in _normalize_indices(self, packed_index). 154 ob",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:1549,safety,log,log,1549,".var_names[0:4], groupby='clusters', color_map = 'Reds'). #these do not. sc.pl.dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds'). ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-72-4fc81df5ca3f> in <module>. ----> 1 sc.pl.dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds'). ~\Anaconda3\envs\UMCU\lib\site-packages\scanpy\plotting\_anndata.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, expression_cutoff, mean_only_expressed, color_map, dot_max, dot_min, standard_scale, smallest_dot, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1809 num_categories,. 1810 layer=layer,. -> 1811 gene_symbols=gene_symbols,. 1812 ). 1813 . ~\Anaconda3\envs\UMCU\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols). 2911 matrix = adata[:, var_names].layers[layer]. 2912 elif use_raw:. -> 2913 matrix = adata.raw[:, var_names].X. 2914 else:. 2915 matrix = adata[:, var_names].X. ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\raw.py in __getitem__(self, index). 94 . 95 def __getitem__(self, index):. ---> 96 oidx, vidx = self._normalize_indices(index). 97 . 98 # To preserve two dimensional shape. ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\raw.py in _normalize_indices(self, packed_index). 154 obs, var = unpack_index(packed_index). 155 obs = _normalize_index(obs, self._adata.obs_names). --> 156 var = _normalize_index(var, self.var_names). 157 return obs, var. 158 . ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\index.py in _normalize_index(indexer, index). 91 not_found = indexer[positions < 0]. 92 raise KeyError(. ---> 93 f""Values {list(not_found)}, from {list(indexer)}, "". 94 ""are not valid obs/ var names or indices."". 95 ). KeyError: ""Val",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:2496,safety,valid,valid,2496,">. ----> 1 sc.pl.dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds'). ~\Anaconda3\envs\UMCU\lib\site-packages\scanpy\plotting\_anndata.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, expression_cutoff, mean_only_expressed, color_map, dot_max, dot_min, standard_scale, smallest_dot, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1809 num_categories,. 1810 layer=layer,. -> 1811 gene_symbols=gene_symbols,. 1812 ). 1813 . ~\Anaconda3\envs\UMCU\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols). 2911 matrix = adata[:, var_names].layers[layer]. 2912 elif use_raw:. -> 2913 matrix = adata.raw[:, var_names].X. 2914 else:. 2915 matrix = adata[:, var_names].X. ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\raw.py in __getitem__(self, index). 94 . 95 def __getitem__(self, index):. ---> 96 oidx, vidx = self._normalize_indices(index). 97 . 98 # To preserve two dimensional shape. ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\raw.py in _normalize_indices(self, packed_index). 154 obs, var = unpack_index(packed_index). 155 obs = _normalize_index(obs, self._adata.obs_names). --> 156 var = _normalize_index(var, self.var_names). 157 return obs, var. 158 . ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\index.py in _normalize_index(indexer, index). 91 not_found = indexer[positions < 0]. 92 raise KeyError(. ---> 93 f""Values {list(not_found)}, from {list(indexer)}, "". 94 ""are not valid obs/ var names or indices."". 95 ). KeyError: ""Values ['Rgs20', 'Oprk1', 'St18', 'Gm26901'], from ['Rgs20', 'Oprk1', 'St18', 'Gm26901'], are not valid obs/ var names or indices."". sc.pl.matrixplot(adata_2, adata_2.var_names[0:4], groupby='celltype') #same error. sc.pl.tsne(adata_2, color=adata_2.var_names[0:4]) #KeyError: 'Rgs20' . ```. Any help would be much appreciated!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:2646,safety,valid,valid,2646,">. ----> 1 sc.pl.dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds'). ~\Anaconda3\envs\UMCU\lib\site-packages\scanpy\plotting\_anndata.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, expression_cutoff, mean_only_expressed, color_map, dot_max, dot_min, standard_scale, smallest_dot, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1809 num_categories,. 1810 layer=layer,. -> 1811 gene_symbols=gene_symbols,. 1812 ). 1813 . ~\Anaconda3\envs\UMCU\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols). 2911 matrix = adata[:, var_names].layers[layer]. 2912 elif use_raw:. -> 2913 matrix = adata.raw[:, var_names].X. 2914 else:. 2915 matrix = adata[:, var_names].X. ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\raw.py in __getitem__(self, index). 94 . 95 def __getitem__(self, index):. ---> 96 oidx, vidx = self._normalize_indices(index). 97 . 98 # To preserve two dimensional shape. ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\raw.py in _normalize_indices(self, packed_index). 154 obs, var = unpack_index(packed_index). 155 obs = _normalize_index(obs, self._adata.obs_names). --> 156 var = _normalize_index(var, self.var_names). 157 return obs, var. 158 . ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\index.py in _normalize_index(indexer, index). 91 not_found = indexer[positions < 0]. 92 raise KeyError(. ---> 93 f""Values {list(not_found)}, from {list(indexer)}, "". 94 ""are not valid obs/ var names or indices."". 95 ). KeyError: ""Values ['Rgs20', 'Oprk1', 'St18', 'Gm26901'], from ['Rgs20', 'Oprk1', 'St18', 'Gm26901'], are not valid obs/ var names or indices."". sc.pl.matrixplot(adata_2, adata_2.var_names[0:4], groupby='celltype') #same error. sc.pl.tsne(adata_2, color=adata_2.var_names[0:4]) #KeyError: 'Rgs20' . ```. Any help would be much appreciated!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:2757,safety,error,error,2757,">. ----> 1 sc.pl.dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds'). ~\Anaconda3\envs\UMCU\lib\site-packages\scanpy\plotting\_anndata.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, expression_cutoff, mean_only_expressed, color_map, dot_max, dot_min, standard_scale, smallest_dot, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1809 num_categories,. 1810 layer=layer,. -> 1811 gene_symbols=gene_symbols,. 1812 ). 1813 . ~\Anaconda3\envs\UMCU\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols). 2911 matrix = adata[:, var_names].layers[layer]. 2912 elif use_raw:. -> 2913 matrix = adata.raw[:, var_names].X. 2914 else:. 2915 matrix = adata[:, var_names].X. ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\raw.py in __getitem__(self, index). 94 . 95 def __getitem__(self, index):. ---> 96 oidx, vidx = self._normalize_indices(index). 97 . 98 # To preserve two dimensional shape. ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\raw.py in _normalize_indices(self, packed_index). 154 obs, var = unpack_index(packed_index). 155 obs = _normalize_index(obs, self._adata.obs_names). --> 156 var = _normalize_index(var, self.var_names). 157 return obs, var. 158 . ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\index.py in _normalize_index(indexer, index). 91 not_found = indexer[positions < 0]. 92 raise KeyError(. ---> 93 f""Values {list(not_found)}, from {list(indexer)}, "". 94 ""are not valid obs/ var names or indices."". 95 ). KeyError: ""Values ['Rgs20', 'Oprk1', 'St18', 'Gm26901'], from ['Rgs20', 'Oprk1', 'St18', 'Gm26901'], are not valid obs/ var names or indices."". sc.pl.matrixplot(adata_2, adata_2.var_names[0:4], groupby='celltype') #same error. sc.pl.tsne(adata_2, color=adata_2.var_names[0:4]) #KeyError: 'Rgs20' . ```. Any help would be much appreciated!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:1089,security,log,log,1089,"object which I converted to .h5ad files using Seurat-disk, I have read them into scanpy and want to plot a few markers using a dotplot. For one dataset, I can plot any gene I want and it succeeds. For the other dataset, I can't plot **any** gene, in any type of plot (tsne, violin, dotplot, matrixplot, etc.). ```. adata_1 = sc.read_h5ad(""path/to/file1.h5ad""). adata_2 = sc.read_h5ad(""path/to/file2.h5ad""). ```. ```. #this works. sc.pl.dotplot(adata_1, adata_1.var_names[0:4], groupby='clusters', color_map = 'Reds'). #these do not. sc.pl.dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds'). ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-72-4fc81df5ca3f> in <module>. ----> 1 sc.pl.dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds'). ~\Anaconda3\envs\UMCU\lib\site-packages\scanpy\plotting\_anndata.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, expression_cutoff, mean_only_expressed, color_map, dot_max, dot_min, standard_scale, smallest_dot, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1809 num_categories,. 1810 layer=layer,. -> 1811 gene_symbols=gene_symbols,. 1812 ). 1813 . ~\Anaconda3\envs\UMCU\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols). 2911 matrix = adata[:, var_names].layers[layer]. 2912 elif use_raw:. -> 2913 matrix = adata.raw[:, var_names].X. 2914 else:. 2915 matrix = adata[:, var_names].X. ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\raw.py in __getitem__(self, index). 94 . 95 def __getitem__(self, index):. ---> 96 oidx, vidx = self._normalize_indices(index). 97 . 98 # To preserve two dimensional shape. ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\raw.py in _normalize_indices(self, packed_index). 154 ob",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:1549,security,log,log,1549,".var_names[0:4], groupby='clusters', color_map = 'Reds'). #these do not. sc.pl.dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds'). ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-72-4fc81df5ca3f> in <module>. ----> 1 sc.pl.dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds'). ~\Anaconda3\envs\UMCU\lib\site-packages\scanpy\plotting\_anndata.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, expression_cutoff, mean_only_expressed, color_map, dot_max, dot_min, standard_scale, smallest_dot, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1809 num_categories,. 1810 layer=layer,. -> 1811 gene_symbols=gene_symbols,. 1812 ). 1813 . ~\Anaconda3\envs\UMCU\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols). 2911 matrix = adata[:, var_names].layers[layer]. 2912 elif use_raw:. -> 2913 matrix = adata.raw[:, var_names].X. 2914 else:. 2915 matrix = adata[:, var_names].X. ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\raw.py in __getitem__(self, index). 94 . 95 def __getitem__(self, index):. ---> 96 oidx, vidx = self._normalize_indices(index). 97 . 98 # To preserve two dimensional shape. ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\raw.py in _normalize_indices(self, packed_index). 154 obs, var = unpack_index(packed_index). 155 obs = _normalize_index(obs, self._adata.obs_names). --> 156 var = _normalize_index(var, self.var_names). 157 return obs, var. 158 . ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\index.py in _normalize_index(indexer, index). 91 not_found = indexer[positions < 0]. 92 raise KeyError(. ---> 93 f""Values {list(not_found)}, from {list(indexer)}, "". 94 ""are not valid obs/ var names or indices."". 95 ). KeyError: ""Val",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:798,testability,Trace,Traceback,798,"Hi! Same issue here, but for me no plotting function works for any gene. I have two Seurat object which I converted to .h5ad files using Seurat-disk, I have read them into scanpy and want to plot a few markers using a dotplot. For one dataset, I can plot any gene I want and it succeeds. For the other dataset, I can't plot **any** gene, in any type of plot (tsne, violin, dotplot, matrixplot, etc.). ```. adata_1 = sc.read_h5ad(""path/to/file1.h5ad""). adata_2 = sc.read_h5ad(""path/to/file2.h5ad""). ```. ```. #this works. sc.pl.dotplot(adata_1, adata_1.var_names[0:4], groupby='clusters', color_map = 'Reds'). #these do not. sc.pl.dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds'). ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-72-4fc81df5ca3f> in <module>. ----> 1 sc.pl.dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds'). ~\Anaconda3\envs\UMCU\lib\site-packages\scanpy\plotting\_anndata.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, expression_cutoff, mean_only_expressed, color_map, dot_max, dot_min, standard_scale, smallest_dot, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1809 num_categories,. 1810 layer=layer,. -> 1811 gene_symbols=gene_symbols,. 1812 ). 1813 . ~\Anaconda3\envs\UMCU\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols). 2911 matrix = adata[:, var_names].layers[layer]. 2912 elif use_raw:. -> 2913 matrix = adata.raw[:, var_names].X. 2914 else:. 2915 matrix = adata[:, var_names].X. ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\raw.py in __getitem__(self, index). 94 . 95 def __getitem__(self, index):. ---> 96 oidx, vidx = self._normalize_indices(index). 97 . 98 # To preserve two dimensional shape. ~\Anaconda3\envs\UM",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:1089,testability,log,log,1089,"object which I converted to .h5ad files using Seurat-disk, I have read them into scanpy and want to plot a few markers using a dotplot. For one dataset, I can plot any gene I want and it succeeds. For the other dataset, I can't plot **any** gene, in any type of plot (tsne, violin, dotplot, matrixplot, etc.). ```. adata_1 = sc.read_h5ad(""path/to/file1.h5ad""). adata_2 = sc.read_h5ad(""path/to/file2.h5ad""). ```. ```. #this works. sc.pl.dotplot(adata_1, adata_1.var_names[0:4], groupby='clusters', color_map = 'Reds'). #these do not. sc.pl.dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds'). ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-72-4fc81df5ca3f> in <module>. ----> 1 sc.pl.dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds'). ~\Anaconda3\envs\UMCU\lib\site-packages\scanpy\plotting\_anndata.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, expression_cutoff, mean_only_expressed, color_map, dot_max, dot_min, standard_scale, smallest_dot, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1809 num_categories,. 1810 layer=layer,. -> 1811 gene_symbols=gene_symbols,. 1812 ). 1813 . ~\Anaconda3\envs\UMCU\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols). 2911 matrix = adata[:, var_names].layers[layer]. 2912 elif use_raw:. -> 2913 matrix = adata.raw[:, var_names].X. 2914 else:. 2915 matrix = adata[:, var_names].X. ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\raw.py in __getitem__(self, index). 94 . 95 def __getitem__(self, index):. ---> 96 oidx, vidx = self._normalize_indices(index). 97 . 98 # To preserve two dimensional shape. ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\raw.py in _normalize_indices(self, packed_index). 154 ob",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:1549,testability,log,log,1549,".var_names[0:4], groupby='clusters', color_map = 'Reds'). #these do not. sc.pl.dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds'). ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-72-4fc81df5ca3f> in <module>. ----> 1 sc.pl.dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds'). ~\Anaconda3\envs\UMCU\lib\site-packages\scanpy\plotting\_anndata.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, expression_cutoff, mean_only_expressed, color_map, dot_max, dot_min, standard_scale, smallest_dot, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1809 num_categories,. 1810 layer=layer,. -> 1811 gene_symbols=gene_symbols,. 1812 ). 1813 . ~\Anaconda3\envs\UMCU\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols). 2911 matrix = adata[:, var_names].layers[layer]. 2912 elif use_raw:. -> 2913 matrix = adata.raw[:, var_names].X. 2914 else:. 2915 matrix = adata[:, var_names].X. ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\raw.py in __getitem__(self, index). 94 . 95 def __getitem__(self, index):. ---> 96 oidx, vidx = self._normalize_indices(index). 97 . 98 # To preserve two dimensional shape. ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\raw.py in _normalize_indices(self, packed_index). 154 obs, var = unpack_index(packed_index). 155 obs = _normalize_index(obs, self._adata.obs_names). --> 156 var = _normalize_index(var, self.var_names). 157 return obs, var. 158 . ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\index.py in _normalize_index(indexer, index). 91 not_found = indexer[positions < 0]. 92 raise KeyError(. ---> 93 f""Values {list(not_found)}, from {list(indexer)}, "". 94 ""are not valid obs/ var names or indices."". 95 ). KeyError: ""Val",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:842,usability,input,input-,842,"Hi! Same issue here, but for me no plotting function works for any gene. I have two Seurat object which I converted to .h5ad files using Seurat-disk, I have read them into scanpy and want to plot a few markers using a dotplot. For one dataset, I can plot any gene I want and it succeeds. For the other dataset, I can't plot **any** gene, in any type of plot (tsne, violin, dotplot, matrixplot, etc.). ```. adata_1 = sc.read_h5ad(""path/to/file1.h5ad""). adata_2 = sc.read_h5ad(""path/to/file2.h5ad""). ```. ```. #this works. sc.pl.dotplot(adata_1, adata_1.var_names[0:4], groupby='clusters', color_map = 'Reds'). #these do not. sc.pl.dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds'). ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-72-4fc81df5ca3f> in <module>. ----> 1 sc.pl.dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds'). ~\Anaconda3\envs\UMCU\lib\site-packages\scanpy\plotting\_anndata.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, expression_cutoff, mean_only_expressed, color_map, dot_max, dot_min, standard_scale, smallest_dot, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1809 num_categories,. 1810 layer=layer,. -> 1811 gene_symbols=gene_symbols,. 1812 ). 1813 . ~\Anaconda3\envs\UMCU\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols). 2911 matrix = adata[:, var_names].layers[layer]. 2912 elif use_raw:. -> 2913 matrix = adata.raw[:, var_names].X. 2914 else:. 2915 matrix = adata[:, var_names].X. ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\raw.py in __getitem__(self, index). 94 . 95 def __getitem__(self, index):. ---> 96 oidx, vidx = self._normalize_indices(index). 97 . 98 # To preserve two dimensional shape. ~\Anaconda3\envs\UM",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:2757,usability,error,error,2757,">. ----> 1 sc.pl.dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds'). ~\Anaconda3\envs\UMCU\lib\site-packages\scanpy\plotting\_anndata.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, expression_cutoff, mean_only_expressed, color_map, dot_max, dot_min, standard_scale, smallest_dot, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1809 num_categories,. 1810 layer=layer,. -> 1811 gene_symbols=gene_symbols,. 1812 ). 1813 . ~\Anaconda3\envs\UMCU\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols). 2911 matrix = adata[:, var_names].layers[layer]. 2912 elif use_raw:. -> 2913 matrix = adata.raw[:, var_names].X. 2914 else:. 2915 matrix = adata[:, var_names].X. ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\raw.py in __getitem__(self, index). 94 . 95 def __getitem__(self, index):. ---> 96 oidx, vidx = self._normalize_indices(index). 97 . 98 # To preserve two dimensional shape. ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\raw.py in _normalize_indices(self, packed_index). 154 obs, var = unpack_index(packed_index). 155 obs = _normalize_index(obs, self._adata.obs_names). --> 156 var = _normalize_index(var, self.var_names). 157 return obs, var. 158 . ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\index.py in _normalize_index(indexer, index). 91 not_found = indexer[positions < 0]. 92 raise KeyError(. ---> 93 f""Values {list(not_found)}, from {list(indexer)}, "". 94 ""are not valid obs/ var names or indices."". 95 ). KeyError: ""Values ['Rgs20', 'Oprk1', 'St18', 'Gm26901'], from ['Rgs20', 'Oprk1', 'St18', 'Gm26901'], are not valid obs/ var names or indices."". sc.pl.matrixplot(adata_2, adata_2.var_names[0:4], groupby='celltype') #same error. sc.pl.tsne(adata_2, color=adata_2.var_names[0:4]) #KeyError: 'Rgs20' . ```. Any help would be much appreciated!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:2844,usability,help,help,2844,">. ----> 1 sc.pl.dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds'). ~\Anaconda3\envs\UMCU\lib\site-packages\scanpy\plotting\_anndata.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, expression_cutoff, mean_only_expressed, color_map, dot_max, dot_min, standard_scale, smallest_dot, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds). 1809 num_categories,. 1810 layer=layer,. -> 1811 gene_symbols=gene_symbols,. 1812 ). 1813 . ~\Anaconda3\envs\UMCU\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols). 2911 matrix = adata[:, var_names].layers[layer]. 2912 elif use_raw:. -> 2913 matrix = adata.raw[:, var_names].X. 2914 else:. 2915 matrix = adata[:, var_names].X. ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\raw.py in __getitem__(self, index). 94 . 95 def __getitem__(self, index):. ---> 96 oidx, vidx = self._normalize_indices(index). 97 . 98 # To preserve two dimensional shape. ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\raw.py in _normalize_indices(self, packed_index). 154 obs, var = unpack_index(packed_index). 155 obs = _normalize_index(obs, self._adata.obs_names). --> 156 var = _normalize_index(var, self.var_names). 157 return obs, var. 158 . ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\index.py in _normalize_index(indexer, index). 91 not_found = indexer[positions < 0]. 92 raise KeyError(. ---> 93 f""Values {list(not_found)}, from {list(indexer)}, "". 94 ""are not valid obs/ var names or indices."". 95 ). KeyError: ""Values ['Rgs20', 'Oprk1', 'St18', 'Gm26901'], from ['Rgs20', 'Oprk1', 'St18', 'Gm26901'], are not valid obs/ var names or indices."". sc.pl.matrixplot(adata_2, adata_2.var_names[0:4], groupby='celltype') #same error. sc.pl.tsne(adata_2, color=adata_2.var_names[0:4]) #KeyError: 'Rgs20' . ```. Any help would be much appreciated!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:39,usability,user,user-images,39,@giovp `adata_2.var`. ![image](https://user-images.githubusercontent.com/60929679/95211155-829dfb80-07ec-11eb-819b-a4102b762a59.png).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:297,deployability,version,version,297,"mmh, I just tried with a random anndata and it works for me:. ```python. sc.pl.dotplot(adata, adata.var_names[0:4], groupby=""leiden"", color_map=""Reds""). ```. ![image](https://user-images.githubusercontent.com/25887487/95213441-1c66a800-07ef-11eb-8b4b-f754d14eb12c.png). ```python. print(f""anndata version: {anndata.__version__}, scanpy version: {sc.__version__}""). >>> anndata version: 0.7.4, scanpy version: 1.6.0. ```. can you check your versions? Also, can you print shape of `adata.raw` and `adata.X`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:336,deployability,version,version,336,"mmh, I just tried with a random anndata and it works for me:. ```python. sc.pl.dotplot(adata, adata.var_names[0:4], groupby=""leiden"", color_map=""Reds""). ```. ![image](https://user-images.githubusercontent.com/25887487/95213441-1c66a800-07ef-11eb-8b4b-f754d14eb12c.png). ```python. print(f""anndata version: {anndata.__version__}, scanpy version: {sc.__version__}""). >>> anndata version: 0.7.4, scanpy version: 1.6.0. ```. can you check your versions? Also, can you print shape of `adata.raw` and `adata.X`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:377,deployability,version,version,377,"mmh, I just tried with a random anndata and it works for me:. ```python. sc.pl.dotplot(adata, adata.var_names[0:4], groupby=""leiden"", color_map=""Reds""). ```. ![image](https://user-images.githubusercontent.com/25887487/95213441-1c66a800-07ef-11eb-8b4b-f754d14eb12c.png). ```python. print(f""anndata version: {anndata.__version__}, scanpy version: {sc.__version__}""). >>> anndata version: 0.7.4, scanpy version: 1.6.0. ```. can you check your versions? Also, can you print shape of `adata.raw` and `adata.X`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:400,deployability,version,version,400,"mmh, I just tried with a random anndata and it works for me:. ```python. sc.pl.dotplot(adata, adata.var_names[0:4], groupby=""leiden"", color_map=""Reds""). ```. ![image](https://user-images.githubusercontent.com/25887487/95213441-1c66a800-07ef-11eb-8b4b-f754d14eb12c.png). ```python. print(f""anndata version: {anndata.__version__}, scanpy version: {sc.__version__}""). >>> anndata version: 0.7.4, scanpy version: 1.6.0. ```. can you check your versions? Also, can you print shape of `adata.raw` and `adata.X`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:440,deployability,version,versions,440,"mmh, I just tried with a random anndata and it works for me:. ```python. sc.pl.dotplot(adata, adata.var_names[0:4], groupby=""leiden"", color_map=""Reds""). ```. ![image](https://user-images.githubusercontent.com/25887487/95213441-1c66a800-07ef-11eb-8b4b-f754d14eb12c.png). ```python. print(f""anndata version: {anndata.__version__}, scanpy version: {sc.__version__}""). >>> anndata version: 0.7.4, scanpy version: 1.6.0. ```. can you check your versions? Also, can you print shape of `adata.raw` and `adata.X`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:297,integrability,version,version,297,"mmh, I just tried with a random anndata and it works for me:. ```python. sc.pl.dotplot(adata, adata.var_names[0:4], groupby=""leiden"", color_map=""Reds""). ```. ![image](https://user-images.githubusercontent.com/25887487/95213441-1c66a800-07ef-11eb-8b4b-f754d14eb12c.png). ```python. print(f""anndata version: {anndata.__version__}, scanpy version: {sc.__version__}""). >>> anndata version: 0.7.4, scanpy version: 1.6.0. ```. can you check your versions? Also, can you print shape of `adata.raw` and `adata.X`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:336,integrability,version,version,336,"mmh, I just tried with a random anndata and it works for me:. ```python. sc.pl.dotplot(adata, adata.var_names[0:4], groupby=""leiden"", color_map=""Reds""). ```. ![image](https://user-images.githubusercontent.com/25887487/95213441-1c66a800-07ef-11eb-8b4b-f754d14eb12c.png). ```python. print(f""anndata version: {anndata.__version__}, scanpy version: {sc.__version__}""). >>> anndata version: 0.7.4, scanpy version: 1.6.0. ```. can you check your versions? Also, can you print shape of `adata.raw` and `adata.X`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:377,integrability,version,version,377,"mmh, I just tried with a random anndata and it works for me:. ```python. sc.pl.dotplot(adata, adata.var_names[0:4], groupby=""leiden"", color_map=""Reds""). ```. ![image](https://user-images.githubusercontent.com/25887487/95213441-1c66a800-07ef-11eb-8b4b-f754d14eb12c.png). ```python. print(f""anndata version: {anndata.__version__}, scanpy version: {sc.__version__}""). >>> anndata version: 0.7.4, scanpy version: 1.6.0. ```. can you check your versions? Also, can you print shape of `adata.raw` and `adata.X`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:400,integrability,version,version,400,"mmh, I just tried with a random anndata and it works for me:. ```python. sc.pl.dotplot(adata, adata.var_names[0:4], groupby=""leiden"", color_map=""Reds""). ```. ![image](https://user-images.githubusercontent.com/25887487/95213441-1c66a800-07ef-11eb-8b4b-f754d14eb12c.png). ```python. print(f""anndata version: {anndata.__version__}, scanpy version: {sc.__version__}""). >>> anndata version: 0.7.4, scanpy version: 1.6.0. ```. can you check your versions? Also, can you print shape of `adata.raw` and `adata.X`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:440,integrability,version,versions,440,"mmh, I just tried with a random anndata and it works for me:. ```python. sc.pl.dotplot(adata, adata.var_names[0:4], groupby=""leiden"", color_map=""Reds""). ```. ![image](https://user-images.githubusercontent.com/25887487/95213441-1c66a800-07ef-11eb-8b4b-f754d14eb12c.png). ```python. print(f""anndata version: {anndata.__version__}, scanpy version: {sc.__version__}""). >>> anndata version: 0.7.4, scanpy version: 1.6.0. ```. can you check your versions? Also, can you print shape of `adata.raw` and `adata.X`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:297,modifiability,version,version,297,"mmh, I just tried with a random anndata and it works for me:. ```python. sc.pl.dotplot(adata, adata.var_names[0:4], groupby=""leiden"", color_map=""Reds""). ```. ![image](https://user-images.githubusercontent.com/25887487/95213441-1c66a800-07ef-11eb-8b4b-f754d14eb12c.png). ```python. print(f""anndata version: {anndata.__version__}, scanpy version: {sc.__version__}""). >>> anndata version: 0.7.4, scanpy version: 1.6.0. ```. can you check your versions? Also, can you print shape of `adata.raw` and `adata.X`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:336,modifiability,version,version,336,"mmh, I just tried with a random anndata and it works for me:. ```python. sc.pl.dotplot(adata, adata.var_names[0:4], groupby=""leiden"", color_map=""Reds""). ```. ![image](https://user-images.githubusercontent.com/25887487/95213441-1c66a800-07ef-11eb-8b4b-f754d14eb12c.png). ```python. print(f""anndata version: {anndata.__version__}, scanpy version: {sc.__version__}""). >>> anndata version: 0.7.4, scanpy version: 1.6.0. ```. can you check your versions? Also, can you print shape of `adata.raw` and `adata.X`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:377,modifiability,version,version,377,"mmh, I just tried with a random anndata and it works for me:. ```python. sc.pl.dotplot(adata, adata.var_names[0:4], groupby=""leiden"", color_map=""Reds""). ```. ![image](https://user-images.githubusercontent.com/25887487/95213441-1c66a800-07ef-11eb-8b4b-f754d14eb12c.png). ```python. print(f""anndata version: {anndata.__version__}, scanpy version: {sc.__version__}""). >>> anndata version: 0.7.4, scanpy version: 1.6.0. ```. can you check your versions? Also, can you print shape of `adata.raw` and `adata.X`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:400,modifiability,version,version,400,"mmh, I just tried with a random anndata and it works for me:. ```python. sc.pl.dotplot(adata, adata.var_names[0:4], groupby=""leiden"", color_map=""Reds""). ```. ![image](https://user-images.githubusercontent.com/25887487/95213441-1c66a800-07ef-11eb-8b4b-f754d14eb12c.png). ```python. print(f""anndata version: {anndata.__version__}, scanpy version: {sc.__version__}""). >>> anndata version: 0.7.4, scanpy version: 1.6.0. ```. can you check your versions? Also, can you print shape of `adata.raw` and `adata.X`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:440,modifiability,version,versions,440,"mmh, I just tried with a random anndata and it works for me:. ```python. sc.pl.dotplot(adata, adata.var_names[0:4], groupby=""leiden"", color_map=""Reds""). ```. ![image](https://user-images.githubusercontent.com/25887487/95213441-1c66a800-07ef-11eb-8b4b-f754d14eb12c.png). ```python. print(f""anndata version: {anndata.__version__}, scanpy version: {sc.__version__}""). >>> anndata version: 0.7.4, scanpy version: 1.6.0. ```. can you check your versions? Also, can you print shape of `adata.raw` and `adata.X`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:175,usability,user,user-images,175,"mmh, I just tried with a random anndata and it works for me:. ```python. sc.pl.dotplot(adata, adata.var_names[0:4], groupby=""leiden"", color_map=""Reds""). ```. ![image](https://user-images.githubusercontent.com/25887487/95213441-1c66a800-07ef-11eb-8b4b-f754d14eb12c.png). ```python. print(f""anndata version: {anndata.__version__}, scanpy version: {sc.__version__}""). >>> anndata version: 0.7.4, scanpy version: 1.6.0. ```. can you check your versions? Also, can you print shape of `adata.raw` and `adata.X`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:8,deployability,log,logging,8,"```. sc.logging.print_versions(). scanpy==1.4.6 anndata==0.7.1 umap==0.3.10 numpy==1.18.1 scipy==1.4.1 pandas==1.0.1 scikit-learn==0.22.1 statsmodels==0.11.1 python-igraph==0.8.0 louvain==0.6.1. ```. ```. adata_2.raw.shape. > (5558, 2000). adata_2.X.shape. > (5558, 2000). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:8,safety,log,logging,8,"```. sc.logging.print_versions(). scanpy==1.4.6 anndata==0.7.1 umap==0.3.10 numpy==1.18.1 scipy==1.4.1 pandas==1.0.1 scikit-learn==0.22.1 statsmodels==0.11.1 python-igraph==0.8.0 louvain==0.6.1. ```. ```. adata_2.raw.shape. > (5558, 2000). adata_2.X.shape. > (5558, 2000). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:8,security,log,logging,8,"```. sc.logging.print_versions(). scanpy==1.4.6 anndata==0.7.1 umap==0.3.10 numpy==1.18.1 scipy==1.4.1 pandas==1.0.1 scikit-learn==0.22.1 statsmodels==0.11.1 python-igraph==0.8.0 louvain==0.6.1. ```. ```. adata_2.raw.shape. > (5558, 2000). adata_2.X.shape. > (5558, 2000). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:8,testability,log,logging,8,"```. sc.logging.print_versions(). scanpy==1.4.6 anndata==0.7.1 umap==0.3.10 numpy==1.18.1 scipy==1.4.1 pandas==1.0.1 scikit-learn==0.22.1 statsmodels==0.11.1 python-igraph==0.8.0 louvain==0.6.1. ```. ```. adata_2.raw.shape. > (5558, 2000). adata_2.X.shape. > (5558, 2000). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:124,usability,learn,learn,124,"```. sc.logging.print_versions(). scanpy==1.4.6 anndata==0.7.1 umap==0.3.10 numpy==1.18.1 scipy==1.4.1 pandas==1.0.1 scikit-learn==0.22.1 statsmodels==0.11.1 python-igraph==0.8.0 louvain==0.6.1. ```. ```. adata_2.raw.shape. > (5558, 2000). adata_2.X.shape. > (5558, 2000). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:16,deployability,updat,updating,16,"ok, do you mind updating both scanpy and anndata and try again? There has been a major refactoring of plotting functions in 1.6.0 and it could address your issue",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:87,modifiability,refact,refactoring,87,"ok, do you mind updating both scanpy and anndata and try again? There has been a major refactoring of plotting functions in 1.6.0 and it could address your issue",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:87,performance,refactor,refactoring,87,"ok, do you mind updating both scanpy and anndata and try again? There has been a major refactoring of plotting functions in 1.6.0 and it could address your issue",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:16,safety,updat,updating,16,"ok, do you mind updating both scanpy and anndata and try again? There has been a major refactoring of plotting functions in 1.6.0 and it could address your issue",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:16,security,updat,updating,16,"ok, do you mind updating both scanpy and anndata and try again? There has been a major refactoring of plotting functions in 1.6.0 and it could address your issue",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:41,deployability,updat,update,41,mmh can I help you with how to go wrt to update? @bz520251 is this still an issue for you?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:41,safety,updat,update,41,mmh can I help you with how to go wrt to update? @bz520251 is this still an issue for you?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:41,security,updat,update,41,mmh can I help you with how to go wrt to update? @bz520251 is this still an issue for you?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:10,usability,help,help,10,mmh can I help you with how to go wrt to update? @bz520251 is this still an issue for you?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:25,deployability,Updat,Updating,25,I have the same problem. Updating both scanpy and anndata could not solve it.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:25,safety,Updat,Updating,25,I have the same problem. Updating both scanpy and anndata could not solve it.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:25,security,Updat,Updating,25,I have the same problem. Updating both scanpy and anndata could not solve it.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:206,availability,error,error,206,"@Chenyanjuan1993 thank you for reporting this. I think it's some typing problem with the var_names index, and so possibly regarding your anndata. Can you share the list of commands that you run before that error? Like do you concat/merge? Also, could you run that with this anndata `adata = scanpy.datasets.pbmc3k()`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:154,interoperability,share,share,154,"@Chenyanjuan1993 thank you for reporting this. I think it's some typing problem with the var_names index, and so possibly regarding your anndata. Can you share the list of commands that you run before that error? Like do you concat/merge? Also, could you run that with this anndata `adata = scanpy.datasets.pbmc3k()`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:206,performance,error,error,206,"@Chenyanjuan1993 thank you for reporting this. I think it's some typing problem with the var_names index, and so possibly regarding your anndata. Can you share the list of commands that you run before that error? Like do you concat/merge? Also, could you run that with this anndata `adata = scanpy.datasets.pbmc3k()`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:206,safety,error,error,206,"@Chenyanjuan1993 thank you for reporting this. I think it's some typing problem with the var_names index, and so possibly regarding your anndata. Can you share the list of commands that you run before that error? Like do you concat/merge? Also, could you run that with this anndata `adata = scanpy.datasets.pbmc3k()`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:172,usability,command,commands,172,"@Chenyanjuan1993 thank you for reporting this. I think it's some typing problem with the var_names index, and so possibly regarding your anndata. Can you share the list of commands that you run before that error? Like do you concat/merge? Also, could you run that with this anndata `adata = scanpy.datasets.pbmc3k()`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:206,usability,error,error,206,"@Chenyanjuan1993 thank you for reporting this. I think it's some typing problem with the var_names index, and so possibly regarding your anndata. Can you share the list of commands that you run before that error? Like do you concat/merge? Also, could you run that with this anndata `adata = scanpy.datasets.pbmc3k()`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:33,deployability,updat,updated,33,"I am having this same problem. I updated:. anndata 0.7.5. scanpy 1.6.0. It did not fix the problem for me. . I can try with the pbmc data set, what should I use for the groupby?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:33,safety,updat,updated,33,"I am having this same problem. I updated:. anndata 0.7.5. scanpy 1.6.0. It did not fix the problem for me. . I can try with the pbmc data set, what should I use for the groupby?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:33,security,updat,updated,33,"I am having this same problem. I updated:. anndata 0.7.5. scanpy 1.6.0. It did not fix the problem for me. . I can try with the pbmc data set, what should I use for the groupby?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:265,availability,avail,available,265,"@liliblu `""louvain""` would work. @kleurless, sorry for such a late reponse to this! If you are still having this problem, does your `adata_2` have `.raw` set? `adata.raw.var_names` ca be different than `adata.var_names`, but is is used by default for plotting when available. Does your second call work with `sc.pl.dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds', use_raw=False)`? If this is the issue, we should at least have a more clear error in the next release (#1583).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:472,availability,error,error,472,"@liliblu `""louvain""` would work. @kleurless, sorry for such a late reponse to this! If you are still having this problem, does your `adata_2` have `.raw` set? `adata.raw.var_names` ca be different than `adata.var_names`, but is is used by default for plotting when available. Does your second call work with `sc.pl.dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds', use_raw=False)`? If this is the issue, we should at least have a more clear error in the next release (#1583).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:490,deployability,releas,release,490,"@liliblu `""louvain""` would work. @kleurless, sorry for such a late reponse to this! If you are still having this problem, does your `adata_2` have `.raw` set? `adata.raw.var_names` ca be different than `adata.var_names`, but is is used by default for plotting when available. Does your second call work with `sc.pl.dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds', use_raw=False)`? If this is the issue, we should at least have a more clear error in the next release (#1583).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:472,performance,error,error,472,"@liliblu `""louvain""` would work. @kleurless, sorry for such a late reponse to this! If you are still having this problem, does your `adata_2` have `.raw` set? `adata.raw.var_names` ca be different than `adata.var_names`, but is is used by default for plotting when available. Does your second call work with `sc.pl.dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds', use_raw=False)`? If this is the issue, we should at least have a more clear error in the next release (#1583).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:122,reliability,doe,does,122,"@liliblu `""louvain""` would work. @kleurless, sorry for such a late reponse to this! If you are still having this problem, does your `adata_2` have `.raw` set? `adata.raw.var_names` ca be different than `adata.var_names`, but is is used by default for plotting when available. Does your second call work with `sc.pl.dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds', use_raw=False)`? If this is the issue, we should at least have a more clear error in the next release (#1583).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:265,reliability,availab,available,265,"@liliblu `""louvain""` would work. @kleurless, sorry for such a late reponse to this! If you are still having this problem, does your `adata_2` have `.raw` set? `adata.raw.var_names` ca be different than `adata.var_names`, but is is used by default for plotting when available. Does your second call work with `sc.pl.dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds', use_raw=False)`? If this is the issue, we should at least have a more clear error in the next release (#1583).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:276,reliability,Doe,Does,276,"@liliblu `""louvain""` would work. @kleurless, sorry for such a late reponse to this! If you are still having this problem, does your `adata_2` have `.raw` set? `adata.raw.var_names` ca be different than `adata.var_names`, but is is used by default for plotting when available. Does your second call work with `sc.pl.dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds', use_raw=False)`? If this is the issue, we should at least have a more clear error in the next release (#1583).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:265,safety,avail,available,265,"@liliblu `""louvain""` would work. @kleurless, sorry for such a late reponse to this! If you are still having this problem, does your `adata_2` have `.raw` set? `adata.raw.var_names` ca be different than `adata.var_names`, but is is used by default for plotting when available. Does your second call work with `sc.pl.dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds', use_raw=False)`? If this is the issue, we should at least have a more clear error in the next release (#1583).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:472,safety,error,error,472,"@liliblu `""louvain""` would work. @kleurless, sorry for such a late reponse to this! If you are still having this problem, does your `adata_2` have `.raw` set? `adata.raw.var_names` ca be different than `adata.var_names`, but is is used by default for plotting when available. Does your second call work with `sc.pl.dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds', use_raw=False)`? If this is the issue, we should at least have a more clear error in the next release (#1583).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:265,security,availab,available,265,"@liliblu `""louvain""` would work. @kleurless, sorry for such a late reponse to this! If you are still having this problem, does your `adata_2` have `.raw` set? `adata.raw.var_names` ca be different than `adata.var_names`, but is is used by default for plotting when available. Does your second call work with `sc.pl.dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds', use_raw=False)`? If this is the issue, we should at least have a more clear error in the next release (#1583).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:466,usability,clear,clear,466,"@liliblu `""louvain""` would work. @kleurless, sorry for such a late reponse to this! If you are still having this problem, does your `adata_2` have `.raw` set? `adata.raw.var_names` ca be different than `adata.var_names`, but is is used by default for plotting when available. Does your second call work with `sc.pl.dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds', use_raw=False)`? If this is the issue, we should at least have a more clear error in the next release (#1583).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:472,usability,error,error,472,"@liliblu `""louvain""` would work. @kleurless, sorry for such a late reponse to this! If you are still having this problem, does your `adata_2` have `.raw` set? `adata.raw.var_names` ca be different than `adata.var_names`, but is is used by default for plotting when available. Does your second call work with `sc.pl.dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds', use_raw=False)`? If this is the issue, we should at least have a more clear error in the next release (#1583).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:82,availability,cluster,clustering,82,"I encountered the same problem. I just followed the tutorial of preprocessing and clustering 3k PBMCs(https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html). I can not figure out why the gene list of the result of sc.tl.rank_genes_groups is different from that of adata.var.index or adata.var_names. And when I tried to use sc.pl.dotplot to get the dotplot, there existed some marker genes chosen based on the result of sc.tl.rank_genes_groups not found in obs / var names or indices, especially when I set use_raw = False. ![](https://user-images.githubusercontent.com/32971171/117529616-0d590a00-b00b-11eb-9a3a-41ff95ec864d.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:82,deployability,cluster,clustering,82,"I encountered the same problem. I just followed the tutorial of preprocessing and clustering 3k PBMCs(https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html). I can not figure out why the gene list of the result of sc.tl.rank_genes_groups is different from that of adata.var.index or adata.var_names. And when I tried to use sc.pl.dotplot to get the dotplot, there existed some marker genes chosen based on the result of sc.tl.rank_genes_groups not found in obs / var names or indices, especially when I set use_raw = False. ![](https://user-images.githubusercontent.com/32971171/117529616-0d590a00-b00b-11eb-9a3a-41ff95ec864d.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:546,usability,user,user-images,546,"I encountered the same problem. I just followed the tutorial of preprocessing and clustering 3k PBMCs(https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html). I can not figure out why the gene list of the result of sc.tl.rank_genes_groups is different from that of adata.var.index or adata.var_names. And when I tried to use sc.pl.dotplot to get the dotplot, there existed some marker genes chosen based on the result of sc.tl.rank_genes_groups not found in obs / var names or indices, especially when I set use_raw = False. ![](https://user-images.githubusercontent.com/32971171/117529616-0d590a00-b00b-11eb-9a3a-41ff95ec864d.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:358,deployability,updat,updates,358,"> Hi, Just wanted to comment that I had this issue. Converted from Seurat to h5ad using SeuratDisk. `adata.raw.var_names` is different than `adata.var_names`. As a result, I couldn't plot since none of my features were found (keys). Using `use_raw=False` worked. Yep, I have the same problem, if you worked on a Seurat-converted h5ad adata. Any solutions or updates on this? Or we have to use useRaw=False????",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:358,safety,updat,updates,358,"> Hi, Just wanted to comment that I had this issue. Converted from Seurat to h5ad using SeuratDisk. `adata.raw.var_names` is different than `adata.var_names`. As a result, I couldn't plot since none of my features were found (keys). Using `use_raw=False` worked. Yep, I have the same problem, if you worked on a Seurat-converted h5ad adata. Any solutions or updates on this? Or we have to use useRaw=False????",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:358,security,updat,updates,358,"> Hi, Just wanted to comment that I had this issue. Converted from Seurat to h5ad using SeuratDisk. `adata.raw.var_names` is different than `adata.var_names`. As a result, I couldn't plot since none of my features were found (keys). Using `use_raw=False` worked. Yep, I have the same problem, if you worked on a Seurat-converted h5ad adata. Any solutions or updates on this? Or we have to use useRaw=False????",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:393,usability,useR,useRaw,393,"> Hi, Just wanted to comment that I had this issue. Converted from Seurat to h5ad using SeuratDisk. `adata.raw.var_names` is different than `adata.var_names`. As a result, I couldn't plot since none of my features were found (keys). Using `use_raw=False` worked. Yep, I have the same problem, if you worked on a Seurat-converted h5ad adata. Any solutions or updates on this? Or we have to use useRaw=False????",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:138,energy efficiency,reduc,reduced,138,"`adata.raw.var_names` will have a different set of variables than `adata.var_names`, see #2018. This is by design, to allow you to have a reduced set of features in a dense matrix in `adata.X`, but have the full dataset in `adata.raw`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:51,modifiability,variab,variables,51,"`adata.raw.var_names` will have a different set of variables than `adata.var_names`, see #2018. This is by design, to allow you to have a reduced set of features in a dense matrix in `adata.X`, but have the full dataset in `adata.raw`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:82,availability,error,error,82,Can this just be inferred under the hood/raise a warning? It's a very frustrating error and not clear at all what the root issue for an end user.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:82,performance,error,error,82,Can this just be inferred under the hood/raise a warning? It's a very frustrating error and not clear at all what the root issue for an end user.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:82,safety,error,error,82,Can this just be inferred under the hood/raise a warning? It's a very frustrating error and not clear at all what the root issue for an end user.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:82,usability,error,error,82,Can this just be inferred under the hood/raise a warning? It's a very frustrating error and not clear at all what the root issue for an end user.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:96,usability,clear,clear,96,Can this just be inferred under the hood/raise a warning? It's a very frustrating error and not clear at all what the root issue for an end user.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:140,usability,user,user,140,Can this just be inferred under the hood/raise a warning? It's a very frustrating error and not clear at all what the root issue for an end user.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:179,deployability,automat,automatically,179,"Maybe this helps someone who encounters this problem as well. . Here is what worked for me: Set the var **indices** of both X and raw and thereby the respective var_names get set automatically. ```python. # load the adata object, converted using SeuratDisk. adata = sc.read_h5ad(object_path). # Set the new variable names by setting the respective indices otherwise gene names are not found. adata.var.index = adata.var.features. adata.raw.var.index = adata.var.features. # convert the cell type label data to type category (otherwise some methods do not work). adata.obs['cell_type'] = adata.obs['cell_type'].astype('category'). ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:207,energy efficiency,load,load,207,"Maybe this helps someone who encounters this problem as well. . Here is what worked for me: Set the var **indices** of both X and raw and thereby the respective var_names get set automatically. ```python. # load the adata object, converted using SeuratDisk. adata = sc.read_h5ad(object_path). # Set the new variable names by setting the respective indices otherwise gene names are not found. adata.var.index = adata.var.features. adata.raw.var.index = adata.var.features. # convert the cell type label data to type category (otherwise some methods do not work). adata.obs['cell_type'] = adata.obs['cell_type'].astype('category'). ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:307,modifiability,variab,variable,307,"Maybe this helps someone who encounters this problem as well. . Here is what worked for me: Set the var **indices** of both X and raw and thereby the respective var_names get set automatically. ```python. # load the adata object, converted using SeuratDisk. adata = sc.read_h5ad(object_path). # Set the new variable names by setting the respective indices otherwise gene names are not found. adata.var.index = adata.var.features. adata.raw.var.index = adata.var.features. # convert the cell type label data to type category (otherwise some methods do not work). adata.obs['cell_type'] = adata.obs['cell_type'].astype('category'). ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:207,performance,load,load,207,"Maybe this helps someone who encounters this problem as well. . Here is what worked for me: Set the var **indices** of both X and raw and thereby the respective var_names get set automatically. ```python. # load the adata object, converted using SeuratDisk. adata = sc.read_h5ad(object_path). # Set the new variable names by setting the respective indices otherwise gene names are not found. adata.var.index = adata.var.features. adata.raw.var.index = adata.var.features. # convert the cell type label data to type category (otherwise some methods do not work). adata.obs['cell_type'] = adata.obs['cell_type'].astype('category'). ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:179,testability,automat,automatically,179,"Maybe this helps someone who encounters this problem as well. . Here is what worked for me: Set the var **indices** of both X and raw and thereby the respective var_names get set automatically. ```python. # load the adata object, converted using SeuratDisk. adata = sc.read_h5ad(object_path). # Set the new variable names by setting the respective indices otherwise gene names are not found. adata.var.index = adata.var.features. adata.raw.var.index = adata.var.features. # convert the cell type label data to type category (otherwise some methods do not work). adata.obs['cell_type'] = adata.obs['cell_type'].astype('category'). ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:11,usability,help,helps,11,"Maybe this helps someone who encounters this problem as well. . Here is what worked for me: Set the var **indices** of both X and raw and thereby the respective var_names get set automatically. ```python. # load the adata object, converted using SeuratDisk. adata = sc.read_h5ad(object_path). # Set the new variable names by setting the respective indices otherwise gene names are not found. adata.var.index = adata.var.features. adata.raw.var.index = adata.var.features. # convert the cell type label data to type category (otherwise some methods do not work). adata.obs['cell_type'] = adata.obs['cell_type'].astype('category'). ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1406:138,interoperability,convers,conversion,138,uhm for me the dimensions were the same and only the `adata.var.features` had the correct gene names stored... But that might stem from a conversion from Seurat to scanpy object (adata) using SeuratDisk.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406
https://github.com/scverse/scanpy/issues/1407:18,security,ident,identical,18,Getting this same identical issue,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1407
https://github.com/scverse/scanpy/issues/1407:68,deployability,updat,update,68,"Also since recently. Used to work like a charm previously, before I update some packages. I guess it has to do with the latter. Any solutions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1407
https://github.com/scverse/scanpy/issues/1407:80,modifiability,pac,packages,80,"Also since recently. Used to work like a charm previously, before I update some packages. I guess it has to do with the latter. Any solutions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1407
https://github.com/scverse/scanpy/issues/1407:68,safety,updat,update,68,"Also since recently. Used to work like a charm previously, before I update some packages. I guess it has to do with the latter. Any solutions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1407
https://github.com/scverse/scanpy/issues/1407:68,security,updat,update,68,"Also since recently. Used to work like a charm previously, before I update some packages. I guess it has to do with the latter. Any solutions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1407
https://github.com/scverse/scanpy/issues/1407:53,availability,Down,Downgrading,53,I guess it has to do with the dependency on `scipy`. Downgrading to a previous setup of packages did the trick for me.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1407
https://github.com/scverse/scanpy/issues/1407:30,deployability,depend,dependency,30,I guess it has to do with the dependency on `scipy`. Downgrading to a previous setup of packages did the trick for me.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1407
https://github.com/scverse/scanpy/issues/1407:30,integrability,depend,dependency,30,I guess it has to do with the dependency on `scipy`. Downgrading to a previous setup of packages did the trick for me.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1407
https://github.com/scverse/scanpy/issues/1407:30,modifiability,depend,dependency,30,I guess it has to do with the dependency on `scipy`. Downgrading to a previous setup of packages did the trick for me.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1407
https://github.com/scverse/scanpy/issues/1407:88,modifiability,pac,packages,88,I guess it has to do with the dependency on `scipy`. Downgrading to a previous setup of packages did the trick for me.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1407
https://github.com/scverse/scanpy/issues/1407:30,safety,depend,dependency,30,I guess it has to do with the dependency on `scipy`. Downgrading to a previous setup of packages did the trick for me.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1407
https://github.com/scverse/scanpy/issues/1407:30,testability,depend,dependency,30,I guess it has to do with the dependency on `scipy`. Downgrading to a previous setup of packages did the trick for me.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1407
https://github.com/scverse/scanpy/issues/1407:11,availability,down,downgrading,11,"Same here, downgrading made everything work",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1407
https://github.com/scverse/scanpy/issues/1407:23,deployability,version,version,23,@prubbens @mat10d what version of scipy worked for you?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1407
https://github.com/scverse/scanpy/issues/1407:23,integrability,version,version,23,@prubbens @mat10d what version of scipy worked for you?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1407
https://github.com/scverse/scanpy/issues/1407:23,modifiability,version,version,23,@prubbens @mat10d what version of scipy worked for you?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1407
https://github.com/scverse/scanpy/issues/1407:33,deployability,depend,dependencies,33,1.4.2. 1.5.X seemed to break the dependencies.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1407
https://github.com/scverse/scanpy/issues/1407:33,integrability,depend,dependencies,33,1.4.2. 1.5.X seemed to break the dependencies.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1407
https://github.com/scverse/scanpy/issues/1407:33,modifiability,depend,dependencies,33,1.4.2. 1.5.X seemed to break the dependencies.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1407
https://github.com/scverse/scanpy/issues/1407:33,safety,depend,dependencies,33,1.4.2. 1.5.X seemed to break the dependencies.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1407
https://github.com/scverse/scanpy/issues/1407:33,testability,depend,dependencies,33,1.4.2. 1.5.X seemed to break the dependencies.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1407
https://github.com/scverse/scanpy/issues/1407:24,availability,error,error,24,Hmmm. I am getting this error with scipy 1.4.1. New install of phenograph so I don't have an older version to roll back to. Hopefully they fix this soon!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1407
https://github.com/scverse/scanpy/issues/1407:52,deployability,instal,install,52,Hmmm. I am getting this error with scipy 1.4.1. New install of phenograph so I don't have an older version to roll back to. Hopefully they fix this soon!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1407
https://github.com/scverse/scanpy/issues/1407:99,deployability,version,version,99,Hmmm. I am getting this error with scipy 1.4.1. New install of phenograph so I don't have an older version to roll back to. Hopefully they fix this soon!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1407
https://github.com/scverse/scanpy/issues/1407:110,deployability,roll,roll,110,Hmmm. I am getting this error with scipy 1.4.1. New install of phenograph so I don't have an older version to roll back to. Hopefully they fix this soon!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1407
https://github.com/scverse/scanpy/issues/1407:99,integrability,version,version,99,Hmmm. I am getting this error with scipy 1.4.1. New install of phenograph so I don't have an older version to roll back to. Hopefully they fix this soon!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1407
https://github.com/scverse/scanpy/issues/1407:99,modifiability,version,version,99,Hmmm. I am getting this error with scipy 1.4.1. New install of phenograph so I don't have an older version to roll back to. Hopefully they fix this soon!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1407
https://github.com/scverse/scanpy/issues/1407:24,performance,error,error,24,Hmmm. I am getting this error with scipy 1.4.1. New install of phenograph so I don't have an older version to roll back to. Hopefully they fix this soon!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1407
https://github.com/scverse/scanpy/issues/1407:24,safety,error,error,24,Hmmm. I am getting this error with scipy 1.4.1. New install of phenograph so I don't have an older version to roll back to. Hopefully they fix this soon!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1407
https://github.com/scverse/scanpy/issues/1407:24,usability,error,error,24,Hmmm. I am getting this error with scipy 1.4.1. New install of phenograph so I don't have an older version to roll back to. Hopefully they fix this soon!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1407
https://github.com/scverse/scanpy/issues/1407:58,availability,avail,available,58,Phneograph was recently updated and also new wrappers are available in external thanks to @awnimo @Koncopd . Does this work for you @asmariyaz23 ? I will close this but feel free to reopen,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1407
https://github.com/scverse/scanpy/issues/1407:24,deployability,updat,updated,24,Phneograph was recently updated and also new wrappers are available in external thanks to @awnimo @Koncopd . Does this work for you @asmariyaz23 ? I will close this but feel free to reopen,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1407
https://github.com/scverse/scanpy/issues/1407:45,integrability,wrap,wrappers,45,Phneograph was recently updated and also new wrappers are available in external thanks to @awnimo @Koncopd . Does this work for you @asmariyaz23 ? I will close this but feel free to reopen,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1407
https://github.com/scverse/scanpy/issues/1407:45,interoperability,wrapper,wrappers,45,Phneograph was recently updated and also new wrappers are available in external thanks to @awnimo @Koncopd . Does this work for you @asmariyaz23 ? I will close this but feel free to reopen,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1407
https://github.com/scverse/scanpy/issues/1407:58,reliability,availab,available,58,Phneograph was recently updated and also new wrappers are available in external thanks to @awnimo @Koncopd . Does this work for you @asmariyaz23 ? I will close this but feel free to reopen,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1407
https://github.com/scverse/scanpy/issues/1407:109,reliability,Doe,Does,109,Phneograph was recently updated and also new wrappers are available in external thanks to @awnimo @Koncopd . Does this work for you @asmariyaz23 ? I will close this but feel free to reopen,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1407
https://github.com/scverse/scanpy/issues/1407:24,safety,updat,updated,24,Phneograph was recently updated and also new wrappers are available in external thanks to @awnimo @Koncopd . Does this work for you @asmariyaz23 ? I will close this but feel free to reopen,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1407
https://github.com/scverse/scanpy/issues/1407:58,safety,avail,available,58,Phneograph was recently updated and also new wrappers are available in external thanks to @awnimo @Koncopd . Does this work for you @asmariyaz23 ? I will close this but feel free to reopen,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1407
https://github.com/scverse/scanpy/issues/1407:24,security,updat,updated,24,Phneograph was recently updated and also new wrappers are available in external thanks to @awnimo @Koncopd . Does this work for you @asmariyaz23 ? I will close this but feel free to reopen,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1407
https://github.com/scverse/scanpy/issues/1407:58,security,availab,available,58,Phneograph was recently updated and also new wrappers are available in external thanks to @awnimo @Koncopd . Does this work for you @asmariyaz23 ? I will close this but feel free to reopen,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1407
https://github.com/scverse/scanpy/issues/1407:154,usability,close,close,154,Phneograph was recently updated and also new wrappers are available in external thanks to @awnimo @Koncopd . Does this work for you @asmariyaz23 ? I will close this but feel free to reopen,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1407
https://github.com/scverse/scanpy/issues/1407:27,deployability,instal,installing,27,I solved the problem after installing with this command:. pip install scipy==1.4.1 --use-feature=2020-resolver.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1407
https://github.com/scverse/scanpy/issues/1407:62,deployability,instal,install,62,I solved the problem after installing with this command:. pip install scipy==1.4.1 --use-feature=2020-resolver.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1407
https://github.com/scverse/scanpy/issues/1407:48,usability,command,command,48,I solved the problem after installing with this command:. pip install scipy==1.4.1 --use-feature=2020-resolver.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1407
https://github.com/scverse/scanpy/issues/1407:92,availability,error,error,92,Still have to use scipy 1.4.1 or 1.4.2. There's now 1.6.1 but this also results in the same error...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1407
https://github.com/scverse/scanpy/issues/1407:92,performance,error,error,92,Still have to use scipy 1.4.1 or 1.4.2. There's now 1.6.1 but this also results in the same error...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1407
https://github.com/scverse/scanpy/issues/1407:92,safety,error,error,92,Still have to use scipy 1.4.1 or 1.4.2. There's now 1.6.1 but this also results in the same error...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1407
https://github.com/scverse/scanpy/issues/1407:92,usability,error,error,92,Still have to use scipy 1.4.1 or 1.4.2. There's now 1.6.1 but this also results in the same error...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1407
https://github.com/scverse/scanpy/issues/1407:35,deployability,instal,install,35,I had the same issue using . `pip3 install git+https://github.com/jacoblevine/phenograph.git` that gave version 1.5.2. but it worked using . `pip install Phenograph==1.5.7`,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1407
https://github.com/scverse/scanpy/issues/1407:104,deployability,version,version,104,I had the same issue using . `pip3 install git+https://github.com/jacoblevine/phenograph.git` that gave version 1.5.2. but it worked using . `pip install Phenograph==1.5.7`,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1407
https://github.com/scverse/scanpy/issues/1407:146,deployability,instal,install,146,I had the same issue using . `pip3 install git+https://github.com/jacoblevine/phenograph.git` that gave version 1.5.2. but it worked using . `pip install Phenograph==1.5.7`,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1407
https://github.com/scverse/scanpy/issues/1407:104,integrability,version,version,104,I had the same issue using . `pip3 install git+https://github.com/jacoblevine/phenograph.git` that gave version 1.5.2. but it worked using . `pip install Phenograph==1.5.7`,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1407
https://github.com/scverse/scanpy/issues/1407:104,modifiability,version,version,104,I had the same issue using . `pip3 install git+https://github.com/jacoblevine/phenograph.git` that gave version 1.5.2. but it worked using . `pip install Phenograph==1.5.7`,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1407
https://github.com/scverse/scanpy/issues/1407:70,deployability,version,versions,70,I've Phenograph 1.5.7 scipy 1.7.3 and the issue comes up. Tried older versions same issue plus some incompatibility with Phenograph.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1407
https://github.com/scverse/scanpy/issues/1407:70,integrability,version,versions,70,I've Phenograph 1.5.7 scipy 1.7.3 and the issue comes up. Tried older versions same issue plus some incompatibility with Phenograph.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1407
https://github.com/scverse/scanpy/issues/1407:100,interoperability,incompatib,incompatibility,100,I've Phenograph 1.5.7 scipy 1.7.3 and the issue comes up. Tried older versions same issue plus some incompatibility with Phenograph.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1407
https://github.com/scverse/scanpy/issues/1407:70,modifiability,version,versions,70,I've Phenograph 1.5.7 scipy 1.7.3 and the issue comes up. Tried older versions same issue plus some incompatibility with Phenograph.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1407
https://github.com/scverse/scanpy/issues/1408:68,reliability,doe,doesn,68,"> I renamed the genes.tsv.gz file to features.tsv.gz but that still doesn't fix my problem. You should not be re-naming the file. Scanpy handles CellRanger V3 and V2 outputs, but does so [based on](https://github.com/theislab/scanpy/blob/43379e038c5db3f917f45cc69889de0dcb6caa35/scanpy/readwrite.py#L470) the way the files are named. Perhaps you just need to `gunzip` the file to `genes.tsv`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1408
https://github.com/scverse/scanpy/issues/1408:179,reliability,doe,does,179,"> I renamed the genes.tsv.gz file to features.tsv.gz but that still doesn't fix my problem. You should not be re-naming the file. Scanpy handles CellRanger V3 and V2 outputs, but does so [based on](https://github.com/theislab/scanpy/blob/43379e038c5db3f917f45cc69889de0dcb6caa35/scanpy/readwrite.py#L470) the way the files are named. Perhaps you just need to `gunzip` the file to `genes.tsv`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1408
https://github.com/scverse/scanpy/issues/1408:215,integrability,rout,routinely,215,"No worries. Arguably scanpy should check for common compression formats with V2 (and no compression for V3) while reading the input files, or at least provide a more informative traceback, especially if data on GEO routinely includes these formats",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1408
https://github.com/scverse/scanpy/issues/1408:64,interoperability,format,formats,64,"No worries. Arguably scanpy should check for common compression formats with V2 (and no compression for V3) while reading the input files, or at least provide a more informative traceback, especially if data on GEO routinely includes these formats",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1408
https://github.com/scverse/scanpy/issues/1408:240,interoperability,format,formats,240,"No worries. Arguably scanpy should check for common compression formats with V2 (and no compression for V3) while reading the input files, or at least provide a more informative traceback, especially if data on GEO routinely includes these formats",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1408
https://github.com/scverse/scanpy/issues/1408:126,safety,input,input,126,"No worries. Arguably scanpy should check for common compression formats with V2 (and no compression for V3) while reading the input files, or at least provide a more informative traceback, especially if data on GEO routinely includes these formats",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1408
https://github.com/scverse/scanpy/issues/1408:178,testability,trace,traceback,178,"No worries. Arguably scanpy should check for common compression formats with V2 (and no compression for V3) while reading the input files, or at least provide a more informative traceback, especially if data on GEO routinely includes these formats",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1408
https://github.com/scverse/scanpy/issues/1408:126,usability,input,input,126,"No worries. Arguably scanpy should check for common compression formats with V2 (and no compression for V3) while reading the input files, or at least provide a more informative traceback, especially if data on GEO routinely includes these formats",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1408
https://github.com/scverse/scanpy/issues/1409:1850,availability,error,error,1850,"rkx 2.5. numba 0.51.2. numexpr 2.7.1. numpy 1.19.2. packaging 20.4. pandas 1.0.1. parso 0.5.2. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prometheus_client NA. prompt_toolkit 3.0.7. ptyprocess 0.6.0. pvectorc NA. pygments 2.7.0. pylab NA. pyparsing 2.4.7. pyrsistent NA. pytz 2020.1. requests 2.23.0. requests_cache 0.5.2. scanpy 1.6.0. scipy 1.5.2. seaborn 0.11.0. send2trash NA. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.23.2. socks 1.7.1. soupsieve 2.0.1. statsmodels 0.12.0. storemagic NA. tables 3.6.1. terminado 0.8.3. tornado 6.0.4. traitlets 5.0.4. urllib3 1.25.10. wcwidth 0.2.5. wrapt 1.12.1. xlsxwriter 1.3.3. zmq 19.0.2. -----. IPython 7.18.1. jupyter_client 6.1.7. jupyter_core 4.6.3. jupyterlab 2.2.8. notebook 6.1.4. -----. Python 3.8.5 | packaged by conda-forge | (default, Aug 29 2020, 01:22:49) [GCC 7.5.0]. Linux-4.4.0-142-generic-x86_64-with-glibc2.10. 64 logical CPU cores, x86_64. -----. Session information updated at 2020-09-16 11:03. ```. Here is the error message:. ```. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-37-b22ada65a1cd> in <module>. 1 # Create Concatenated anndata object for all timepoints. 2 #alldays = e125.concatenate(e135, e145, e155, uns_merge=""unique""). ----> 3 alldays = e125.concatenate(e135). ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/anndata.py in concatenate(self, join, batch_key, batch_categories, uns_merge, index_unique, fill_value, *adatas). 1696 all_adatas = (self,) + tuple(adatas). 1697 . -> 1698 out = concat(. 1699 all_adatas,. 1700 axis=0,. ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in concat(adatas, axis, join, merge, uns_merge, label, keys, index_unique, fill_value, pairwise). 799 [dim_indices(a, axis=1 - axis) for a in adatas], join=join. 800 ). --> 801 reindexers = [. 802 gen_reindexer(alt_indices, dim_indices(a, axis=1 - a",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:3748,availability,toler,tolerance,3748,"l last). <ipython-input-37-b22ada65a1cd> in <module>. 1 # Create Concatenated anndata object for all timepoints. 2 #alldays = e125.concatenate(e135, e145, e155, uns_merge=""unique""). ----> 3 alldays = e125.concatenate(e135). ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/anndata.py in concatenate(self, join, batch_key, batch_categories, uns_merge, index_unique, fill_value, *adatas). 1696 all_adatas = (self,) + tuple(adatas). 1697 . -> 1698 out = concat(. 1699 all_adatas,. 1700 axis=0,. ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in concat(adatas, axis, join, merge, uns_merge, label, keys, index_unique, fill_value, pairwise). 799 [dim_indices(a, axis=1 - axis) for a in adatas], join=join. 800 ). --> 801 reindexers = [. 802 gen_reindexer(alt_indices, dim_indices(a, axis=1 - axis)) for a in adatas. 803 ]. ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in <listcomp>(.0). 800 ). 801 reindexers = [. --> 802 gen_reindexer(alt_indices, dim_indices(a, axis=1 - axis)) for a in adatas. 803 ]. 804 . ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in gen_reindexer(new_var, cur_var). 393 [1., 0., 0.]], dtype=float32). 394 """""". --> 395 return Reindexer(cur_var, new_var). 396 . 397 . ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in __init__(self, old_idx, new_idx). 265 self.no_change = new_idx.equals(old_idx). 266 . --> 267 new_pos = new_idx.get_indexer(old_idx). 268 old_pos = np.arange(len(new_pos)). 269 . ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance). 2731 . 2732 if not self.is_unique:. -> 2733 raise InvalidIndexError(. 2734 ""Reindexing only valid with uniquely valued Index objects"". 2735 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects. ```. Thanks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:93,deployability,version,version,93,"Did you get an idea how to solve this problem? I encountered the same problem. . Here is the version info:. ```. WARNING: If you miss a compact list, please try `print_header`! -----. anndata 0.7.4. scanpy 1.6.0. sinfo 0.3.1. -----. OpenSSL 19.1.0. PIL 7.2.0. anndata 0.7.4. appdirs 1.4.4. attr 20.2.0. backcall 0.2.0. bioservices 1.7.9. brotli NA. bs4 4.9.1. certifi 2020.06.20. cffi 1.14.1. chardet 3.0.4. colorama 0.4.3. colorlog NA. cryptography 3.1. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. decorator 4.4.2. easydev 0.10.1. get_version 2.1. gseapy 0.10.1. h5py 2.10.0. idna 2.10. ipykernel 5.3.4. ipython_genutils 0.2.0. jedi 0.15.2. jinja2 2.11.2. joblib 0.16.0. jsonschema 3.2.0. kiwisolver 1.2.0. legacy_api_wrap 1.2. llvmlite 0.34.0. lxml 4.5.2. markupsafe 1.1.1. matplotlib 3.3.2. mpl_toolkits NA. natsort 7.0.1. nbformat 5.0.7. networkx 2.5. numba 0.51.2. numexpr 2.7.1. numpy 1.19.2. packaging 20.4. pandas 1.0.1. parso 0.5.2. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prometheus_client NA. prompt_toolkit 3.0.7. ptyprocess 0.6.0. pvectorc NA. pygments 2.7.0. pylab NA. pyparsing 2.4.7. pyrsistent NA. pytz 2020.1. requests 2.23.0. requests_cache 0.5.2. scanpy 1.6.0. scipy 1.5.2. seaborn 0.11.0. send2trash NA. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.23.2. socks 1.7.1. soupsieve 2.0.1. statsmodels 0.12.0. storemagic NA. tables 3.6.1. terminado 0.8.3. tornado 6.0.4. traitlets 5.0.4. urllib3 1.25.10. wcwidth 0.2.5. wrapt 1.12.1. xlsxwriter 1.3.3. zmq 19.0.2. -----. IPython 7.18.1. jupyter_client 6.1.7. jupyter_core 4.6.3. jupyterlab 2.2.8. notebook 6.1.4. -----. Python 3.8.5 | packaged by conda-forge | (default, Aug 29 2020, 01:22:49) [GCC 7.5.0]. Linux-4.4.0-142-generic-x86_64-with-glibc2.10. 64 logical CPU cores, x86_64. -----. Session information updated at 2020-09-16 11:03. ```. Here is the error message:. ```. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:1750,deployability,log,logical,1750,"ml 4.5.2. markupsafe 1.1.1. matplotlib 3.3.2. mpl_toolkits NA. natsort 7.0.1. nbformat 5.0.7. networkx 2.5. numba 0.51.2. numexpr 2.7.1. numpy 1.19.2. packaging 20.4. pandas 1.0.1. parso 0.5.2. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prometheus_client NA. prompt_toolkit 3.0.7. ptyprocess 0.6.0. pvectorc NA. pygments 2.7.0. pylab NA. pyparsing 2.4.7. pyrsistent NA. pytz 2020.1. requests 2.23.0. requests_cache 0.5.2. scanpy 1.6.0. scipy 1.5.2. seaborn 0.11.0. send2trash NA. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.23.2. socks 1.7.1. soupsieve 2.0.1. statsmodels 0.12.0. storemagic NA. tables 3.6.1. terminado 0.8.3. tornado 6.0.4. traitlets 5.0.4. urllib3 1.25.10. wcwidth 0.2.5. wrapt 1.12.1. xlsxwriter 1.3.3. zmq 19.0.2. -----. IPython 7.18.1. jupyter_client 6.1.7. jupyter_core 4.6.3. jupyterlab 2.2.8. notebook 6.1.4. -----. Python 3.8.5 | packaged by conda-forge | (default, Aug 29 2020, 01:22:49) [GCC 7.5.0]. Linux-4.4.0-142-generic-x86_64-with-glibc2.10. 64 logical CPU cores, x86_64. -----. Session information updated at 2020-09-16 11:03. ```. Here is the error message:. ```. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-37-b22ada65a1cd> in <module>. 1 # Create Concatenated anndata object for all timepoints. 2 #alldays = e125.concatenate(e135, e145, e155, uns_merge=""unique""). ----> 3 alldays = e125.concatenate(e135). ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/anndata.py in concatenate(self, join, batch_key, batch_categories, uns_merge, index_unique, fill_value, *adatas). 1696 all_adatas = (self,) + tuple(adatas). 1697 . -> 1698 out = concat(. 1699 all_adatas,. 1700 axis=0,. ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in concat(adatas, axis, join, merge, uns_merge, label, keys, index_unique, fill_value, pairwise). 799 [dim_indices(a, axis=1 - axis) for a in adatas],",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:1804,deployability,updat,updated,1804,"kits NA. natsort 7.0.1. nbformat 5.0.7. networkx 2.5. numba 0.51.2. numexpr 2.7.1. numpy 1.19.2. packaging 20.4. pandas 1.0.1. parso 0.5.2. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prometheus_client NA. prompt_toolkit 3.0.7. ptyprocess 0.6.0. pvectorc NA. pygments 2.7.0. pylab NA. pyparsing 2.4.7. pyrsistent NA. pytz 2020.1. requests 2.23.0. requests_cache 0.5.2. scanpy 1.6.0. scipy 1.5.2. seaborn 0.11.0. send2trash NA. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.23.2. socks 1.7.1. soupsieve 2.0.1. statsmodels 0.12.0. storemagic NA. tables 3.6.1. terminado 0.8.3. tornado 6.0.4. traitlets 5.0.4. urllib3 1.25.10. wcwidth 0.2.5. wrapt 1.12.1. xlsxwriter 1.3.3. zmq 19.0.2. -----. IPython 7.18.1. jupyter_client 6.1.7. jupyter_core 4.6.3. jupyterlab 2.2.8. notebook 6.1.4. -----. Python 3.8.5 | packaged by conda-forge | (default, Aug 29 2020, 01:22:49) [GCC 7.5.0]. Linux-4.4.0-142-generic-x86_64-with-glibc2.10. 64 logical CPU cores, x86_64. -----. Session information updated at 2020-09-16 11:03. ```. Here is the error message:. ```. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-37-b22ada65a1cd> in <module>. 1 # Create Concatenated anndata object for all timepoints. 2 #alldays = e125.concatenate(e135, e145, e155, uns_merge=""unique""). ----> 3 alldays = e125.concatenate(e135). ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/anndata.py in concatenate(self, join, batch_key, batch_categories, uns_merge, index_unique, fill_value, *adatas). 1696 all_adatas = (self,) + tuple(adatas). 1697 . -> 1698 out = concat(. 1699 all_adatas,. 1700 axis=0,. ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in concat(adatas, axis, join, merge, uns_merge, label, keys, index_unique, fill_value, pairwise). 799 [dim_indices(a, axis=1 - axis) for a in adatas], join=join. 800 ). --> 801 reindexers = [. 802 gen_rei",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:2037,deployability,modul,module,2037,".7. ptyprocess 0.6.0. pvectorc NA. pygments 2.7.0. pylab NA. pyparsing 2.4.7. pyrsistent NA. pytz 2020.1. requests 2.23.0. requests_cache 0.5.2. scanpy 1.6.0. scipy 1.5.2. seaborn 0.11.0. send2trash NA. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.23.2. socks 1.7.1. soupsieve 2.0.1. statsmodels 0.12.0. storemagic NA. tables 3.6.1. terminado 0.8.3. tornado 6.0.4. traitlets 5.0.4. urllib3 1.25.10. wcwidth 0.2.5. wrapt 1.12.1. xlsxwriter 1.3.3. zmq 19.0.2. -----. IPython 7.18.1. jupyter_client 6.1.7. jupyter_core 4.6.3. jupyterlab 2.2.8. notebook 6.1.4. -----. Python 3.8.5 | packaged by conda-forge | (default, Aug 29 2020, 01:22:49) [GCC 7.5.0]. Linux-4.4.0-142-generic-x86_64-with-glibc2.10. 64 logical CPU cores, x86_64. -----. Session information updated at 2020-09-16 11:03. ```. Here is the error message:. ```. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-37-b22ada65a1cd> in <module>. 1 # Create Concatenated anndata object for all timepoints. 2 #alldays = e125.concatenate(e135, e145, e155, uns_merge=""unique""). ----> 3 alldays = e125.concatenate(e135). ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/anndata.py in concatenate(self, join, batch_key, batch_categories, uns_merge, index_unique, fill_value, *adatas). 1696 all_adatas = (self,) + tuple(adatas). 1697 . -> 1698 out = concat(. 1699 all_adatas,. 1700 axis=0,. ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in concat(adatas, axis, join, merge, uns_merge, label, keys, index_unique, fill_value, pairwise). 799 [dim_indices(a, axis=1 - axis) for a in adatas], join=join. 800 ). --> 801 reindexers = [. 802 gen_reindexer(alt_indices, dim_indices(a, axis=1 - axis)) for a in adatas. 803 ]. ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in <listcomp>(.0). 800 ). 801 reindexers = [. --> 802 gen_reindexer(",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:1758,energy efficiency,CPU,CPU,1758,".2. markupsafe 1.1.1. matplotlib 3.3.2. mpl_toolkits NA. natsort 7.0.1. nbformat 5.0.7. networkx 2.5. numba 0.51.2. numexpr 2.7.1. numpy 1.19.2. packaging 20.4. pandas 1.0.1. parso 0.5.2. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prometheus_client NA. prompt_toolkit 3.0.7. ptyprocess 0.6.0. pvectorc NA. pygments 2.7.0. pylab NA. pyparsing 2.4.7. pyrsistent NA. pytz 2020.1. requests 2.23.0. requests_cache 0.5.2. scanpy 1.6.0. scipy 1.5.2. seaborn 0.11.0. send2trash NA. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.23.2. socks 1.7.1. soupsieve 2.0.1. statsmodels 0.12.0. storemagic NA. tables 3.6.1. terminado 0.8.3. tornado 6.0.4. traitlets 5.0.4. urllib3 1.25.10. wcwidth 0.2.5. wrapt 1.12.1. xlsxwriter 1.3.3. zmq 19.0.2. -----. IPython 7.18.1. jupyter_client 6.1.7. jupyter_core 4.6.3. jupyterlab 2.2.8. notebook 6.1.4. -----. Python 3.8.5 | packaged by conda-forge | (default, Aug 29 2020, 01:22:49) [GCC 7.5.0]. Linux-4.4.0-142-generic-x86_64-with-glibc2.10. 64 logical CPU cores, x86_64. -----. Session information updated at 2020-09-16 11:03. ```. Here is the error message:. ```. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-37-b22ada65a1cd> in <module>. 1 # Create Concatenated anndata object for all timepoints. 2 #alldays = e125.concatenate(e135, e145, e155, uns_merge=""unique""). ----> 3 alldays = e125.concatenate(e135). ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/anndata.py in concatenate(self, join, batch_key, batch_categories, uns_merge, index_unique, fill_value, *adatas). 1696 all_adatas = (self,) + tuple(adatas). 1697 . -> 1698 out = concat(. 1699 all_adatas,. 1700 axis=0,. ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in concat(adatas, axis, join, merge, uns_merge, label, keys, index_unique, fill_value, pairwise). 799 [dim_indices(a, axis=1 - axis) for a in adatas], join=",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:1762,energy efficiency,core,cores,1762,"arkupsafe 1.1.1. matplotlib 3.3.2. mpl_toolkits NA. natsort 7.0.1. nbformat 5.0.7. networkx 2.5. numba 0.51.2. numexpr 2.7.1. numpy 1.19.2. packaging 20.4. pandas 1.0.1. parso 0.5.2. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prometheus_client NA. prompt_toolkit 3.0.7. ptyprocess 0.6.0. pvectorc NA. pygments 2.7.0. pylab NA. pyparsing 2.4.7. pyrsistent NA. pytz 2020.1. requests 2.23.0. requests_cache 0.5.2. scanpy 1.6.0. scipy 1.5.2. seaborn 0.11.0. send2trash NA. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.23.2. socks 1.7.1. soupsieve 2.0.1. statsmodels 0.12.0. storemagic NA. tables 3.6.1. terminado 0.8.3. tornado 6.0.4. traitlets 5.0.4. urllib3 1.25.10. wcwidth 0.2.5. wrapt 1.12.1. xlsxwriter 1.3.3. zmq 19.0.2. -----. IPython 7.18.1. jupyter_client 6.1.7. jupyter_core 4.6.3. jupyterlab 2.2.8. notebook 6.1.4. -----. Python 3.8.5 | packaged by conda-forge | (default, Aug 29 2020, 01:22:49) [GCC 7.5.0]. Linux-4.4.0-142-generic-x86_64-with-glibc2.10. 64 logical CPU cores, x86_64. -----. Session information updated at 2020-09-16 11:03. ```. Here is the error message:. ```. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-37-b22ada65a1cd> in <module>. 1 # Create Concatenated anndata object for all timepoints. 2 #alldays = e125.concatenate(e135, e145, e155, uns_merge=""unique""). ----> 3 alldays = e125.concatenate(e135). ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/anndata.py in concatenate(self, join, batch_key, batch_categories, uns_merge, index_unique, fill_value, *adatas). 1696 all_adatas = (self,) + tuple(adatas). 1697 . -> 1698 out = concat(. 1699 all_adatas,. 1700 axis=0,. ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in concat(adatas, axis, join, merge, uns_merge, label, keys, index_unique, fill_value, pairwise). 799 [dim_indices(a, axis=1 - axis) for a in adatas], join=join.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:3683,energy efficiency,core,core,3683,"l last). <ipython-input-37-b22ada65a1cd> in <module>. 1 # Create Concatenated anndata object for all timepoints. 2 #alldays = e125.concatenate(e135, e145, e155, uns_merge=""unique""). ----> 3 alldays = e125.concatenate(e135). ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/anndata.py in concatenate(self, join, batch_key, batch_categories, uns_merge, index_unique, fill_value, *adatas). 1696 all_adatas = (self,) + tuple(adatas). 1697 . -> 1698 out = concat(. 1699 all_adatas,. 1700 axis=0,. ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in concat(adatas, axis, join, merge, uns_merge, label, keys, index_unique, fill_value, pairwise). 799 [dim_indices(a, axis=1 - axis) for a in adatas], join=join. 800 ). --> 801 reindexers = [. 802 gen_reindexer(alt_indices, dim_indices(a, axis=1 - axis)) for a in adatas. 803 ]. ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in <listcomp>(.0). 800 ). 801 reindexers = [. --> 802 gen_reindexer(alt_indices, dim_indices(a, axis=1 - axis)) for a in adatas. 803 ]. 804 . ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in gen_reindexer(new_var, cur_var). 393 [1., 0., 0.]], dtype=float32). 394 """""". --> 395 return Reindexer(cur_var, new_var). 396 . 397 . ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in __init__(self, old_idx, new_idx). 265 self.no_change = new_idx.equals(old_idx). 266 . --> 267 new_pos = new_idx.get_indexer(old_idx). 268 old_pos = np.arange(len(new_pos)). 269 . ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance). 2731 . 2732 if not self.is_unique:. -> 2733 raise InvalidIndexError(. 2734 ""Reindexing only valid with uniquely valued Index objects"". 2735 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects. ```. Thanks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:93,integrability,version,version,93,"Did you get an idea how to solve this problem? I encountered the same problem. . Here is the version info:. ```. WARNING: If you miss a compact list, please try `print_header`! -----. anndata 0.7.4. scanpy 1.6.0. sinfo 0.3.1. -----. OpenSSL 19.1.0. PIL 7.2.0. anndata 0.7.4. appdirs 1.4.4. attr 20.2.0. backcall 0.2.0. bioservices 1.7.9. brotli NA. bs4 4.9.1. certifi 2020.06.20. cffi 1.14.1. chardet 3.0.4. colorama 0.4.3. colorlog NA. cryptography 3.1. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. decorator 4.4.2. easydev 0.10.1. get_version 2.1. gseapy 0.10.1. h5py 2.10.0. idna 2.10. ipykernel 5.3.4. ipython_genutils 0.2.0. jedi 0.15.2. jinja2 2.11.2. joblib 0.16.0. jsonschema 3.2.0. kiwisolver 1.2.0. legacy_api_wrap 1.2. llvmlite 0.34.0. lxml 4.5.2. markupsafe 1.1.1. matplotlib 3.3.2. mpl_toolkits NA. natsort 7.0.1. nbformat 5.0.7. networkx 2.5. numba 0.51.2. numexpr 2.7.1. numpy 1.19.2. packaging 20.4. pandas 1.0.1. parso 0.5.2. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prometheus_client NA. prompt_toolkit 3.0.7. ptyprocess 0.6.0. pvectorc NA. pygments 2.7.0. pylab NA. pyparsing 2.4.7. pyrsistent NA. pytz 2020.1. requests 2.23.0. requests_cache 0.5.2. scanpy 1.6.0. scipy 1.5.2. seaborn 0.11.0. send2trash NA. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.23.2. socks 1.7.1. soupsieve 2.0.1. statsmodels 0.12.0. storemagic NA. tables 3.6.1. terminado 0.8.3. tornado 6.0.4. traitlets 5.0.4. urllib3 1.25.10. wcwidth 0.2.5. wrapt 1.12.1. xlsxwriter 1.3.3. zmq 19.0.2. -----. IPython 7.18.1. jupyter_client 6.1.7. jupyter_core 4.6.3. jupyterlab 2.2.8. notebook 6.1.4. -----. Python 3.8.5 | packaged by conda-forge | (default, Aug 29 2020, 01:22:49) [GCC 7.5.0]. Linux-4.4.0-142-generic-x86_64-with-glibc2.10. 64 logical CPU cores, x86_64. -----. Session information updated at 2020-09-16 11:03. ```. Here is the error message:. ```. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:1463,integrability,wrap,wrapt,1463,".0. cython_runtime NA. dateutil 2.8.1. decorator 4.4.2. easydev 0.10.1. get_version 2.1. gseapy 0.10.1. h5py 2.10.0. idna 2.10. ipykernel 5.3.4. ipython_genutils 0.2.0. jedi 0.15.2. jinja2 2.11.2. joblib 0.16.0. jsonschema 3.2.0. kiwisolver 1.2.0. legacy_api_wrap 1.2. llvmlite 0.34.0. lxml 4.5.2. markupsafe 1.1.1. matplotlib 3.3.2. mpl_toolkits NA. natsort 7.0.1. nbformat 5.0.7. networkx 2.5. numba 0.51.2. numexpr 2.7.1. numpy 1.19.2. packaging 20.4. pandas 1.0.1. parso 0.5.2. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prometheus_client NA. prompt_toolkit 3.0.7. ptyprocess 0.6.0. pvectorc NA. pygments 2.7.0. pylab NA. pyparsing 2.4.7. pyrsistent NA. pytz 2020.1. requests 2.23.0. requests_cache 0.5.2. scanpy 1.6.0. scipy 1.5.2. seaborn 0.11.0. send2trash NA. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.23.2. socks 1.7.1. soupsieve 2.0.1. statsmodels 0.12.0. storemagic NA. tables 3.6.1. terminado 0.8.3. tornado 6.0.4. traitlets 5.0.4. urllib3 1.25.10. wcwidth 0.2.5. wrapt 1.12.1. xlsxwriter 1.3.3. zmq 19.0.2. -----. IPython 7.18.1. jupyter_client 6.1.7. jupyter_core 4.6.3. jupyterlab 2.2.8. notebook 6.1.4. -----. Python 3.8.5 | packaged by conda-forge | (default, Aug 29 2020, 01:22:49) [GCC 7.5.0]. Linux-4.4.0-142-generic-x86_64-with-glibc2.10. 64 logical CPU cores, x86_64. -----. Session information updated at 2020-09-16 11:03. ```. Here is the error message:. ```. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-37-b22ada65a1cd> in <module>. 1 # Create Concatenated anndata object for all timepoints. 2 #alldays = e125.concatenate(e135, e145, e155, uns_merge=""unique""). ----> 3 alldays = e125.concatenate(e135). ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/anndata.py in concatenate(self, join, batch_key, batch_categories, uns_merge, index_unique, fill_value, *adatas). 1696 all_adatas = (self,) + tuple(adatas). 1697 . -> 169",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:1856,integrability,messag,message,1856,". numba 0.51.2. numexpr 2.7.1. numpy 1.19.2. packaging 20.4. pandas 1.0.1. parso 0.5.2. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prometheus_client NA. prompt_toolkit 3.0.7. ptyprocess 0.6.0. pvectorc NA. pygments 2.7.0. pylab NA. pyparsing 2.4.7. pyrsistent NA. pytz 2020.1. requests 2.23.0. requests_cache 0.5.2. scanpy 1.6.0. scipy 1.5.2. seaborn 0.11.0. send2trash NA. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.23.2. socks 1.7.1. soupsieve 2.0.1. statsmodels 0.12.0. storemagic NA. tables 3.6.1. terminado 0.8.3. tornado 6.0.4. traitlets 5.0.4. urllib3 1.25.10. wcwidth 0.2.5. wrapt 1.12.1. xlsxwriter 1.3.3. zmq 19.0.2. -----. IPython 7.18.1. jupyter_client 6.1.7. jupyter_core 4.6.3. jupyterlab 2.2.8. notebook 6.1.4. -----. Python 3.8.5 | packaged by conda-forge | (default, Aug 29 2020, 01:22:49) [GCC 7.5.0]. Linux-4.4.0-142-generic-x86_64-with-glibc2.10. 64 logical CPU cores, x86_64. -----. Session information updated at 2020-09-16 11:03. ```. Here is the error message:. ```. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-37-b22ada65a1cd> in <module>. 1 # Create Concatenated anndata object for all timepoints. 2 #alldays = e125.concatenate(e135, e145, e155, uns_merge=""unique""). ----> 3 alldays = e125.concatenate(e135). ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/anndata.py in concatenate(self, join, batch_key, batch_categories, uns_merge, index_unique, fill_value, *adatas). 1696 all_adatas = (self,) + tuple(adatas). 1697 . -> 1698 out = concat(. 1699 all_adatas,. 1700 axis=0,. ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in concat(adatas, axis, join, merge, uns_merge, label, keys, index_unique, fill_value, pairwise). 799 [dim_indices(a, axis=1 - axis) for a in adatas], join=join. 800 ). --> 801 reindexers = [. 802 gen_reindexer(alt_indices, dim_indices(a, axis=1 - axis)) f",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:1856,interoperability,messag,message,1856,". numba 0.51.2. numexpr 2.7.1. numpy 1.19.2. packaging 20.4. pandas 1.0.1. parso 0.5.2. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prometheus_client NA. prompt_toolkit 3.0.7. ptyprocess 0.6.0. pvectorc NA. pygments 2.7.0. pylab NA. pyparsing 2.4.7. pyrsistent NA. pytz 2020.1. requests 2.23.0. requests_cache 0.5.2. scanpy 1.6.0. scipy 1.5.2. seaborn 0.11.0. send2trash NA. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.23.2. socks 1.7.1. soupsieve 2.0.1. statsmodels 0.12.0. storemagic NA. tables 3.6.1. terminado 0.8.3. tornado 6.0.4. traitlets 5.0.4. urllib3 1.25.10. wcwidth 0.2.5. wrapt 1.12.1. xlsxwriter 1.3.3. zmq 19.0.2. -----. IPython 7.18.1. jupyter_client 6.1.7. jupyter_core 4.6.3. jupyterlab 2.2.8. notebook 6.1.4. -----. Python 3.8.5 | packaged by conda-forge | (default, Aug 29 2020, 01:22:49) [GCC 7.5.0]. Linux-4.4.0-142-generic-x86_64-with-glibc2.10. 64 logical CPU cores, x86_64. -----. Session information updated at 2020-09-16 11:03. ```. Here is the error message:. ```. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-37-b22ada65a1cd> in <module>. 1 # Create Concatenated anndata object for all timepoints. 2 #alldays = e125.concatenate(e135, e145, e155, uns_merge=""unique""). ----> 3 alldays = e125.concatenate(e135). ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/anndata.py in concatenate(self, join, batch_key, batch_categories, uns_merge, index_unique, fill_value, *adatas). 1696 all_adatas = (self,) + tuple(adatas). 1697 . -> 1698 out = concat(. 1699 all_adatas,. 1700 axis=0,. ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in concat(adatas, axis, join, merge, uns_merge, label, keys, index_unique, fill_value, pairwise). 799 [dim_indices(a, axis=1 - axis) for a in adatas], join=join. 800 ). --> 801 reindexers = [. 802 gen_reindexer(alt_indices, dim_indices(a, axis=1 - axis)) f",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:93,modifiability,version,version,93,"Did you get an idea how to solve this problem? I encountered the same problem. . Here is the version info:. ```. WARNING: If you miss a compact list, please try `print_header`! -----. anndata 0.7.4. scanpy 1.6.0. sinfo 0.3.1. -----. OpenSSL 19.1.0. PIL 7.2.0. anndata 0.7.4. appdirs 1.4.4. attr 20.2.0. backcall 0.2.0. bioservices 1.7.9. brotli NA. bs4 4.9.1. certifi 2020.06.20. cffi 1.14.1. chardet 3.0.4. colorama 0.4.3. colorlog NA. cryptography 3.1. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. decorator 4.4.2. easydev 0.10.1. get_version 2.1. gseapy 0.10.1. h5py 2.10.0. idna 2.10. ipykernel 5.3.4. ipython_genutils 0.2.0. jedi 0.15.2. jinja2 2.11.2. joblib 0.16.0. jsonschema 3.2.0. kiwisolver 1.2.0. legacy_api_wrap 1.2. llvmlite 0.34.0. lxml 4.5.2. markupsafe 1.1.1. matplotlib 3.3.2. mpl_toolkits NA. natsort 7.0.1. nbformat 5.0.7. networkx 2.5. numba 0.51.2. numexpr 2.7.1. numpy 1.19.2. packaging 20.4. pandas 1.0.1. parso 0.5.2. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prometheus_client NA. prompt_toolkit 3.0.7. ptyprocess 0.6.0. pvectorc NA. pygments 2.7.0. pylab NA. pyparsing 2.4.7. pyrsistent NA. pytz 2020.1. requests 2.23.0. requests_cache 0.5.2. scanpy 1.6.0. scipy 1.5.2. seaborn 0.11.0. send2trash NA. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.23.2. socks 1.7.1. soupsieve 2.0.1. statsmodels 0.12.0. storemagic NA. tables 3.6.1. terminado 0.8.3. tornado 6.0.4. traitlets 5.0.4. urllib3 1.25.10. wcwidth 0.2.5. wrapt 1.12.1. xlsxwriter 1.3.3. zmq 19.0.2. -----. IPython 7.18.1. jupyter_client 6.1.7. jupyter_core 4.6.3. jupyterlab 2.2.8. notebook 6.1.4. -----. Python 3.8.5 | packaged by conda-forge | (default, Aug 29 2020, 01:22:49) [GCC 7.5.0]. Linux-4.4.0-142-generic-x86_64-with-glibc2.10. 64 logical CPU cores, x86_64. -----. Session information updated at 2020-09-16 11:03. ```. Here is the error message:. ```. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:505,modifiability,deco,decorator,505,"Did you get an idea how to solve this problem? I encountered the same problem. . Here is the version info:. ```. WARNING: If you miss a compact list, please try `print_header`! -----. anndata 0.7.4. scanpy 1.6.0. sinfo 0.3.1. -----. OpenSSL 19.1.0. PIL 7.2.0. anndata 0.7.4. appdirs 1.4.4. attr 20.2.0. backcall 0.2.0. bioservices 1.7.9. brotli NA. bs4 4.9.1. certifi 2020.06.20. cffi 1.14.1. chardet 3.0.4. colorama 0.4.3. colorlog NA. cryptography 3.1. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. decorator 4.4.2. easydev 0.10.1. get_version 2.1. gseapy 0.10.1. h5py 2.10.0. idna 2.10. ipykernel 5.3.4. ipython_genutils 0.2.0. jedi 0.15.2. jinja2 2.11.2. joblib 0.16.0. jsonschema 3.2.0. kiwisolver 1.2.0. legacy_api_wrap 1.2. llvmlite 0.34.0. lxml 4.5.2. markupsafe 1.1.1. matplotlib 3.3.2. mpl_toolkits NA. natsort 7.0.1. nbformat 5.0.7. networkx 2.5. numba 0.51.2. numexpr 2.7.1. numpy 1.19.2. packaging 20.4. pandas 1.0.1. parso 0.5.2. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prometheus_client NA. prompt_toolkit 3.0.7. ptyprocess 0.6.0. pvectorc NA. pygments 2.7.0. pylab NA. pyparsing 2.4.7. pyrsistent NA. pytz 2020.1. requests 2.23.0. requests_cache 0.5.2. scanpy 1.6.0. scipy 1.5.2. seaborn 0.11.0. send2trash NA. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.23.2. socks 1.7.1. soupsieve 2.0.1. statsmodels 0.12.0. storemagic NA. tables 3.6.1. terminado 0.8.3. tornado 6.0.4. traitlets 5.0.4. urllib3 1.25.10. wcwidth 0.2.5. wrapt 1.12.1. xlsxwriter 1.3.3. zmq 19.0.2. -----. IPython 7.18.1. jupyter_client 6.1.7. jupyter_core 4.6.3. jupyterlab 2.2.8. notebook 6.1.4. -----. Python 3.8.5 | packaged by conda-forge | (default, Aug 29 2020, 01:22:49) [GCC 7.5.0]. Linux-4.4.0-142-generic-x86_64-with-glibc2.10. 64 logical CPU cores, x86_64. -----. Session information updated at 2020-09-16 11:03. ```. Here is the error message:. ```. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:905,modifiability,pac,packaging,905,"Did you get an idea how to solve this problem? I encountered the same problem. . Here is the version info:. ```. WARNING: If you miss a compact list, please try `print_header`! -----. anndata 0.7.4. scanpy 1.6.0. sinfo 0.3.1. -----. OpenSSL 19.1.0. PIL 7.2.0. anndata 0.7.4. appdirs 1.4.4. attr 20.2.0. backcall 0.2.0. bioservices 1.7.9. brotli NA. bs4 4.9.1. certifi 2020.06.20. cffi 1.14.1. chardet 3.0.4. colorama 0.4.3. colorlog NA. cryptography 3.1. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. decorator 4.4.2. easydev 0.10.1. get_version 2.1. gseapy 0.10.1. h5py 2.10.0. idna 2.10. ipykernel 5.3.4. ipython_genutils 0.2.0. jedi 0.15.2. jinja2 2.11.2. joblib 0.16.0. jsonschema 3.2.0. kiwisolver 1.2.0. legacy_api_wrap 1.2. llvmlite 0.34.0. lxml 4.5.2. markupsafe 1.1.1. matplotlib 3.3.2. mpl_toolkits NA. natsort 7.0.1. nbformat 5.0.7. networkx 2.5. numba 0.51.2. numexpr 2.7.1. numpy 1.19.2. packaging 20.4. pandas 1.0.1. parso 0.5.2. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prometheus_client NA. prompt_toolkit 3.0.7. ptyprocess 0.6.0. pvectorc NA. pygments 2.7.0. pylab NA. pyparsing 2.4.7. pyrsistent NA. pytz 2020.1. requests 2.23.0. requests_cache 0.5.2. scanpy 1.6.0. scipy 1.5.2. seaborn 0.11.0. send2trash NA. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.23.2. socks 1.7.1. soupsieve 2.0.1. statsmodels 0.12.0. storemagic NA. tables 3.6.1. terminado 0.8.3. tornado 6.0.4. traitlets 5.0.4. urllib3 1.25.10. wcwidth 0.2.5. wrapt 1.12.1. xlsxwriter 1.3.3. zmq 19.0.2. -----. IPython 7.18.1. jupyter_client 6.1.7. jupyter_core 4.6.3. jupyterlab 2.2.8. notebook 6.1.4. -----. Python 3.8.5 | packaged by conda-forge | (default, Aug 29 2020, 01:22:49) [GCC 7.5.0]. Linux-4.4.0-142-generic-x86_64-with-glibc2.10. 64 logical CPU cores, x86_64. -----. Session information updated at 2020-09-16 11:03. ```. Here is the error message:. ```. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:1628,modifiability,pac,packaged,1628,"0. jedi 0.15.2. jinja2 2.11.2. joblib 0.16.0. jsonschema 3.2.0. kiwisolver 1.2.0. legacy_api_wrap 1.2. llvmlite 0.34.0. lxml 4.5.2. markupsafe 1.1.1. matplotlib 3.3.2. mpl_toolkits NA. natsort 7.0.1. nbformat 5.0.7. networkx 2.5. numba 0.51.2. numexpr 2.7.1. numpy 1.19.2. packaging 20.4. pandas 1.0.1. parso 0.5.2. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prometheus_client NA. prompt_toolkit 3.0.7. ptyprocess 0.6.0. pvectorc NA. pygments 2.7.0. pylab NA. pyparsing 2.4.7. pyrsistent NA. pytz 2020.1. requests 2.23.0. requests_cache 0.5.2. scanpy 1.6.0. scipy 1.5.2. seaborn 0.11.0. send2trash NA. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.23.2. socks 1.7.1. soupsieve 2.0.1. statsmodels 0.12.0. storemagic NA. tables 3.6.1. terminado 0.8.3. tornado 6.0.4. traitlets 5.0.4. urllib3 1.25.10. wcwidth 0.2.5. wrapt 1.12.1. xlsxwriter 1.3.3. zmq 19.0.2. -----. IPython 7.18.1. jupyter_client 6.1.7. jupyter_core 4.6.3. jupyterlab 2.2.8. notebook 6.1.4. -----. Python 3.8.5 | packaged by conda-forge | (default, Aug 29 2020, 01:22:49) [GCC 7.5.0]. Linux-4.4.0-142-generic-x86_64-with-glibc2.10. 64 logical CPU cores, x86_64. -----. Session information updated at 2020-09-16 11:03. ```. Here is the error message:. ```. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-37-b22ada65a1cd> in <module>. 1 # Create Concatenated anndata object for all timepoints. 2 #alldays = e125.concatenate(e135, e145, e155, uns_merge=""unique""). ----> 3 alldays = e125.concatenate(e135). ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/anndata.py in concatenate(self, join, batch_key, batch_categories, uns_merge, index_unique, fill_value, *adatas). 1696 all_adatas = (self,) + tuple(adatas). 1697 . -> 1698 out = concat(. 1699 all_adatas,. 1700 axis=0,. ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in concat(adatas, axis, join",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:2037,modifiability,modul,module,2037,".7. ptyprocess 0.6.0. pvectorc NA. pygments 2.7.0. pylab NA. pyparsing 2.4.7. pyrsistent NA. pytz 2020.1. requests 2.23.0. requests_cache 0.5.2. scanpy 1.6.0. scipy 1.5.2. seaborn 0.11.0. send2trash NA. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.23.2. socks 1.7.1. soupsieve 2.0.1. statsmodels 0.12.0. storemagic NA. tables 3.6.1. terminado 0.8.3. tornado 6.0.4. traitlets 5.0.4. urllib3 1.25.10. wcwidth 0.2.5. wrapt 1.12.1. xlsxwriter 1.3.3. zmq 19.0.2. -----. IPython 7.18.1. jupyter_client 6.1.7. jupyter_core 4.6.3. jupyterlab 2.2.8. notebook 6.1.4. -----. Python 3.8.5 | packaged by conda-forge | (default, Aug 29 2020, 01:22:49) [GCC 7.5.0]. Linux-4.4.0-142-generic-x86_64-with-glibc2.10. 64 logical CPU cores, x86_64. -----. Session information updated at 2020-09-16 11:03. ```. Here is the error message:. ```. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-37-b22ada65a1cd> in <module>. 1 # Create Concatenated anndata object for all timepoints. 2 #alldays = e125.concatenate(e135, e145, e155, uns_merge=""unique""). ----> 3 alldays = e125.concatenate(e135). ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/anndata.py in concatenate(self, join, batch_key, batch_categories, uns_merge, index_unique, fill_value, *adatas). 1696 all_adatas = (self,) + tuple(adatas). 1697 . -> 1698 out = concat(. 1699 all_adatas,. 1700 axis=0,. ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in concat(adatas, axis, join, merge, uns_merge, label, keys, index_unique, fill_value, pairwise). 799 [dim_indices(a, axis=1 - axis) for a in adatas], join=join. 800 ). --> 801 reindexers = [. 802 gen_reindexer(alt_indices, dim_indices(a, axis=1 - axis)) for a in adatas. 803 ]. ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in <listcomp>(.0). 800 ). 801 reindexers = [. --> 802 gen_reindexer(",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:2273,modifiability,pac,packages,2273,"x 1.15.0. sklearn 0.23.2. socks 1.7.1. soupsieve 2.0.1. statsmodels 0.12.0. storemagic NA. tables 3.6.1. terminado 0.8.3. tornado 6.0.4. traitlets 5.0.4. urllib3 1.25.10. wcwidth 0.2.5. wrapt 1.12.1. xlsxwriter 1.3.3. zmq 19.0.2. -----. IPython 7.18.1. jupyter_client 6.1.7. jupyter_core 4.6.3. jupyterlab 2.2.8. notebook 6.1.4. -----. Python 3.8.5 | packaged by conda-forge | (default, Aug 29 2020, 01:22:49) [GCC 7.5.0]. Linux-4.4.0-142-generic-x86_64-with-glibc2.10. 64 logical CPU cores, x86_64. -----. Session information updated at 2020-09-16 11:03. ```. Here is the error message:. ```. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-37-b22ada65a1cd> in <module>. 1 # Create Concatenated anndata object for all timepoints. 2 #alldays = e125.concatenate(e135, e145, e155, uns_merge=""unique""). ----> 3 alldays = e125.concatenate(e135). ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/anndata.py in concatenate(self, join, batch_key, batch_categories, uns_merge, index_unique, fill_value, *adatas). 1696 all_adatas = (self,) + tuple(adatas). 1697 . -> 1698 out = concat(. 1699 all_adatas,. 1700 axis=0,. ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in concat(adatas, axis, join, merge, uns_merge, label, keys, index_unique, fill_value, pairwise). 799 [dim_indices(a, axis=1 - axis) for a in adatas], join=join. 800 ). --> 801 reindexers = [. 802 gen_reindexer(alt_indices, dim_indices(a, axis=1 - axis)) for a in adatas. 803 ]. ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in <listcomp>(.0). 800 ). 801 reindexers = [. --> 802 gen_reindexer(alt_indices, dim_indices(a, axis=1 - axis)) for a in adatas. 803 ]. 804 . ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in gen_reindexer(new_var, cur_var). 393 [1., 0., 0.]], dtype=float32). 394",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:2572,modifiability,pac,packages,2572,"terlab 2.2.8. notebook 6.1.4. -----. Python 3.8.5 | packaged by conda-forge | (default, Aug 29 2020, 01:22:49) [GCC 7.5.0]. Linux-4.4.0-142-generic-x86_64-with-glibc2.10. 64 logical CPU cores, x86_64. -----. Session information updated at 2020-09-16 11:03. ```. Here is the error message:. ```. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-37-b22ada65a1cd> in <module>. 1 # Create Concatenated anndata object for all timepoints. 2 #alldays = e125.concatenate(e135, e145, e155, uns_merge=""unique""). ----> 3 alldays = e125.concatenate(e135). ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/anndata.py in concatenate(self, join, batch_key, batch_categories, uns_merge, index_unique, fill_value, *adatas). 1696 all_adatas = (self,) + tuple(adatas). 1697 . -> 1698 out = concat(. 1699 all_adatas,. 1700 axis=0,. ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in concat(adatas, axis, join, merge, uns_merge, label, keys, index_unique, fill_value, pairwise). 799 [dim_indices(a, axis=1 - axis) for a in adatas], join=join. 800 ). --> 801 reindexers = [. 802 gen_reindexer(alt_indices, dim_indices(a, axis=1 - axis)) for a in adatas. 803 ]. ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in <listcomp>(.0). 800 ). 801 reindexers = [. --> 802 gen_reindexer(alt_indices, dim_indices(a, axis=1 - axis)) for a in adatas. 803 ]. 804 . ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in gen_reindexer(new_var, cur_var). 393 [1., 0., 0.]], dtype=float32). 394 """""". --> 395 return Reindexer(cur_var, new_var). 396 . 397 . ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in __init__(self, old_idx, new_idx). 265 self.no_change = new_idx.equals(old_idx). 266 . --> 267 new_pos = new_idx.get_indexer(old_idx). 268 old_pos",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:2940,modifiability,pac,packages,2940,"--. InvalidIndexError Traceback (most recent call last). <ipython-input-37-b22ada65a1cd> in <module>. 1 # Create Concatenated anndata object for all timepoints. 2 #alldays = e125.concatenate(e135, e145, e155, uns_merge=""unique""). ----> 3 alldays = e125.concatenate(e135). ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/anndata.py in concatenate(self, join, batch_key, batch_categories, uns_merge, index_unique, fill_value, *adatas). 1696 all_adatas = (self,) + tuple(adatas). 1697 . -> 1698 out = concat(. 1699 all_adatas,. 1700 axis=0,. ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in concat(adatas, axis, join, merge, uns_merge, label, keys, index_unique, fill_value, pairwise). 799 [dim_indices(a, axis=1 - axis) for a in adatas], join=join. 800 ). --> 801 reindexers = [. 802 gen_reindexer(alt_indices, dim_indices(a, axis=1 - axis)) for a in adatas. 803 ]. ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in <listcomp>(.0). 800 ). 801 reindexers = [. --> 802 gen_reindexer(alt_indices, dim_indices(a, axis=1 - axis)) for a in adatas. 803 ]. 804 . ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in gen_reindexer(new_var, cur_var). 393 [1., 0., 0.]], dtype=float32). 394 """""". --> 395 return Reindexer(cur_var, new_var). 396 . 397 . ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in __init__(self, old_idx, new_idx). 265 self.no_change = new_idx.equals(old_idx). 266 . --> 267 new_pos = new_idx.get_indexer(old_idx). 268 old_pos = np.arange(len(new_pos)). 269 . ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance). 2731 . 2732 if not self.is_unique:. -> 2733 raise InvalidIndexError(. 2734 ""Reindexing only valid with uniquely valued Index objects"". 2735 ). InvalidIndexError: Reindexing only valid ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:3171,modifiability,pac,packages,3171,"l last). <ipython-input-37-b22ada65a1cd> in <module>. 1 # Create Concatenated anndata object for all timepoints. 2 #alldays = e125.concatenate(e135, e145, e155, uns_merge=""unique""). ----> 3 alldays = e125.concatenate(e135). ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/anndata.py in concatenate(self, join, batch_key, batch_categories, uns_merge, index_unique, fill_value, *adatas). 1696 all_adatas = (self,) + tuple(adatas). 1697 . -> 1698 out = concat(. 1699 all_adatas,. 1700 axis=0,. ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in concat(adatas, axis, join, merge, uns_merge, label, keys, index_unique, fill_value, pairwise). 799 [dim_indices(a, axis=1 - axis) for a in adatas], join=join. 800 ). --> 801 reindexers = [. 802 gen_reindexer(alt_indices, dim_indices(a, axis=1 - axis)) for a in adatas. 803 ]. ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in <listcomp>(.0). 800 ). 801 reindexers = [. --> 802 gen_reindexer(alt_indices, dim_indices(a, axis=1 - axis)) for a in adatas. 803 ]. 804 . ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in gen_reindexer(new_var, cur_var). 393 [1., 0., 0.]], dtype=float32). 394 """""". --> 395 return Reindexer(cur_var, new_var). 396 . 397 . ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in __init__(self, old_idx, new_idx). 265 self.no_change = new_idx.equals(old_idx). 266 . --> 267 new_pos = new_idx.get_indexer(old_idx). 268 old_pos = np.arange(len(new_pos)). 269 . ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance). 2731 . 2732 if not self.is_unique:. -> 2733 raise InvalidIndexError(. 2734 ""Reindexing only valid with uniquely valued Index objects"". 2735 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects. ```. Thanks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:3396,modifiability,pac,packages,3396,"l last). <ipython-input-37-b22ada65a1cd> in <module>. 1 # Create Concatenated anndata object for all timepoints. 2 #alldays = e125.concatenate(e135, e145, e155, uns_merge=""unique""). ----> 3 alldays = e125.concatenate(e135). ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/anndata.py in concatenate(self, join, batch_key, batch_categories, uns_merge, index_unique, fill_value, *adatas). 1696 all_adatas = (self,) + tuple(adatas). 1697 . -> 1698 out = concat(. 1699 all_adatas,. 1700 axis=0,. ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in concat(adatas, axis, join, merge, uns_merge, label, keys, index_unique, fill_value, pairwise). 799 [dim_indices(a, axis=1 - axis) for a in adatas], join=join. 800 ). --> 801 reindexers = [. 802 gen_reindexer(alt_indices, dim_indices(a, axis=1 - axis)) for a in adatas. 803 ]. ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in <listcomp>(.0). 800 ). 801 reindexers = [. --> 802 gen_reindexer(alt_indices, dim_indices(a, axis=1 - axis)) for a in adatas. 803 ]. 804 . ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in gen_reindexer(new_var, cur_var). 393 [1., 0., 0.]], dtype=float32). 394 """""". --> 395 return Reindexer(cur_var, new_var). 396 . 397 . ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in __init__(self, old_idx, new_idx). 265 self.no_change = new_idx.equals(old_idx). 266 . --> 267 new_pos = new_idx.get_indexer(old_idx). 268 old_pos = np.arange(len(new_pos)). 269 . ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance). 2731 . 2732 if not self.is_unique:. -> 2733 raise InvalidIndexError(. 2734 ""Reindexing only valid with uniquely valued Index objects"". 2735 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects. ```. Thanks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:3667,modifiability,pac,packages,3667,"l last). <ipython-input-37-b22ada65a1cd> in <module>. 1 # Create Concatenated anndata object for all timepoints. 2 #alldays = e125.concatenate(e135, e145, e155, uns_merge=""unique""). ----> 3 alldays = e125.concatenate(e135). ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/anndata.py in concatenate(self, join, batch_key, batch_categories, uns_merge, index_unique, fill_value, *adatas). 1696 all_adatas = (self,) + tuple(adatas). 1697 . -> 1698 out = concat(. 1699 all_adatas,. 1700 axis=0,. ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in concat(adatas, axis, join, merge, uns_merge, label, keys, index_unique, fill_value, pairwise). 799 [dim_indices(a, axis=1 - axis) for a in adatas], join=join. 800 ). --> 801 reindexers = [. 802 gen_reindexer(alt_indices, dim_indices(a, axis=1 - axis)) for a in adatas. 803 ]. ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in <listcomp>(.0). 800 ). 801 reindexers = [. --> 802 gen_reindexer(alt_indices, dim_indices(a, axis=1 - axis)) for a in adatas. 803 ]. 804 . ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in gen_reindexer(new_var, cur_var). 393 [1., 0., 0.]], dtype=float32). 394 """""". --> 395 return Reindexer(cur_var, new_var). 396 . 397 . ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in __init__(self, old_idx, new_idx). 265 self.no_change = new_idx.equals(old_idx). 266 . --> 267 new_pos = new_idx.get_indexer(old_idx). 268 old_pos = np.arange(len(new_pos)). 269 . ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance). 2731 . 2732 if not self.is_unique:. -> 2733 raise InvalidIndexError(. 2734 ""Reindexing only valid with uniquely valued Index objects"". 2735 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects. ```. Thanks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:848,performance,network,networkx,848,"Did you get an idea how to solve this problem? I encountered the same problem. . Here is the version info:. ```. WARNING: If you miss a compact list, please try `print_header`! -----. anndata 0.7.4. scanpy 1.6.0. sinfo 0.3.1. -----. OpenSSL 19.1.0. PIL 7.2.0. anndata 0.7.4. appdirs 1.4.4. attr 20.2.0. backcall 0.2.0. bioservices 1.7.9. brotli NA. bs4 4.9.1. certifi 2020.06.20. cffi 1.14.1. chardet 3.0.4. colorama 0.4.3. colorlog NA. cryptography 3.1. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. decorator 4.4.2. easydev 0.10.1. get_version 2.1. gseapy 0.10.1. h5py 2.10.0. idna 2.10. ipykernel 5.3.4. ipython_genutils 0.2.0. jedi 0.15.2. jinja2 2.11.2. joblib 0.16.0. jsonschema 3.2.0. kiwisolver 1.2.0. legacy_api_wrap 1.2. llvmlite 0.34.0. lxml 4.5.2. markupsafe 1.1.1. matplotlib 3.3.2. mpl_toolkits NA. natsort 7.0.1. nbformat 5.0.7. networkx 2.5. numba 0.51.2. numexpr 2.7.1. numpy 1.19.2. packaging 20.4. pandas 1.0.1. parso 0.5.2. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prometheus_client NA. prompt_toolkit 3.0.7. ptyprocess 0.6.0. pvectorc NA. pygments 2.7.0. pylab NA. pyparsing 2.4.7. pyrsistent NA. pytz 2020.1. requests 2.23.0. requests_cache 0.5.2. scanpy 1.6.0. scipy 1.5.2. seaborn 0.11.0. send2trash NA. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.23.2. socks 1.7.1. soupsieve 2.0.1. statsmodels 0.12.0. storemagic NA. tables 3.6.1. terminado 0.8.3. tornado 6.0.4. traitlets 5.0.4. urllib3 1.25.10. wcwidth 0.2.5. wrapt 1.12.1. xlsxwriter 1.3.3. zmq 19.0.2. -----. IPython 7.18.1. jupyter_client 6.1.7. jupyter_core 4.6.3. jupyterlab 2.2.8. notebook 6.1.4. -----. Python 3.8.5 | packaged by conda-forge | (default, Aug 29 2020, 01:22:49) [GCC 7.5.0]. Linux-4.4.0-142-generic-x86_64-with-glibc2.10. 64 logical CPU cores, x86_64. -----. Session information updated at 2020-09-16 11:03. ```. Here is the error message:. ```. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:1758,performance,CPU,CPU,1758,".2. markupsafe 1.1.1. matplotlib 3.3.2. mpl_toolkits NA. natsort 7.0.1. nbformat 5.0.7. networkx 2.5. numba 0.51.2. numexpr 2.7.1. numpy 1.19.2. packaging 20.4. pandas 1.0.1. parso 0.5.2. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prometheus_client NA. prompt_toolkit 3.0.7. ptyprocess 0.6.0. pvectorc NA. pygments 2.7.0. pylab NA. pyparsing 2.4.7. pyrsistent NA. pytz 2020.1. requests 2.23.0. requests_cache 0.5.2. scanpy 1.6.0. scipy 1.5.2. seaborn 0.11.0. send2trash NA. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.23.2. socks 1.7.1. soupsieve 2.0.1. statsmodels 0.12.0. storemagic NA. tables 3.6.1. terminado 0.8.3. tornado 6.0.4. traitlets 5.0.4. urllib3 1.25.10. wcwidth 0.2.5. wrapt 1.12.1. xlsxwriter 1.3.3. zmq 19.0.2. -----. IPython 7.18.1. jupyter_client 6.1.7. jupyter_core 4.6.3. jupyterlab 2.2.8. notebook 6.1.4. -----. Python 3.8.5 | packaged by conda-forge | (default, Aug 29 2020, 01:22:49) [GCC 7.5.0]. Linux-4.4.0-142-generic-x86_64-with-glibc2.10. 64 logical CPU cores, x86_64. -----. Session information updated at 2020-09-16 11:03. ```. Here is the error message:. ```. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-37-b22ada65a1cd> in <module>. 1 # Create Concatenated anndata object for all timepoints. 2 #alldays = e125.concatenate(e135, e145, e155, uns_merge=""unique""). ----> 3 alldays = e125.concatenate(e135). ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/anndata.py in concatenate(self, join, batch_key, batch_categories, uns_merge, index_unique, fill_value, *adatas). 1696 all_adatas = (self,) + tuple(adatas). 1697 . -> 1698 out = concat(. 1699 all_adatas,. 1700 axis=0,. ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in concat(adatas, axis, join, merge, uns_merge, label, keys, index_unique, fill_value, pairwise). 799 [dim_indices(a, axis=1 - axis) for a in adatas], join=",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:1850,performance,error,error,1850,"rkx 2.5. numba 0.51.2. numexpr 2.7.1. numpy 1.19.2. packaging 20.4. pandas 1.0.1. parso 0.5.2. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prometheus_client NA. prompt_toolkit 3.0.7. ptyprocess 0.6.0. pvectorc NA. pygments 2.7.0. pylab NA. pyparsing 2.4.7. pyrsistent NA. pytz 2020.1. requests 2.23.0. requests_cache 0.5.2. scanpy 1.6.0. scipy 1.5.2. seaborn 0.11.0. send2trash NA. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.23.2. socks 1.7.1. soupsieve 2.0.1. statsmodels 0.12.0. storemagic NA. tables 3.6.1. terminado 0.8.3. tornado 6.0.4. traitlets 5.0.4. urllib3 1.25.10. wcwidth 0.2.5. wrapt 1.12.1. xlsxwriter 1.3.3. zmq 19.0.2. -----. IPython 7.18.1. jupyter_client 6.1.7. jupyter_core 4.6.3. jupyterlab 2.2.8. notebook 6.1.4. -----. Python 3.8.5 | packaged by conda-forge | (default, Aug 29 2020, 01:22:49) [GCC 7.5.0]. Linux-4.4.0-142-generic-x86_64-with-glibc2.10. 64 logical CPU cores, x86_64. -----. Session information updated at 2020-09-16 11:03. ```. Here is the error message:. ```. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-37-b22ada65a1cd> in <module>. 1 # Create Concatenated anndata object for all timepoints. 2 #alldays = e125.concatenate(e135, e145, e155, uns_merge=""unique""). ----> 3 alldays = e125.concatenate(e135). ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/anndata.py in concatenate(self, join, batch_key, batch_categories, uns_merge, index_unique, fill_value, *adatas). 1696 all_adatas = (self,) + tuple(adatas). 1697 . -> 1698 out = concat(. 1699 all_adatas,. 1700 axis=0,. ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in concat(adatas, axis, join, merge, uns_merge, label, keys, index_unique, fill_value, pairwise). 799 [dim_indices(a, axis=1 - axis) for a in adatas], join=join. 800 ). --> 801 reindexers = [. 802 gen_reindexer(alt_indices, dim_indices(a, axis=1 - a",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:2093,performance,time,timepoints,2093,"A. pyparsing 2.4.7. pyrsistent NA. pytz 2020.1. requests 2.23.0. requests_cache 0.5.2. scanpy 1.6.0. scipy 1.5.2. seaborn 0.11.0. send2trash NA. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.23.2. socks 1.7.1. soupsieve 2.0.1. statsmodels 0.12.0. storemagic NA. tables 3.6.1. terminado 0.8.3. tornado 6.0.4. traitlets 5.0.4. urllib3 1.25.10. wcwidth 0.2.5. wrapt 1.12.1. xlsxwriter 1.3.3. zmq 19.0.2. -----. IPython 7.18.1. jupyter_client 6.1.7. jupyter_core 4.6.3. jupyterlab 2.2.8. notebook 6.1.4. -----. Python 3.8.5 | packaged by conda-forge | (default, Aug 29 2020, 01:22:49) [GCC 7.5.0]. Linux-4.4.0-142-generic-x86_64-with-glibc2.10. 64 logical CPU cores, x86_64. -----. Session information updated at 2020-09-16 11:03. ```. Here is the error message:. ```. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-37-b22ada65a1cd> in <module>. 1 # Create Concatenated anndata object for all timepoints. 2 #alldays = e125.concatenate(e135, e145, e155, uns_merge=""unique""). ----> 3 alldays = e125.concatenate(e135). ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/anndata.py in concatenate(self, join, batch_key, batch_categories, uns_merge, index_unique, fill_value, *adatas). 1696 all_adatas = (self,) + tuple(adatas). 1697 . -> 1698 out = concat(. 1699 all_adatas,. 1700 axis=0,. ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in concat(adatas, axis, join, merge, uns_merge, label, keys, index_unique, fill_value, pairwise). 799 [dim_indices(a, axis=1 - axis) for a in adatas], join=join. 800 ). --> 801 reindexers = [. 802 gen_reindexer(alt_indices, dim_indices(a, axis=1 - axis)) for a in adatas. 803 ]. ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in <listcomp>(.0). 800 ). 801 reindexers = [. --> 802 gen_reindexer(alt_indices, dim_indices(a, axis=1 - axis)) for a in adata",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:2781,performance,reindex,reindexers,2781,"ssion information updated at 2020-09-16 11:03. ```. Here is the error message:. ```. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-37-b22ada65a1cd> in <module>. 1 # Create Concatenated anndata object for all timepoints. 2 #alldays = e125.concatenate(e135, e145, e155, uns_merge=""unique""). ----> 3 alldays = e125.concatenate(e135). ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/anndata.py in concatenate(self, join, batch_key, batch_categories, uns_merge, index_unique, fill_value, *adatas). 1696 all_adatas = (self,) + tuple(adatas). 1697 . -> 1698 out = concat(. 1699 all_adatas,. 1700 axis=0,. ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in concat(adatas, axis, join, merge, uns_merge, label, keys, index_unique, fill_value, pairwise). 799 [dim_indices(a, axis=1 - axis) for a in adatas], join=join. 800 ). --> 801 reindexers = [. 802 gen_reindexer(alt_indices, dim_indices(a, axis=1 - axis)) for a in adatas. 803 ]. ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in <listcomp>(.0). 800 ). 801 reindexers = [. --> 802 gen_reindexer(alt_indices, dim_indices(a, axis=1 - axis)) for a in adatas. 803 ]. 804 . ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in gen_reindexer(new_var, cur_var). 393 [1., 0., 0.]], dtype=float32). 394 """""". --> 395 return Reindexer(cur_var, new_var). 396 . 397 . ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in __init__(self, old_idx, new_idx). 265 self.no_change = new_idx.equals(old_idx). 266 . --> 267 new_pos = new_idx.get_indexer(old_idx). 268 old_pos = np.arange(len(new_pos)). 269 . ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance). 2731 . 2732 if not self.is",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:3002,performance,reindex,reindexers,3002,"l last). <ipython-input-37-b22ada65a1cd> in <module>. 1 # Create Concatenated anndata object for all timepoints. 2 #alldays = e125.concatenate(e135, e145, e155, uns_merge=""unique""). ----> 3 alldays = e125.concatenate(e135). ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/anndata.py in concatenate(self, join, batch_key, batch_categories, uns_merge, index_unique, fill_value, *adatas). 1696 all_adatas = (self,) + tuple(adatas). 1697 . -> 1698 out = concat(. 1699 all_adatas,. 1700 axis=0,. ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in concat(adatas, axis, join, merge, uns_merge, label, keys, index_unique, fill_value, pairwise). 799 [dim_indices(a, axis=1 - axis) for a in adatas], join=join. 800 ). --> 801 reindexers = [. 802 gen_reindexer(alt_indices, dim_indices(a, axis=1 - axis)) for a in adatas. 803 ]. ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in <listcomp>(.0). 800 ). 801 reindexers = [. --> 802 gen_reindexer(alt_indices, dim_indices(a, axis=1 - axis)) for a in adatas. 803 ]. 804 . ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in gen_reindexer(new_var, cur_var). 393 [1., 0., 0.]], dtype=float32). 394 """""". --> 395 return Reindexer(cur_var, new_var). 396 . 397 . ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in __init__(self, old_idx, new_idx). 265 self.no_change = new_idx.equals(old_idx). 266 . --> 267 new_pos = new_idx.get_indexer(old_idx). 268 old_pos = np.arange(len(new_pos)). 269 . ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance). 2731 . 2732 if not self.is_unique:. -> 2733 raise InvalidIndexError(. 2734 ""Reindexing only valid with uniquely valued Index objects"". 2735 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects. ```. Thanks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:3298,performance,Reindex,Reindexer,3298,"l last). <ipython-input-37-b22ada65a1cd> in <module>. 1 # Create Concatenated anndata object for all timepoints. 2 #alldays = e125.concatenate(e135, e145, e155, uns_merge=""unique""). ----> 3 alldays = e125.concatenate(e135). ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/anndata.py in concatenate(self, join, batch_key, batch_categories, uns_merge, index_unique, fill_value, *adatas). 1696 all_adatas = (self,) + tuple(adatas). 1697 . -> 1698 out = concat(. 1699 all_adatas,. 1700 axis=0,. ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in concat(adatas, axis, join, merge, uns_merge, label, keys, index_unique, fill_value, pairwise). 799 [dim_indices(a, axis=1 - axis) for a in adatas], join=join. 800 ). --> 801 reindexers = [. 802 gen_reindexer(alt_indices, dim_indices(a, axis=1 - axis)) for a in adatas. 803 ]. ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in <listcomp>(.0). 800 ). 801 reindexers = [. --> 802 gen_reindexer(alt_indices, dim_indices(a, axis=1 - axis)) for a in adatas. 803 ]. 804 . ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in gen_reindexer(new_var, cur_var). 393 [1., 0., 0.]], dtype=float32). 394 """""". --> 395 return Reindexer(cur_var, new_var). 396 . 397 . ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in __init__(self, old_idx, new_idx). 265 self.no_change = new_idx.equals(old_idx). 266 . --> 267 new_pos = new_idx.get_indexer(old_idx). 268 old_pos = np.arange(len(new_pos)). 269 . ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance). 2731 . 2732 if not self.is_unique:. -> 2733 raise InvalidIndexError(. 2734 ""Reindexing only valid with uniquely valued Index objects"". 2735 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects. ```. Thanks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:3836,performance,Reindex,Reindexing,3836,"l last). <ipython-input-37-b22ada65a1cd> in <module>. 1 # Create Concatenated anndata object for all timepoints. 2 #alldays = e125.concatenate(e135, e145, e155, uns_merge=""unique""). ----> 3 alldays = e125.concatenate(e135). ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/anndata.py in concatenate(self, join, batch_key, batch_categories, uns_merge, index_unique, fill_value, *adatas). 1696 all_adatas = (self,) + tuple(adatas). 1697 . -> 1698 out = concat(. 1699 all_adatas,. 1700 axis=0,. ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in concat(adatas, axis, join, merge, uns_merge, label, keys, index_unique, fill_value, pairwise). 799 [dim_indices(a, axis=1 - axis) for a in adatas], join=join. 800 ). --> 801 reindexers = [. 802 gen_reindexer(alt_indices, dim_indices(a, axis=1 - axis)) for a in adatas. 803 ]. ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in <listcomp>(.0). 800 ). 801 reindexers = [. --> 802 gen_reindexer(alt_indices, dim_indices(a, axis=1 - axis)) for a in adatas. 803 ]. 804 . ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in gen_reindexer(new_var, cur_var). 393 [1., 0., 0.]], dtype=float32). 394 """""". --> 395 return Reindexer(cur_var, new_var). 396 . 397 . ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in __init__(self, old_idx, new_idx). 265 self.no_change = new_idx.equals(old_idx). 266 . --> 267 new_pos = new_idx.get_indexer(old_idx). 268 old_pos = np.arange(len(new_pos)). 269 . ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance). 2731 . 2732 if not self.is_unique:. -> 2733 raise InvalidIndexError(. 2734 ""Reindexing only valid with uniquely valued Index objects"". 2735 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects. ```. Thanks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:3922,performance,Reindex,Reindexing,3922,"l last). <ipython-input-37-b22ada65a1cd> in <module>. 1 # Create Concatenated anndata object for all timepoints. 2 #alldays = e125.concatenate(e135, e145, e155, uns_merge=""unique""). ----> 3 alldays = e125.concatenate(e135). ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/anndata.py in concatenate(self, join, batch_key, batch_categories, uns_merge, index_unique, fill_value, *adatas). 1696 all_adatas = (self,) + tuple(adatas). 1697 . -> 1698 out = concat(. 1699 all_adatas,. 1700 axis=0,. ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in concat(adatas, axis, join, merge, uns_merge, label, keys, index_unique, fill_value, pairwise). 799 [dim_indices(a, axis=1 - axis) for a in adatas], join=join. 800 ). --> 801 reindexers = [. 802 gen_reindexer(alt_indices, dim_indices(a, axis=1 - axis)) for a in adatas. 803 ]. ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in <listcomp>(.0). 800 ). 801 reindexers = [. --> 802 gen_reindexer(alt_indices, dim_indices(a, axis=1 - axis)) for a in adatas. 803 ]. 804 . ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in gen_reindexer(new_var, cur_var). 393 [1., 0., 0.]], dtype=float32). 394 """""". --> 395 return Reindexer(cur_var, new_var). 396 . 397 . ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in __init__(self, old_idx, new_idx). 265 self.no_change = new_idx.equals(old_idx). 266 . --> 267 new_pos = new_idx.get_indexer(old_idx). 268 old_pos = np.arange(len(new_pos)). 269 . ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance). 2731 . 2732 if not self.is_unique:. -> 2733 raise InvalidIndexError(. 2734 ""Reindexing only valid with uniquely valued Index objects"". 2735 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects. ```. Thanks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:3748,reliability,toleran,tolerance,3748,"l last). <ipython-input-37-b22ada65a1cd> in <module>. 1 # Create Concatenated anndata object for all timepoints. 2 #alldays = e125.concatenate(e135, e145, e155, uns_merge=""unique""). ----> 3 alldays = e125.concatenate(e135). ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/anndata.py in concatenate(self, join, batch_key, batch_categories, uns_merge, index_unique, fill_value, *adatas). 1696 all_adatas = (self,) + tuple(adatas). 1697 . -> 1698 out = concat(. 1699 all_adatas,. 1700 axis=0,. ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in concat(adatas, axis, join, merge, uns_merge, label, keys, index_unique, fill_value, pairwise). 799 [dim_indices(a, axis=1 - axis) for a in adatas], join=join. 800 ). --> 801 reindexers = [. 802 gen_reindexer(alt_indices, dim_indices(a, axis=1 - axis)) for a in adatas. 803 ]. ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in <listcomp>(.0). 800 ). 801 reindexers = [. --> 802 gen_reindexer(alt_indices, dim_indices(a, axis=1 - axis)) for a in adatas. 803 ]. 804 . ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in gen_reindexer(new_var, cur_var). 393 [1., 0., 0.]], dtype=float32). 394 """""". --> 395 return Reindexer(cur_var, new_var). 396 . 397 . ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in __init__(self, old_idx, new_idx). 265 self.no_change = new_idx.equals(old_idx). 266 . --> 267 new_pos = new_idx.get_indexer(old_idx). 268 old_pos = np.arange(len(new_pos)). 269 . ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance). 2731 . 2732 if not self.is_unique:. -> 2733 raise InvalidIndexError(. 2734 ""Reindexing only valid with uniquely valued Index objects"". 2735 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects. ```. Thanks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:1750,safety,log,logical,1750,"ml 4.5.2. markupsafe 1.1.1. matplotlib 3.3.2. mpl_toolkits NA. natsort 7.0.1. nbformat 5.0.7. networkx 2.5. numba 0.51.2. numexpr 2.7.1. numpy 1.19.2. packaging 20.4. pandas 1.0.1. parso 0.5.2. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prometheus_client NA. prompt_toolkit 3.0.7. ptyprocess 0.6.0. pvectorc NA. pygments 2.7.0. pylab NA. pyparsing 2.4.7. pyrsistent NA. pytz 2020.1. requests 2.23.0. requests_cache 0.5.2. scanpy 1.6.0. scipy 1.5.2. seaborn 0.11.0. send2trash NA. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.23.2. socks 1.7.1. soupsieve 2.0.1. statsmodels 0.12.0. storemagic NA. tables 3.6.1. terminado 0.8.3. tornado 6.0.4. traitlets 5.0.4. urllib3 1.25.10. wcwidth 0.2.5. wrapt 1.12.1. xlsxwriter 1.3.3. zmq 19.0.2. -----. IPython 7.18.1. jupyter_client 6.1.7. jupyter_core 4.6.3. jupyterlab 2.2.8. notebook 6.1.4. -----. Python 3.8.5 | packaged by conda-forge | (default, Aug 29 2020, 01:22:49) [GCC 7.5.0]. Linux-4.4.0-142-generic-x86_64-with-glibc2.10. 64 logical CPU cores, x86_64. -----. Session information updated at 2020-09-16 11:03. ```. Here is the error message:. ```. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-37-b22ada65a1cd> in <module>. 1 # Create Concatenated anndata object for all timepoints. 2 #alldays = e125.concatenate(e135, e145, e155, uns_merge=""unique""). ----> 3 alldays = e125.concatenate(e135). ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/anndata.py in concatenate(self, join, batch_key, batch_categories, uns_merge, index_unique, fill_value, *adatas). 1696 all_adatas = (self,) + tuple(adatas). 1697 . -> 1698 out = concat(. 1699 all_adatas,. 1700 axis=0,. ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in concat(adatas, axis, join, merge, uns_merge, label, keys, index_unique, fill_value, pairwise). 799 [dim_indices(a, axis=1 - axis) for a in adatas],",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:1804,safety,updat,updated,1804,"kits NA. natsort 7.0.1. nbformat 5.0.7. networkx 2.5. numba 0.51.2. numexpr 2.7.1. numpy 1.19.2. packaging 20.4. pandas 1.0.1. parso 0.5.2. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prometheus_client NA. prompt_toolkit 3.0.7. ptyprocess 0.6.0. pvectorc NA. pygments 2.7.0. pylab NA. pyparsing 2.4.7. pyrsistent NA. pytz 2020.1. requests 2.23.0. requests_cache 0.5.2. scanpy 1.6.0. scipy 1.5.2. seaborn 0.11.0. send2trash NA. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.23.2. socks 1.7.1. soupsieve 2.0.1. statsmodels 0.12.0. storemagic NA. tables 3.6.1. terminado 0.8.3. tornado 6.0.4. traitlets 5.0.4. urllib3 1.25.10. wcwidth 0.2.5. wrapt 1.12.1. xlsxwriter 1.3.3. zmq 19.0.2. -----. IPython 7.18.1. jupyter_client 6.1.7. jupyter_core 4.6.3. jupyterlab 2.2.8. notebook 6.1.4. -----. Python 3.8.5 | packaged by conda-forge | (default, Aug 29 2020, 01:22:49) [GCC 7.5.0]. Linux-4.4.0-142-generic-x86_64-with-glibc2.10. 64 logical CPU cores, x86_64. -----. Session information updated at 2020-09-16 11:03. ```. Here is the error message:. ```. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-37-b22ada65a1cd> in <module>. 1 # Create Concatenated anndata object for all timepoints. 2 #alldays = e125.concatenate(e135, e145, e155, uns_merge=""unique""). ----> 3 alldays = e125.concatenate(e135). ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/anndata.py in concatenate(self, join, batch_key, batch_categories, uns_merge, index_unique, fill_value, *adatas). 1696 all_adatas = (self,) + tuple(adatas). 1697 . -> 1698 out = concat(. 1699 all_adatas,. 1700 axis=0,. ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in concat(adatas, axis, join, merge, uns_merge, label, keys, index_unique, fill_value, pairwise). 799 [dim_indices(a, axis=1 - axis) for a in adatas], join=join. 800 ). --> 801 reindexers = [. 802 gen_rei",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:1850,safety,error,error,1850,"rkx 2.5. numba 0.51.2. numexpr 2.7.1. numpy 1.19.2. packaging 20.4. pandas 1.0.1. parso 0.5.2. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prometheus_client NA. prompt_toolkit 3.0.7. ptyprocess 0.6.0. pvectorc NA. pygments 2.7.0. pylab NA. pyparsing 2.4.7. pyrsistent NA. pytz 2020.1. requests 2.23.0. requests_cache 0.5.2. scanpy 1.6.0. scipy 1.5.2. seaborn 0.11.0. send2trash NA. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.23.2. socks 1.7.1. soupsieve 2.0.1. statsmodels 0.12.0. storemagic NA. tables 3.6.1. terminado 0.8.3. tornado 6.0.4. traitlets 5.0.4. urllib3 1.25.10. wcwidth 0.2.5. wrapt 1.12.1. xlsxwriter 1.3.3. zmq 19.0.2. -----. IPython 7.18.1. jupyter_client 6.1.7. jupyter_core 4.6.3. jupyterlab 2.2.8. notebook 6.1.4. -----. Python 3.8.5 | packaged by conda-forge | (default, Aug 29 2020, 01:22:49) [GCC 7.5.0]. Linux-4.4.0-142-generic-x86_64-with-glibc2.10. 64 logical CPU cores, x86_64. -----. Session information updated at 2020-09-16 11:03. ```. Here is the error message:. ```. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-37-b22ada65a1cd> in <module>. 1 # Create Concatenated anndata object for all timepoints. 2 #alldays = e125.concatenate(e135, e145, e155, uns_merge=""unique""). ----> 3 alldays = e125.concatenate(e135). ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/anndata.py in concatenate(self, join, batch_key, batch_categories, uns_merge, index_unique, fill_value, *adatas). 1696 all_adatas = (self,) + tuple(adatas). 1697 . -> 1698 out = concat(. 1699 all_adatas,. 1700 axis=0,. ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in concat(adatas, axis, join, merge, uns_merge, label, keys, index_unique, fill_value, pairwise). 799 [dim_indices(a, axis=1 - axis) for a in adatas], join=join. 800 ). --> 801 reindexers = [. 802 gen_reindexer(alt_indices, dim_indices(a, axis=1 - a",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:2010,safety,input,input-,2010,"ient NA. prompt_toolkit 3.0.7. ptyprocess 0.6.0. pvectorc NA. pygments 2.7.0. pylab NA. pyparsing 2.4.7. pyrsistent NA. pytz 2020.1. requests 2.23.0. requests_cache 0.5.2. scanpy 1.6.0. scipy 1.5.2. seaborn 0.11.0. send2trash NA. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.23.2. socks 1.7.1. soupsieve 2.0.1. statsmodels 0.12.0. storemagic NA. tables 3.6.1. terminado 0.8.3. tornado 6.0.4. traitlets 5.0.4. urllib3 1.25.10. wcwidth 0.2.5. wrapt 1.12.1. xlsxwriter 1.3.3. zmq 19.0.2. -----. IPython 7.18.1. jupyter_client 6.1.7. jupyter_core 4.6.3. jupyterlab 2.2.8. notebook 6.1.4. -----. Python 3.8.5 | packaged by conda-forge | (default, Aug 29 2020, 01:22:49) [GCC 7.5.0]. Linux-4.4.0-142-generic-x86_64-with-glibc2.10. 64 logical CPU cores, x86_64. -----. Session information updated at 2020-09-16 11:03. ```. Here is the error message:. ```. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-37-b22ada65a1cd> in <module>. 1 # Create Concatenated anndata object for all timepoints. 2 #alldays = e125.concatenate(e135, e145, e155, uns_merge=""unique""). ----> 3 alldays = e125.concatenate(e135). ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/anndata.py in concatenate(self, join, batch_key, batch_categories, uns_merge, index_unique, fill_value, *adatas). 1696 all_adatas = (self,) + tuple(adatas). 1697 . -> 1698 out = concat(. 1699 all_adatas,. 1700 axis=0,. ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in concat(adatas, axis, join, merge, uns_merge, label, keys, index_unique, fill_value, pairwise). 799 [dim_indices(a, axis=1 - axis) for a in adatas], join=join. 800 ). --> 801 reindexers = [. 802 gen_reindexer(alt_indices, dim_indices(a, axis=1 - axis)) for a in adatas. 803 ]. ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in <listcomp>(.0). 800 ). 801 reindexers ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:2037,safety,modul,module,2037,".7. ptyprocess 0.6.0. pvectorc NA. pygments 2.7.0. pylab NA. pyparsing 2.4.7. pyrsistent NA. pytz 2020.1. requests 2.23.0. requests_cache 0.5.2. scanpy 1.6.0. scipy 1.5.2. seaborn 0.11.0. send2trash NA. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.23.2. socks 1.7.1. soupsieve 2.0.1. statsmodels 0.12.0. storemagic NA. tables 3.6.1. terminado 0.8.3. tornado 6.0.4. traitlets 5.0.4. urllib3 1.25.10. wcwidth 0.2.5. wrapt 1.12.1. xlsxwriter 1.3.3. zmq 19.0.2. -----. IPython 7.18.1. jupyter_client 6.1.7. jupyter_core 4.6.3. jupyterlab 2.2.8. notebook 6.1.4. -----. Python 3.8.5 | packaged by conda-forge | (default, Aug 29 2020, 01:22:49) [GCC 7.5.0]. Linux-4.4.0-142-generic-x86_64-with-glibc2.10. 64 logical CPU cores, x86_64. -----. Session information updated at 2020-09-16 11:03. ```. Here is the error message:. ```. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-37-b22ada65a1cd> in <module>. 1 # Create Concatenated anndata object for all timepoints. 2 #alldays = e125.concatenate(e135, e145, e155, uns_merge=""unique""). ----> 3 alldays = e125.concatenate(e135). ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/anndata.py in concatenate(self, join, batch_key, batch_categories, uns_merge, index_unique, fill_value, *adatas). 1696 all_adatas = (self,) + tuple(adatas). 1697 . -> 1698 out = concat(. 1699 all_adatas,. 1700 axis=0,. ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in concat(adatas, axis, join, merge, uns_merge, label, keys, index_unique, fill_value, pairwise). 799 [dim_indices(a, axis=1 - axis) for a in adatas], join=join. 800 ). --> 801 reindexers = [. 802 gen_reindexer(alt_indices, dim_indices(a, axis=1 - axis)) for a in adatas. 803 ]. ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in <listcomp>(.0). 800 ). 801 reindexers = [. --> 802 gen_reindexer(",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:3852,safety,valid,valid,3852,"l last). <ipython-input-37-b22ada65a1cd> in <module>. 1 # Create Concatenated anndata object for all timepoints. 2 #alldays = e125.concatenate(e135, e145, e155, uns_merge=""unique""). ----> 3 alldays = e125.concatenate(e135). ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/anndata.py in concatenate(self, join, batch_key, batch_categories, uns_merge, index_unique, fill_value, *adatas). 1696 all_adatas = (self,) + tuple(adatas). 1697 . -> 1698 out = concat(. 1699 all_adatas,. 1700 axis=0,. ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in concat(adatas, axis, join, merge, uns_merge, label, keys, index_unique, fill_value, pairwise). 799 [dim_indices(a, axis=1 - axis) for a in adatas], join=join. 800 ). --> 801 reindexers = [. 802 gen_reindexer(alt_indices, dim_indices(a, axis=1 - axis)) for a in adatas. 803 ]. ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in <listcomp>(.0). 800 ). 801 reindexers = [. --> 802 gen_reindexer(alt_indices, dim_indices(a, axis=1 - axis)) for a in adatas. 803 ]. 804 . ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in gen_reindexer(new_var, cur_var). 393 [1., 0., 0.]], dtype=float32). 394 """""". --> 395 return Reindexer(cur_var, new_var). 396 . 397 . ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in __init__(self, old_idx, new_idx). 265 self.no_change = new_idx.equals(old_idx). 266 . --> 267 new_pos = new_idx.get_indexer(old_idx). 268 old_pos = np.arange(len(new_pos)). 269 . ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance). 2731 . 2732 if not self.is_unique:. -> 2733 raise InvalidIndexError(. 2734 ""Reindexing only valid with uniquely valued Index objects"". 2735 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects. ```. Thanks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:3938,safety,valid,valid,3938,"l last). <ipython-input-37-b22ada65a1cd> in <module>. 1 # Create Concatenated anndata object for all timepoints. 2 #alldays = e125.concatenate(e135, e145, e155, uns_merge=""unique""). ----> 3 alldays = e125.concatenate(e135). ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/anndata.py in concatenate(self, join, batch_key, batch_categories, uns_merge, index_unique, fill_value, *adatas). 1696 all_adatas = (self,) + tuple(adatas). 1697 . -> 1698 out = concat(. 1699 all_adatas,. 1700 axis=0,. ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in concat(adatas, axis, join, merge, uns_merge, label, keys, index_unique, fill_value, pairwise). 799 [dim_indices(a, axis=1 - axis) for a in adatas], join=join. 800 ). --> 801 reindexers = [. 802 gen_reindexer(alt_indices, dim_indices(a, axis=1 - axis)) for a in adatas. 803 ]. ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in <listcomp>(.0). 800 ). 801 reindexers = [. --> 802 gen_reindexer(alt_indices, dim_indices(a, axis=1 - axis)) for a in adatas. 803 ]. 804 . ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in gen_reindexer(new_var, cur_var). 393 [1., 0., 0.]], dtype=float32). 394 """""". --> 395 return Reindexer(cur_var, new_var). 396 . 397 . ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in __init__(self, old_idx, new_idx). 265 self.no_change = new_idx.equals(old_idx). 266 . --> 267 new_pos = new_idx.get_indexer(old_idx). 268 old_pos = np.arange(len(new_pos)). 269 . ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance). 2731 . 2732 if not self.is_unique:. -> 2733 raise InvalidIndexError(. 2734 ""Reindexing only valid with uniquely valued Index objects"". 2735 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects. ```. Thanks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:360,security,certif,certifi,360,"Did you get an idea how to solve this problem? I encountered the same problem. . Here is the version info:. ```. WARNING: If you miss a compact list, please try `print_header`! -----. anndata 0.7.4. scanpy 1.6.0. sinfo 0.3.1. -----. OpenSSL 19.1.0. PIL 7.2.0. anndata 0.7.4. appdirs 1.4.4. attr 20.2.0. backcall 0.2.0. bioservices 1.7.9. brotli NA. bs4 4.9.1. certifi 2020.06.20. cffi 1.14.1. chardet 3.0.4. colorama 0.4.3. colorlog NA. cryptography 3.1. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. decorator 4.4.2. easydev 0.10.1. get_version 2.1. gseapy 0.10.1. h5py 2.10.0. idna 2.10. ipykernel 5.3.4. ipython_genutils 0.2.0. jedi 0.15.2. jinja2 2.11.2. joblib 0.16.0. jsonschema 3.2.0. kiwisolver 1.2.0. legacy_api_wrap 1.2. llvmlite 0.34.0. lxml 4.5.2. markupsafe 1.1.1. matplotlib 3.3.2. mpl_toolkits NA. natsort 7.0.1. nbformat 5.0.7. networkx 2.5. numba 0.51.2. numexpr 2.7.1. numpy 1.19.2. packaging 20.4. pandas 1.0.1. parso 0.5.2. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prometheus_client NA. prompt_toolkit 3.0.7. ptyprocess 0.6.0. pvectorc NA. pygments 2.7.0. pylab NA. pyparsing 2.4.7. pyrsistent NA. pytz 2020.1. requests 2.23.0. requests_cache 0.5.2. scanpy 1.6.0. scipy 1.5.2. seaborn 0.11.0. send2trash NA. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.23.2. socks 1.7.1. soupsieve 2.0.1. statsmodels 0.12.0. storemagic NA. tables 3.6.1. terminado 0.8.3. tornado 6.0.4. traitlets 5.0.4. urllib3 1.25.10. wcwidth 0.2.5. wrapt 1.12.1. xlsxwriter 1.3.3. zmq 19.0.2. -----. IPython 7.18.1. jupyter_client 6.1.7. jupyter_core 4.6.3. jupyterlab 2.2.8. notebook 6.1.4. -----. Python 3.8.5 | packaged by conda-forge | (default, Aug 29 2020, 01:22:49) [GCC 7.5.0]. Linux-4.4.0-142-generic-x86_64-with-glibc2.10. 64 logical CPU cores, x86_64. -----. Session information updated at 2020-09-16 11:03. ```. Here is the error message:. ```. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:437,security,cryptograph,cryptography,437,"Did you get an idea how to solve this problem? I encountered the same problem. . Here is the version info:. ```. WARNING: If you miss a compact list, please try `print_header`! -----. anndata 0.7.4. scanpy 1.6.0. sinfo 0.3.1. -----. OpenSSL 19.1.0. PIL 7.2.0. anndata 0.7.4. appdirs 1.4.4. attr 20.2.0. backcall 0.2.0. bioservices 1.7.9. brotli NA. bs4 4.9.1. certifi 2020.06.20. cffi 1.14.1. chardet 3.0.4. colorama 0.4.3. colorlog NA. cryptography 3.1. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. decorator 4.4.2. easydev 0.10.1. get_version 2.1. gseapy 0.10.1. h5py 2.10.0. idna 2.10. ipykernel 5.3.4. ipython_genutils 0.2.0. jedi 0.15.2. jinja2 2.11.2. joblib 0.16.0. jsonschema 3.2.0. kiwisolver 1.2.0. legacy_api_wrap 1.2. llvmlite 0.34.0. lxml 4.5.2. markupsafe 1.1.1. matplotlib 3.3.2. mpl_toolkits NA. natsort 7.0.1. nbformat 5.0.7. networkx 2.5. numba 0.51.2. numexpr 2.7.1. numpy 1.19.2. packaging 20.4. pandas 1.0.1. parso 0.5.2. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prometheus_client NA. prompt_toolkit 3.0.7. ptyprocess 0.6.0. pvectorc NA. pygments 2.7.0. pylab NA. pyparsing 2.4.7. pyrsistent NA. pytz 2020.1. requests 2.23.0. requests_cache 0.5.2. scanpy 1.6.0. scipy 1.5.2. seaborn 0.11.0. send2trash NA. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.23.2. socks 1.7.1. soupsieve 2.0.1. statsmodels 0.12.0. storemagic NA. tables 3.6.1. terminado 0.8.3. tornado 6.0.4. traitlets 5.0.4. urllib3 1.25.10. wcwidth 0.2.5. wrapt 1.12.1. xlsxwriter 1.3.3. zmq 19.0.2. -----. IPython 7.18.1. jupyter_client 6.1.7. jupyter_core 4.6.3. jupyterlab 2.2.8. notebook 6.1.4. -----. Python 3.8.5 | packaged by conda-forge | (default, Aug 29 2020, 01:22:49) [GCC 7.5.0]. Linux-4.4.0-142-generic-x86_64-with-glibc2.10. 64 logical CPU cores, x86_64. -----. Session information updated at 2020-09-16 11:03. ```. Here is the error message:. ```. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:848,security,network,networkx,848,"Did you get an idea how to solve this problem? I encountered the same problem. . Here is the version info:. ```. WARNING: If you miss a compact list, please try `print_header`! -----. anndata 0.7.4. scanpy 1.6.0. sinfo 0.3.1. -----. OpenSSL 19.1.0. PIL 7.2.0. anndata 0.7.4. appdirs 1.4.4. attr 20.2.0. backcall 0.2.0. bioservices 1.7.9. brotli NA. bs4 4.9.1. certifi 2020.06.20. cffi 1.14.1. chardet 3.0.4. colorama 0.4.3. colorlog NA. cryptography 3.1. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. decorator 4.4.2. easydev 0.10.1. get_version 2.1. gseapy 0.10.1. h5py 2.10.0. idna 2.10. ipykernel 5.3.4. ipython_genutils 0.2.0. jedi 0.15.2. jinja2 2.11.2. joblib 0.16.0. jsonschema 3.2.0. kiwisolver 1.2.0. legacy_api_wrap 1.2. llvmlite 0.34.0. lxml 4.5.2. markupsafe 1.1.1. matplotlib 3.3.2. mpl_toolkits NA. natsort 7.0.1. nbformat 5.0.7. networkx 2.5. numba 0.51.2. numexpr 2.7.1. numpy 1.19.2. packaging 20.4. pandas 1.0.1. parso 0.5.2. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prometheus_client NA. prompt_toolkit 3.0.7. ptyprocess 0.6.0. pvectorc NA. pygments 2.7.0. pylab NA. pyparsing 2.4.7. pyrsistent NA. pytz 2020.1. requests 2.23.0. requests_cache 0.5.2. scanpy 1.6.0. scipy 1.5.2. seaborn 0.11.0. send2trash NA. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.23.2. socks 1.7.1. soupsieve 2.0.1. statsmodels 0.12.0. storemagic NA. tables 3.6.1. terminado 0.8.3. tornado 6.0.4. traitlets 5.0.4. urllib3 1.25.10. wcwidth 0.2.5. wrapt 1.12.1. xlsxwriter 1.3.3. zmq 19.0.2. -----. IPython 7.18.1. jupyter_client 6.1.7. jupyter_core 4.6.3. jupyterlab 2.2.8. notebook 6.1.4. -----. Python 3.8.5 | packaged by conda-forge | (default, Aug 29 2020, 01:22:49) [GCC 7.5.0]. Linux-4.4.0-142-generic-x86_64-with-glibc2.10. 64 logical CPU cores, x86_64. -----. Session information updated at 2020-09-16 11:03. ```. Here is the error message:. ```. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:1303,security,soc,socks,1303,"kcall 0.2.0. bioservices 1.7.9. brotli NA. bs4 4.9.1. certifi 2020.06.20. cffi 1.14.1. chardet 3.0.4. colorama 0.4.3. colorlog NA. cryptography 3.1. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. decorator 4.4.2. easydev 0.10.1. get_version 2.1. gseapy 0.10.1. h5py 2.10.0. idna 2.10. ipykernel 5.3.4. ipython_genutils 0.2.0. jedi 0.15.2. jinja2 2.11.2. joblib 0.16.0. jsonschema 3.2.0. kiwisolver 1.2.0. legacy_api_wrap 1.2. llvmlite 0.34.0. lxml 4.5.2. markupsafe 1.1.1. matplotlib 3.3.2. mpl_toolkits NA. natsort 7.0.1. nbformat 5.0.7. networkx 2.5. numba 0.51.2. numexpr 2.7.1. numpy 1.19.2. packaging 20.4. pandas 1.0.1. parso 0.5.2. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prometheus_client NA. prompt_toolkit 3.0.7. ptyprocess 0.6.0. pvectorc NA. pygments 2.7.0. pylab NA. pyparsing 2.4.7. pyrsistent NA. pytz 2020.1. requests 2.23.0. requests_cache 0.5.2. scanpy 1.6.0. scipy 1.5.2. seaborn 0.11.0. send2trash NA. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.23.2. socks 1.7.1. soupsieve 2.0.1. statsmodels 0.12.0. storemagic NA. tables 3.6.1. terminado 0.8.3. tornado 6.0.4. traitlets 5.0.4. urllib3 1.25.10. wcwidth 0.2.5. wrapt 1.12.1. xlsxwriter 1.3.3. zmq 19.0.2. -----. IPython 7.18.1. jupyter_client 6.1.7. jupyter_core 4.6.3. jupyterlab 2.2.8. notebook 6.1.4. -----. Python 3.8.5 | packaged by conda-forge | (default, Aug 29 2020, 01:22:49) [GCC 7.5.0]. Linux-4.4.0-142-generic-x86_64-with-glibc2.10. 64 logical CPU cores, x86_64. -----. Session information updated at 2020-09-16 11:03. ```. Here is the error message:. ```. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-37-b22ada65a1cd> in <module>. 1 # Create Concatenated anndata object for all timepoints. 2 #alldays = e125.concatenate(e135, e145, e155, uns_merge=""unique""). ----> 3 alldays = e125.concatenate(e135). ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/anndata.py",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:1750,security,log,logical,1750,"ml 4.5.2. markupsafe 1.1.1. matplotlib 3.3.2. mpl_toolkits NA. natsort 7.0.1. nbformat 5.0.7. networkx 2.5. numba 0.51.2. numexpr 2.7.1. numpy 1.19.2. packaging 20.4. pandas 1.0.1. parso 0.5.2. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prometheus_client NA. prompt_toolkit 3.0.7. ptyprocess 0.6.0. pvectorc NA. pygments 2.7.0. pylab NA. pyparsing 2.4.7. pyrsistent NA. pytz 2020.1. requests 2.23.0. requests_cache 0.5.2. scanpy 1.6.0. scipy 1.5.2. seaborn 0.11.0. send2trash NA. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.23.2. socks 1.7.1. soupsieve 2.0.1. statsmodels 0.12.0. storemagic NA. tables 3.6.1. terminado 0.8.3. tornado 6.0.4. traitlets 5.0.4. urllib3 1.25.10. wcwidth 0.2.5. wrapt 1.12.1. xlsxwriter 1.3.3. zmq 19.0.2. -----. IPython 7.18.1. jupyter_client 6.1.7. jupyter_core 4.6.3. jupyterlab 2.2.8. notebook 6.1.4. -----. Python 3.8.5 | packaged by conda-forge | (default, Aug 29 2020, 01:22:49) [GCC 7.5.0]. Linux-4.4.0-142-generic-x86_64-with-glibc2.10. 64 logical CPU cores, x86_64. -----. Session information updated at 2020-09-16 11:03. ```. Here is the error message:. ```. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-37-b22ada65a1cd> in <module>. 1 # Create Concatenated anndata object for all timepoints. 2 #alldays = e125.concatenate(e135, e145, e155, uns_merge=""unique""). ----> 3 alldays = e125.concatenate(e135). ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/anndata.py in concatenate(self, join, batch_key, batch_categories, uns_merge, index_unique, fill_value, *adatas). 1696 all_adatas = (self,) + tuple(adatas). 1697 . -> 1698 out = concat(. 1699 all_adatas,. 1700 axis=0,. ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in concat(adatas, axis, join, merge, uns_merge, label, keys, index_unique, fill_value, pairwise). 799 [dim_indices(a, axis=1 - axis) for a in adatas],",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:1784,security,Session,Session,1784,"tlib 3.3.2. mpl_toolkits NA. natsort 7.0.1. nbformat 5.0.7. networkx 2.5. numba 0.51.2. numexpr 2.7.1. numpy 1.19.2. packaging 20.4. pandas 1.0.1. parso 0.5.2. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prometheus_client NA. prompt_toolkit 3.0.7. ptyprocess 0.6.0. pvectorc NA. pygments 2.7.0. pylab NA. pyparsing 2.4.7. pyrsistent NA. pytz 2020.1. requests 2.23.0. requests_cache 0.5.2. scanpy 1.6.0. scipy 1.5.2. seaborn 0.11.0. send2trash NA. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.23.2. socks 1.7.1. soupsieve 2.0.1. statsmodels 0.12.0. storemagic NA. tables 3.6.1. terminado 0.8.3. tornado 6.0.4. traitlets 5.0.4. urllib3 1.25.10. wcwidth 0.2.5. wrapt 1.12.1. xlsxwriter 1.3.3. zmq 19.0.2. -----. IPython 7.18.1. jupyter_client 6.1.7. jupyter_core 4.6.3. jupyterlab 2.2.8. notebook 6.1.4. -----. Python 3.8.5 | packaged by conda-forge | (default, Aug 29 2020, 01:22:49) [GCC 7.5.0]. Linux-4.4.0-142-generic-x86_64-with-glibc2.10. 64 logical CPU cores, x86_64. -----. Session information updated at 2020-09-16 11:03. ```. Here is the error message:. ```. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-37-b22ada65a1cd> in <module>. 1 # Create Concatenated anndata object for all timepoints. 2 #alldays = e125.concatenate(e135, e145, e155, uns_merge=""unique""). ----> 3 alldays = e125.concatenate(e135). ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/anndata.py in concatenate(self, join, batch_key, batch_categories, uns_merge, index_unique, fill_value, *adatas). 1696 all_adatas = (self,) + tuple(adatas). 1697 . -> 1698 out = concat(. 1699 all_adatas,. 1700 axis=0,. ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in concat(adatas, axis, join, merge, uns_merge, label, keys, index_unique, fill_value, pairwise). 799 [dim_indices(a, axis=1 - axis) for a in adatas], join=join. 800 ). --> 801 reindex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:1804,security,updat,updated,1804,"kits NA. natsort 7.0.1. nbformat 5.0.7. networkx 2.5. numba 0.51.2. numexpr 2.7.1. numpy 1.19.2. packaging 20.4. pandas 1.0.1. parso 0.5.2. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prometheus_client NA. prompt_toolkit 3.0.7. ptyprocess 0.6.0. pvectorc NA. pygments 2.7.0. pylab NA. pyparsing 2.4.7. pyrsistent NA. pytz 2020.1. requests 2.23.0. requests_cache 0.5.2. scanpy 1.6.0. scipy 1.5.2. seaborn 0.11.0. send2trash NA. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.23.2. socks 1.7.1. soupsieve 2.0.1. statsmodels 0.12.0. storemagic NA. tables 3.6.1. terminado 0.8.3. tornado 6.0.4. traitlets 5.0.4. urllib3 1.25.10. wcwidth 0.2.5. wrapt 1.12.1. xlsxwriter 1.3.3. zmq 19.0.2. -----. IPython 7.18.1. jupyter_client 6.1.7. jupyter_core 4.6.3. jupyterlab 2.2.8. notebook 6.1.4. -----. Python 3.8.5 | packaged by conda-forge | (default, Aug 29 2020, 01:22:49) [GCC 7.5.0]. Linux-4.4.0-142-generic-x86_64-with-glibc2.10. 64 logical CPU cores, x86_64. -----. Session information updated at 2020-09-16 11:03. ```. Here is the error message:. ```. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-37-b22ada65a1cd> in <module>. 1 # Create Concatenated anndata object for all timepoints. 2 #alldays = e125.concatenate(e135, e145, e155, uns_merge=""unique""). ----> 3 alldays = e125.concatenate(e135). ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/anndata.py in concatenate(self, join, batch_key, batch_categories, uns_merge, index_unique, fill_value, *adatas). 1696 all_adatas = (self,) + tuple(adatas). 1697 . -> 1698 out = concat(. 1699 all_adatas,. 1700 axis=0,. ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in concat(adatas, axis, join, merge, uns_merge, label, keys, index_unique, fill_value, pairwise). 799 [dim_indices(a, axis=1 - axis) for a in adatas], join=join. 800 ). --> 801 reindexers = [. 802 gen_rei",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:1750,testability,log,logical,1750,"ml 4.5.2. markupsafe 1.1.1. matplotlib 3.3.2. mpl_toolkits NA. natsort 7.0.1. nbformat 5.0.7. networkx 2.5. numba 0.51.2. numexpr 2.7.1. numpy 1.19.2. packaging 20.4. pandas 1.0.1. parso 0.5.2. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prometheus_client NA. prompt_toolkit 3.0.7. ptyprocess 0.6.0. pvectorc NA. pygments 2.7.0. pylab NA. pyparsing 2.4.7. pyrsistent NA. pytz 2020.1. requests 2.23.0. requests_cache 0.5.2. scanpy 1.6.0. scipy 1.5.2. seaborn 0.11.0. send2trash NA. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.23.2. socks 1.7.1. soupsieve 2.0.1. statsmodels 0.12.0. storemagic NA. tables 3.6.1. terminado 0.8.3. tornado 6.0.4. traitlets 5.0.4. urllib3 1.25.10. wcwidth 0.2.5. wrapt 1.12.1. xlsxwriter 1.3.3. zmq 19.0.2. -----. IPython 7.18.1. jupyter_client 6.1.7. jupyter_core 4.6.3. jupyterlab 2.2.8. notebook 6.1.4. -----. Python 3.8.5 | packaged by conda-forge | (default, Aug 29 2020, 01:22:49) [GCC 7.5.0]. Linux-4.4.0-142-generic-x86_64-with-glibc2.10. 64 logical CPU cores, x86_64. -----. Session information updated at 2020-09-16 11:03. ```. Here is the error message:. ```. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-37-b22ada65a1cd> in <module>. 1 # Create Concatenated anndata object for all timepoints. 2 #alldays = e125.concatenate(e135, e145, e155, uns_merge=""unique""). ----> 3 alldays = e125.concatenate(e135). ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/anndata.py in concatenate(self, join, batch_key, batch_categories, uns_merge, index_unique, fill_value, *adatas). 1696 all_adatas = (self,) + tuple(adatas). 1697 . -> 1698 out = concat(. 1699 all_adatas,. 1700 axis=0,. ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in concat(adatas, axis, join, merge, uns_merge, label, keys, index_unique, fill_value, pairwise). 799 [dim_indices(a, axis=1 - axis) for a in adatas],",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:1966,testability,Trace,Traceback,1966,"are 0.7.5. pkg_resources NA. prometheus_client NA. prompt_toolkit 3.0.7. ptyprocess 0.6.0. pvectorc NA. pygments 2.7.0. pylab NA. pyparsing 2.4.7. pyrsistent NA. pytz 2020.1. requests 2.23.0. requests_cache 0.5.2. scanpy 1.6.0. scipy 1.5.2. seaborn 0.11.0. send2trash NA. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.23.2. socks 1.7.1. soupsieve 2.0.1. statsmodels 0.12.0. storemagic NA. tables 3.6.1. terminado 0.8.3. tornado 6.0.4. traitlets 5.0.4. urllib3 1.25.10. wcwidth 0.2.5. wrapt 1.12.1. xlsxwriter 1.3.3. zmq 19.0.2. -----. IPython 7.18.1. jupyter_client 6.1.7. jupyter_core 4.6.3. jupyterlab 2.2.8. notebook 6.1.4. -----. Python 3.8.5 | packaged by conda-forge | (default, Aug 29 2020, 01:22:49) [GCC 7.5.0]. Linux-4.4.0-142-generic-x86_64-with-glibc2.10. 64 logical CPU cores, x86_64. -----. Session information updated at 2020-09-16 11:03. ```. Here is the error message:. ```. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-37-b22ada65a1cd> in <module>. 1 # Create Concatenated anndata object for all timepoints. 2 #alldays = e125.concatenate(e135, e145, e155, uns_merge=""unique""). ----> 3 alldays = e125.concatenate(e135). ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/anndata.py in concatenate(self, join, batch_key, batch_categories, uns_merge, index_unique, fill_value, *adatas). 1696 all_adatas = (self,) + tuple(adatas). 1697 . -> 1698 out = concat(. 1699 all_adatas,. 1700 axis=0,. ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in concat(adatas, axis, join, merge, uns_merge, label, keys, index_unique, fill_value, pairwise). 799 [dim_indices(a, axis=1 - axis) for a in adatas], join=join. 800 ). --> 801 reindexers = [. 802 gen_reindexer(alt_indices, dim_indices(a, axis=1 - axis)) for a in adatas. 803 ]. ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:1850,usability,error,error,1850,"rkx 2.5. numba 0.51.2. numexpr 2.7.1. numpy 1.19.2. packaging 20.4. pandas 1.0.1. parso 0.5.2. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prometheus_client NA. prompt_toolkit 3.0.7. ptyprocess 0.6.0. pvectorc NA. pygments 2.7.0. pylab NA. pyparsing 2.4.7. pyrsistent NA. pytz 2020.1. requests 2.23.0. requests_cache 0.5.2. scanpy 1.6.0. scipy 1.5.2. seaborn 0.11.0. send2trash NA. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.23.2. socks 1.7.1. soupsieve 2.0.1. statsmodels 0.12.0. storemagic NA. tables 3.6.1. terminado 0.8.3. tornado 6.0.4. traitlets 5.0.4. urllib3 1.25.10. wcwidth 0.2.5. wrapt 1.12.1. xlsxwriter 1.3.3. zmq 19.0.2. -----. IPython 7.18.1. jupyter_client 6.1.7. jupyter_core 4.6.3. jupyterlab 2.2.8. notebook 6.1.4. -----. Python 3.8.5 | packaged by conda-forge | (default, Aug 29 2020, 01:22:49) [GCC 7.5.0]. Linux-4.4.0-142-generic-x86_64-with-glibc2.10. 64 logical CPU cores, x86_64. -----. Session information updated at 2020-09-16 11:03. ```. Here is the error message:. ```. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-37-b22ada65a1cd> in <module>. 1 # Create Concatenated anndata object for all timepoints. 2 #alldays = e125.concatenate(e135, e145, e155, uns_merge=""unique""). ----> 3 alldays = e125.concatenate(e135). ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/anndata.py in concatenate(self, join, batch_key, batch_categories, uns_merge, index_unique, fill_value, *adatas). 1696 all_adatas = (self,) + tuple(adatas). 1697 . -> 1698 out = concat(. 1699 all_adatas,. 1700 axis=0,. ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in concat(adatas, axis, join, merge, uns_merge, label, keys, index_unique, fill_value, pairwise). 799 [dim_indices(a, axis=1 - axis) for a in adatas], join=join. 800 ). --> 801 reindexers = [. 802 gen_reindexer(alt_indices, dim_indices(a, axis=1 - a",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:2010,usability,input,input-,2010,"ient NA. prompt_toolkit 3.0.7. ptyprocess 0.6.0. pvectorc NA. pygments 2.7.0. pylab NA. pyparsing 2.4.7. pyrsistent NA. pytz 2020.1. requests 2.23.0. requests_cache 0.5.2. scanpy 1.6.0. scipy 1.5.2. seaborn 0.11.0. send2trash NA. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.23.2. socks 1.7.1. soupsieve 2.0.1. statsmodels 0.12.0. storemagic NA. tables 3.6.1. terminado 0.8.3. tornado 6.0.4. traitlets 5.0.4. urllib3 1.25.10. wcwidth 0.2.5. wrapt 1.12.1. xlsxwriter 1.3.3. zmq 19.0.2. -----. IPython 7.18.1. jupyter_client 6.1.7. jupyter_core 4.6.3. jupyterlab 2.2.8. notebook 6.1.4. -----. Python 3.8.5 | packaged by conda-forge | (default, Aug 29 2020, 01:22:49) [GCC 7.5.0]. Linux-4.4.0-142-generic-x86_64-with-glibc2.10. 64 logical CPU cores, x86_64. -----. Session information updated at 2020-09-16 11:03. ```. Here is the error message:. ```. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-37-b22ada65a1cd> in <module>. 1 # Create Concatenated anndata object for all timepoints. 2 #alldays = e125.concatenate(e135, e145, e155, uns_merge=""unique""). ----> 3 alldays = e125.concatenate(e135). ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/anndata.py in concatenate(self, join, batch_key, batch_categories, uns_merge, index_unique, fill_value, *adatas). 1696 all_adatas = (self,) + tuple(adatas). 1697 . -> 1698 out = concat(. 1699 all_adatas,. 1700 axis=0,. ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in concat(adatas, axis, join, merge, uns_merge, label, keys, index_unique, fill_value, pairwise). 799 [dim_indices(a, axis=1 - axis) for a in adatas], join=join. 800 ). --> 801 reindexers = [. 802 gen_reindexer(alt_indices, dim_indices(a, axis=1 - axis)) for a in adatas. 803 ]. ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in <listcomp>(.0). 800 ). 801 reindexers ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:8,deployability,updat,updating,8,@xie186 updating pandas should fix the problem.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:8,safety,updat,updating,8,@xie186 updating pandas should fix the problem.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:8,security,updat,updating,8,@xie186 updating pandas should fix the problem.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:35,deployability,Updat,Updating,35,@Koncopd Thanks for your response. Updating pandas to 1.1.2 doesn't work in my case. I'm wondering whether there is a docker image or Dockerfile for the latest version from your lab.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:160,deployability,version,version,160,@Koncopd Thanks for your response. Updating pandas to 1.1.2 doesn't work in my case. I'm wondering whether there is a docker image or Dockerfile for the latest version from your lab.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:160,integrability,version,version,160,@Koncopd Thanks for your response. Updating pandas to 1.1.2 doesn't work in my case. I'm wondering whether there is a docker image or Dockerfile for the latest version from your lab.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:160,modifiability,version,version,160,@Koncopd Thanks for your response. Updating pandas to 1.1.2 doesn't work in my case. I'm wondering whether there is a docker image or Dockerfile for the latest version from your lab.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:60,reliability,doe,doesn,60,@Koncopd Thanks for your response. Updating pandas to 1.1.2 doesn't work in my case. I'm wondering whether there is a docker image or Dockerfile for the latest version from your lab.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:35,safety,Updat,Updating,35,@Koncopd Thanks for your response. Updating pandas to 1.1.2 doesn't work in my case. I'm wondering whether there is a docker image or Dockerfile for the latest version from your lab.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:35,security,Updat,Updating,35,@Koncopd Thanks for your response. Updating pandas to 1.1.2 doesn't work in my case. I'm wondering whether there is a docker image or Dockerfile for the latest version from your lab.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:25,availability,error,error,25,"To make you reproduce my error, here is what I did with a Docker image:. 1. `singularity -d build sc_velocyto0.17.17_scannpy1.6.0_scvelo0.2.2.img docker://xie186/scrna_tutorial:0.1.0`. 2. sudo ufw allow 8689. 3. `singularity run --nv sc_velocyto0.17.17_scannpy1.6.0_scvelo0.2.2.img jupyter-lab --ip=<your_ip> --port=8689 --no-browser `. Then in browser, open '<your_ip>:8689' and run the code below in jupyterlab. . The data and code I ran is shown as below:. * Go to: https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE132188. * Search ""GSE132188_RAW.tar"". * Download the file. Uncompress the files:. ```. tar xvf GSE132188_RAW.tar. ```. You will get the following files. ```. GSM3852752_E12_5_counts.tar.gz. GSM3852753_E13_5_counts.tar.gz. GSM3852754_E14_5_counts.tar.gz. GSM3852755_E15_5_counts.tar.gz. ```. Uncompress: . ```. mkdir -p E12_5_counts/. tar zxvf GSM3852752_E12_5_counts.tar.gz --directory E12_5_counts/. mkdir -p E13_5_counts/. tar zxvf GSM3852753_E13_5_counts.tar.gz --directory E13_5_counts/. mkdir -p E14_5_counts/. tar zxvf GSM3852754_E14_5_counts.tar.gz --directory E14_5_counts/. mkdir -p E15_5_counts/. tar zxvf GSM3852755_E15_5_counts.tar.gz --directory E15_5_counts/. ```. Code:. ```. import numpy as np. import matplotlib.pyplot as pl. import numpy as np. import scanpy as sc. import scanpy.external as sce. import pandas as pd. from anndata import AnnData. import seaborn as sns. from scipy.sparse import csr_matrix. import networkx as nx. import xlsxwriter. from matplotlib import rcParams. import seaborn as sns. import scipy as sci. #GSEApy: Gene Set Enrichment Analysis in Python. #import gseapy as gp. sc.settings.verbosity = 3. sc.logging.print_versions(). # Read cellranger files for all four samples. filename = './E12_5_counts/mm10/matrix.mtx'. filename_genes = './E12_5_counts/mm10/genes.tsv'. filename_barcodes = './E12_5_counts/mm10/barcodes.tsv'. e125 = sc.read(filename).transpose(). e125.var_names = np.genfromtxt(filename_genes, dtype=str)[:, 1]. e125.o",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:563,availability,Down,Download,563,"To make you reproduce my error, here is what I did with a Docker image:. 1. `singularity -d build sc_velocyto0.17.17_scannpy1.6.0_scvelo0.2.2.img docker://xie186/scrna_tutorial:0.1.0`. 2. sudo ufw allow 8689. 3. `singularity run --nv sc_velocyto0.17.17_scannpy1.6.0_scvelo0.2.2.img jupyter-lab --ip=<your_ip> --port=8689 --no-browser `. Then in browser, open '<your_ip>:8689' and run the code below in jupyterlab. . The data and code I ran is shown as below:. * Go to: https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE132188. * Search ""GSE132188_RAW.tar"". * Download the file. Uncompress the files:. ```. tar xvf GSE132188_RAW.tar. ```. You will get the following files. ```. GSM3852752_E12_5_counts.tar.gz. GSM3852753_E13_5_counts.tar.gz. GSM3852754_E14_5_counts.tar.gz. GSM3852755_E15_5_counts.tar.gz. ```. Uncompress: . ```. mkdir -p E12_5_counts/. tar zxvf GSM3852752_E12_5_counts.tar.gz --directory E12_5_counts/. mkdir -p E13_5_counts/. tar zxvf GSM3852753_E13_5_counts.tar.gz --directory E13_5_counts/. mkdir -p E14_5_counts/. tar zxvf GSM3852754_E14_5_counts.tar.gz --directory E14_5_counts/. mkdir -p E15_5_counts/. tar zxvf GSM3852755_E15_5_counts.tar.gz --directory E15_5_counts/. ```. Code:. ```. import numpy as np. import matplotlib.pyplot as pl. import numpy as np. import scanpy as sc. import scanpy.external as sce. import pandas as pd. from anndata import AnnData. import seaborn as sns. from scipy.sparse import csr_matrix. import networkx as nx. import xlsxwriter. from matplotlib import rcParams. import seaborn as sns. import scipy as sci. #GSEApy: Gene Set Enrichment Analysis in Python. #import gseapy as gp. sc.settings.verbosity = 3. sc.logging.print_versions(). # Read cellranger files for all four samples. filename = './E12_5_counts/mm10/matrix.mtx'. filename_genes = './E12_5_counts/mm10/genes.tsv'. filename_barcodes = './E12_5_counts/mm10/barcodes.tsv'. e125 = sc.read(filename).transpose(). e125.var_names = np.genfromtxt(filename_genes, dtype=str)[:, 1]. e125.o",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:92,deployability,build,build,92,"To make you reproduce my error, here is what I did with a Docker image:. 1. `singularity -d build sc_velocyto0.17.17_scannpy1.6.0_scvelo0.2.2.img docker://xie186/scrna_tutorial:0.1.0`. 2. sudo ufw allow 8689. 3. `singularity run --nv sc_velocyto0.17.17_scannpy1.6.0_scvelo0.2.2.img jupyter-lab --ip=<your_ip> --port=8689 --no-browser `. Then in browser, open '<your_ip>:8689' and run the code below in jupyterlab. . The data and code I ran is shown as below:. * Go to: https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE132188. * Search ""GSE132188_RAW.tar"". * Download the file. Uncompress the files:. ```. tar xvf GSE132188_RAW.tar. ```. You will get the following files. ```. GSM3852752_E12_5_counts.tar.gz. GSM3852753_E13_5_counts.tar.gz. GSM3852754_E14_5_counts.tar.gz. GSM3852755_E15_5_counts.tar.gz. ```. Uncompress: . ```. mkdir -p E12_5_counts/. tar zxvf GSM3852752_E12_5_counts.tar.gz --directory E12_5_counts/. mkdir -p E13_5_counts/. tar zxvf GSM3852753_E13_5_counts.tar.gz --directory E13_5_counts/. mkdir -p E14_5_counts/. tar zxvf GSM3852754_E14_5_counts.tar.gz --directory E14_5_counts/. mkdir -p E15_5_counts/. tar zxvf GSM3852755_E15_5_counts.tar.gz --directory E15_5_counts/. ```. Code:. ```. import numpy as np. import matplotlib.pyplot as pl. import numpy as np. import scanpy as sc. import scanpy.external as sce. import pandas as pd. from anndata import AnnData. import seaborn as sns. from scipy.sparse import csr_matrix. import networkx as nx. import xlsxwriter. from matplotlib import rcParams. import seaborn as sns. import scipy as sci. #GSEApy: Gene Set Enrichment Analysis in Python. #import gseapy as gp. sc.settings.verbosity = 3. sc.logging.print_versions(). # Read cellranger files for all four samples. filename = './E12_5_counts/mm10/matrix.mtx'. filename_genes = './E12_5_counts/mm10/genes.tsv'. filename_barcodes = './E12_5_counts/mm10/barcodes.tsv'. e125 = sc.read(filename).transpose(). e125.var_names = np.genfromtxt(filename_genes, dtype=str)[:, 1]. e125.o",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:1668,deployability,log,logging,1668,"es. ```. GSM3852752_E12_5_counts.tar.gz. GSM3852753_E13_5_counts.tar.gz. GSM3852754_E14_5_counts.tar.gz. GSM3852755_E15_5_counts.tar.gz. ```. Uncompress: . ```. mkdir -p E12_5_counts/. tar zxvf GSM3852752_E12_5_counts.tar.gz --directory E12_5_counts/. mkdir -p E13_5_counts/. tar zxvf GSM3852753_E13_5_counts.tar.gz --directory E13_5_counts/. mkdir -p E14_5_counts/. tar zxvf GSM3852754_E14_5_counts.tar.gz --directory E14_5_counts/. mkdir -p E15_5_counts/. tar zxvf GSM3852755_E15_5_counts.tar.gz --directory E15_5_counts/. ```. Code:. ```. import numpy as np. import matplotlib.pyplot as pl. import numpy as np. import scanpy as sc. import scanpy.external as sce. import pandas as pd. from anndata import AnnData. import seaborn as sns. from scipy.sparse import csr_matrix. import networkx as nx. import xlsxwriter. from matplotlib import rcParams. import seaborn as sns. import scipy as sci. #GSEApy: Gene Set Enrichment Analysis in Python. #import gseapy as gp. sc.settings.verbosity = 3. sc.logging.print_versions(). # Read cellranger files for all four samples. filename = './E12_5_counts/mm10/matrix.mtx'. filename_genes = './E12_5_counts/mm10/genes.tsv'. filename_barcodes = './E12_5_counts/mm10/barcodes.tsv'. e125 = sc.read(filename).transpose(). e125.var_names = np.genfromtxt(filename_genes, dtype=str)[:, 1]. e125.obs_names = np.genfromtxt(filename_barcodes, dtype=str). filename = './E13_5_counts/mm10/matrix.mtx'. filename_genes = './E13_5_counts/mm10/genes.tsv'. filename_barcodes = './E13_5_counts/mm10/barcodes.tsv'. e135 = sc.read(filename).transpose(). e135.var_names = np.genfromtxt(filename_genes, dtype=str)[:, 1]. e135.obs_names = np.genfromtxt(filename_barcodes, dtype=str). filename = './E14_5_counts/mm10/matrix.mtx'. filename_genes = './E14_5_counts/mm10/genes.tsv'. filename_barcodes = './E14_5_counts/mm10/barcodes.tsv'. e145 = sc.read(filename).transpose(). e145.var_names = np.genfromtxt(filename_genes, dtype=str)[:, 1]. e145.obs_names = np.genfromtxt(filename_barcod",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:25,performance,error,error,25,"To make you reproduce my error, here is what I did with a Docker image:. 1. `singularity -d build sc_velocyto0.17.17_scannpy1.6.0_scvelo0.2.2.img docker://xie186/scrna_tutorial:0.1.0`. 2. sudo ufw allow 8689. 3. `singularity run --nv sc_velocyto0.17.17_scannpy1.6.0_scvelo0.2.2.img jupyter-lab --ip=<your_ip> --port=8689 --no-browser `. Then in browser, open '<your_ip>:8689' and run the code below in jupyterlab. . The data and code I ran is shown as below:. * Go to: https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE132188. * Search ""GSE132188_RAW.tar"". * Download the file. Uncompress the files:. ```. tar xvf GSE132188_RAW.tar. ```. You will get the following files. ```. GSM3852752_E12_5_counts.tar.gz. GSM3852753_E13_5_counts.tar.gz. GSM3852754_E14_5_counts.tar.gz. GSM3852755_E15_5_counts.tar.gz. ```. Uncompress: . ```. mkdir -p E12_5_counts/. tar zxvf GSM3852752_E12_5_counts.tar.gz --directory E12_5_counts/. mkdir -p E13_5_counts/. tar zxvf GSM3852753_E13_5_counts.tar.gz --directory E13_5_counts/. mkdir -p E14_5_counts/. tar zxvf GSM3852754_E14_5_counts.tar.gz --directory E14_5_counts/. mkdir -p E15_5_counts/. tar zxvf GSM3852755_E15_5_counts.tar.gz --directory E15_5_counts/. ```. Code:. ```. import numpy as np. import matplotlib.pyplot as pl. import numpy as np. import scanpy as sc. import scanpy.external as sce. import pandas as pd. from anndata import AnnData. import seaborn as sns. from scipy.sparse import csr_matrix. import networkx as nx. import xlsxwriter. from matplotlib import rcParams. import seaborn as sns. import scipy as sci. #GSEApy: Gene Set Enrichment Analysis in Python. #import gseapy as gp. sc.settings.verbosity = 3. sc.logging.print_versions(). # Read cellranger files for all four samples. filename = './E12_5_counts/mm10/matrix.mtx'. filename_genes = './E12_5_counts/mm10/genes.tsv'. filename_barcodes = './E12_5_counts/mm10/barcodes.tsv'. e125 = sc.read(filename).transpose(). e125.var_names = np.genfromtxt(filename_genes, dtype=str)[:, 1]. e125.o",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:1455,performance,network,networkx,1455," * Go to: https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE132188. * Search ""GSE132188_RAW.tar"". * Download the file. Uncompress the files:. ```. tar xvf GSE132188_RAW.tar. ```. You will get the following files. ```. GSM3852752_E12_5_counts.tar.gz. GSM3852753_E13_5_counts.tar.gz. GSM3852754_E14_5_counts.tar.gz. GSM3852755_E15_5_counts.tar.gz. ```. Uncompress: . ```. mkdir -p E12_5_counts/. tar zxvf GSM3852752_E12_5_counts.tar.gz --directory E12_5_counts/. mkdir -p E13_5_counts/. tar zxvf GSM3852753_E13_5_counts.tar.gz --directory E13_5_counts/. mkdir -p E14_5_counts/. tar zxvf GSM3852754_E14_5_counts.tar.gz --directory E14_5_counts/. mkdir -p E15_5_counts/. tar zxvf GSM3852755_E15_5_counts.tar.gz --directory E15_5_counts/. ```. Code:. ```. import numpy as np. import matplotlib.pyplot as pl. import numpy as np. import scanpy as sc. import scanpy.external as sce. import pandas as pd. from anndata import AnnData. import seaborn as sns. from scipy.sparse import csr_matrix. import networkx as nx. import xlsxwriter. from matplotlib import rcParams. import seaborn as sns. import scipy as sci. #GSEApy: Gene Set Enrichment Analysis in Python. #import gseapy as gp. sc.settings.verbosity = 3. sc.logging.print_versions(). # Read cellranger files for all four samples. filename = './E12_5_counts/mm10/matrix.mtx'. filename_genes = './E12_5_counts/mm10/genes.tsv'. filename_barcodes = './E12_5_counts/mm10/barcodes.tsv'. e125 = sc.read(filename).transpose(). e125.var_names = np.genfromtxt(filename_genes, dtype=str)[:, 1]. e125.obs_names = np.genfromtxt(filename_barcodes, dtype=str). filename = './E13_5_counts/mm10/matrix.mtx'. filename_genes = './E13_5_counts/mm10/genes.tsv'. filename_barcodes = './E13_5_counts/mm10/barcodes.tsv'. e135 = sc.read(filename).transpose(). e135.var_names = np.genfromtxt(filename_genes, dtype=str)[:, 1]. e135.obs_names = np.genfromtxt(filename_barcodes, dtype=str). filename = './E14_5_counts/mm10/matrix.mtx'. filename_genes = './E14_5_counts/mm10/gene",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:3015,performance,time,timepoint,3015,"Code:. ```. import numpy as np. import matplotlib.pyplot as pl. import numpy as np. import scanpy as sc. import scanpy.external as sce. import pandas as pd. from anndata import AnnData. import seaborn as sns. from scipy.sparse import csr_matrix. import networkx as nx. import xlsxwriter. from matplotlib import rcParams. import seaborn as sns. import scipy as sci. #GSEApy: Gene Set Enrichment Analysis in Python. #import gseapy as gp. sc.settings.verbosity = 3. sc.logging.print_versions(). # Read cellranger files for all four samples. filename = './E12_5_counts/mm10/matrix.mtx'. filename_genes = './E12_5_counts/mm10/genes.tsv'. filename_barcodes = './E12_5_counts/mm10/barcodes.tsv'. e125 = sc.read(filename).transpose(). e125.var_names = np.genfromtxt(filename_genes, dtype=str)[:, 1]. e125.obs_names = np.genfromtxt(filename_barcodes, dtype=str). filename = './E13_5_counts/mm10/matrix.mtx'. filename_genes = './E13_5_counts/mm10/genes.tsv'. filename_barcodes = './E13_5_counts/mm10/barcodes.tsv'. e135 = sc.read(filename).transpose(). e135.var_names = np.genfromtxt(filename_genes, dtype=str)[:, 1]. e135.obs_names = np.genfromtxt(filename_barcodes, dtype=str). filename = './E14_5_counts/mm10/matrix.mtx'. filename_genes = './E14_5_counts/mm10/genes.tsv'. filename_barcodes = './E14_5_counts/mm10/barcodes.tsv'. e145 = sc.read(filename).transpose(). e145.var_names = np.genfromtxt(filename_genes, dtype=str)[:, 1]. e145.obs_names = np.genfromtxt(filename_barcodes, dtype=str). filename = './E15_5_counts/mm10/matrix.mtx'. filename_genes = './E15_5_counts/mm10/genes.tsv'. filename_barcodes = './E15_5_counts/mm10/barcodes.tsv'. e155 = sc.read(filename).transpose(). e155.var_names = np.genfromtxt(filename_genes, dtype=str)[:, 1]. e155.obs_names = np.genfromtxt(filename_barcodes, dtype=str). # Add dev. timepoint label for each sample. e125.obs['day'] = '12.5'. e135.obs['day'] = '13.5'. e145.obs['day'] = '14.5'. e155.obs['day'] = '15.5'. alldays = e125.concatenate(e135, e145, e155). ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:25,safety,error,error,25,"To make you reproduce my error, here is what I did with a Docker image:. 1. `singularity -d build sc_velocyto0.17.17_scannpy1.6.0_scvelo0.2.2.img docker://xie186/scrna_tutorial:0.1.0`. 2. sudo ufw allow 8689. 3. `singularity run --nv sc_velocyto0.17.17_scannpy1.6.0_scvelo0.2.2.img jupyter-lab --ip=<your_ip> --port=8689 --no-browser `. Then in browser, open '<your_ip>:8689' and run the code below in jupyterlab. . The data and code I ran is shown as below:. * Go to: https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE132188. * Search ""GSE132188_RAW.tar"". * Download the file. Uncompress the files:. ```. tar xvf GSE132188_RAW.tar. ```. You will get the following files. ```. GSM3852752_E12_5_counts.tar.gz. GSM3852753_E13_5_counts.tar.gz. GSM3852754_E14_5_counts.tar.gz. GSM3852755_E15_5_counts.tar.gz. ```. Uncompress: . ```. mkdir -p E12_5_counts/. tar zxvf GSM3852752_E12_5_counts.tar.gz --directory E12_5_counts/. mkdir -p E13_5_counts/. tar zxvf GSM3852753_E13_5_counts.tar.gz --directory E13_5_counts/. mkdir -p E14_5_counts/. tar zxvf GSM3852754_E14_5_counts.tar.gz --directory E14_5_counts/. mkdir -p E15_5_counts/. tar zxvf GSM3852755_E15_5_counts.tar.gz --directory E15_5_counts/. ```. Code:. ```. import numpy as np. import matplotlib.pyplot as pl. import numpy as np. import scanpy as sc. import scanpy.external as sce. import pandas as pd. from anndata import AnnData. import seaborn as sns. from scipy.sparse import csr_matrix. import networkx as nx. import xlsxwriter. from matplotlib import rcParams. import seaborn as sns. import scipy as sci. #GSEApy: Gene Set Enrichment Analysis in Python. #import gseapy as gp. sc.settings.verbosity = 3. sc.logging.print_versions(). # Read cellranger files for all four samples. filename = './E12_5_counts/mm10/matrix.mtx'. filename_genes = './E12_5_counts/mm10/genes.tsv'. filename_barcodes = './E12_5_counts/mm10/barcodes.tsv'. e125 = sc.read(filename).transpose(). e125.var_names = np.genfromtxt(filename_genes, dtype=str)[:, 1]. e125.o",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:1668,safety,log,logging,1668,"es. ```. GSM3852752_E12_5_counts.tar.gz. GSM3852753_E13_5_counts.tar.gz. GSM3852754_E14_5_counts.tar.gz. GSM3852755_E15_5_counts.tar.gz. ```. Uncompress: . ```. mkdir -p E12_5_counts/. tar zxvf GSM3852752_E12_5_counts.tar.gz --directory E12_5_counts/. mkdir -p E13_5_counts/. tar zxvf GSM3852753_E13_5_counts.tar.gz --directory E13_5_counts/. mkdir -p E14_5_counts/. tar zxvf GSM3852754_E14_5_counts.tar.gz --directory E14_5_counts/. mkdir -p E15_5_counts/. tar zxvf GSM3852755_E15_5_counts.tar.gz --directory E15_5_counts/. ```. Code:. ```. import numpy as np. import matplotlib.pyplot as pl. import numpy as np. import scanpy as sc. import scanpy.external as sce. import pandas as pd. from anndata import AnnData. import seaborn as sns. from scipy.sparse import csr_matrix. import networkx as nx. import xlsxwriter. from matplotlib import rcParams. import seaborn as sns. import scipy as sci. #GSEApy: Gene Set Enrichment Analysis in Python. #import gseapy as gp. sc.settings.verbosity = 3. sc.logging.print_versions(). # Read cellranger files for all four samples. filename = './E12_5_counts/mm10/matrix.mtx'. filename_genes = './E12_5_counts/mm10/genes.tsv'. filename_barcodes = './E12_5_counts/mm10/barcodes.tsv'. e125 = sc.read(filename).transpose(). e125.var_names = np.genfromtxt(filename_genes, dtype=str)[:, 1]. e125.obs_names = np.genfromtxt(filename_barcodes, dtype=str). filename = './E13_5_counts/mm10/matrix.mtx'. filename_genes = './E13_5_counts/mm10/genes.tsv'. filename_barcodes = './E13_5_counts/mm10/barcodes.tsv'. e135 = sc.read(filename).transpose(). e135.var_names = np.genfromtxt(filename_genes, dtype=str)[:, 1]. e135.obs_names = np.genfromtxt(filename_barcodes, dtype=str). filename = './E14_5_counts/mm10/matrix.mtx'. filename_genes = './E14_5_counts/mm10/genes.tsv'. filename_barcodes = './E14_5_counts/mm10/barcodes.tsv'. e145 = sc.read(filename).transpose(). e145.var_names = np.genfromtxt(filename_genes, dtype=str)[:, 1]. e145.obs_names = np.genfromtxt(filename_barcod",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:1455,security,network,networkx,1455," * Go to: https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE132188. * Search ""GSE132188_RAW.tar"". * Download the file. Uncompress the files:. ```. tar xvf GSE132188_RAW.tar. ```. You will get the following files. ```. GSM3852752_E12_5_counts.tar.gz. GSM3852753_E13_5_counts.tar.gz. GSM3852754_E14_5_counts.tar.gz. GSM3852755_E15_5_counts.tar.gz. ```. Uncompress: . ```. mkdir -p E12_5_counts/. tar zxvf GSM3852752_E12_5_counts.tar.gz --directory E12_5_counts/. mkdir -p E13_5_counts/. tar zxvf GSM3852753_E13_5_counts.tar.gz --directory E13_5_counts/. mkdir -p E14_5_counts/. tar zxvf GSM3852754_E14_5_counts.tar.gz --directory E14_5_counts/. mkdir -p E15_5_counts/. tar zxvf GSM3852755_E15_5_counts.tar.gz --directory E15_5_counts/. ```. Code:. ```. import numpy as np. import matplotlib.pyplot as pl. import numpy as np. import scanpy as sc. import scanpy.external as sce. import pandas as pd. from anndata import AnnData. import seaborn as sns. from scipy.sparse import csr_matrix. import networkx as nx. import xlsxwriter. from matplotlib import rcParams. import seaborn as sns. import scipy as sci. #GSEApy: Gene Set Enrichment Analysis in Python. #import gseapy as gp. sc.settings.verbosity = 3. sc.logging.print_versions(). # Read cellranger files for all four samples. filename = './E12_5_counts/mm10/matrix.mtx'. filename_genes = './E12_5_counts/mm10/genes.tsv'. filename_barcodes = './E12_5_counts/mm10/barcodes.tsv'. e125 = sc.read(filename).transpose(). e125.var_names = np.genfromtxt(filename_genes, dtype=str)[:, 1]. e125.obs_names = np.genfromtxt(filename_barcodes, dtype=str). filename = './E13_5_counts/mm10/matrix.mtx'. filename_genes = './E13_5_counts/mm10/genes.tsv'. filename_barcodes = './E13_5_counts/mm10/barcodes.tsv'. e135 = sc.read(filename).transpose(). e135.var_names = np.genfromtxt(filename_genes, dtype=str)[:, 1]. e135.obs_names = np.genfromtxt(filename_barcodes, dtype=str). filename = './E14_5_counts/mm10/matrix.mtx'. filename_genes = './E14_5_counts/mm10/gene",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:1668,security,log,logging,1668,"es. ```. GSM3852752_E12_5_counts.tar.gz. GSM3852753_E13_5_counts.tar.gz. GSM3852754_E14_5_counts.tar.gz. GSM3852755_E15_5_counts.tar.gz. ```. Uncompress: . ```. mkdir -p E12_5_counts/. tar zxvf GSM3852752_E12_5_counts.tar.gz --directory E12_5_counts/. mkdir -p E13_5_counts/. tar zxvf GSM3852753_E13_5_counts.tar.gz --directory E13_5_counts/. mkdir -p E14_5_counts/. tar zxvf GSM3852754_E14_5_counts.tar.gz --directory E14_5_counts/. mkdir -p E15_5_counts/. tar zxvf GSM3852755_E15_5_counts.tar.gz --directory E15_5_counts/. ```. Code:. ```. import numpy as np. import matplotlib.pyplot as pl. import numpy as np. import scanpy as sc. import scanpy.external as sce. import pandas as pd. from anndata import AnnData. import seaborn as sns. from scipy.sparse import csr_matrix. import networkx as nx. import xlsxwriter. from matplotlib import rcParams. import seaborn as sns. import scipy as sci. #GSEApy: Gene Set Enrichment Analysis in Python. #import gseapy as gp. sc.settings.verbosity = 3. sc.logging.print_versions(). # Read cellranger files for all four samples. filename = './E12_5_counts/mm10/matrix.mtx'. filename_genes = './E12_5_counts/mm10/genes.tsv'. filename_barcodes = './E12_5_counts/mm10/barcodes.tsv'. e125 = sc.read(filename).transpose(). e125.var_names = np.genfromtxt(filename_genes, dtype=str)[:, 1]. e125.obs_names = np.genfromtxt(filename_barcodes, dtype=str). filename = './E13_5_counts/mm10/matrix.mtx'. filename_genes = './E13_5_counts/mm10/genes.tsv'. filename_barcodes = './E13_5_counts/mm10/barcodes.tsv'. e135 = sc.read(filename).transpose(). e135.var_names = np.genfromtxt(filename_genes, dtype=str)[:, 1]. e135.obs_names = np.genfromtxt(filename_barcodes, dtype=str). filename = './E14_5_counts/mm10/matrix.mtx'. filename_genes = './E14_5_counts/mm10/genes.tsv'. filename_barcodes = './E14_5_counts/mm10/barcodes.tsv'. e145 = sc.read(filename).transpose(). e145.var_names = np.genfromtxt(filename_genes, dtype=str)[:, 1]. e145.obs_names = np.genfromtxt(filename_barcod",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:1668,testability,log,logging,1668,"es. ```. GSM3852752_E12_5_counts.tar.gz. GSM3852753_E13_5_counts.tar.gz. GSM3852754_E14_5_counts.tar.gz. GSM3852755_E15_5_counts.tar.gz. ```. Uncompress: . ```. mkdir -p E12_5_counts/. tar zxvf GSM3852752_E12_5_counts.tar.gz --directory E12_5_counts/. mkdir -p E13_5_counts/. tar zxvf GSM3852753_E13_5_counts.tar.gz --directory E13_5_counts/. mkdir -p E14_5_counts/. tar zxvf GSM3852754_E14_5_counts.tar.gz --directory E14_5_counts/. mkdir -p E15_5_counts/. tar zxvf GSM3852755_E15_5_counts.tar.gz --directory E15_5_counts/. ```. Code:. ```. import numpy as np. import matplotlib.pyplot as pl. import numpy as np. import scanpy as sc. import scanpy.external as sce. import pandas as pd. from anndata import AnnData. import seaborn as sns. from scipy.sparse import csr_matrix. import networkx as nx. import xlsxwriter. from matplotlib import rcParams. import seaborn as sns. import scipy as sci. #GSEApy: Gene Set Enrichment Analysis in Python. #import gseapy as gp. sc.settings.verbosity = 3. sc.logging.print_versions(). # Read cellranger files for all four samples. filename = './E12_5_counts/mm10/matrix.mtx'. filename_genes = './E12_5_counts/mm10/genes.tsv'. filename_barcodes = './E12_5_counts/mm10/barcodes.tsv'. e125 = sc.read(filename).transpose(). e125.var_names = np.genfromtxt(filename_genes, dtype=str)[:, 1]. e125.obs_names = np.genfromtxt(filename_barcodes, dtype=str). filename = './E13_5_counts/mm10/matrix.mtx'. filename_genes = './E13_5_counts/mm10/genes.tsv'. filename_barcodes = './E13_5_counts/mm10/barcodes.tsv'. e135 = sc.read(filename).transpose(). e135.var_names = np.genfromtxt(filename_genes, dtype=str)[:, 1]. e135.obs_names = np.genfromtxt(filename_barcodes, dtype=str). filename = './E14_5_counts/mm10/matrix.mtx'. filename_genes = './E14_5_counts/mm10/genes.tsv'. filename_barcodes = './E14_5_counts/mm10/barcodes.tsv'. e145 = sc.read(filename).transpose(). e145.var_names = np.genfromtxt(filename_genes, dtype=str)[:, 1]. e145.obs_names = np.genfromtxt(filename_barcod",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:25,usability,error,error,25,"To make you reproduce my error, here is what I did with a Docker image:. 1. `singularity -d build sc_velocyto0.17.17_scannpy1.6.0_scvelo0.2.2.img docker://xie186/scrna_tutorial:0.1.0`. 2. sudo ufw allow 8689. 3. `singularity run --nv sc_velocyto0.17.17_scannpy1.6.0_scvelo0.2.2.img jupyter-lab --ip=<your_ip> --port=8689 --no-browser `. Then in browser, open '<your_ip>:8689' and run the code below in jupyterlab. . The data and code I ran is shown as below:. * Go to: https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE132188. * Search ""GSE132188_RAW.tar"". * Download the file. Uncompress the files:. ```. tar xvf GSE132188_RAW.tar. ```. You will get the following files. ```. GSM3852752_E12_5_counts.tar.gz. GSM3852753_E13_5_counts.tar.gz. GSM3852754_E14_5_counts.tar.gz. GSM3852755_E15_5_counts.tar.gz. ```. Uncompress: . ```. mkdir -p E12_5_counts/. tar zxvf GSM3852752_E12_5_counts.tar.gz --directory E12_5_counts/. mkdir -p E13_5_counts/. tar zxvf GSM3852753_E13_5_counts.tar.gz --directory E13_5_counts/. mkdir -p E14_5_counts/. tar zxvf GSM3852754_E14_5_counts.tar.gz --directory E14_5_counts/. mkdir -p E15_5_counts/. tar zxvf GSM3852755_E15_5_counts.tar.gz --directory E15_5_counts/. ```. Code:. ```. import numpy as np. import matplotlib.pyplot as pl. import numpy as np. import scanpy as sc. import scanpy.external as sce. import pandas as pd. from anndata import AnnData. import seaborn as sns. from scipy.sparse import csr_matrix. import networkx as nx. import xlsxwriter. from matplotlib import rcParams. import seaborn as sns. import scipy as sci. #GSEApy: Gene Set Enrichment Analysis in Python. #import gseapy as gp. sc.settings.verbosity = 3. sc.logging.print_versions(). # Read cellranger files for all four samples. filename = './E12_5_counts/mm10/matrix.mtx'. filename_genes = './E12_5_counts/mm10/genes.tsv'. filename_barcodes = './E12_5_counts/mm10/barcodes.tsv'. e125 = sc.read(filename).transpose(). e125.var_names = np.genfromtxt(filename_genes, dtype=str)[:, 1]. e125.o",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:156,availability,error,error,156,"@xie186, are the variable names within each of your objects are unique? I would guess that would be the problem. Here's a simple case that would throw this error:. ```python. import anndata as ad, numpy as np, pandas as pd. a = ad.AnnData(np.ones((3, 2)), var=pd.DataFrame(index=[""a"", ""a""])). b = ad.AnnData(np.ones((3, 3)), var=pd.DataFrame(index=[""a"", ""b"", ""c""])). a.concatenate(b). ```. I think our merge operation for the variables is only well defined if variable names are unique with each of the objects. I'm not sure there's a good default result here other than throwing an error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:408,availability,operat,operation,408,"@xie186, are the variable names within each of your objects are unique? I would guess that would be the problem. Here's a simple case that would throw this error:. ```python. import anndata as ad, numpy as np, pandas as pd. a = ad.AnnData(np.ones((3, 2)), var=pd.DataFrame(index=[""a"", ""a""])). b = ad.AnnData(np.ones((3, 3)), var=pd.DataFrame(index=[""a"", ""b"", ""c""])). a.concatenate(b). ```. I think our merge operation for the variables is only well defined if variable names are unique with each of the objects. I'm not sure there's a good default result here other than throwing an error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:583,availability,error,error,583,"@xie186, are the variable names within each of your objects are unique? I would guess that would be the problem. Here's a simple case that would throw this error:. ```python. import anndata as ad, numpy as np, pandas as pd. a = ad.AnnData(np.ones((3, 2)), var=pd.DataFrame(index=[""a"", ""a""])). b = ad.AnnData(np.ones((3, 3)), var=pd.DataFrame(index=[""a"", ""b"", ""c""])). a.concatenate(b). ```. I think our merge operation for the variables is only well defined if variable names are unique with each of the objects. I'm not sure there's a good default result here other than throwing an error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:17,modifiability,variab,variable,17,"@xie186, are the variable names within each of your objects are unique? I would guess that would be the problem. Here's a simple case that would throw this error:. ```python. import anndata as ad, numpy as np, pandas as pd. a = ad.AnnData(np.ones((3, 2)), var=pd.DataFrame(index=[""a"", ""a""])). b = ad.AnnData(np.ones((3, 3)), var=pd.DataFrame(index=[""a"", ""b"", ""c""])). a.concatenate(b). ```. I think our merge operation for the variables is only well defined if variable names are unique with each of the objects. I'm not sure there's a good default result here other than throwing an error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:426,modifiability,variab,variables,426,"@xie186, are the variable names within each of your objects are unique? I would guess that would be the problem. Here's a simple case that would throw this error:. ```python. import anndata as ad, numpy as np, pandas as pd. a = ad.AnnData(np.ones((3, 2)), var=pd.DataFrame(index=[""a"", ""a""])). b = ad.AnnData(np.ones((3, 3)), var=pd.DataFrame(index=[""a"", ""b"", ""c""])). a.concatenate(b). ```. I think our merge operation for the variables is only well defined if variable names are unique with each of the objects. I'm not sure there's a good default result here other than throwing an error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:460,modifiability,variab,variable,460,"@xie186, are the variable names within each of your objects are unique? I would guess that would be the problem. Here's a simple case that would throw this error:. ```python. import anndata as ad, numpy as np, pandas as pd. a = ad.AnnData(np.ones((3, 2)), var=pd.DataFrame(index=[""a"", ""a""])). b = ad.AnnData(np.ones((3, 3)), var=pd.DataFrame(index=[""a"", ""b"", ""c""])). a.concatenate(b). ```. I think our merge operation for the variables is only well defined if variable names are unique with each of the objects. I'm not sure there's a good default result here other than throwing an error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:156,performance,error,error,156,"@xie186, are the variable names within each of your objects are unique? I would guess that would be the problem. Here's a simple case that would throw this error:. ```python. import anndata as ad, numpy as np, pandas as pd. a = ad.AnnData(np.ones((3, 2)), var=pd.DataFrame(index=[""a"", ""a""])). b = ad.AnnData(np.ones((3, 3)), var=pd.DataFrame(index=[""a"", ""b"", ""c""])). a.concatenate(b). ```. I think our merge operation for the variables is only well defined if variable names are unique with each of the objects. I'm not sure there's a good default result here other than throwing an error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:583,performance,error,error,583,"@xie186, are the variable names within each of your objects are unique? I would guess that would be the problem. Here's a simple case that would throw this error:. ```python. import anndata as ad, numpy as np, pandas as pd. a = ad.AnnData(np.ones((3, 2)), var=pd.DataFrame(index=[""a"", ""a""])). b = ad.AnnData(np.ones((3, 3)), var=pd.DataFrame(index=[""a"", ""b"", ""c""])). a.concatenate(b). ```. I think our merge operation for the variables is only well defined if variable names are unique with each of the objects. I'm not sure there's a good default result here other than throwing an error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:156,safety,error,error,156,"@xie186, are the variable names within each of your objects are unique? I would guess that would be the problem. Here's a simple case that would throw this error:. ```python. import anndata as ad, numpy as np, pandas as pd. a = ad.AnnData(np.ones((3, 2)), var=pd.DataFrame(index=[""a"", ""a""])). b = ad.AnnData(np.ones((3, 3)), var=pd.DataFrame(index=[""a"", ""b"", ""c""])). a.concatenate(b). ```. I think our merge operation for the variables is only well defined if variable names are unique with each of the objects. I'm not sure there's a good default result here other than throwing an error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:583,safety,error,error,583,"@xie186, are the variable names within each of your objects are unique? I would guess that would be the problem. Here's a simple case that would throw this error:. ```python. import anndata as ad, numpy as np, pandas as pd. a = ad.AnnData(np.ones((3, 2)), var=pd.DataFrame(index=[""a"", ""a""])). b = ad.AnnData(np.ones((3, 3)), var=pd.DataFrame(index=[""a"", ""b"", ""c""])). a.concatenate(b). ```. I think our merge operation for the variables is only well defined if variable names are unique with each of the objects. I'm not sure there's a good default result here other than throwing an error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:122,testability,simpl,simple,122,"@xie186, are the variable names within each of your objects are unique? I would guess that would be the problem. Here's a simple case that would throw this error:. ```python. import anndata as ad, numpy as np, pandas as pd. a = ad.AnnData(np.ones((3, 2)), var=pd.DataFrame(index=[""a"", ""a""])). b = ad.AnnData(np.ones((3, 3)), var=pd.DataFrame(index=[""a"", ""b"", ""c""])). a.concatenate(b). ```. I think our merge operation for the variables is only well defined if variable names are unique with each of the objects. I'm not sure there's a good default result here other than throwing an error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:122,usability,simpl,simple,122,"@xie186, are the variable names within each of your objects are unique? I would guess that would be the problem. Here's a simple case that would throw this error:. ```python. import anndata as ad, numpy as np, pandas as pd. a = ad.AnnData(np.ones((3, 2)), var=pd.DataFrame(index=[""a"", ""a""])). b = ad.AnnData(np.ones((3, 3)), var=pd.DataFrame(index=[""a"", ""b"", ""c""])). a.concatenate(b). ```. I think our merge operation for the variables is only well defined if variable names are unique with each of the objects. I'm not sure there's a good default result here other than throwing an error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:156,usability,error,error,156,"@xie186, are the variable names within each of your objects are unique? I would guess that would be the problem. Here's a simple case that would throw this error:. ```python. import anndata as ad, numpy as np, pandas as pd. a = ad.AnnData(np.ones((3, 2)), var=pd.DataFrame(index=[""a"", ""a""])). b = ad.AnnData(np.ones((3, 3)), var=pd.DataFrame(index=[""a"", ""b"", ""c""])). a.concatenate(b). ```. I think our merge operation for the variables is only well defined if variable names are unique with each of the objects. I'm not sure there's a good default result here other than throwing an error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:583,usability,error,error,583,"@xie186, are the variable names within each of your objects are unique? I would guess that would be the problem. Here's a simple case that would throw this error:. ```python. import anndata as ad, numpy as np, pandas as pd. a = ad.AnnData(np.ones((3, 2)), var=pd.DataFrame(index=[""a"", ""a""])). b = ad.AnnData(np.ones((3, 3)), var=pd.DataFrame(index=[""a"", ""b"", ""c""])). a.concatenate(b). ```. I think our merge operation for the variables is only well defined if variable names are unique with each of the objects. I'm not sure there's a good default result here other than throwing an error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:171,deployability,version,version,171,"@ivirshup Indeed, there are 48 indexes that are not unique which is weird to me. In the link I gave above, the authors didn't have this problem. Maybe it's because of the version differences. . Thanks. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:171,integrability,version,version,171,"@ivirshup Indeed, there are 48 indexes that are not unique which is weird to me. In the link I gave above, the authors didn't have this problem. Maybe it's because of the version differences. . Thanks. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:171,modifiability,version,version,171,"@ivirshup Indeed, there are 48 indexes that are not unique which is weird to me. In the link I gave above, the authors didn't have this problem. Maybe it's because of the version differences. . Thanks. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:111,security,auth,authors,111,"@ivirshup Indeed, there are 48 indexes that are not unique which is weird to me. In the link I gave above, the authors didn't have this problem. Maybe it's because of the version differences. . Thanks. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:39,usability,help,help,39,`adata.obs_names_make_unique()` should help you here before concatenating.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1409:179,security,sign,signs,179,"Ah, I missed that it's gene IDs and not obs ids. In that case it's `var_names_make_unique()` you need. I'm not sure why you would have duplicate genes. Sometimes it can be due to signs that aren't read in properly, i.e. maybe you have gene names `ABC-1` and `ABC-1 ` with a space or some other character?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409
https://github.com/scverse/scanpy/issues/1410:96,availability,cluster,clustering,96,"Hi @cartal,. Could it be that you are using a subsetted anndata object on which you are running clustering? If you run an `adata_subset = adata_subset.copy()` You turn that from a view into an anndata object, which might avoid this. I'm however not sure why using a view would make the clustering results end up in `adata.uns`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1410
https://github.com/scverse/scanpy/issues/1410:286,availability,cluster,clustering,286,"Hi @cartal,. Could it be that you are using a subsetted anndata object on which you are running clustering? If you run an `adata_subset = adata_subset.copy()` You turn that from a view into an anndata object, which might avoid this. I'm however not sure why using a view would make the clustering results end up in `adata.uns`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1410
https://github.com/scverse/scanpy/issues/1410:96,deployability,cluster,clustering,96,"Hi @cartal,. Could it be that you are using a subsetted anndata object on which you are running clustering? If you run an `adata_subset = adata_subset.copy()` You turn that from a view into an anndata object, which might avoid this. I'm however not sure why using a view would make the clustering results end up in `adata.uns`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1410
https://github.com/scverse/scanpy/issues/1410:286,deployability,cluster,clustering,286,"Hi @cartal,. Could it be that you are using a subsetted anndata object on which you are running clustering? If you run an `adata_subset = adata_subset.copy()` You turn that from a view into an anndata object, which might avoid this. I'm however not sure why using a view would make the clustering results end up in `adata.uns`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1410
https://github.com/scverse/scanpy/issues/1410:46,integrability,sub,subsetted,46,"Hi @cartal,. Could it be that you are using a subsetted anndata object on which you are running clustering? If you run an `adata_subset = adata_subset.copy()` You turn that from a view into an anndata object, which might avoid this. I'm however not sure why using a view would make the clustering results end up in `adata.uns`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1410
https://github.com/scverse/scanpy/issues/1410:221,safety,avoid,avoid,221,"Hi @cartal,. Could it be that you are using a subsetted anndata object on which you are running clustering? If you run an `adata_subset = adata_subset.copy()` You turn that from a view into an anndata object, which might avoid this. I'm however not sure why using a view would make the clustering results end up in `adata.uns`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1410
https://github.com/scverse/scanpy/issues/1410:73,integrability,sub,subsetting,73,"Thanks for the quick reply @LuckyMD! This is the whole dataset, the only subsetting I've done is when I have filtered low quality cells.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1410
https://github.com/scverse/scanpy/issues/1410:109,integrability,filter,filtered,109,"Thanks for the quick reply @LuckyMD! This is the whole dataset, the only subsetting I've done is when I have filtered low quality cells.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1410
https://github.com/scverse/scanpy/issues/1410:213,integrability,batch,batch,213,"I see. . The output for `adata` is:. ```. AnnData object with n_obs  n_vars = 106774  33538. obs: 'Cell', 'sampleID', 'sample_name', 'UMI_count', 'detectedGenesPerCell', 'percent_mito', 'data_set', 'IsPassed', 'batch', 'status', 'n_genes', 'n_counts', 'leiden', 'leiden_scVI'. var: 'name'. uns: 'log1p'. ```. I can see that my attempts to `leiden`ing have ended up here in `adata.obs` ...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1410
https://github.com/scverse/scanpy/issues/1410:213,performance,batch,batch,213,"I see. . The output for `adata` is:. ```. AnnData object with n_obs  n_vars = 106774  33538. obs: 'Cell', 'sampleID', 'sample_name', 'UMI_count', 'detectedGenesPerCell', 'percent_mito', 'data_set', 'IsPassed', 'batch', 'status', 'n_genes', 'n_counts', 'leiden', 'leiden_scVI'. var: 'name'. uns: 'log1p'. ```. I can see that my attempts to `leiden`ing have ended up here in `adata.obs` ...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1410
https://github.com/scverse/scanpy/issues/1410:149,safety,detect,detectedGenesPerCell,149,"I see. . The output for `adata` is:. ```. AnnData object with n_obs  n_vars = 106774  33538. obs: 'Cell', 'sampleID', 'sample_name', 'UMI_count', 'detectedGenesPerCell', 'percent_mito', 'data_set', 'IsPassed', 'batch', 'status', 'n_genes', 'n_counts', 'leiden', 'leiden_scVI'. var: 'name'. uns: 'log1p'. ```. I can see that my attempts to `leiden`ing have ended up here in `adata.obs` ...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1410
https://github.com/scverse/scanpy/issues/1410:149,security,detect,detectedGenesPerCell,149,"I see. . The output for `adata` is:. ```. AnnData object with n_obs  n_vars = 106774  33538. obs: 'Cell', 'sampleID', 'sample_name', 'UMI_count', 'detectedGenesPerCell', 'percent_mito', 'data_set', 'IsPassed', 'batch', 'status', 'n_genes', 'n_counts', 'leiden', 'leiden_scVI'. var: 'name'. uns: 'log1p'. ```. I can see that my attempts to `leiden`ing have ended up here in `adata.obs` ...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1410
https://github.com/scverse/scanpy/issues/1410:222,usability,statu,status,222,"I see. . The output for `adata` is:. ```. AnnData object with n_obs  n_vars = 106774  33538. obs: 'Cell', 'sampleID', 'sample_name', 'UMI_count', 'detectedGenesPerCell', 'percent_mito', 'data_set', 'IsPassed', 'batch', 'status', 'n_genes', 'n_counts', 'leiden', 'leiden_scVI'. var: 'name'. uns: 'log1p'. ```. I can see that my attempts to `leiden`ing have ended up here in `adata.obs` ...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1410
https://github.com/scverse/scanpy/issues/1410:236,availability,cluster,clusters,236,"This looks as though it is now no longer a view, or the respective warning is from a line of code in the `sc.tl.leiden()` function. Either way, it looks to be resolved now. Might the `adata.uns['leiden']` be the colours assigned to the clusters after plotting?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1410
https://github.com/scverse/scanpy/issues/1410:236,deployability,cluster,clusters,236,"This looks as though it is now no longer a view, or the respective warning is from a line of code in the `sc.tl.leiden()` function. Either way, it looks to be resolved now. Might the `adata.uns['leiden']` be the colours assigned to the clusters after plotting?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1410
https://github.com/scverse/scanpy/issues/1410:45,deployability,pipelin,pipeline,45,"I tested this in a couple of machine and the pipeline works fine there. However, I just re-installed `leidenalg` and this is now resolved! . Thanks a lot for the feedback. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1410
https://github.com/scverse/scanpy/issues/1410:91,deployability,instal,installed,91,"I tested this in a couple of machine and the pipeline works fine there. However, I just re-installed `leidenalg` and this is now resolved! . Thanks a lot for the feedback. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1410
https://github.com/scverse/scanpy/issues/1410:19,integrability,coupl,couple,19,"I tested this in a couple of machine and the pipeline works fine there. However, I just re-installed `leidenalg` and this is now resolved! . Thanks a lot for the feedback. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1410
https://github.com/scverse/scanpy/issues/1410:45,integrability,pipelin,pipeline,45,"I tested this in a couple of machine and the pipeline works fine there. However, I just re-installed `leidenalg` and this is now resolved! . Thanks a lot for the feedback. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1410
https://github.com/scverse/scanpy/issues/1410:19,modifiability,coupl,couple,19,"I tested this in a couple of machine and the pipeline works fine there. However, I just re-installed `leidenalg` and this is now resolved! . Thanks a lot for the feedback. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1410
https://github.com/scverse/scanpy/issues/1410:2,safety,test,tested,2,"I tested this in a couple of machine and the pipeline works fine there. However, I just re-installed `leidenalg` and this is now resolved! . Thanks a lot for the feedback. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1410
https://github.com/scverse/scanpy/issues/1410:2,testability,test,tested,2,"I tested this in a couple of machine and the pipeline works fine there. However, I just re-installed `leidenalg` and this is now resolved! . Thanks a lot for the feedback. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1410
https://github.com/scverse/scanpy/issues/1410:19,testability,coupl,couple,19,"I tested this in a couple of machine and the pipeline works fine there. However, I just re-installed `leidenalg` and this is now resolved! . Thanks a lot for the feedback. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1410
https://github.com/scverse/scanpy/issues/1410:162,usability,feedback,feedback,162,"I tested this in a couple of machine and the pipeline works fine there. However, I just re-installed `leidenalg` and this is now resolved! . Thanks a lot for the feedback. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1410
https://github.com/scverse/scanpy/issues/1410:20,usability,help,help,20,Great! Glad I could help.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1410
https://github.com/scverse/scanpy/issues/1411:108,usability,learn,learn,108,Same issue here. . scanpy==1.6.0 anndata==0.7.4 umap==0.4.4 numpy==1.19.0 scipy==1.4.1 pandas==1.0.5 scikit-learn==0.23.1 statsmodels==0.11.1 python-igraph==0.8.2 louvain==0.7.0 leidenalg==0.8.0,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1411
https://github.com/scverse/scanpy/issues/1411:111,interoperability,specif,specific,111,"This appears to have something to do with the default value of `top_n_markers = None`, because when I use some specific number like `top_n_markers = 20`, it works normally.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1411
https://github.com/scverse/scanpy/issues/1411:327,deployability,automat,automatically,327,"Thanks @jpreall and @suzhanhao for jumping in with solutions here. I have finally found some time to look at this. The recent change is that `sc.tl.rank_genes_groups` now saves DE results not just for the top 100 genes, but for all genes. So the default for `top_n_markers`, which was to use all genes that have results stored automatically shifted from 100 genes to all genes. And if you look at overlaps between all genes and your gene list, you will of course get 1. The only change that is needed here would be to set the default `top_n_markers = 100`. That should be a 5 character change to the function ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1411
https://github.com/scverse/scanpy/issues/1411:93,performance,time,time,93,"Thanks @jpreall and @suzhanhao for jumping in with solutions here. I have finally found some time to look at this. The recent change is that `sc.tl.rank_genes_groups` now saves DE results not just for the top 100 genes, but for all genes. So the default for `top_n_markers`, which was to use all genes that have results stored automatically shifted from 100 genes to all genes. And if you look at overlaps between all genes and your gene list, you will of course get 1. The only change that is needed here would be to set the default `top_n_markers = 100`. That should be a 5 character change to the function ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1411
https://github.com/scverse/scanpy/issues/1411:327,testability,automat,automatically,327,"Thanks @jpreall and @suzhanhao for jumping in with solutions here. I have finally found some time to look at this. The recent change is that `sc.tl.rank_genes_groups` now saves DE results not just for the top 100 genes, but for all genes. So the default for `top_n_markers`, which was to use all genes that have results stored automatically shifted from 100 genes to all genes. And if you look at overlaps between all genes and your gene list, you will of course get 1. The only change that is needed here would be to set the default `top_n_markers = 100`. That should be a 5 character change to the function ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1411
https://github.com/scverse/scanpy/issues/1412:247,modifiability,paramet,parameter,247,"> My understanding is that it should search for 'ENSG00000104814' in the index column and return the corresponding value in 'symbols', but it seems that is directly searching 'ENSG00000104814' in the 'symbols' column. If you add the `gene_symbol` parameter, scanpy will look for the `var_names` in this column instead of looking into `adata.var index`. It will not map between index and the gene_symbol. I you want to do the mapping of labels, you can do something like follows:. ```PYTHON. # set show=False to get the axes dictionary. ax_dict = sc.pl.dotplot(adata, myg, groupby=condition). # get ensembl ids and map them to gene symbol. Although you can directly map `myg`,. # the following method will work in any case, including `sc.pl.rank_genes_groups_dotplot`. # This method also works for `sc.pl.matrixplot`. ticklabels = [adata.var.loc[x]['gene_symbol'] for x in ax_dict['mainplot_ax'].get_xticklabels()]. # replace ensembls ids by gene symbol in plot. _ = ax_dict['mainplot_ax'].set_xticklabels(ticklabels). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1412
https://github.com/scverse/scanpy/issues/1412:5,testability,understand,understanding,5,"> My understanding is that it should search for 'ENSG00000104814' in the index column and return the corresponding value in 'symbols', but it seems that is directly searching 'ENSG00000104814' in the 'symbols' column. If you add the `gene_symbol` parameter, scanpy will look for the `var_names` in this column instead of looking into `adata.var index`. It will not map between index and the gene_symbol. I you want to do the mapping of labels, you can do something like follows:. ```PYTHON. # set show=False to get the axes dictionary. ax_dict = sc.pl.dotplot(adata, myg, groupby=condition). # get ensembl ids and map them to gene symbol. Although you can directly map `myg`,. # the following method will work in any case, including `sc.pl.rank_genes_groups_dotplot`. # This method also works for `sc.pl.matrixplot`. ticklabels = [adata.var.loc[x]['gene_symbol'] for x in ax_dict['mainplot_ax'].get_xticklabels()]. # replace ensembls ids by gene symbol in plot. _ = ax_dict['mainplot_ax'].set_xticklabels(ticklabels). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1412
https://github.com/scverse/scanpy/issues/1412:7,deployability,updat,update,7,Please update to the latest scanpy release (1.6),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1412
https://github.com/scverse/scanpy/issues/1412:35,deployability,releas,release,35,Please update to the latest scanpy release (1.6),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1412
https://github.com/scverse/scanpy/issues/1412:7,safety,updat,update,7,Please update to the latest scanpy release (1.6),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1412
https://github.com/scverse/scanpy/issues/1412:7,security,updat,update,7,Please update to the latest scanpy release (1.6),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1412
https://github.com/scverse/scanpy/pull/1413:13,deployability,fail,failed,13,The CI tests failed due to the image matching problems (in test_violin and test_pbmc3k) fixed by PR #1422 which is now merged into master. Should I merge master into my fix branch?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413
https://github.com/scverse/scanpy/pull/1413:13,reliability,fail,failed,13,The CI tests failed due to the image matching problems (in test_violin and test_pbmc3k) fixed by PR #1422 which is now merged into master. Should I merge master into my fix branch?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413
https://github.com/scverse/scanpy/pull/1413:7,safety,test,tests,7,The CI tests failed due to the image matching problems (in test_violin and test_pbmc3k) fixed by PR #1422 which is now merged into master. Should I merge master into my fix branch?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413
https://github.com/scverse/scanpy/pull/1413:7,testability,test,tests,7,The CI tests failed due to the image matching problems (in test_violin and test_pbmc3k) fixed by PR #1422 which is now merged into master. Should I merge master into my fix branch?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413
https://github.com/scverse/scanpy/pull/1413:202,deployability,API,API,202,"Thanks so much for your contribution @dwnorton ! Looks good. Two questions: 1) is there any way to do without densifying the matrix? it's not a big deal though since the matrix is small. 2) Is the UMAP API for the `umap.distances.pairwise_special_metric` function version-dependent somehow in other words do we need to constrain our umap dependency to a specific version, or is the API stable across all commonly used UMAP versions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413
https://github.com/scverse/scanpy/pull/1413:264,deployability,version,version-dependent,264,"Thanks so much for your contribution @dwnorton ! Looks good. Two questions: 1) is there any way to do without densifying the matrix? it's not a big deal though since the matrix is small. 2) Is the UMAP API for the `umap.distances.pairwise_special_metric` function version-dependent somehow in other words do we need to constrain our umap dependency to a specific version, or is the API stable across all commonly used UMAP versions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413
https://github.com/scverse/scanpy/pull/1413:338,deployability,depend,dependency,338,"Thanks so much for your contribution @dwnorton ! Looks good. Two questions: 1) is there any way to do without densifying the matrix? it's not a big deal though since the matrix is small. 2) Is the UMAP API for the `umap.distances.pairwise_special_metric` function version-dependent somehow in other words do we need to constrain our umap dependency to a specific version, or is the API stable across all commonly used UMAP versions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413
https://github.com/scverse/scanpy/pull/1413:363,deployability,version,version,363,"Thanks so much for your contribution @dwnorton ! Looks good. Two questions: 1) is there any way to do without densifying the matrix? it's not a big deal though since the matrix is small. 2) Is the UMAP API for the `umap.distances.pairwise_special_metric` function version-dependent somehow in other words do we need to constrain our umap dependency to a specific version, or is the API stable across all commonly used UMAP versions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413
https://github.com/scverse/scanpy/pull/1413:382,deployability,API,API,382,"Thanks so much for your contribution @dwnorton ! Looks good. Two questions: 1) is there any way to do without densifying the matrix? it's not a big deal though since the matrix is small. 2) Is the UMAP API for the `umap.distances.pairwise_special_metric` function version-dependent somehow in other words do we need to constrain our umap dependency to a specific version, or is the API stable across all commonly used UMAP versions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413
https://github.com/scverse/scanpy/pull/1413:423,deployability,version,versions,423,"Thanks so much for your contribution @dwnorton ! Looks good. Two questions: 1) is there any way to do without densifying the matrix? it's not a big deal though since the matrix is small. 2) Is the UMAP API for the `umap.distances.pairwise_special_metric` function version-dependent somehow in other words do we need to constrain our umap dependency to a specific version, or is the API stable across all commonly used UMAP versions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413
https://github.com/scverse/scanpy/pull/1413:202,integrability,API,API,202,"Thanks so much for your contribution @dwnorton ! Looks good. Two questions: 1) is there any way to do without densifying the matrix? it's not a big deal though since the matrix is small. 2) Is the UMAP API for the `umap.distances.pairwise_special_metric` function version-dependent somehow in other words do we need to constrain our umap dependency to a specific version, or is the API stable across all commonly used UMAP versions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413
https://github.com/scverse/scanpy/pull/1413:264,integrability,version,version-dependent,264,"Thanks so much for your contribution @dwnorton ! Looks good. Two questions: 1) is there any way to do without densifying the matrix? it's not a big deal though since the matrix is small. 2) Is the UMAP API for the `umap.distances.pairwise_special_metric` function version-dependent somehow in other words do we need to constrain our umap dependency to a specific version, or is the API stable across all commonly used UMAP versions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413
https://github.com/scverse/scanpy/pull/1413:338,integrability,depend,dependency,338,"Thanks so much for your contribution @dwnorton ! Looks good. Two questions: 1) is there any way to do without densifying the matrix? it's not a big deal though since the matrix is small. 2) Is the UMAP API for the `umap.distances.pairwise_special_metric` function version-dependent somehow in other words do we need to constrain our umap dependency to a specific version, or is the API stable across all commonly used UMAP versions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413
https://github.com/scverse/scanpy/pull/1413:363,integrability,version,version,363,"Thanks so much for your contribution @dwnorton ! Looks good. Two questions: 1) is there any way to do without densifying the matrix? it's not a big deal though since the matrix is small. 2) Is the UMAP API for the `umap.distances.pairwise_special_metric` function version-dependent somehow in other words do we need to constrain our umap dependency to a specific version, or is the API stable across all commonly used UMAP versions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413
https://github.com/scverse/scanpy/pull/1413:382,integrability,API,API,382,"Thanks so much for your contribution @dwnorton ! Looks good. Two questions: 1) is there any way to do without densifying the matrix? it's not a big deal though since the matrix is small. 2) Is the UMAP API for the `umap.distances.pairwise_special_metric` function version-dependent somehow in other words do we need to constrain our umap dependency to a specific version, or is the API stable across all commonly used UMAP versions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413
https://github.com/scverse/scanpy/pull/1413:423,integrability,version,versions,423,"Thanks so much for your contribution @dwnorton ! Looks good. Two questions: 1) is there any way to do without densifying the matrix? it's not a big deal though since the matrix is small. 2) Is the UMAP API for the `umap.distances.pairwise_special_metric` function version-dependent somehow in other words do we need to constrain our umap dependency to a specific version, or is the API stable across all commonly used UMAP versions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413
https://github.com/scverse/scanpy/pull/1413:202,interoperability,API,API,202,"Thanks so much for your contribution @dwnorton ! Looks good. Two questions: 1) is there any way to do without densifying the matrix? it's not a big deal though since the matrix is small. 2) Is the UMAP API for the `umap.distances.pairwise_special_metric` function version-dependent somehow in other words do we need to constrain our umap dependency to a specific version, or is the API stable across all commonly used UMAP versions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413
https://github.com/scverse/scanpy/pull/1413:354,interoperability,specif,specific,354,"Thanks so much for your contribution @dwnorton ! Looks good. Two questions: 1) is there any way to do without densifying the matrix? it's not a big deal though since the matrix is small. 2) Is the UMAP API for the `umap.distances.pairwise_special_metric` function version-dependent somehow in other words do we need to constrain our umap dependency to a specific version, or is the API stable across all commonly used UMAP versions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413
https://github.com/scverse/scanpy/pull/1413:382,interoperability,API,API,382,"Thanks so much for your contribution @dwnorton ! Looks good. Two questions: 1) is there any way to do without densifying the matrix? it's not a big deal though since the matrix is small. 2) Is the UMAP API for the `umap.distances.pairwise_special_metric` function version-dependent somehow in other words do we need to constrain our umap dependency to a specific version, or is the API stable across all commonly used UMAP versions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413
https://github.com/scverse/scanpy/pull/1413:264,modifiability,version,version-dependent,264,"Thanks so much for your contribution @dwnorton ! Looks good. Two questions: 1) is there any way to do without densifying the matrix? it's not a big deal though since the matrix is small. 2) Is the UMAP API for the `umap.distances.pairwise_special_metric` function version-dependent somehow in other words do we need to constrain our umap dependency to a specific version, or is the API stable across all commonly used UMAP versions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413
https://github.com/scverse/scanpy/pull/1413:338,modifiability,depend,dependency,338,"Thanks so much for your contribution @dwnorton ! Looks good. Two questions: 1) is there any way to do without densifying the matrix? it's not a big deal though since the matrix is small. 2) Is the UMAP API for the `umap.distances.pairwise_special_metric` function version-dependent somehow in other words do we need to constrain our umap dependency to a specific version, or is the API stable across all commonly used UMAP versions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413
https://github.com/scverse/scanpy/pull/1413:363,modifiability,version,version,363,"Thanks so much for your contribution @dwnorton ! Looks good. Two questions: 1) is there any way to do without densifying the matrix? it's not a big deal though since the matrix is small. 2) Is the UMAP API for the `umap.distances.pairwise_special_metric` function version-dependent somehow in other words do we need to constrain our umap dependency to a specific version, or is the API stable across all commonly used UMAP versions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413
https://github.com/scverse/scanpy/pull/1413:423,modifiability,version,versions,423,"Thanks so much for your contribution @dwnorton ! Looks good. Two questions: 1) is there any way to do without densifying the matrix? it's not a big deal though since the matrix is small. 2) Is the UMAP API for the `umap.distances.pairwise_special_metric` function version-dependent somehow in other words do we need to constrain our umap dependency to a specific version, or is the API stable across all commonly used UMAP versions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413
https://github.com/scverse/scanpy/pull/1413:272,safety,depend,dependent,272,"Thanks so much for your contribution @dwnorton ! Looks good. Two questions: 1) is there any way to do without densifying the matrix? it's not a big deal though since the matrix is small. 2) Is the UMAP API for the `umap.distances.pairwise_special_metric` function version-dependent somehow in other words do we need to constrain our umap dependency to a specific version, or is the API stable across all commonly used UMAP versions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413
https://github.com/scverse/scanpy/pull/1413:338,safety,depend,dependency,338,"Thanks so much for your contribution @dwnorton ! Looks good. Two questions: 1) is there any way to do without densifying the matrix? it's not a big deal though since the matrix is small. 2) Is the UMAP API for the `umap.distances.pairwise_special_metric` function version-dependent somehow in other words do we need to constrain our umap dependency to a specific version, or is the API stable across all commonly used UMAP versions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413
https://github.com/scverse/scanpy/pull/1413:272,testability,depend,dependent,272,"Thanks so much for your contribution @dwnorton ! Looks good. Two questions: 1) is there any way to do without densifying the matrix? it's not a big deal though since the matrix is small. 2) Is the UMAP API for the `umap.distances.pairwise_special_metric` function version-dependent somehow in other words do we need to constrain our umap dependency to a specific version, or is the API stable across all commonly used UMAP versions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413
https://github.com/scverse/scanpy/pull/1413:338,testability,depend,dependency,338,"Thanks so much for your contribution @dwnorton ! Looks good. Two questions: 1) is there any way to do without densifying the matrix? it's not a big deal though since the matrix is small. 2) Is the UMAP API for the `umap.distances.pairwise_special_metric` function version-dependent somehow in other words do we need to constrain our umap dependency to a specific version, or is the API stable across all commonly used UMAP versions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413
https://github.com/scverse/scanpy/pull/1413:920,deployability,version,version,920,"Thanks for the feedback. I have now merged master into the fix branch and the CI tests are happy. To address the shrewd questions asked by @gokceneraslan:. 1) I pondered over this myself. As I mentioned (above), I based my approach on a pull request in umap's own codebase which also resorts to densification of the matrix. As far as I can see, `pairwise_special_metric` doesn't directly support a sparse matrix and I don't see an alternative approach that wouldn't involve duplicating some of its implementation and (probably) sacrificing some of the benefits (i.e. parallel processing). A deeper analysis by another brain might draw different conclusions. 2) Another good question. Based on my ""git blame"" detective work, `pairwise_special_metric` was introduced in [a change on 20 Nov 2018](https://github.com/lmcinnes/umap/commit/edade6841bd9b3c80454bf7f4386177c9aa35ab5) which should have seen it incorporated into version [0.3.7](https://github.com/lmcinnes/umap/tree/0.3.7). Since then its signature has remained compatible (with the only change being the addition of the `kwds` argument). scanpy's `requirements.txt` already has `umap-learn` set to a minimum version of 0.3.10 so I believe we're good on that front.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413
https://github.com/scverse/scanpy/pull/1413:1167,deployability,version,version,1167,"Thanks for the feedback. I have now merged master into the fix branch and the CI tests are happy. To address the shrewd questions asked by @gokceneraslan:. 1) I pondered over this myself. As I mentioned (above), I based my approach on a pull request in umap's own codebase which also resorts to densification of the matrix. As far as I can see, `pairwise_special_metric` doesn't directly support a sparse matrix and I don't see an alternative approach that wouldn't involve duplicating some of its implementation and (probably) sacrificing some of the benefits (i.e. parallel processing). A deeper analysis by another brain might draw different conclusions. 2) Another good question. Based on my ""git blame"" detective work, `pairwise_special_metric` was introduced in [a change on 20 Nov 2018](https://github.com/lmcinnes/umap/commit/edade6841bd9b3c80454bf7f4386177c9aa35ab5) which should have seen it incorporated into version [0.3.7](https://github.com/lmcinnes/umap/tree/0.3.7). Since then its signature has remained compatible (with the only change being the addition of the `kwds` argument). scanpy's `requirements.txt` already has `umap-learn` set to a minimum version of 0.3.10 so I believe we're good on that front.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413
https://github.com/scverse/scanpy/pull/1413:630,energy efficiency,draw,draw,630,"Thanks for the feedback. I have now merged master into the fix branch and the CI tests are happy. To address the shrewd questions asked by @gokceneraslan:. 1) I pondered over this myself. As I mentioned (above), I based my approach on a pull request in umap's own codebase which also resorts to densification of the matrix. As far as I can see, `pairwise_special_metric` doesn't directly support a sparse matrix and I don't see an alternative approach that wouldn't involve duplicating some of its implementation and (probably) sacrificing some of the benefits (i.e. parallel processing). A deeper analysis by another brain might draw different conclusions. 2) Another good question. Based on my ""git blame"" detective work, `pairwise_special_metric` was introduced in [a change on 20 Nov 2018](https://github.com/lmcinnes/umap/commit/edade6841bd9b3c80454bf7f4386177c9aa35ab5) which should have seen it incorporated into version [0.3.7](https://github.com/lmcinnes/umap/tree/0.3.7). Since then its signature has remained compatible (with the only change being the addition of the `kwds` argument). scanpy's `requirements.txt` already has `umap-learn` set to a minimum version of 0.3.10 so I believe we're good on that front.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413
https://github.com/scverse/scanpy/pull/1413:920,integrability,version,version,920,"Thanks for the feedback. I have now merged master into the fix branch and the CI tests are happy. To address the shrewd questions asked by @gokceneraslan:. 1) I pondered over this myself. As I mentioned (above), I based my approach on a pull request in umap's own codebase which also resorts to densification of the matrix. As far as I can see, `pairwise_special_metric` doesn't directly support a sparse matrix and I don't see an alternative approach that wouldn't involve duplicating some of its implementation and (probably) sacrificing some of the benefits (i.e. parallel processing). A deeper analysis by another brain might draw different conclusions. 2) Another good question. Based on my ""git blame"" detective work, `pairwise_special_metric` was introduced in [a change on 20 Nov 2018](https://github.com/lmcinnes/umap/commit/edade6841bd9b3c80454bf7f4386177c9aa35ab5) which should have seen it incorporated into version [0.3.7](https://github.com/lmcinnes/umap/tree/0.3.7). Since then its signature has remained compatible (with the only change being the addition of the `kwds` argument). scanpy's `requirements.txt` already has `umap-learn` set to a minimum version of 0.3.10 so I believe we're good on that front.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413
https://github.com/scverse/scanpy/pull/1413:1167,integrability,version,version,1167,"Thanks for the feedback. I have now merged master into the fix branch and the CI tests are happy. To address the shrewd questions asked by @gokceneraslan:. 1) I pondered over this myself. As I mentioned (above), I based my approach on a pull request in umap's own codebase which also resorts to densification of the matrix. As far as I can see, `pairwise_special_metric` doesn't directly support a sparse matrix and I don't see an alternative approach that wouldn't involve duplicating some of its implementation and (probably) sacrificing some of the benefits (i.e. parallel processing). A deeper analysis by another brain might draw different conclusions. 2) Another good question. Based on my ""git blame"" detective work, `pairwise_special_metric` was introduced in [a change on 20 Nov 2018](https://github.com/lmcinnes/umap/commit/edade6841bd9b3c80454bf7f4386177c9aa35ab5) which should have seen it incorporated into version [0.3.7](https://github.com/lmcinnes/umap/tree/0.3.7). Since then its signature has remained compatible (with the only change being the addition of the `kwds` argument). scanpy's `requirements.txt` already has `umap-learn` set to a minimum version of 0.3.10 so I believe we're good on that front.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413
https://github.com/scverse/scanpy/pull/1413:1020,interoperability,compatib,compatible,1020,"Thanks for the feedback. I have now merged master into the fix branch and the CI tests are happy. To address the shrewd questions asked by @gokceneraslan:. 1) I pondered over this myself. As I mentioned (above), I based my approach on a pull request in umap's own codebase which also resorts to densification of the matrix. As far as I can see, `pairwise_special_metric` doesn't directly support a sparse matrix and I don't see an alternative approach that wouldn't involve duplicating some of its implementation and (probably) sacrificing some of the benefits (i.e. parallel processing). A deeper analysis by another brain might draw different conclusions. 2) Another good question. Based on my ""git blame"" detective work, `pairwise_special_metric` was introduced in [a change on 20 Nov 2018](https://github.com/lmcinnes/umap/commit/edade6841bd9b3c80454bf7f4386177c9aa35ab5) which should have seen it incorporated into version [0.3.7](https://github.com/lmcinnes/umap/tree/0.3.7). Since then its signature has remained compatible (with the only change being the addition of the `kwds` argument). scanpy's `requirements.txt` already has `umap-learn` set to a minimum version of 0.3.10 so I believe we're good on that front.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413
https://github.com/scverse/scanpy/pull/1413:920,modifiability,version,version,920,"Thanks for the feedback. I have now merged master into the fix branch and the CI tests are happy. To address the shrewd questions asked by @gokceneraslan:. 1) I pondered over this myself. As I mentioned (above), I based my approach on a pull request in umap's own codebase which also resorts to densification of the matrix. As far as I can see, `pairwise_special_metric` doesn't directly support a sparse matrix and I don't see an alternative approach that wouldn't involve duplicating some of its implementation and (probably) sacrificing some of the benefits (i.e. parallel processing). A deeper analysis by another brain might draw different conclusions. 2) Another good question. Based on my ""git blame"" detective work, `pairwise_special_metric` was introduced in [a change on 20 Nov 2018](https://github.com/lmcinnes/umap/commit/edade6841bd9b3c80454bf7f4386177c9aa35ab5) which should have seen it incorporated into version [0.3.7](https://github.com/lmcinnes/umap/tree/0.3.7). Since then its signature has remained compatible (with the only change being the addition of the `kwds` argument). scanpy's `requirements.txt` already has `umap-learn` set to a minimum version of 0.3.10 so I believe we're good on that front.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413
https://github.com/scverse/scanpy/pull/1413:1167,modifiability,version,version,1167,"Thanks for the feedback. I have now merged master into the fix branch and the CI tests are happy. To address the shrewd questions asked by @gokceneraslan:. 1) I pondered over this myself. As I mentioned (above), I based my approach on a pull request in umap's own codebase which also resorts to densification of the matrix. As far as I can see, `pairwise_special_metric` doesn't directly support a sparse matrix and I don't see an alternative approach that wouldn't involve duplicating some of its implementation and (probably) sacrificing some of the benefits (i.e. parallel processing). A deeper analysis by another brain might draw different conclusions. 2) Another good question. Based on my ""git blame"" detective work, `pairwise_special_metric` was introduced in [a change on 20 Nov 2018](https://github.com/lmcinnes/umap/commit/edade6841bd9b3c80454bf7f4386177c9aa35ab5) which should have seen it incorporated into version [0.3.7](https://github.com/lmcinnes/umap/tree/0.3.7). Since then its signature has remained compatible (with the only change being the addition of the `kwds` argument). scanpy's `requirements.txt` already has `umap-learn` set to a minimum version of 0.3.10 so I believe we're good on that front.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413
https://github.com/scverse/scanpy/pull/1413:567,performance,parallel,parallel,567,"Thanks for the feedback. I have now merged master into the fix branch and the CI tests are happy. To address the shrewd questions asked by @gokceneraslan:. 1) I pondered over this myself. As I mentioned (above), I based my approach on a pull request in umap's own codebase which also resorts to densification of the matrix. As far as I can see, `pairwise_special_metric` doesn't directly support a sparse matrix and I don't see an alternative approach that wouldn't involve duplicating some of its implementation and (probably) sacrificing some of the benefits (i.e. parallel processing). A deeper analysis by another brain might draw different conclusions. 2) Another good question. Based on my ""git blame"" detective work, `pairwise_special_metric` was introduced in [a change on 20 Nov 2018](https://github.com/lmcinnes/umap/commit/edade6841bd9b3c80454bf7f4386177c9aa35ab5) which should have seen it incorporated into version [0.3.7](https://github.com/lmcinnes/umap/tree/0.3.7). Since then its signature has remained compatible (with the only change being the addition of the `kwds` argument). scanpy's `requirements.txt` already has `umap-learn` set to a minimum version of 0.3.10 so I believe we're good on that front.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413
https://github.com/scverse/scanpy/pull/1413:371,reliability,doe,doesn,371,"Thanks for the feedback. I have now merged master into the fix branch and the CI tests are happy. To address the shrewd questions asked by @gokceneraslan:. 1) I pondered over this myself. As I mentioned (above), I based my approach on a pull request in umap's own codebase which also resorts to densification of the matrix. As far as I can see, `pairwise_special_metric` doesn't directly support a sparse matrix and I don't see an alternative approach that wouldn't involve duplicating some of its implementation and (probably) sacrificing some of the benefits (i.e. parallel processing). A deeper analysis by another brain might draw different conclusions. 2) Another good question. Based on my ""git blame"" detective work, `pairwise_special_metric` was introduced in [a change on 20 Nov 2018](https://github.com/lmcinnes/umap/commit/edade6841bd9b3c80454bf7f4386177c9aa35ab5) which should have seen it incorporated into version [0.3.7](https://github.com/lmcinnes/umap/tree/0.3.7). Since then its signature has remained compatible (with the only change being the addition of the `kwds` argument). scanpy's `requirements.txt` already has `umap-learn` set to a minimum version of 0.3.10 so I believe we're good on that front.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413
https://github.com/scverse/scanpy/pull/1413:81,safety,test,tests,81,"Thanks for the feedback. I have now merged master into the fix branch and the CI tests are happy. To address the shrewd questions asked by @gokceneraslan:. 1) I pondered over this myself. As I mentioned (above), I based my approach on a pull request in umap's own codebase which also resorts to densification of the matrix. As far as I can see, `pairwise_special_metric` doesn't directly support a sparse matrix and I don't see an alternative approach that wouldn't involve duplicating some of its implementation and (probably) sacrificing some of the benefits (i.e. parallel processing). A deeper analysis by another brain might draw different conclusions. 2) Another good question. Based on my ""git blame"" detective work, `pairwise_special_metric` was introduced in [a change on 20 Nov 2018](https://github.com/lmcinnes/umap/commit/edade6841bd9b3c80454bf7f4386177c9aa35ab5) which should have seen it incorporated into version [0.3.7](https://github.com/lmcinnes/umap/tree/0.3.7). Since then its signature has remained compatible (with the only change being the addition of the `kwds` argument). scanpy's `requirements.txt` already has `umap-learn` set to a minimum version of 0.3.10 so I believe we're good on that front.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413
https://github.com/scverse/scanpy/pull/1413:708,safety,detect,detective,708,"Thanks for the feedback. I have now merged master into the fix branch and the CI tests are happy. To address the shrewd questions asked by @gokceneraslan:. 1) I pondered over this myself. As I mentioned (above), I based my approach on a pull request in umap's own codebase which also resorts to densification of the matrix. As far as I can see, `pairwise_special_metric` doesn't directly support a sparse matrix and I don't see an alternative approach that wouldn't involve duplicating some of its implementation and (probably) sacrificing some of the benefits (i.e. parallel processing). A deeper analysis by another brain might draw different conclusions. 2) Another good question. Based on my ""git blame"" detective work, `pairwise_special_metric` was introduced in [a change on 20 Nov 2018](https://github.com/lmcinnes/umap/commit/edade6841bd9b3c80454bf7f4386177c9aa35ab5) which should have seen it incorporated into version [0.3.7](https://github.com/lmcinnes/umap/tree/0.3.7). Since then its signature has remained compatible (with the only change being the addition of the `kwds` argument). scanpy's `requirements.txt` already has `umap-learn` set to a minimum version of 0.3.10 so I believe we're good on that front.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413
https://github.com/scverse/scanpy/pull/1413:708,security,detect,detective,708,"Thanks for the feedback. I have now merged master into the fix branch and the CI tests are happy. To address the shrewd questions asked by @gokceneraslan:. 1) I pondered over this myself. As I mentioned (above), I based my approach on a pull request in umap's own codebase which also resorts to densification of the matrix. As far as I can see, `pairwise_special_metric` doesn't directly support a sparse matrix and I don't see an alternative approach that wouldn't involve duplicating some of its implementation and (probably) sacrificing some of the benefits (i.e. parallel processing). A deeper analysis by another brain might draw different conclusions. 2) Another good question. Based on my ""git blame"" detective work, `pairwise_special_metric` was introduced in [a change on 20 Nov 2018](https://github.com/lmcinnes/umap/commit/edade6841bd9b3c80454bf7f4386177c9aa35ab5) which should have seen it incorporated into version [0.3.7](https://github.com/lmcinnes/umap/tree/0.3.7). Since then its signature has remained compatible (with the only change being the addition of the `kwds` argument). scanpy's `requirements.txt` already has `umap-learn` set to a minimum version of 0.3.10 so I believe we're good on that front.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413
https://github.com/scverse/scanpy/pull/1413:997,security,sign,signature,997,"Thanks for the feedback. I have now merged master into the fix branch and the CI tests are happy. To address the shrewd questions asked by @gokceneraslan:. 1) I pondered over this myself. As I mentioned (above), I based my approach on a pull request in umap's own codebase which also resorts to densification of the matrix. As far as I can see, `pairwise_special_metric` doesn't directly support a sparse matrix and I don't see an alternative approach that wouldn't involve duplicating some of its implementation and (probably) sacrificing some of the benefits (i.e. parallel processing). A deeper analysis by another brain might draw different conclusions. 2) Another good question. Based on my ""git blame"" detective work, `pairwise_special_metric` was introduced in [a change on 20 Nov 2018](https://github.com/lmcinnes/umap/commit/edade6841bd9b3c80454bf7f4386177c9aa35ab5) which should have seen it incorporated into version [0.3.7](https://github.com/lmcinnes/umap/tree/0.3.7). Since then its signature has remained compatible (with the only change being the addition of the `kwds` argument). scanpy's `requirements.txt` already has `umap-learn` set to a minimum version of 0.3.10 so I believe we're good on that front.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413
https://github.com/scverse/scanpy/pull/1413:81,testability,test,tests,81,"Thanks for the feedback. I have now merged master into the fix branch and the CI tests are happy. To address the shrewd questions asked by @gokceneraslan:. 1) I pondered over this myself. As I mentioned (above), I based my approach on a pull request in umap's own codebase which also resorts to densification of the matrix. As far as I can see, `pairwise_special_metric` doesn't directly support a sparse matrix and I don't see an alternative approach that wouldn't involve duplicating some of its implementation and (probably) sacrificing some of the benefits (i.e. parallel processing). A deeper analysis by another brain might draw different conclusions. 2) Another good question. Based on my ""git blame"" detective work, `pairwise_special_metric` was introduced in [a change on 20 Nov 2018](https://github.com/lmcinnes/umap/commit/edade6841bd9b3c80454bf7f4386177c9aa35ab5) which should have seen it incorporated into version [0.3.7](https://github.com/lmcinnes/umap/tree/0.3.7). Since then its signature has remained compatible (with the only change being the addition of the `kwds` argument). scanpy's `requirements.txt` already has `umap-learn` set to a minimum version of 0.3.10 so I believe we're good on that front.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413
https://github.com/scverse/scanpy/pull/1413:15,usability,feedback,feedback,15,"Thanks for the feedback. I have now merged master into the fix branch and the CI tests are happy. To address the shrewd questions asked by @gokceneraslan:. 1) I pondered over this myself. As I mentioned (above), I based my approach on a pull request in umap's own codebase which also resorts to densification of the matrix. As far as I can see, `pairwise_special_metric` doesn't directly support a sparse matrix and I don't see an alternative approach that wouldn't involve duplicating some of its implementation and (probably) sacrificing some of the benefits (i.e. parallel processing). A deeper analysis by another brain might draw different conclusions. 2) Another good question. Based on my ""git blame"" detective work, `pairwise_special_metric` was introduced in [a change on 20 Nov 2018](https://github.com/lmcinnes/umap/commit/edade6841bd9b3c80454bf7f4386177c9aa35ab5) which should have seen it incorporated into version [0.3.7](https://github.com/lmcinnes/umap/tree/0.3.7). Since then its signature has remained compatible (with the only change being the addition of the `kwds` argument). scanpy's `requirements.txt` already has `umap-learn` set to a minimum version of 0.3.10 so I believe we're good on that front.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413
https://github.com/scverse/scanpy/pull/1413:388,usability,support,support,388,"Thanks for the feedback. I have now merged master into the fix branch and the CI tests are happy. To address the shrewd questions asked by @gokceneraslan:. 1) I pondered over this myself. As I mentioned (above), I based my approach on a pull request in umap's own codebase which also resorts to densification of the matrix. As far as I can see, `pairwise_special_metric` doesn't directly support a sparse matrix and I don't see an alternative approach that wouldn't involve duplicating some of its implementation and (probably) sacrificing some of the benefits (i.e. parallel processing). A deeper analysis by another brain might draw different conclusions. 2) Another good question. Based on my ""git blame"" detective work, `pairwise_special_metric` was introduced in [a change on 20 Nov 2018](https://github.com/lmcinnes/umap/commit/edade6841bd9b3c80454bf7f4386177c9aa35ab5) which should have seen it incorporated into version [0.3.7](https://github.com/lmcinnes/umap/tree/0.3.7). Since then its signature has remained compatible (with the only change being the addition of the `kwds` argument). scanpy's `requirements.txt` already has `umap-learn` set to a minimum version of 0.3.10 so I believe we're good on that front.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413
https://github.com/scverse/scanpy/pull/1413:1143,usability,learn,learn,1143,"Thanks for the feedback. I have now merged master into the fix branch and the CI tests are happy. To address the shrewd questions asked by @gokceneraslan:. 1) I pondered over this myself. As I mentioned (above), I based my approach on a pull request in umap's own codebase which also resorts to densification of the matrix. As far as I can see, `pairwise_special_metric` doesn't directly support a sparse matrix and I don't see an alternative approach that wouldn't involve duplicating some of its implementation and (probably) sacrificing some of the benefits (i.e. parallel processing). A deeper analysis by another brain might draw different conclusions. 2) Another good question. Based on my ""git blame"" detective work, `pairwise_special_metric` was introduced in [a change on 20 Nov 2018](https://github.com/lmcinnes/umap/commit/edade6841bd9b3c80454bf7f4386177c9aa35ab5) which should have seen it incorporated into version [0.3.7](https://github.com/lmcinnes/umap/tree/0.3.7). Since then its signature has remained compatible (with the only change being the addition of the `kwds` argument). scanpy's `requirements.txt` already has `umap-learn` set to a minimum version of 0.3.10 so I believe we're good on that front.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413
https://github.com/scverse/scanpy/pull/1413:1159,usability,minim,minimum,1159,"Thanks for the feedback. I have now merged master into the fix branch and the CI tests are happy. To address the shrewd questions asked by @gokceneraslan:. 1) I pondered over this myself. As I mentioned (above), I based my approach on a pull request in umap's own codebase which also resorts to densification of the matrix. As far as I can see, `pairwise_special_metric` doesn't directly support a sparse matrix and I don't see an alternative approach that wouldn't involve duplicating some of its implementation and (probably) sacrificing some of the benefits (i.e. parallel processing). A deeper analysis by another brain might draw different conclusions. 2) Another good question. Based on my ""git blame"" detective work, `pairwise_special_metric` was introduced in [a change on 20 Nov 2018](https://github.com/lmcinnes/umap/commit/edade6841bd9b3c80454bf7f4386177c9aa35ab5) which should have seen it incorporated into version [0.3.7](https://github.com/lmcinnes/umap/tree/0.3.7). Since then its signature has remained compatible (with the only change being the addition of the `kwds` argument). scanpy's `requirements.txt` already has `umap-learn` set to a minimum version of 0.3.10 so I believe we're good on that front.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413
https://github.com/scverse/scanpy/pull/1413:541,safety,safe,safer,541,"Hi @dwnorton, . Sorry for the delay. I can see now that there are a few changes about this on the [UMAP side](https://github.com/lmcinnes/umap/blob/master/umap/spectral.py#L98):. <img width=""617"" alt=""image"" src=""https://user-images.githubusercontent.com/1140359/115237075-1f214e80-a0ea-11eb-8257-8352f16ac2ad.png"">. Few questions:. - Can we improve the sparse array handling (e.g. mimicking UMAP e.g. using `SPARSE_SPECIAL_METRICS`) . - Can we also use the `SKLEARN_PAIRWISE_VALID_METRICS` instead of catching the ValueError? Would that be safer? - If we start using these keywords, do we need to change any of the requirements of scanpy?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413
https://github.com/scverse/scanpy/pull/1413:221,usability,user,user-images,221,"Hi @dwnorton, . Sorry for the delay. I can see now that there are a few changes about this on the [UMAP side](https://github.com/lmcinnes/umap/blob/master/umap/spectral.py#L98):. <img width=""617"" alt=""image"" src=""https://user-images.githubusercontent.com/1140359/115237075-1f214e80-a0ea-11eb-8257-8352f16ac2ad.png"">. Few questions:. - Can we improve the sparse array handling (e.g. mimicking UMAP e.g. using `SPARSE_SPECIAL_METRICS`) . - Can we also use the `SKLEARN_PAIRWISE_VALID_METRICS` instead of catching the ValueError? Would that be safer? - If we start using these keywords, do we need to change any of the requirements of scanpy?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413
https://github.com/scverse/scanpy/pull/1413:166,usability,custom,custom-metric-for-scanpy-pp-neighbors,166,"Would it be easier to just use NNDescent's handling of distances here? [Also, this was brought up on the discourse recently](https://scanpy.discourse.group/t/using-a-custom-metric-for-scanpy-pp-neighbors/477/3).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413
https://github.com/scverse/scanpy/pull/1413:129,energy efficiency,current,current,129,"@dwnorton seems fine, but could you please commit anything here to trigger ci? @ivirshup do we want to merge this or replace the current neighbors computation method with nndescent?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413
https://github.com/scverse/scanpy/pull/1413:240,safety,test,tests,240,Hello. Sorry I have been silent but I haven't used scanpy for a few months and thought there was nothing more for me to do with this pull request. @Koncopd: the commit I made in September last year (e4483e9) triggered the CI and passed the tests. Is there something further you need me to do? Thanks.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413
https://github.com/scverse/scanpy/pull/1413:240,testability,test,tests,240,Hello. Sorry I have been silent but I haven't used scanpy for a few months and thought there was nothing more for me to do with this pull request. @Koncopd: the commit I made in September last year (e4483e9) triggered the CI and passed the tests. Is there something further you need me to do? Thanks.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413
https://github.com/scverse/scanpy/pull/1413:71,testability,simpl,simple,71,"Long term, I'd prefer to just use pynndescent since it would be a more simple implementation. That could change some results. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413
https://github.com/scverse/scanpy/pull/1413:15,usability,prefer,prefer,15,"Long term, I'd prefer to just use pynndescent since it would be a more simple implementation. That could change some results. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413
https://github.com/scverse/scanpy/pull/1413:71,usability,simpl,simple,71,"Long term, I'd prefer to just use pynndescent since it would be a more simple implementation. That could change some results. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413
https://github.com/scverse/scanpy/pull/1413:23,usability,prefer,prefer,23,"@ivirshup i would also prefer pynndescent. I can write it, but i need to look and check that there are no potential problems with this replacement.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413
https://github.com/scverse/scanpy/pull/1413:115,deployability,log,logic,115,"@Koncopd when we talked about this last there were concerns about backwards reproducibility. I'm wondering if this logic would fix that:. * If the dataset is ""small"" and the metric is defined by scikit-learn, compute complete distances. * For all other cases use pynndescent. Would this be sufficient to keep results the same, or do we compute dense distances for the more esoteric metrics now?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413
https://github.com/scverse/scanpy/pull/1413:51,modifiability,concern,concerns,51,"@Koncopd when we talked about this last there were concerns about backwards reproducibility. I'm wondering if this logic would fix that:. * If the dataset is ""small"" and the metric is defined by scikit-learn, compute complete distances. * For all other cases use pynndescent. Would this be sufficient to keep results the same, or do we compute dense distances for the more esoteric metrics now?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413
https://github.com/scverse/scanpy/pull/1413:115,safety,log,logic,115,"@Koncopd when we talked about this last there were concerns about backwards reproducibility. I'm wondering if this logic would fix that:. * If the dataset is ""small"" and the metric is defined by scikit-learn, compute complete distances. * For all other cases use pynndescent. Would this be sufficient to keep results the same, or do we compute dense distances for the more esoteric metrics now?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413
https://github.com/scverse/scanpy/pull/1413:217,safety,compl,complete,217,"@Koncopd when we talked about this last there were concerns about backwards reproducibility. I'm wondering if this logic would fix that:. * If the dataset is ""small"" and the metric is defined by scikit-learn, compute complete distances. * For all other cases use pynndescent. Would this be sufficient to keep results the same, or do we compute dense distances for the more esoteric metrics now?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413
https://github.com/scverse/scanpy/pull/1413:115,security,log,logic,115,"@Koncopd when we talked about this last there were concerns about backwards reproducibility. I'm wondering if this logic would fix that:. * If the dataset is ""small"" and the metric is defined by scikit-learn, compute complete distances. * For all other cases use pynndescent. Would this be sufficient to keep results the same, or do we compute dense distances for the more esoteric metrics now?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413
https://github.com/scverse/scanpy/pull/1413:217,security,compl,complete,217,"@Koncopd when we talked about this last there were concerns about backwards reproducibility. I'm wondering if this logic would fix that:. * If the dataset is ""small"" and the metric is defined by scikit-learn, compute complete distances. * For all other cases use pynndescent. Would this be sufficient to keep results the same, or do we compute dense distances for the more esoteric metrics now?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413
https://github.com/scverse/scanpy/pull/1413:51,testability,concern,concerns,51,"@Koncopd when we talked about this last there were concerns about backwards reproducibility. I'm wondering if this logic would fix that:. * If the dataset is ""small"" and the metric is defined by scikit-learn, compute complete distances. * For all other cases use pynndescent. Would this be sufficient to keep results the same, or do we compute dense distances for the more esoteric metrics now?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413
https://github.com/scverse/scanpy/pull/1413:115,testability,log,logic,115,"@Koncopd when we talked about this last there were concerns about backwards reproducibility. I'm wondering if this logic would fix that:. * If the dataset is ""small"" and the metric is defined by scikit-learn, compute complete distances. * For all other cases use pynndescent. Would this be sufficient to keep results the same, or do we compute dense distances for the more esoteric metrics now?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413
https://github.com/scverse/scanpy/pull/1413:202,usability,learn,learn,202,"@Koncopd when we talked about this last there were concerns about backwards reproducibility. I'm wondering if this logic would fix that:. * If the dataset is ""small"" and the metric is defined by scikit-learn, compute complete distances. * For all other cases use pynndescent. Would this be sufficient to keep results the same, or do we compute dense distances for the more esoteric metrics now?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413
https://github.com/scverse/scanpy/pull/1413:239,deployability,API,API,239,"This is all possible with `sc.pp.neighbors(..., transformer=""pynndescent"")`, except for metrics that are only implemented in umap, like `ll_dirichlet`. If we want to support these, we should find a way to get them work nicely with the new API, so Im closing this for now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413
https://github.com/scverse/scanpy/pull/1413:48,integrability,transform,transformer,48,"This is all possible with `sc.pp.neighbors(..., transformer=""pynndescent"")`, except for metrics that are only implemented in umap, like `ll_dirichlet`. If we want to support these, we should find a way to get them work nicely with the new API, so Im closing this for now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413
https://github.com/scverse/scanpy/pull/1413:239,integrability,API,API,239,"This is all possible with `sc.pp.neighbors(..., transformer=""pynndescent"")`, except for metrics that are only implemented in umap, like `ll_dirichlet`. If we want to support these, we should find a way to get them work nicely with the new API, so Im closing this for now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413
https://github.com/scverse/scanpy/pull/1413:48,interoperability,transform,transformer,48,"This is all possible with `sc.pp.neighbors(..., transformer=""pynndescent"")`, except for metrics that are only implemented in umap, like `ll_dirichlet`. If we want to support these, we should find a way to get them work nicely with the new API, so Im closing this for now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413
https://github.com/scverse/scanpy/pull/1413:239,interoperability,API,API,239,"This is all possible with `sc.pp.neighbors(..., transformer=""pynndescent"")`, except for metrics that are only implemented in umap, like `ll_dirichlet`. If we want to support these, we should find a way to get them work nicely with the new API, so Im closing this for now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413
https://github.com/scverse/scanpy/pull/1413:77,safety,except,except,77,"This is all possible with `sc.pp.neighbors(..., transformer=""pynndescent"")`, except for metrics that are only implemented in umap, like `ll_dirichlet`. If we want to support these, we should find a way to get them work nicely with the new API, so Im closing this for now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413
https://github.com/scverse/scanpy/pull/1413:166,usability,support,support,166,"This is all possible with `sc.pp.neighbors(..., transformer=""pynndescent"")`, except for metrics that are only implemented in umap, like `ll_dirichlet`. If we want to support these, we should find a way to get them work nicely with the new API, so Im closing this for now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413
https://github.com/scverse/scanpy/issues/1414:7,safety,detect,detect,7,"when I detect the with adata.obs[""seurat_clusters""].dtype.name != 'category'. the resault shows Ture",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1414
https://github.com/scverse/scanpy/issues/1414:7,security,detect,detect,7,"when I detect the with adata.obs[""seurat_clusters""].dtype.name != 'category'. the resault shows Ture",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1414
https://github.com/scverse/scanpy/issues/1414:25,availability,error,error,25,"Hi @john-jiangyong,. The error is that your covariate you group by should be a categorical, while it is not at the moment. You could run:. `adata.obs[seurat_clusters] = adata.obs[seurat_clusters].astype(category)` To turn the covariate into a categorical. Note the code above might not be 100% correct as im typing from my phone and havent verified.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1414
https://github.com/scverse/scanpy/issues/1414:25,performance,error,error,25,"Hi @john-jiangyong,. The error is that your covariate you group by should be a categorical, while it is not at the moment. You could run:. `adata.obs[seurat_clusters] = adata.obs[seurat_clusters].astype(category)` To turn the covariate into a categorical. Note the code above might not be 100% correct as im typing from my phone and havent verified.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1414
https://github.com/scverse/scanpy/issues/1414:25,safety,error,error,25,"Hi @john-jiangyong,. The error is that your covariate you group by should be a categorical, while it is not at the moment. You could run:. `adata.obs[seurat_clusters] = adata.obs[seurat_clusters].astype(category)` To turn the covariate into a categorical. Note the code above might not be 100% correct as im typing from my phone and havent verified.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1414
https://github.com/scverse/scanpy/issues/1414:348,testability,verif,verified,348,"Hi @john-jiangyong,. The error is that your covariate you group by should be a categorical, while it is not at the moment. You could run:. `adata.obs[seurat_clusters] = adata.obs[seurat_clusters].astype(category)` To turn the covariate into a categorical. Note the code above might not be 100% correct as im typing from my phone and havent verified.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1414
https://github.com/scverse/scanpy/issues/1414:25,usability,error,error,25,"Hi @john-jiangyong,. The error is that your covariate you group by should be a categorical, while it is not at the moment. You could run:. `adata.obs[seurat_clusters] = adata.obs[seurat_clusters].astype(category)` To turn the covariate into a categorical. Note the code above might not be 100% correct as im typing from my phone and havent verified.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1414
https://github.com/scverse/scanpy/issues/1415:261,availability,operat,operations,261,"I'm a little confused about the question. `normalize_total` does try to modify the data inplace:. ```python. adata = sc.AnnData(np.arange(16).reshape((4, 4))). a = adata.X. sc.pp.normalize_total(adata). assert a is adata.X. ```. In general, we try to make most operations in place for efficiency. This should allow people to work with datasets that fit uncomfortably in memory, which might not be an option if a copy was made. Your screenshot does show some weirdness in the anndata constructor where a copy get's made, which there are more details on here: https://github.com/theislab/anndata/issues/129. Basically there's a line in the constructor where:. ```python. adata.X = X.astype(dtype=dtype). ```. where `dtype` defaults to float32. This is something we'd like to remove, but it was troublesome when last attempted.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1415
https://github.com/scverse/scanpy/issues/1415:370,performance,memor,memory,370,"I'm a little confused about the question. `normalize_total` does try to modify the data inplace:. ```python. adata = sc.AnnData(np.arange(16).reshape((4, 4))). a = adata.X. sc.pp.normalize_total(adata). assert a is adata.X. ```. In general, we try to make most operations in place for efficiency. This should allow people to work with datasets that fit uncomfortably in memory, which might not be an option if a copy was made. Your screenshot does show some weirdness in the anndata constructor where a copy get's made, which there are more details on here: https://github.com/theislab/anndata/issues/129. Basically there's a line in the constructor where:. ```python. adata.X = X.astype(dtype=dtype). ```. where `dtype` defaults to float32. This is something we'd like to remove, but it was troublesome when last attempted.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1415
https://github.com/scverse/scanpy/issues/1415:60,reliability,doe,does,60,"I'm a little confused about the question. `normalize_total` does try to modify the data inplace:. ```python. adata = sc.AnnData(np.arange(16).reshape((4, 4))). a = adata.X. sc.pp.normalize_total(adata). assert a is adata.X. ```. In general, we try to make most operations in place for efficiency. This should allow people to work with datasets that fit uncomfortably in memory, which might not be an option if a copy was made. Your screenshot does show some weirdness in the anndata constructor where a copy get's made, which there are more details on here: https://github.com/theislab/anndata/issues/129. Basically there's a line in the constructor where:. ```python. adata.X = X.astype(dtype=dtype). ```. where `dtype` defaults to float32. This is something we'd like to remove, but it was troublesome when last attempted.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1415
https://github.com/scverse/scanpy/issues/1415:443,reliability,doe,does,443,"I'm a little confused about the question. `normalize_total` does try to modify the data inplace:. ```python. adata = sc.AnnData(np.arange(16).reshape((4, 4))). a = adata.X. sc.pp.normalize_total(adata). assert a is adata.X. ```. In general, we try to make most operations in place for efficiency. This should allow people to work with datasets that fit uncomfortably in memory, which might not be an option if a copy was made. Your screenshot does show some weirdness in the anndata constructor where a copy get's made, which there are more details on here: https://github.com/theislab/anndata/issues/129. Basically there's a line in the constructor where:. ```python. adata.X = X.astype(dtype=dtype). ```. where `dtype` defaults to float32. This is something we'd like to remove, but it was troublesome when last attempted.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1415
https://github.com/scverse/scanpy/issues/1415:72,security,modif,modify,72,"I'm a little confused about the question. `normalize_total` does try to modify the data inplace:. ```python. adata = sc.AnnData(np.arange(16).reshape((4, 4))). a = adata.X. sc.pp.normalize_total(adata). assert a is adata.X. ```. In general, we try to make most operations in place for efficiency. This should allow people to work with datasets that fit uncomfortably in memory, which might not be an option if a copy was made. Your screenshot does show some weirdness in the anndata constructor where a copy get's made, which there are more details on here: https://github.com/theislab/anndata/issues/129. Basically there's a line in the constructor where:. ```python. adata.X = X.astype(dtype=dtype). ```. where `dtype` defaults to float32. This is something we'd like to remove, but it was troublesome when last attempted.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1415
https://github.com/scverse/scanpy/issues/1415:203,testability,assert,assert,203,"I'm a little confused about the question. `normalize_total` does try to modify the data inplace:. ```python. adata = sc.AnnData(np.arange(16).reshape((4, 4))). a = adata.X. sc.pp.normalize_total(adata). assert a is adata.X. ```. In general, we try to make most operations in place for efficiency. This should allow people to work with datasets that fit uncomfortably in memory, which might not be an option if a copy was made. Your screenshot does show some weirdness in the anndata constructor where a copy get's made, which there are more details on here: https://github.com/theislab/anndata/issues/129. Basically there's a line in the constructor where:. ```python. adata.X = X.astype(dtype=dtype). ```. where `dtype` defaults to float32. This is something we'd like to remove, but it was troublesome when last attempted.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1415
https://github.com/scverse/scanpy/issues/1415:285,usability,efficien,efficiency,285,"I'm a little confused about the question. `normalize_total` does try to modify the data inplace:. ```python. adata = sc.AnnData(np.arange(16).reshape((4, 4))). a = adata.X. sc.pp.normalize_total(adata). assert a is adata.X. ```. In general, we try to make most operations in place for efficiency. This should allow people to work with datasets that fit uncomfortably in memory, which might not be an option if a copy was made. Your screenshot does show some weirdness in the anndata constructor where a copy get's made, which there are more details on here: https://github.com/theislab/anndata/issues/129. Basically there's a line in the constructor where:. ```python. adata.X = X.astype(dtype=dtype). ```. where `dtype` defaults to float32. This is something we'd like to remove, but it was troublesome when last attempted.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1415
https://github.com/scverse/scanpy/issues/1415:370,usability,memor,memory,370,"I'm a little confused about the question. `normalize_total` does try to modify the data inplace:. ```python. adata = sc.AnnData(np.arange(16).reshape((4, 4))). a = adata.X. sc.pp.normalize_total(adata). assert a is adata.X. ```. In general, we try to make most operations in place for efficiency. This should allow people to work with datasets that fit uncomfortably in memory, which might not be an option if a copy was made. Your screenshot does show some weirdness in the anndata constructor where a copy get's made, which there are more details on here: https://github.com/theislab/anndata/issues/129. Basically there's a line in the constructor where:. ```python. adata.X = X.astype(dtype=dtype). ```. where `dtype` defaults to float32. This is something we'd like to remove, but it was troublesome when last attempted.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1415
https://github.com/scverse/scanpy/issues/1415:111,availability,state,stated,111,"I must've mixed up `normalize_total` and `normalize_per_cell`, which I know is deprecated. I know it's clearly stated in the anndata docs regarding the float32 copy issue, but it's really quite confusing! ```python. In [1]: import scanpy as sc. In [2]: import numpy as np. In [3]: a = np.arange(16, dtype=np.float32).reshape((4, 4)). In [4]: adata = sc.AnnData(a). In [5]: sc.pp.normalize_total(adata). In [6]: adata.X. Out[6]: . array([[ 0. , 5. , 10. , 15. ],. [ 5.4545455, 6.8181815, 8.181818 , 9.545454 ],. [ 6.3157897, 7.105263 , 7.894737 , 8.684211 ],. [ 6.666667 , 7.2222223, 7.777778 , 8.333334 ]], dtype=float32). In [7]: a. Out[7]: . array([[ 0. , 5. , 10. , 15. ],. [ 5.4545455, 6.8181815, 8.181818 , 9.545454 ],. [ 6.3157897, 7.105263 , 7.894737 , 8.684211 ],. [ 6.666667 , 7.2222223, 7.777778 , 8.333334 ]], dtype=float32). In [9]: a = np.arange(16, dtype=np.float32).reshape((4, 4)). In [10]: adata = sc.AnnData(a). In [11]: sc.pp.normalize_per_cell(adata). In [12]: adata.X. Out[12]: . array([[ 0. , 5. , 10. , 15. ],. [ 5.4545455, 6.8181815, 8.181818 , 9.545454 ],. [ 6.3157897, 7.105263 , 7.894737 , 8.684211 ],. [ 6.666667 , 7.2222223, 7.777778 , 8.333334 ]], dtype=float32). In [13]: a. Out[13]: . array([[ 0., 1., 2., 3.],. [ 4., 5., 6., 7.],. [ 8., 9., 10., 11.],. [12., 13., 14., 15.]], dtype=float32). ```. Edit: So I guess the real culprit is the float32 issue with AnnData. Is this something you all plan to address soon?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1415
https://github.com/scverse/scanpy/issues/1415:111,integrability,state,stated,111,"I must've mixed up `normalize_total` and `normalize_per_cell`, which I know is deprecated. I know it's clearly stated in the anndata docs regarding the float32 copy issue, but it's really quite confusing! ```python. In [1]: import scanpy as sc. In [2]: import numpy as np. In [3]: a = np.arange(16, dtype=np.float32).reshape((4, 4)). In [4]: adata = sc.AnnData(a). In [5]: sc.pp.normalize_total(adata). In [6]: adata.X. Out[6]: . array([[ 0. , 5. , 10. , 15. ],. [ 5.4545455, 6.8181815, 8.181818 , 9.545454 ],. [ 6.3157897, 7.105263 , 7.894737 , 8.684211 ],. [ 6.666667 , 7.2222223, 7.777778 , 8.333334 ]], dtype=float32). In [7]: a. Out[7]: . array([[ 0. , 5. , 10. , 15. ],. [ 5.4545455, 6.8181815, 8.181818 , 9.545454 ],. [ 6.3157897, 7.105263 , 7.894737 , 8.684211 ],. [ 6.666667 , 7.2222223, 7.777778 , 8.333334 ]], dtype=float32). In [9]: a = np.arange(16, dtype=np.float32).reshape((4, 4)). In [10]: adata = sc.AnnData(a). In [11]: sc.pp.normalize_per_cell(adata). In [12]: adata.X. Out[12]: . array([[ 0. , 5. , 10. , 15. ],. [ 5.4545455, 6.8181815, 8.181818 , 9.545454 ],. [ 6.3157897, 7.105263 , 7.894737 , 8.684211 ],. [ 6.666667 , 7.2222223, 7.777778 , 8.333334 ]], dtype=float32). In [13]: a. Out[13]: . array([[ 0., 1., 2., 3.],. [ 4., 5., 6., 7.],. [ 8., 9., 10., 11.],. [12., 13., 14., 15.]], dtype=float32). ```. Edit: So I guess the real culprit is the float32 issue with AnnData. Is this something you all plan to address soon?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1415
https://github.com/scverse/scanpy/issues/1415:1425,testability,plan,plan,1425,"I must've mixed up `normalize_total` and `normalize_per_cell`, which I know is deprecated. I know it's clearly stated in the anndata docs regarding the float32 copy issue, but it's really quite confusing! ```python. In [1]: import scanpy as sc. In [2]: import numpy as np. In [3]: a = np.arange(16, dtype=np.float32).reshape((4, 4)). In [4]: adata = sc.AnnData(a). In [5]: sc.pp.normalize_total(adata). In [6]: adata.X. Out[6]: . array([[ 0. , 5. , 10. , 15. ],. [ 5.4545455, 6.8181815, 8.181818 , 9.545454 ],. [ 6.3157897, 7.105263 , 7.894737 , 8.684211 ],. [ 6.666667 , 7.2222223, 7.777778 , 8.333334 ]], dtype=float32). In [7]: a. Out[7]: . array([[ 0. , 5. , 10. , 15. ],. [ 5.4545455, 6.8181815, 8.181818 , 9.545454 ],. [ 6.3157897, 7.105263 , 7.894737 , 8.684211 ],. [ 6.666667 , 7.2222223, 7.777778 , 8.333334 ]], dtype=float32). In [9]: a = np.arange(16, dtype=np.float32).reshape((4, 4)). In [10]: adata = sc.AnnData(a). In [11]: sc.pp.normalize_per_cell(adata). In [12]: adata.X. Out[12]: . array([[ 0. , 5. , 10. , 15. ],. [ 5.4545455, 6.8181815, 8.181818 , 9.545454 ],. [ 6.3157897, 7.105263 , 7.894737 , 8.684211 ],. [ 6.666667 , 7.2222223, 7.777778 , 8.333334 ]], dtype=float32). In [13]: a. Out[13]: . array([[ 0., 1., 2., 3.],. [ 4., 5., 6., 7.],. [ 8., 9., 10., 11.],. [12., 13., 14., 15.]], dtype=float32). ```. Edit: So I guess the real culprit is the float32 issue with AnnData. Is this something you all plan to address soon?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1415
https://github.com/scverse/scanpy/issues/1415:103,usability,clear,clearly,103,"I must've mixed up `normalize_total` and `normalize_per_cell`, which I know is deprecated. I know it's clearly stated in the anndata docs regarding the float32 copy issue, but it's really quite confusing! ```python. In [1]: import scanpy as sc. In [2]: import numpy as np. In [3]: a = np.arange(16, dtype=np.float32).reshape((4, 4)). In [4]: adata = sc.AnnData(a). In [5]: sc.pp.normalize_total(adata). In [6]: adata.X. Out[6]: . array([[ 0. , 5. , 10. , 15. ],. [ 5.4545455, 6.8181815, 8.181818 , 9.545454 ],. [ 6.3157897, 7.105263 , 7.894737 , 8.684211 ],. [ 6.666667 , 7.2222223, 7.777778 , 8.333334 ]], dtype=float32). In [7]: a. Out[7]: . array([[ 0. , 5. , 10. , 15. ],. [ 5.4545455, 6.8181815, 8.181818 , 9.545454 ],. [ 6.3157897, 7.105263 , 7.894737 , 8.684211 ],. [ 6.666667 , 7.2222223, 7.777778 , 8.333334 ]], dtype=float32). In [9]: a = np.arange(16, dtype=np.float32).reshape((4, 4)). In [10]: adata = sc.AnnData(a). In [11]: sc.pp.normalize_per_cell(adata). In [12]: adata.X. Out[12]: . array([[ 0. , 5. , 10. , 15. ],. [ 5.4545455, 6.8181815, 8.181818 , 9.545454 ],. [ 6.3157897, 7.105263 , 7.894737 , 8.684211 ],. [ 6.666667 , 7.2222223, 7.777778 , 8.333334 ]], dtype=float32). In [13]: a. Out[13]: . array([[ 0., 1., 2., 3.],. [ 4., 5., 6., 7.],. [ 8., 9., 10., 11.],. [12., 13., 14., 15.]], dtype=float32). ```. Edit: So I guess the real culprit is the float32 issue with AnnData. Is this something you all plan to address soon?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1415
https://github.com/scverse/scanpy/issues/1415:207,availability,failur,failures,207,"> So I guess the real culprit is the float32 issue with AnnData. Is this something you all plan to address soon? I think we can do that. I did a quick check and it's pretty benign in anndata. It causes test failures a few places in scanpy, but I think that's solvable with some conversion. It is a breaking change, so it will need to be in anndata 0.8. But there's a few more minor changes I'd like to make, so maybe we can be quick on that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1415
https://github.com/scverse/scanpy/issues/1415:207,deployability,fail,failures,207,"> So I guess the real culprit is the float32 issue with AnnData. Is this something you all plan to address soon? I think we can do that. I did a quick check and it's pretty benign in anndata. It causes test failures a few places in scanpy, but I think that's solvable with some conversion. It is a breaking change, so it will need to be in anndata 0.8. But there's a few more minor changes I'd like to make, so maybe we can be quick on that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1415
https://github.com/scverse/scanpy/issues/1415:278,interoperability,convers,conversion,278,"> So I guess the real culprit is the float32 issue with AnnData. Is this something you all plan to address soon? I think we can do that. I did a quick check and it's pretty benign in anndata. It causes test failures a few places in scanpy, but I think that's solvable with some conversion. It is a breaking change, so it will need to be in anndata 0.8. But there's a few more minor changes I'd like to make, so maybe we can be quick on that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1415
https://github.com/scverse/scanpy/issues/1415:207,performance,failur,failures,207,"> So I guess the real culprit is the float32 issue with AnnData. Is this something you all plan to address soon? I think we can do that. I did a quick check and it's pretty benign in anndata. It causes test failures a few places in scanpy, but I think that's solvable with some conversion. It is a breaking change, so it will need to be in anndata 0.8. But there's a few more minor changes I'd like to make, so maybe we can be quick on that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1415
https://github.com/scverse/scanpy/issues/1415:207,reliability,fail,failures,207,"> So I guess the real culprit is the float32 issue with AnnData. Is this something you all plan to address soon? I think we can do that. I did a quick check and it's pretty benign in anndata. It causes test failures a few places in scanpy, but I think that's solvable with some conversion. It is a breaking change, so it will need to be in anndata 0.8. But there's a few more minor changes I'd like to make, so maybe we can be quick on that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1415
https://github.com/scverse/scanpy/issues/1415:202,safety,test,test,202,"> So I guess the real culprit is the float32 issue with AnnData. Is this something you all plan to address soon? I think we can do that. I did a quick check and it's pretty benign in anndata. It causes test failures a few places in scanpy, but I think that's solvable with some conversion. It is a breaking change, so it will need to be in anndata 0.8. But there's a few more minor changes I'd like to make, so maybe we can be quick on that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1415
https://github.com/scverse/scanpy/issues/1415:91,testability,plan,plan,91,"> So I guess the real culprit is the float32 issue with AnnData. Is this something you all plan to address soon? I think we can do that. I did a quick check and it's pretty benign in anndata. It causes test failures a few places in scanpy, but I think that's solvable with some conversion. It is a breaking change, so it will need to be in anndata 0.8. But there's a few more minor changes I'd like to make, so maybe we can be quick on that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1415
https://github.com/scverse/scanpy/issues/1415:202,testability,test,test,202,"> So I guess the real culprit is the float32 issue with AnnData. Is this something you all plan to address soon? I think we can do that. I did a quick check and it's pretty benign in anndata. It causes test failures a few places in scanpy, but I think that's solvable with some conversion. It is a breaking change, so it will need to be in anndata 0.8. But there's a few more minor changes I'd like to make, so maybe we can be quick on that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1415
https://github.com/scverse/scanpy/issues/1415:131,safety,avoid,avoid,131,"We will always try to make `.X` a reference to the passed array. There may be cases where we need to make a copy, but we'll try to avoid that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1415
https://github.com/scverse/scanpy/pull/1417:10,deployability,fail,failing,10,Tests are failing and I suspect that this is caused by an update on seaborn or matplotlib...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1417
https://github.com/scverse/scanpy/pull/1417:58,deployability,updat,update,58,Tests are failing and I suspect that this is caused by an update on seaborn or matplotlib...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1417
https://github.com/scverse/scanpy/pull/1417:10,reliability,fail,failing,10,Tests are failing and I suspect that this is caused by an update on seaborn or matplotlib...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1417
https://github.com/scverse/scanpy/pull/1417:0,safety,Test,Tests,0,Tests are failing and I suspect that this is caused by an update on seaborn or matplotlib...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1417
https://github.com/scverse/scanpy/pull/1417:58,safety,updat,update,58,Tests are failing and I suspect that this is caused by an update on seaborn or matplotlib...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1417
https://github.com/scverse/scanpy/pull/1417:58,security,updat,update,58,Tests are failing and I suspect that this is caused by an update on seaborn or matplotlib...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1417
https://github.com/scverse/scanpy/pull/1417:0,testability,Test,Tests,0,Tests are failing and I suspect that this is caused by an update on seaborn or matplotlib...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1417
https://github.com/scverse/scanpy/pull/1417:12,deployability,fail,failing,12,"> Tests are failing and I suspect that this is caused by an update on seaborn or matplotlib... Yes, should be as the introduced changes are not linked to the failing tests. I also checked and both `seaborn` and `matplotlib` have been updated in the last few days. See [here](https://pypi.org/project/seaborn/#history) and [here](https://pypi.org/project/matplotlib/#history).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1417
https://github.com/scverse/scanpy/pull/1417:60,deployability,updat,update,60,"> Tests are failing and I suspect that this is caused by an update on seaborn or matplotlib... Yes, should be as the introduced changes are not linked to the failing tests. I also checked and both `seaborn` and `matplotlib` have been updated in the last few days. See [here](https://pypi.org/project/seaborn/#history) and [here](https://pypi.org/project/matplotlib/#history).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1417
https://github.com/scverse/scanpy/pull/1417:158,deployability,fail,failing,158,"> Tests are failing and I suspect that this is caused by an update on seaborn or matplotlib... Yes, should be as the introduced changes are not linked to the failing tests. I also checked and both `seaborn` and `matplotlib` have been updated in the last few days. See [here](https://pypi.org/project/seaborn/#history) and [here](https://pypi.org/project/matplotlib/#history).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1417
https://github.com/scverse/scanpy/pull/1417:234,deployability,updat,updated,234,"> Tests are failing and I suspect that this is caused by an update on seaborn or matplotlib... Yes, should be as the introduced changes are not linked to the failing tests. I also checked and both `seaborn` and `matplotlib` have been updated in the last few days. See [here](https://pypi.org/project/seaborn/#history) and [here](https://pypi.org/project/matplotlib/#history).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1417
https://github.com/scverse/scanpy/pull/1417:12,reliability,fail,failing,12,"> Tests are failing and I suspect that this is caused by an update on seaborn or matplotlib... Yes, should be as the introduced changes are not linked to the failing tests. I also checked and both `seaborn` and `matplotlib` have been updated in the last few days. See [here](https://pypi.org/project/seaborn/#history) and [here](https://pypi.org/project/matplotlib/#history).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1417
https://github.com/scverse/scanpy/pull/1417:158,reliability,fail,failing,158,"> Tests are failing and I suspect that this is caused by an update on seaborn or matplotlib... Yes, should be as the introduced changes are not linked to the failing tests. I also checked and both `seaborn` and `matplotlib` have been updated in the last few days. See [here](https://pypi.org/project/seaborn/#history) and [here](https://pypi.org/project/matplotlib/#history).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1417
https://github.com/scverse/scanpy/pull/1417:2,safety,Test,Tests,2,"> Tests are failing and I suspect that this is caused by an update on seaborn or matplotlib... Yes, should be as the introduced changes are not linked to the failing tests. I also checked and both `seaborn` and `matplotlib` have been updated in the last few days. See [here](https://pypi.org/project/seaborn/#history) and [here](https://pypi.org/project/matplotlib/#history).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1417
https://github.com/scverse/scanpy/pull/1417:60,safety,updat,update,60,"> Tests are failing and I suspect that this is caused by an update on seaborn or matplotlib... Yes, should be as the introduced changes are not linked to the failing tests. I also checked and both `seaborn` and `matplotlib` have been updated in the last few days. See [here](https://pypi.org/project/seaborn/#history) and [here](https://pypi.org/project/matplotlib/#history).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1417
https://github.com/scverse/scanpy/pull/1417:166,safety,test,tests,166,"> Tests are failing and I suspect that this is caused by an update on seaborn or matplotlib... Yes, should be as the introduced changes are not linked to the failing tests. I also checked and both `seaborn` and `matplotlib` have been updated in the last few days. See [here](https://pypi.org/project/seaborn/#history) and [here](https://pypi.org/project/matplotlib/#history).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1417
https://github.com/scverse/scanpy/pull/1417:234,safety,updat,updated,234,"> Tests are failing and I suspect that this is caused by an update on seaborn or matplotlib... Yes, should be as the introduced changes are not linked to the failing tests. I also checked and both `seaborn` and `matplotlib` have been updated in the last few days. See [here](https://pypi.org/project/seaborn/#history) and [here](https://pypi.org/project/matplotlib/#history).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1417
https://github.com/scverse/scanpy/pull/1417:60,security,updat,update,60,"> Tests are failing and I suspect that this is caused by an update on seaborn or matplotlib... Yes, should be as the introduced changes are not linked to the failing tests. I also checked and both `seaborn` and `matplotlib` have been updated in the last few days. See [here](https://pypi.org/project/seaborn/#history) and [here](https://pypi.org/project/matplotlib/#history).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1417
https://github.com/scverse/scanpy/pull/1417:234,security,updat,updated,234,"> Tests are failing and I suspect that this is caused by an update on seaborn or matplotlib... Yes, should be as the introduced changes are not linked to the failing tests. I also checked and both `seaborn` and `matplotlib` have been updated in the last few days. See [here](https://pypi.org/project/seaborn/#history) and [here](https://pypi.org/project/matplotlib/#history).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1417
https://github.com/scverse/scanpy/pull/1417:2,testability,Test,Tests,2,"> Tests are failing and I suspect that this is caused by an update on seaborn or matplotlib... Yes, should be as the introduced changes are not linked to the failing tests. I also checked and both `seaborn` and `matplotlib` have been updated in the last few days. See [here](https://pypi.org/project/seaborn/#history) and [here](https://pypi.org/project/matplotlib/#history).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1417
https://github.com/scverse/scanpy/pull/1417:166,testability,test,tests,166,"> Tests are failing and I suspect that this is caused by an update on seaborn or matplotlib... Yes, should be as the introduced changes are not linked to the failing tests. I also checked and both `seaborn` and `matplotlib` have been updated in the last few days. See [here](https://pypi.org/project/seaborn/#history) and [here](https://pypi.org/project/matplotlib/#history).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1417
https://github.com/scverse/scanpy/pull/1417:40,deployability,version,versions,40,Thanks. I just confirmed that the newer versions for both packages were used in the tests. I am checking now what is the difference.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1417
https://github.com/scverse/scanpy/pull/1417:40,integrability,version,versions,40,Thanks. I just confirmed that the newer versions for both packages were used in the tests. I am checking now what is the difference.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1417
https://github.com/scverse/scanpy/pull/1417:40,modifiability,version,versions,40,Thanks. I just confirmed that the newer versions for both packages were used in the tests. I am checking now what is the difference.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1417
https://github.com/scverse/scanpy/pull/1417:58,modifiability,pac,packages,58,Thanks. I just confirmed that the newer versions for both packages were used in the tests. I am checking now what is the difference.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1417
https://github.com/scverse/scanpy/pull/1417:84,safety,test,tests,84,Thanks. I just confirmed that the newer versions for both packages were used in the tests. I am checking now what is the difference.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1417
https://github.com/scverse/scanpy/pull/1417:84,testability,test,tests,84,Thanks. I just confirmed that the newer versions for both packages were used in the tests. I am checking now what is the difference.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1417
https://github.com/scverse/scanpy/pull/1417:15,usability,confirm,confirmed,15,Thanks. I just confirmed that the newer versions for both packages were used in the tests. I am checking now what is the difference.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1417
https://github.com/scverse/scanpy/pull/1417:17,safety,test,tests,17,"When running the tests locally with Python 3.7, `seaborn` seems to be the problem, as the tests pass for `matplotlib==3.3.2` and `seaborn==0.10.1`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1417
https://github.com/scverse/scanpy/pull/1417:90,safety,test,tests,90,"When running the tests locally with Python 3.7, `seaborn` seems to be the problem, as the tests pass for `matplotlib==3.3.2` and `seaborn==0.10.1`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1417
https://github.com/scverse/scanpy/pull/1417:17,testability,test,tests,17,"When running the tests locally with Python 3.7, `seaborn` seems to be the problem, as the tests pass for `matplotlib==3.3.2` and `seaborn==0.10.1`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1417
https://github.com/scverse/scanpy/pull/1417:90,testability,test,tests,90,"When running the tests locally with Python 3.7, `seaborn` seems to be the problem, as the tests pass for `matplotlib==3.3.2` and `seaborn==0.10.1`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1417
https://github.com/scverse/scanpy/pull/1417:114,usability,user,user-images,114,"Indeed, seaborn is the problem. I am getting the plots horizontal when they should be vertical:. ![image](https://user-images.githubusercontent.com/4964309/93359636-1c543780-f843-11ea-87fd-32b113819fe1.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1417
https://github.com/scverse/scanpy/pull/1417:393,deployability,stack,stack,393,The problem seems to be [here](https://github.com/theislab/scanpy/blob/f704f724529def21769ee6407f9b47b5c161564c/scanpy/plotting/_anndata.py#L745) and [here](https://github.com/theislab/scanpy/blob/f704f724529def21769ee6407f9b47b5c161564c/scanpy/plotting/_anndata.py#L747) as `seaborn` throws the warning. ```bash. UserWarning: Vertical orientation ignored with only `x` specified. ```. in the stack trace of the failed tests. I'll open a separate issue as it is unrelated to this PR.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1417
https://github.com/scverse/scanpy/pull/1417:412,deployability,fail,failed,412,The problem seems to be [here](https://github.com/theislab/scanpy/blob/f704f724529def21769ee6407f9b47b5c161564c/scanpy/plotting/_anndata.py#L745) and [here](https://github.com/theislab/scanpy/blob/f704f724529def21769ee6407f9b47b5c161564c/scanpy/plotting/_anndata.py#L747) as `seaborn` throws the warning. ```bash. UserWarning: Vertical orientation ignored with only `x` specified. ```. in the stack trace of the failed tests. I'll open a separate issue as it is unrelated to this PR.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1417
https://github.com/scverse/scanpy/pull/1417:370,interoperability,specif,specified,370,The problem seems to be [here](https://github.com/theislab/scanpy/blob/f704f724529def21769ee6407f9b47b5c161564c/scanpy/plotting/_anndata.py#L745) and [here](https://github.com/theislab/scanpy/blob/f704f724529def21769ee6407f9b47b5c161564c/scanpy/plotting/_anndata.py#L747) as `seaborn` throws the warning. ```bash. UserWarning: Vertical orientation ignored with only `x` specified. ```. in the stack trace of the failed tests. I'll open a separate issue as it is unrelated to this PR.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1417
https://github.com/scverse/scanpy/pull/1417:412,reliability,fail,failed,412,The problem seems to be [here](https://github.com/theislab/scanpy/blob/f704f724529def21769ee6407f9b47b5c161564c/scanpy/plotting/_anndata.py#L745) and [here](https://github.com/theislab/scanpy/blob/f704f724529def21769ee6407f9b47b5c161564c/scanpy/plotting/_anndata.py#L747) as `seaborn` throws the warning. ```bash. UserWarning: Vertical orientation ignored with only `x` specified. ```. in the stack trace of the failed tests. I'll open a separate issue as it is unrelated to this PR.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1417
https://github.com/scverse/scanpy/pull/1417:419,safety,test,tests,419,The problem seems to be [here](https://github.com/theislab/scanpy/blob/f704f724529def21769ee6407f9b47b5c161564c/scanpy/plotting/_anndata.py#L745) and [here](https://github.com/theislab/scanpy/blob/f704f724529def21769ee6407f9b47b5c161564c/scanpy/plotting/_anndata.py#L747) as `seaborn` throws the warning. ```bash. UserWarning: Vertical orientation ignored with only `x` specified. ```. in the stack trace of the failed tests. I'll open a separate issue as it is unrelated to this PR.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1417
https://github.com/scverse/scanpy/pull/1417:399,testability,trace,trace,399,The problem seems to be [here](https://github.com/theislab/scanpy/blob/f704f724529def21769ee6407f9b47b5c161564c/scanpy/plotting/_anndata.py#L745) and [here](https://github.com/theislab/scanpy/blob/f704f724529def21769ee6407f9b47b5c161564c/scanpy/plotting/_anndata.py#L747) as `seaborn` throws the warning. ```bash. UserWarning: Vertical orientation ignored with only `x` specified. ```. in the stack trace of the failed tests. I'll open a separate issue as it is unrelated to this PR.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1417
https://github.com/scverse/scanpy/pull/1417:419,testability,test,tests,419,The problem seems to be [here](https://github.com/theislab/scanpy/blob/f704f724529def21769ee6407f9b47b5c161564c/scanpy/plotting/_anndata.py#L745) and [here](https://github.com/theislab/scanpy/blob/f704f724529def21769ee6407f9b47b5c161564c/scanpy/plotting/_anndata.py#L747) as `seaborn` throws the warning. ```bash. UserWarning: Vertical orientation ignored with only `x` specified. ```. in the stack trace of the failed tests. I'll open a separate issue as it is unrelated to this PR.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1417
https://github.com/scverse/scanpy/pull/1417:314,usability,User,UserWarning,314,The problem seems to be [here](https://github.com/theislab/scanpy/blob/f704f724529def21769ee6407f9b47b5c161564c/scanpy/plotting/_anndata.py#L745) and [here](https://github.com/theislab/scanpy/blob/f704f724529def21769ee6407f9b47b5c161564c/scanpy/plotting/_anndata.py#L747) as `seaborn` throws the warning. ```bash. UserWarning: Vertical orientation ignored with only `x` specified. ```. in the stack trace of the failed tests. I'll open a separate issue as it is unrelated to this PR.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1417
https://github.com/scverse/scanpy/pull/1417:40,safety,test,tests,40,"@fidelram, I merged with master and all tests pass. Should be ready to merge.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1417
https://github.com/scverse/scanpy/pull/1417:40,testability,test,tests,40,"@fidelram, I merged with master and all tests pass. Should be ready to merge.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1417
https://github.com/scverse/scanpy/issues/1418:95,availability,error,errors,95,Did you check if the expected and actual images were significantly different. Some times I get errors locally because is difficult to have an identical environment as the one used by Travis.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1418
https://github.com/scverse/scanpy/issues/1418:83,performance,time,times,83,Did you check if the expected and actual images were significantly different. Some times I get errors locally because is difficult to have an identical environment as the one used by Travis.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1418
https://github.com/scverse/scanpy/issues/1418:95,performance,error,errors,95,Did you check if the expected and actual images were significantly different. Some times I get errors locally because is difficult to have an identical environment as the one used by Travis.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1418
https://github.com/scverse/scanpy/issues/1418:95,safety,error,errors,95,Did you check if the expected and actual images were significantly different. Some times I get errors locally because is difficult to have an identical environment as the one used by Travis.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1418
https://github.com/scverse/scanpy/issues/1418:53,security,sign,significantly,53,Did you check if the expected and actual images were significantly different. Some times I get errors locally because is difficult to have an identical environment as the one used by Travis.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1418
https://github.com/scverse/scanpy/issues/1418:142,security,ident,identical,142,Did you check if the expected and actual images were significantly different. Some times I get errors locally because is difficult to have an identical environment as the one used by Travis.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1418
https://github.com/scverse/scanpy/issues/1418:95,usability,error,errors,95,Did you check if the expected and actual images were significantly different. Some times I get errors locally because is difficult to have an identical environment as the one used by Travis.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1418
https://github.com/scverse/scanpy/issues/1419:39,availability,avail,available,39,"Realised this functionality is already available via `pip install "".[dev]""`. May be good to mention somewhere, though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419
https://github.com/scverse/scanpy/issues/1419:58,deployability,instal,install,58,"Realised this functionality is already available via `pip install "".[dev]""`. May be good to mention somewhere, though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419
https://github.com/scverse/scanpy/issues/1419:39,reliability,availab,available,39,"Realised this functionality is already available via `pip install "".[dev]""`. May be good to mention somewhere, though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419
https://github.com/scverse/scanpy/issues/1419:39,safety,avail,available,39,"Realised this functionality is already available via `pip install "".[dev]""`. May be good to mention somewhere, though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419
https://github.com/scverse/scanpy/issues/1419:39,security,availab,available,39,"Realised this functionality is already available via `pip install "".[dev]""`. May be good to mention somewhere, though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419
https://github.com/scverse/scanpy/issues/1419:68,deployability,updat,updating,68,This is indeed very valuable information. . Do you mind adding a PR updating https://github.com/theislab/scanpy/blob/master/docs/installation.rst ?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419
https://github.com/scverse/scanpy/issues/1419:129,deployability,instal,installation,129,This is indeed very valuable information. . Do you mind adding a PR updating https://github.com/theislab/scanpy/blob/master/docs/installation.rst ?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419
https://github.com/scverse/scanpy/issues/1419:68,safety,updat,updating,68,This is indeed very valuable information. . Do you mind adding a PR updating https://github.com/theislab/scanpy/blob/master/docs/installation.rst ?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419
https://github.com/scverse/scanpy/issues/1419:68,security,updat,updating,68,This is indeed very valuable information. . Do you mind adding a PR updating https://github.com/theislab/scanpy/blob/master/docs/installation.rst ?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419
https://github.com/scverse/scanpy/issues/1419:22,deployability,updat,update,22,"Yes, no problem. I'll update it in a bit.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419
https://github.com/scverse/scanpy/issues/1419:22,safety,updat,update,22,"Yes, no problem. I'll update it in a bit.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419
https://github.com/scverse/scanpy/issues/1419:22,security,updat,update,22,"Yes, no problem. I'll update it in a bit.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419
https://github.com/scverse/scanpy/issues/1419:146,deployability,instal,install,146,"I had a look at scanpy's [setup file](https://github.com/theislab/scanpy/blob/master/setup.py) `setup.py` and realised that running. ```bash. pip install -e . pip install "".[dev]"". ```. does neither install all packages used within Scanpy's code base nor packages required for documentation or testing. IMO, these packages should be installed in a _developer installation_ as they are all part of the development cycle. Adding a file `requirements-dev.txt` including all needed packages would be an option to allow for an easy _developer installation_ via. ```bash. pip install -e . pip install -r requirements-dev.txt. ```. Any thoughts on this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419
https://github.com/scverse/scanpy/issues/1419:163,deployability,instal,install,163,"I had a look at scanpy's [setup file](https://github.com/theislab/scanpy/blob/master/setup.py) `setup.py` and realised that running. ```bash. pip install -e . pip install "".[dev]"". ```. does neither install all packages used within Scanpy's code base nor packages required for documentation or testing. IMO, these packages should be installed in a _developer installation_ as they are all part of the development cycle. Adding a file `requirements-dev.txt` including all needed packages would be an option to allow for an easy _developer installation_ via. ```bash. pip install -e . pip install -r requirements-dev.txt. ```. Any thoughts on this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419
https://github.com/scverse/scanpy/issues/1419:199,deployability,instal,install,199,"I had a look at scanpy's [setup file](https://github.com/theislab/scanpy/blob/master/setup.py) `setup.py` and realised that running. ```bash. pip install -e . pip install "".[dev]"". ```. does neither install all packages used within Scanpy's code base nor packages required for documentation or testing. IMO, these packages should be installed in a _developer installation_ as they are all part of the development cycle. Adding a file `requirements-dev.txt` including all needed packages would be an option to allow for an easy _developer installation_ via. ```bash. pip install -e . pip install -r requirements-dev.txt. ```. Any thoughts on this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419
https://github.com/scverse/scanpy/issues/1419:333,deployability,instal,installed,333,"I had a look at scanpy's [setup file](https://github.com/theislab/scanpy/blob/master/setup.py) `setup.py` and realised that running. ```bash. pip install -e . pip install "".[dev]"". ```. does neither install all packages used within Scanpy's code base nor packages required for documentation or testing. IMO, these packages should be installed in a _developer installation_ as they are all part of the development cycle. Adding a file `requirements-dev.txt` including all needed packages would be an option to allow for an easy _developer installation_ via. ```bash. pip install -e . pip install -r requirements-dev.txt. ```. Any thoughts on this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419
https://github.com/scverse/scanpy/issues/1419:570,deployability,instal,install,570,"I had a look at scanpy's [setup file](https://github.com/theislab/scanpy/blob/master/setup.py) `setup.py` and realised that running. ```bash. pip install -e . pip install "".[dev]"". ```. does neither install all packages used within Scanpy's code base nor packages required for documentation or testing. IMO, these packages should be installed in a _developer installation_ as they are all part of the development cycle. Adding a file `requirements-dev.txt` including all needed packages would be an option to allow for an easy _developer installation_ via. ```bash. pip install -e . pip install -r requirements-dev.txt. ```. Any thoughts on this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419
https://github.com/scverse/scanpy/issues/1419:587,deployability,instal,install,587,"I had a look at scanpy's [setup file](https://github.com/theislab/scanpy/blob/master/setup.py) `setup.py` and realised that running. ```bash. pip install -e . pip install "".[dev]"". ```. does neither install all packages used within Scanpy's code base nor packages required for documentation or testing. IMO, these packages should be installed in a _developer installation_ as they are all part of the development cycle. Adding a file `requirements-dev.txt` including all needed packages would be an option to allow for an easy _developer installation_ via. ```bash. pip install -e . pip install -r requirements-dev.txt. ```. Any thoughts on this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419
https://github.com/scverse/scanpy/issues/1419:211,modifiability,pac,packages,211,"I had a look at scanpy's [setup file](https://github.com/theislab/scanpy/blob/master/setup.py) `setup.py` and realised that running. ```bash. pip install -e . pip install "".[dev]"". ```. does neither install all packages used within Scanpy's code base nor packages required for documentation or testing. IMO, these packages should be installed in a _developer installation_ as they are all part of the development cycle. Adding a file `requirements-dev.txt` including all needed packages would be an option to allow for an easy _developer installation_ via. ```bash. pip install -e . pip install -r requirements-dev.txt. ```. Any thoughts on this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419
https://github.com/scverse/scanpy/issues/1419:255,modifiability,pac,packages,255,"I had a look at scanpy's [setup file](https://github.com/theislab/scanpy/blob/master/setup.py) `setup.py` and realised that running. ```bash. pip install -e . pip install "".[dev]"". ```. does neither install all packages used within Scanpy's code base nor packages required for documentation or testing. IMO, these packages should be installed in a _developer installation_ as they are all part of the development cycle. Adding a file `requirements-dev.txt` including all needed packages would be an option to allow for an easy _developer installation_ via. ```bash. pip install -e . pip install -r requirements-dev.txt. ```. Any thoughts on this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419
https://github.com/scverse/scanpy/issues/1419:314,modifiability,pac,packages,314,"I had a look at scanpy's [setup file](https://github.com/theislab/scanpy/blob/master/setup.py) `setup.py` and realised that running. ```bash. pip install -e . pip install "".[dev]"". ```. does neither install all packages used within Scanpy's code base nor packages required for documentation or testing. IMO, these packages should be installed in a _developer installation_ as they are all part of the development cycle. Adding a file `requirements-dev.txt` including all needed packages would be an option to allow for an easy _developer installation_ via. ```bash. pip install -e . pip install -r requirements-dev.txt. ```. Any thoughts on this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419
https://github.com/scverse/scanpy/issues/1419:478,modifiability,pac,packages,478,"I had a look at scanpy's [setup file](https://github.com/theislab/scanpy/blob/master/setup.py) `setup.py` and realised that running. ```bash. pip install -e . pip install "".[dev]"". ```. does neither install all packages used within Scanpy's code base nor packages required for documentation or testing. IMO, these packages should be installed in a _developer installation_ as they are all part of the development cycle. Adding a file `requirements-dev.txt` including all needed packages would be an option to allow for an easy _developer installation_ via. ```bash. pip install -e . pip install -r requirements-dev.txt. ```. Any thoughts on this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419
https://github.com/scverse/scanpy/issues/1419:186,reliability,doe,does,186,"I had a look at scanpy's [setup file](https://github.com/theislab/scanpy/blob/master/setup.py) `setup.py` and realised that running. ```bash. pip install -e . pip install "".[dev]"". ```. does neither install all packages used within Scanpy's code base nor packages required for documentation or testing. IMO, these packages should be installed in a _developer installation_ as they are all part of the development cycle. Adding a file `requirements-dev.txt` including all needed packages would be an option to allow for an easy _developer installation_ via. ```bash. pip install -e . pip install -r requirements-dev.txt. ```. Any thoughts on this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419
https://github.com/scverse/scanpy/issues/1419:294,safety,test,testing,294,"I had a look at scanpy's [setup file](https://github.com/theislab/scanpy/blob/master/setup.py) `setup.py` and realised that running. ```bash. pip install -e . pip install "".[dev]"". ```. does neither install all packages used within Scanpy's code base nor packages required for documentation or testing. IMO, these packages should be installed in a _developer installation_ as they are all part of the development cycle. Adding a file `requirements-dev.txt` including all needed packages would be an option to allow for an easy _developer installation_ via. ```bash. pip install -e . pip install -r requirements-dev.txt. ```. Any thoughts on this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419
https://github.com/scverse/scanpy/issues/1419:294,testability,test,testing,294,"I had a look at scanpy's [setup file](https://github.com/theislab/scanpy/blob/master/setup.py) `setup.py` and realised that running. ```bash. pip install -e . pip install "".[dev]"". ```. does neither install all packages used within Scanpy's code base nor packages required for documentation or testing. IMO, these packages should be installed in a _developer installation_ as they are all part of the development cycle. Adding a file `requirements-dev.txt` including all needed packages would be an option to allow for an easy _developer installation_ via. ```bash. pip install -e . pip install -r requirements-dev.txt. ```. Any thoughts on this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419
https://github.com/scverse/scanpy/issues/1419:277,usability,document,documentation,277,"I had a look at scanpy's [setup file](https://github.com/theislab/scanpy/blob/master/setup.py) `setup.py` and realised that running. ```bash. pip install -e . pip install "".[dev]"". ```. does neither install all packages used within Scanpy's code base nor packages required for documentation or testing. IMO, these packages should be installed in a _developer installation_ as they are all part of the development cycle. Adding a file `requirements-dev.txt` including all needed packages would be an option to allow for an easy _developer installation_ via. ```bash. pip install -e . pip install -r requirements-dev.txt. ```. Any thoughts on this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419
https://github.com/scverse/scanpy/issues/1419:98,deployability,automat,automatically,98,"Were there every any thoughts or discussions on adding pre-commit hooks? This would, for example, automatically enforce the black coding style prior to committing new changes. The `.pre-commit-hooks.yaml` file could, for example, look as follows:. ```yaml. # .pre-commit-hooks.yaml. repos:. - repo: https://github.com/ambv/black. rev: stable. hooks:. - id: black. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419
https://github.com/scverse/scanpy/issues/1419:66,testability,hook,hooks,66,"Were there every any thoughts or discussions on adding pre-commit hooks? This would, for example, automatically enforce the black coding style prior to committing new changes. The `.pre-commit-hooks.yaml` file could, for example, look as follows:. ```yaml. # .pre-commit-hooks.yaml. repos:. - repo: https://github.com/ambv/black. rev: stable. hooks:. - id: black. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419
https://github.com/scverse/scanpy/issues/1419:98,testability,automat,automatically,98,"Were there every any thoughts or discussions on adding pre-commit hooks? This would, for example, automatically enforce the black coding style prior to committing new changes. The `.pre-commit-hooks.yaml` file could, for example, look as follows:. ```yaml. # .pre-commit-hooks.yaml. repos:. - repo: https://github.com/ambv/black. rev: stable. hooks:. - id: black. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419
https://github.com/scverse/scanpy/issues/1419:193,testability,hook,hooks,193,"Were there every any thoughts or discussions on adding pre-commit hooks? This would, for example, automatically enforce the black coding style prior to committing new changes. The `.pre-commit-hooks.yaml` file could, for example, look as follows:. ```yaml. # .pre-commit-hooks.yaml. repos:. - repo: https://github.com/ambv/black. rev: stable. hooks:. - id: black. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419
https://github.com/scverse/scanpy/issues/1419:271,testability,hook,hooks,271,"Were there every any thoughts or discussions on adding pre-commit hooks? This would, for example, automatically enforce the black coding style prior to committing new changes. The `.pre-commit-hooks.yaml` file could, for example, look as follows:. ```yaml. # .pre-commit-hooks.yaml. repos:. - repo: https://github.com/ambv/black. rev: stable. hooks:. - id: black. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419
https://github.com/scverse/scanpy/issues/1419:343,testability,hook,hooks,343,"Were there every any thoughts or discussions on adding pre-commit hooks? This would, for example, automatically enforce the black coding style prior to committing new changes. The `.pre-commit-hooks.yaml` file could, for example, look as follows:. ```yaml. # .pre-commit-hooks.yaml. repos:. - repo: https://github.com/ambv/black. rev: stable. hooks:. - id: black. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419
https://github.com/scverse/scanpy/issues/1419:73,integrability,sub,submitted,73,"can we follow keep discussing on #1563 ? . @WeilerP not sure if you ever submitted that PR, but otherwise we can close this",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419
https://github.com/scverse/scanpy/issues/1419:113,usability,close,close,113,"can we follow keep discussing on #1563 ? . @WeilerP not sure if you ever submitted that PR, but otherwise we can close this",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419
https://github.com/scverse/scanpy/issues/1419:222,deployability,instal,installation,222,"Sorry, no, I didn't open a PR since I hadn't heard back regarding above comments. Fine to close it in favour of #1563, although it only concerns pre-commit hooks, right? The original idea of this issue was to make the dev installation easier, i.e. installing all required packages to run the full code base, tests etc. This currently doesn't happen when installing through `pip install "".[dev]""`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419
https://github.com/scverse/scanpy/issues/1419:248,deployability,instal,installing,248,"Sorry, no, I didn't open a PR since I hadn't heard back regarding above comments. Fine to close it in favour of #1563, although it only concerns pre-commit hooks, right? The original idea of this issue was to make the dev installation easier, i.e. installing all required packages to run the full code base, tests etc. This currently doesn't happen when installing through `pip install "".[dev]""`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419
https://github.com/scverse/scanpy/issues/1419:354,deployability,instal,installing,354,"Sorry, no, I didn't open a PR since I hadn't heard back regarding above comments. Fine to close it in favour of #1563, although it only concerns pre-commit hooks, right? The original idea of this issue was to make the dev installation easier, i.e. installing all required packages to run the full code base, tests etc. This currently doesn't happen when installing through `pip install "".[dev]""`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419
https://github.com/scverse/scanpy/issues/1419:378,deployability,instal,install,378,"Sorry, no, I didn't open a PR since I hadn't heard back regarding above comments. Fine to close it in favour of #1563, although it only concerns pre-commit hooks, right? The original idea of this issue was to make the dev installation easier, i.e. installing all required packages to run the full code base, tests etc. This currently doesn't happen when installing through `pip install "".[dev]""`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419
https://github.com/scverse/scanpy/issues/1419:324,energy efficiency,current,currently,324,"Sorry, no, I didn't open a PR since I hadn't heard back regarding above comments. Fine to close it in favour of #1563, although it only concerns pre-commit hooks, right? The original idea of this issue was to make the dev installation easier, i.e. installing all required packages to run the full code base, tests etc. This currently doesn't happen when installing through `pip install "".[dev]""`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419
https://github.com/scverse/scanpy/issues/1419:136,modifiability,concern,concerns,136,"Sorry, no, I didn't open a PR since I hadn't heard back regarding above comments. Fine to close it in favour of #1563, although it only concerns pre-commit hooks, right? The original idea of this issue was to make the dev installation easier, i.e. installing all required packages to run the full code base, tests etc. This currently doesn't happen when installing through `pip install "".[dev]""`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419
https://github.com/scverse/scanpy/issues/1419:272,modifiability,pac,packages,272,"Sorry, no, I didn't open a PR since I hadn't heard back regarding above comments. Fine to close it in favour of #1563, although it only concerns pre-commit hooks, right? The original idea of this issue was to make the dev installation easier, i.e. installing all required packages to run the full code base, tests etc. This currently doesn't happen when installing through `pip install "".[dev]""`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419
https://github.com/scverse/scanpy/issues/1419:334,reliability,doe,doesn,334,"Sorry, no, I didn't open a PR since I hadn't heard back regarding above comments. Fine to close it in favour of #1563, although it only concerns pre-commit hooks, right? The original idea of this issue was to make the dev installation easier, i.e. installing all required packages to run the full code base, tests etc. This currently doesn't happen when installing through `pip install "".[dev]""`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419
https://github.com/scverse/scanpy/issues/1419:308,safety,test,tests,308,"Sorry, no, I didn't open a PR since I hadn't heard back regarding above comments. Fine to close it in favour of #1563, although it only concerns pre-commit hooks, right? The original idea of this issue was to make the dev installation easier, i.e. installing all required packages to run the full code base, tests etc. This currently doesn't happen when installing through `pip install "".[dev]""`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419
https://github.com/scverse/scanpy/issues/1419:136,testability,concern,concerns,136,"Sorry, no, I didn't open a PR since I hadn't heard back regarding above comments. Fine to close it in favour of #1563, although it only concerns pre-commit hooks, right? The original idea of this issue was to make the dev installation easier, i.e. installing all required packages to run the full code base, tests etc. This currently doesn't happen when installing through `pip install "".[dev]""`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419
https://github.com/scverse/scanpy/issues/1419:156,testability,hook,hooks,156,"Sorry, no, I didn't open a PR since I hadn't heard back regarding above comments. Fine to close it in favour of #1563, although it only concerns pre-commit hooks, right? The original idea of this issue was to make the dev installation easier, i.e. installing all required packages to run the full code base, tests etc. This currently doesn't happen when installing through `pip install "".[dev]""`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419
https://github.com/scverse/scanpy/issues/1419:308,testability,test,tests,308,"Sorry, no, I didn't open a PR since I hadn't heard back regarding above comments. Fine to close it in favour of #1563, although it only concerns pre-commit hooks, right? The original idea of this issue was to make the dev installation easier, i.e. installing all required packages to run the full code base, tests etc. This currently doesn't happen when installing through `pip install "".[dev]""`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419
https://github.com/scverse/scanpy/issues/1419:90,usability,close,close,90,"Sorry, no, I didn't open a PR since I hadn't heard back regarding above comments. Fine to close it in favour of #1563, although it only concerns pre-commit hooks, right? The original idea of this issue was to make the dev installation easier, i.e. installing all required packages to run the full code base, tests etc. This currently doesn't happen when installing through `pip install "".[dev]""`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419
https://github.com/scverse/scanpy/issues/1419:95,security,team,team,95,I think #1527 is coming along nicely. It seems to mostly work now. It'd be great if some other team members could try it out and see if it works for them!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419
https://github.com/scverse/scanpy/issues/1419:42,deployability,instal,install,42,"As far as I can tell, #1527 still doesn't install all packages in a dev installation required to run the entire code base and tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419
https://github.com/scverse/scanpy/issues/1419:72,deployability,instal,installation,72,"As far as I can tell, #1527 still doesn't install all packages in a dev installation required to run the entire code base and tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419
https://github.com/scverse/scanpy/issues/1419:54,modifiability,pac,packages,54,"As far as I can tell, #1527 still doesn't install all packages in a dev installation required to run the entire code base and tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419
https://github.com/scverse/scanpy/issues/1419:34,reliability,doe,doesn,34,"As far as I can tell, #1527 still doesn't install all packages in a dev installation required to run the entire code base and tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419
https://github.com/scverse/scanpy/issues/1419:126,safety,test,tests,126,"As far as I can tell, #1527 still doesn't install all packages in a dev installation required to run the entire code base and tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419
https://github.com/scverse/scanpy/issues/1419:126,testability,test,tests,126,"As far as I can tell, #1527 still doesn't install all packages in a dev installation required to run the entire code base and tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419
https://github.com/scverse/scanpy/issues/1419:6,deployability,instal,install,6,"`flit install -s` by default would install everything, passing `--deps=develop` actually leads to fewer things being installed. I think this is weird behavior, but it's the way flit works.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419
https://github.com/scverse/scanpy/issues/1419:35,deployability,instal,install,35,"`flit install -s` by default would install everything, passing `--deps=develop` actually leads to fewer things being installed. I think this is weird behavior, but it's the way flit works.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419
https://github.com/scverse/scanpy/issues/1419:117,deployability,instal,installed,117,"`flit install -s` by default would install everything, passing `--deps=develop` actually leads to fewer things being installed. I think this is weird behavior, but it's the way flit works.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419
https://github.com/scverse/scanpy/issues/1419:150,usability,behavi,behavior,150,"`flit install -s` by default would install everything, passing `--deps=develop` actually leads to fewer things being installed. I think this is weird behavior, but it's the way flit works.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1419
https://github.com/scverse/scanpy/issues/1420:131,availability,error,error,131,I skimmed through the code only briefly yesterday but I think the problem might be that both `x` and `y` need to be specified: The error message seems to indicate so and the two lines seemingly responsible for the problem are the only ones where `x` and `y` are **not** both specified. I'll have a closer look at it in a bit.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1420
https://github.com/scverse/scanpy/issues/1420:137,integrability,messag,message,137,I skimmed through the code only briefly yesterday but I think the problem might be that both `x` and `y` need to be specified: The error message seems to indicate so and the two lines seemingly responsible for the problem are the only ones where `x` and `y` are **not** both specified. I'll have a closer look at it in a bit.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1420
https://github.com/scverse/scanpy/issues/1420:116,interoperability,specif,specified,116,I skimmed through the code only briefly yesterday but I think the problem might be that both `x` and `y` need to be specified: The error message seems to indicate so and the two lines seemingly responsible for the problem are the only ones where `x` and `y` are **not** both specified. I'll have a closer look at it in a bit.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1420
https://github.com/scverse/scanpy/issues/1420:137,interoperability,messag,message,137,I skimmed through the code only briefly yesterday but I think the problem might be that both `x` and `y` need to be specified: The error message seems to indicate so and the two lines seemingly responsible for the problem are the only ones where `x` and `y` are **not** both specified. I'll have a closer look at it in a bit.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1420
https://github.com/scverse/scanpy/issues/1420:275,interoperability,specif,specified,275,I skimmed through the code only briefly yesterday but I think the problem might be that both `x` and `y` need to be specified: The error message seems to indicate so and the two lines seemingly responsible for the problem are the only ones where `x` and `y` are **not** both specified. I'll have a closer look at it in a bit.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1420
https://github.com/scverse/scanpy/issues/1420:131,performance,error,error,131,I skimmed through the code only briefly yesterday but I think the problem might be that both `x` and `y` need to be specified: The error message seems to indicate so and the two lines seemingly responsible for the problem are the only ones where `x` and `y` are **not** both specified. I'll have a closer look at it in a bit.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1420
https://github.com/scverse/scanpy/issues/1420:131,safety,error,error,131,I skimmed through the code only briefly yesterday but I think the problem might be that both `x` and `y` need to be specified: The error message seems to indicate so and the two lines seemingly responsible for the problem are the only ones where `x` and `y` are **not** both specified. I'll have a closer look at it in a bit.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1420
https://github.com/scverse/scanpy/issues/1420:131,usability,error,error,131,I skimmed through the code only briefly yesterday but I think the problem might be that both `x` and `y` need to be specified: The error message seems to indicate so and the two lines seemingly responsible for the problem are the only ones where `x` and `y` are **not** both specified. I'll have a closer look at it in a bit.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1420
https://github.com/scverse/scanpy/issues/1420:154,usability,indicat,indicate,154,I skimmed through the code only briefly yesterday but I think the problem might be that both `x` and `y` need to be specified: The error message seems to indicate so and the two lines seemingly responsible for the problem are the only ones where `x` and `y` are **not** both specified. I'll have a closer look at it in a bit.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1420
https://github.com/scverse/scanpy/issues/1420:298,usability,close,closer,298,I skimmed through the code only briefly yesterday but I think the problem might be that both `x` and `y` need to be specified: The error message seems to indicate so and the two lines seemingly responsible for the problem are the only ones where `x` and `y` are **not** both specified. I'll have a closer look at it in a bit.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1420
https://github.com/scverse/scanpy/issues/1420:505,deployability,scale,scales,505,"Yes, I saw that too but I was hoping it could be prevented as it seems suboptimal to me and is probably confusing when looking at the code. Is there a specific reason for using `seaborn.FacetGrid` instead of `seaborn.catplot` (see [here](https://seaborn.pydata.org/generated/seaborn.catplot.html#seaborn.catplot))? Replacing the lines. ```python. # ... kwds.setdefault('cut', 0). kwds.setdefault('inner'). if multi_panel and groupby is None and len(ys) == 1:. # This is a quick and dirty way for adapting scales across several. # keys if groupby is None. y = ys[0]. g = sns.FacetGrid(obs_tidy, col=x, col_order=keys, sharey=False). # don't really know why this gives a warning without passing `order`. g = g.map(sns.violinplot, y, orient='vertical', scale=scale, order=keys, **kwds). ```. [here](https://github.com/theislab/scanpy/blob/f704f724529def21769ee6407f9b47b5c161564c/scanpy/plotting/_anndata.py#L736) by. ```python. if multi_panel and groupby is None and len(ys) == 1:. g = sns.catplot(y=y, data=obs_tidy, kind=""violin"", col=x, col_order=keys, sharey=False, order=keys, **kwds). ```. gives the desired plot (except for different size). The strip plot can be added on top by calling `seaborn.stripplot` afterwards:. ```python. if multi_panel and groupby is None and len(ys) == 1:. g = sns.catplot(y=y, data=obs_tidy, kind=""violin"", col=x, col_order=keys, sharey=False, order=keys, **kwds). if stripplot:. sns.stripplot(y=y, data=obs_tidy, jitter=jitter, color=""black""). ```. At the moment, I am just unsure how to plot to each subplot of the new `g`. It might be possible to loop through the plot grid so not to add everything on top of the last plot. ![master_violin_multi_panel](https://user-images.githubusercontent.com/28675704/93460801-16626300-f8e4-11ea-8f46-ed7ff64d8efb.png). Besides the not yet solved strip plot problem, would that be a valid alternative?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1420
https://github.com/scverse/scanpy/issues/1420:750,deployability,scale,scale,750,"Yes, I saw that too but I was hoping it could be prevented as it seems suboptimal to me and is probably confusing when looking at the code. Is there a specific reason for using `seaborn.FacetGrid` instead of `seaborn.catplot` (see [here](https://seaborn.pydata.org/generated/seaborn.catplot.html#seaborn.catplot))? Replacing the lines. ```python. # ... kwds.setdefault('cut', 0). kwds.setdefault('inner'). if multi_panel and groupby is None and len(ys) == 1:. # This is a quick and dirty way for adapting scales across several. # keys if groupby is None. y = ys[0]. g = sns.FacetGrid(obs_tidy, col=x, col_order=keys, sharey=False). # don't really know why this gives a warning without passing `order`. g = g.map(sns.violinplot, y, orient='vertical', scale=scale, order=keys, **kwds). ```. [here](https://github.com/theislab/scanpy/blob/f704f724529def21769ee6407f9b47b5c161564c/scanpy/plotting/_anndata.py#L736) by. ```python. if multi_panel and groupby is None and len(ys) == 1:. g = sns.catplot(y=y, data=obs_tidy, kind=""violin"", col=x, col_order=keys, sharey=False, order=keys, **kwds). ```. gives the desired plot (except for different size). The strip plot can be added on top by calling `seaborn.stripplot` afterwards:. ```python. if multi_panel and groupby is None and len(ys) == 1:. g = sns.catplot(y=y, data=obs_tidy, kind=""violin"", col=x, col_order=keys, sharey=False, order=keys, **kwds). if stripplot:. sns.stripplot(y=y, data=obs_tidy, jitter=jitter, color=""black""). ```. At the moment, I am just unsure how to plot to each subplot of the new `g`. It might be possible to loop through the plot grid so not to add everything on top of the last plot. ![master_violin_multi_panel](https://user-images.githubusercontent.com/28675704/93460801-16626300-f8e4-11ea-8f46-ed7ff64d8efb.png). Besides the not yet solved strip plot problem, would that be a valid alternative?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1420
https://github.com/scverse/scanpy/issues/1420:756,deployability,scale,scale,756,"Yes, I saw that too but I was hoping it could be prevented as it seems suboptimal to me and is probably confusing when looking at the code. Is there a specific reason for using `seaborn.FacetGrid` instead of `seaborn.catplot` (see [here](https://seaborn.pydata.org/generated/seaborn.catplot.html#seaborn.catplot))? Replacing the lines. ```python. # ... kwds.setdefault('cut', 0). kwds.setdefault('inner'). if multi_panel and groupby is None and len(ys) == 1:. # This is a quick and dirty way for adapting scales across several. # keys if groupby is None. y = ys[0]. g = sns.FacetGrid(obs_tidy, col=x, col_order=keys, sharey=False). # don't really know why this gives a warning without passing `order`. g = g.map(sns.violinplot, y, orient='vertical', scale=scale, order=keys, **kwds). ```. [here](https://github.com/theislab/scanpy/blob/f704f724529def21769ee6407f9b47b5c161564c/scanpy/plotting/_anndata.py#L736) by. ```python. if multi_panel and groupby is None and len(ys) == 1:. g = sns.catplot(y=y, data=obs_tidy, kind=""violin"", col=x, col_order=keys, sharey=False, order=keys, **kwds). ```. gives the desired plot (except for different size). The strip plot can be added on top by calling `seaborn.stripplot` afterwards:. ```python. if multi_panel and groupby is None and len(ys) == 1:. g = sns.catplot(y=y, data=obs_tidy, kind=""violin"", col=x, col_order=keys, sharey=False, order=keys, **kwds). if stripplot:. sns.stripplot(y=y, data=obs_tidy, jitter=jitter, color=""black""). ```. At the moment, I am just unsure how to plot to each subplot of the new `g`. It might be possible to loop through the plot grid so not to add everything on top of the last plot. ![master_violin_multi_panel](https://user-images.githubusercontent.com/28675704/93460801-16626300-f8e4-11ea-8f46-ed7ff64d8efb.png). Besides the not yet solved strip plot problem, would that be a valid alternative?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1420
https://github.com/scverse/scanpy/issues/1420:496,energy efficiency,adapt,adapting,496,"Yes, I saw that too but I was hoping it could be prevented as it seems suboptimal to me and is probably confusing when looking at the code. Is there a specific reason for using `seaborn.FacetGrid` instead of `seaborn.catplot` (see [here](https://seaborn.pydata.org/generated/seaborn.catplot.html#seaborn.catplot))? Replacing the lines. ```python. # ... kwds.setdefault('cut', 0). kwds.setdefault('inner'). if multi_panel and groupby is None and len(ys) == 1:. # This is a quick and dirty way for adapting scales across several. # keys if groupby is None. y = ys[0]. g = sns.FacetGrid(obs_tidy, col=x, col_order=keys, sharey=False). # don't really know why this gives a warning without passing `order`. g = g.map(sns.violinplot, y, orient='vertical', scale=scale, order=keys, **kwds). ```. [here](https://github.com/theislab/scanpy/blob/f704f724529def21769ee6407f9b47b5c161564c/scanpy/plotting/_anndata.py#L736) by. ```python. if multi_panel and groupby is None and len(ys) == 1:. g = sns.catplot(y=y, data=obs_tidy, kind=""violin"", col=x, col_order=keys, sharey=False, order=keys, **kwds). ```. gives the desired plot (except for different size). The strip plot can be added on top by calling `seaborn.stripplot` afterwards:. ```python. if multi_panel and groupby is None and len(ys) == 1:. g = sns.catplot(y=y, data=obs_tidy, kind=""violin"", col=x, col_order=keys, sharey=False, order=keys, **kwds). if stripplot:. sns.stripplot(y=y, data=obs_tidy, jitter=jitter, color=""black""). ```. At the moment, I am just unsure how to plot to each subplot of the new `g`. It might be possible to loop through the plot grid so not to add everything on top of the last plot. ![master_violin_multi_panel](https://user-images.githubusercontent.com/28675704/93460801-16626300-f8e4-11ea-8f46-ed7ff64d8efb.png). Besides the not yet solved strip plot problem, would that be a valid alternative?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1420
https://github.com/scverse/scanpy/issues/1420:505,energy efficiency,scale,scales,505,"Yes, I saw that too but I was hoping it could be prevented as it seems suboptimal to me and is probably confusing when looking at the code. Is there a specific reason for using `seaborn.FacetGrid` instead of `seaborn.catplot` (see [here](https://seaborn.pydata.org/generated/seaborn.catplot.html#seaborn.catplot))? Replacing the lines. ```python. # ... kwds.setdefault('cut', 0). kwds.setdefault('inner'). if multi_panel and groupby is None and len(ys) == 1:. # This is a quick and dirty way for adapting scales across several. # keys if groupby is None. y = ys[0]. g = sns.FacetGrid(obs_tidy, col=x, col_order=keys, sharey=False). # don't really know why this gives a warning without passing `order`. g = g.map(sns.violinplot, y, orient='vertical', scale=scale, order=keys, **kwds). ```. [here](https://github.com/theislab/scanpy/blob/f704f724529def21769ee6407f9b47b5c161564c/scanpy/plotting/_anndata.py#L736) by. ```python. if multi_panel and groupby is None and len(ys) == 1:. g = sns.catplot(y=y, data=obs_tidy, kind=""violin"", col=x, col_order=keys, sharey=False, order=keys, **kwds). ```. gives the desired plot (except for different size). The strip plot can be added on top by calling `seaborn.stripplot` afterwards:. ```python. if multi_panel and groupby is None and len(ys) == 1:. g = sns.catplot(y=y, data=obs_tidy, kind=""violin"", col=x, col_order=keys, sharey=False, order=keys, **kwds). if stripplot:. sns.stripplot(y=y, data=obs_tidy, jitter=jitter, color=""black""). ```. At the moment, I am just unsure how to plot to each subplot of the new `g`. It might be possible to loop through the plot grid so not to add everything on top of the last plot. ![master_violin_multi_panel](https://user-images.githubusercontent.com/28675704/93460801-16626300-f8e4-11ea-8f46-ed7ff64d8efb.png). Besides the not yet solved strip plot problem, would that be a valid alternative?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1420
https://github.com/scverse/scanpy/issues/1420:750,energy efficiency,scale,scale,750,"Yes, I saw that too but I was hoping it could be prevented as it seems suboptimal to me and is probably confusing when looking at the code. Is there a specific reason for using `seaborn.FacetGrid` instead of `seaborn.catplot` (see [here](https://seaborn.pydata.org/generated/seaborn.catplot.html#seaborn.catplot))? Replacing the lines. ```python. # ... kwds.setdefault('cut', 0). kwds.setdefault('inner'). if multi_panel and groupby is None and len(ys) == 1:. # This is a quick and dirty way for adapting scales across several. # keys if groupby is None. y = ys[0]. g = sns.FacetGrid(obs_tidy, col=x, col_order=keys, sharey=False). # don't really know why this gives a warning without passing `order`. g = g.map(sns.violinplot, y, orient='vertical', scale=scale, order=keys, **kwds). ```. [here](https://github.com/theislab/scanpy/blob/f704f724529def21769ee6407f9b47b5c161564c/scanpy/plotting/_anndata.py#L736) by. ```python. if multi_panel and groupby is None and len(ys) == 1:. g = sns.catplot(y=y, data=obs_tidy, kind=""violin"", col=x, col_order=keys, sharey=False, order=keys, **kwds). ```. gives the desired plot (except for different size). The strip plot can be added on top by calling `seaborn.stripplot` afterwards:. ```python. if multi_panel and groupby is None and len(ys) == 1:. g = sns.catplot(y=y, data=obs_tidy, kind=""violin"", col=x, col_order=keys, sharey=False, order=keys, **kwds). if stripplot:. sns.stripplot(y=y, data=obs_tidy, jitter=jitter, color=""black""). ```. At the moment, I am just unsure how to plot to each subplot of the new `g`. It might be possible to loop through the plot grid so not to add everything on top of the last plot. ![master_violin_multi_panel](https://user-images.githubusercontent.com/28675704/93460801-16626300-f8e4-11ea-8f46-ed7ff64d8efb.png). Besides the not yet solved strip plot problem, would that be a valid alternative?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1420
https://github.com/scverse/scanpy/issues/1420:756,energy efficiency,scale,scale,756,"Yes, I saw that too but I was hoping it could be prevented as it seems suboptimal to me and is probably confusing when looking at the code. Is there a specific reason for using `seaborn.FacetGrid` instead of `seaborn.catplot` (see [here](https://seaborn.pydata.org/generated/seaborn.catplot.html#seaborn.catplot))? Replacing the lines. ```python. # ... kwds.setdefault('cut', 0). kwds.setdefault('inner'). if multi_panel and groupby is None and len(ys) == 1:. # This is a quick and dirty way for adapting scales across several. # keys if groupby is None. y = ys[0]. g = sns.FacetGrid(obs_tidy, col=x, col_order=keys, sharey=False). # don't really know why this gives a warning without passing `order`. g = g.map(sns.violinplot, y, orient='vertical', scale=scale, order=keys, **kwds). ```. [here](https://github.com/theislab/scanpy/blob/f704f724529def21769ee6407f9b47b5c161564c/scanpy/plotting/_anndata.py#L736) by. ```python. if multi_panel and groupby is None and len(ys) == 1:. g = sns.catplot(y=y, data=obs_tidy, kind=""violin"", col=x, col_order=keys, sharey=False, order=keys, **kwds). ```. gives the desired plot (except for different size). The strip plot can be added on top by calling `seaborn.stripplot` afterwards:. ```python. if multi_panel and groupby is None and len(ys) == 1:. g = sns.catplot(y=y, data=obs_tidy, kind=""violin"", col=x, col_order=keys, sharey=False, order=keys, **kwds). if stripplot:. sns.stripplot(y=y, data=obs_tidy, jitter=jitter, color=""black""). ```. At the moment, I am just unsure how to plot to each subplot of the new `g`. It might be possible to loop through the plot grid so not to add everything on top of the last plot. ![master_violin_multi_panel](https://user-images.githubusercontent.com/28675704/93460801-16626300-f8e4-11ea-8f46-ed7ff64d8efb.png). Besides the not yet solved strip plot problem, would that be a valid alternative?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1420
https://github.com/scverse/scanpy/issues/1420:71,integrability,sub,suboptimal,71,"Yes, I saw that too but I was hoping it could be prevented as it seems suboptimal to me and is probably confusing when looking at the code. Is there a specific reason for using `seaborn.FacetGrid` instead of `seaborn.catplot` (see [here](https://seaborn.pydata.org/generated/seaborn.catplot.html#seaborn.catplot))? Replacing the lines. ```python. # ... kwds.setdefault('cut', 0). kwds.setdefault('inner'). if multi_panel and groupby is None and len(ys) == 1:. # This is a quick and dirty way for adapting scales across several. # keys if groupby is None. y = ys[0]. g = sns.FacetGrid(obs_tidy, col=x, col_order=keys, sharey=False). # don't really know why this gives a warning without passing `order`. g = g.map(sns.violinplot, y, orient='vertical', scale=scale, order=keys, **kwds). ```. [here](https://github.com/theislab/scanpy/blob/f704f724529def21769ee6407f9b47b5c161564c/scanpy/plotting/_anndata.py#L736) by. ```python. if multi_panel and groupby is None and len(ys) == 1:. g = sns.catplot(y=y, data=obs_tidy, kind=""violin"", col=x, col_order=keys, sharey=False, order=keys, **kwds). ```. gives the desired plot (except for different size). The strip plot can be added on top by calling `seaborn.stripplot` afterwards:. ```python. if multi_panel and groupby is None and len(ys) == 1:. g = sns.catplot(y=y, data=obs_tidy, kind=""violin"", col=x, col_order=keys, sharey=False, order=keys, **kwds). if stripplot:. sns.stripplot(y=y, data=obs_tidy, jitter=jitter, color=""black""). ```. At the moment, I am just unsure how to plot to each subplot of the new `g`. It might be possible to loop through the plot grid so not to add everything on top of the last plot. ![master_violin_multi_panel](https://user-images.githubusercontent.com/28675704/93460801-16626300-f8e4-11ea-8f46-ed7ff64d8efb.png). Besides the not yet solved strip plot problem, would that be a valid alternative?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1420
https://github.com/scverse/scanpy/issues/1420:496,integrability,adapt,adapting,496,"Yes, I saw that too but I was hoping it could be prevented as it seems suboptimal to me and is probably confusing when looking at the code. Is there a specific reason for using `seaborn.FacetGrid` instead of `seaborn.catplot` (see [here](https://seaborn.pydata.org/generated/seaborn.catplot.html#seaborn.catplot))? Replacing the lines. ```python. # ... kwds.setdefault('cut', 0). kwds.setdefault('inner'). if multi_panel and groupby is None and len(ys) == 1:. # This is a quick and dirty way for adapting scales across several. # keys if groupby is None. y = ys[0]. g = sns.FacetGrid(obs_tidy, col=x, col_order=keys, sharey=False). # don't really know why this gives a warning without passing `order`. g = g.map(sns.violinplot, y, orient='vertical', scale=scale, order=keys, **kwds). ```. [here](https://github.com/theislab/scanpy/blob/f704f724529def21769ee6407f9b47b5c161564c/scanpy/plotting/_anndata.py#L736) by. ```python. if multi_panel and groupby is None and len(ys) == 1:. g = sns.catplot(y=y, data=obs_tidy, kind=""violin"", col=x, col_order=keys, sharey=False, order=keys, **kwds). ```. gives the desired plot (except for different size). The strip plot can be added on top by calling `seaborn.stripplot` afterwards:. ```python. if multi_panel and groupby is None and len(ys) == 1:. g = sns.catplot(y=y, data=obs_tidy, kind=""violin"", col=x, col_order=keys, sharey=False, order=keys, **kwds). if stripplot:. sns.stripplot(y=y, data=obs_tidy, jitter=jitter, color=""black""). ```. At the moment, I am just unsure how to plot to each subplot of the new `g`. It might be possible to loop through the plot grid so not to add everything on top of the last plot. ![master_violin_multi_panel](https://user-images.githubusercontent.com/28675704/93460801-16626300-f8e4-11ea-8f46-ed7ff64d8efb.png). Besides the not yet solved strip plot problem, would that be a valid alternative?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1420
https://github.com/scverse/scanpy/issues/1420:1536,integrability,sub,subplot,1536,"Yes, I saw that too but I was hoping it could be prevented as it seems suboptimal to me and is probably confusing when looking at the code. Is there a specific reason for using `seaborn.FacetGrid` instead of `seaborn.catplot` (see [here](https://seaborn.pydata.org/generated/seaborn.catplot.html#seaborn.catplot))? Replacing the lines. ```python. # ... kwds.setdefault('cut', 0). kwds.setdefault('inner'). if multi_panel and groupby is None and len(ys) == 1:. # This is a quick and dirty way for adapting scales across several. # keys if groupby is None. y = ys[0]. g = sns.FacetGrid(obs_tidy, col=x, col_order=keys, sharey=False). # don't really know why this gives a warning without passing `order`. g = g.map(sns.violinplot, y, orient='vertical', scale=scale, order=keys, **kwds). ```. [here](https://github.com/theislab/scanpy/blob/f704f724529def21769ee6407f9b47b5c161564c/scanpy/plotting/_anndata.py#L736) by. ```python. if multi_panel and groupby is None and len(ys) == 1:. g = sns.catplot(y=y, data=obs_tidy, kind=""violin"", col=x, col_order=keys, sharey=False, order=keys, **kwds). ```. gives the desired plot (except for different size). The strip plot can be added on top by calling `seaborn.stripplot` afterwards:. ```python. if multi_panel and groupby is None and len(ys) == 1:. g = sns.catplot(y=y, data=obs_tidy, kind=""violin"", col=x, col_order=keys, sharey=False, order=keys, **kwds). if stripplot:. sns.stripplot(y=y, data=obs_tidy, jitter=jitter, color=""black""). ```. At the moment, I am just unsure how to plot to each subplot of the new `g`. It might be possible to loop through the plot grid so not to add everything on top of the last plot. ![master_violin_multi_panel](https://user-images.githubusercontent.com/28675704/93460801-16626300-f8e4-11ea-8f46-ed7ff64d8efb.png). Besides the not yet solved strip plot problem, would that be a valid alternative?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1420
https://github.com/scverse/scanpy/issues/1420:151,interoperability,specif,specific,151,"Yes, I saw that too but I was hoping it could be prevented as it seems suboptimal to me and is probably confusing when looking at the code. Is there a specific reason for using `seaborn.FacetGrid` instead of `seaborn.catplot` (see [here](https://seaborn.pydata.org/generated/seaborn.catplot.html#seaborn.catplot))? Replacing the lines. ```python. # ... kwds.setdefault('cut', 0). kwds.setdefault('inner'). if multi_panel and groupby is None and len(ys) == 1:. # This is a quick and dirty way for adapting scales across several. # keys if groupby is None. y = ys[0]. g = sns.FacetGrid(obs_tidy, col=x, col_order=keys, sharey=False). # don't really know why this gives a warning without passing `order`. g = g.map(sns.violinplot, y, orient='vertical', scale=scale, order=keys, **kwds). ```. [here](https://github.com/theislab/scanpy/blob/f704f724529def21769ee6407f9b47b5c161564c/scanpy/plotting/_anndata.py#L736) by. ```python. if multi_panel and groupby is None and len(ys) == 1:. g = sns.catplot(y=y, data=obs_tidy, kind=""violin"", col=x, col_order=keys, sharey=False, order=keys, **kwds). ```. gives the desired plot (except for different size). The strip plot can be added on top by calling `seaborn.stripplot` afterwards:. ```python. if multi_panel and groupby is None and len(ys) == 1:. g = sns.catplot(y=y, data=obs_tidy, kind=""violin"", col=x, col_order=keys, sharey=False, order=keys, **kwds). if stripplot:. sns.stripplot(y=y, data=obs_tidy, jitter=jitter, color=""black""). ```. At the moment, I am just unsure how to plot to each subplot of the new `g`. It might be possible to loop through the plot grid so not to add everything on top of the last plot. ![master_violin_multi_panel](https://user-images.githubusercontent.com/28675704/93460801-16626300-f8e4-11ea-8f46-ed7ff64d8efb.png). Besides the not yet solved strip plot problem, would that be a valid alternative?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1420
https://github.com/scverse/scanpy/issues/1420:496,interoperability,adapt,adapting,496,"Yes, I saw that too but I was hoping it could be prevented as it seems suboptimal to me and is probably confusing when looking at the code. Is there a specific reason for using `seaborn.FacetGrid` instead of `seaborn.catplot` (see [here](https://seaborn.pydata.org/generated/seaborn.catplot.html#seaborn.catplot))? Replacing the lines. ```python. # ... kwds.setdefault('cut', 0). kwds.setdefault('inner'). if multi_panel and groupby is None and len(ys) == 1:. # This is a quick and dirty way for adapting scales across several. # keys if groupby is None. y = ys[0]. g = sns.FacetGrid(obs_tidy, col=x, col_order=keys, sharey=False). # don't really know why this gives a warning without passing `order`. g = g.map(sns.violinplot, y, orient='vertical', scale=scale, order=keys, **kwds). ```. [here](https://github.com/theislab/scanpy/blob/f704f724529def21769ee6407f9b47b5c161564c/scanpy/plotting/_anndata.py#L736) by. ```python. if multi_panel and groupby is None and len(ys) == 1:. g = sns.catplot(y=y, data=obs_tidy, kind=""violin"", col=x, col_order=keys, sharey=False, order=keys, **kwds). ```. gives the desired plot (except for different size). The strip plot can be added on top by calling `seaborn.stripplot` afterwards:. ```python. if multi_panel and groupby is None and len(ys) == 1:. g = sns.catplot(y=y, data=obs_tidy, kind=""violin"", col=x, col_order=keys, sharey=False, order=keys, **kwds). if stripplot:. sns.stripplot(y=y, data=obs_tidy, jitter=jitter, color=""black""). ```. At the moment, I am just unsure how to plot to each subplot of the new `g`. It might be possible to loop through the plot grid so not to add everything on top of the last plot. ![master_violin_multi_panel](https://user-images.githubusercontent.com/28675704/93460801-16626300-f8e4-11ea-8f46-ed7ff64d8efb.png). Besides the not yet solved strip plot problem, would that be a valid alternative?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1420
https://github.com/scverse/scanpy/issues/1420:617,interoperability,share,sharey,617,"Yes, I saw that too but I was hoping it could be prevented as it seems suboptimal to me and is probably confusing when looking at the code. Is there a specific reason for using `seaborn.FacetGrid` instead of `seaborn.catplot` (see [here](https://seaborn.pydata.org/generated/seaborn.catplot.html#seaborn.catplot))? Replacing the lines. ```python. # ... kwds.setdefault('cut', 0). kwds.setdefault('inner'). if multi_panel and groupby is None and len(ys) == 1:. # This is a quick and dirty way for adapting scales across several. # keys if groupby is None. y = ys[0]. g = sns.FacetGrid(obs_tidy, col=x, col_order=keys, sharey=False). # don't really know why this gives a warning without passing `order`. g = g.map(sns.violinplot, y, orient='vertical', scale=scale, order=keys, **kwds). ```. [here](https://github.com/theislab/scanpy/blob/f704f724529def21769ee6407f9b47b5c161564c/scanpy/plotting/_anndata.py#L736) by. ```python. if multi_panel and groupby is None and len(ys) == 1:. g = sns.catplot(y=y, data=obs_tidy, kind=""violin"", col=x, col_order=keys, sharey=False, order=keys, **kwds). ```. gives the desired plot (except for different size). The strip plot can be added on top by calling `seaborn.stripplot` afterwards:. ```python. if multi_panel and groupby is None and len(ys) == 1:. g = sns.catplot(y=y, data=obs_tidy, kind=""violin"", col=x, col_order=keys, sharey=False, order=keys, **kwds). if stripplot:. sns.stripplot(y=y, data=obs_tidy, jitter=jitter, color=""black""). ```. At the moment, I am just unsure how to plot to each subplot of the new `g`. It might be possible to loop through the plot grid so not to add everything on top of the last plot. ![master_violin_multi_panel](https://user-images.githubusercontent.com/28675704/93460801-16626300-f8e4-11ea-8f46-ed7ff64d8efb.png). Besides the not yet solved strip plot problem, would that be a valid alternative?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1420
https://github.com/scverse/scanpy/issues/1420:1054,interoperability,share,sharey,1054,"Yes, I saw that too but I was hoping it could be prevented as it seems suboptimal to me and is probably confusing when looking at the code. Is there a specific reason for using `seaborn.FacetGrid` instead of `seaborn.catplot` (see [here](https://seaborn.pydata.org/generated/seaborn.catplot.html#seaborn.catplot))? Replacing the lines. ```python. # ... kwds.setdefault('cut', 0). kwds.setdefault('inner'). if multi_panel and groupby is None and len(ys) == 1:. # This is a quick and dirty way for adapting scales across several. # keys if groupby is None. y = ys[0]. g = sns.FacetGrid(obs_tidy, col=x, col_order=keys, sharey=False). # don't really know why this gives a warning without passing `order`. g = g.map(sns.violinplot, y, orient='vertical', scale=scale, order=keys, **kwds). ```. [here](https://github.com/theislab/scanpy/blob/f704f724529def21769ee6407f9b47b5c161564c/scanpy/plotting/_anndata.py#L736) by. ```python. if multi_panel and groupby is None and len(ys) == 1:. g = sns.catplot(y=y, data=obs_tidy, kind=""violin"", col=x, col_order=keys, sharey=False, order=keys, **kwds). ```. gives the desired plot (except for different size). The strip plot can be added on top by calling `seaborn.stripplot` afterwards:. ```python. if multi_panel and groupby is None and len(ys) == 1:. g = sns.catplot(y=y, data=obs_tidy, kind=""violin"", col=x, col_order=keys, sharey=False, order=keys, **kwds). if stripplot:. sns.stripplot(y=y, data=obs_tidy, jitter=jitter, color=""black""). ```. At the moment, I am just unsure how to plot to each subplot of the new `g`. It might be possible to loop through the plot grid so not to add everything on top of the last plot. ![master_violin_multi_panel](https://user-images.githubusercontent.com/28675704/93460801-16626300-f8e4-11ea-8f46-ed7ff64d8efb.png). Besides the not yet solved strip plot problem, would that be a valid alternative?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1420
https://github.com/scverse/scanpy/issues/1420:1364,interoperability,share,sharey,1364,"Yes, I saw that too but I was hoping it could be prevented as it seems suboptimal to me and is probably confusing when looking at the code. Is there a specific reason for using `seaborn.FacetGrid` instead of `seaborn.catplot` (see [here](https://seaborn.pydata.org/generated/seaborn.catplot.html#seaborn.catplot))? Replacing the lines. ```python. # ... kwds.setdefault('cut', 0). kwds.setdefault('inner'). if multi_panel and groupby is None and len(ys) == 1:. # This is a quick and dirty way for adapting scales across several. # keys if groupby is None. y = ys[0]. g = sns.FacetGrid(obs_tidy, col=x, col_order=keys, sharey=False). # don't really know why this gives a warning without passing `order`. g = g.map(sns.violinplot, y, orient='vertical', scale=scale, order=keys, **kwds). ```. [here](https://github.com/theislab/scanpy/blob/f704f724529def21769ee6407f9b47b5c161564c/scanpy/plotting/_anndata.py#L736) by. ```python. if multi_panel and groupby is None and len(ys) == 1:. g = sns.catplot(y=y, data=obs_tidy, kind=""violin"", col=x, col_order=keys, sharey=False, order=keys, **kwds). ```. gives the desired plot (except for different size). The strip plot can be added on top by calling `seaborn.stripplot` afterwards:. ```python. if multi_panel and groupby is None and len(ys) == 1:. g = sns.catplot(y=y, data=obs_tidy, kind=""violin"", col=x, col_order=keys, sharey=False, order=keys, **kwds). if stripplot:. sns.stripplot(y=y, data=obs_tidy, jitter=jitter, color=""black""). ```. At the moment, I am just unsure how to plot to each subplot of the new `g`. It might be possible to loop through the plot grid so not to add everything on top of the last plot. ![master_violin_multi_panel](https://user-images.githubusercontent.com/28675704/93460801-16626300-f8e4-11ea-8f46-ed7ff64d8efb.png). Besides the not yet solved strip plot problem, would that be a valid alternative?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1420
https://github.com/scverse/scanpy/issues/1420:496,modifiability,adapt,adapting,496,"Yes, I saw that too but I was hoping it could be prevented as it seems suboptimal to me and is probably confusing when looking at the code. Is there a specific reason for using `seaborn.FacetGrid` instead of `seaborn.catplot` (see [here](https://seaborn.pydata.org/generated/seaborn.catplot.html#seaborn.catplot))? Replacing the lines. ```python. # ... kwds.setdefault('cut', 0). kwds.setdefault('inner'). if multi_panel and groupby is None and len(ys) == 1:. # This is a quick and dirty way for adapting scales across several. # keys if groupby is None. y = ys[0]. g = sns.FacetGrid(obs_tidy, col=x, col_order=keys, sharey=False). # don't really know why this gives a warning without passing `order`. g = g.map(sns.violinplot, y, orient='vertical', scale=scale, order=keys, **kwds). ```. [here](https://github.com/theislab/scanpy/blob/f704f724529def21769ee6407f9b47b5c161564c/scanpy/plotting/_anndata.py#L736) by. ```python. if multi_panel and groupby is None and len(ys) == 1:. g = sns.catplot(y=y, data=obs_tidy, kind=""violin"", col=x, col_order=keys, sharey=False, order=keys, **kwds). ```. gives the desired plot (except for different size). The strip plot can be added on top by calling `seaborn.stripplot` afterwards:. ```python. if multi_panel and groupby is None and len(ys) == 1:. g = sns.catplot(y=y, data=obs_tidy, kind=""violin"", col=x, col_order=keys, sharey=False, order=keys, **kwds). if stripplot:. sns.stripplot(y=y, data=obs_tidy, jitter=jitter, color=""black""). ```. At the moment, I am just unsure how to plot to each subplot of the new `g`. It might be possible to loop through the plot grid so not to add everything on top of the last plot. ![master_violin_multi_panel](https://user-images.githubusercontent.com/28675704/93460801-16626300-f8e4-11ea-8f46-ed7ff64d8efb.png). Besides the not yet solved strip plot problem, would that be a valid alternative?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1420
https://github.com/scverse/scanpy/issues/1420:505,modifiability,scal,scales,505,"Yes, I saw that too but I was hoping it could be prevented as it seems suboptimal to me and is probably confusing when looking at the code. Is there a specific reason for using `seaborn.FacetGrid` instead of `seaborn.catplot` (see [here](https://seaborn.pydata.org/generated/seaborn.catplot.html#seaborn.catplot))? Replacing the lines. ```python. # ... kwds.setdefault('cut', 0). kwds.setdefault('inner'). if multi_panel and groupby is None and len(ys) == 1:. # This is a quick and dirty way for adapting scales across several. # keys if groupby is None. y = ys[0]. g = sns.FacetGrid(obs_tidy, col=x, col_order=keys, sharey=False). # don't really know why this gives a warning without passing `order`. g = g.map(sns.violinplot, y, orient='vertical', scale=scale, order=keys, **kwds). ```. [here](https://github.com/theislab/scanpy/blob/f704f724529def21769ee6407f9b47b5c161564c/scanpy/plotting/_anndata.py#L736) by. ```python. if multi_panel and groupby is None and len(ys) == 1:. g = sns.catplot(y=y, data=obs_tidy, kind=""violin"", col=x, col_order=keys, sharey=False, order=keys, **kwds). ```. gives the desired plot (except for different size). The strip plot can be added on top by calling `seaborn.stripplot` afterwards:. ```python. if multi_panel and groupby is None and len(ys) == 1:. g = sns.catplot(y=y, data=obs_tidy, kind=""violin"", col=x, col_order=keys, sharey=False, order=keys, **kwds). if stripplot:. sns.stripplot(y=y, data=obs_tidy, jitter=jitter, color=""black""). ```. At the moment, I am just unsure how to plot to each subplot of the new `g`. It might be possible to loop through the plot grid so not to add everything on top of the last plot. ![master_violin_multi_panel](https://user-images.githubusercontent.com/28675704/93460801-16626300-f8e4-11ea-8f46-ed7ff64d8efb.png). Besides the not yet solved strip plot problem, would that be a valid alternative?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1420
https://github.com/scverse/scanpy/issues/1420:750,modifiability,scal,scale,750,"Yes, I saw that too but I was hoping it could be prevented as it seems suboptimal to me and is probably confusing when looking at the code. Is there a specific reason for using `seaborn.FacetGrid` instead of `seaborn.catplot` (see [here](https://seaborn.pydata.org/generated/seaborn.catplot.html#seaborn.catplot))? Replacing the lines. ```python. # ... kwds.setdefault('cut', 0). kwds.setdefault('inner'). if multi_panel and groupby is None and len(ys) == 1:. # This is a quick and dirty way for adapting scales across several. # keys if groupby is None. y = ys[0]. g = sns.FacetGrid(obs_tidy, col=x, col_order=keys, sharey=False). # don't really know why this gives a warning without passing `order`. g = g.map(sns.violinplot, y, orient='vertical', scale=scale, order=keys, **kwds). ```. [here](https://github.com/theislab/scanpy/blob/f704f724529def21769ee6407f9b47b5c161564c/scanpy/plotting/_anndata.py#L736) by. ```python. if multi_panel and groupby is None and len(ys) == 1:. g = sns.catplot(y=y, data=obs_tidy, kind=""violin"", col=x, col_order=keys, sharey=False, order=keys, **kwds). ```. gives the desired plot (except for different size). The strip plot can be added on top by calling `seaborn.stripplot` afterwards:. ```python. if multi_panel and groupby is None and len(ys) == 1:. g = sns.catplot(y=y, data=obs_tidy, kind=""violin"", col=x, col_order=keys, sharey=False, order=keys, **kwds). if stripplot:. sns.stripplot(y=y, data=obs_tidy, jitter=jitter, color=""black""). ```. At the moment, I am just unsure how to plot to each subplot of the new `g`. It might be possible to loop through the plot grid so not to add everything on top of the last plot. ![master_violin_multi_panel](https://user-images.githubusercontent.com/28675704/93460801-16626300-f8e4-11ea-8f46-ed7ff64d8efb.png). Besides the not yet solved strip plot problem, would that be a valid alternative?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1420
https://github.com/scverse/scanpy/issues/1420:756,modifiability,scal,scale,756,"Yes, I saw that too but I was hoping it could be prevented as it seems suboptimal to me and is probably confusing when looking at the code. Is there a specific reason for using `seaborn.FacetGrid` instead of `seaborn.catplot` (see [here](https://seaborn.pydata.org/generated/seaborn.catplot.html#seaborn.catplot))? Replacing the lines. ```python. # ... kwds.setdefault('cut', 0). kwds.setdefault('inner'). if multi_panel and groupby is None and len(ys) == 1:. # This is a quick and dirty way for adapting scales across several. # keys if groupby is None. y = ys[0]. g = sns.FacetGrid(obs_tidy, col=x, col_order=keys, sharey=False). # don't really know why this gives a warning without passing `order`. g = g.map(sns.violinplot, y, orient='vertical', scale=scale, order=keys, **kwds). ```. [here](https://github.com/theislab/scanpy/blob/f704f724529def21769ee6407f9b47b5c161564c/scanpy/plotting/_anndata.py#L736) by. ```python. if multi_panel and groupby is None and len(ys) == 1:. g = sns.catplot(y=y, data=obs_tidy, kind=""violin"", col=x, col_order=keys, sharey=False, order=keys, **kwds). ```. gives the desired plot (except for different size). The strip plot can be added on top by calling `seaborn.stripplot` afterwards:. ```python. if multi_panel and groupby is None and len(ys) == 1:. g = sns.catplot(y=y, data=obs_tidy, kind=""violin"", col=x, col_order=keys, sharey=False, order=keys, **kwds). if stripplot:. sns.stripplot(y=y, data=obs_tidy, jitter=jitter, color=""black""). ```. At the moment, I am just unsure how to plot to each subplot of the new `g`. It might be possible to loop through the plot grid so not to add everything on top of the last plot. ![master_violin_multi_panel](https://user-images.githubusercontent.com/28675704/93460801-16626300-f8e4-11ea-8f46-ed7ff64d8efb.png). Besides the not yet solved strip plot problem, would that be a valid alternative?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1420
https://github.com/scverse/scanpy/issues/1420:505,performance,scale,scales,505,"Yes, I saw that too but I was hoping it could be prevented as it seems suboptimal to me and is probably confusing when looking at the code. Is there a specific reason for using `seaborn.FacetGrid` instead of `seaborn.catplot` (see [here](https://seaborn.pydata.org/generated/seaborn.catplot.html#seaborn.catplot))? Replacing the lines. ```python. # ... kwds.setdefault('cut', 0). kwds.setdefault('inner'). if multi_panel and groupby is None and len(ys) == 1:. # This is a quick and dirty way for adapting scales across several. # keys if groupby is None. y = ys[0]. g = sns.FacetGrid(obs_tidy, col=x, col_order=keys, sharey=False). # don't really know why this gives a warning without passing `order`. g = g.map(sns.violinplot, y, orient='vertical', scale=scale, order=keys, **kwds). ```. [here](https://github.com/theislab/scanpy/blob/f704f724529def21769ee6407f9b47b5c161564c/scanpy/plotting/_anndata.py#L736) by. ```python. if multi_panel and groupby is None and len(ys) == 1:. g = sns.catplot(y=y, data=obs_tidy, kind=""violin"", col=x, col_order=keys, sharey=False, order=keys, **kwds). ```. gives the desired plot (except for different size). The strip plot can be added on top by calling `seaborn.stripplot` afterwards:. ```python. if multi_panel and groupby is None and len(ys) == 1:. g = sns.catplot(y=y, data=obs_tidy, kind=""violin"", col=x, col_order=keys, sharey=False, order=keys, **kwds). if stripplot:. sns.stripplot(y=y, data=obs_tidy, jitter=jitter, color=""black""). ```. At the moment, I am just unsure how to plot to each subplot of the new `g`. It might be possible to loop through the plot grid so not to add everything on top of the last plot. ![master_violin_multi_panel](https://user-images.githubusercontent.com/28675704/93460801-16626300-f8e4-11ea-8f46-ed7ff64d8efb.png). Besides the not yet solved strip plot problem, would that be a valid alternative?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1420
https://github.com/scverse/scanpy/issues/1420:750,performance,scale,scale,750,"Yes, I saw that too but I was hoping it could be prevented as it seems suboptimal to me and is probably confusing when looking at the code. Is there a specific reason for using `seaborn.FacetGrid` instead of `seaborn.catplot` (see [here](https://seaborn.pydata.org/generated/seaborn.catplot.html#seaborn.catplot))? Replacing the lines. ```python. # ... kwds.setdefault('cut', 0). kwds.setdefault('inner'). if multi_panel and groupby is None and len(ys) == 1:. # This is a quick and dirty way for adapting scales across several. # keys if groupby is None. y = ys[0]. g = sns.FacetGrid(obs_tidy, col=x, col_order=keys, sharey=False). # don't really know why this gives a warning without passing `order`. g = g.map(sns.violinplot, y, orient='vertical', scale=scale, order=keys, **kwds). ```. [here](https://github.com/theislab/scanpy/blob/f704f724529def21769ee6407f9b47b5c161564c/scanpy/plotting/_anndata.py#L736) by. ```python. if multi_panel and groupby is None and len(ys) == 1:. g = sns.catplot(y=y, data=obs_tidy, kind=""violin"", col=x, col_order=keys, sharey=False, order=keys, **kwds). ```. gives the desired plot (except for different size). The strip plot can be added on top by calling `seaborn.stripplot` afterwards:. ```python. if multi_panel and groupby is None and len(ys) == 1:. g = sns.catplot(y=y, data=obs_tidy, kind=""violin"", col=x, col_order=keys, sharey=False, order=keys, **kwds). if stripplot:. sns.stripplot(y=y, data=obs_tidy, jitter=jitter, color=""black""). ```. At the moment, I am just unsure how to plot to each subplot of the new `g`. It might be possible to loop through the plot grid so not to add everything on top of the last plot. ![master_violin_multi_panel](https://user-images.githubusercontent.com/28675704/93460801-16626300-f8e4-11ea-8f46-ed7ff64d8efb.png). Besides the not yet solved strip plot problem, would that be a valid alternative?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1420
https://github.com/scverse/scanpy/issues/1420:756,performance,scale,scale,756,"Yes, I saw that too but I was hoping it could be prevented as it seems suboptimal to me and is probably confusing when looking at the code. Is there a specific reason for using `seaborn.FacetGrid` instead of `seaborn.catplot` (see [here](https://seaborn.pydata.org/generated/seaborn.catplot.html#seaborn.catplot))? Replacing the lines. ```python. # ... kwds.setdefault('cut', 0). kwds.setdefault('inner'). if multi_panel and groupby is None and len(ys) == 1:. # This is a quick and dirty way for adapting scales across several. # keys if groupby is None. y = ys[0]. g = sns.FacetGrid(obs_tidy, col=x, col_order=keys, sharey=False). # don't really know why this gives a warning without passing `order`. g = g.map(sns.violinplot, y, orient='vertical', scale=scale, order=keys, **kwds). ```. [here](https://github.com/theislab/scanpy/blob/f704f724529def21769ee6407f9b47b5c161564c/scanpy/plotting/_anndata.py#L736) by. ```python. if multi_panel and groupby is None and len(ys) == 1:. g = sns.catplot(y=y, data=obs_tidy, kind=""violin"", col=x, col_order=keys, sharey=False, order=keys, **kwds). ```. gives the desired plot (except for different size). The strip plot can be added on top by calling `seaborn.stripplot` afterwards:. ```python. if multi_panel and groupby is None and len(ys) == 1:. g = sns.catplot(y=y, data=obs_tidy, kind=""violin"", col=x, col_order=keys, sharey=False, order=keys, **kwds). if stripplot:. sns.stripplot(y=y, data=obs_tidy, jitter=jitter, color=""black""). ```. At the moment, I am just unsure how to plot to each subplot of the new `g`. It might be possible to loop through the plot grid so not to add everything on top of the last plot. ![master_violin_multi_panel](https://user-images.githubusercontent.com/28675704/93460801-16626300-f8e4-11ea-8f46-ed7ff64d8efb.png). Besides the not yet solved strip plot problem, would that be a valid alternative?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1420
https://github.com/scverse/scanpy/issues/1420:49,safety,prevent,prevented,49,"Yes, I saw that too but I was hoping it could be prevented as it seems suboptimal to me and is probably confusing when looking at the code. Is there a specific reason for using `seaborn.FacetGrid` instead of `seaborn.catplot` (see [here](https://seaborn.pydata.org/generated/seaborn.catplot.html#seaborn.catplot))? Replacing the lines. ```python. # ... kwds.setdefault('cut', 0). kwds.setdefault('inner'). if multi_panel and groupby is None and len(ys) == 1:. # This is a quick and dirty way for adapting scales across several. # keys if groupby is None. y = ys[0]. g = sns.FacetGrid(obs_tidy, col=x, col_order=keys, sharey=False). # don't really know why this gives a warning without passing `order`. g = g.map(sns.violinplot, y, orient='vertical', scale=scale, order=keys, **kwds). ```. [here](https://github.com/theislab/scanpy/blob/f704f724529def21769ee6407f9b47b5c161564c/scanpy/plotting/_anndata.py#L736) by. ```python. if multi_panel and groupby is None and len(ys) == 1:. g = sns.catplot(y=y, data=obs_tidy, kind=""violin"", col=x, col_order=keys, sharey=False, order=keys, **kwds). ```. gives the desired plot (except for different size). The strip plot can be added on top by calling `seaborn.stripplot` afterwards:. ```python. if multi_panel and groupby is None and len(ys) == 1:. g = sns.catplot(y=y, data=obs_tidy, kind=""violin"", col=x, col_order=keys, sharey=False, order=keys, **kwds). if stripplot:. sns.stripplot(y=y, data=obs_tidy, jitter=jitter, color=""black""). ```. At the moment, I am just unsure how to plot to each subplot of the new `g`. It might be possible to loop through the plot grid so not to add everything on top of the last plot. ![master_violin_multi_panel](https://user-images.githubusercontent.com/28675704/93460801-16626300-f8e4-11ea-8f46-ed7ff64d8efb.png). Besides the not yet solved strip plot problem, would that be a valid alternative?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1420
https://github.com/scverse/scanpy/issues/1420:1118,safety,except,except,1118,"Yes, I saw that too but I was hoping it could be prevented as it seems suboptimal to me and is probably confusing when looking at the code. Is there a specific reason for using `seaborn.FacetGrid` instead of `seaborn.catplot` (see [here](https://seaborn.pydata.org/generated/seaborn.catplot.html#seaborn.catplot))? Replacing the lines. ```python. # ... kwds.setdefault('cut', 0). kwds.setdefault('inner'). if multi_panel and groupby is None and len(ys) == 1:. # This is a quick and dirty way for adapting scales across several. # keys if groupby is None. y = ys[0]. g = sns.FacetGrid(obs_tidy, col=x, col_order=keys, sharey=False). # don't really know why this gives a warning without passing `order`. g = g.map(sns.violinplot, y, orient='vertical', scale=scale, order=keys, **kwds). ```. [here](https://github.com/theislab/scanpy/blob/f704f724529def21769ee6407f9b47b5c161564c/scanpy/plotting/_anndata.py#L736) by. ```python. if multi_panel and groupby is None and len(ys) == 1:. g = sns.catplot(y=y, data=obs_tidy, kind=""violin"", col=x, col_order=keys, sharey=False, order=keys, **kwds). ```. gives the desired plot (except for different size). The strip plot can be added on top by calling `seaborn.stripplot` afterwards:. ```python. if multi_panel and groupby is None and len(ys) == 1:. g = sns.catplot(y=y, data=obs_tidy, kind=""violin"", col=x, col_order=keys, sharey=False, order=keys, **kwds). if stripplot:. sns.stripplot(y=y, data=obs_tidy, jitter=jitter, color=""black""). ```. At the moment, I am just unsure how to plot to each subplot of the new `g`. It might be possible to loop through the plot grid so not to add everything on top of the last plot. ![master_violin_multi_panel](https://user-images.githubusercontent.com/28675704/93460801-16626300-f8e4-11ea-8f46-ed7ff64d8efb.png). Besides the not yet solved strip plot problem, would that be a valid alternative?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1420
https://github.com/scverse/scanpy/issues/1420:1856,safety,valid,valid,1856,"Yes, I saw that too but I was hoping it could be prevented as it seems suboptimal to me and is probably confusing when looking at the code. Is there a specific reason for using `seaborn.FacetGrid` instead of `seaborn.catplot` (see [here](https://seaborn.pydata.org/generated/seaborn.catplot.html#seaborn.catplot))? Replacing the lines. ```python. # ... kwds.setdefault('cut', 0). kwds.setdefault('inner'). if multi_panel and groupby is None and len(ys) == 1:. # This is a quick and dirty way for adapting scales across several. # keys if groupby is None. y = ys[0]. g = sns.FacetGrid(obs_tidy, col=x, col_order=keys, sharey=False). # don't really know why this gives a warning without passing `order`. g = g.map(sns.violinplot, y, orient='vertical', scale=scale, order=keys, **kwds). ```. [here](https://github.com/theislab/scanpy/blob/f704f724529def21769ee6407f9b47b5c161564c/scanpy/plotting/_anndata.py#L736) by. ```python. if multi_panel and groupby is None and len(ys) == 1:. g = sns.catplot(y=y, data=obs_tidy, kind=""violin"", col=x, col_order=keys, sharey=False, order=keys, **kwds). ```. gives the desired plot (except for different size). The strip plot can be added on top by calling `seaborn.stripplot` afterwards:. ```python. if multi_panel and groupby is None and len(ys) == 1:. g = sns.catplot(y=y, data=obs_tidy, kind=""violin"", col=x, col_order=keys, sharey=False, order=keys, **kwds). if stripplot:. sns.stripplot(y=y, data=obs_tidy, jitter=jitter, color=""black""). ```. At the moment, I am just unsure how to plot to each subplot of the new `g`. It might be possible to loop through the plot grid so not to add everything on top of the last plot. ![master_violin_multi_panel](https://user-images.githubusercontent.com/28675704/93460801-16626300-f8e4-11ea-8f46-ed7ff64d8efb.png). Besides the not yet solved strip plot problem, would that be a valid alternative?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1420
https://github.com/scverse/scanpy/issues/1420:49,security,preven,prevented,49,"Yes, I saw that too but I was hoping it could be prevented as it seems suboptimal to me and is probably confusing when looking at the code. Is there a specific reason for using `seaborn.FacetGrid` instead of `seaborn.catplot` (see [here](https://seaborn.pydata.org/generated/seaborn.catplot.html#seaborn.catplot))? Replacing the lines. ```python. # ... kwds.setdefault('cut', 0). kwds.setdefault('inner'). if multi_panel and groupby is None and len(ys) == 1:. # This is a quick and dirty way for adapting scales across several. # keys if groupby is None. y = ys[0]. g = sns.FacetGrid(obs_tidy, col=x, col_order=keys, sharey=False). # don't really know why this gives a warning without passing `order`. g = g.map(sns.violinplot, y, orient='vertical', scale=scale, order=keys, **kwds). ```. [here](https://github.com/theislab/scanpy/blob/f704f724529def21769ee6407f9b47b5c161564c/scanpy/plotting/_anndata.py#L736) by. ```python. if multi_panel and groupby is None and len(ys) == 1:. g = sns.catplot(y=y, data=obs_tidy, kind=""violin"", col=x, col_order=keys, sharey=False, order=keys, **kwds). ```. gives the desired plot (except for different size). The strip plot can be added on top by calling `seaborn.stripplot` afterwards:. ```python. if multi_panel and groupby is None and len(ys) == 1:. g = sns.catplot(y=y, data=obs_tidy, kind=""violin"", col=x, col_order=keys, sharey=False, order=keys, **kwds). if stripplot:. sns.stripplot(y=y, data=obs_tidy, jitter=jitter, color=""black""). ```. At the moment, I am just unsure how to plot to each subplot of the new `g`. It might be possible to loop through the plot grid so not to add everything on top of the last plot. ![master_violin_multi_panel](https://user-images.githubusercontent.com/28675704/93460801-16626300-f8e4-11ea-8f46-ed7ff64d8efb.png). Besides the not yet solved strip plot problem, would that be a valid alternative?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1420
https://github.com/scverse/scanpy/issues/1420:1698,usability,user,user-images,1698,"Yes, I saw that too but I was hoping it could be prevented as it seems suboptimal to me and is probably confusing when looking at the code. Is there a specific reason for using `seaborn.FacetGrid` instead of `seaborn.catplot` (see [here](https://seaborn.pydata.org/generated/seaborn.catplot.html#seaborn.catplot))? Replacing the lines. ```python. # ... kwds.setdefault('cut', 0). kwds.setdefault('inner'). if multi_panel and groupby is None and len(ys) == 1:. # This is a quick and dirty way for adapting scales across several. # keys if groupby is None. y = ys[0]. g = sns.FacetGrid(obs_tidy, col=x, col_order=keys, sharey=False). # don't really know why this gives a warning without passing `order`. g = g.map(sns.violinplot, y, orient='vertical', scale=scale, order=keys, **kwds). ```. [here](https://github.com/theislab/scanpy/blob/f704f724529def21769ee6407f9b47b5c161564c/scanpy/plotting/_anndata.py#L736) by. ```python. if multi_panel and groupby is None and len(ys) == 1:. g = sns.catplot(y=y, data=obs_tidy, kind=""violin"", col=x, col_order=keys, sharey=False, order=keys, **kwds). ```. gives the desired plot (except for different size). The strip plot can be added on top by calling `seaborn.stripplot` afterwards:. ```python. if multi_panel and groupby is None and len(ys) == 1:. g = sns.catplot(y=y, data=obs_tidy, kind=""violin"", col=x, col_order=keys, sharey=False, order=keys, **kwds). if stripplot:. sns.stripplot(y=y, data=obs_tidy, jitter=jitter, color=""black""). ```. At the moment, I am just unsure how to plot to each subplot of the new `g`. It might be possible to loop through the plot grid so not to add everything on top of the last plot. ![master_violin_multi_panel](https://user-images.githubusercontent.com/28675704/93460801-16626300-f8e4-11ea-8f46-ed7ff64d8efb.png). Besides the not yet solved strip plot problem, would that be a valid alternative?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1420
https://github.com/scverse/scanpy/issues/1420:60,reliability,doe,does,60,There is not particular reason to use FacetGrid. If catplot does the job then it is ok.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1420
https://github.com/scverse/scanpy/issues/1420:248,deployability,scale,scales,248,"The following code produces the desired plot (up to permutation of the subplots and the image size) in case of only one row of subplots:. ```python. if multi_panel and groupby is None and len(ys) == 1:. # This is a quick and dirty way for adapting scales across several. # keys if groupby is None. y = ys[0]. g = sns.catplot(y=y, data=obs_tidy, kind=""violin"", col=x, col_order=keys, sharey=False, order=keys, **kwds). if stripplot:. grouped_df = obs_tidy.groupby(x). for ax_id, key in zip(range(g.axes.shape[1]), keys):. sns.stripplot(y=y, data=grouped_df.get_group(key), jitter=jitter, size=size, color=""black"", ax=g.axes[0, ax_id], **kwds). ```. ![master_violin_multi_panel](https://user-images.githubusercontent.com/28675704/93485925-e5922600-f903-11ea-9edb-0f7523a67c0d.png). Seems a bit hacky to me. What do you think @fidelram?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1420
https://github.com/scverse/scanpy/issues/1420:239,energy efficiency,adapt,adapting,239,"The following code produces the desired plot (up to permutation of the subplots and the image size) in case of only one row of subplots:. ```python. if multi_panel and groupby is None and len(ys) == 1:. # This is a quick and dirty way for adapting scales across several. # keys if groupby is None. y = ys[0]. g = sns.catplot(y=y, data=obs_tidy, kind=""violin"", col=x, col_order=keys, sharey=False, order=keys, **kwds). if stripplot:. grouped_df = obs_tidy.groupby(x). for ax_id, key in zip(range(g.axes.shape[1]), keys):. sns.stripplot(y=y, data=grouped_df.get_group(key), jitter=jitter, size=size, color=""black"", ax=g.axes[0, ax_id], **kwds). ```. ![master_violin_multi_panel](https://user-images.githubusercontent.com/28675704/93485925-e5922600-f903-11ea-9edb-0f7523a67c0d.png). Seems a bit hacky to me. What do you think @fidelram?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1420
https://github.com/scverse/scanpy/issues/1420:248,energy efficiency,scale,scales,248,"The following code produces the desired plot (up to permutation of the subplots and the image size) in case of only one row of subplots:. ```python. if multi_panel and groupby is None and len(ys) == 1:. # This is a quick and dirty way for adapting scales across several. # keys if groupby is None. y = ys[0]. g = sns.catplot(y=y, data=obs_tidy, kind=""violin"", col=x, col_order=keys, sharey=False, order=keys, **kwds). if stripplot:. grouped_df = obs_tidy.groupby(x). for ax_id, key in zip(range(g.axes.shape[1]), keys):. sns.stripplot(y=y, data=grouped_df.get_group(key), jitter=jitter, size=size, color=""black"", ax=g.axes[0, ax_id], **kwds). ```. ![master_violin_multi_panel](https://user-images.githubusercontent.com/28675704/93485925-e5922600-f903-11ea-9edb-0f7523a67c0d.png). Seems a bit hacky to me. What do you think @fidelram?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1420
https://github.com/scverse/scanpy/issues/1420:71,integrability,sub,subplots,71,"The following code produces the desired plot (up to permutation of the subplots and the image size) in case of only one row of subplots:. ```python. if multi_panel and groupby is None and len(ys) == 1:. # This is a quick and dirty way for adapting scales across several. # keys if groupby is None. y = ys[0]. g = sns.catplot(y=y, data=obs_tidy, kind=""violin"", col=x, col_order=keys, sharey=False, order=keys, **kwds). if stripplot:. grouped_df = obs_tidy.groupby(x). for ax_id, key in zip(range(g.axes.shape[1]), keys):. sns.stripplot(y=y, data=grouped_df.get_group(key), jitter=jitter, size=size, color=""black"", ax=g.axes[0, ax_id], **kwds). ```. ![master_violin_multi_panel](https://user-images.githubusercontent.com/28675704/93485925-e5922600-f903-11ea-9edb-0f7523a67c0d.png). Seems a bit hacky to me. What do you think @fidelram?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1420
https://github.com/scverse/scanpy/issues/1420:127,integrability,sub,subplots,127,"The following code produces the desired plot (up to permutation of the subplots and the image size) in case of only one row of subplots:. ```python. if multi_panel and groupby is None and len(ys) == 1:. # This is a quick and dirty way for adapting scales across several. # keys if groupby is None. y = ys[0]. g = sns.catplot(y=y, data=obs_tidy, kind=""violin"", col=x, col_order=keys, sharey=False, order=keys, **kwds). if stripplot:. grouped_df = obs_tidy.groupby(x). for ax_id, key in zip(range(g.axes.shape[1]), keys):. sns.stripplot(y=y, data=grouped_df.get_group(key), jitter=jitter, size=size, color=""black"", ax=g.axes[0, ax_id], **kwds). ```. ![master_violin_multi_panel](https://user-images.githubusercontent.com/28675704/93485925-e5922600-f903-11ea-9edb-0f7523a67c0d.png). Seems a bit hacky to me. What do you think @fidelram?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1420
https://github.com/scverse/scanpy/issues/1420:239,integrability,adapt,adapting,239,"The following code produces the desired plot (up to permutation of the subplots and the image size) in case of only one row of subplots:. ```python. if multi_panel and groupby is None and len(ys) == 1:. # This is a quick and dirty way for adapting scales across several. # keys if groupby is None. y = ys[0]. g = sns.catplot(y=y, data=obs_tidy, kind=""violin"", col=x, col_order=keys, sharey=False, order=keys, **kwds). if stripplot:. grouped_df = obs_tidy.groupby(x). for ax_id, key in zip(range(g.axes.shape[1]), keys):. sns.stripplot(y=y, data=grouped_df.get_group(key), jitter=jitter, size=size, color=""black"", ax=g.axes[0, ax_id], **kwds). ```. ![master_violin_multi_panel](https://user-images.githubusercontent.com/28675704/93485925-e5922600-f903-11ea-9edb-0f7523a67c0d.png). Seems a bit hacky to me. What do you think @fidelram?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1420
https://github.com/scverse/scanpy/issues/1420:239,interoperability,adapt,adapting,239,"The following code produces the desired plot (up to permutation of the subplots and the image size) in case of only one row of subplots:. ```python. if multi_panel and groupby is None and len(ys) == 1:. # This is a quick and dirty way for adapting scales across several. # keys if groupby is None. y = ys[0]. g = sns.catplot(y=y, data=obs_tidy, kind=""violin"", col=x, col_order=keys, sharey=False, order=keys, **kwds). if stripplot:. grouped_df = obs_tidy.groupby(x). for ax_id, key in zip(range(g.axes.shape[1]), keys):. sns.stripplot(y=y, data=grouped_df.get_group(key), jitter=jitter, size=size, color=""black"", ax=g.axes[0, ax_id], **kwds). ```. ![master_violin_multi_panel](https://user-images.githubusercontent.com/28675704/93485925-e5922600-f903-11ea-9edb-0f7523a67c0d.png). Seems a bit hacky to me. What do you think @fidelram?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1420
https://github.com/scverse/scanpy/issues/1420:383,interoperability,share,sharey,383,"The following code produces the desired plot (up to permutation of the subplots and the image size) in case of only one row of subplots:. ```python. if multi_panel and groupby is None and len(ys) == 1:. # This is a quick and dirty way for adapting scales across several. # keys if groupby is None. y = ys[0]. g = sns.catplot(y=y, data=obs_tidy, kind=""violin"", col=x, col_order=keys, sharey=False, order=keys, **kwds). if stripplot:. grouped_df = obs_tidy.groupby(x). for ax_id, key in zip(range(g.axes.shape[1]), keys):. sns.stripplot(y=y, data=grouped_df.get_group(key), jitter=jitter, size=size, color=""black"", ax=g.axes[0, ax_id], **kwds). ```. ![master_violin_multi_panel](https://user-images.githubusercontent.com/28675704/93485925-e5922600-f903-11ea-9edb-0f7523a67c0d.png). Seems a bit hacky to me. What do you think @fidelram?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1420
https://github.com/scverse/scanpy/issues/1420:239,modifiability,adapt,adapting,239,"The following code produces the desired plot (up to permutation of the subplots and the image size) in case of only one row of subplots:. ```python. if multi_panel and groupby is None and len(ys) == 1:. # This is a quick and dirty way for adapting scales across several. # keys if groupby is None. y = ys[0]. g = sns.catplot(y=y, data=obs_tidy, kind=""violin"", col=x, col_order=keys, sharey=False, order=keys, **kwds). if stripplot:. grouped_df = obs_tidy.groupby(x). for ax_id, key in zip(range(g.axes.shape[1]), keys):. sns.stripplot(y=y, data=grouped_df.get_group(key), jitter=jitter, size=size, color=""black"", ax=g.axes[0, ax_id], **kwds). ```. ![master_violin_multi_panel](https://user-images.githubusercontent.com/28675704/93485925-e5922600-f903-11ea-9edb-0f7523a67c0d.png). Seems a bit hacky to me. What do you think @fidelram?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1420
https://github.com/scverse/scanpy/issues/1420:248,modifiability,scal,scales,248,"The following code produces the desired plot (up to permutation of the subplots and the image size) in case of only one row of subplots:. ```python. if multi_panel and groupby is None and len(ys) == 1:. # This is a quick and dirty way for adapting scales across several. # keys if groupby is None. y = ys[0]. g = sns.catplot(y=y, data=obs_tidy, kind=""violin"", col=x, col_order=keys, sharey=False, order=keys, **kwds). if stripplot:. grouped_df = obs_tidy.groupby(x). for ax_id, key in zip(range(g.axes.shape[1]), keys):. sns.stripplot(y=y, data=grouped_df.get_group(key), jitter=jitter, size=size, color=""black"", ax=g.axes[0, ax_id], **kwds). ```. ![master_violin_multi_panel](https://user-images.githubusercontent.com/28675704/93485925-e5922600-f903-11ea-9edb-0f7523a67c0d.png). Seems a bit hacky to me. What do you think @fidelram?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1420
https://github.com/scverse/scanpy/issues/1420:248,performance,scale,scales,248,"The following code produces the desired plot (up to permutation of the subplots and the image size) in case of only one row of subplots:. ```python. if multi_panel and groupby is None and len(ys) == 1:. # This is a quick and dirty way for adapting scales across several. # keys if groupby is None. y = ys[0]. g = sns.catplot(y=y, data=obs_tidy, kind=""violin"", col=x, col_order=keys, sharey=False, order=keys, **kwds). if stripplot:. grouped_df = obs_tidy.groupby(x). for ax_id, key in zip(range(g.axes.shape[1]), keys):. sns.stripplot(y=y, data=grouped_df.get_group(key), jitter=jitter, size=size, color=""black"", ax=g.axes[0, ax_id], **kwds). ```. ![master_violin_multi_panel](https://user-images.githubusercontent.com/28675704/93485925-e5922600-f903-11ea-9edb-0f7523a67c0d.png). Seems a bit hacky to me. What do you think @fidelram?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1420
https://github.com/scverse/scanpy/issues/1420:792,security,hack,hacky,792,"The following code produces the desired plot (up to permutation of the subplots and the image size) in case of only one row of subplots:. ```python. if multi_panel and groupby is None and len(ys) == 1:. # This is a quick and dirty way for adapting scales across several. # keys if groupby is None. y = ys[0]. g = sns.catplot(y=y, data=obs_tidy, kind=""violin"", col=x, col_order=keys, sharey=False, order=keys, **kwds). if stripplot:. grouped_df = obs_tidy.groupby(x). for ax_id, key in zip(range(g.axes.shape[1]), keys):. sns.stripplot(y=y, data=grouped_df.get_group(key), jitter=jitter, size=size, color=""black"", ax=g.axes[0, ax_id], **kwds). ```. ![master_violin_multi_panel](https://user-images.githubusercontent.com/28675704/93485925-e5922600-f903-11ea-9edb-0f7523a67c0d.png). Seems a bit hacky to me. What do you think @fidelram?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1420
https://github.com/scverse/scanpy/issues/1420:685,usability,user,user-images,685,"The following code produces the desired plot (up to permutation of the subplots and the image size) in case of only one row of subplots:. ```python. if multi_panel and groupby is None and len(ys) == 1:. # This is a quick and dirty way for adapting scales across several. # keys if groupby is None. y = ys[0]. g = sns.catplot(y=y, data=obs_tidy, kind=""violin"", col=x, col_order=keys, sharey=False, order=keys, **kwds). if stripplot:. grouped_df = obs_tidy.groupby(x). for ax_id, key in zip(range(g.axes.shape[1]), keys):. sns.stripplot(y=y, data=grouped_df.get_group(key), jitter=jitter, size=size, color=""black"", ax=g.axes[0, ax_id], **kwds). ```. ![master_violin_multi_panel](https://user-images.githubusercontent.com/28675704/93485925-e5922600-f903-11ea-9edb-0f7523a67c0d.png). Seems a bit hacky to me. What do you think @fidelram?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1420
https://github.com/scverse/scanpy/issues/1420:157,safety,test,tests,157,@WeilerP probably there is a more direct way to overlay the stripplot but I don't think that it makes any big difference. . Can you make a PR to see how the tests work?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1420
https://github.com/scverse/scanpy/issues/1420:157,testability,test,tests,157,@WeilerP probably there is a more direct way to overlay the stripplot but I don't think that it makes any big difference. . Can you make a PR to see how the tests work?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1420
https://github.com/scverse/scanpy/pull/1421:21,deployability,updat,update,21,@adamgayoso A recent update of seaborn caused some trouble with the tests but is now fixed. Can you merge with master to trigger again the tests?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1421
https://github.com/scverse/scanpy/pull/1421:21,safety,updat,update,21,@adamgayoso A recent update of seaborn caused some trouble with the tests but is now fixed. Can you merge with master to trigger again the tests?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1421
https://github.com/scverse/scanpy/pull/1421:68,safety,test,tests,68,@adamgayoso A recent update of seaborn caused some trouble with the tests but is now fixed. Can you merge with master to trigger again the tests?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1421
https://github.com/scverse/scanpy/pull/1421:139,safety,test,tests,139,@adamgayoso A recent update of seaborn caused some trouble with the tests but is now fixed. Can you merge with master to trigger again the tests?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1421
https://github.com/scverse/scanpy/pull/1421:21,security,updat,update,21,@adamgayoso A recent update of seaborn caused some trouble with the tests but is now fixed. Can you merge with master to trigger again the tests?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1421
https://github.com/scverse/scanpy/pull/1421:68,testability,test,tests,68,@adamgayoso A recent update of seaborn caused some trouble with the tests but is now fixed. Can you merge with master to trigger again the tests?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1421
https://github.com/scverse/scanpy/pull/1421:139,testability,test,tests,139,@adamgayoso A recent update of seaborn caused some trouble with the tests but is now fixed. Can you merge with master to trigger again the tests?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1421
https://github.com/scverse/scanpy/pull/1422:102,availability,failur,failures,102,"@fidelram, from `seaborn==0.12` on, the only valid positional argument will be `data` which may cause failures or unexpected behaviour. This is relevant at least [here](https://github.com/theislab/scanpy/blob/f704f724529def21769ee6407f9b47b5c161564c/scanpy/plotting/_anndata.py#L774) and [here](https://github.com/theislab/scanpy/blob/f704f724529def21769ee6407f9b47b5c161564c/scanpy/plotting/_anndata.py#L785). Should we add / specify the keyword argument `x` in this PR or open a separate issue and PR?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422
https://github.com/scverse/scanpy/pull/1422:102,deployability,fail,failures,102,"@fidelram, from `seaborn==0.12` on, the only valid positional argument will be `data` which may cause failures or unexpected behaviour. This is relevant at least [here](https://github.com/theislab/scanpy/blob/f704f724529def21769ee6407f9b47b5c161564c/scanpy/plotting/_anndata.py#L774) and [here](https://github.com/theislab/scanpy/blob/f704f724529def21769ee6407f9b47b5c161564c/scanpy/plotting/_anndata.py#L785). Should we add / specify the keyword argument `x` in this PR or open a separate issue and PR?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422
https://github.com/scverse/scanpy/pull/1422:427,interoperability,specif,specify,427,"@fidelram, from `seaborn==0.12` on, the only valid positional argument will be `data` which may cause failures or unexpected behaviour. This is relevant at least [here](https://github.com/theislab/scanpy/blob/f704f724529def21769ee6407f9b47b5c161564c/scanpy/plotting/_anndata.py#L774) and [here](https://github.com/theislab/scanpy/blob/f704f724529def21769ee6407f9b47b5c161564c/scanpy/plotting/_anndata.py#L785). Should we add / specify the keyword argument `x` in this PR or open a separate issue and PR?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422
https://github.com/scverse/scanpy/pull/1422:102,performance,failur,failures,102,"@fidelram, from `seaborn==0.12` on, the only valid positional argument will be `data` which may cause failures or unexpected behaviour. This is relevant at least [here](https://github.com/theislab/scanpy/blob/f704f724529def21769ee6407f9b47b5c161564c/scanpy/plotting/_anndata.py#L774) and [here](https://github.com/theislab/scanpy/blob/f704f724529def21769ee6407f9b47b5c161564c/scanpy/plotting/_anndata.py#L785). Should we add / specify the keyword argument `x` in this PR or open a separate issue and PR?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422
https://github.com/scverse/scanpy/pull/1422:102,reliability,fail,failures,102,"@fidelram, from `seaborn==0.12` on, the only valid positional argument will be `data` which may cause failures or unexpected behaviour. This is relevant at least [here](https://github.com/theislab/scanpy/blob/f704f724529def21769ee6407f9b47b5c161564c/scanpy/plotting/_anndata.py#L774) and [here](https://github.com/theislab/scanpy/blob/f704f724529def21769ee6407f9b47b5c161564c/scanpy/plotting/_anndata.py#L785). Should we add / specify the keyword argument `x` in this PR or open a separate issue and PR?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422
https://github.com/scverse/scanpy/pull/1422:45,safety,valid,valid,45,"@fidelram, from `seaborn==0.12` on, the only valid positional argument will be `data` which may cause failures or unexpected behaviour. This is relevant at least [here](https://github.com/theislab/scanpy/blob/f704f724529def21769ee6407f9b47b5c161564c/scanpy/plotting/_anndata.py#L774) and [here](https://github.com/theislab/scanpy/blob/f704f724529def21769ee6407f9b47b5c161564c/scanpy/plotting/_anndata.py#L785). Should we add / specify the keyword argument `x` in this PR or open a separate issue and PR?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422
https://github.com/scverse/scanpy/pull/1422:125,usability,behavi,behaviour,125,"@fidelram, from `seaborn==0.12` on, the only valid positional argument will be `data` which may cause failures or unexpected behaviour. This is relevant at least [here](https://github.com/theislab/scanpy/blob/f704f724529def21769ee6407f9b47b5c161564c/scanpy/plotting/_anndata.py#L774) and [here](https://github.com/theislab/scanpy/blob/f704f724529def21769ee6407f9b47b5c161564c/scanpy/plotting/_anndata.py#L785). Should we add / specify the keyword argument `x` in this PR or open a separate issue and PR?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422
https://github.com/scverse/scanpy/pull/1422:29,safety,test,tests,29,"I noticed, however, that the tests pass even when removing the `if stripplot:` part. Any idea on why this is happening and how to prevent it?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422
https://github.com/scverse/scanpy/pull/1422:130,safety,prevent,prevent,130,"I noticed, however, that the tests pass even when removing the `if stripplot:` part. Any idea on why this is happening and how to prevent it?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422
https://github.com/scverse/scanpy/pull/1422:130,security,preven,prevent,130,"I noticed, however, that the tests pass even when removing the `if stripplot:` part. Any idea on why this is happening and how to prevent it?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422
https://github.com/scverse/scanpy/pull/1422:29,testability,test,tests,29,"I noticed, however, that the tests pass even when removing the `if stripplot:` part. Any idea on why this is happening and how to prevent it?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422
https://github.com/scverse/scanpy/pull/1422:17,availability,toler,tolerance,17,"The tests have a tolerance parameter that is set high. The problem is that the stripplot shows different results each time. Also, different versions of matplotlib and seaborn have slight differences.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422
https://github.com/scverse/scanpy/pull/1422:180,availability,sli,slight,180,"The tests have a tolerance parameter that is set high. The problem is that the stripplot shows different results each time. Also, different versions of matplotlib and seaborn have slight differences.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422
https://github.com/scverse/scanpy/pull/1422:140,deployability,version,versions,140,"The tests have a tolerance parameter that is set high. The problem is that the stripplot shows different results each time. Also, different versions of matplotlib and seaborn have slight differences.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422
https://github.com/scverse/scanpy/pull/1422:140,integrability,version,versions,140,"The tests have a tolerance parameter that is set high. The problem is that the stripplot shows different results each time. Also, different versions of matplotlib and seaborn have slight differences.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422
https://github.com/scverse/scanpy/pull/1422:27,modifiability,paramet,parameter,27,"The tests have a tolerance parameter that is set high. The problem is that the stripplot shows different results each time. Also, different versions of matplotlib and seaborn have slight differences.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422
https://github.com/scverse/scanpy/pull/1422:140,modifiability,version,versions,140,"The tests have a tolerance parameter that is set high. The problem is that the stripplot shows different results each time. Also, different versions of matplotlib and seaborn have slight differences.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422
https://github.com/scverse/scanpy/pull/1422:118,performance,time,time,118,"The tests have a tolerance parameter that is set high. The problem is that the stripplot shows different results each time. Also, different versions of matplotlib and seaborn have slight differences.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422
https://github.com/scverse/scanpy/pull/1422:17,reliability,toleran,tolerance,17,"The tests have a tolerance parameter that is set high. The problem is that the stripplot shows different results each time. Also, different versions of matplotlib and seaborn have slight differences.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422
https://github.com/scverse/scanpy/pull/1422:180,reliability,sli,slight,180,"The tests have a tolerance parameter that is set high. The problem is that the stripplot shows different results each time. Also, different versions of matplotlib and seaborn have slight differences.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422
https://github.com/scverse/scanpy/pull/1422:4,safety,test,tests,4,"The tests have a tolerance parameter that is set high. The problem is that the stripplot shows different results each time. Also, different versions of matplotlib and seaborn have slight differences.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422
https://github.com/scverse/scanpy/pull/1422:4,testability,test,tests,4,"The tests have a tolerance parameter that is set high. The problem is that the stripplot shows different results each time. Also, different versions of matplotlib and seaborn have slight differences.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422
https://github.com/scverse/scanpy/pull/1422:17,safety,test,tests,17,I wonder why the tests are not working now?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422
https://github.com/scverse/scanpy/pull/1422:17,testability,test,tests,17,I wonder why the tests are not working now?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422
https://github.com/scverse/scanpy/pull/1422:65,deployability,updat,update,65,"> I wonder why the tests are not working now? Sorry, I forgot to update `violin.png` after the latest changes to `_anndata.py`. Let's see if the tests pass now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422
https://github.com/scverse/scanpy/pull/1422:19,safety,test,tests,19,"> I wonder why the tests are not working now? Sorry, I forgot to update `violin.png` after the latest changes to `_anndata.py`. Let's see if the tests pass now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422
https://github.com/scverse/scanpy/pull/1422:65,safety,updat,update,65,"> I wonder why the tests are not working now? Sorry, I forgot to update `violin.png` after the latest changes to `_anndata.py`. Let's see if the tests pass now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422
https://github.com/scverse/scanpy/pull/1422:145,safety,test,tests,145,"> I wonder why the tests are not working now? Sorry, I forgot to update `violin.png` after the latest changes to `_anndata.py`. Let's see if the tests pass now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422
https://github.com/scverse/scanpy/pull/1422:65,security,updat,update,65,"> I wonder why the tests are not working now? Sorry, I forgot to update `violin.png` after the latest changes to `_anndata.py`. Let's see if the tests pass now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422
https://github.com/scverse/scanpy/pull/1422:19,testability,test,tests,19,"> I wonder why the tests are not working now? Sorry, I forgot to update `violin.png` after the latest changes to `_anndata.py`. Let's see if the tests pass now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422
https://github.com/scverse/scanpy/pull/1422:145,testability,test,tests,145,"> I wonder why the tests are not working now? Sorry, I forgot to update `violin.png` after the latest changes to `_anndata.py`. Let's see if the tests pass now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422
https://github.com/scverse/scanpy/pull/1422:19,availability,toler,tolerance,19,"> The tests have a tolerance parameter that is set high. The problem is that the stripplot shows different results each time. Also, different versions of matplotlib and seaborn have slight differences. Ah yes, I see. The stripplot result could be fixed by setting a seed with `np.random.seed`. I doubt it will fix the difference due to the used version, though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422
https://github.com/scverse/scanpy/pull/1422:182,availability,sli,slight,182,"> The tests have a tolerance parameter that is set high. The problem is that the stripplot shows different results each time. Also, different versions of matplotlib and seaborn have slight differences. Ah yes, I see. The stripplot result could be fixed by setting a seed with `np.random.seed`. I doubt it will fix the difference due to the used version, though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422
https://github.com/scverse/scanpy/pull/1422:142,deployability,version,versions,142,"> The tests have a tolerance parameter that is set high. The problem is that the stripplot shows different results each time. Also, different versions of matplotlib and seaborn have slight differences. Ah yes, I see. The stripplot result could be fixed by setting a seed with `np.random.seed`. I doubt it will fix the difference due to the used version, though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422
https://github.com/scverse/scanpy/pull/1422:345,deployability,version,version,345,"> The tests have a tolerance parameter that is set high. The problem is that the stripplot shows different results each time. Also, different versions of matplotlib and seaborn have slight differences. Ah yes, I see. The stripplot result could be fixed by setting a seed with `np.random.seed`. I doubt it will fix the difference due to the used version, though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422
https://github.com/scverse/scanpy/pull/1422:142,integrability,version,versions,142,"> The tests have a tolerance parameter that is set high. The problem is that the stripplot shows different results each time. Also, different versions of matplotlib and seaborn have slight differences. Ah yes, I see. The stripplot result could be fixed by setting a seed with `np.random.seed`. I doubt it will fix the difference due to the used version, though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422
https://github.com/scverse/scanpy/pull/1422:345,integrability,version,version,345,"> The tests have a tolerance parameter that is set high. The problem is that the stripplot shows different results each time. Also, different versions of matplotlib and seaborn have slight differences. Ah yes, I see. The stripplot result could be fixed by setting a seed with `np.random.seed`. I doubt it will fix the difference due to the used version, though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422
https://github.com/scverse/scanpy/pull/1422:29,modifiability,paramet,parameter,29,"> The tests have a tolerance parameter that is set high. The problem is that the stripplot shows different results each time. Also, different versions of matplotlib and seaborn have slight differences. Ah yes, I see. The stripplot result could be fixed by setting a seed with `np.random.seed`. I doubt it will fix the difference due to the used version, though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422
https://github.com/scverse/scanpy/pull/1422:142,modifiability,version,versions,142,"> The tests have a tolerance parameter that is set high. The problem is that the stripplot shows different results each time. Also, different versions of matplotlib and seaborn have slight differences. Ah yes, I see. The stripplot result could be fixed by setting a seed with `np.random.seed`. I doubt it will fix the difference due to the used version, though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422
https://github.com/scverse/scanpy/pull/1422:345,modifiability,version,version,345,"> The tests have a tolerance parameter that is set high. The problem is that the stripplot shows different results each time. Also, different versions of matplotlib and seaborn have slight differences. Ah yes, I see. The stripplot result could be fixed by setting a seed with `np.random.seed`. I doubt it will fix the difference due to the used version, though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422
https://github.com/scverse/scanpy/pull/1422:120,performance,time,time,120,"> The tests have a tolerance parameter that is set high. The problem is that the stripplot shows different results each time. Also, different versions of matplotlib and seaborn have slight differences. Ah yes, I see. The stripplot result could be fixed by setting a seed with `np.random.seed`. I doubt it will fix the difference due to the used version, though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422
https://github.com/scverse/scanpy/pull/1422:19,reliability,toleran,tolerance,19,"> The tests have a tolerance parameter that is set high. The problem is that the stripplot shows different results each time. Also, different versions of matplotlib and seaborn have slight differences. Ah yes, I see. The stripplot result could be fixed by setting a seed with `np.random.seed`. I doubt it will fix the difference due to the used version, though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422
https://github.com/scverse/scanpy/pull/1422:182,reliability,sli,slight,182,"> The tests have a tolerance parameter that is set high. The problem is that the stripplot shows different results each time. Also, different versions of matplotlib and seaborn have slight differences. Ah yes, I see. The stripplot result could be fixed by setting a seed with `np.random.seed`. I doubt it will fix the difference due to the used version, though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422
https://github.com/scverse/scanpy/pull/1422:6,safety,test,tests,6,"> The tests have a tolerance parameter that is set high. The problem is that the stripplot shows different results each time. Also, different versions of matplotlib and seaborn have slight differences. Ah yes, I see. The stripplot result could be fixed by setting a seed with `np.random.seed`. I doubt it will fix the difference due to the used version, though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422
https://github.com/scverse/scanpy/pull/1422:6,testability,test,tests,6,"> The tests have a tolerance parameter that is set high. The problem is that the stripplot shows different results each time. Also, different versions of matplotlib and seaborn have slight differences. Ah yes, I see. The stripplot result could be fixed by setting a seed with `np.random.seed`. I doubt it will fix the difference due to the used version, though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422
https://github.com/scverse/scanpy/pull/1422:149,deployability,version,version,149,"Hi Developers,. I got the same issue when using seaborn==0.11, and fortunately I got the issue solved by replacing the annotate.py with the modified version. Howerver, I was wondering why I tried to use 'pip install scanpy' to update the scripts, it failed? Is there any other easier method to modify the script, not to locate the file and replace it?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422
https://github.com/scverse/scanpy/pull/1422:208,deployability,instal,install,208,"Hi Developers,. I got the same issue when using seaborn==0.11, and fortunately I got the issue solved by replacing the annotate.py with the modified version. Howerver, I was wondering why I tried to use 'pip install scanpy' to update the scripts, it failed? Is there any other easier method to modify the script, not to locate the file and replace it?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422
https://github.com/scverse/scanpy/pull/1422:227,deployability,updat,update,227,"Hi Developers,. I got the same issue when using seaborn==0.11, and fortunately I got the issue solved by replacing the annotate.py with the modified version. Howerver, I was wondering why I tried to use 'pip install scanpy' to update the scripts, it failed? Is there any other easier method to modify the script, not to locate the file and replace it?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422
https://github.com/scverse/scanpy/pull/1422:250,deployability,fail,failed,250,"Hi Developers,. I got the same issue when using seaborn==0.11, and fortunately I got the issue solved by replacing the annotate.py with the modified version. Howerver, I was wondering why I tried to use 'pip install scanpy' to update the scripts, it failed? Is there any other easier method to modify the script, not to locate the file and replace it?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422
https://github.com/scverse/scanpy/pull/1422:149,integrability,version,version,149,"Hi Developers,. I got the same issue when using seaborn==0.11, and fortunately I got the issue solved by replacing the annotate.py with the modified version. Howerver, I was wondering why I tried to use 'pip install scanpy' to update the scripts, it failed? Is there any other easier method to modify the script, not to locate the file and replace it?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422
https://github.com/scverse/scanpy/pull/1422:149,modifiability,version,version,149,"Hi Developers,. I got the same issue when using seaborn==0.11, and fortunately I got the issue solved by replacing the annotate.py with the modified version. Howerver, I was wondering why I tried to use 'pip install scanpy' to update the scripts, it failed? Is there any other easier method to modify the script, not to locate the file and replace it?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422
https://github.com/scverse/scanpy/pull/1422:250,reliability,fail,failed,250,"Hi Developers,. I got the same issue when using seaborn==0.11, and fortunately I got the issue solved by replacing the annotate.py with the modified version. Howerver, I was wondering why I tried to use 'pip install scanpy' to update the scripts, it failed? Is there any other easier method to modify the script, not to locate the file and replace it?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422
https://github.com/scverse/scanpy/pull/1422:227,safety,updat,update,227,"Hi Developers,. I got the same issue when using seaborn==0.11, and fortunately I got the issue solved by replacing the annotate.py with the modified version. Howerver, I was wondering why I tried to use 'pip install scanpy' to update the scripts, it failed? Is there any other easier method to modify the script, not to locate the file and replace it?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422
https://github.com/scverse/scanpy/pull/1422:140,security,modif,modified,140,"Hi Developers,. I got the same issue when using seaborn==0.11, and fortunately I got the issue solved by replacing the annotate.py with the modified version. Howerver, I was wondering why I tried to use 'pip install scanpy' to update the scripts, it failed? Is there any other easier method to modify the script, not to locate the file and replace it?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422
https://github.com/scverse/scanpy/pull/1422:227,security,updat,update,227,"Hi Developers,. I got the same issue when using seaborn==0.11, and fortunately I got the issue solved by replacing the annotate.py with the modified version. Howerver, I was wondering why I tried to use 'pip install scanpy' to update the scripts, it failed? Is there any other easier method to modify the script, not to locate the file and replace it?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422
https://github.com/scverse/scanpy/pull/1422:294,security,modif,modify,294,"Hi Developers,. I got the same issue when using seaborn==0.11, and fortunately I got the issue solved by replacing the annotate.py with the modified version. Howerver, I was wondering why I tried to use 'pip install scanpy' to update the scripts, it failed? Is there any other easier method to modify the script, not to locate the file and replace it?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422
https://github.com/scverse/scanpy/pull/1422:353,availability,avail,available,353,"@hhhh1230511, this PR is not part of any release yet (the latest version `scanpy==1.6` was released August 15, 2020). If you want to have the latest version from GitHub you can follow the instructions for a developer installation [here](https://scanpy.readthedocs.io/en/stable/installation.html) in the documentation, for example. Once a new release is available on `pip`, you can install it via `pip install --upgrade scanpy`. In general, you should avoid modifying the code by e.g. simply copying and pasting. This will either easily cause conflicts when updating the package or cause problems when functions from other files which depend on the content you changed but were not updated accordingly. Hope this helped and clarified things.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422
https://github.com/scverse/scanpy/pull/1422:41,deployability,releas,release,41,"@hhhh1230511, this PR is not part of any release yet (the latest version `scanpy==1.6` was released August 15, 2020). If you want to have the latest version from GitHub you can follow the instructions for a developer installation [here](https://scanpy.readthedocs.io/en/stable/installation.html) in the documentation, for example. Once a new release is available on `pip`, you can install it via `pip install --upgrade scanpy`. In general, you should avoid modifying the code by e.g. simply copying and pasting. This will either easily cause conflicts when updating the package or cause problems when functions from other files which depend on the content you changed but were not updated accordingly. Hope this helped and clarified things.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422
https://github.com/scverse/scanpy/pull/1422:65,deployability,version,version,65,"@hhhh1230511, this PR is not part of any release yet (the latest version `scanpy==1.6` was released August 15, 2020). If you want to have the latest version from GitHub you can follow the instructions for a developer installation [here](https://scanpy.readthedocs.io/en/stable/installation.html) in the documentation, for example. Once a new release is available on `pip`, you can install it via `pip install --upgrade scanpy`. In general, you should avoid modifying the code by e.g. simply copying and pasting. This will either easily cause conflicts when updating the package or cause problems when functions from other files which depend on the content you changed but were not updated accordingly. Hope this helped and clarified things.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422
https://github.com/scverse/scanpy/pull/1422:91,deployability,releas,released,91,"@hhhh1230511, this PR is not part of any release yet (the latest version `scanpy==1.6` was released August 15, 2020). If you want to have the latest version from GitHub you can follow the instructions for a developer installation [here](https://scanpy.readthedocs.io/en/stable/installation.html) in the documentation, for example. Once a new release is available on `pip`, you can install it via `pip install --upgrade scanpy`. In general, you should avoid modifying the code by e.g. simply copying and pasting. This will either easily cause conflicts when updating the package or cause problems when functions from other files which depend on the content you changed but were not updated accordingly. Hope this helped and clarified things.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422
https://github.com/scverse/scanpy/pull/1422:149,deployability,version,version,149,"@hhhh1230511, this PR is not part of any release yet (the latest version `scanpy==1.6` was released August 15, 2020). If you want to have the latest version from GitHub you can follow the instructions for a developer installation [here](https://scanpy.readthedocs.io/en/stable/installation.html) in the documentation, for example. Once a new release is available on `pip`, you can install it via `pip install --upgrade scanpy`. In general, you should avoid modifying the code by e.g. simply copying and pasting. This will either easily cause conflicts when updating the package or cause problems when functions from other files which depend on the content you changed but were not updated accordingly. Hope this helped and clarified things.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422
https://github.com/scverse/scanpy/pull/1422:217,deployability,instal,installation,217,"@hhhh1230511, this PR is not part of any release yet (the latest version `scanpy==1.6` was released August 15, 2020). If you want to have the latest version from GitHub you can follow the instructions for a developer installation [here](https://scanpy.readthedocs.io/en/stable/installation.html) in the documentation, for example. Once a new release is available on `pip`, you can install it via `pip install --upgrade scanpy`. In general, you should avoid modifying the code by e.g. simply copying and pasting. This will either easily cause conflicts when updating the package or cause problems when functions from other files which depend on the content you changed but were not updated accordingly. Hope this helped and clarified things.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422
https://github.com/scverse/scanpy/pull/1422:277,deployability,instal,installation,277,"@hhhh1230511, this PR is not part of any release yet (the latest version `scanpy==1.6` was released August 15, 2020). If you want to have the latest version from GitHub you can follow the instructions for a developer installation [here](https://scanpy.readthedocs.io/en/stable/installation.html) in the documentation, for example. Once a new release is available on `pip`, you can install it via `pip install --upgrade scanpy`. In general, you should avoid modifying the code by e.g. simply copying and pasting. This will either easily cause conflicts when updating the package or cause problems when functions from other files which depend on the content you changed but were not updated accordingly. Hope this helped and clarified things.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422
https://github.com/scverse/scanpy/pull/1422:342,deployability,releas,release,342,"@hhhh1230511, this PR is not part of any release yet (the latest version `scanpy==1.6` was released August 15, 2020). If you want to have the latest version from GitHub you can follow the instructions for a developer installation [here](https://scanpy.readthedocs.io/en/stable/installation.html) in the documentation, for example. Once a new release is available on `pip`, you can install it via `pip install --upgrade scanpy`. In general, you should avoid modifying the code by e.g. simply copying and pasting. This will either easily cause conflicts when updating the package or cause problems when functions from other files which depend on the content you changed but were not updated accordingly. Hope this helped and clarified things.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422
https://github.com/scverse/scanpy/pull/1422:381,deployability,instal,install,381,"@hhhh1230511, this PR is not part of any release yet (the latest version `scanpy==1.6` was released August 15, 2020). If you want to have the latest version from GitHub you can follow the instructions for a developer installation [here](https://scanpy.readthedocs.io/en/stable/installation.html) in the documentation, for example. Once a new release is available on `pip`, you can install it via `pip install --upgrade scanpy`. In general, you should avoid modifying the code by e.g. simply copying and pasting. This will either easily cause conflicts when updating the package or cause problems when functions from other files which depend on the content you changed but were not updated accordingly. Hope this helped and clarified things.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422
https://github.com/scverse/scanpy/pull/1422:401,deployability,instal,install,401,"@hhhh1230511, this PR is not part of any release yet (the latest version `scanpy==1.6` was released August 15, 2020). If you want to have the latest version from GitHub you can follow the instructions for a developer installation [here](https://scanpy.readthedocs.io/en/stable/installation.html) in the documentation, for example. Once a new release is available on `pip`, you can install it via `pip install --upgrade scanpy`. In general, you should avoid modifying the code by e.g. simply copying and pasting. This will either easily cause conflicts when updating the package or cause problems when functions from other files which depend on the content you changed but were not updated accordingly. Hope this helped and clarified things.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422
https://github.com/scverse/scanpy/pull/1422:411,deployability,upgrad,upgrade,411,"@hhhh1230511, this PR is not part of any release yet (the latest version `scanpy==1.6` was released August 15, 2020). If you want to have the latest version from GitHub you can follow the instructions for a developer installation [here](https://scanpy.readthedocs.io/en/stable/installation.html) in the documentation, for example. Once a new release is available on `pip`, you can install it via `pip install --upgrade scanpy`. In general, you should avoid modifying the code by e.g. simply copying and pasting. This will either easily cause conflicts when updating the package or cause problems when functions from other files which depend on the content you changed but were not updated accordingly. Hope this helped and clarified things.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422
https://github.com/scverse/scanpy/pull/1422:557,deployability,updat,updating,557,"@hhhh1230511, this PR is not part of any release yet (the latest version `scanpy==1.6` was released August 15, 2020). If you want to have the latest version from GitHub you can follow the instructions for a developer installation [here](https://scanpy.readthedocs.io/en/stable/installation.html) in the documentation, for example. Once a new release is available on `pip`, you can install it via `pip install --upgrade scanpy`. In general, you should avoid modifying the code by e.g. simply copying and pasting. This will either easily cause conflicts when updating the package or cause problems when functions from other files which depend on the content you changed but were not updated accordingly. Hope this helped and clarified things.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422
https://github.com/scverse/scanpy/pull/1422:634,deployability,depend,depend,634,"@hhhh1230511, this PR is not part of any release yet (the latest version `scanpy==1.6` was released August 15, 2020). If you want to have the latest version from GitHub you can follow the instructions for a developer installation [here](https://scanpy.readthedocs.io/en/stable/installation.html) in the documentation, for example. Once a new release is available on `pip`, you can install it via `pip install --upgrade scanpy`. In general, you should avoid modifying the code by e.g. simply copying and pasting. This will either easily cause conflicts when updating the package or cause problems when functions from other files which depend on the content you changed but were not updated accordingly. Hope this helped and clarified things.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422
https://github.com/scverse/scanpy/pull/1422:681,deployability,updat,updated,681,"@hhhh1230511, this PR is not part of any release yet (the latest version `scanpy==1.6` was released August 15, 2020). If you want to have the latest version from GitHub you can follow the instructions for a developer installation [here](https://scanpy.readthedocs.io/en/stable/installation.html) in the documentation, for example. Once a new release is available on `pip`, you can install it via `pip install --upgrade scanpy`. In general, you should avoid modifying the code by e.g. simply copying and pasting. This will either easily cause conflicts when updating the package or cause problems when functions from other files which depend on the content you changed but were not updated accordingly. Hope this helped and clarified things.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422
https://github.com/scverse/scanpy/pull/1422:65,integrability,version,version,65,"@hhhh1230511, this PR is not part of any release yet (the latest version `scanpy==1.6` was released August 15, 2020). If you want to have the latest version from GitHub you can follow the instructions for a developer installation [here](https://scanpy.readthedocs.io/en/stable/installation.html) in the documentation, for example. Once a new release is available on `pip`, you can install it via `pip install --upgrade scanpy`. In general, you should avoid modifying the code by e.g. simply copying and pasting. This will either easily cause conflicts when updating the package or cause problems when functions from other files which depend on the content you changed but were not updated accordingly. Hope this helped and clarified things.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422
https://github.com/scverse/scanpy/pull/1422:149,integrability,version,version,149,"@hhhh1230511, this PR is not part of any release yet (the latest version `scanpy==1.6` was released August 15, 2020). If you want to have the latest version from GitHub you can follow the instructions for a developer installation [here](https://scanpy.readthedocs.io/en/stable/installation.html) in the documentation, for example. Once a new release is available on `pip`, you can install it via `pip install --upgrade scanpy`. In general, you should avoid modifying the code by e.g. simply copying and pasting. This will either easily cause conflicts when updating the package or cause problems when functions from other files which depend on the content you changed but were not updated accordingly. Hope this helped and clarified things.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422
