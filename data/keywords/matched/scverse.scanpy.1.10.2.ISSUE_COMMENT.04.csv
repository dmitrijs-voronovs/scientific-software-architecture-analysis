id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/scverse/scanpy/pull/316:649,deployability,updat,update,649,"Oh, I think I misunderstood earlier when you said:. > I just think that you should probably also add the top-level function to the qc.py file in preprocessing. . I wasn't sure if you meant move `calculate_qc_metrics` to `qc.py` or add `top_proportions` and `top_segment_proportions` to the preprocessing module. If you're not asking for that, I'm not sure if they're important enough to go there. I use `top_proportions` to make a `plotScater` kind of plot, but that's about it. Otherwise, I think this might be good for now. I was thinking I'd update the tutorial to use this function after the PR is merged. Once that's done, is there a script to update the tests under `notebooks` or is that done manually?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:304,modifiability,modul,module,304,"Oh, I think I misunderstood earlier when you said:. > I just think that you should probably also add the top-level function to the qc.py file in preprocessing. . I wasn't sure if you meant move `calculate_qc_metrics` to `qc.py` or add `top_proportions` and `top_segment_proportions` to the preprocessing module. If you're not asking for that, I'm not sure if they're important enough to go there. I use `top_proportions` to make a `plotScater` kind of plot, but that's about it. Otherwise, I think this might be good for now. I was thinking I'd update the tutorial to use this function after the PR is merged. Once that's done, is there a script to update the tests under `notebooks` or is that done manually?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:304,safety,modul,module,304,"Oh, I think I misunderstood earlier when you said:. > I just think that you should probably also add the top-level function to the qc.py file in preprocessing. . I wasn't sure if you meant move `calculate_qc_metrics` to `qc.py` or add `top_proportions` and `top_segment_proportions` to the preprocessing module. If you're not asking for that, I'm not sure if they're important enough to go there. I use `top_proportions` to make a `plotScater` kind of plot, but that's about it. Otherwise, I think this might be good for now. I was thinking I'd update the tutorial to use this function after the PR is merged. Once that's done, is there a script to update the tests under `notebooks` or is that done manually?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:545,safety,updat,update,545,"Oh, I think I misunderstood earlier when you said:. > I just think that you should probably also add the top-level function to the qc.py file in preprocessing. . I wasn't sure if you meant move `calculate_qc_metrics` to `qc.py` or add `top_proportions` and `top_segment_proportions` to the preprocessing module. If you're not asking for that, I'm not sure if they're important enough to go there. I use `top_proportions` to make a `plotScater` kind of plot, but that's about it. Otherwise, I think this might be good for now. I was thinking I'd update the tutorial to use this function after the PR is merged. Once that's done, is there a script to update the tests under `notebooks` or is that done manually?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:649,safety,updat,update,649,"Oh, I think I misunderstood earlier when you said:. > I just think that you should probably also add the top-level function to the qc.py file in preprocessing. . I wasn't sure if you meant move `calculate_qc_metrics` to `qc.py` or add `top_proportions` and `top_segment_proportions` to the preprocessing module. If you're not asking for that, I'm not sure if they're important enough to go there. I use `top_proportions` to make a `plotScater` kind of plot, but that's about it. Otherwise, I think this might be good for now. I was thinking I'd update the tutorial to use this function after the PR is merged. Once that's done, is there a script to update the tests under `notebooks` or is that done manually?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:660,safety,test,tests,660,"Oh, I think I misunderstood earlier when you said:. > I just think that you should probably also add the top-level function to the qc.py file in preprocessing. . I wasn't sure if you meant move `calculate_qc_metrics` to `qc.py` or add `top_proportions` and `top_segment_proportions` to the preprocessing module. If you're not asking for that, I'm not sure if they're important enough to go there. I use `top_proportions` to make a `plotScater` kind of plot, but that's about it. Otherwise, I think this might be good for now. I was thinking I'd update the tutorial to use this function after the PR is merged. Once that's done, is there a script to update the tests under `notebooks` or is that done manually?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:545,security,updat,update,545,"Oh, I think I misunderstood earlier when you said:. > I just think that you should probably also add the top-level function to the qc.py file in preprocessing. . I wasn't sure if you meant move `calculate_qc_metrics` to `qc.py` or add `top_proportions` and `top_segment_proportions` to the preprocessing module. If you're not asking for that, I'm not sure if they're important enough to go there. I use `top_proportions` to make a `plotScater` kind of plot, but that's about it. Otherwise, I think this might be good for now. I was thinking I'd update the tutorial to use this function after the PR is merged. Once that's done, is there a script to update the tests under `notebooks` or is that done manually?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:649,security,updat,update,649,"Oh, I think I misunderstood earlier when you said:. > I just think that you should probably also add the top-level function to the qc.py file in preprocessing. . I wasn't sure if you meant move `calculate_qc_metrics` to `qc.py` or add `top_proportions` and `top_segment_proportions` to the preprocessing module. If you're not asking for that, I'm not sure if they're important enough to go there. I use `top_proportions` to make a `plotScater` kind of plot, but that's about it. Otherwise, I think this might be good for now. I was thinking I'd update the tutorial to use this function after the PR is merged. Once that's done, is there a script to update the tests under `notebooks` or is that done manually?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:660,testability,test,tests,660,"Oh, I think I misunderstood earlier when you said:. > I just think that you should probably also add the top-level function to the qc.py file in preprocessing. . I wasn't sure if you meant move `calculate_qc_metrics` to `qc.py` or add `top_proportions` and `top_segment_proportions` to the preprocessing module. If you're not asking for that, I'm not sure if they're important enough to go there. I use `top_proportions` to make a `plotScater` kind of plot, but that's about it. Otherwise, I think this might be good for now. I was thinking I'd update the tutorial to use this function after the PR is merged. Once that's done, is there a script to update the tests under `notebooks` or is that done manually?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:68,deployability,Updat,Updating,68,"Thank you, great! I meant moving `calculate_qc_metrics` to `qc.py`. Updating the tutorial after this is good! The tests under `notebooks/` are currently updated manually. . :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:153,deployability,updat,updated,153,"Thank you, great! I meant moving `calculate_qc_metrics` to `qc.py`. Updating the tutorial after this is good! The tests under `notebooks/` are currently updated manually. . :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:143,energy efficiency,current,currently,143,"Thank you, great! I meant moving `calculate_qc_metrics` to `qc.py`. Updating the tutorial after this is good! The tests under `notebooks/` are currently updated manually. . :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:68,safety,Updat,Updating,68,"Thank you, great! I meant moving `calculate_qc_metrics` to `qc.py`. Updating the tutorial after this is good! The tests under `notebooks/` are currently updated manually. . :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:114,safety,test,tests,114,"Thank you, great! I meant moving `calculate_qc_metrics` to `qc.py`. Updating the tutorial after this is good! The tests under `notebooks/` are currently updated manually. . :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:153,safety,updat,updated,153,"Thank you, great! I meant moving `calculate_qc_metrics` to `qc.py`. Updating the tutorial after this is good! The tests under `notebooks/` are currently updated manually. . :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:68,security,Updat,Updating,68,"Thank you, great! I meant moving `calculate_qc_metrics` to `qc.py`. Updating the tutorial after this is good! The tests under `notebooks/` are currently updated manually. . :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:153,security,updat,updated,153,"Thank you, great! I meant moving `calculate_qc_metrics` to `qc.py`. Updating the tutorial after this is good! The tests under `notebooks/` are currently updated manually. . :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:114,testability,test,tests,114,"Thank you, great! I meant moving `calculate_qc_metrics` to `qc.py`. Updating the tutorial after this is good! The tests under `notebooks/` are currently updated manually. . :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:36,deployability,releas,release,36,Would you mind adding a note to the release notes and adding the function to the docs to complete the whole PR: https://github.com/theislab/scanpy/blob/master/docs/release_notes.rst?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:89,safety,compl,complete,89,Would you mind adding a note to the release notes and adding the function to the docs to complete the whole PR: https://github.com/theislab/scanpy/blob/master/docs/release_notes.rst?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:89,security,compl,complete,89,Would you mind adding a note to the release notes and adding the function to the docs to complete the whole PR: https://github.com/theislab/scanpy/blob/master/docs/release_notes.rst?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:449,availability,consist,consistent,449,"I think I got the docs right, let me know if there are any issues. I'm noticing some conflict with convention for metric names. I'm copying `scater`, and using labels like `total_counts` and `total_features_by_counts`. `filter_genes`, `filter_cells`, `spring_project`, and a couple of the recipes use `n_counts` or `n_cells`. My preference is for the `scater ` way, since formatting allows it to be bit more flexible. It'd be nice for there to be a consistent default key for these features. For example, while updating the clustering tutorial, I ended up with both `n_counts` and `total_counts` in the same `adata.obs`. As changing the defaults could break some code, what's the right path forward? When `scater` updated their metric names, I think they used both the old and new keys with a deprecation warning. They talk about it a bit under the documentation for `scatter::calcuateQCMetrics`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:524,availability,cluster,clustering,524,"I think I got the docs right, let me know if there are any issues. I'm noticing some conflict with convention for metric names. I'm copying `scater`, and using labels like `total_counts` and `total_features_by_counts`. `filter_genes`, `filter_cells`, `spring_project`, and a couple of the recipes use `n_counts` or `n_cells`. My preference is for the `scater ` way, since formatting allows it to be bit more flexible. It'd be nice for there to be a consistent default key for these features. For example, while updating the clustering tutorial, I ended up with both `n_counts` and `total_counts` in the same `adata.obs`. As changing the defaults could break some code, what's the right path forward? When `scater` updated their metric names, I think they used both the old and new keys with a deprecation warning. They talk about it a bit under the documentation for `scatter::calcuateQCMetrics`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:511,deployability,updat,updating,511,"I think I got the docs right, let me know if there are any issues. I'm noticing some conflict with convention for metric names. I'm copying `scater`, and using labels like `total_counts` and `total_features_by_counts`. `filter_genes`, `filter_cells`, `spring_project`, and a couple of the recipes use `n_counts` or `n_cells`. My preference is for the `scater ` way, since formatting allows it to be bit more flexible. It'd be nice for there to be a consistent default key for these features. For example, while updating the clustering tutorial, I ended up with both `n_counts` and `total_counts` in the same `adata.obs`. As changing the defaults could break some code, what's the right path forward? When `scater` updated their metric names, I think they used both the old and new keys with a deprecation warning. They talk about it a bit under the documentation for `scatter::calcuateQCMetrics`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:524,deployability,cluster,clustering,524,"I think I got the docs right, let me know if there are any issues. I'm noticing some conflict with convention for metric names. I'm copying `scater`, and using labels like `total_counts` and `total_features_by_counts`. `filter_genes`, `filter_cells`, `spring_project`, and a couple of the recipes use `n_counts` or `n_cells`. My preference is for the `scater ` way, since formatting allows it to be bit more flexible. It'd be nice for there to be a consistent default key for these features. For example, while updating the clustering tutorial, I ended up with both `n_counts` and `total_counts` in the same `adata.obs`. As changing the defaults could break some code, what's the right path forward? When `scater` updated their metric names, I think they used both the old and new keys with a deprecation warning. They talk about it a bit under the documentation for `scatter::calcuateQCMetrics`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:714,deployability,updat,updated,714,"I think I got the docs right, let me know if there are any issues. I'm noticing some conflict with convention for metric names. I'm copying `scater`, and using labels like `total_counts` and `total_features_by_counts`. `filter_genes`, `filter_cells`, `spring_project`, and a couple of the recipes use `n_counts` or `n_cells`. My preference is for the `scater ` way, since formatting allows it to be bit more flexible. It'd be nice for there to be a consistent default key for these features. For example, while updating the clustering tutorial, I ended up with both `n_counts` and `total_counts` in the same `adata.obs`. As changing the defaults could break some code, what's the right path forward? When `scater` updated their metric names, I think they used both the old and new keys with a deprecation warning. They talk about it a bit under the documentation for `scatter::calcuateQCMetrics`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:275,integrability,coupl,couple,275,"I think I got the docs right, let me know if there are any issues. I'm noticing some conflict with convention for metric names. I'm copying `scater`, and using labels like `total_counts` and `total_features_by_counts`. `filter_genes`, `filter_cells`, `spring_project`, and a couple of the recipes use `n_counts` or `n_cells`. My preference is for the `scater ` way, since formatting allows it to be bit more flexible. It'd be nice for there to be a consistent default key for these features. For example, while updating the clustering tutorial, I ended up with both `n_counts` and `total_counts` in the same `adata.obs`. As changing the defaults could break some code, what's the right path forward? When `scater` updated their metric names, I think they used both the old and new keys with a deprecation warning. They talk about it a bit under the documentation for `scatter::calcuateQCMetrics`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:85,interoperability,conflict,conflict,85,"I think I got the docs right, let me know if there are any issues. I'm noticing some conflict with convention for metric names. I'm copying `scater`, and using labels like `total_counts` and `total_features_by_counts`. `filter_genes`, `filter_cells`, `spring_project`, and a couple of the recipes use `n_counts` or `n_cells`. My preference is for the `scater ` way, since formatting allows it to be bit more flexible. It'd be nice for there to be a consistent default key for these features. For example, while updating the clustering tutorial, I ended up with both `n_counts` and `total_counts` in the same `adata.obs`. As changing the defaults could break some code, what's the right path forward? When `scater` updated their metric names, I think they used both the old and new keys with a deprecation warning. They talk about it a bit under the documentation for `scatter::calcuateQCMetrics`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:372,interoperability,format,formatting,372,"I think I got the docs right, let me know if there are any issues. I'm noticing some conflict with convention for metric names. I'm copying `scater`, and using labels like `total_counts` and `total_features_by_counts`. `filter_genes`, `filter_cells`, `spring_project`, and a couple of the recipes use `n_counts` or `n_cells`. My preference is for the `scater ` way, since formatting allows it to be bit more flexible. It'd be nice for there to be a consistent default key for these features. For example, while updating the clustering tutorial, I ended up with both `n_counts` and `total_counts` in the same `adata.obs`. As changing the defaults could break some code, what's the right path forward? When `scater` updated their metric names, I think they used both the old and new keys with a deprecation warning. They talk about it a bit under the documentation for `scatter::calcuateQCMetrics`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:275,modifiability,coupl,couple,275,"I think I got the docs right, let me know if there are any issues. I'm noticing some conflict with convention for metric names. I'm copying `scater`, and using labels like `total_counts` and `total_features_by_counts`. `filter_genes`, `filter_cells`, `spring_project`, and a couple of the recipes use `n_counts` or `n_cells`. My preference is for the `scater ` way, since formatting allows it to be bit more flexible. It'd be nice for there to be a consistent default key for these features. For example, while updating the clustering tutorial, I ended up with both `n_counts` and `total_counts` in the same `adata.obs`. As changing the defaults could break some code, what's the right path forward? When `scater` updated their metric names, I think they used both the old and new keys with a deprecation warning. They talk about it a bit under the documentation for `scatter::calcuateQCMetrics`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:511,safety,updat,updating,511,"I think I got the docs right, let me know if there are any issues. I'm noticing some conflict with convention for metric names. I'm copying `scater`, and using labels like `total_counts` and `total_features_by_counts`. `filter_genes`, `filter_cells`, `spring_project`, and a couple of the recipes use `n_counts` or `n_cells`. My preference is for the `scater ` way, since formatting allows it to be bit more flexible. It'd be nice for there to be a consistent default key for these features. For example, while updating the clustering tutorial, I ended up with both `n_counts` and `total_counts` in the same `adata.obs`. As changing the defaults could break some code, what's the right path forward? When `scater` updated their metric names, I think they used both the old and new keys with a deprecation warning. They talk about it a bit under the documentation for `scatter::calcuateQCMetrics`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:714,safety,updat,updated,714,"I think I got the docs right, let me know if there are any issues. I'm noticing some conflict with convention for metric names. I'm copying `scater`, and using labels like `total_counts` and `total_features_by_counts`. `filter_genes`, `filter_cells`, `spring_project`, and a couple of the recipes use `n_counts` or `n_cells`. My preference is for the `scater ` way, since formatting allows it to be bit more flexible. It'd be nice for there to be a consistent default key for these features. For example, while updating the clustering tutorial, I ended up with both `n_counts` and `total_counts` in the same `adata.obs`. As changing the defaults could break some code, what's the right path forward? When `scater` updated their metric names, I think they used both the old and new keys with a deprecation warning. They talk about it a bit under the documentation for `scatter::calcuateQCMetrics`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:511,security,updat,updating,511,"I think I got the docs right, let me know if there are any issues. I'm noticing some conflict with convention for metric names. I'm copying `scater`, and using labels like `total_counts` and `total_features_by_counts`. `filter_genes`, `filter_cells`, `spring_project`, and a couple of the recipes use `n_counts` or `n_cells`. My preference is for the `scater ` way, since formatting allows it to be bit more flexible. It'd be nice for there to be a consistent default key for these features. For example, while updating the clustering tutorial, I ended up with both `n_counts` and `total_counts` in the same `adata.obs`. As changing the defaults could break some code, what's the right path forward? When `scater` updated their metric names, I think they used both the old and new keys with a deprecation warning. They talk about it a bit under the documentation for `scatter::calcuateQCMetrics`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:714,security,updat,updated,714,"I think I got the docs right, let me know if there are any issues. I'm noticing some conflict with convention for metric names. I'm copying `scater`, and using labels like `total_counts` and `total_features_by_counts`. `filter_genes`, `filter_cells`, `spring_project`, and a couple of the recipes use `n_counts` or `n_cells`. My preference is for the `scater ` way, since formatting allows it to be bit more flexible. It'd be nice for there to be a consistent default key for these features. For example, while updating the clustering tutorial, I ended up with both `n_counts` and `total_counts` in the same `adata.obs`. As changing the defaults could break some code, what's the right path forward? When `scater` updated their metric names, I think they used both the old and new keys with a deprecation warning. They talk about it a bit under the documentation for `scatter::calcuateQCMetrics`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:275,testability,coupl,couple,275,"I think I got the docs right, let me know if there are any issues. I'm noticing some conflict with convention for metric names. I'm copying `scater`, and using labels like `total_counts` and `total_features_by_counts`. `filter_genes`, `filter_cells`, `spring_project`, and a couple of the recipes use `n_counts` or `n_cells`. My preference is for the `scater ` way, since formatting allows it to be bit more flexible. It'd be nice for there to be a consistent default key for these features. For example, while updating the clustering tutorial, I ended up with both `n_counts` and `total_counts` in the same `adata.obs`. As changing the defaults could break some code, what's the right path forward? When `scater` updated their metric names, I think they used both the old and new keys with a deprecation warning. They talk about it a bit under the documentation for `scatter::calcuateQCMetrics`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:329,usability,prefer,preference,329,"I think I got the docs right, let me know if there are any issues. I'm noticing some conflict with convention for metric names. I'm copying `scater`, and using labels like `total_counts` and `total_features_by_counts`. `filter_genes`, `filter_cells`, `spring_project`, and a couple of the recipes use `n_counts` or `n_cells`. My preference is for the `scater ` way, since formatting allows it to be bit more flexible. It'd be nice for there to be a consistent default key for these features. For example, while updating the clustering tutorial, I ended up with both `n_counts` and `total_counts` in the same `adata.obs`. As changing the defaults could break some code, what's the right path forward? When `scater` updated their metric names, I think they used both the old and new keys with a deprecation warning. They talk about it a bit under the documentation for `scatter::calcuateQCMetrics`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:449,usability,consist,consistent,449,"I think I got the docs right, let me know if there are any issues. I'm noticing some conflict with convention for metric names. I'm copying `scater`, and using labels like `total_counts` and `total_features_by_counts`. `filter_genes`, `filter_cells`, `spring_project`, and a couple of the recipes use `n_counts` or `n_cells`. My preference is for the `scater ` way, since formatting allows it to be bit more flexible. It'd be nice for there to be a consistent default key for these features. For example, while updating the clustering tutorial, I ended up with both `n_counts` and `total_counts` in the same `adata.obs`. As changing the defaults could break some code, what's the right path forward? When `scater` updated their metric names, I think they used both the old and new keys with a deprecation warning. They talk about it a bit under the documentation for `scatter::calcuateQCMetrics`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:849,usability,document,documentation,849,"I think I got the docs right, let me know if there are any issues. I'm noticing some conflict with convention for metric names. I'm copying `scater`, and using labels like `total_counts` and `total_features_by_counts`. `filter_genes`, `filter_cells`, `spring_project`, and a couple of the recipes use `n_counts` or `n_cells`. My preference is for the `scater ` way, since formatting allows it to be bit more flexible. It'd be nice for there to be a consistent default key for these features. For example, while updating the clustering tutorial, I ended up with both `n_counts` and `total_counts` in the same `adata.obs`. As changing the defaults could break some code, what's the right path forward? When `scater` updated their metric names, I think they used both the old and new keys with a deprecation warning. They talk about it a bit under the documentation for `scatter::calcuateQCMetrics`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:74,energy efficiency,adapt,adapt,74,"Hm, `n_counts` and `total_counts` is of course non-sense. Scanpy tries to adapt the `n_...` convention in scikit-learn and statsmodels for anything that is a number. We'll soon expose the quantile normalization preprocessing function to the users in a proper way. Then we'll have 95%-quantile counts vs. total counts. Then it starts making sense to use the notion `total_`. So, in the light of that, we could think about moving there. Yes, we'd deprecate old names and output a warning, too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:74,integrability,adapt,adapt,74,"Hm, `n_counts` and `total_counts` is of course non-sense. Scanpy tries to adapt the `n_...` convention in scikit-learn and statsmodels for anything that is a number. We'll soon expose the quantile normalization preprocessing function to the users in a proper way. Then we'll have 95%-quantile counts vs. total counts. Then it starts making sense to use the notion `total_`. So, in the light of that, we could think about moving there. Yes, we'd deprecate old names and output a warning, too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:74,interoperability,adapt,adapt,74,"Hm, `n_counts` and `total_counts` is of course non-sense. Scanpy tries to adapt the `n_...` convention in scikit-learn and statsmodels for anything that is a number. We'll soon expose the quantile normalization preprocessing function to the users in a proper way. Then we'll have 95%-quantile counts vs. total counts. Then it starts making sense to use the notion `total_`. So, in the light of that, we could think about moving there. Yes, we'd deprecate old names and output a warning, too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:74,modifiability,adapt,adapt,74,"Hm, `n_counts` and `total_counts` is of course non-sense. Scanpy tries to adapt the `n_...` convention in scikit-learn and statsmodels for anything that is a number. We'll soon expose the quantile normalization preprocessing function to the users in a proper way. Then we'll have 95%-quantile counts vs. total counts. Then it starts making sense to use the notion `total_`. So, in the light of that, we could think about moving there. Yes, we'd deprecate old names and output a warning, too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:177,security,expos,expose,177,"Hm, `n_counts` and `total_counts` is of course non-sense. Scanpy tries to adapt the `n_...` convention in scikit-learn and statsmodels for anything that is a number. We'll soon expose the quantile normalization preprocessing function to the users in a proper way. Then we'll have 95%-quantile counts vs. total counts. Then it starts making sense to use the notion `total_`. So, in the light of that, we could think about moving there. Yes, we'd deprecate old names and output a warning, too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:113,usability,learn,learn,113,"Hm, `n_counts` and `total_counts` is of course non-sense. Scanpy tries to adapt the `n_...` convention in scikit-learn and statsmodels for anything that is a number. We'll soon expose the quantile normalization preprocessing function to the users in a proper way. Then we'll have 95%-quantile counts vs. total counts. Then it starts making sense to use the notion `total_`. So, in the light of that, we could think about moving there. Yes, we'd deprecate old names and output a warning, too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:241,usability,user,users,241,"Hm, `n_counts` and `total_counts` is of course non-sense. Scanpy tries to adapt the `n_...` convention in scikit-learn and statsmodels for anything that is a number. We'll soon expose the quantile normalization preprocessing function to the users in a proper way. Then we'll have 95%-quantile counts vs. total counts. Then it starts making sense to use the notion `total_`. So, in the light of that, we could think about moving there. Yes, we'd deprecate old names and output a warning, too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:64,security,auth,author,64,"So, this is really awesome! Thank you so much! Added you to the author list just before... :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:119,availability,consist,consistent,119,"Thanks for the tutorial! Let's get most of this right now that no one has still used the function. We don't want to be consistent with Scater, we want to be consistent with the rest of Scanpy and the other python ecosystem. - Can we replace all occurances of `features` with `variables`? We had quite some discussions whether an AnnData is samples of features or observations of variables, and throughout, we stick with the latter convention. It's a very simple change. . - Can we replace `exprs_values` with `expr_type` or something more suggestive of the fact that it's just a string denoting the kind of expression values? . - Can we replace `total_features_by_counts` with `n_genes`? Why so complicated? And in contrast to `total_counts`, `features` does not suggest that it's a number, so there has to be an `n_...` before it, otherwise it completely breaks the convention. - Can we call `feature_controls`, `control_variables`, which would be a much more intuitive name? . - Why is `n_cells_by_{expr_values}` not simply `n_cells`? Am I missing something? Regarding `n_counts` versus `total_counts`, I mentioned already that I see that `total_counts` has some advantages when starting to compare with quantile counts, etc. Also, it doesn't require an `n_` as it's clear that it's a number. But for all the rest that I mentioned above, I don't see these arguments. What do you think? I really like that you use an `inplace` parameter instead of the usual `copy`, we might have exaggerated it in some places.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:157,availability,consist,consistent,157,"Thanks for the tutorial! Let's get most of this right now that no one has still used the function. We don't want to be consistent with Scater, we want to be consistent with the rest of Scanpy and the other python ecosystem. - Can we replace all occurances of `features` with `variables`? We had quite some discussions whether an AnnData is samples of features or observations of variables, and throughout, we stick with the latter convention. It's a very simple change. . - Can we replace `exprs_values` with `expr_type` or something more suggestive of the fact that it's just a string denoting the kind of expression values? . - Can we replace `total_features_by_counts` with `n_genes`? Why so complicated? And in contrast to `total_counts`, `features` does not suggest that it's a number, so there has to be an `n_...` before it, otherwise it completely breaks the convention. - Can we call `feature_controls`, `control_variables`, which would be a much more intuitive name? . - Why is `n_cells_by_{expr_values}` not simply `n_cells`? Am I missing something? Regarding `n_counts` versus `total_counts`, I mentioned already that I see that `total_counts` has some advantages when starting to compare with quantile counts, etc. Also, it doesn't require an `n_` as it's clear that it's a number. But for all the rest that I mentioned above, I don't see these arguments. What do you think? I really like that you use an `inplace` parameter instead of the usual `copy`, we might have exaggerated it in some places.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:363,deployability,observ,observations,363,"Thanks for the tutorial! Let's get most of this right now that no one has still used the function. We don't want to be consistent with Scater, we want to be consistent with the rest of Scanpy and the other python ecosystem. - Can we replace all occurances of `features` with `variables`? We had quite some discussions whether an AnnData is samples of features or observations of variables, and throughout, we stick with the latter convention. It's a very simple change. . - Can we replace `exprs_values` with `expr_type` or something more suggestive of the fact that it's just a string denoting the kind of expression values? . - Can we replace `total_features_by_counts` with `n_genes`? Why so complicated? And in contrast to `total_counts`, `features` does not suggest that it's a number, so there has to be an `n_...` before it, otherwise it completely breaks the convention. - Can we call `feature_controls`, `control_variables`, which would be a much more intuitive name? . - Why is `n_cells_by_{expr_values}` not simply `n_cells`? Am I missing something? Regarding `n_counts` versus `total_counts`, I mentioned already that I see that `total_counts` has some advantages when starting to compare with quantile counts, etc. Also, it doesn't require an `n_` as it's clear that it's a number. But for all the rest that I mentioned above, I don't see these arguments. What do you think? I really like that you use an `inplace` parameter instead of the usual `copy`, we might have exaggerated it in some places.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:276,modifiability,variab,variables,276,"Thanks for the tutorial! Let's get most of this right now that no one has still used the function. We don't want to be consistent with Scater, we want to be consistent with the rest of Scanpy and the other python ecosystem. - Can we replace all occurances of `features` with `variables`? We had quite some discussions whether an AnnData is samples of features or observations of variables, and throughout, we stick with the latter convention. It's a very simple change. . - Can we replace `exprs_values` with `expr_type` or something more suggestive of the fact that it's just a string denoting the kind of expression values? . - Can we replace `total_features_by_counts` with `n_genes`? Why so complicated? And in contrast to `total_counts`, `features` does not suggest that it's a number, so there has to be an `n_...` before it, otherwise it completely breaks the convention. - Can we call `feature_controls`, `control_variables`, which would be a much more intuitive name? . - Why is `n_cells_by_{expr_values}` not simply `n_cells`? Am I missing something? Regarding `n_counts` versus `total_counts`, I mentioned already that I see that `total_counts` has some advantages when starting to compare with quantile counts, etc. Also, it doesn't require an `n_` as it's clear that it's a number. But for all the rest that I mentioned above, I don't see these arguments. What do you think? I really like that you use an `inplace` parameter instead of the usual `copy`, we might have exaggerated it in some places.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:379,modifiability,variab,variables,379,"Thanks for the tutorial! Let's get most of this right now that no one has still used the function. We don't want to be consistent with Scater, we want to be consistent with the rest of Scanpy and the other python ecosystem. - Can we replace all occurances of `features` with `variables`? We had quite some discussions whether an AnnData is samples of features or observations of variables, and throughout, we stick with the latter convention. It's a very simple change. . - Can we replace `exprs_values` with `expr_type` or something more suggestive of the fact that it's just a string denoting the kind of expression values? . - Can we replace `total_features_by_counts` with `n_genes`? Why so complicated? And in contrast to `total_counts`, `features` does not suggest that it's a number, so there has to be an `n_...` before it, otherwise it completely breaks the convention. - Can we call `feature_controls`, `control_variables`, which would be a much more intuitive name? . - Why is `n_cells_by_{expr_values}` not simply `n_cells`? Am I missing something? Regarding `n_counts` versus `total_counts`, I mentioned already that I see that `total_counts` has some advantages when starting to compare with quantile counts, etc. Also, it doesn't require an `n_` as it's clear that it's a number. But for all the rest that I mentioned above, I don't see these arguments. What do you think? I really like that you use an `inplace` parameter instead of the usual `copy`, we might have exaggerated it in some places.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:1428,modifiability,paramet,parameter,1428,"Thanks for the tutorial! Let's get most of this right now that no one has still used the function. We don't want to be consistent with Scater, we want to be consistent with the rest of Scanpy and the other python ecosystem. - Can we replace all occurances of `features` with `variables`? We had quite some discussions whether an AnnData is samples of features or observations of variables, and throughout, we stick with the latter convention. It's a very simple change. . - Can we replace `exprs_values` with `expr_type` or something more suggestive of the fact that it's just a string denoting the kind of expression values? . - Can we replace `total_features_by_counts` with `n_genes`? Why so complicated? And in contrast to `total_counts`, `features` does not suggest that it's a number, so there has to be an `n_...` before it, otherwise it completely breaks the convention. - Can we call `feature_controls`, `control_variables`, which would be a much more intuitive name? . - Why is `n_cells_by_{expr_values}` not simply `n_cells`? Am I missing something? Regarding `n_counts` versus `total_counts`, I mentioned already that I see that `total_counts` has some advantages when starting to compare with quantile counts, etc. Also, it doesn't require an `n_` as it's clear that it's a number. But for all the rest that I mentioned above, I don't see these arguments. What do you think? I really like that you use an `inplace` parameter instead of the usual `copy`, we might have exaggerated it in some places.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:754,reliability,doe,does,754,"Thanks for the tutorial! Let's get most of this right now that no one has still used the function. We don't want to be consistent with Scater, we want to be consistent with the rest of Scanpy and the other python ecosystem. - Can we replace all occurances of `features` with `variables`? We had quite some discussions whether an AnnData is samples of features or observations of variables, and throughout, we stick with the latter convention. It's a very simple change. . - Can we replace `exprs_values` with `expr_type` or something more suggestive of the fact that it's just a string denoting the kind of expression values? . - Can we replace `total_features_by_counts` with `n_genes`? Why so complicated? And in contrast to `total_counts`, `features` does not suggest that it's a number, so there has to be an `n_...` before it, otherwise it completely breaks the convention. - Can we call `feature_controls`, `control_variables`, which would be a much more intuitive name? . - Why is `n_cells_by_{expr_values}` not simply `n_cells`? Am I missing something? Regarding `n_counts` versus `total_counts`, I mentioned already that I see that `total_counts` has some advantages when starting to compare with quantile counts, etc. Also, it doesn't require an `n_` as it's clear that it's a number. But for all the rest that I mentioned above, I don't see these arguments. What do you think? I really like that you use an `inplace` parameter instead of the usual `copy`, we might have exaggerated it in some places.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:1237,reliability,doe,doesn,1237,"Thanks for the tutorial! Let's get most of this right now that no one has still used the function. We don't want to be consistent with Scater, we want to be consistent with the rest of Scanpy and the other python ecosystem. - Can we replace all occurances of `features` with `variables`? We had quite some discussions whether an AnnData is samples of features or observations of variables, and throughout, we stick with the latter convention. It's a very simple change. . - Can we replace `exprs_values` with `expr_type` or something more suggestive of the fact that it's just a string denoting the kind of expression values? . - Can we replace `total_features_by_counts` with `n_genes`? Why so complicated? And in contrast to `total_counts`, `features` does not suggest that it's a number, so there has to be an `n_...` before it, otherwise it completely breaks the convention. - Can we call `feature_controls`, `control_variables`, which would be a much more intuitive name? . - Why is `n_cells_by_{expr_values}` not simply `n_cells`? Am I missing something? Regarding `n_counts` versus `total_counts`, I mentioned already that I see that `total_counts` has some advantages when starting to compare with quantile counts, etc. Also, it doesn't require an `n_` as it's clear that it's a number. But for all the rest that I mentioned above, I don't see these arguments. What do you think? I really like that you use an `inplace` parameter instead of the usual `copy`, we might have exaggerated it in some places.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:695,safety,compl,complicated,695,"Thanks for the tutorial! Let's get most of this right now that no one has still used the function. We don't want to be consistent with Scater, we want to be consistent with the rest of Scanpy and the other python ecosystem. - Can we replace all occurances of `features` with `variables`? We had quite some discussions whether an AnnData is samples of features or observations of variables, and throughout, we stick with the latter convention. It's a very simple change. . - Can we replace `exprs_values` with `expr_type` or something more suggestive of the fact that it's just a string denoting the kind of expression values? . - Can we replace `total_features_by_counts` with `n_genes`? Why so complicated? And in contrast to `total_counts`, `features` does not suggest that it's a number, so there has to be an `n_...` before it, otherwise it completely breaks the convention. - Can we call `feature_controls`, `control_variables`, which would be a much more intuitive name? . - Why is `n_cells_by_{expr_values}` not simply `n_cells`? Am I missing something? Regarding `n_counts` versus `total_counts`, I mentioned already that I see that `total_counts` has some advantages when starting to compare with quantile counts, etc. Also, it doesn't require an `n_` as it's clear that it's a number. But for all the rest that I mentioned above, I don't see these arguments. What do you think? I really like that you use an `inplace` parameter instead of the usual `copy`, we might have exaggerated it in some places.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:845,safety,compl,completely,845,"Thanks for the tutorial! Let's get most of this right now that no one has still used the function. We don't want to be consistent with Scater, we want to be consistent with the rest of Scanpy and the other python ecosystem. - Can we replace all occurances of `features` with `variables`? We had quite some discussions whether an AnnData is samples of features or observations of variables, and throughout, we stick with the latter convention. It's a very simple change. . - Can we replace `exprs_values` with `expr_type` or something more suggestive of the fact that it's just a string denoting the kind of expression values? . - Can we replace `total_features_by_counts` with `n_genes`? Why so complicated? And in contrast to `total_counts`, `features` does not suggest that it's a number, so there has to be an `n_...` before it, otherwise it completely breaks the convention. - Can we call `feature_controls`, `control_variables`, which would be a much more intuitive name? . - Why is `n_cells_by_{expr_values}` not simply `n_cells`? Am I missing something? Regarding `n_counts` versus `total_counts`, I mentioned already that I see that `total_counts` has some advantages when starting to compare with quantile counts, etc. Also, it doesn't require an `n_` as it's clear that it's a number. But for all the rest that I mentioned above, I don't see these arguments. What do you think? I really like that you use an `inplace` parameter instead of the usual `copy`, we might have exaggerated it in some places.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:695,security,compl,complicated,695,"Thanks for the tutorial! Let's get most of this right now that no one has still used the function. We don't want to be consistent with Scater, we want to be consistent with the rest of Scanpy and the other python ecosystem. - Can we replace all occurances of `features` with `variables`? We had quite some discussions whether an AnnData is samples of features or observations of variables, and throughout, we stick with the latter convention. It's a very simple change. . - Can we replace `exprs_values` with `expr_type` or something more suggestive of the fact that it's just a string denoting the kind of expression values? . - Can we replace `total_features_by_counts` with `n_genes`? Why so complicated? And in contrast to `total_counts`, `features` does not suggest that it's a number, so there has to be an `n_...` before it, otherwise it completely breaks the convention. - Can we call `feature_controls`, `control_variables`, which would be a much more intuitive name? . - Why is `n_cells_by_{expr_values}` not simply `n_cells`? Am I missing something? Regarding `n_counts` versus `total_counts`, I mentioned already that I see that `total_counts` has some advantages when starting to compare with quantile counts, etc. Also, it doesn't require an `n_` as it's clear that it's a number. But for all the rest that I mentioned above, I don't see these arguments. What do you think? I really like that you use an `inplace` parameter instead of the usual `copy`, we might have exaggerated it in some places.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:845,security,compl,completely,845,"Thanks for the tutorial! Let's get most of this right now that no one has still used the function. We don't want to be consistent with Scater, we want to be consistent with the rest of Scanpy and the other python ecosystem. - Can we replace all occurances of `features` with `variables`? We had quite some discussions whether an AnnData is samples of features or observations of variables, and throughout, we stick with the latter convention. It's a very simple change. . - Can we replace `exprs_values` with `expr_type` or something more suggestive of the fact that it's just a string denoting the kind of expression values? . - Can we replace `total_features_by_counts` with `n_genes`? Why so complicated? And in contrast to `total_counts`, `features` does not suggest that it's a number, so there has to be an `n_...` before it, otherwise it completely breaks the convention. - Can we call `feature_controls`, `control_variables`, which would be a much more intuitive name? . - Why is `n_cells_by_{expr_values}` not simply `n_cells`? Am I missing something? Regarding `n_counts` versus `total_counts`, I mentioned already that I see that `total_counts` has some advantages when starting to compare with quantile counts, etc. Also, it doesn't require an `n_` as it's clear that it's a number. But for all the rest that I mentioned above, I don't see these arguments. What do you think? I really like that you use an `inplace` parameter instead of the usual `copy`, we might have exaggerated it in some places.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:363,testability,observ,observations,363,"Thanks for the tutorial! Let's get most of this right now that no one has still used the function. We don't want to be consistent with Scater, we want to be consistent with the rest of Scanpy and the other python ecosystem. - Can we replace all occurances of `features` with `variables`? We had quite some discussions whether an AnnData is samples of features or observations of variables, and throughout, we stick with the latter convention. It's a very simple change. . - Can we replace `exprs_values` with `expr_type` or something more suggestive of the fact that it's just a string denoting the kind of expression values? . - Can we replace `total_features_by_counts` with `n_genes`? Why so complicated? And in contrast to `total_counts`, `features` does not suggest that it's a number, so there has to be an `n_...` before it, otherwise it completely breaks the convention. - Can we call `feature_controls`, `control_variables`, which would be a much more intuitive name? . - Why is `n_cells_by_{expr_values}` not simply `n_cells`? Am I missing something? Regarding `n_counts` versus `total_counts`, I mentioned already that I see that `total_counts` has some advantages when starting to compare with quantile counts, etc. Also, it doesn't require an `n_` as it's clear that it's a number. But for all the rest that I mentioned above, I don't see these arguments. What do you think? I really like that you use an `inplace` parameter instead of the usual `copy`, we might have exaggerated it in some places.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:455,testability,simpl,simple,455,"Thanks for the tutorial! Let's get most of this right now that no one has still used the function. We don't want to be consistent with Scater, we want to be consistent with the rest of Scanpy and the other python ecosystem. - Can we replace all occurances of `features` with `variables`? We had quite some discussions whether an AnnData is samples of features or observations of variables, and throughout, we stick with the latter convention. It's a very simple change. . - Can we replace `exprs_values` with `expr_type` or something more suggestive of the fact that it's just a string denoting the kind of expression values? . - Can we replace `total_features_by_counts` with `n_genes`? Why so complicated? And in contrast to `total_counts`, `features` does not suggest that it's a number, so there has to be an `n_...` before it, otherwise it completely breaks the convention. - Can we call `feature_controls`, `control_variables`, which would be a much more intuitive name? . - Why is `n_cells_by_{expr_values}` not simply `n_cells`? Am I missing something? Regarding `n_counts` versus `total_counts`, I mentioned already that I see that `total_counts` has some advantages when starting to compare with quantile counts, etc. Also, it doesn't require an `n_` as it's clear that it's a number. But for all the rest that I mentioned above, I don't see these arguments. What do you think? I really like that you use an `inplace` parameter instead of the usual `copy`, we might have exaggerated it in some places.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:1019,testability,simpl,simply,1019,"Thanks for the tutorial! Let's get most of this right now that no one has still used the function. We don't want to be consistent with Scater, we want to be consistent with the rest of Scanpy and the other python ecosystem. - Can we replace all occurances of `features` with `variables`? We had quite some discussions whether an AnnData is samples of features or observations of variables, and throughout, we stick with the latter convention. It's a very simple change. . - Can we replace `exprs_values` with `expr_type` or something more suggestive of the fact that it's just a string denoting the kind of expression values? . - Can we replace `total_features_by_counts` with `n_genes`? Why so complicated? And in contrast to `total_counts`, `features` does not suggest that it's a number, so there has to be an `n_...` before it, otherwise it completely breaks the convention. - Can we call `feature_controls`, `control_variables`, which would be a much more intuitive name? . - Why is `n_cells_by_{expr_values}` not simply `n_cells`? Am I missing something? Regarding `n_counts` versus `total_counts`, I mentioned already that I see that `total_counts` has some advantages when starting to compare with quantile counts, etc. Also, it doesn't require an `n_` as it's clear that it's a number. But for all the rest that I mentioned above, I don't see these arguments. What do you think? I really like that you use an `inplace` parameter instead of the usual `copy`, we might have exaggerated it in some places.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:119,usability,consist,consistent,119,"Thanks for the tutorial! Let's get most of this right now that no one has still used the function. We don't want to be consistent with Scater, we want to be consistent with the rest of Scanpy and the other python ecosystem. - Can we replace all occurances of `features` with `variables`? We had quite some discussions whether an AnnData is samples of features or observations of variables, and throughout, we stick with the latter convention. It's a very simple change. . - Can we replace `exprs_values` with `expr_type` or something more suggestive of the fact that it's just a string denoting the kind of expression values? . - Can we replace `total_features_by_counts` with `n_genes`? Why so complicated? And in contrast to `total_counts`, `features` does not suggest that it's a number, so there has to be an `n_...` before it, otherwise it completely breaks the convention. - Can we call `feature_controls`, `control_variables`, which would be a much more intuitive name? . - Why is `n_cells_by_{expr_values}` not simply `n_cells`? Am I missing something? Regarding `n_counts` versus `total_counts`, I mentioned already that I see that `total_counts` has some advantages when starting to compare with quantile counts, etc. Also, it doesn't require an `n_` as it's clear that it's a number. But for all the rest that I mentioned above, I don't see these arguments. What do you think? I really like that you use an `inplace` parameter instead of the usual `copy`, we might have exaggerated it in some places.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:157,usability,consist,consistent,157,"Thanks for the tutorial! Let's get most of this right now that no one has still used the function. We don't want to be consistent with Scater, we want to be consistent with the rest of Scanpy and the other python ecosystem. - Can we replace all occurances of `features` with `variables`? We had quite some discussions whether an AnnData is samples of features or observations of variables, and throughout, we stick with the latter convention. It's a very simple change. . - Can we replace `exprs_values` with `expr_type` or something more suggestive of the fact that it's just a string denoting the kind of expression values? . - Can we replace `total_features_by_counts` with `n_genes`? Why so complicated? And in contrast to `total_counts`, `features` does not suggest that it's a number, so there has to be an `n_...` before it, otherwise it completely breaks the convention. - Can we call `feature_controls`, `control_variables`, which would be a much more intuitive name? . - Why is `n_cells_by_{expr_values}` not simply `n_cells`? Am I missing something? Regarding `n_counts` versus `total_counts`, I mentioned already that I see that `total_counts` has some advantages when starting to compare with quantile counts, etc. Also, it doesn't require an `n_` as it's clear that it's a number. But for all the rest that I mentioned above, I don't see these arguments. What do you think? I really like that you use an `inplace` parameter instead of the usual `copy`, we might have exaggerated it in some places.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:455,usability,simpl,simple,455,"Thanks for the tutorial! Let's get most of this right now that no one has still used the function. We don't want to be consistent with Scater, we want to be consistent with the rest of Scanpy and the other python ecosystem. - Can we replace all occurances of `features` with `variables`? We had quite some discussions whether an AnnData is samples of features or observations of variables, and throughout, we stick with the latter convention. It's a very simple change. . - Can we replace `exprs_values` with `expr_type` or something more suggestive of the fact that it's just a string denoting the kind of expression values? . - Can we replace `total_features_by_counts` with `n_genes`? Why so complicated? And in contrast to `total_counts`, `features` does not suggest that it's a number, so there has to be an `n_...` before it, otherwise it completely breaks the convention. - Can we call `feature_controls`, `control_variables`, which would be a much more intuitive name? . - Why is `n_cells_by_{expr_values}` not simply `n_cells`? Am I missing something? Regarding `n_counts` versus `total_counts`, I mentioned already that I see that `total_counts` has some advantages when starting to compare with quantile counts, etc. Also, it doesn't require an `n_` as it's clear that it's a number. But for all the rest that I mentioned above, I don't see these arguments. What do you think? I really like that you use an `inplace` parameter instead of the usual `copy`, we might have exaggerated it in some places.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:961,usability,intuit,intuitive,961,"Thanks for the tutorial! Let's get most of this right now that no one has still used the function. We don't want to be consistent with Scater, we want to be consistent with the rest of Scanpy and the other python ecosystem. - Can we replace all occurances of `features` with `variables`? We had quite some discussions whether an AnnData is samples of features or observations of variables, and throughout, we stick with the latter convention. It's a very simple change. . - Can we replace `exprs_values` with `expr_type` or something more suggestive of the fact that it's just a string denoting the kind of expression values? . - Can we replace `total_features_by_counts` with `n_genes`? Why so complicated? And in contrast to `total_counts`, `features` does not suggest that it's a number, so there has to be an `n_...` before it, otherwise it completely breaks the convention. - Can we call `feature_controls`, `control_variables`, which would be a much more intuitive name? . - Why is `n_cells_by_{expr_values}` not simply `n_cells`? Am I missing something? Regarding `n_counts` versus `total_counts`, I mentioned already that I see that `total_counts` has some advantages when starting to compare with quantile counts, etc. Also, it doesn't require an `n_` as it's clear that it's a number. But for all the rest that I mentioned above, I don't see these arguments. What do you think? I really like that you use an `inplace` parameter instead of the usual `copy`, we might have exaggerated it in some places.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:1019,usability,simpl,simply,1019,"Thanks for the tutorial! Let's get most of this right now that no one has still used the function. We don't want to be consistent with Scater, we want to be consistent with the rest of Scanpy and the other python ecosystem. - Can we replace all occurances of `features` with `variables`? We had quite some discussions whether an AnnData is samples of features or observations of variables, and throughout, we stick with the latter convention. It's a very simple change. . - Can we replace `exprs_values` with `expr_type` or something more suggestive of the fact that it's just a string denoting the kind of expression values? . - Can we replace `total_features_by_counts` with `n_genes`? Why so complicated? And in contrast to `total_counts`, `features` does not suggest that it's a number, so there has to be an `n_...` before it, otherwise it completely breaks the convention. - Can we call `feature_controls`, `control_variables`, which would be a much more intuitive name? . - Why is `n_cells_by_{expr_values}` not simply `n_cells`? Am I missing something? Regarding `n_counts` versus `total_counts`, I mentioned already that I see that `total_counts` has some advantages when starting to compare with quantile counts, etc. Also, it doesn't require an `n_` as it's clear that it's a number. But for all the rest that I mentioned above, I don't see these arguments. What do you think? I really like that you use an `inplace` parameter instead of the usual `copy`, we might have exaggerated it in some places.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:1269,usability,clear,clear,1269,"Thanks for the tutorial! Let's get most of this right now that no one has still used the function. We don't want to be consistent with Scater, we want to be consistent with the rest of Scanpy and the other python ecosystem. - Can we replace all occurances of `features` with `variables`? We had quite some discussions whether an AnnData is samples of features or observations of variables, and throughout, we stick with the latter convention. It's a very simple change. . - Can we replace `exprs_values` with `expr_type` or something more suggestive of the fact that it's just a string denoting the kind of expression values? . - Can we replace `total_features_by_counts` with `n_genes`? Why so complicated? And in contrast to `total_counts`, `features` does not suggest that it's a number, so there has to be an `n_...` before it, otherwise it completely breaks the convention. - Can we call `feature_controls`, `control_variables`, which would be a much more intuitive name? . - Why is `n_cells_by_{expr_values}` not simply `n_cells`? Am I missing something? Regarding `n_counts` versus `total_counts`, I mentioned already that I see that `total_counts` has some advantages when starting to compare with quantile counts, etc. Also, it doesn't require an `n_` as it's clear that it's a number. But for all the rest that I mentioned above, I don't see these arguments. What do you think? I really like that you use an `inplace` parameter instead of the usual `copy`, we might have exaggerated it in some places.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:1267,availability,consist,consistency,1267,"No problem! * `features` sounds more natural to me, but `variables` is fine. Maybe we could do `vars` instead of `variables` for reduced verbosity? * `expr_type` would work. Maybe `vars_type`? * How about `n_genes_by_{exprs_type/vars_type}`? `n` works great for this, since it's integer valued. I might like `vars` over `genes` since the variables could be transcripts or surface markers, but I'm not sure on this. I like the `by_{vars_type}` convention for a couple reasons, which also apply to your last point:. * It allows recording at multiple steps in the process. You could imagine: `n_{vars/genes}_by_counts` and `n_{vars/genes}_by_imputed_counts` or `n_{vars/genes}_by_normed_expression`. * The convention allows for multi-omic measurements on a gene, `n_{vars/genes}_by_fluorescence` for example. This is a case where `genes` makes more sense than `vars`. * `control_variables` does sound more natural. I'd possibly like to replace `control` as well, since these aren't necessarily controlled variables. * Largely similar thoughts as the third point, e.g. * Recording at multiple steps: `n_cells_by_counts` and `n_cells_by_imputed_counts`. * Multi-omic measurements: `n_cells_by_fluorescence`. I think `total` can be more widely used than `n`, allowing more consistency. To me, `total_cells` or `total_vars` make sense while `n_fluorescence` or `n_log_counts` don't. It's also totally fine to have a mix. Yeah, I figured I didn't want to make a whole copy of the object if I didn't want update or add all the metrics. About places in the codebase where naming would need to change, I'd argue the default shouldn't be to use a pre-computed value. I hadn't realized that `n_counts` fields were being stored or used until I started looking around. Since summing over a matrix is likely a pretty light computation compared to what follows, I don't think there's a strong performance argument for keeping it as the default.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:1496,deployability,updat,update,1496,"No problem! * `features` sounds more natural to me, but `variables` is fine. Maybe we could do `vars` instead of `variables` for reduced verbosity? * `expr_type` would work. Maybe `vars_type`? * How about `n_genes_by_{exprs_type/vars_type}`? `n` works great for this, since it's integer valued. I might like `vars` over `genes` since the variables could be transcripts or surface markers, but I'm not sure on this. I like the `by_{vars_type}` convention for a couple reasons, which also apply to your last point:. * It allows recording at multiple steps in the process. You could imagine: `n_{vars/genes}_by_counts` and `n_{vars/genes}_by_imputed_counts` or `n_{vars/genes}_by_normed_expression`. * The convention allows for multi-omic measurements on a gene, `n_{vars/genes}_by_fluorescence` for example. This is a case where `genes` makes more sense than `vars`. * `control_variables` does sound more natural. I'd possibly like to replace `control` as well, since these aren't necessarily controlled variables. * Largely similar thoughts as the third point, e.g. * Recording at multiple steps: `n_cells_by_counts` and `n_cells_by_imputed_counts`. * Multi-omic measurements: `n_cells_by_fluorescence`. I think `total` can be more widely used than `n`, allowing more consistency. To me, `total_cells` or `total_vars` make sense while `n_fluorescence` or `n_log_counts` don't. It's also totally fine to have a mix. Yeah, I figured I didn't want to make a whole copy of the object if I didn't want update or add all the metrics. About places in the codebase where naming would need to change, I'd argue the default shouldn't be to use a pre-computed value. I hadn't realized that `n_counts` fields were being stored or used until I started looking around. Since summing over a matrix is likely a pretty light computation compared to what follows, I don't think there's a strong performance argument for keeping it as the default.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:129,energy efficiency,reduc,reduced,129,"No problem! * `features` sounds more natural to me, but `variables` is fine. Maybe we could do `vars` instead of `variables` for reduced verbosity? * `expr_type` would work. Maybe `vars_type`? * How about `n_genes_by_{exprs_type/vars_type}`? `n` works great for this, since it's integer valued. I might like `vars` over `genes` since the variables could be transcripts or surface markers, but I'm not sure on this. I like the `by_{vars_type}` convention for a couple reasons, which also apply to your last point:. * It allows recording at multiple steps in the process. You could imagine: `n_{vars/genes}_by_counts` and `n_{vars/genes}_by_imputed_counts` or `n_{vars/genes}_by_normed_expression`. * The convention allows for multi-omic measurements on a gene, `n_{vars/genes}_by_fluorescence` for example. This is a case where `genes` makes more sense than `vars`. * `control_variables` does sound more natural. I'd possibly like to replace `control` as well, since these aren't necessarily controlled variables. * Largely similar thoughts as the third point, e.g. * Recording at multiple steps: `n_cells_by_counts` and `n_cells_by_imputed_counts`. * Multi-omic measurements: `n_cells_by_fluorescence`. I think `total` can be more widely used than `n`, allowing more consistency. To me, `total_cells` or `total_vars` make sense while `n_fluorescence` or `n_log_counts` don't. It's also totally fine to have a mix. Yeah, I figured I didn't want to make a whole copy of the object if I didn't want update or add all the metrics. About places in the codebase where naming would need to change, I'd argue the default shouldn't be to use a pre-computed value. I hadn't realized that `n_counts` fields were being stored or used until I started looking around. Since summing over a matrix is likely a pretty light computation compared to what follows, I don't think there's a strong performance argument for keeping it as the default.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:736,energy efficiency,measur,measurements,736,"No problem! * `features` sounds more natural to me, but `variables` is fine. Maybe we could do `vars` instead of `variables` for reduced verbosity? * `expr_type` would work. Maybe `vars_type`? * How about `n_genes_by_{exprs_type/vars_type}`? `n` works great for this, since it's integer valued. I might like `vars` over `genes` since the variables could be transcripts or surface markers, but I'm not sure on this. I like the `by_{vars_type}` convention for a couple reasons, which also apply to your last point:. * It allows recording at multiple steps in the process. You could imagine: `n_{vars/genes}_by_counts` and `n_{vars/genes}_by_imputed_counts` or `n_{vars/genes}_by_normed_expression`. * The convention allows for multi-omic measurements on a gene, `n_{vars/genes}_by_fluorescence` for example. This is a case where `genes` makes more sense than `vars`. * `control_variables` does sound more natural. I'd possibly like to replace `control` as well, since these aren't necessarily controlled variables. * Largely similar thoughts as the third point, e.g. * Recording at multiple steps: `n_cells_by_counts` and `n_cells_by_imputed_counts`. * Multi-omic measurements: `n_cells_by_fluorescence`. I think `total` can be more widely used than `n`, allowing more consistency. To me, `total_cells` or `total_vars` make sense while `n_fluorescence` or `n_log_counts` don't. It's also totally fine to have a mix. Yeah, I figured I didn't want to make a whole copy of the object if I didn't want update or add all the metrics. About places in the codebase where naming would need to change, I'd argue the default shouldn't be to use a pre-computed value. I hadn't realized that `n_counts` fields were being stored or used until I started looking around. Since summing over a matrix is likely a pretty light computation compared to what follows, I don't think there's a strong performance argument for keeping it as the default.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:1162,energy efficiency,measur,measurements,1162,"No problem! * `features` sounds more natural to me, but `variables` is fine. Maybe we could do `vars` instead of `variables` for reduced verbosity? * `expr_type` would work. Maybe `vars_type`? * How about `n_genes_by_{exprs_type/vars_type}`? `n` works great for this, since it's integer valued. I might like `vars` over `genes` since the variables could be transcripts or surface markers, but I'm not sure on this. I like the `by_{vars_type}` convention for a couple reasons, which also apply to your last point:. * It allows recording at multiple steps in the process. You could imagine: `n_{vars/genes}_by_counts` and `n_{vars/genes}_by_imputed_counts` or `n_{vars/genes}_by_normed_expression`. * The convention allows for multi-omic measurements on a gene, `n_{vars/genes}_by_fluorescence` for example. This is a case where `genes` makes more sense than `vars`. * `control_variables` does sound more natural. I'd possibly like to replace `control` as well, since these aren't necessarily controlled variables. * Largely similar thoughts as the third point, e.g. * Recording at multiple steps: `n_cells_by_counts` and `n_cells_by_imputed_counts`. * Multi-omic measurements: `n_cells_by_fluorescence`. I think `total` can be more widely used than `n`, allowing more consistency. To me, `total_cells` or `total_vars` make sense while `n_fluorescence` or `n_log_counts` don't. It's also totally fine to have a mix. Yeah, I figured I didn't want to make a whole copy of the object if I didn't want update or add all the metrics. About places in the codebase where naming would need to change, I'd argue the default shouldn't be to use a pre-computed value. I hadn't realized that `n_counts` fields were being stored or used until I started looking around. Since summing over a matrix is likely a pretty light computation compared to what follows, I don't think there's a strong performance argument for keeping it as the default.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:460,integrability,coupl,couple,460,"No problem! * `features` sounds more natural to me, but `variables` is fine. Maybe we could do `vars` instead of `variables` for reduced verbosity? * `expr_type` would work. Maybe `vars_type`? * How about `n_genes_by_{exprs_type/vars_type}`? `n` works great for this, since it's integer valued. I might like `vars` over `genes` since the variables could be transcripts or surface markers, but I'm not sure on this. I like the `by_{vars_type}` convention for a couple reasons, which also apply to your last point:. * It allows recording at multiple steps in the process. You could imagine: `n_{vars/genes}_by_counts` and `n_{vars/genes}_by_imputed_counts` or `n_{vars/genes}_by_normed_expression`. * The convention allows for multi-omic measurements on a gene, `n_{vars/genes}_by_fluorescence` for example. This is a case where `genes` makes more sense than `vars`. * `control_variables` does sound more natural. I'd possibly like to replace `control` as well, since these aren't necessarily controlled variables. * Largely similar thoughts as the third point, e.g. * Recording at multiple steps: `n_cells_by_counts` and `n_cells_by_imputed_counts`. * Multi-omic measurements: `n_cells_by_fluorescence`. I think `total` can be more widely used than `n`, allowing more consistency. To me, `total_cells` or `total_vars` make sense while `n_fluorescence` or `n_log_counts` don't. It's also totally fine to have a mix. Yeah, I figured I didn't want to make a whole copy of the object if I didn't want update or add all the metrics. About places in the codebase where naming would need to change, I'd argue the default shouldn't be to use a pre-computed value. I hadn't realized that `n_counts` fields were being stored or used until I started looking around. Since summing over a matrix is likely a pretty light computation compared to what follows, I don't think there's a strong performance argument for keeping it as the default.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:57,modifiability,variab,variables,57,"No problem! * `features` sounds more natural to me, but `variables` is fine. Maybe we could do `vars` instead of `variables` for reduced verbosity? * `expr_type` would work. Maybe `vars_type`? * How about `n_genes_by_{exprs_type/vars_type}`? `n` works great for this, since it's integer valued. I might like `vars` over `genes` since the variables could be transcripts or surface markers, but I'm not sure on this. I like the `by_{vars_type}` convention for a couple reasons, which also apply to your last point:. * It allows recording at multiple steps in the process. You could imagine: `n_{vars/genes}_by_counts` and `n_{vars/genes}_by_imputed_counts` or `n_{vars/genes}_by_normed_expression`. * The convention allows for multi-omic measurements on a gene, `n_{vars/genes}_by_fluorescence` for example. This is a case where `genes` makes more sense than `vars`. * `control_variables` does sound more natural. I'd possibly like to replace `control` as well, since these aren't necessarily controlled variables. * Largely similar thoughts as the third point, e.g. * Recording at multiple steps: `n_cells_by_counts` and `n_cells_by_imputed_counts`. * Multi-omic measurements: `n_cells_by_fluorescence`. I think `total` can be more widely used than `n`, allowing more consistency. To me, `total_cells` or `total_vars` make sense while `n_fluorescence` or `n_log_counts` don't. It's also totally fine to have a mix. Yeah, I figured I didn't want to make a whole copy of the object if I didn't want update or add all the metrics. About places in the codebase where naming would need to change, I'd argue the default shouldn't be to use a pre-computed value. I hadn't realized that `n_counts` fields were being stored or used until I started looking around. Since summing over a matrix is likely a pretty light computation compared to what follows, I don't think there's a strong performance argument for keeping it as the default.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:114,modifiability,variab,variables,114,"No problem! * `features` sounds more natural to me, but `variables` is fine. Maybe we could do `vars` instead of `variables` for reduced verbosity? * `expr_type` would work. Maybe `vars_type`? * How about `n_genes_by_{exprs_type/vars_type}`? `n` works great for this, since it's integer valued. I might like `vars` over `genes` since the variables could be transcripts or surface markers, but I'm not sure on this. I like the `by_{vars_type}` convention for a couple reasons, which also apply to your last point:. * It allows recording at multiple steps in the process. You could imagine: `n_{vars/genes}_by_counts` and `n_{vars/genes}_by_imputed_counts` or `n_{vars/genes}_by_normed_expression`. * The convention allows for multi-omic measurements on a gene, `n_{vars/genes}_by_fluorescence` for example. This is a case where `genes` makes more sense than `vars`. * `control_variables` does sound more natural. I'd possibly like to replace `control` as well, since these aren't necessarily controlled variables. * Largely similar thoughts as the third point, e.g. * Recording at multiple steps: `n_cells_by_counts` and `n_cells_by_imputed_counts`. * Multi-omic measurements: `n_cells_by_fluorescence`. I think `total` can be more widely used than `n`, allowing more consistency. To me, `total_cells` or `total_vars` make sense while `n_fluorescence` or `n_log_counts` don't. It's also totally fine to have a mix. Yeah, I figured I didn't want to make a whole copy of the object if I didn't want update or add all the metrics. About places in the codebase where naming would need to change, I'd argue the default shouldn't be to use a pre-computed value. I hadn't realized that `n_counts` fields were being stored or used until I started looking around. Since summing over a matrix is likely a pretty light computation compared to what follows, I don't think there's a strong performance argument for keeping it as the default.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:338,modifiability,variab,variables,338,"No problem! * `features` sounds more natural to me, but `variables` is fine. Maybe we could do `vars` instead of `variables` for reduced verbosity? * `expr_type` would work. Maybe `vars_type`? * How about `n_genes_by_{exprs_type/vars_type}`? `n` works great for this, since it's integer valued. I might like `vars` over `genes` since the variables could be transcripts or surface markers, but I'm not sure on this. I like the `by_{vars_type}` convention for a couple reasons, which also apply to your last point:. * It allows recording at multiple steps in the process. You could imagine: `n_{vars/genes}_by_counts` and `n_{vars/genes}_by_imputed_counts` or `n_{vars/genes}_by_normed_expression`. * The convention allows for multi-omic measurements on a gene, `n_{vars/genes}_by_fluorescence` for example. This is a case where `genes` makes more sense than `vars`. * `control_variables` does sound more natural. I'd possibly like to replace `control` as well, since these aren't necessarily controlled variables. * Largely similar thoughts as the third point, e.g. * Recording at multiple steps: `n_cells_by_counts` and `n_cells_by_imputed_counts`. * Multi-omic measurements: `n_cells_by_fluorescence`. I think `total` can be more widely used than `n`, allowing more consistency. To me, `total_cells` or `total_vars` make sense while `n_fluorescence` or `n_log_counts` don't. It's also totally fine to have a mix. Yeah, I figured I didn't want to make a whole copy of the object if I didn't want update or add all the metrics. About places in the codebase where naming would need to change, I'd argue the default shouldn't be to use a pre-computed value. I hadn't realized that `n_counts` fields were being stored or used until I started looking around. Since summing over a matrix is likely a pretty light computation compared to what follows, I don't think there's a strong performance argument for keeping it as the default.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:460,modifiability,coupl,couple,460,"No problem! * `features` sounds more natural to me, but `variables` is fine. Maybe we could do `vars` instead of `variables` for reduced verbosity? * `expr_type` would work. Maybe `vars_type`? * How about `n_genes_by_{exprs_type/vars_type}`? `n` works great for this, since it's integer valued. I might like `vars` over `genes` since the variables could be transcripts or surface markers, but I'm not sure on this. I like the `by_{vars_type}` convention for a couple reasons, which also apply to your last point:. * It allows recording at multiple steps in the process. You could imagine: `n_{vars/genes}_by_counts` and `n_{vars/genes}_by_imputed_counts` or `n_{vars/genes}_by_normed_expression`. * The convention allows for multi-omic measurements on a gene, `n_{vars/genes}_by_fluorescence` for example. This is a case where `genes` makes more sense than `vars`. * `control_variables` does sound more natural. I'd possibly like to replace `control` as well, since these aren't necessarily controlled variables. * Largely similar thoughts as the third point, e.g. * Recording at multiple steps: `n_cells_by_counts` and `n_cells_by_imputed_counts`. * Multi-omic measurements: `n_cells_by_fluorescence`. I think `total` can be more widely used than `n`, allowing more consistency. To me, `total_cells` or `total_vars` make sense while `n_fluorescence` or `n_log_counts` don't. It's also totally fine to have a mix. Yeah, I figured I didn't want to make a whole copy of the object if I didn't want update or add all the metrics. About places in the codebase where naming would need to change, I'd argue the default shouldn't be to use a pre-computed value. I hadn't realized that `n_counts` fields were being stored or used until I started looking around. Since summing over a matrix is likely a pretty light computation compared to what follows, I don't think there's a strong performance argument for keeping it as the default.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:1002,modifiability,variab,variables,1002,"No problem! * `features` sounds more natural to me, but `variables` is fine. Maybe we could do `vars` instead of `variables` for reduced verbosity? * `expr_type` would work. Maybe `vars_type`? * How about `n_genes_by_{exprs_type/vars_type}`? `n` works great for this, since it's integer valued. I might like `vars` over `genes` since the variables could be transcripts or surface markers, but I'm not sure on this. I like the `by_{vars_type}` convention for a couple reasons, which also apply to your last point:. * It allows recording at multiple steps in the process. You could imagine: `n_{vars/genes}_by_counts` and `n_{vars/genes}_by_imputed_counts` or `n_{vars/genes}_by_normed_expression`. * The convention allows for multi-omic measurements on a gene, `n_{vars/genes}_by_fluorescence` for example. This is a case where `genes` makes more sense than `vars`. * `control_variables` does sound more natural. I'd possibly like to replace `control` as well, since these aren't necessarily controlled variables. * Largely similar thoughts as the third point, e.g. * Recording at multiple steps: `n_cells_by_counts` and `n_cells_by_imputed_counts`. * Multi-omic measurements: `n_cells_by_fluorescence`. I think `total` can be more widely used than `n`, allowing more consistency. To me, `total_cells` or `total_vars` make sense while `n_fluorescence` or `n_log_counts` don't. It's also totally fine to have a mix. Yeah, I figured I didn't want to make a whole copy of the object if I didn't want update or add all the metrics. About places in the codebase where naming would need to change, I'd argue the default shouldn't be to use a pre-computed value. I hadn't realized that `n_counts` fields were being stored or used until I started looking around. Since summing over a matrix is likely a pretty light computation compared to what follows, I don't think there's a strong performance argument for keeping it as the default.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:1876,performance,perform,performance,1876,"No problem! * `features` sounds more natural to me, but `variables` is fine. Maybe we could do `vars` instead of `variables` for reduced verbosity? * `expr_type` would work. Maybe `vars_type`? * How about `n_genes_by_{exprs_type/vars_type}`? `n` works great for this, since it's integer valued. I might like `vars` over `genes` since the variables could be transcripts or surface markers, but I'm not sure on this. I like the `by_{vars_type}` convention for a couple reasons, which also apply to your last point:. * It allows recording at multiple steps in the process. You could imagine: `n_{vars/genes}_by_counts` and `n_{vars/genes}_by_imputed_counts` or `n_{vars/genes}_by_normed_expression`. * The convention allows for multi-omic measurements on a gene, `n_{vars/genes}_by_fluorescence` for example. This is a case where `genes` makes more sense than `vars`. * `control_variables` does sound more natural. I'd possibly like to replace `control` as well, since these aren't necessarily controlled variables. * Largely similar thoughts as the third point, e.g. * Recording at multiple steps: `n_cells_by_counts` and `n_cells_by_imputed_counts`. * Multi-omic measurements: `n_cells_by_fluorescence`. I think `total` can be more widely used than `n`, allowing more consistency. To me, `total_cells` or `total_vars` make sense while `n_fluorescence` or `n_log_counts` don't. It's also totally fine to have a mix. Yeah, I figured I didn't want to make a whole copy of the object if I didn't want update or add all the metrics. About places in the codebase where naming would need to change, I'd argue the default shouldn't be to use a pre-computed value. I hadn't realized that `n_counts` fields were being stored or used until I started looking around. Since summing over a matrix is likely a pretty light computation compared to what follows, I don't think there's a strong performance argument for keeping it as the default.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:887,reliability,doe,does,887,"No problem! * `features` sounds more natural to me, but `variables` is fine. Maybe we could do `vars` instead of `variables` for reduced verbosity? * `expr_type` would work. Maybe `vars_type`? * How about `n_genes_by_{exprs_type/vars_type}`? `n` works great for this, since it's integer valued. I might like `vars` over `genes` since the variables could be transcripts or surface markers, but I'm not sure on this. I like the `by_{vars_type}` convention for a couple reasons, which also apply to your last point:. * It allows recording at multiple steps in the process. You could imagine: `n_{vars/genes}_by_counts` and `n_{vars/genes}_by_imputed_counts` or `n_{vars/genes}_by_normed_expression`. * The convention allows for multi-omic measurements on a gene, `n_{vars/genes}_by_fluorescence` for example. This is a case where `genes` makes more sense than `vars`. * `control_variables` does sound more natural. I'd possibly like to replace `control` as well, since these aren't necessarily controlled variables. * Largely similar thoughts as the third point, e.g. * Recording at multiple steps: `n_cells_by_counts` and `n_cells_by_imputed_counts`. * Multi-omic measurements: `n_cells_by_fluorescence`. I think `total` can be more widely used than `n`, allowing more consistency. To me, `total_cells` or `total_vars` make sense while `n_fluorescence` or `n_log_counts` don't. It's also totally fine to have a mix. Yeah, I figured I didn't want to make a whole copy of the object if I didn't want update or add all the metrics. About places in the codebase where naming would need to change, I'd argue the default shouldn't be to use a pre-computed value. I hadn't realized that `n_counts` fields were being stored or used until I started looking around. Since summing over a matrix is likely a pretty light computation compared to what follows, I don't think there's a strong performance argument for keeping it as the default.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:1496,safety,updat,update,1496,"No problem! * `features` sounds more natural to me, but `variables` is fine. Maybe we could do `vars` instead of `variables` for reduced verbosity? * `expr_type` would work. Maybe `vars_type`? * How about `n_genes_by_{exprs_type/vars_type}`? `n` works great for this, since it's integer valued. I might like `vars` over `genes` since the variables could be transcripts or surface markers, but I'm not sure on this. I like the `by_{vars_type}` convention for a couple reasons, which also apply to your last point:. * It allows recording at multiple steps in the process. You could imagine: `n_{vars/genes}_by_counts` and `n_{vars/genes}_by_imputed_counts` or `n_{vars/genes}_by_normed_expression`. * The convention allows for multi-omic measurements on a gene, `n_{vars/genes}_by_fluorescence` for example. This is a case where `genes` makes more sense than `vars`. * `control_variables` does sound more natural. I'd possibly like to replace `control` as well, since these aren't necessarily controlled variables. * Largely similar thoughts as the third point, e.g. * Recording at multiple steps: `n_cells_by_counts` and `n_cells_by_imputed_counts`. * Multi-omic measurements: `n_cells_by_fluorescence`. I think `total` can be more widely used than `n`, allowing more consistency. To me, `total_cells` or `total_vars` make sense while `n_fluorescence` or `n_log_counts` don't. It's also totally fine to have a mix. Yeah, I figured I didn't want to make a whole copy of the object if I didn't want update or add all the metrics. About places in the codebase where naming would need to change, I'd argue the default shouldn't be to use a pre-computed value. I hadn't realized that `n_counts` fields were being stored or used until I started looking around. Since summing over a matrix is likely a pretty light computation compared to what follows, I don't think there's a strong performance argument for keeping it as the default.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:942,security,control,control,942,"No problem! * `features` sounds more natural to me, but `variables` is fine. Maybe we could do `vars` instead of `variables` for reduced verbosity? * `expr_type` would work. Maybe `vars_type`? * How about `n_genes_by_{exprs_type/vars_type}`? `n` works great for this, since it's integer valued. I might like `vars` over `genes` since the variables could be transcripts or surface markers, but I'm not sure on this. I like the `by_{vars_type}` convention for a couple reasons, which also apply to your last point:. * It allows recording at multiple steps in the process. You could imagine: `n_{vars/genes}_by_counts` and `n_{vars/genes}_by_imputed_counts` or `n_{vars/genes}_by_normed_expression`. * The convention allows for multi-omic measurements on a gene, `n_{vars/genes}_by_fluorescence` for example. This is a case where `genes` makes more sense than `vars`. * `control_variables` does sound more natural. I'd possibly like to replace `control` as well, since these aren't necessarily controlled variables. * Largely similar thoughts as the third point, e.g. * Recording at multiple steps: `n_cells_by_counts` and `n_cells_by_imputed_counts`. * Multi-omic measurements: `n_cells_by_fluorescence`. I think `total` can be more widely used than `n`, allowing more consistency. To me, `total_cells` or `total_vars` make sense while `n_fluorescence` or `n_log_counts` don't. It's also totally fine to have a mix. Yeah, I figured I didn't want to make a whole copy of the object if I didn't want update or add all the metrics. About places in the codebase where naming would need to change, I'd argue the default shouldn't be to use a pre-computed value. I hadn't realized that `n_counts` fields were being stored or used until I started looking around. Since summing over a matrix is likely a pretty light computation compared to what follows, I don't think there's a strong performance argument for keeping it as the default.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:991,security,control,controlled,991,"No problem! * `features` sounds more natural to me, but `variables` is fine. Maybe we could do `vars` instead of `variables` for reduced verbosity? * `expr_type` would work. Maybe `vars_type`? * How about `n_genes_by_{exprs_type/vars_type}`? `n` works great for this, since it's integer valued. I might like `vars` over `genes` since the variables could be transcripts or surface markers, but I'm not sure on this. I like the `by_{vars_type}` convention for a couple reasons, which also apply to your last point:. * It allows recording at multiple steps in the process. You could imagine: `n_{vars/genes}_by_counts` and `n_{vars/genes}_by_imputed_counts` or `n_{vars/genes}_by_normed_expression`. * The convention allows for multi-omic measurements on a gene, `n_{vars/genes}_by_fluorescence` for example. This is a case where `genes` makes more sense than `vars`. * `control_variables` does sound more natural. I'd possibly like to replace `control` as well, since these aren't necessarily controlled variables. * Largely similar thoughts as the third point, e.g. * Recording at multiple steps: `n_cells_by_counts` and `n_cells_by_imputed_counts`. * Multi-omic measurements: `n_cells_by_fluorescence`. I think `total` can be more widely used than `n`, allowing more consistency. To me, `total_cells` or `total_vars` make sense while `n_fluorescence` or `n_log_counts` don't. It's also totally fine to have a mix. Yeah, I figured I didn't want to make a whole copy of the object if I didn't want update or add all the metrics. About places in the codebase where naming would need to change, I'd argue the default shouldn't be to use a pre-computed value. I hadn't realized that `n_counts` fields were being stored or used until I started looking around. Since summing over a matrix is likely a pretty light computation compared to what follows, I don't think there's a strong performance argument for keeping it as the default.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:1496,security,updat,update,1496,"No problem! * `features` sounds more natural to me, but `variables` is fine. Maybe we could do `vars` instead of `variables` for reduced verbosity? * `expr_type` would work. Maybe `vars_type`? * How about `n_genes_by_{exprs_type/vars_type}`? `n` works great for this, since it's integer valued. I might like `vars` over `genes` since the variables could be transcripts or surface markers, but I'm not sure on this. I like the `by_{vars_type}` convention for a couple reasons, which also apply to your last point:. * It allows recording at multiple steps in the process. You could imagine: `n_{vars/genes}_by_counts` and `n_{vars/genes}_by_imputed_counts` or `n_{vars/genes}_by_normed_expression`. * The convention allows for multi-omic measurements on a gene, `n_{vars/genes}_by_fluorescence` for example. This is a case where `genes` makes more sense than `vars`. * `control_variables` does sound more natural. I'd possibly like to replace `control` as well, since these aren't necessarily controlled variables. * Largely similar thoughts as the third point, e.g. * Recording at multiple steps: `n_cells_by_counts` and `n_cells_by_imputed_counts`. * Multi-omic measurements: `n_cells_by_fluorescence`. I think `total` can be more widely used than `n`, allowing more consistency. To me, `total_cells` or `total_vars` make sense while `n_fluorescence` or `n_log_counts` don't. It's also totally fine to have a mix. Yeah, I figured I didn't want to make a whole copy of the object if I didn't want update or add all the metrics. About places in the codebase where naming would need to change, I'd argue the default shouldn't be to use a pre-computed value. I hadn't realized that `n_counts` fields were being stored or used until I started looking around. Since summing over a matrix is likely a pretty light computation compared to what follows, I don't think there's a strong performance argument for keeping it as the default.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:460,testability,coupl,couple,460,"No problem! * `features` sounds more natural to me, but `variables` is fine. Maybe we could do `vars` instead of `variables` for reduced verbosity? * `expr_type` would work. Maybe `vars_type`? * How about `n_genes_by_{exprs_type/vars_type}`? `n` works great for this, since it's integer valued. I might like `vars` over `genes` since the variables could be transcripts or surface markers, but I'm not sure on this. I like the `by_{vars_type}` convention for a couple reasons, which also apply to your last point:. * It allows recording at multiple steps in the process. You could imagine: `n_{vars/genes}_by_counts` and `n_{vars/genes}_by_imputed_counts` or `n_{vars/genes}_by_normed_expression`. * The convention allows for multi-omic measurements on a gene, `n_{vars/genes}_by_fluorescence` for example. This is a case where `genes` makes more sense than `vars`. * `control_variables` does sound more natural. I'd possibly like to replace `control` as well, since these aren't necessarily controlled variables. * Largely similar thoughts as the third point, e.g. * Recording at multiple steps: `n_cells_by_counts` and `n_cells_by_imputed_counts`. * Multi-omic measurements: `n_cells_by_fluorescence`. I think `total` can be more widely used than `n`, allowing more consistency. To me, `total_cells` or `total_vars` make sense while `n_fluorescence` or `n_log_counts` don't. It's also totally fine to have a mix. Yeah, I figured I didn't want to make a whole copy of the object if I didn't want update or add all the metrics. About places in the codebase where naming would need to change, I'd argue the default shouldn't be to use a pre-computed value. I hadn't realized that `n_counts` fields were being stored or used until I started looking around. Since summing over a matrix is likely a pretty light computation compared to what follows, I don't think there's a strong performance argument for keeping it as the default.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:942,testability,control,control,942,"No problem! * `features` sounds more natural to me, but `variables` is fine. Maybe we could do `vars` instead of `variables` for reduced verbosity? * `expr_type` would work. Maybe `vars_type`? * How about `n_genes_by_{exprs_type/vars_type}`? `n` works great for this, since it's integer valued. I might like `vars` over `genes` since the variables could be transcripts or surface markers, but I'm not sure on this. I like the `by_{vars_type}` convention for a couple reasons, which also apply to your last point:. * It allows recording at multiple steps in the process. You could imagine: `n_{vars/genes}_by_counts` and `n_{vars/genes}_by_imputed_counts` or `n_{vars/genes}_by_normed_expression`. * The convention allows for multi-omic measurements on a gene, `n_{vars/genes}_by_fluorescence` for example. This is a case where `genes` makes more sense than `vars`. * `control_variables` does sound more natural. I'd possibly like to replace `control` as well, since these aren't necessarily controlled variables. * Largely similar thoughts as the third point, e.g. * Recording at multiple steps: `n_cells_by_counts` and `n_cells_by_imputed_counts`. * Multi-omic measurements: `n_cells_by_fluorescence`. I think `total` can be more widely used than `n`, allowing more consistency. To me, `total_cells` or `total_vars` make sense while `n_fluorescence` or `n_log_counts` don't. It's also totally fine to have a mix. Yeah, I figured I didn't want to make a whole copy of the object if I didn't want update or add all the metrics. About places in the codebase where naming would need to change, I'd argue the default shouldn't be to use a pre-computed value. I hadn't realized that `n_counts` fields were being stored or used until I started looking around. Since summing over a matrix is likely a pretty light computation compared to what follows, I don't think there's a strong performance argument for keeping it as the default.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:991,testability,control,controlled,991,"No problem! * `features` sounds more natural to me, but `variables` is fine. Maybe we could do `vars` instead of `variables` for reduced verbosity? * `expr_type` would work. Maybe `vars_type`? * How about `n_genes_by_{exprs_type/vars_type}`? `n` works great for this, since it's integer valued. I might like `vars` over `genes` since the variables could be transcripts or surface markers, but I'm not sure on this. I like the `by_{vars_type}` convention for a couple reasons, which also apply to your last point:. * It allows recording at multiple steps in the process. You could imagine: `n_{vars/genes}_by_counts` and `n_{vars/genes}_by_imputed_counts` or `n_{vars/genes}_by_normed_expression`. * The convention allows for multi-omic measurements on a gene, `n_{vars/genes}_by_fluorescence` for example. This is a case where `genes` makes more sense than `vars`. * `control_variables` does sound more natural. I'd possibly like to replace `control` as well, since these aren't necessarily controlled variables. * Largely similar thoughts as the third point, e.g. * Recording at multiple steps: `n_cells_by_counts` and `n_cells_by_imputed_counts`. * Multi-omic measurements: `n_cells_by_fluorescence`. I think `total` can be more widely used than `n`, allowing more consistency. To me, `total_cells` or `total_vars` make sense while `n_fluorescence` or `n_log_counts` don't. It's also totally fine to have a mix. Yeah, I figured I didn't want to make a whole copy of the object if I didn't want update or add all the metrics. About places in the codebase where naming would need to change, I'd argue the default shouldn't be to use a pre-computed value. I hadn't realized that `n_counts` fields were being stored or used until I started looking around. Since summing over a matrix is likely a pretty light computation compared to what follows, I don't think there's a strong performance argument for keeping it as the default.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:1267,usability,consist,consistency,1267,"No problem! * `features` sounds more natural to me, but `variables` is fine. Maybe we could do `vars` instead of `variables` for reduced verbosity? * `expr_type` would work. Maybe `vars_type`? * How about `n_genes_by_{exprs_type/vars_type}`? `n` works great for this, since it's integer valued. I might like `vars` over `genes` since the variables could be transcripts or surface markers, but I'm not sure on this. I like the `by_{vars_type}` convention for a couple reasons, which also apply to your last point:. * It allows recording at multiple steps in the process. You could imagine: `n_{vars/genes}_by_counts` and `n_{vars/genes}_by_imputed_counts` or `n_{vars/genes}_by_normed_expression`. * The convention allows for multi-omic measurements on a gene, `n_{vars/genes}_by_fluorescence` for example. This is a case where `genes` makes more sense than `vars`. * `control_variables` does sound more natural. I'd possibly like to replace `control` as well, since these aren't necessarily controlled variables. * Largely similar thoughts as the third point, e.g. * Recording at multiple steps: `n_cells_by_counts` and `n_cells_by_imputed_counts`. * Multi-omic measurements: `n_cells_by_fluorescence`. I think `total` can be more widely used than `n`, allowing more consistency. To me, `total_cells` or `total_vars` make sense while `n_fluorescence` or `n_log_counts` don't. It's also totally fine to have a mix. Yeah, I figured I didn't want to make a whole copy of the object if I didn't want update or add all the metrics. About places in the codebase where naming would need to change, I'd argue the default shouldn't be to use a pre-computed value. I hadn't realized that `n_counts` fields were being stored or used until I started looking around. Since summing over a matrix is likely a pretty light computation compared to what follows, I don't think there's a strong performance argument for keeping it as the default.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:1876,usability,perform,performance,1876,"No problem! * `features` sounds more natural to me, but `variables` is fine. Maybe we could do `vars` instead of `variables` for reduced verbosity? * `expr_type` would work. Maybe `vars_type`? * How about `n_genes_by_{exprs_type/vars_type}`? `n` works great for this, since it's integer valued. I might like `vars` over `genes` since the variables could be transcripts or surface markers, but I'm not sure on this. I like the `by_{vars_type}` convention for a couple reasons, which also apply to your last point:. * It allows recording at multiple steps in the process. You could imagine: `n_{vars/genes}_by_counts` and `n_{vars/genes}_by_imputed_counts` or `n_{vars/genes}_by_normed_expression`. * The convention allows for multi-omic measurements on a gene, `n_{vars/genes}_by_fluorescence` for example. This is a case where `genes` makes more sense than `vars`. * `control_variables` does sound more natural. I'd possibly like to replace `control` as well, since these aren't necessarily controlled variables. * Largely similar thoughts as the third point, e.g. * Recording at multiple steps: `n_cells_by_counts` and `n_cells_by_imputed_counts`. * Multi-omic measurements: `n_cells_by_fluorescence`. I think `total` can be more widely used than `n`, allowing more consistency. To me, `total_cells` or `total_vars` make sense while `n_fluorescence` or `n_log_counts` don't. It's also totally fine to have a mix. Yeah, I figured I didn't want to make a whole copy of the object if I didn't want update or add all the metrics. About places in the codebase where naming would need to change, I'd argue the default shouldn't be to use a pre-computed value. I hadn't realized that `n_counts` fields were being stored or used until I started looking around. Since summing over a matrix is likely a pretty light computation compared to what follows, I don't think there's a strong performance argument for keeping it as the default.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:446,modifiability,paramet,parameter,446,"thank you for your thoughts and suggestions! great! :smile:. - `vars` is fine, `genes` is also fine as this is a function that will predominantly used for RNA-seq, I guess, `features` would be fine if we had adopted a different convention. - `expr_type` is better than `vars_type`; we could do `values_type`, which is suggestive and anticipates that at some point, `.X` might get deprecated and replaced by `.values`; but maybe a simple `suffix` parameter is better? - why not simply `n_genes` and `n_genes_{suffix} if suffix is not None`? if people want to distinguish between different processing steps or parts of the data matrix? but right now, the canonical use would only complete these things once for the raw data; I hardly imagine computing this stuff on imputed counts or normed expression... if people want, ok, they have the suffix argument for... if people do multi-omics with anndata (we're starting to do this a lot), yes, they should also be able to do it, but a `suffix` would be fine in this case, too; regarding `by_{suffix}`: I usually associate conditioning on something when I read `by` and I guess many people do, are we sure we want this here? - `control_variables` is more clear than the super-generic `variables` argument, which usually indicates in scanpy that you want a function to restrict to a set of variables; but here, it's different. - largely similar thoughts on `n_cells` vs `n_cells_{suffix}`. - `n_...` versus `total_...`; sure you're absolutely right, `total_vars` is much easier to swallow than `n_fluorescence`; so, I'm ok with `total_...` if you like to move forward with that; on the other hand, having `n_...` for things that are numbers and counted and `sum_...` for things that are simply sums within columns or rows would be even clearer, I imagine; but maybe confusing to implement; your decision! :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:1228,modifiability,variab,variables,1228,"thank you for your thoughts and suggestions! great! :smile:. - `vars` is fine, `genes` is also fine as this is a function that will predominantly used for RNA-seq, I guess, `features` would be fine if we had adopted a different convention. - `expr_type` is better than `vars_type`; we could do `values_type`, which is suggestive and anticipates that at some point, `.X` might get deprecated and replaced by `.values`; but maybe a simple `suffix` parameter is better? - why not simply `n_genes` and `n_genes_{suffix} if suffix is not None`? if people want to distinguish between different processing steps or parts of the data matrix? but right now, the canonical use would only complete these things once for the raw data; I hardly imagine computing this stuff on imputed counts or normed expression... if people want, ok, they have the suffix argument for... if people do multi-omics with anndata (we're starting to do this a lot), yes, they should also be able to do it, but a `suffix` would be fine in this case, too; regarding `by_{suffix}`: I usually associate conditioning on something when I read `by` and I guess many people do, are we sure we want this here? - `control_variables` is more clear than the super-generic `variables` argument, which usually indicates in scanpy that you want a function to restrict to a set of variables; but here, it's different. - largely similar thoughts on `n_cells` vs `n_cells_{suffix}`. - `n_...` versus `total_...`; sure you're absolutely right, `total_vars` is much easier to swallow than `n_fluorescence`; so, I'm ok with `total_...` if you like to move forward with that; on the other hand, having `n_...` for things that are numbers and counted and `sum_...` for things that are simply sums within columns or rows would be even clearer, I imagine; but maybe confusing to implement; your decision! :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:1332,modifiability,variab,variables,1332,"thank you for your thoughts and suggestions! great! :smile:. - `vars` is fine, `genes` is also fine as this is a function that will predominantly used for RNA-seq, I guess, `features` would be fine if we had adopted a different convention. - `expr_type` is better than `vars_type`; we could do `values_type`, which is suggestive and anticipates that at some point, `.X` might get deprecated and replaced by `.values`; but maybe a simple `suffix` parameter is better? - why not simply `n_genes` and `n_genes_{suffix} if suffix is not None`? if people want to distinguish between different processing steps or parts of the data matrix? but right now, the canonical use would only complete these things once for the raw data; I hardly imagine computing this stuff on imputed counts or normed expression... if people want, ok, they have the suffix argument for... if people do multi-omics with anndata (we're starting to do this a lot), yes, they should also be able to do it, but a `suffix` would be fine in this case, too; regarding `by_{suffix}`: I usually associate conditioning on something when I read `by` and I guess many people do, are we sure we want this here? - `control_variables` is more clear than the super-generic `variables` argument, which usually indicates in scanpy that you want a function to restrict to a set of variables; but here, it's different. - largely similar thoughts on `n_cells` vs `n_cells_{suffix}`. - `n_...` versus `total_...`; sure you're absolutely right, `total_vars` is much easier to swallow than `n_fluorescence`; so, I'm ok with `total_...` if you like to move forward with that; on the other hand, having `n_...` for things that are numbers and counted and `sum_...` for things that are simply sums within columns or rows would be even clearer, I imagine; but maybe confusing to implement; your decision! :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:678,safety,compl,complete,678,"thank you for your thoughts and suggestions! great! :smile:. - `vars` is fine, `genes` is also fine as this is a function that will predominantly used for RNA-seq, I guess, `features` would be fine if we had adopted a different convention. - `expr_type` is better than `vars_type`; we could do `values_type`, which is suggestive and anticipates that at some point, `.X` might get deprecated and replaced by `.values`; but maybe a simple `suffix` parameter is better? - why not simply `n_genes` and `n_genes_{suffix} if suffix is not None`? if people want to distinguish between different processing steps or parts of the data matrix? but right now, the canonical use would only complete these things once for the raw data; I hardly imagine computing this stuff on imputed counts or normed expression... if people want, ok, they have the suffix argument for... if people do multi-omics with anndata (we're starting to do this a lot), yes, they should also be able to do it, but a `suffix` would be fine in this case, too; regarding `by_{suffix}`: I usually associate conditioning on something when I read `by` and I guess many people do, are we sure we want this here? - `control_variables` is more clear than the super-generic `variables` argument, which usually indicates in scanpy that you want a function to restrict to a set of variables; but here, it's different. - largely similar thoughts on `n_cells` vs `n_cells_{suffix}`. - `n_...` versus `total_...`; sure you're absolutely right, `total_vars` is much easier to swallow than `n_fluorescence`; so, I'm ok with `total_...` if you like to move forward with that; on the other hand, having `n_...` for things that are numbers and counted and `sum_...` for things that are simply sums within columns or rows would be even clearer, I imagine; but maybe confusing to implement; your decision! :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:678,security,compl,complete,678,"thank you for your thoughts and suggestions! great! :smile:. - `vars` is fine, `genes` is also fine as this is a function that will predominantly used for RNA-seq, I guess, `features` would be fine if we had adopted a different convention. - `expr_type` is better than `vars_type`; we could do `values_type`, which is suggestive and anticipates that at some point, `.X` might get deprecated and replaced by `.values`; but maybe a simple `suffix` parameter is better? - why not simply `n_genes` and `n_genes_{suffix} if suffix is not None`? if people want to distinguish between different processing steps or parts of the data matrix? but right now, the canonical use would only complete these things once for the raw data; I hardly imagine computing this stuff on imputed counts or normed expression... if people want, ok, they have the suffix argument for... if people do multi-omics with anndata (we're starting to do this a lot), yes, they should also be able to do it, but a `suffix` would be fine in this case, too; regarding `by_{suffix}`: I usually associate conditioning on something when I read `by` and I guess many people do, are we sure we want this here? - `control_variables` is more clear than the super-generic `variables` argument, which usually indicates in scanpy that you want a function to restrict to a set of variables; but here, it's different. - largely similar thoughts on `n_cells` vs `n_cells_{suffix}`. - `n_...` versus `total_...`; sure you're absolutely right, `total_vars` is much easier to swallow than `n_fluorescence`; so, I'm ok with `total_...` if you like to move forward with that; on the other hand, having `n_...` for things that are numbers and counted and `sum_...` for things that are simply sums within columns or rows would be even clearer, I imagine; but maybe confusing to implement; your decision! :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:430,testability,simpl,simple,430,"thank you for your thoughts and suggestions! great! :smile:. - `vars` is fine, `genes` is also fine as this is a function that will predominantly used for RNA-seq, I guess, `features` would be fine if we had adopted a different convention. - `expr_type` is better than `vars_type`; we could do `values_type`, which is suggestive and anticipates that at some point, `.X` might get deprecated and replaced by `.values`; but maybe a simple `suffix` parameter is better? - why not simply `n_genes` and `n_genes_{suffix} if suffix is not None`? if people want to distinguish between different processing steps or parts of the data matrix? but right now, the canonical use would only complete these things once for the raw data; I hardly imagine computing this stuff on imputed counts or normed expression... if people want, ok, they have the suffix argument for... if people do multi-omics with anndata (we're starting to do this a lot), yes, they should also be able to do it, but a `suffix` would be fine in this case, too; regarding `by_{suffix}`: I usually associate conditioning on something when I read `by` and I guess many people do, are we sure we want this here? - `control_variables` is more clear than the super-generic `variables` argument, which usually indicates in scanpy that you want a function to restrict to a set of variables; but here, it's different. - largely similar thoughts on `n_cells` vs `n_cells_{suffix}`. - `n_...` versus `total_...`; sure you're absolutely right, `total_vars` is much easier to swallow than `n_fluorescence`; so, I'm ok with `total_...` if you like to move forward with that; on the other hand, having `n_...` for things that are numbers and counted and `sum_...` for things that are simply sums within columns or rows would be even clearer, I imagine; but maybe confusing to implement; your decision! :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:477,testability,simpl,simply,477,"thank you for your thoughts and suggestions! great! :smile:. - `vars` is fine, `genes` is also fine as this is a function that will predominantly used for RNA-seq, I guess, `features` would be fine if we had adopted a different convention. - `expr_type` is better than `vars_type`; we could do `values_type`, which is suggestive and anticipates that at some point, `.X` might get deprecated and replaced by `.values`; but maybe a simple `suffix` parameter is better? - why not simply `n_genes` and `n_genes_{suffix} if suffix is not None`? if people want to distinguish between different processing steps or parts of the data matrix? but right now, the canonical use would only complete these things once for the raw data; I hardly imagine computing this stuff on imputed counts or normed expression... if people want, ok, they have the suffix argument for... if people do multi-omics with anndata (we're starting to do this a lot), yes, they should also be able to do it, but a `suffix` would be fine in this case, too; regarding `by_{suffix}`: I usually associate conditioning on something when I read `by` and I guess many people do, are we sure we want this here? - `control_variables` is more clear than the super-generic `variables` argument, which usually indicates in scanpy that you want a function to restrict to a set of variables; but here, it's different. - largely similar thoughts on `n_cells` vs `n_cells_{suffix}`. - `n_...` versus `total_...`; sure you're absolutely right, `total_vars` is much easier to swallow than `n_fluorescence`; so, I'm ok with `total_...` if you like to move forward with that; on the other hand, having `n_...` for things that are numbers and counted and `sum_...` for things that are simply sums within columns or rows would be even clearer, I imagine; but maybe confusing to implement; your decision! :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:1729,testability,simpl,simply,1729,"thank you for your thoughts and suggestions! great! :smile:. - `vars` is fine, `genes` is also fine as this is a function that will predominantly used for RNA-seq, I guess, `features` would be fine if we had adopted a different convention. - `expr_type` is better than `vars_type`; we could do `values_type`, which is suggestive and anticipates that at some point, `.X` might get deprecated and replaced by `.values`; but maybe a simple `suffix` parameter is better? - why not simply `n_genes` and `n_genes_{suffix} if suffix is not None`? if people want to distinguish between different processing steps or parts of the data matrix? but right now, the canonical use would only complete these things once for the raw data; I hardly imagine computing this stuff on imputed counts or normed expression... if people want, ok, they have the suffix argument for... if people do multi-omics with anndata (we're starting to do this a lot), yes, they should also be able to do it, but a `suffix` would be fine in this case, too; regarding `by_{suffix}`: I usually associate conditioning on something when I read `by` and I guess many people do, are we sure we want this here? - `control_variables` is more clear than the super-generic `variables` argument, which usually indicates in scanpy that you want a function to restrict to a set of variables; but here, it's different. - largely similar thoughts on `n_cells` vs `n_cells_{suffix}`. - `n_...` versus `total_...`; sure you're absolutely right, `total_vars` is much easier to swallow than `n_fluorescence`; so, I'm ok with `total_...` if you like to move forward with that; on the other hand, having `n_...` for things that are numbers and counted and `sum_...` for things that are simply sums within columns or rows would be even clearer, I imagine; but maybe confusing to implement; your decision! :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:430,usability,simpl,simple,430,"thank you for your thoughts and suggestions! great! :smile:. - `vars` is fine, `genes` is also fine as this is a function that will predominantly used for RNA-seq, I guess, `features` would be fine if we had adopted a different convention. - `expr_type` is better than `vars_type`; we could do `values_type`, which is suggestive and anticipates that at some point, `.X` might get deprecated and replaced by `.values`; but maybe a simple `suffix` parameter is better? - why not simply `n_genes` and `n_genes_{suffix} if suffix is not None`? if people want to distinguish between different processing steps or parts of the data matrix? but right now, the canonical use would only complete these things once for the raw data; I hardly imagine computing this stuff on imputed counts or normed expression... if people want, ok, they have the suffix argument for... if people do multi-omics with anndata (we're starting to do this a lot), yes, they should also be able to do it, but a `suffix` would be fine in this case, too; regarding `by_{suffix}`: I usually associate conditioning on something when I read `by` and I guess many people do, are we sure we want this here? - `control_variables` is more clear than the super-generic `variables` argument, which usually indicates in scanpy that you want a function to restrict to a set of variables; but here, it's different. - largely similar thoughts on `n_cells` vs `n_cells_{suffix}`. - `n_...` versus `total_...`; sure you're absolutely right, `total_vars` is much easier to swallow than `n_fluorescence`; so, I'm ok with `total_...` if you like to move forward with that; on the other hand, having `n_...` for things that are numbers and counted and `sum_...` for things that are simply sums within columns or rows would be even clearer, I imagine; but maybe confusing to implement; your decision! :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:477,usability,simpl,simply,477,"thank you for your thoughts and suggestions! great! :smile:. - `vars` is fine, `genes` is also fine as this is a function that will predominantly used for RNA-seq, I guess, `features` would be fine if we had adopted a different convention. - `expr_type` is better than `vars_type`; we could do `values_type`, which is suggestive and anticipates that at some point, `.X` might get deprecated and replaced by `.values`; but maybe a simple `suffix` parameter is better? - why not simply `n_genes` and `n_genes_{suffix} if suffix is not None`? if people want to distinguish between different processing steps or parts of the data matrix? but right now, the canonical use would only complete these things once for the raw data; I hardly imagine computing this stuff on imputed counts or normed expression... if people want, ok, they have the suffix argument for... if people do multi-omics with anndata (we're starting to do this a lot), yes, they should also be able to do it, but a `suffix` would be fine in this case, too; regarding `by_{suffix}`: I usually associate conditioning on something when I read `by` and I guess many people do, are we sure we want this here? - `control_variables` is more clear than the super-generic `variables` argument, which usually indicates in scanpy that you want a function to restrict to a set of variables; but here, it's different. - largely similar thoughts on `n_cells` vs `n_cells_{suffix}`. - `n_...` versus `total_...`; sure you're absolutely right, `total_vars` is much easier to swallow than `n_fluorescence`; so, I'm ok with `total_...` if you like to move forward with that; on the other hand, having `n_...` for things that are numbers and counted and `sum_...` for things that are simply sums within columns or rows would be even clearer, I imagine; but maybe confusing to implement; your decision! :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:1198,usability,clear,clear,1198,"thank you for your thoughts and suggestions! great! :smile:. - `vars` is fine, `genes` is also fine as this is a function that will predominantly used for RNA-seq, I guess, `features` would be fine if we had adopted a different convention. - `expr_type` is better than `vars_type`; we could do `values_type`, which is suggestive and anticipates that at some point, `.X` might get deprecated and replaced by `.values`; but maybe a simple `suffix` parameter is better? - why not simply `n_genes` and `n_genes_{suffix} if suffix is not None`? if people want to distinguish between different processing steps or parts of the data matrix? but right now, the canonical use would only complete these things once for the raw data; I hardly imagine computing this stuff on imputed counts or normed expression... if people want, ok, they have the suffix argument for... if people do multi-omics with anndata (we're starting to do this a lot), yes, they should also be able to do it, but a `suffix` would be fine in this case, too; regarding `by_{suffix}`: I usually associate conditioning on something when I read `by` and I guess many people do, are we sure we want this here? - `control_variables` is more clear than the super-generic `variables` argument, which usually indicates in scanpy that you want a function to restrict to a set of variables; but here, it's different. - largely similar thoughts on `n_cells` vs `n_cells_{suffix}`. - `n_...` versus `total_...`; sure you're absolutely right, `total_vars` is much easier to swallow than `n_fluorescence`; so, I'm ok with `total_...` if you like to move forward with that; on the other hand, having `n_...` for things that are numbers and counted and `sum_...` for things that are simply sums within columns or rows would be even clearer, I imagine; but maybe confusing to implement; your decision! :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:1263,usability,indicat,indicates,1263,"thank you for your thoughts and suggestions! great! :smile:. - `vars` is fine, `genes` is also fine as this is a function that will predominantly used for RNA-seq, I guess, `features` would be fine if we had adopted a different convention. - `expr_type` is better than `vars_type`; we could do `values_type`, which is suggestive and anticipates that at some point, `.X` might get deprecated and replaced by `.values`; but maybe a simple `suffix` parameter is better? - why not simply `n_genes` and `n_genes_{suffix} if suffix is not None`? if people want to distinguish between different processing steps or parts of the data matrix? but right now, the canonical use would only complete these things once for the raw data; I hardly imagine computing this stuff on imputed counts or normed expression... if people want, ok, they have the suffix argument for... if people do multi-omics with anndata (we're starting to do this a lot), yes, they should also be able to do it, but a `suffix` would be fine in this case, too; regarding `by_{suffix}`: I usually associate conditioning on something when I read `by` and I guess many people do, are we sure we want this here? - `control_variables` is more clear than the super-generic `variables` argument, which usually indicates in scanpy that you want a function to restrict to a set of variables; but here, it's different. - largely similar thoughts on `n_cells` vs `n_cells_{suffix}`. - `n_...` versus `total_...`; sure you're absolutely right, `total_vars` is much easier to swallow than `n_fluorescence`; so, I'm ok with `total_...` if you like to move forward with that; on the other hand, having `n_...` for things that are numbers and counted and `sum_...` for things that are simply sums within columns or rows would be even clearer, I imagine; but maybe confusing to implement; your decision! :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:1729,usability,simpl,simply,1729,"thank you for your thoughts and suggestions! great! :smile:. - `vars` is fine, `genes` is also fine as this is a function that will predominantly used for RNA-seq, I guess, `features` would be fine if we had adopted a different convention. - `expr_type` is better than `vars_type`; we could do `values_type`, which is suggestive and anticipates that at some point, `.X` might get deprecated and replaced by `.values`; but maybe a simple `suffix` parameter is better? - why not simply `n_genes` and `n_genes_{suffix} if suffix is not None`? if people want to distinguish between different processing steps or parts of the data matrix? but right now, the canonical use would only complete these things once for the raw data; I hardly imagine computing this stuff on imputed counts or normed expression... if people want, ok, they have the suffix argument for... if people do multi-omics with anndata (we're starting to do this a lot), yes, they should also be able to do it, but a `suffix` would be fine in this case, too; regarding `by_{suffix}`: I usually associate conditioning on something when I read `by` and I guess many people do, are we sure we want this here? - `control_variables` is more clear than the super-generic `variables` argument, which usually indicates in scanpy that you want a function to restrict to a set of variables; but here, it's different. - largely similar thoughts on `n_cells` vs `n_cells_{suffix}`. - `n_...` versus `total_...`; sure you're absolutely right, `total_vars` is much easier to swallow than `n_fluorescence`; so, I'm ok with `total_...` if you like to move forward with that; on the other hand, having `n_...` for things that are numbers and counted and `sum_...` for things that are simply sums within columns or rows would be even clearer, I imagine; but maybe confusing to implement; your decision! :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:1778,usability,clear,clearer,1778,"thank you for your thoughts and suggestions! great! :smile:. - `vars` is fine, `genes` is also fine as this is a function that will predominantly used for RNA-seq, I guess, `features` would be fine if we had adopted a different convention. - `expr_type` is better than `vars_type`; we could do `values_type`, which is suggestive and anticipates that at some point, `.X` might get deprecated and replaced by `.values`; but maybe a simple `suffix` parameter is better? - why not simply `n_genes` and `n_genes_{suffix} if suffix is not None`? if people want to distinguish between different processing steps or parts of the data matrix? but right now, the canonical use would only complete these things once for the raw data; I hardly imagine computing this stuff on imputed counts or normed expression... if people want, ok, they have the suffix argument for... if people do multi-omics with anndata (we're starting to do this a lot), yes, they should also be able to do it, but a `suffix` would be fine in this case, too; regarding `by_{suffix}`: I usually associate conditioning on something when I read `by` and I guess many people do, are we sure we want this here? - `control_variables` is more clear than the super-generic `variables` argument, which usually indicates in scanpy that you want a function to restrict to a set of variables; but here, it's different. - largely similar thoughts on `n_cells` vs `n_cells_{suffix}`. - `n_...` versus `total_...`; sure you're absolutely right, `total_vars` is much easier to swallow than `n_fluorescence`; so, I'm ok with `total_...` if you like to move forward with that; on the other hand, having `n_...` for things that are numbers and counted and `sum_...` for things that are simply sums within columns or rows would be even clearer, I imagine; but maybe confusing to implement; your decision! :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:162,safety,compl,completely,162,"PS: I like your use of `inplace` so much that I think that it should become the canonical way of triggering inplace computations in all function calls of scanpy, completely replacing `copy`; in most cases, though, it should default to `inplace=True`, I'd say... something to consider for Scanpy 2, I guess... or, we could have that in addition to `copy` and deprecate that at some point...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:162,security,compl,completely,162,"PS: I like your use of `inplace` so much that I think that it should become the canonical way of triggering inplace computations in all function calls of scanpy, completely replacing `copy`; in most cases, though, it should default to `inplace=True`, I'd say... something to consider for Scanpy 2, I guess... or, we could have that in addition to `copy` and deprecate that at some point...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:1001,deployability,observ,observations,1001,"bout a `var_type` argument which defaults to `""genes""`? Could even replace occurrences of `cells` with a `obs_type` variable. * Agreed, `expr_type` it is. * I'd considered an option of a suffix argument, but I think there are some issues with it being a string. If `suffix=None, expr_type=""counts""` is it still `total_counts`? If `suffix=""_by_counts""` is it still `total_counts`? A possible solution to this is making suffix a boolean argument, where `True` adds `_by_{expr_type}` as appropriate. * Additionally `n_genes` in particular is already an overloaded term, as it's already used with multiple different meanings in the codebase (typically number of genes to consider, or number of columns in the data matrix). . * Could you say a bit more about the conditioning? I'm not sure I follow how it would be confusing here. * `control_variables` definitely makes more sense here than `variables`, I was just wondering if there's a better word than `control`. * If there was an equivalent for observations (something like `mean_counts_in_donor1`), what would be a good name for that argument? * I think I'm happy with `n_...` for some things, `total_...` for others. * On other convention I'm unsure about, should the range of the `pct_...` values be 0-100 or 0-1? ---------------. Here's what I'm thinking it'll look like right now (pending `suffix` decision). For obs:. | current | proposed |. | ------- | -------- |. |`total_features_by_{expr_values}` | `n_{var_type}_by_{expr_type}`|. |`total_{expr_values}` | `total_{expr_type}`|. |`pct_{expr_values}_in_top_{n}_features` | `pct_{expr_type}_in_top_{n}_{var_type}`|. |`total_{expr_values}_{feature_control}` | `total_{expr_type}_{control_var}`|. |`pct_{expr_values}_{feature_control}` | `pct_{expr_type}_{control_var}`|. For var:. | current | proposed |. | ------- | -------- |. |`total_{expr_values}` | `total_{expr_type}`|. |`mean_{expr_values}` | `mean_{expr_type}`|. |`n_cells_by_{expr_values}` | `n_cells_by_{expr_type}`|. |`pct_dropout_by_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:1382,energy efficiency,current,current,1382,"genes""`? Could even replace occurrences of `cells` with a `obs_type` variable. * Agreed, `expr_type` it is. * I'd considered an option of a suffix argument, but I think there are some issues with it being a string. If `suffix=None, expr_type=""counts""` is it still `total_counts`? If `suffix=""_by_counts""` is it still `total_counts`? A possible solution to this is making suffix a boolean argument, where `True` adds `_by_{expr_type}` as appropriate. * Additionally `n_genes` in particular is already an overloaded term, as it's already used with multiple different meanings in the codebase (typically number of genes to consider, or number of columns in the data matrix). . * Could you say a bit more about the conditioning? I'm not sure I follow how it would be confusing here. * `control_variables` definitely makes more sense here than `variables`, I was just wondering if there's a better word than `control`. * If there was an equivalent for observations (something like `mean_counts_in_donor1`), what would be a good name for that argument? * I think I'm happy with `n_...` for some things, `total_...` for others. * On other convention I'm unsure about, should the range of the `pct_...` values be 0-100 or 0-1? ---------------. Here's what I'm thinking it'll look like right now (pending `suffix` decision). For obs:. | current | proposed |. | ------- | -------- |. |`total_features_by_{expr_values}` | `n_{var_type}_by_{expr_type}`|. |`total_{expr_values}` | `total_{expr_type}`|. |`pct_{expr_values}_in_top_{n}_features` | `pct_{expr_type}_in_top_{n}_{var_type}`|. |`total_{expr_values}_{feature_control}` | `total_{expr_type}_{control_var}`|. |`pct_{expr_values}_{feature_control}` | `pct_{expr_type}_{control_var}`|. For var:. | current | proposed |. | ------- | -------- |. |`total_{expr_values}` | `total_{expr_type}`|. |`mean_{expr_values}` | `mean_{expr_type}`|. |`n_cells_by_{expr_values}` | `n_cells_by_{expr_type}`|. |`pct_dropout_by_{expr_values}` | `pct_dropout_by_{expr_type}`|.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:1795,energy efficiency,current,current,1795,"genes""`? Could even replace occurrences of `cells` with a `obs_type` variable. * Agreed, `expr_type` it is. * I'd considered an option of a suffix argument, but I think there are some issues with it being a string. If `suffix=None, expr_type=""counts""` is it still `total_counts`? If `suffix=""_by_counts""` is it still `total_counts`? A possible solution to this is making suffix a boolean argument, where `True` adds `_by_{expr_type}` as appropriate. * Additionally `n_genes` in particular is already an overloaded term, as it's already used with multiple different meanings in the codebase (typically number of genes to consider, or number of columns in the data matrix). . * Could you say a bit more about the conditioning? I'm not sure I follow how it would be confusing here. * `control_variables` definitely makes more sense here than `variables`, I was just wondering if there's a better word than `control`. * If there was an equivalent for observations (something like `mean_counts_in_donor1`), what would be a good name for that argument? * I think I'm happy with `n_...` for some things, `total_...` for others. * On other convention I'm unsure about, should the range of the `pct_...` values be 0-100 or 0-1? ---------------. Here's what I'm thinking it'll look like right now (pending `suffix` decision). For obs:. | current | proposed |. | ------- | -------- |. |`total_features_by_{expr_values}` | `n_{var_type}_by_{expr_type}`|. |`total_{expr_values}` | `total_{expr_type}`|. |`pct_{expr_values}_in_top_{n}_features` | `pct_{expr_type}_in_top_{n}_{var_type}`|. |`total_{expr_values}_{feature_control}` | `total_{expr_type}_{control_var}`|. |`pct_{expr_values}_{feature_control}` | `pct_{expr_type}_{control_var}`|. For var:. | current | proposed |. | ------- | -------- |. |`total_{expr_values}` | `total_{expr_type}`|. |`mean_{expr_values}` | `mean_{expr_type}`|. |`n_cells_by_{expr_values}` | `n_cells_by_{expr_type}`|. |`pct_dropout_by_{expr_values}` | `pct_dropout_by_{expr_type}`|.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:123,modifiability,variab,variable,123,"* How about a `var_type` argument which defaults to `""genes""`? Could even replace occurrences of `cells` with a `obs_type` variable. * Agreed, `expr_type` it is. * I'd considered an option of a suffix argument, but I think there are some issues with it being a string. If `suffix=None, expr_type=""counts""` is it still `total_counts`? If `suffix=""_by_counts""` is it still `total_counts`? A possible solution to this is making suffix a boolean argument, where `True` adds `_by_{expr_type}` as appropriate. * Additionally `n_genes` in particular is already an overloaded term, as it's already used with multiple different meanings in the codebase (typically number of genes to consider, or number of columns in the data matrix). . * Could you say a bit more about the conditioning? I'm not sure I follow how it would be confusing here. * `control_variables` definitely makes more sense here than `variables`, I was just wondering if there's a better word than `control`. * If there was an equivalent for observations (something like `mean_counts_in_donor1`), what would be a good name for that argument? * I think I'm happy with `n_...` for some things, `total_...` for others. * On other convention I'm unsure about, should the range of the `pct_...` values be 0-100 or 0-1? ---------------. Here's what I'm thinking it'll look like right now (pending `suffix` decision). For obs:. | current | proposed |. | ------- | -------- |. |`total_features_by_{expr_values}` | `n_{var_type}_by_{expr_type}`|. |`total_{expr_values}` | `total_{expr_type}`|. |`pct_{expr_values}_in_top_{n}_features` | `pct_{expr_type}_in_top_{n}_{var_type}`|. |`total_{expr_values}_{feature_control}` | `total_{expr_type}_{control_var}`|. |`pct_{expr_values}_{feature_control}` | `pct_{expr_type}_{control_var}`|. For var:. | current | proposed |. | ------- | -------- |. |`total_{expr_values}` | `total_{expr_type}`|. |`mean_{expr_values}` | `mean_{expr_type}`|. |`n_cells_by_{expr_values}` | `n_cells_by_{expr_type}`|. |`pct_drop",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:894,modifiability,variab,variables,894,"* How about a `var_type` argument which defaults to `""genes""`? Could even replace occurrences of `cells` with a `obs_type` variable. * Agreed, `expr_type` it is. * I'd considered an option of a suffix argument, but I think there are some issues with it being a string. If `suffix=None, expr_type=""counts""` is it still `total_counts`? If `suffix=""_by_counts""` is it still `total_counts`? A possible solution to this is making suffix a boolean argument, where `True` adds `_by_{expr_type}` as appropriate. * Additionally `n_genes` in particular is already an overloaded term, as it's already used with multiple different meanings in the codebase (typically number of genes to consider, or number of columns in the data matrix). . * Could you say a bit more about the conditioning? I'm not sure I follow how it would be confusing here. * `control_variables` definitely makes more sense here than `variables`, I was just wondering if there's a better word than `control`. * If there was an equivalent for observations (something like `mean_counts_in_donor1`), what would be a good name for that argument? * I think I'm happy with `n_...` for some things, `total_...` for others. * On other convention I'm unsure about, should the range of the `pct_...` values be 0-100 or 0-1? ---------------. Here's what I'm thinking it'll look like right now (pending `suffix` decision). For obs:. | current | proposed |. | ------- | -------- |. |`total_features_by_{expr_values}` | `n_{var_type}_by_{expr_type}`|. |`total_{expr_values}` | `total_{expr_type}`|. |`pct_{expr_values}_in_top_{n}_features` | `pct_{expr_type}_in_top_{n}_{var_type}`|. |`total_{expr_values}_{feature_control}` | `total_{expr_type}_{control_var}`|. |`pct_{expr_values}_{feature_control}` | `pct_{expr_type}_{control_var}`|. For var:. | current | proposed |. | ------- | -------- |. |`total_{expr_values}` | `total_{expr_type}`|. |`mean_{expr_values}` | `mean_{expr_type}`|. |`n_cells_by_{expr_values}` | `n_cells_by_{expr_type}`|. |`pct_drop",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:958,security,control,control,958,"* How about a `var_type` argument which defaults to `""genes""`? Could even replace occurrences of `cells` with a `obs_type` variable. * Agreed, `expr_type` it is. * I'd considered an option of a suffix argument, but I think there are some issues with it being a string. If `suffix=None, expr_type=""counts""` is it still `total_counts`? If `suffix=""_by_counts""` is it still `total_counts`? A possible solution to this is making suffix a boolean argument, where `True` adds `_by_{expr_type}` as appropriate. * Additionally `n_genes` in particular is already an overloaded term, as it's already used with multiple different meanings in the codebase (typically number of genes to consider, or number of columns in the data matrix). . * Could you say a bit more about the conditioning? I'm not sure I follow how it would be confusing here. * `control_variables` definitely makes more sense here than `variables`, I was just wondering if there's a better word than `control`. * If there was an equivalent for observations (something like `mean_counts_in_donor1`), what would be a good name for that argument? * I think I'm happy with `n_...` for some things, `total_...` for others. * On other convention I'm unsure about, should the range of the `pct_...` values be 0-100 or 0-1? ---------------. Here's what I'm thinking it'll look like right now (pending `suffix` decision). For obs:. | current | proposed |. | ------- | -------- |. |`total_features_by_{expr_values}` | `n_{var_type}_by_{expr_type}`|. |`total_{expr_values}` | `total_{expr_type}`|. |`pct_{expr_values}_in_top_{n}_features` | `pct_{expr_type}_in_top_{n}_{var_type}`|. |`total_{expr_values}_{feature_control}` | `total_{expr_type}_{control_var}`|. |`pct_{expr_values}_{feature_control}` | `pct_{expr_type}_{control_var}`|. For var:. | current | proposed |. | ------- | -------- |. |`total_{expr_values}` | `total_{expr_type}`|. |`mean_{expr_values}` | `mean_{expr_type}`|. |`n_cells_by_{expr_values}` | `n_cells_by_{expr_type}`|. |`pct_drop",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:958,testability,control,control,958,"* How about a `var_type` argument which defaults to `""genes""`? Could even replace occurrences of `cells` with a `obs_type` variable. * Agreed, `expr_type` it is. * I'd considered an option of a suffix argument, but I think there are some issues with it being a string. If `suffix=None, expr_type=""counts""` is it still `total_counts`? If `suffix=""_by_counts""` is it still `total_counts`? A possible solution to this is making suffix a boolean argument, where `True` adds `_by_{expr_type}` as appropriate. * Additionally `n_genes` in particular is already an overloaded term, as it's already used with multiple different meanings in the codebase (typically number of genes to consider, or number of columns in the data matrix). . * Could you say a bit more about the conditioning? I'm not sure I follow how it would be confusing here. * `control_variables` definitely makes more sense here than `variables`, I was just wondering if there's a better word than `control`. * If there was an equivalent for observations (something like `mean_counts_in_donor1`), what would be a good name for that argument? * I think I'm happy with `n_...` for some things, `total_...` for others. * On other convention I'm unsure about, should the range of the `pct_...` values be 0-100 or 0-1? ---------------. Here's what I'm thinking it'll look like right now (pending `suffix` decision). For obs:. | current | proposed |. | ------- | -------- |. |`total_features_by_{expr_values}` | `n_{var_type}_by_{expr_type}`|. |`total_{expr_values}` | `total_{expr_type}`|. |`pct_{expr_values}_in_top_{n}_features` | `pct_{expr_type}_in_top_{n}_{var_type}`|. |`total_{expr_values}_{feature_control}` | `total_{expr_type}_{control_var}`|. |`pct_{expr_values}_{feature_control}` | `pct_{expr_type}_{control_var}`|. For var:. | current | proposed |. | ------- | -------- |. |`total_{expr_values}` | `total_{expr_type}`|. |`mean_{expr_values}` | `mean_{expr_type}`|. |`n_cells_by_{expr_values}` | `n_cells_by_{expr_type}`|. |`pct_drop",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:1001,testability,observ,observations,1001,"bout a `var_type` argument which defaults to `""genes""`? Could even replace occurrences of `cells` with a `obs_type` variable. * Agreed, `expr_type` it is. * I'd considered an option of a suffix argument, but I think there are some issues with it being a string. If `suffix=None, expr_type=""counts""` is it still `total_counts`? If `suffix=""_by_counts""` is it still `total_counts`? A possible solution to this is making suffix a boolean argument, where `True` adds `_by_{expr_type}` as appropriate. * Additionally `n_genes` in particular is already an overloaded term, as it's already used with multiple different meanings in the codebase (typically number of genes to consider, or number of columns in the data matrix). . * Could you say a bit more about the conditioning? I'm not sure I follow how it would be confusing here. * `control_variables` definitely makes more sense here than `variables`, I was just wondering if there's a better word than `control`. * If there was an equivalent for observations (something like `mean_counts_in_donor1`), what would be a good name for that argument? * I think I'm happy with `n_...` for some things, `total_...` for others. * On other convention I'm unsure about, should the range of the `pct_...` values be 0-100 or 0-1? ---------------. Here's what I'm thinking it'll look like right now (pending `suffix` decision). For obs:. | current | proposed |. | ------- | -------- |. |`total_features_by_{expr_values}` | `n_{var_type}_by_{expr_type}`|. |`total_{expr_values}` | `total_{expr_type}`|. |`pct_{expr_values}_in_top_{n}_features` | `pct_{expr_type}_in_top_{n}_{var_type}`|. |`total_{expr_values}_{feature_control}` | `total_{expr_type}_{control_var}`|. |`pct_{expr_values}_{feature_control}` | `pct_{expr_type}_{control_var}`|. For var:. | current | proposed |. | ------- | -------- |. |`total_{expr_values}` | `total_{expr_type}`|. |`mean_{expr_values}` | `mean_{expr_type}`|. |`n_cells_by_{expr_values}` | `n_cells_by_{expr_type}`|. |`pct_dropout_by_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:187,deployability,depend,depending,187,"> How about a var_type argument which defaults to ""genes""? Could even replace occurrences of cells with a obs_type variable. Sorry about the late response. The week passes so quickly and depending on how it goes, I can only go back to Scanpy during the weekends these days... I like your new table much better than the previous one. If you'd put that forward in a PR, I'd be super happy! PS: I think `control_variables` is fine, maybe `focus_variables` better reflects that it's not only about ""control"", but just a subset of variables (or observations) of interest. Let's not move forward on the suffix argument, your imposed structure will make things cleaner for people! :smile: `pct` is ok if in 0 - 100. No `obs_type` needed right now...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:540,deployability,observ,observations,540,"> How about a var_type argument which defaults to ""genes""? Could even replace occurrences of cells with a obs_type variable. Sorry about the late response. The week passes so quickly and depending on how it goes, I can only go back to Scanpy during the weekends these days... I like your new table much better than the previous one. If you'd put that forward in a PR, I'd be super happy! PS: I think `control_variables` is fine, maybe `focus_variables` better reflects that it's not only about ""control"", but just a subset of variables (or observations) of interest. Let's not move forward on the suffix argument, your imposed structure will make things cleaner for people! :smile: `pct` is ok if in 0 - 100. No `obs_type` needed right now...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:187,integrability,depend,depending,187,"> How about a var_type argument which defaults to ""genes""? Could even replace occurrences of cells with a obs_type variable. Sorry about the late response. The week passes so quickly and depending on how it goes, I can only go back to Scanpy during the weekends these days... I like your new table much better than the previous one. If you'd put that forward in a PR, I'd be super happy! PS: I think `control_variables` is fine, maybe `focus_variables` better reflects that it's not only about ""control"", but just a subset of variables (or observations) of interest. Let's not move forward on the suffix argument, your imposed structure will make things cleaner for people! :smile: `pct` is ok if in 0 - 100. No `obs_type` needed right now...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:516,integrability,sub,subset,516,"> How about a var_type argument which defaults to ""genes""? Could even replace occurrences of cells with a obs_type variable. Sorry about the late response. The week passes so quickly and depending on how it goes, I can only go back to Scanpy during the weekends these days... I like your new table much better than the previous one. If you'd put that forward in a PR, I'd be super happy! PS: I think `control_variables` is fine, maybe `focus_variables` better reflects that it's not only about ""control"", but just a subset of variables (or observations) of interest. Let's not move forward on the suffix argument, your imposed structure will make things cleaner for people! :smile: `pct` is ok if in 0 - 100. No `obs_type` needed right now...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:115,modifiability,variab,variable,115,"> How about a var_type argument which defaults to ""genes""? Could even replace occurrences of cells with a obs_type variable. Sorry about the late response. The week passes so quickly and depending on how it goes, I can only go back to Scanpy during the weekends these days... I like your new table much better than the previous one. If you'd put that forward in a PR, I'd be super happy! PS: I think `control_variables` is fine, maybe `focus_variables` better reflects that it's not only about ""control"", but just a subset of variables (or observations) of interest. Let's not move forward on the suffix argument, your imposed structure will make things cleaner for people! :smile: `pct` is ok if in 0 - 100. No `obs_type` needed right now...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:187,modifiability,depend,depending,187,"> How about a var_type argument which defaults to ""genes""? Could even replace occurrences of cells with a obs_type variable. Sorry about the late response. The week passes so quickly and depending on how it goes, I can only go back to Scanpy during the weekends these days... I like your new table much better than the previous one. If you'd put that forward in a PR, I'd be super happy! PS: I think `control_variables` is fine, maybe `focus_variables` better reflects that it's not only about ""control"", but just a subset of variables (or observations) of interest. Let's not move forward on the suffix argument, your imposed structure will make things cleaner for people! :smile: `pct` is ok if in 0 - 100. No `obs_type` needed right now...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:526,modifiability,variab,variables,526,"> How about a var_type argument which defaults to ""genes""? Could even replace occurrences of cells with a obs_type variable. Sorry about the late response. The week passes so quickly and depending on how it goes, I can only go back to Scanpy during the weekends these days... I like your new table much better than the previous one. If you'd put that forward in a PR, I'd be super happy! PS: I think `control_variables` is fine, maybe `focus_variables` better reflects that it's not only about ""control"", but just a subset of variables (or observations) of interest. Let's not move forward on the suffix argument, your imposed structure will make things cleaner for people! :smile: `pct` is ok if in 0 - 100. No `obs_type` needed right now...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:187,safety,depend,depending,187,"> How about a var_type argument which defaults to ""genes""? Could even replace occurrences of cells with a obs_type variable. Sorry about the late response. The week passes so quickly and depending on how it goes, I can only go back to Scanpy during the weekends these days... I like your new table much better than the previous one. If you'd put that forward in a PR, I'd be super happy! PS: I think `control_variables` is fine, maybe `focus_variables` better reflects that it's not only about ""control"", but just a subset of variables (or observations) of interest. Let's not move forward on the suffix argument, your imposed structure will make things cleaner for people! :smile: `pct` is ok if in 0 - 100. No `obs_type` needed right now...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:495,security,control,control,495,"> How about a var_type argument which defaults to ""genes""? Could even replace occurrences of cells with a obs_type variable. Sorry about the late response. The week passes so quickly and depending on how it goes, I can only go back to Scanpy during the weekends these days... I like your new table much better than the previous one. If you'd put that forward in a PR, I'd be super happy! PS: I think `control_variables` is fine, maybe `focus_variables` better reflects that it's not only about ""control"", but just a subset of variables (or observations) of interest. Let's not move forward on the suffix argument, your imposed structure will make things cleaner for people! :smile: `pct` is ok if in 0 - 100. No `obs_type` needed right now...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:187,testability,depend,depending,187,"> How about a var_type argument which defaults to ""genes""? Could even replace occurrences of cells with a obs_type variable. Sorry about the late response. The week passes so quickly and depending on how it goes, I can only go back to Scanpy during the weekends these days... I like your new table much better than the previous one. If you'd put that forward in a PR, I'd be super happy! PS: I think `control_variables` is fine, maybe `focus_variables` better reflects that it's not only about ""control"", but just a subset of variables (or observations) of interest. Let's not move forward on the suffix argument, your imposed structure will make things cleaner for people! :smile: `pct` is ok if in 0 - 100. No `obs_type` needed right now...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:495,testability,control,control,495,"> How about a var_type argument which defaults to ""genes""? Could even replace occurrences of cells with a obs_type variable. Sorry about the late response. The week passes so quickly and depending on how it goes, I can only go back to Scanpy during the weekends these days... I like your new table much better than the previous one. If you'd put that forward in a PR, I'd be super happy! PS: I think `control_variables` is fine, maybe `focus_variables` better reflects that it's not only about ""control"", but just a subset of variables (or observations) of interest. Let's not move forward on the suffix argument, your imposed structure will make things cleaner for people! :smile: `pct` is ok if in 0 - 100. No `obs_type` needed right now...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:540,testability,observ,observations,540,"> How about a var_type argument which defaults to ""genes""? Could even replace occurrences of cells with a obs_type variable. Sorry about the late response. The week passes so quickly and depending on how it goes, I can only go back to Scanpy during the weekends these days... I like your new table much better than the previous one. If you'd put that forward in a PR, I'd be super happy! PS: I think `control_variables` is fine, maybe `focus_variables` better reflects that it's not only about ""control"", but just a subset of variables (or observations) of interest. Let's not move forward on the suffix argument, your imposed structure will make things cleaner for people! :smile: `pct` is ok if in 0 - 100. No `obs_type` needed right now...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:97,availability,down,down,97,"No problem, gave a chance to optimize the code a bit (peak memory was about 3x AnnData size, now down to about 2x).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:29,energy efficiency,optim,optimize,29,"No problem, gave a chance to optimize the code a bit (peak memory was about 3x AnnData size, now down to about 2x).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:29,performance,optimiz,optimize,29,"No problem, gave a chance to optimize the code a bit (peak memory was about 3x AnnData size, now down to about 2x).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:59,performance,memor,memory,59,"No problem, gave a chance to optimize the code a bit (peak memory was about 3x AnnData size, now down to about 2x).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:59,usability,memor,memory,59,"No problem, gave a chance to optimize the code a bit (peak memory was about 3x AnnData size, now down to about 2x).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/issues/317:44,availability,toler,tolerance,44,The only solution so far is to increase the tolerance threshold of the tests! I don't know where those differences come from. Is always a problem. I will be very glad if you find a better solution.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:44,reliability,toleran,tolerance,44,The only solution so far is to increase the tolerance threshold of the tests! I don't know where those differences come from. Is always a problem. I will be very glad if you find a better solution.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:71,safety,test,tests,71,The only solution so far is to increase the tolerance threshold of the tests! I don't know where those differences come from. Is always a problem. I will be very glad if you find a better solution.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:71,testability,test,tests,71,The only solution so far is to increase the tolerance threshold of the tests! I don't know where those differences come from. Is always a problem. I will be very glad if you find a better solution.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:31,availability,error,errors,31,"I don't remember getting these errors before. Are you not getting them now, and did you get them before? Also, where did the images in the repo get generated?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:31,performance,error,errors,31,"I don't remember getting these errors before. Are you not getting them now, and did you get them before? Also, where did the images in the repo get generated?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:8,safety,reme,remember,8,"I don't remember getting these errors before. Are you not getting them now, and did you get them before? Also, where did the images in the repo get generated?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:31,safety,error,errors,31,"I don't remember getting these errors before. Are you not getting them now, and did you get them before? Also, where did the images in the repo get generated?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:31,usability,error,errors,31,"I don't remember getting these errors before. Are you not getting them now, and did you get them before? Also, where did the images in the repo get generated?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:67,deployability,version,versions,67,I think that differences in underlying libraries and in matplotlib versions cause this small changes.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:67,integrability,version,versions,67,I think that differences in underlying libraries and in matplotlib versions cause this small changes.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:67,modifiability,version,versions,67,I think that differences in underlying libraries and in matplotlib versions cause this small changes.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:48,interoperability,specif,specific,48,"Hm, the tests work for me. And I never set up a specific environment. Obviously, they also run through on Travis.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:8,safety,test,tests,8,"Hm, the tests work for me. And I never set up a specific environment. Obviously, they also run through on Travis.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:8,testability,test,tests,8,"Hm, the tests work for me. And I never set up a specific environment. Obviously, they also run through on Travis.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:106,deployability,version,version,106,"Could you send me some info about your environment so I can try to diagnose this? I'm thinking matplotlib version, matplotlibrc, and os info. [Here's my current pipfile](https://gist.github.com/ivirshup/6de8273535e3f5ec9734e5a3efc5df6a).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:153,energy efficiency,current,current,153,"Could you send me some info about your environment so I can try to diagnose this? I'm thinking matplotlib version, matplotlibrc, and os info. [Here's my current pipfile](https://gist.github.com/ivirshup/6de8273535e3f5ec9734e5a3efc5df6a).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:106,integrability,version,version,106,"Could you send me some info about your environment so I can try to diagnose this? I'm thinking matplotlib version, matplotlibrc, and os info. [Here's my current pipfile](https://gist.github.com/ivirshup/6de8273535e3f5ec9734e5a3efc5df6a).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:106,modifiability,version,version,106,"Could you send me some info about your environment so I can try to diagnose this? I'm thinking matplotlib version, matplotlibrc, and os info. [Here's my current pipfile](https://gist.github.com/ivirshup/6de8273535e3f5ec9734e5a3efc5df6a).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:67,reliability,diagno,diagnose,67,"Could you send me some info about your environment so I can try to diagnose this? I'm thinking matplotlib version, matplotlibrc, and os info. [Here's my current pipfile](https://gist.github.com/ivirshup/6de8273535e3f5ec9734e5a3efc5df6a).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:67,testability,diagno,diagnose,67,"Could you send me some info about your environment so I can try to diagnose this? I'm thinking matplotlib version, matplotlibrc, and os info. [Here's my current pipfile](https://gist.github.com/ivirshup/6de8273535e3f5ec9734e5a3efc5df6a).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:26,safety,test,tests,26,"I've been able to get the tests to pass on a different machine (this one running linux). I can get rid of most of the discrepancies between plots between the two machines by replacing:. ```python. mpl.use(""agg""). ```. with. ```python. from matplotlib.testing import setup. setup(). ```. However violin plots still get a large RMS value.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:251,safety,test,testing,251,"I've been able to get the tests to pass on a different machine (this one running linux). I can get rid of most of the discrepancies between plots between the two machines by replacing:. ```python. mpl.use(""agg""). ```. with. ```python. from matplotlib.testing import setup. setup(). ```. However violin plots still get a large RMS value.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:26,testability,test,tests,26,"I've been able to get the tests to pass on a different machine (this one running linux). I can get rid of most of the discrepancies between plots between the two machines by replacing:. ```python. mpl.use(""agg""). ```. with. ```python. from matplotlib.testing import setup. setup(). ```. However violin plots still get a large RMS value.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:251,testability,test,testing,251,"I've been able to get the tests to pass on a different machine (this one running linux). I can get rid of most of the discrepancies between plots between the two machines by replacing:. ```python. mpl.use(""agg""). ```. with. ```python. from matplotlib.testing import setup. setup(). ```. However violin plots still get a large RMS value.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:126,deployability,fail,fail,126,"For me, exactly the opposite happens; adding. ```. from matplotlib.testing import setup. setup(). ```. makes almost all tests fail. If I use it from the beginning and produce all test images with it, then, of course, it works.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:126,reliability,fail,fail,126,"For me, exactly the opposite happens; adding. ```. from matplotlib.testing import setup. setup(). ```. makes almost all tests fail. If I use it from the beginning and produce all test images with it, then, of course, it works.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:67,safety,test,testing,67,"For me, exactly the opposite happens; adding. ```. from matplotlib.testing import setup. setup(). ```. makes almost all tests fail. If I use it from the beginning and produce all test images with it, then, of course, it works.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:120,safety,test,tests,120,"For me, exactly the opposite happens; adding. ```. from matplotlib.testing import setup. setup(). ```. makes almost all tests fail. If I use it from the beginning and produce all test images with it, then, of course, it works.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:179,safety,test,test,179,"For me, exactly the opposite happens; adding. ```. from matplotlib.testing import setup. setup(). ```. makes almost all tests fail. If I use it from the beginning and produce all test images with it, then, of course, it works.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:67,testability,test,testing,67,"For me, exactly the opposite happens; adding. ```. from matplotlib.testing import setup. setup(). ```. makes almost all tests fail. If I use it from the beginning and produce all test images with it, then, of course, it works.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:120,testability,test,tests,120,"For me, exactly the opposite happens; adding. ```. from matplotlib.testing import setup. setup(). ```. makes almost all tests fail. If I use it from the beginning and produce all test images with it, then, of course, it works.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:179,testability,test,test,179,"For me, exactly the opposite happens; adding. ```. from matplotlib.testing import setup. setup(). ```. makes almost all tests fail. If I use it from the beginning and produce all test images with it, then, of course, it works.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:307,energy efficiency,current,current,307,"Hm, I've wanted to add a PAGA notebook to the tests and I'm struggling to get it to run on travis. The tests run through for me on a MacBook and on a remote linux machine, but travis seems even to be able to produce differently-shaped output images: https://travis-ci.org/theislab/scanpy/jobs/450416143. My current suspicion is that networkx does something strange as it really only affects the graph plot... I'll investigate further, but if you've seen this already, a hint would be very welcome! ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:333,performance,network,networkx,333,"Hm, I've wanted to add a PAGA notebook to the tests and I'm struggling to get it to run on travis. The tests run through for me on a MacBook and on a remote linux machine, but travis seems even to be able to produce differently-shaped output images: https://travis-ci.org/theislab/scanpy/jobs/450416143. My current suspicion is that networkx does something strange as it really only affects the graph plot... I'll investigate further, but if you've seen this already, a hint would be very welcome! ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:342,reliability,doe,does,342,"Hm, I've wanted to add a PAGA notebook to the tests and I'm struggling to get it to run on travis. The tests run through for me on a MacBook and on a remote linux machine, but travis seems even to be able to produce differently-shaped output images: https://travis-ci.org/theislab/scanpy/jobs/450416143. My current suspicion is that networkx does something strange as it really only affects the graph plot... I'll investigate further, but if you've seen this already, a hint would be very welcome! ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:46,safety,test,tests,46,"Hm, I've wanted to add a PAGA notebook to the tests and I'm struggling to get it to run on travis. The tests run through for me on a MacBook and on a remote linux machine, but travis seems even to be able to produce differently-shaped output images: https://travis-ci.org/theislab/scanpy/jobs/450416143. My current suspicion is that networkx does something strange as it really only affects the graph plot... I'll investigate further, but if you've seen this already, a hint would be very welcome! ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:103,safety,test,tests,103,"Hm, I've wanted to add a PAGA notebook to the tests and I'm struggling to get it to run on travis. The tests run through for me on a MacBook and on a remote linux machine, but travis seems even to be able to produce differently-shaped output images: https://travis-ci.org/theislab/scanpy/jobs/450416143. My current suspicion is that networkx does something strange as it really only affects the graph plot... I'll investigate further, but if you've seen this already, a hint would be very welcome! ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:333,security,network,networkx,333,"Hm, I've wanted to add a PAGA notebook to the tests and I'm struggling to get it to run on travis. The tests run through for me on a MacBook and on a remote linux machine, but travis seems even to be able to produce differently-shaped output images: https://travis-ci.org/theislab/scanpy/jobs/450416143. My current suspicion is that networkx does something strange as it really only affects the graph plot... I'll investigate further, but if you've seen this already, a hint would be very welcome! ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:46,testability,test,tests,46,"Hm, I've wanted to add a PAGA notebook to the tests and I'm struggling to get it to run on travis. The tests run through for me on a MacBook and on a remote linux machine, but travis seems even to be able to produce differently-shaped output images: https://travis-ci.org/theislab/scanpy/jobs/450416143. My current suspicion is that networkx does something strange as it really only affects the graph plot... I'll investigate further, but if you've seen this already, a hint would be very welcome! ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:103,testability,test,tests,103,"Hm, I've wanted to add a PAGA notebook to the tests and I'm struggling to get it to run on travis. The tests run through for me on a MacBook and on a remote linux machine, but travis seems even to be able to produce differently-shaped output images: https://travis-ci.org/theislab/scanpy/jobs/450416143. My current suspicion is that networkx does something strange as it really only affects the graph plot... I'll investigate further, but if you've seen this already, a hint would be very welcome! ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:470,usability,hint,hint,470,"Hm, I've wanted to add a PAGA notebook to the tests and I'm struggling to get it to run on travis. The tests run through for me on a MacBook and on a remote linux machine, but travis seems even to be able to produce differently-shaped output images: https://travis-ci.org/theislab/scanpy/jobs/450416143. My current suspicion is that networkx does something strange as it really only affects the graph plot... I'll investigate further, but if you've seen this already, a hint would be very welcome! ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:222,availability,error,errors,222,"Ah, yeah that's what I meant. If I use `setup()`, the tests on the linux server fail. However, the images generated are similar (RMSD < 10) to images made on my MacBook after running `setup()`. On the PAGA notebook, I saw errors like that when I was playing around with the dpi. Maybe fig size or dpi is being changed?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:80,deployability,fail,fail,80,"Ah, yeah that's what I meant. If I use `setup()`, the tests on the linux server fail. However, the images generated are similar (RMSD < 10) to images made on my MacBook after running `setup()`. On the PAGA notebook, I saw errors like that when I was playing around with the dpi. Maybe fig size or dpi is being changed?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:222,performance,error,errors,222,"Ah, yeah that's what I meant. If I use `setup()`, the tests on the linux server fail. However, the images generated are similar (RMSD < 10) to images made on my MacBook after running `setup()`. On the PAGA notebook, I saw errors like that when I was playing around with the dpi. Maybe fig size or dpi is being changed?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:80,reliability,fail,fail,80,"Ah, yeah that's what I meant. If I use `setup()`, the tests on the linux server fail. However, the images generated are similar (RMSD < 10) to images made on my MacBook after running `setup()`. On the PAGA notebook, I saw errors like that when I was playing around with the dpi. Maybe fig size or dpi is being changed?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:54,safety,test,tests,54,"Ah, yeah that's what I meant. If I use `setup()`, the tests on the linux server fail. However, the images generated are similar (RMSD < 10) to images made on my MacBook after running `setup()`. On the PAGA notebook, I saw errors like that when I was playing around with the dpi. Maybe fig size or dpi is being changed?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:222,safety,error,errors,222,"Ah, yeah that's what I meant. If I use `setup()`, the tests on the linux server fail. However, the images generated are similar (RMSD < 10) to images made on my MacBook after running `setup()`. On the PAGA notebook, I saw errors like that when I was playing around with the dpi. Maybe fig size or dpi is being changed?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:54,testability,test,tests,54,"Ah, yeah that's what I meant. If I use `setup()`, the tests on the linux server fail. However, the images generated are similar (RMSD < 10) to images made on my MacBook after running `setup()`. On the PAGA notebook, I saw errors like that when I was playing around with the dpi. Maybe fig size or dpi is being changed?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:222,usability,error,errors,222,"Ah, yeah that's what I meant. If I use `setup()`, the tests on the linux server fail. However, the images generated are similar (RMSD < 10) to images made on my MacBook after running `setup()`. On the PAGA notebook, I saw errors like that when I was playing around with the dpi. Maybe fig size or dpi is being changed?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:215,availability,reliab,reliable,215,"OK! Thanks! @fidelram Should we simply regenerate all images using `matplotlib.testing.setup()`, which seems to be the most stable way to go and in the future restrict ourselves to that? I guess this is closer to a reliable test setup for all the images than the current solution via `mpl.use(""agg"")`. Also the name suggests that matplotlib does it this way. But you did some research at the time when introducing the first tests, right? Thanks for the comment on the PAGA notebook, too, @ivirshup. I'll make sure that I didn't hard-code anything into the plotting functions that might collide with anything else happening on travis... but it's astonishing... In the meanwhile I work-around with a data-base test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:263,energy efficiency,current,current,263,"OK! Thanks! @fidelram Should we simply regenerate all images using `matplotlib.testing.setup()`, which seems to be the most stable way to go and in the future restrict ourselves to that? I guess this is closer to a reliable test setup for all the images than the current solution via `mpl.use(""agg"")`. Also the name suggests that matplotlib does it this way. But you did some research at the time when introducing the first tests, right? Thanks for the comment on the PAGA notebook, too, @ivirshup. I'll make sure that I didn't hard-code anything into the plotting functions that might collide with anything else happening on travis... but it's astonishing... In the meanwhile I work-around with a data-base test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:392,performance,time,time,392,"OK! Thanks! @fidelram Should we simply regenerate all images using `matplotlib.testing.setup()`, which seems to be the most stable way to go and in the future restrict ourselves to that? I guess this is closer to a reliable test setup for all the images than the current solution via `mpl.use(""agg"")`. Also the name suggests that matplotlib does it this way. But you did some research at the time when introducing the first tests, right? Thanks for the comment on the PAGA notebook, too, @ivirshup. I'll make sure that I didn't hard-code anything into the plotting functions that might collide with anything else happening on travis... but it's astonishing... In the meanwhile I work-around with a data-base test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:215,reliability,reliab,reliable,215,"OK! Thanks! @fidelram Should we simply regenerate all images using `matplotlib.testing.setup()`, which seems to be the most stable way to go and in the future restrict ourselves to that? I guess this is closer to a reliable test setup for all the images than the current solution via `mpl.use(""agg"")`. Also the name suggests that matplotlib does it this way. But you did some research at the time when introducing the first tests, right? Thanks for the comment on the PAGA notebook, too, @ivirshup. I'll make sure that I didn't hard-code anything into the plotting functions that might collide with anything else happening on travis... but it's astonishing... In the meanwhile I work-around with a data-base test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:341,reliability,doe,does,341,"OK! Thanks! @fidelram Should we simply regenerate all images using `matplotlib.testing.setup()`, which seems to be the most stable way to go and in the future restrict ourselves to that? I guess this is closer to a reliable test setup for all the images than the current solution via `mpl.use(""agg"")`. Also the name suggests that matplotlib does it this way. But you did some research at the time when introducing the first tests, right? Thanks for the comment on the PAGA notebook, too, @ivirshup. I'll make sure that I didn't hard-code anything into the plotting functions that might collide with anything else happening on travis... but it's astonishing... In the meanwhile I work-around with a data-base test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:79,safety,test,testing,79,"OK! Thanks! @fidelram Should we simply regenerate all images using `matplotlib.testing.setup()`, which seems to be the most stable way to go and in the future restrict ourselves to that? I guess this is closer to a reliable test setup for all the images than the current solution via `mpl.use(""agg"")`. Also the name suggests that matplotlib does it this way. But you did some research at the time when introducing the first tests, right? Thanks for the comment on the PAGA notebook, too, @ivirshup. I'll make sure that I didn't hard-code anything into the plotting functions that might collide with anything else happening on travis... but it's astonishing... In the meanwhile I work-around with a data-base test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:224,safety,test,test,224,"OK! Thanks! @fidelram Should we simply regenerate all images using `matplotlib.testing.setup()`, which seems to be the most stable way to go and in the future restrict ourselves to that? I guess this is closer to a reliable test setup for all the images than the current solution via `mpl.use(""agg"")`. Also the name suggests that matplotlib does it this way. But you did some research at the time when introducing the first tests, right? Thanks for the comment on the PAGA notebook, too, @ivirshup. I'll make sure that I didn't hard-code anything into the plotting functions that might collide with anything else happening on travis... but it's astonishing... In the meanwhile I work-around with a data-base test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:424,safety,test,tests,424,"OK! Thanks! @fidelram Should we simply regenerate all images using `matplotlib.testing.setup()`, which seems to be the most stable way to go and in the future restrict ourselves to that? I guess this is closer to a reliable test setup for all the images than the current solution via `mpl.use(""agg"")`. Also the name suggests that matplotlib does it this way. But you did some research at the time when introducing the first tests, right? Thanks for the comment on the PAGA notebook, too, @ivirshup. I'll make sure that I didn't hard-code anything into the plotting functions that might collide with anything else happening on travis... but it's astonishing... In the meanwhile I work-around with a data-base test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:708,safety,test,test,708,"OK! Thanks! @fidelram Should we simply regenerate all images using `matplotlib.testing.setup()`, which seems to be the most stable way to go and in the future restrict ourselves to that? I guess this is closer to a reliable test setup for all the images than the current solution via `mpl.use(""agg"")`. Also the name suggests that matplotlib does it this way. But you did some research at the time when introducing the first tests, right? Thanks for the comment on the PAGA notebook, too, @ivirshup. I'll make sure that I didn't hard-code anything into the plotting functions that might collide with anything else happening on travis... but it's astonishing... In the meanwhile I work-around with a data-base test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:32,testability,simpl,simply,32,"OK! Thanks! @fidelram Should we simply regenerate all images using `matplotlib.testing.setup()`, which seems to be the most stable way to go and in the future restrict ourselves to that? I guess this is closer to a reliable test setup for all the images than the current solution via `mpl.use(""agg"")`. Also the name suggests that matplotlib does it this way. But you did some research at the time when introducing the first tests, right? Thanks for the comment on the PAGA notebook, too, @ivirshup. I'll make sure that I didn't hard-code anything into the plotting functions that might collide with anything else happening on travis... but it's astonishing... In the meanwhile I work-around with a data-base test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:79,testability,test,testing,79,"OK! Thanks! @fidelram Should we simply regenerate all images using `matplotlib.testing.setup()`, which seems to be the most stable way to go and in the future restrict ourselves to that? I guess this is closer to a reliable test setup for all the images than the current solution via `mpl.use(""agg"")`. Also the name suggests that matplotlib does it this way. But you did some research at the time when introducing the first tests, right? Thanks for the comment on the PAGA notebook, too, @ivirshup. I'll make sure that I didn't hard-code anything into the plotting functions that might collide with anything else happening on travis... but it's astonishing... In the meanwhile I work-around with a data-base test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:224,testability,test,test,224,"OK! Thanks! @fidelram Should we simply regenerate all images using `matplotlib.testing.setup()`, which seems to be the most stable way to go and in the future restrict ourselves to that? I guess this is closer to a reliable test setup for all the images than the current solution via `mpl.use(""agg"")`. Also the name suggests that matplotlib does it this way. But you did some research at the time when introducing the first tests, right? Thanks for the comment on the PAGA notebook, too, @ivirshup. I'll make sure that I didn't hard-code anything into the plotting functions that might collide with anything else happening on travis... but it's astonishing... In the meanwhile I work-around with a data-base test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:424,testability,test,tests,424,"OK! Thanks! @fidelram Should we simply regenerate all images using `matplotlib.testing.setup()`, which seems to be the most stable way to go and in the future restrict ourselves to that? I guess this is closer to a reliable test setup for all the images than the current solution via `mpl.use(""agg"")`. Also the name suggests that matplotlib does it this way. But you did some research at the time when introducing the first tests, right? Thanks for the comment on the PAGA notebook, too, @ivirshup. I'll make sure that I didn't hard-code anything into the plotting functions that might collide with anything else happening on travis... but it's astonishing... In the meanwhile I work-around with a data-base test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:708,testability,test,test,708,"OK! Thanks! @fidelram Should we simply regenerate all images using `matplotlib.testing.setup()`, which seems to be the most stable way to go and in the future restrict ourselves to that? I guess this is closer to a reliable test setup for all the images than the current solution via `mpl.use(""agg"")`. Also the name suggests that matplotlib does it this way. But you did some research at the time when introducing the first tests, right? Thanks for the comment on the PAGA notebook, too, @ivirshup. I'll make sure that I didn't hard-code anything into the plotting functions that might collide with anything else happening on travis... but it's astonishing... In the meanwhile I work-around with a data-base test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:32,usability,simpl,simply,32,"OK! Thanks! @fidelram Should we simply regenerate all images using `matplotlib.testing.setup()`, which seems to be the most stable way to go and in the future restrict ourselves to that? I guess this is closer to a reliable test setup for all the images than the current solution via `mpl.use(""agg"")`. Also the name suggests that matplotlib does it this way. But you did some research at the time when introducing the first tests, right? Thanks for the comment on the PAGA notebook, too, @ivirshup. I'll make sure that I didn't hard-code anything into the plotting functions that might collide with anything else happening on travis... but it's astonishing... In the meanwhile I work-around with a data-base test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:203,usability,close,closer,203,"OK! Thanks! @fidelram Should we simply regenerate all images using `matplotlib.testing.setup()`, which seems to be the most stable way to go and in the future restrict ourselves to that? I guess this is closer to a reliable test setup for all the images than the current solution via `mpl.use(""agg"")`. Also the name suggests that matplotlib does it this way. But you did some research at the time when introducing the first tests, right? Thanks for the comment on the PAGA notebook, too, @ivirshup. I'll make sure that I didn't hard-code anything into the plotting functions that might collide with anything else happening on travis... but it's astonishing... In the meanwhile I work-around with a data-base test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:37,security,auth,author,37,"PS: @ivirshup, I'm adding you to the author list. ;)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:21,safety,test,testing,21,I didn't know about `testing.setup()` I will take a look. Seems very promising.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:21,testability,test,testing,21,I didn't know about `testing.setup()` I will take a look. Seems very promising.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:18,availability,error,errors,18,No longer getting errors on plotting tests. Was this being actively worked on? I think it's ready to close otherwise.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:18,performance,error,errors,18,No longer getting errors on plotting tests. Was this being actively worked on? I think it's ready to close otherwise.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:18,safety,error,errors,18,No longer getting errors on plotting tests. Was this being actively worked on? I think it's ready to close otherwise.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:37,safety,test,tests,37,No longer getting errors on plotting tests. Was this being actively worked on? I think it's ready to close otherwise.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:37,testability,test,tests,37,No longer getting errors on plotting tests. Was this being actively worked on? I think it's ready to close otherwise.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:18,usability,error,errors,18,No longer getting errors on plotting tests. Was this being actively worked on? I think it's ready to close otherwise.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:101,usability,close,close,101,No longer getting errors on plotting tests. Was this being actively worked on? I think it's ready to close otherwise.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:11,safety,test,tests,11,Indeed the tests have been modified and now `testing.setup` is being used. Thanks for the tip. You can close this issue.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:45,safety,test,testing,45,Indeed the tests have been modified and now `testing.setup` is being used. Thanks for the tip. You can close this issue.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:27,security,modif,modified,27,Indeed the tests have been modified and now `testing.setup` is being used. Thanks for the tip. You can close this issue.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:11,testability,test,tests,11,Indeed the tests have been modified and now `testing.setup` is being used. Thanks for the tip. You can close this issue.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:45,testability,test,testing,45,Indeed the tests have been modified and now `testing.setup` is being used. Thanks for the tip. You can close this issue.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:90,usability,tip,tip,90,Indeed the tests have been modified and now `testing.setup` is being used. Thanks for the tip. You can close this issue.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:103,usability,close,close,103,Indeed the tests have been modified and now `testing.setup` is being used. Thanks for the tip. You can close this issue.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:245,integrability,coupl,couple,245,"Yeah, @fidelram added it in this PR: https://github.com/theislab/scanpy/pull/369. :smile: It's awesome, tests run through for me, too, everywhere... . PS: Only thing that remains is the unnecessarily long runtime due to too-large test data in a couple of instances. I'll take care of it very soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:245,modifiability,coupl,couple,245,"Yeah, @fidelram added it in this PR: https://github.com/theislab/scanpy/pull/369. :smile: It's awesome, tests run through for me, too, everywhere... . PS: Only thing that remains is the unnecessarily long runtime due to too-large test data in a couple of instances. I'll take care of it very soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:104,safety,test,tests,104,"Yeah, @fidelram added it in this PR: https://github.com/theislab/scanpy/pull/369. :smile: It's awesome, tests run through for me, too, everywhere... . PS: Only thing that remains is the unnecessarily long runtime due to too-large test data in a couple of instances. I'll take care of it very soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:230,safety,test,test,230,"Yeah, @fidelram added it in this PR: https://github.com/theislab/scanpy/pull/369. :smile: It's awesome, tests run through for me, too, everywhere... . PS: Only thing that remains is the unnecessarily long runtime due to too-large test data in a couple of instances. I'll take care of it very soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:104,testability,test,tests,104,"Yeah, @fidelram added it in this PR: https://github.com/theislab/scanpy/pull/369. :smile: It's awesome, tests run through for me, too, everywhere... . PS: Only thing that remains is the unnecessarily long runtime due to too-large test data in a couple of instances. I'll take care of it very soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:230,testability,test,test,230,"Yeah, @fidelram added it in this PR: https://github.com/theislab/scanpy/pull/369. :smile: It's awesome, tests run through for me, too, everywhere... . PS: Only thing that remains is the unnecessarily long runtime due to too-large test data in a couple of instances. I'll take care of it very soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/317:245,testability,coupl,couple,245,"Yeah, @fidelram added it in this PR: https://github.com/theislab/scanpy/pull/369. :smile: It's awesome, tests run through for me, too, everywhere... . PS: Only thing that remains is the unnecessarily long runtime due to too-large test data in a couple of instances. I'll take care of it very soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/317
https://github.com/scverse/scanpy/issues/318:15,modifiability,variab,variable,15,"Very strange. `variable` is some assigned name after an internal `pandas.melt`. . First, I would not recommend to plot all `adata.var_names` unless they are fewer (<30). But that seems not to be the problem. To discard a problem with seaborn violin plot, can you try `sc.pl.matrixplot` instead? Also, do you get the same output in both cases after. ```. adata.obs.head(). ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/318
https://github.com/scverse/scanpy/issues/319:40,availability,cluster,clusters,40,"This is normal, means that the far away clusters are ""globally"" more different from the cells that are closer together. UMAP is one way of preserving the global distance, whereas tSNE is pretty much ignorant of the global distance (so one should not consider global distance to make inferences from tSNE plot). I frequently see the UMAP when some very different contaminating cell types are in the sample.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/319
https://github.com/scverse/scanpy/issues/319:40,deployability,cluster,clusters,40,"This is normal, means that the far away clusters are ""globally"" more different from the cells that are closer together. UMAP is one way of preserving the global distance, whereas tSNE is pretty much ignorant of the global distance (so one should not consider global distance to make inferences from tSNE plot). I frequently see the UMAP when some very different contaminating cell types are in the sample.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/319
https://github.com/scverse/scanpy/issues/319:103,usability,close,closer,103,"This is normal, means that the far away clusters are ""globally"" more different from the cells that are closer together. UMAP is one way of preserving the global distance, whereas tSNE is pretty much ignorant of the global distance (so one should not consider global distance to make inferences from tSNE plot). I frequently see the UMAP when some very different contaminating cell types are in the sample.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/319
https://github.com/scverse/scanpy/issues/319:39,availability,cluster,clusters,39,"UMAP also has no meaning attached when clusters are completely disconnected (Supplemental Figure 10 of [this](https://rawgit.com/falexwolf/paga_paper/master/paga.pdf), soon updated on [here](https://doi.org/10.1101/208819) on bioRxiv and finally in a journal...); and I'd tend to think that this is such a case. Then, UMAP's parameters have to be adjusted (mostly `min_disd` and `spread`). It's true that UMAP has less tendency to tear apart connected things than tSNE. Overall, it's more faithful to the global topology.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/319
https://github.com/scverse/scanpy/issues/319:39,deployability,cluster,clusters,39,"UMAP also has no meaning attached when clusters are completely disconnected (Supplemental Figure 10 of [this](https://rawgit.com/falexwolf/paga_paper/master/paga.pdf), soon updated on [here](https://doi.org/10.1101/208819) on bioRxiv and finally in a journal...); and I'd tend to think that this is such a case. Then, UMAP's parameters have to be adjusted (mostly `min_disd` and `spread`). It's true that UMAP has less tendency to tear apart connected things than tSNE. Overall, it's more faithful to the global topology.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/319
https://github.com/scverse/scanpy/issues/319:173,deployability,updat,updated,173,"UMAP also has no meaning attached when clusters are completely disconnected (Supplemental Figure 10 of [this](https://rawgit.com/falexwolf/paga_paper/master/paga.pdf), soon updated on [here](https://doi.org/10.1101/208819) on bioRxiv and finally in a journal...); and I'd tend to think that this is such a case. Then, UMAP's parameters have to be adjusted (mostly `min_disd` and `spread`). It's true that UMAP has less tendency to tear apart connected things than tSNE. Overall, it's more faithful to the global topology.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/319
https://github.com/scverse/scanpy/issues/319:325,modifiability,paramet,parameters,325,"UMAP also has no meaning attached when clusters are completely disconnected (Supplemental Figure 10 of [this](https://rawgit.com/falexwolf/paga_paper/master/paga.pdf), soon updated on [here](https://doi.org/10.1101/208819) on bioRxiv and finally in a journal...); and I'd tend to think that this is such a case. Then, UMAP's parameters have to be adjusted (mostly `min_disd` and `spread`). It's true that UMAP has less tendency to tear apart connected things than tSNE. Overall, it's more faithful to the global topology.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/319
https://github.com/scverse/scanpy/issues/319:52,safety,compl,completely,52,"UMAP also has no meaning attached when clusters are completely disconnected (Supplemental Figure 10 of [this](https://rawgit.com/falexwolf/paga_paper/master/paga.pdf), soon updated on [here](https://doi.org/10.1101/208819) on bioRxiv and finally in a journal...); and I'd tend to think that this is such a case. Then, UMAP's parameters have to be adjusted (mostly `min_disd` and `spread`). It's true that UMAP has less tendency to tear apart connected things than tSNE. Overall, it's more faithful to the global topology.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/319
https://github.com/scverse/scanpy/issues/319:173,safety,updat,updated,173,"UMAP also has no meaning attached when clusters are completely disconnected (Supplemental Figure 10 of [this](https://rawgit.com/falexwolf/paga_paper/master/paga.pdf), soon updated on [here](https://doi.org/10.1101/208819) on bioRxiv and finally in a journal...); and I'd tend to think that this is such a case. Then, UMAP's parameters have to be adjusted (mostly `min_disd` and `spread`). It's true that UMAP has less tendency to tear apart connected things than tSNE. Overall, it's more faithful to the global topology.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/319
https://github.com/scverse/scanpy/issues/319:52,security,compl,completely,52,"UMAP also has no meaning attached when clusters are completely disconnected (Supplemental Figure 10 of [this](https://rawgit.com/falexwolf/paga_paper/master/paga.pdf), soon updated on [here](https://doi.org/10.1101/208819) on bioRxiv and finally in a journal...); and I'd tend to think that this is such a case. Then, UMAP's parameters have to be adjusted (mostly `min_disd` and `spread`). It's true that UMAP has less tendency to tear apart connected things than tSNE. Overall, it's more faithful to the global topology.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/319
https://github.com/scverse/scanpy/issues/319:173,security,updat,updated,173,"UMAP also has no meaning attached when clusters are completely disconnected (Supplemental Figure 10 of [this](https://rawgit.com/falexwolf/paga_paper/master/paga.pdf), soon updated on [here](https://doi.org/10.1101/208819) on bioRxiv and finally in a journal...); and I'd tend to think that this is such a case. Then, UMAP's parameters have to be adjusted (mostly `min_disd` and `spread`). It's true that UMAP has less tendency to tear apart connected things than tSNE. Overall, it's more faithful to the global topology.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/319
https://github.com/scverse/scanpy/issues/319:283,interoperability,specif,specifics,283,"@falexwolf Just out of curiosity, have you compared your method with PHATE? (https://www.biorxiv.org/content/early/2017/03/24/120378 ). I have yet to try out PAGA but have found PHATE working fairly well of showing the trajectory inference. (I am just a biologist, so don't know the specifics of comparing methodologies)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/319
https://github.com/scverse/scanpy/issues/319:101,performance,content,content,101,"@falexwolf Just out of curiosity, have you compared your method with PHATE? (https://www.biorxiv.org/content/early/2017/03/24/120378 ). I have yet to try out PAGA but have found PHATE working fairly well of showing the trajectory inference. (I am just a biologist, so don't know the specifics of comparing methodologies)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/319
https://github.com/scverse/scanpy/issues/319:142,deployability,continu,continue,142,"Thank you all for your feedback here - that was helpful. I'll close this so it doesn't look like an issue needs to be handled, but please, do continue any discussion.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/319
https://github.com/scverse/scanpy/issues/319:79,reliability,doe,doesn,79,"Thank you all for your feedback here - that was helpful. I'll close this so it doesn't look like an issue needs to be handled, but please, do continue any discussion.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/319
https://github.com/scverse/scanpy/issues/319:23,usability,feedback,feedback,23,"Thank you all for your feedback here - that was helpful. I'll close this so it doesn't look like an issue needs to be handled, but please, do continue any discussion.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/319
https://github.com/scverse/scanpy/issues/319:48,usability,help,helpful,48,"Thank you all for your feedback here - that was helpful. I'll close this so it doesn't look like an issue needs to be handled, but please, do continue any discussion.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/319
https://github.com/scverse/scanpy/issues/319:62,usability,close,close,62,"Thank you all for your feedback here - that was helpful. I'll close this so it doesn't look like an issue needs to be handled, but please, do continue any discussion.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/319
https://github.com/scverse/scanpy/issues/320:41,deployability,updat,updating,41,That seems to be a loompy issue. Doesn't updating loompy help?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/320
https://github.com/scverse/scanpy/issues/320:33,reliability,Doe,Doesn,33,That seems to be a loompy issue. Doesn't updating loompy help?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/320
https://github.com/scverse/scanpy/issues/320:41,safety,updat,updating,41,That seems to be a loompy issue. Doesn't updating loompy help?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/320
https://github.com/scverse/scanpy/issues/320:41,security,updat,updating,41,That seems to be a loompy issue. Doesn't updating loompy help?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/320
https://github.com/scverse/scanpy/issues/320:57,usability,help,help,57,That seems to be a loompy issue. Doesn't updating loompy help?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/320
https://github.com/scverse/scanpy/issues/320:98,deployability,updat,update,98,"Hi Alex, thank you for your quick response! I contacted Sten over at loompy and he just pushed an update that allows for a little more flexibility when reading in loom files. It now works for me. The change he applied only applies to the loompy function `loompy.connect`, so I think I would still get this same problem when using scanpy function `read_loom`. I have the latest version of loompy. I don't think this is something that necessarily needs to be fixed on your end. It sounds like the loom format has changed a little bit, and maybe the people who made the loom file I was using did not follow all the rules when making the file. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/320
https://github.com/scverse/scanpy/issues/320:377,deployability,version,version,377,"Hi Alex, thank you for your quick response! I contacted Sten over at loompy and he just pushed an update that allows for a little more flexibility when reading in loom files. It now works for me. The change he applied only applies to the loompy function `loompy.connect`, so I think I would still get this same problem when using scanpy function `read_loom`. I have the latest version of loompy. I don't think this is something that necessarily needs to be fixed on your end. It sounds like the loom format has changed a little bit, and maybe the people who made the loom file I was using did not follow all the rules when making the file. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/320
https://github.com/scverse/scanpy/issues/320:377,integrability,version,version,377,"Hi Alex, thank you for your quick response! I contacted Sten over at loompy and he just pushed an update that allows for a little more flexibility when reading in loom files. It now works for me. The change he applied only applies to the loompy function `loompy.connect`, so I think I would still get this same problem when using scanpy function `read_loom`. I have the latest version of loompy. I don't think this is something that necessarily needs to be fixed on your end. It sounds like the loom format has changed a little bit, and maybe the people who made the loom file I was using did not follow all the rules when making the file. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/320
https://github.com/scverse/scanpy/issues/320:500,interoperability,format,format,500,"Hi Alex, thank you for your quick response! I contacted Sten over at loompy and he just pushed an update that allows for a little more flexibility when reading in loom files. It now works for me. The change he applied only applies to the loompy function `loompy.connect`, so I think I would still get this same problem when using scanpy function `read_loom`. I have the latest version of loompy. I don't think this is something that necessarily needs to be fixed on your end. It sounds like the loom format has changed a little bit, and maybe the people who made the loom file I was using did not follow all the rules when making the file. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/320
https://github.com/scverse/scanpy/issues/320:377,modifiability,version,version,377,"Hi Alex, thank you for your quick response! I contacted Sten over at loompy and he just pushed an update that allows for a little more flexibility when reading in loom files. It now works for me. The change he applied only applies to the loompy function `loompy.connect`, so I think I would still get this same problem when using scanpy function `read_loom`. I have the latest version of loompy. I don't think this is something that necessarily needs to be fixed on your end. It sounds like the loom format has changed a little bit, and maybe the people who made the loom file I was using did not follow all the rules when making the file. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/320
https://github.com/scverse/scanpy/issues/320:98,safety,updat,update,98,"Hi Alex, thank you for your quick response! I contacted Sten over at loompy and he just pushed an update that allows for a little more flexibility when reading in loom files. It now works for me. The change he applied only applies to the loompy function `loompy.connect`, so I think I would still get this same problem when using scanpy function `read_loom`. I have the latest version of loompy. I don't think this is something that necessarily needs to be fixed on your end. It sounds like the loom format has changed a little bit, and maybe the people who made the loom file I was using did not follow all the rules when making the file. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/320
https://github.com/scverse/scanpy/issues/320:98,security,updat,update,98,"Hi Alex, thank you for your quick response! I contacted Sten over at loompy and he just pushed an update that allows for a little more flexibility when reading in loom files. It now works for me. The change he applied only applies to the loompy function `loompy.connect`, so I think I would still get this same problem when using scanpy function `read_loom`. I have the latest version of loompy. I don't think this is something that necessarily needs to be fixed on your end. It sounds like the loom format has changed a little bit, and maybe the people who made the loom file I was using did not follow all the rules when making the file. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/320
https://github.com/scverse/scanpy/pull/321:72,deployability,updat,updates,72,"Thank you! One further thing to consider: with all these frequent image updates the repository will at some point explode in size. In all the image-based tests, we should use the smallest sizes possible. Images are already relatively small, but we can further reduce the size in the future. No necessary to remake all of them now, but something to keep in mind for future PRs. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/321
https://github.com/scverse/scanpy/pull/321:260,energy efficiency,reduc,reduce,260,"Thank you! One further thing to consider: with all these frequent image updates the repository will at some point explode in size. In all the image-based tests, we should use the smallest sizes possible. Images are already relatively small, but we can further reduce the size in the future. No necessary to remake all of them now, but something to keep in mind for future PRs. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/321
https://github.com/scverse/scanpy/pull/321:84,integrability,repositor,repository,84,"Thank you! One further thing to consider: with all these frequent image updates the repository will at some point explode in size. In all the image-based tests, we should use the smallest sizes possible. Images are already relatively small, but we can further reduce the size in the future. No necessary to remake all of them now, but something to keep in mind for future PRs. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/321
https://github.com/scverse/scanpy/pull/321:84,interoperability,repositor,repository,84,"Thank you! One further thing to consider: with all these frequent image updates the repository will at some point explode in size. In all the image-based tests, we should use the smallest sizes possible. Images are already relatively small, but we can further reduce the size in the future. No necessary to remake all of them now, but something to keep in mind for future PRs. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/321
https://github.com/scverse/scanpy/pull/321:72,safety,updat,updates,72,"Thank you! One further thing to consider: with all these frequent image updates the repository will at some point explode in size. In all the image-based tests, we should use the smallest sizes possible. Images are already relatively small, but we can further reduce the size in the future. No necessary to remake all of them now, but something to keep in mind for future PRs. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/321
https://github.com/scverse/scanpy/pull/321:154,safety,test,tests,154,"Thank you! One further thing to consider: with all these frequent image updates the repository will at some point explode in size. In all the image-based tests, we should use the smallest sizes possible. Images are already relatively small, but we can further reduce the size in the future. No necessary to remake all of them now, but something to keep in mind for future PRs. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/321
https://github.com/scverse/scanpy/pull/321:72,security,updat,updates,72,"Thank you! One further thing to consider: with all these frequent image updates the repository will at some point explode in size. In all the image-based tests, we should use the smallest sizes possible. Images are already relatively small, but we can further reduce the size in the future. No necessary to remake all of them now, but something to keep in mind for future PRs. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/321
https://github.com/scverse/scanpy/pull/321:154,testability,test,tests,154,"Thank you! One further thing to consider: with all these frequent image updates the repository will at some point explode in size. In all the image-based tests, we should use the smallest sizes possible. Images are already relatively small, but we can further reduce the size in the future. No necessary to remake all of them now, but something to keep in mind for future PRs. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/321
https://github.com/scverse/scanpy/issues/323:4111,availability,Error,Error,4111,"ce.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(. [125](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:125) filename,. [126](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:126) backed=backed,. [127](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:127) sheet=sheet,. [128](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:128) ext=ext,. ... [65](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:65) anno = anno.copy(deep=False). [66](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:66) if not is_string_dtype(anno.index):. ValueError: Observations annot. `var` must have as many rows as `X` has columns (1), but has 33538 rows. Error raised while reading key '' of <class 'h5py._hl.files.File'> from /. Output is truncated. View as a [scrollable element](command:cellOutput.enableScrolling?21de049a-219f-41e8-9d9d-5aabf61d8031) or open in a [text editor](command:workbench.action.openLargeOutput?21de049a-219f-41e8-9d9d-5aabf61d8031). Adjust cell output [settings](command:workbench.action.openSettings?%5B%22%40tag%3AnotebookOutputLayout%22%5D)... ```. This is my version:. ```. print(sc.__version__). print(ad.__version__). 1.10.0. 0.10.6. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:1006,deployability,resourc,resource,1006," this bug too. when i run:. ```. import scanpy as sc. import anndata as ad. adata = sc.read(filepath). ```. it turns out:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). Cell In[6], [line 8](vscode-notebook-cell:?execution_count=6&line=8). [6](vscode-notebook-cell:?execution_count=6&line=6) if filename.endswith('.h5ad'):. [7](vscode-notebook-cell:?execution_count=6&line=7) filepath = os.path.join(directory, filename). ----> [8](vscode-notebook-cell:?execution_count=6&line=8) adata = sc.read(filepath). [10](vscode-notebook-cell:?execution_count=6&line=10) # Rename columns with periods in `.obs` attribute. [11](vscode-notebook-cell:?execution_count=6&line=11) for col in adata.obs.columns:. File /data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). [77](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:77) @wraps(fn). [78](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:78) def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. [79](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:79) if len(args_all) <= n_positional:. ---> [80](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80) return fn(*args_all, **kw). [82](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:82) args_pos: P.args. [83](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/da",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:1182,deployability,resourc,resource,1182,"---------------------------. ValueError Traceback (most recent call last). Cell In[6], [line 8](vscode-notebook-cell:?execution_count=6&line=8). [6](vscode-notebook-cell:?execution_count=6&line=6) if filename.endswith('.h5ad'):. [7](vscode-notebook-cell:?execution_count=6&line=7) filepath = os.path.join(directory, filename). ----> [8](vscode-notebook-cell:?execution_count=6&line=8) adata = sc.read(filepath). [10](vscode-notebook-cell:?execution_count=6&line=10) # Rename columns with periods in `.obs` attribute. [11](vscode-notebook-cell:?execution_count=6&line=11) for col in adata.obs.columns:. File /data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). [77](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:77) @wraps(fn). [78](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:78) def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. [79](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:79) if len(args_all) <= n_positional:. ---> [80](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80) return fn(*args_all, **kw). [82](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:82) args_pos: P.args. [83](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:83) args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /data/LSY/venv/lib/python3",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:1406,deployability,resourc,resource,1406,"'):. [7](vscode-notebook-cell:?execution_count=6&line=7) filepath = os.path.join(directory, filename). ----> [8](vscode-notebook-cell:?execution_count=6&line=8) adata = sc.read(filepath). [10](vscode-notebook-cell:?execution_count=6&line=10) # Rename columns with periods in `.obs` attribute. [11](vscode-notebook-cell:?execution_count=6&line=11) for col in adata.obs.columns:. File /data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). [77](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:77) @wraps(fn). [78](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:78) def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. [79](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:79) if len(args_all) <= n_positional:. ---> [80](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80) return fn(*args_all, **kw). [82](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:82) args_pos: P.args. [83](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:83) args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs). [122](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vsco",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:1610,deployability,resourc,resource,1610,"book-cell:?execution_count=6&line=10) # Rename columns with periods in `.obs` attribute. [11](vscode-notebook-cell:?execution_count=6&line=11) for col in adata.obs.columns:. File /data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). [77](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:77) @wraps(fn). [78](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:78) def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. [79](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:79) if len(args_all) <= n_positional:. ---> [80](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80) return fn(*args_all, **kw). [82](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:82) args_pos: P.args. [83](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:83) args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs). [122](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:122) filename = Path(filename) # allow passing strings. [123](https://vscode-remote+ssh-002dremote-002baws-005fcpu",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:1802,deployability,resourc,resource,1802,"v/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). [77](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:77) @wraps(fn). [78](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:78) def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. [79](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:79) if len(args_all) <= n_positional:. ---> [80](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80) return fn(*args_all, **kw). [82](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:82) args_pos: P.args. [83](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:83) args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs). [122](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:122) filename = Path(filename) # allow passing strings. [123](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005f",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:1984,deployability,resourc,resource,1984,"ws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:77) @wraps(fn). [78](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:78) def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. [79](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:79) if len(args_all) <= n_positional:. ---> [80](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80) return fn(*args_all, **kw). [82](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:82) args_pos: P.args. [83](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:83) args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs). [122](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:122) filename = Path(filename) # allow passing strings. [123](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(. [125](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscod",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:2413,deployability,resourc,resource,2413,"code-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:79) if len(args_all) <= n_positional:. ---> [80](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80) return fn(*args_all, **kw). [82](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:82) args_pos: P.args. [83](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:83) args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs). [122](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:122) filename = Path(filename) # allow passing strings. [123](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(. [125](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:125) filename,. [126](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:126) backed=backed,. [127](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:2622,deployability,resourc,resource,2622,"cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80) return fn(*args_all, **kw). [82](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:82) args_pos: P.args. [83](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:83) args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs). [122](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:122) filename = Path(filename) # allow passing strings. [123](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(. [125](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:125) filename,. [126](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:126) backed=backed,. [127](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:127) sheet=sheet,. [128](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:128) ext=ext,. ... [65](https://vs",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:2817,deployability,resourc,resource,2817,".net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:82) args_pos: P.args. [83](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:83) args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs). [122](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:122) filename = Path(filename) # allow passing strings. [123](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(. [125](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:125) filename,. [126](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:126) backed=backed,. [127](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:127) sheet=sheet,. [128](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:128) ext=ext,. ... [65](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:65) anno = anno.copy(deep=False). [66](https://vsc",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:2990,deployability,resourc,resource,2990,"scode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:83) args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs). [122](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:122) filename = Path(filename) # allow passing strings. [123](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(. [125](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:125) filename,. [126](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:126) backed=backed,. [127](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:127) sheet=sheet,. [128](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:128) ext=ext,. ... [65](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:65) anno = anno.copy(deep=False). [66](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:66) if not is_string_dtype(an",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:3159,deployability,resourc,resource,3159,"ta/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs). [122](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:122) filename = Path(filename) # allow passing strings. [123](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(. [125](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:125) filename,. [126](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:126) backed=backed,. [127](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:127) sheet=sheet,. [128](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:128) ext=ext,. ... [65](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:65) anno = anno.copy(deep=False). [66](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:66) if not is_string_dtype(anno.index):. ValueError: Observations annot. `var` must have as many rows as `X` has columns (1), but has 33538 rows. Error raised while reading key '' of <class 'h5py._h",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:3333,deployability,resourc,resource,3333,"*kwargs). [122](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:122) filename = Path(filename) # allow passing strings. [123](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(. [125](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:125) filename,. [126](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:126) backed=backed,. [127](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:127) sheet=sheet,. [128](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:128) ext=ext,. ... [65](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:65) anno = anno.copy(deep=False). [66](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:66) if not is_string_dtype(anno.index):. ValueError: Observations annot. `var` must have as many rows as `X` has columns (1), but has 33538 rows. Error raised while reading key '' of <class 'h5py._hl.files.File'> from /. Output is truncated. View as a [scrollable element](command:cellOutput.enableScrolling?21de049a-219f-41e8-9d9d-5aabf61d8031) or open in a [text editor]",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:3505,deployability,resourc,resource,3505,"name = Path(filename) # allow passing strings. [123](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(. [125](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:125) filename,. [126](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:126) backed=backed,. [127](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:127) sheet=sheet,. [128](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:128) ext=ext,. ... [65](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:65) anno = anno.copy(deep=False). [66](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:66) if not is_string_dtype(anno.index):. ValueError: Observations annot. `var` must have as many rows as `X` has columns (1), but has 33538 rows. Error raised while reading key '' of <class 'h5py._hl.files.File'> from /. Output is truncated. View as a [scrollable element](command:cellOutput.enableScrolling?21de049a-219f-41e8-9d9d-5aabf61d8031) or open in a [text editor](command:workbench.action.openLargeOutput?21de049a-219f-41e8-9d9d-5aabf61d8031). Adjust cell output [settings](command:workbench.action.openSettings?%5B%22%40tag%3Anotebook",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:3676,deployability,resourc,resource,3676,"ce.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(. [125](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:125) filename,. [126](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:126) backed=backed,. [127](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:127) sheet=sheet,. [128](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:128) ext=ext,. ... [65](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:65) anno = anno.copy(deep=False). [66](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:66) if not is_string_dtype(anno.index):. ValueError: Observations annot. `var` must have as many rows as `X` has columns (1), but has 33538 rows. Error raised while reading key '' of <class 'h5py._hl.files.File'> from /. Output is truncated. View as a [scrollable element](command:cellOutput.enableScrolling?21de049a-219f-41e8-9d9d-5aabf61d8031) or open in a [text editor](command:workbench.action.openLargeOutput?21de049a-219f-41e8-9d9d-5aabf61d8031). Adjust cell output [settings](command:workbench.action.openSettings?%5B%22%40tag%3AnotebookOutputLayout%22%5D)... ```. This is my version:. ```. print(sc.__version__). print(ad.__version__). 1.10.0. 0.10.6. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:3870,deployability,resourc,resource,3870,"ce.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(. [125](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:125) filename,. [126](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:126) backed=backed,. [127](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:127) sheet=sheet,. [128](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:128) ext=ext,. ... [65](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:65) anno = anno.copy(deep=False). [66](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:66) if not is_string_dtype(anno.index):. ValueError: Observations annot. `var` must have as many rows as `X` has columns (1), but has 33538 rows. Error raised while reading key '' of <class 'h5py._hl.files.File'> from /. Output is truncated. View as a [scrollable element](command:cellOutput.enableScrolling?21de049a-219f-41e8-9d9d-5aabf61d8031) or open in a [text editor](command:workbench.action.openLargeOutput?21de049a-219f-41e8-9d9d-5aabf61d8031). Adjust cell output [settings](command:workbench.action.openSettings?%5B%22%40tag%3AnotebookOutputLayout%22%5D)... ```. This is my version:. ```. print(sc.__version__). print(ad.__version__). 1.10.0. 0.10.6. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:4018,deployability,Observ,Observations,4018,"ce.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(. [125](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:125) filename,. [126](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:126) backed=backed,. [127](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:127) sheet=sheet,. [128](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:128) ext=ext,. ... [65](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:65) anno = anno.copy(deep=False). [66](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:66) if not is_string_dtype(anno.index):. ValueError: Observations annot. `var` must have as many rows as `X` has columns (1), but has 33538 rows. Error raised while reading key '' of <class 'h5py._hl.files.File'> from /. Output is truncated. View as a [scrollable element](command:cellOutput.enableScrolling?21de049a-219f-41e8-9d9d-5aabf61d8031) or open in a [text editor](command:workbench.action.openLargeOutput?21de049a-219f-41e8-9d9d-5aabf61d8031). Adjust cell output [settings](command:workbench.action.openSettings?%5B%22%40tag%3AnotebookOutputLayout%22%5D)... ```. This is my version:. ```. print(sc.__version__). print(ad.__version__). 1.10.0. 0.10.6. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:4548,deployability,version,version,4548,"ce.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(. [125](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:125) filename,. [126](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:126) backed=backed,. [127](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:127) sheet=sheet,. [128](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:128) ext=ext,. ... [65](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:65) anno = anno.copy(deep=False). [66](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:66) if not is_string_dtype(anno.index):. ValueError: Observations annot. `var` must have as many rows as `X` has columns (1), but has 33538 rows. Error raised while reading key '' of <class 'h5py._hl.files.File'> from /. Output is truncated. View as a [scrollable element](command:cellOutput.enableScrolling?21de049a-219f-41e8-9d9d-5aabf61d8031) or open in a [text editor](command:workbench.action.openLargeOutput?21de049a-219f-41e8-9d9d-5aabf61d8031). Adjust cell output [settings](command:workbench.action.openSettings?%5B%22%40tag%3AnotebookOutputLayout%22%5D)... ```. This is my version:. ```. print(sc.__version__). print(ad.__version__). 1.10.0. 0.10.6. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:1006,energy efficiency,resourc,resource,1006," this bug too. when i run:. ```. import scanpy as sc. import anndata as ad. adata = sc.read(filepath). ```. it turns out:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). Cell In[6], [line 8](vscode-notebook-cell:?execution_count=6&line=8). [6](vscode-notebook-cell:?execution_count=6&line=6) if filename.endswith('.h5ad'):. [7](vscode-notebook-cell:?execution_count=6&line=7) filepath = os.path.join(directory, filename). ----> [8](vscode-notebook-cell:?execution_count=6&line=8) adata = sc.read(filepath). [10](vscode-notebook-cell:?execution_count=6&line=10) # Rename columns with periods in `.obs` attribute. [11](vscode-notebook-cell:?execution_count=6&line=11) for col in adata.obs.columns:. File /data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). [77](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:77) @wraps(fn). [78](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:78) def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. [79](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:79) if len(args_all) <= n_positional:. ---> [80](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80) return fn(*args_all, **kw). [82](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:82) args_pos: P.args. [83](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/da",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:1182,energy efficiency,resourc,resource,1182,"---------------------------. ValueError Traceback (most recent call last). Cell In[6], [line 8](vscode-notebook-cell:?execution_count=6&line=8). [6](vscode-notebook-cell:?execution_count=6&line=6) if filename.endswith('.h5ad'):. [7](vscode-notebook-cell:?execution_count=6&line=7) filepath = os.path.join(directory, filename). ----> [8](vscode-notebook-cell:?execution_count=6&line=8) adata = sc.read(filepath). [10](vscode-notebook-cell:?execution_count=6&line=10) # Rename columns with periods in `.obs` attribute. [11](vscode-notebook-cell:?execution_count=6&line=11) for col in adata.obs.columns:. File /data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). [77](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:77) @wraps(fn). [78](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:78) def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. [79](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:79) if len(args_all) <= n_positional:. ---> [80](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80) return fn(*args_all, **kw). [82](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:82) args_pos: P.args. [83](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:83) args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /data/LSY/venv/lib/python3",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:1406,energy efficiency,resourc,resource,1406,"'):. [7](vscode-notebook-cell:?execution_count=6&line=7) filepath = os.path.join(directory, filename). ----> [8](vscode-notebook-cell:?execution_count=6&line=8) adata = sc.read(filepath). [10](vscode-notebook-cell:?execution_count=6&line=10) # Rename columns with periods in `.obs` attribute. [11](vscode-notebook-cell:?execution_count=6&line=11) for col in adata.obs.columns:. File /data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). [77](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:77) @wraps(fn). [78](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:78) def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. [79](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:79) if len(args_all) <= n_positional:. ---> [80](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80) return fn(*args_all, **kw). [82](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:82) args_pos: P.args. [83](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:83) args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs). [122](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vsco",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:1610,energy efficiency,resourc,resource,1610,"book-cell:?execution_count=6&line=10) # Rename columns with periods in `.obs` attribute. [11](vscode-notebook-cell:?execution_count=6&line=11) for col in adata.obs.columns:. File /data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). [77](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:77) @wraps(fn). [78](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:78) def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. [79](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:79) if len(args_all) <= n_positional:. ---> [80](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80) return fn(*args_all, **kw). [82](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:82) args_pos: P.args. [83](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:83) args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs). [122](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:122) filename = Path(filename) # allow passing strings. [123](https://vscode-remote+ssh-002dremote-002baws-005fcpu",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:1802,energy efficiency,resourc,resource,1802,"v/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). [77](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:77) @wraps(fn). [78](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:78) def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. [79](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:79) if len(args_all) <= n_positional:. ---> [80](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80) return fn(*args_all, **kw). [82](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:82) args_pos: P.args. [83](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:83) args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs). [122](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:122) filename = Path(filename) # allow passing strings. [123](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005f",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:1984,energy efficiency,resourc,resource,1984,"ws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:77) @wraps(fn). [78](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:78) def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. [79](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:79) if len(args_all) <= n_positional:. ---> [80](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80) return fn(*args_all, **kw). [82](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:82) args_pos: P.args. [83](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:83) args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs). [122](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:122) filename = Path(filename) # allow passing strings. [123](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(. [125](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscod",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:2413,energy efficiency,resourc,resource,2413,"code-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:79) if len(args_all) <= n_positional:. ---> [80](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80) return fn(*args_all, **kw). [82](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:82) args_pos: P.args. [83](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:83) args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs). [122](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:122) filename = Path(filename) # allow passing strings. [123](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(. [125](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:125) filename,. [126](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:126) backed=backed,. [127](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:2622,energy efficiency,resourc,resource,2622,"cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80) return fn(*args_all, **kw). [82](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:82) args_pos: P.args. [83](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:83) args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs). [122](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:122) filename = Path(filename) # allow passing strings. [123](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(. [125](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:125) filename,. [126](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:126) backed=backed,. [127](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:127) sheet=sheet,. [128](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:128) ext=ext,. ... [65](https://vs",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:2817,energy efficiency,resourc,resource,2817,".net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:82) args_pos: P.args. [83](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:83) args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs). [122](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:122) filename = Path(filename) # allow passing strings. [123](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(. [125](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:125) filename,. [126](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:126) backed=backed,. [127](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:127) sheet=sheet,. [128](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:128) ext=ext,. ... [65](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:65) anno = anno.copy(deep=False). [66](https://vsc",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:2990,energy efficiency,resourc,resource,2990,"scode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:83) args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs). [122](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:122) filename = Path(filename) # allow passing strings. [123](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(. [125](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:125) filename,. [126](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:126) backed=backed,. [127](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:127) sheet=sheet,. [128](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:128) ext=ext,. ... [65](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:65) anno = anno.copy(deep=False). [66](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:66) if not is_string_dtype(an",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:3159,energy efficiency,resourc,resource,3159,"ta/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs). [122](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:122) filename = Path(filename) # allow passing strings. [123](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(. [125](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:125) filename,. [126](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:126) backed=backed,. [127](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:127) sheet=sheet,. [128](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:128) ext=ext,. ... [65](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:65) anno = anno.copy(deep=False). [66](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:66) if not is_string_dtype(anno.index):. ValueError: Observations annot. `var` must have as many rows as `X` has columns (1), but has 33538 rows. Error raised while reading key '' of <class 'h5py._h",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:3333,energy efficiency,resourc,resource,3333,"*kwargs). [122](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:122) filename = Path(filename) # allow passing strings. [123](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(. [125](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:125) filename,. [126](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:126) backed=backed,. [127](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:127) sheet=sheet,. [128](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:128) ext=ext,. ... [65](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:65) anno = anno.copy(deep=False). [66](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:66) if not is_string_dtype(anno.index):. ValueError: Observations annot. `var` must have as many rows as `X` has columns (1), but has 33538 rows. Error raised while reading key '' of <class 'h5py._hl.files.File'> from /. Output is truncated. View as a [scrollable element](command:cellOutput.enableScrolling?21de049a-219f-41e8-9d9d-5aabf61d8031) or open in a [text editor]",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:3505,energy efficiency,resourc,resource,3505,"name = Path(filename) # allow passing strings. [123](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(. [125](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:125) filename,. [126](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:126) backed=backed,. [127](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:127) sheet=sheet,. [128](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:128) ext=ext,. ... [65](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:65) anno = anno.copy(deep=False). [66](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:66) if not is_string_dtype(anno.index):. ValueError: Observations annot. `var` must have as many rows as `X` has columns (1), but has 33538 rows. Error raised while reading key '' of <class 'h5py._hl.files.File'> from /. Output is truncated. View as a [scrollable element](command:cellOutput.enableScrolling?21de049a-219f-41e8-9d9d-5aabf61d8031) or open in a [text editor](command:workbench.action.openLargeOutput?21de049a-219f-41e8-9d9d-5aabf61d8031). Adjust cell output [settings](command:workbench.action.openSettings?%5B%22%40tag%3Anotebook",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:3676,energy efficiency,resourc,resource,3676,"ce.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(. [125](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:125) filename,. [126](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:126) backed=backed,. [127](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:127) sheet=sheet,. [128](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:128) ext=ext,. ... [65](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:65) anno = anno.copy(deep=False). [66](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:66) if not is_string_dtype(anno.index):. ValueError: Observations annot. `var` must have as many rows as `X` has columns (1), but has 33538 rows. Error raised while reading key '' of <class 'h5py._hl.files.File'> from /. Output is truncated. View as a [scrollable element](command:cellOutput.enableScrolling?21de049a-219f-41e8-9d9d-5aabf61d8031) or open in a [text editor](command:workbench.action.openLargeOutput?21de049a-219f-41e8-9d9d-5aabf61d8031). Adjust cell output [settings](command:workbench.action.openSettings?%5B%22%40tag%3AnotebookOutputLayout%22%5D)... ```. This is my version:. ```. print(sc.__version__). print(ad.__version__). 1.10.0. 0.10.6. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:3870,energy efficiency,resourc,resource,3870,"ce.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(. [125](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:125) filename,. [126](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:126) backed=backed,. [127](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:127) sheet=sheet,. [128](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:128) ext=ext,. ... [65](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:65) anno = anno.copy(deep=False). [66](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:66) if not is_string_dtype(anno.index):. ValueError: Observations annot. `var` must have as many rows as `X` has columns (1), but has 33538 rows. Error raised while reading key '' of <class 'h5py._hl.files.File'> from /. Output is truncated. View as a [scrollable element](command:cellOutput.enableScrolling?21de049a-219f-41e8-9d9d-5aabf61d8031) or open in a [text editor](command:workbench.action.openLargeOutput?21de049a-219f-41e8-9d9d-5aabf61d8031). Adjust cell output [settings](command:workbench.action.openSettings?%5B%22%40tag%3AnotebookOutputLayout%22%5D)... ```. This is my version:. ```. print(sc.__version__). print(ad.__version__). 1.10.0. 0.10.6. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:892,integrability,wrap,wrapper,892,"I run into this bug too. when i run:. ```. import scanpy as sc. import anndata as ad. adata = sc.read(filepath). ```. it turns out:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). Cell In[6], [line 8](vscode-notebook-cell:?execution_count=6&line=8). [6](vscode-notebook-cell:?execution_count=6&line=6) if filename.endswith('.h5ad'):. [7](vscode-notebook-cell:?execution_count=6&line=7) filepath = os.path.join(directory, filename). ----> [8](vscode-notebook-cell:?execution_count=6&line=8) adata = sc.read(filepath). [10](vscode-notebook-cell:?execution_count=6&line=10) # Rename columns with periods in `.obs` attribute. [11](vscode-notebook-cell:?execution_count=6&line=11) for col in adata.obs.columns:. File /data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). [77](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:77) @wraps(fn). [78](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:78) def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. [79](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:79) if len(args_all) <= n_positional:. ---> [80](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80) return fn(*args_all, **kw). [82](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:82) args_pos: P.args. [83](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:1106,integrability,wrap,wraps,1106,"h). ```. it turns out:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). Cell In[6], [line 8](vscode-notebook-cell:?execution_count=6&line=8). [6](vscode-notebook-cell:?execution_count=6&line=6) if filename.endswith('.h5ad'):. [7](vscode-notebook-cell:?execution_count=6&line=7) filepath = os.path.join(directory, filename). ----> [8](vscode-notebook-cell:?execution_count=6&line=8) adata = sc.read(filepath). [10](vscode-notebook-cell:?execution_count=6&line=10) # Rename columns with periods in `.obs` attribute. [11](vscode-notebook-cell:?execution_count=6&line=11) for col in adata.obs.columns:. File /data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). [77](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:77) @wraps(fn). [78](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:78) def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. [79](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:79) if len(args_all) <= n_positional:. ---> [80](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80) return fn(*args_all, **kw). [82](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:82) args_pos: P.args. [83](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:83) args_pos, args_rest = args",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:4548,integrability,version,version,4548,"ce.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(. [125](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:125) filename,. [126](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:126) backed=backed,. [127](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:127) sheet=sheet,. [128](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:128) ext=ext,. ... [65](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:65) anno = anno.copy(deep=False). [66](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:66) if not is_string_dtype(anno.index):. ValueError: Observations annot. `var` must have as many rows as `X` has columns (1), but has 33538 rows. Error raised while reading key '' of <class 'h5py._hl.files.File'> from /. Output is truncated. View as a [scrollable element](command:cellOutput.enableScrolling?21de049a-219f-41e8-9d9d-5aabf61d8031) or open in a [text editor](command:workbench.action.openLargeOutput?21de049a-219f-41e8-9d9d-5aabf61d8031). Adjust cell output [settings](command:workbench.action.openSettings?%5B%22%40tag%3AnotebookOutputLayout%22%5D)... ```. This is my version:. ```. print(sc.__version__). print(ad.__version__). 1.10.0. 0.10.6. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:892,interoperability,wrapper,wrapper,892,"I run into this bug too. when i run:. ```. import scanpy as sc. import anndata as ad. adata = sc.read(filepath). ```. it turns out:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). Cell In[6], [line 8](vscode-notebook-cell:?execution_count=6&line=8). [6](vscode-notebook-cell:?execution_count=6&line=6) if filename.endswith('.h5ad'):. [7](vscode-notebook-cell:?execution_count=6&line=7) filepath = os.path.join(directory, filename). ----> [8](vscode-notebook-cell:?execution_count=6&line=8) adata = sc.read(filepath). [10](vscode-notebook-cell:?execution_count=6&line=10) # Rename columns with periods in `.obs` attribute. [11](vscode-notebook-cell:?execution_count=6&line=11) for col in adata.obs.columns:. File /data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). [77](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:77) @wraps(fn). [78](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:78) def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. [79](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:79) if len(args_all) <= n_positional:. ---> [80](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80) return fn(*args_all, **kw). [82](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:82) args_pos: P.args. [83](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:828,modifiability,pac,packages,828,"I run into this bug too. when i run:. ```. import scanpy as sc. import anndata as ad. adata = sc.read(filepath). ```. it turns out:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). Cell In[6], [line 8](vscode-notebook-cell:?execution_count=6&line=8). [6](vscode-notebook-cell:?execution_count=6&line=6) if filename.endswith('.h5ad'):. [7](vscode-notebook-cell:?execution_count=6&line=7) filepath = os.path.join(directory, filename). ----> [8](vscode-notebook-cell:?execution_count=6&line=8) adata = sc.read(filepath). [10](vscode-notebook-cell:?execution_count=6&line=10) # Rename columns with periods in `.obs` attribute. [11](vscode-notebook-cell:?execution_count=6&line=11) for col in adata.obs.columns:. File /data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). [77](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:77) @wraps(fn). [78](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:78) def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. [79](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:79) if len(args_all) <= n_positional:. ---> [80](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80) return fn(*args_all, **kw). [82](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:82) args_pos: P.args. [83](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:1064,modifiability,pac,packages,1064,"rt anndata as ad. adata = sc.read(filepath). ```. it turns out:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). Cell In[6], [line 8](vscode-notebook-cell:?execution_count=6&line=8). [6](vscode-notebook-cell:?execution_count=6&line=6) if filename.endswith('.h5ad'):. [7](vscode-notebook-cell:?execution_count=6&line=7) filepath = os.path.join(directory, filename). ----> [8](vscode-notebook-cell:?execution_count=6&line=8) adata = sc.read(filepath). [10](vscode-notebook-cell:?execution_count=6&line=10) # Rename columns with periods in `.obs` attribute. [11](vscode-notebook-cell:?execution_count=6&line=11) for col in adata.obs.columns:. File /data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). [77](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:77) @wraps(fn). [78](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:78) def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. [79](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:79) if len(args_all) <= n_positional:. ---> [80](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80) return fn(*args_all, **kw). [82](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:82) args_pos: P.args. [83](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:1240,modifiability,pac,packages,1240,"cent call last). Cell In[6], [line 8](vscode-notebook-cell:?execution_count=6&line=8). [6](vscode-notebook-cell:?execution_count=6&line=6) if filename.endswith('.h5ad'):. [7](vscode-notebook-cell:?execution_count=6&line=7) filepath = os.path.join(directory, filename). ----> [8](vscode-notebook-cell:?execution_count=6&line=8) adata = sc.read(filepath). [10](vscode-notebook-cell:?execution_count=6&line=10) # Rename columns with periods in `.obs` attribute. [11](vscode-notebook-cell:?execution_count=6&line=11) for col in adata.obs.columns:. File /data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). [77](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:77) @wraps(fn). [78](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:78) def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. [79](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:79) if len(args_all) <= n_positional:. ---> [80](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80) return fn(*args_all, **kw). [82](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:82) args_pos: P.args. [83](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:83) args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124, in read(filenam",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:1464,modifiability,pac,packages,1464,"ilepath = os.path.join(directory, filename). ----> [8](vscode-notebook-cell:?execution_count=6&line=8) adata = sc.read(filepath). [10](vscode-notebook-cell:?execution_count=6&line=10) # Rename columns with periods in `.obs` attribute. [11](vscode-notebook-cell:?execution_count=6&line=11) for col in adata.obs.columns:. File /data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). [77](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:77) @wraps(fn). [78](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:78) def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. [79](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:79) if len(args_all) <= n_positional:. ---> [80](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80) return fn(*args_all, **kw). [82](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:82) args_pos: P.args. [83](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:83) args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs). [122](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/si",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:1668,modifiability,pac,packages,1668,"h periods in `.obs` attribute. [11](vscode-notebook-cell:?execution_count=6&line=11) for col in adata.obs.columns:. File /data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). [77](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:77) @wraps(fn). [78](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:78) def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. [79](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:79) if len(args_all) <= n_positional:. ---> [80](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80) return fn(*args_all, **kw). [82](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:82) args_pos: P.args. [83](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:83) args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs). [122](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:122) filename = Path(filename) # allow passing strings. [123](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:1860,modifiability,pac,packages,1860,":80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). [77](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:77) @wraps(fn). [78](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:78) def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. [79](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:79) if len(args_all) <= n_positional:. ---> [80](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80) return fn(*args_all, **kw). [82](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:82) args_pos: P.args. [83](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:83) args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs). [122](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:122) filename = Path(filename) # allow passing strings. [123](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/pytho",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:2042,modifiability,pac,packages,2042,"b/python3.10/site-packages/legacy_api_wrap/__init__.py:77) @wraps(fn). [78](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:78) def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. [79](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:79) if len(args_all) <= n_positional:. ---> [80](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80) return fn(*args_all, **kw). [82](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:82) args_pos: P.args. [83](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:83) args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs). [122](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:122) filename = Path(filename) # allow passing strings. [123](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(. [125](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/sit",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:2195,modifiability,pac,packages,2195,"dn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:78) def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. [79](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:79) if len(args_all) <= n_positional:. ---> [80](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80) return fn(*args_all, **kw). [82](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:82) args_pos: P.args. [83](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:83) args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs). [122](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:122) filename = Path(filename) # allow passing strings. [123](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(. [125](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:125) filename,. [126](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/li",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:2471,modifiability,pac,packages,2471,"gacy_api_wrap/__init__.py:79) if len(args_all) <= n_positional:. ---> [80](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80) return fn(*args_all, **kw). [82](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:82) args_pos: P.args. [83](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:83) args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs). [122](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:122) filename = Path(filename) # allow passing strings. [123](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(. [125](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:125) filename,. [126](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:126) backed=backed,. [127](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:127) sheet=sheet,. [128](https://vscode-remote+ssh-002d",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:2680,modifiability,pac,packages,2680,"api_wrap/__init__.py:80) return fn(*args_all, **kw). [82](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:82) args_pos: P.args. [83](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:83) args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs). [122](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:122) filename = Path(filename) # allow passing strings. [123](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(. [125](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:125) filename,. [126](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:126) backed=backed,. [127](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:127) sheet=sheet,. [128](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:128) ext=ext,. ... [65](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:2875,modifiability,pac,packages,2875,"_wrap/__init__.py:82) args_pos: P.args. [83](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:83) args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs). [122](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:122) filename = Path(filename) # allow passing strings. [123](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(. [125](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:125) filename,. [126](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:126) backed=backed,. [127](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:127) sheet=sheet,. [128](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:128) ext=ext,. ... [65](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:65) anno = anno.copy(deep=False). [66](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:3048,modifiability,pac,packages,3048,"egacy_api_wrap/__init__.py:83) args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs). [122](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:122) filename = Path(filename) # allow passing strings. [123](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(. [125](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:125) filename,. [126](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:126) backed=backed,. [127](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:127) sheet=sheet,. [128](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:128) ext=ext,. ... [65](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:65) anno = anno.copy(deep=False). [66](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:66) if not is_string_dtype(anno.index):. ValueError: Observations annot. `var` must hav",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:3217,modifiability,pac,packages,3217,"py:124, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs). [122](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:122) filename = Path(filename) # allow passing strings. [123](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(. [125](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:125) filename,. [126](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:126) backed=backed,. [127](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:127) sheet=sheet,. [128](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:128) ext=ext,. ... [65](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:65) anno = anno.copy(deep=False). [66](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:66) if not is_string_dtype(anno.index):. ValueError: Observations annot. `var` must have as many rows as `X` has columns (1), but has 33538 rows. Error raised while reading key '' of <class 'h5py._hl.files.File'> from /. Output is truncated. View as a [scr",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:3391,modifiability,pac,packages,3391,"ws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:122) filename = Path(filename) # allow passing strings. [123](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(. [125](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:125) filename,. [126](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:126) backed=backed,. [127](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:127) sheet=sheet,. [128](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:128) ext=ext,. ... [65](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:65) anno = anno.copy(deep=False). [66](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:66) if not is_string_dtype(anno.index):. ValueError: Observations annot. `var` must have as many rows as `X` has columns (1), but has 33538 rows. Error raised while reading key '' of <class 'h5py._hl.files.File'> from /. Output is truncated. View as a [scrollable element](command:cellOutput.enableScrolling?21de049a-219f-41e8-9d9d-5aabf61d8031) or open in a [text editor](command:workbench.action.openLargeOutput?21de049a-219f-41",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:3563,modifiability,pac,packages,3563,"://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(. [125](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:125) filename,. [126](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:126) backed=backed,. [127](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:127) sheet=sheet,. [128](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:128) ext=ext,. ... [65](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:65) anno = anno.copy(deep=False). [66](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:66) if not is_string_dtype(anno.index):. ValueError: Observations annot. `var` must have as many rows as `X` has columns (1), but has 33538 rows. Error raised while reading key '' of <class 'h5py._hl.files.File'> from /. Output is truncated. View as a [scrollable element](command:cellOutput.enableScrolling?21de049a-219f-41e8-9d9d-5aabf61d8031) or open in a [text editor](command:workbench.action.openLargeOutput?21de049a-219f-41e8-9d9d-5aabf61d8031). Adjust cell output [settings](command:workbench.action.openSettings?%5B%22%40tag%3AnotebookOutputLayout%22%5D)... ```. This is my version:. ```. prin",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:3734,modifiability,pac,packages,3734,"ce.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(. [125](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:125) filename,. [126](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:126) backed=backed,. [127](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:127) sheet=sheet,. [128](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:128) ext=ext,. ... [65](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:65) anno = anno.copy(deep=False). [66](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:66) if not is_string_dtype(anno.index):. ValueError: Observations annot. `var` must have as many rows as `X` has columns (1), but has 33538 rows. Error raised while reading key '' of <class 'h5py._hl.files.File'> from /. Output is truncated. View as a [scrollable element](command:cellOutput.enableScrolling?21de049a-219f-41e8-9d9d-5aabf61d8031) or open in a [text editor](command:workbench.action.openLargeOutput?21de049a-219f-41e8-9d9d-5aabf61d8031). Adjust cell output [settings](command:workbench.action.openSettings?%5B%22%40tag%3AnotebookOutputLayout%22%5D)... ```. This is my version:. ```. print(sc.__version__). print(ad.__version__). 1.10.0. 0.10.6. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:3928,modifiability,pac,packages,3928,"ce.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(. [125](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:125) filename,. [126](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:126) backed=backed,. [127](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:127) sheet=sheet,. [128](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:128) ext=ext,. ... [65](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:65) anno = anno.copy(deep=False). [66](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:66) if not is_string_dtype(anno.index):. ValueError: Observations annot. `var` must have as many rows as `X` has columns (1), but has 33538 rows. Error raised while reading key '' of <class 'h5py._hl.files.File'> from /. Output is truncated. View as a [scrollable element](command:cellOutput.enableScrolling?21de049a-219f-41e8-9d9d-5aabf61d8031) or open in a [text editor](command:workbench.action.openLargeOutput?21de049a-219f-41e8-9d9d-5aabf61d8031). Adjust cell output [settings](command:workbench.action.openSettings?%5B%22%40tag%3AnotebookOutputLayout%22%5D)... ```. This is my version:. ```. print(sc.__version__). print(ad.__version__). 1.10.0. 0.10.6. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:4548,modifiability,version,version,4548,"ce.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(. [125](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:125) filename,. [126](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:126) backed=backed,. [127](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:127) sheet=sheet,. [128](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:128) ext=ext,. ... [65](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:65) anno = anno.copy(deep=False). [66](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:66) if not is_string_dtype(anno.index):. ValueError: Observations annot. `var` must have as many rows as `X` has columns (1), but has 33538 rows. Error raised while reading key '' of <class 'h5py._hl.files.File'> from /. Output is truncated. View as a [scrollable element](command:cellOutput.enableScrolling?21de049a-219f-41e8-9d9d-5aabf61d8031) or open in a [text editor](command:workbench.action.openLargeOutput?21de049a-219f-41e8-9d9d-5aabf61d8031). Adjust cell output [settings](command:workbench.action.openSettings?%5B%22%40tag%3AnotebookOutputLayout%22%5D)... ```. This is my version:. ```. print(sc.__version__). print(ad.__version__). 1.10.0. 0.10.6. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:1006,performance,resourc,resource,1006," this bug too. when i run:. ```. import scanpy as sc. import anndata as ad. adata = sc.read(filepath). ```. it turns out:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). Cell In[6], [line 8](vscode-notebook-cell:?execution_count=6&line=8). [6](vscode-notebook-cell:?execution_count=6&line=6) if filename.endswith('.h5ad'):. [7](vscode-notebook-cell:?execution_count=6&line=7) filepath = os.path.join(directory, filename). ----> [8](vscode-notebook-cell:?execution_count=6&line=8) adata = sc.read(filepath). [10](vscode-notebook-cell:?execution_count=6&line=10) # Rename columns with periods in `.obs` attribute. [11](vscode-notebook-cell:?execution_count=6&line=11) for col in adata.obs.columns:. File /data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). [77](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:77) @wraps(fn). [78](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:78) def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. [79](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:79) if len(args_all) <= n_positional:. ---> [80](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80) return fn(*args_all, **kw). [82](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:82) args_pos: P.args. [83](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/da",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:1182,performance,resourc,resource,1182,"---------------------------. ValueError Traceback (most recent call last). Cell In[6], [line 8](vscode-notebook-cell:?execution_count=6&line=8). [6](vscode-notebook-cell:?execution_count=6&line=6) if filename.endswith('.h5ad'):. [7](vscode-notebook-cell:?execution_count=6&line=7) filepath = os.path.join(directory, filename). ----> [8](vscode-notebook-cell:?execution_count=6&line=8) adata = sc.read(filepath). [10](vscode-notebook-cell:?execution_count=6&line=10) # Rename columns with periods in `.obs` attribute. [11](vscode-notebook-cell:?execution_count=6&line=11) for col in adata.obs.columns:. File /data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). [77](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:77) @wraps(fn). [78](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:78) def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. [79](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:79) if len(args_all) <= n_positional:. ---> [80](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80) return fn(*args_all, **kw). [82](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:82) args_pos: P.args. [83](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:83) args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /data/LSY/venv/lib/python3",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:1406,performance,resourc,resource,1406,"'):. [7](vscode-notebook-cell:?execution_count=6&line=7) filepath = os.path.join(directory, filename). ----> [8](vscode-notebook-cell:?execution_count=6&line=8) adata = sc.read(filepath). [10](vscode-notebook-cell:?execution_count=6&line=10) # Rename columns with periods in `.obs` attribute. [11](vscode-notebook-cell:?execution_count=6&line=11) for col in adata.obs.columns:. File /data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). [77](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:77) @wraps(fn). [78](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:78) def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. [79](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:79) if len(args_all) <= n_positional:. ---> [80](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80) return fn(*args_all, **kw). [82](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:82) args_pos: P.args. [83](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:83) args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs). [122](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vsco",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:1610,performance,resourc,resource,1610,"book-cell:?execution_count=6&line=10) # Rename columns with periods in `.obs` attribute. [11](vscode-notebook-cell:?execution_count=6&line=11) for col in adata.obs.columns:. File /data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). [77](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:77) @wraps(fn). [78](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:78) def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. [79](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:79) if len(args_all) <= n_positional:. ---> [80](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80) return fn(*args_all, **kw). [82](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:82) args_pos: P.args. [83](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:83) args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs). [122](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:122) filename = Path(filename) # allow passing strings. [123](https://vscode-remote+ssh-002dremote-002baws-005fcpu",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:1802,performance,resourc,resource,1802,"v/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). [77](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:77) @wraps(fn). [78](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:78) def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. [79](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:79) if len(args_all) <= n_positional:. ---> [80](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80) return fn(*args_all, **kw). [82](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:82) args_pos: P.args. [83](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:83) args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs). [122](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:122) filename = Path(filename) # allow passing strings. [123](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005f",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:1984,performance,resourc,resource,1984,"ws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:77) @wraps(fn). [78](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:78) def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. [79](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:79) if len(args_all) <= n_positional:. ---> [80](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80) return fn(*args_all, **kw). [82](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:82) args_pos: P.args. [83](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:83) args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs). [122](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:122) filename = Path(filename) # allow passing strings. [123](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(. [125](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscod",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:2310,performance,cach,cache,2310,"rgs, **kw: P.kwargs) -> R:. [79](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:79) if len(args_all) <= n_positional:. ---> [80](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80) return fn(*args_all, **kw). [82](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:82) args_pos: P.args. [83](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:83) args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs). [122](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:122) filename = Path(filename) # allow passing strings. [123](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(. [125](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:125) filename,. [126](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:126) backed=backed,. [127](https://vscode-remote+ssh-002dremote-002",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:2413,performance,resourc,resource,2413,"code-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:79) if len(args_all) <= n_positional:. ---> [80](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80) return fn(*args_all, **kw). [82](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:82) args_pos: P.args. [83](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:83) args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs). [122](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:122) filename = Path(filename) # allow passing strings. [123](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(. [125](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:125) filename,. [126](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:126) backed=backed,. [127](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:2622,performance,resourc,resource,2622,"cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80) return fn(*args_all, **kw). [82](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:82) args_pos: P.args. [83](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:83) args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs). [122](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:122) filename = Path(filename) # allow passing strings. [123](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(. [125](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:125) filename,. [126](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:126) backed=backed,. [127](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:127) sheet=sheet,. [128](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:128) ext=ext,. ... [65](https://vs",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:2817,performance,resourc,resource,2817,".net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:82) args_pos: P.args. [83](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:83) args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs). [122](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:122) filename = Path(filename) # allow passing strings. [123](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(. [125](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:125) filename,. [126](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:126) backed=backed,. [127](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:127) sheet=sheet,. [128](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:128) ext=ext,. ... [65](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:65) anno = anno.copy(deep=False). [66](https://vsc",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:2990,performance,resourc,resource,2990,"scode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:83) args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs). [122](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:122) filename = Path(filename) # allow passing strings. [123](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(. [125](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:125) filename,. [126](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:126) backed=backed,. [127](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:127) sheet=sheet,. [128](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:128) ext=ext,. ... [65](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:65) anno = anno.copy(deep=False). [66](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:66) if not is_string_dtype(an",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:3159,performance,resourc,resource,3159,"ta/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs). [122](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:122) filename = Path(filename) # allow passing strings. [123](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(. [125](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:125) filename,. [126](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:126) backed=backed,. [127](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:127) sheet=sheet,. [128](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:128) ext=ext,. ... [65](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:65) anno = anno.copy(deep=False). [66](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:66) if not is_string_dtype(anno.index):. ValueError: Observations annot. `var` must have as many rows as `X` has columns (1), but has 33538 rows. Error raised while reading key '' of <class 'h5py._h",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:3333,performance,resourc,resource,3333,"*kwargs). [122](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:122) filename = Path(filename) # allow passing strings. [123](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(. [125](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:125) filename,. [126](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:126) backed=backed,. [127](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:127) sheet=sheet,. [128](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:128) ext=ext,. ... [65](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:65) anno = anno.copy(deep=False). [66](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:66) if not is_string_dtype(anno.index):. ValueError: Observations annot. `var` must have as many rows as `X` has columns (1), but has 33538 rows. Error raised while reading key '' of <class 'h5py._hl.files.File'> from /. Output is truncated. View as a [scrollable element](command:cellOutput.enableScrolling?21de049a-219f-41e8-9d9d-5aabf61d8031) or open in a [text editor]",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:3505,performance,resourc,resource,3505,"name = Path(filename) # allow passing strings. [123](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(. [125](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:125) filename,. [126](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:126) backed=backed,. [127](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:127) sheet=sheet,. [128](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:128) ext=ext,. ... [65](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:65) anno = anno.copy(deep=False). [66](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:66) if not is_string_dtype(anno.index):. ValueError: Observations annot. `var` must have as many rows as `X` has columns (1), but has 33538 rows. Error raised while reading key '' of <class 'h5py._hl.files.File'> from /. Output is truncated. View as a [scrollable element](command:cellOutput.enableScrolling?21de049a-219f-41e8-9d9d-5aabf61d8031) or open in a [text editor](command:workbench.action.openLargeOutput?21de049a-219f-41e8-9d9d-5aabf61d8031). Adjust cell output [settings](command:workbench.action.openSettings?%5B%22%40tag%3Anotebook",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:3676,performance,resourc,resource,3676,"ce.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(. [125](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:125) filename,. [126](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:126) backed=backed,. [127](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:127) sheet=sheet,. [128](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:128) ext=ext,. ... [65](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:65) anno = anno.copy(deep=False). [66](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:66) if not is_string_dtype(anno.index):. ValueError: Observations annot. `var` must have as many rows as `X` has columns (1), but has 33538 rows. Error raised while reading key '' of <class 'h5py._hl.files.File'> from /. Output is truncated. View as a [scrollable element](command:cellOutput.enableScrolling?21de049a-219f-41e8-9d9d-5aabf61d8031) or open in a [text editor](command:workbench.action.openLargeOutput?21de049a-219f-41e8-9d9d-5aabf61d8031). Adjust cell output [settings](command:workbench.action.openSettings?%5B%22%40tag%3AnotebookOutputLayout%22%5D)... ```. This is my version:. ```. print(sc.__version__). print(ad.__version__). 1.10.0. 0.10.6. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:3870,performance,resourc,resource,3870,"ce.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(. [125](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:125) filename,. [126](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:126) backed=backed,. [127](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:127) sheet=sheet,. [128](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:128) ext=ext,. ... [65](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:65) anno = anno.copy(deep=False). [66](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:66) if not is_string_dtype(anno.index):. ValueError: Observations annot. `var` must have as many rows as `X` has columns (1), but has 33538 rows. Error raised while reading key '' of <class 'h5py._hl.files.File'> from /. Output is truncated. View as a [scrollable element](command:cellOutput.enableScrolling?21de049a-219f-41e8-9d9d-5aabf61d8031) or open in a [text editor](command:workbench.action.openLargeOutput?21de049a-219f-41e8-9d9d-5aabf61d8031). Adjust cell output [settings](command:workbench.action.openSettings?%5B%22%40tag%3AnotebookOutputLayout%22%5D)... ```. This is my version:. ```. print(sc.__version__). print(ad.__version__). 1.10.0. 0.10.6. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:4111,performance,Error,Error,4111,"ce.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(. [125](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:125) filename,. [126](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:126) backed=backed,. [127](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:127) sheet=sheet,. [128](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:128) ext=ext,. ... [65](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:65) anno = anno.copy(deep=False). [66](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:66) if not is_string_dtype(anno.index):. ValueError: Observations annot. `var` must have as many rows as `X` has columns (1), but has 33538 rows. Error raised while reading key '' of <class 'h5py._hl.files.File'> from /. Output is truncated. View as a [scrollable element](command:cellOutput.enableScrolling?21de049a-219f-41e8-9d9d-5aabf61d8031) or open in a [text editor](command:workbench.action.openLargeOutput?21de049a-219f-41e8-9d9d-5aabf61d8031). Adjust cell output [settings](command:workbench.action.openSettings?%5B%22%40tag%3AnotebookOutputLayout%22%5D)... ```. This is my version:. ```. print(sc.__version__). print(ad.__version__). 1.10.0. 0.10.6. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:1006,safety,resourc,resource,1006," this bug too. when i run:. ```. import scanpy as sc. import anndata as ad. adata = sc.read(filepath). ```. it turns out:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). Cell In[6], [line 8](vscode-notebook-cell:?execution_count=6&line=8). [6](vscode-notebook-cell:?execution_count=6&line=6) if filename.endswith('.h5ad'):. [7](vscode-notebook-cell:?execution_count=6&line=7) filepath = os.path.join(directory, filename). ----> [8](vscode-notebook-cell:?execution_count=6&line=8) adata = sc.read(filepath). [10](vscode-notebook-cell:?execution_count=6&line=10) # Rename columns with periods in `.obs` attribute. [11](vscode-notebook-cell:?execution_count=6&line=11) for col in adata.obs.columns:. File /data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). [77](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:77) @wraps(fn). [78](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:78) def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. [79](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:79) if len(args_all) <= n_positional:. ---> [80](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80) return fn(*args_all, **kw). [82](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:82) args_pos: P.args. [83](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/da",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:1182,safety,resourc,resource,1182,"---------------------------. ValueError Traceback (most recent call last). Cell In[6], [line 8](vscode-notebook-cell:?execution_count=6&line=8). [6](vscode-notebook-cell:?execution_count=6&line=6) if filename.endswith('.h5ad'):. [7](vscode-notebook-cell:?execution_count=6&line=7) filepath = os.path.join(directory, filename). ----> [8](vscode-notebook-cell:?execution_count=6&line=8) adata = sc.read(filepath). [10](vscode-notebook-cell:?execution_count=6&line=10) # Rename columns with periods in `.obs` attribute. [11](vscode-notebook-cell:?execution_count=6&line=11) for col in adata.obs.columns:. File /data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). [77](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:77) @wraps(fn). [78](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:78) def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. [79](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:79) if len(args_all) <= n_positional:. ---> [80](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80) return fn(*args_all, **kw). [82](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:82) args_pos: P.args. [83](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:83) args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /data/LSY/venv/lib/python3",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:1406,safety,resourc,resource,1406,"'):. [7](vscode-notebook-cell:?execution_count=6&line=7) filepath = os.path.join(directory, filename). ----> [8](vscode-notebook-cell:?execution_count=6&line=8) adata = sc.read(filepath). [10](vscode-notebook-cell:?execution_count=6&line=10) # Rename columns with periods in `.obs` attribute. [11](vscode-notebook-cell:?execution_count=6&line=11) for col in adata.obs.columns:. File /data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). [77](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:77) @wraps(fn). [78](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:78) def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. [79](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:79) if len(args_all) <= n_positional:. ---> [80](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80) return fn(*args_all, **kw). [82](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:82) args_pos: P.args. [83](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:83) args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs). [122](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vsco",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:1610,safety,resourc,resource,1610,"book-cell:?execution_count=6&line=10) # Rename columns with periods in `.obs` attribute. [11](vscode-notebook-cell:?execution_count=6&line=11) for col in adata.obs.columns:. File /data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). [77](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:77) @wraps(fn). [78](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:78) def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. [79](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:79) if len(args_all) <= n_positional:. ---> [80](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80) return fn(*args_all, **kw). [82](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:82) args_pos: P.args. [83](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:83) args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs). [122](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:122) filename = Path(filename) # allow passing strings. [123](https://vscode-remote+ssh-002dremote-002baws-005fcpu",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:1802,safety,resourc,resource,1802,"v/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). [77](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:77) @wraps(fn). [78](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:78) def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. [79](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:79) if len(args_all) <= n_positional:. ---> [80](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80) return fn(*args_all, **kw). [82](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:82) args_pos: P.args. [83](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:83) args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs). [122](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:122) filename = Path(filename) # allow passing strings. [123](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005f",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:1984,safety,resourc,resource,1984,"ws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:77) @wraps(fn). [78](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:78) def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. [79](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:79) if len(args_all) <= n_positional:. ---> [80](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80) return fn(*args_all, **kw). [82](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:82) args_pos: P.args. [83](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:83) args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs). [122](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:122) filename = Path(filename) # allow passing strings. [123](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(. [125](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscod",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:2413,safety,resourc,resource,2413,"code-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:79) if len(args_all) <= n_positional:. ---> [80](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80) return fn(*args_all, **kw). [82](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:82) args_pos: P.args. [83](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:83) args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs). [122](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:122) filename = Path(filename) # allow passing strings. [123](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(. [125](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:125) filename,. [126](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:126) backed=backed,. [127](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:2622,safety,resourc,resource,2622,"cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80) return fn(*args_all, **kw). [82](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:82) args_pos: P.args. [83](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:83) args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs). [122](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:122) filename = Path(filename) # allow passing strings. [123](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(. [125](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:125) filename,. [126](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:126) backed=backed,. [127](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:127) sheet=sheet,. [128](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:128) ext=ext,. ... [65](https://vs",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:2817,safety,resourc,resource,2817,".net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:82) args_pos: P.args. [83](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:83) args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs). [122](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:122) filename = Path(filename) # allow passing strings. [123](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(. [125](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:125) filename,. [126](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:126) backed=backed,. [127](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:127) sheet=sheet,. [128](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:128) ext=ext,. ... [65](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:65) anno = anno.copy(deep=False). [66](https://vsc",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:2990,safety,resourc,resource,2990,"scode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:83) args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs). [122](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:122) filename = Path(filename) # allow passing strings. [123](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(. [125](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:125) filename,. [126](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:126) backed=backed,. [127](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:127) sheet=sheet,. [128](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:128) ext=ext,. ... [65](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:65) anno = anno.copy(deep=False). [66](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:66) if not is_string_dtype(an",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:3159,safety,resourc,resource,3159,"ta/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs). [122](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:122) filename = Path(filename) # allow passing strings. [123](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(. [125](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:125) filename,. [126](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:126) backed=backed,. [127](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:127) sheet=sheet,. [128](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:128) ext=ext,. ... [65](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:65) anno = anno.copy(deep=False). [66](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:66) if not is_string_dtype(anno.index):. ValueError: Observations annot. `var` must have as many rows as `X` has columns (1), but has 33538 rows. Error raised while reading key '' of <class 'h5py._h",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:3333,safety,resourc,resource,3333,"*kwargs). [122](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:122) filename = Path(filename) # allow passing strings. [123](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(. [125](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:125) filename,. [126](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:126) backed=backed,. [127](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:127) sheet=sheet,. [128](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:128) ext=ext,. ... [65](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:65) anno = anno.copy(deep=False). [66](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:66) if not is_string_dtype(anno.index):. ValueError: Observations annot. `var` must have as many rows as `X` has columns (1), but has 33538 rows. Error raised while reading key '' of <class 'h5py._hl.files.File'> from /. Output is truncated. View as a [scrollable element](command:cellOutput.enableScrolling?21de049a-219f-41e8-9d9d-5aabf61d8031) or open in a [text editor]",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:3505,safety,resourc,resource,3505,"name = Path(filename) # allow passing strings. [123](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(. [125](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:125) filename,. [126](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:126) backed=backed,. [127](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:127) sheet=sheet,. [128](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:128) ext=ext,. ... [65](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:65) anno = anno.copy(deep=False). [66](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:66) if not is_string_dtype(anno.index):. ValueError: Observations annot. `var` must have as many rows as `X` has columns (1), but has 33538 rows. Error raised while reading key '' of <class 'h5py._hl.files.File'> from /. Output is truncated. View as a [scrollable element](command:cellOutput.enableScrolling?21de049a-219f-41e8-9d9d-5aabf61d8031) or open in a [text editor](command:workbench.action.openLargeOutput?21de049a-219f-41e8-9d9d-5aabf61d8031). Adjust cell output [settings](command:workbench.action.openSettings?%5B%22%40tag%3Anotebook",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:3676,safety,resourc,resource,3676,"ce.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(. [125](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:125) filename,. [126](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:126) backed=backed,. [127](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:127) sheet=sheet,. [128](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:128) ext=ext,. ... [65](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:65) anno = anno.copy(deep=False). [66](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:66) if not is_string_dtype(anno.index):. ValueError: Observations annot. `var` must have as many rows as `X` has columns (1), but has 33538 rows. Error raised while reading key '' of <class 'h5py._hl.files.File'> from /. Output is truncated. View as a [scrollable element](command:cellOutput.enableScrolling?21de049a-219f-41e8-9d9d-5aabf61d8031) or open in a [text editor](command:workbench.action.openLargeOutput?21de049a-219f-41e8-9d9d-5aabf61d8031). Adjust cell output [settings](command:workbench.action.openSettings?%5B%22%40tag%3AnotebookOutputLayout%22%5D)... ```. This is my version:. ```. print(sc.__version__). print(ad.__version__). 1.10.0. 0.10.6. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:3870,safety,resourc,resource,3870,"ce.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(. [125](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:125) filename,. [126](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:126) backed=backed,. [127](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:127) sheet=sheet,. [128](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:128) ext=ext,. ... [65](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:65) anno = anno.copy(deep=False). [66](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:66) if not is_string_dtype(anno.index):. ValueError: Observations annot. `var` must have as many rows as `X` has columns (1), but has 33538 rows. Error raised while reading key '' of <class 'h5py._hl.files.File'> from /. Output is truncated. View as a [scrollable element](command:cellOutput.enableScrolling?21de049a-219f-41e8-9d9d-5aabf61d8031) or open in a [text editor](command:workbench.action.openLargeOutput?21de049a-219f-41e8-9d9d-5aabf61d8031). Adjust cell output [settings](command:workbench.action.openSettings?%5B%22%40tag%3AnotebookOutputLayout%22%5D)... ```. This is my version:. ```. print(sc.__version__). print(ad.__version__). 1.10.0. 0.10.6. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:4111,safety,Error,Error,4111,"ce.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(. [125](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:125) filename,. [126](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:126) backed=backed,. [127](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:127) sheet=sheet,. [128](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:128) ext=ext,. ... [65](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:65) anno = anno.copy(deep=False). [66](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:66) if not is_string_dtype(anno.index):. ValueError: Observations annot. `var` must have as many rows as `X` has columns (1), but has 33538 rows. Error raised while reading key '' of <class 'h5py._hl.files.File'> from /. Output is truncated. View as a [scrollable element](command:cellOutput.enableScrolling?21de049a-219f-41e8-9d9d-5aabf61d8031) or open in a [text editor](command:workbench.action.openLargeOutput?21de049a-219f-41e8-9d9d-5aabf61d8031). Adjust cell output [settings](command:workbench.action.openSettings?%5B%22%40tag%3AnotebookOutputLayout%22%5D)... ```. This is my version:. ```. print(sc.__version__). print(ad.__version__). 1.10.0. 0.10.6. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:968,security,ssh,ssh-,968,"I run into this bug too. when i run:. ```. import scanpy as sc. import anndata as ad. adata = sc.read(filepath). ```. it turns out:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). Cell In[6], [line 8](vscode-notebook-cell:?execution_count=6&line=8). [6](vscode-notebook-cell:?execution_count=6&line=6) if filename.endswith('.h5ad'):. [7](vscode-notebook-cell:?execution_count=6&line=7) filepath = os.path.join(directory, filename). ----> [8](vscode-notebook-cell:?execution_count=6&line=8) adata = sc.read(filepath). [10](vscode-notebook-cell:?execution_count=6&line=10) # Rename columns with periods in `.obs` attribute. [11](vscode-notebook-cell:?execution_count=6&line=11) for col in adata.obs.columns:. File /data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). [77](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:77) @wraps(fn). [78](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:78) def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. [79](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:79) if len(args_all) <= n_positional:. ---> [80](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80) return fn(*args_all, **kw). [82](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:82) args_pos: P.args. [83](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:1144,security,ssh,ssh-,1144,"-------------------------------------------------------------------. ValueError Traceback (most recent call last). Cell In[6], [line 8](vscode-notebook-cell:?execution_count=6&line=8). [6](vscode-notebook-cell:?execution_count=6&line=6) if filename.endswith('.h5ad'):. [7](vscode-notebook-cell:?execution_count=6&line=7) filepath = os.path.join(directory, filename). ----> [8](vscode-notebook-cell:?execution_count=6&line=8) adata = sc.read(filepath). [10](vscode-notebook-cell:?execution_count=6&line=10) # Rename columns with periods in `.obs` attribute. [11](vscode-notebook-cell:?execution_count=6&line=11) for col in adata.obs.columns:. File /data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). [77](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:77) @wraps(fn). [78](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:78) def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. [79](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:79) if len(args_all) <= n_positional:. ---> [80](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80) return fn(*args_all, **kw). [82](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:82) args_pos: P.args. [83](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:83) args_pos, args_rest = args_all[:n_positional], args_all[n_posit",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:1368,security,ssh,ssh-,1368,"nt=6&line=6) if filename.endswith('.h5ad'):. [7](vscode-notebook-cell:?execution_count=6&line=7) filepath = os.path.join(directory, filename). ----> [8](vscode-notebook-cell:?execution_count=6&line=8) adata = sc.read(filepath). [10](vscode-notebook-cell:?execution_count=6&line=10) # Rename columns with periods in `.obs` attribute. [11](vscode-notebook-cell:?execution_count=6&line=11) for col in adata.obs.columns:. File /data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). [77](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:77) @wraps(fn). [78](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:78) def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. [79](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:79) if len(args_all) <= n_positional:. ---> [80](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80) return fn(*args_all, **kw). [82](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:82) args_pos: P.args. [83](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:83) args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs). [122](https://vscode-re",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:1572,security,ssh,ssh-,1572,"ta = sc.read(filepath). [10](vscode-notebook-cell:?execution_count=6&line=10) # Rename columns with periods in `.obs` attribute. [11](vscode-notebook-cell:?execution_count=6&line=11) for col in adata.obs.columns:. File /data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). [77](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:77) @wraps(fn). [78](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:78) def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. [79](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:79) if len(args_all) <= n_positional:. ---> [80](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80) return fn(*args_all, **kw). [82](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:82) args_pos: P.args. [83](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:83) args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs). [122](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:122) filename = Path(filename) # allow passing strings. [123](https://vsco",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:1764,security,ssh,ssh-,1764,"n adata.obs.columns:. File /data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). [77](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:77) @wraps(fn). [78](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:78) def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. [79](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:79) if len(args_all) <= n_positional:. ---> [80](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80) return fn(*args_all, **kw). [82](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:82) args_pos: P.args. [83](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:83) args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs). [122](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:122) filename = Path(filename) # allow passing strings. [123](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://v",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:1946,security,ssh,ssh-,1946,"tps://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:77) @wraps(fn). [78](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:78) def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. [79](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:79) if len(args_all) <= n_positional:. ---> [80](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80) return fn(*args_all, **kw). [82](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:82) args_pos: P.args. [83](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:83) args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs). [122](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:122) filename = Path(filename) # allow passing strings. [123](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(. [125](https://vscode-rem",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:2375,security,ssh,ssh-,2375,"emote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:79) if len(args_all) <= n_positional:. ---> [80](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80) return fn(*args_all, **kw). [82](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:82) args_pos: P.args. [83](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:83) args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs). [122](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:122) filename = Path(filename) # allow passing strings. [123](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(. [125](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:125) filename,. [126](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:126) backed=backed,. [127](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/py",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:2584,security,ssh,ssh-,2584,"-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80) return fn(*args_all, **kw). [82](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:82) args_pos: P.args. [83](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:83) args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs). [122](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:122) filename = Path(filename) # allow passing strings. [123](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(. [125](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:125) filename,. [126](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:126) backed=backed,. [127](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:127) sheet=sheet,. [128](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwri",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:2779,security,ssh,ssh-,2779,"2baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:82) args_pos: P.args. [83](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:83) args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs). [122](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:122) filename = Path(filename) # allow passing strings. [123](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(. [125](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:125) filename,. [126](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:126) backed=backed,. [127](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:127) sheet=sheet,. [128](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:128) ext=ext,. ... [65](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:65) anno =",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:2952,security,ssh,ssh-,2952,"remote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:83) args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs). [122](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:122) filename = Path(filename) # allow passing strings. [123](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(. [125](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:125) filename,. [126](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:126) backed=backed,. [127](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:127) sheet=sheet,. [128](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:128) ext=ext,. ... [65](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:65) anno = anno.copy(deep=False). [66](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/ali",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:3121,security,ssh,ssh-,3121,"onal], args_all[n_positional:]. File /data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs). [122](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:122) filename = Path(filename) # allow passing strings. [123](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(. [125](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:125) filename,. [126](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:126) backed=backed,. [127](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:127) sheet=sheet,. [128](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:128) ext=ext,. ... [65](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:65) anno = anno.copy(deep=False). [66](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:66) if not is_string_dtype(anno.index):. ValueError: Observations annot. `var` must have as many rows as `X` has columns (1), but has 33538 rows. Error raised",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:3295,security,ssh,ssh-,3295," backup_url, cache, cache_compression, **kwargs). [122](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:122) filename = Path(filename) # allow passing strings. [123](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(. [125](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:125) filename,. [126](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:126) backed=backed,. [127](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:127) sheet=sheet,. [128](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:128) ext=ext,. ... [65](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:65) anno = anno.copy(deep=False). [66](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:66) if not is_string_dtype(anno.index):. ValueError: Observations annot. `var` must have as many rows as `X` has columns (1), but has 33538 rows. Error raised while reading key '' of <class 'h5py._hl.files.File'> from /. Output is truncated. View as a [scrollable element](command:cellOutput.enableScrolling?21de049a-219f-41e8-9d9d-",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:3467,security,ssh,ssh-,3467,"e-packages/scanpy/readwrite.py:122) filename = Path(filename) # allow passing strings. [123](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(. [125](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:125) filename,. [126](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:126) backed=backed,. [127](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:127) sheet=sheet,. [128](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:128) ext=ext,. ... [65](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:65) anno = anno.copy(deep=False). [66](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:66) if not is_string_dtype(anno.index):. ValueError: Observations annot. `var` must have as many rows as `X` has columns (1), but has 33538 rows. Error raised while reading key '' of <class 'h5py._hl.files.File'> from /. Output is truncated. View as a [scrollable element](command:cellOutput.enableScrolling?21de049a-219f-41e8-9d9d-5aabf61d8031) or open in a [text editor](command:workbench.action.openLargeOutput?21de049a-219f-41e8-9d9d-5aabf61d8031). Adjust cell output [settings](command:workbench.act",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:3638,security,ssh,ssh-,3638,"ce.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(. [125](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:125) filename,. [126](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:126) backed=backed,. [127](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:127) sheet=sheet,. [128](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:128) ext=ext,. ... [65](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:65) anno = anno.copy(deep=False). [66](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:66) if not is_string_dtype(anno.index):. ValueError: Observations annot. `var` must have as many rows as `X` has columns (1), but has 33538 rows. Error raised while reading key '' of <class 'h5py._hl.files.File'> from /. Output is truncated. View as a [scrollable element](command:cellOutput.enableScrolling?21de049a-219f-41e8-9d9d-5aabf61d8031) or open in a [text editor](command:workbench.action.openLargeOutput?21de049a-219f-41e8-9d9d-5aabf61d8031). Adjust cell output [settings](command:workbench.action.openSettings?%5B%22%40tag%3AnotebookOutputLayout%22%5D)... ```. This is my version:. ```. print(sc.__version__). print(ad.__version__). 1.10.0. 0.10.6. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:3832,security,ssh,ssh-,3832,"ce.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(. [125](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:125) filename,. [126](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:126) backed=backed,. [127](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:127) sheet=sheet,. [128](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:128) ext=ext,. ... [65](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:65) anno = anno.copy(deep=False). [66](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:66) if not is_string_dtype(anno.index):. ValueError: Observations annot. `var` must have as many rows as `X` has columns (1), but has 33538 rows. Error raised while reading key '' of <class 'h5py._hl.files.File'> from /. Output is truncated. View as a [scrollable element](command:cellOutput.enableScrolling?21de049a-219f-41e8-9d9d-5aabf61d8031) or open in a [text editor](command:workbench.action.openLargeOutput?21de049a-219f-41e8-9d9d-5aabf61d8031). Adjust cell output [settings](command:workbench.action.openSettings?%5B%22%40tag%3AnotebookOutputLayout%22%5D)... ```. This is my version:. ```. print(sc.__version__). print(ad.__version__). 1.10.0. 0.10.6. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:226,testability,Trace,Traceback,226,"I run into this bug too. when i run:. ```. import scanpy as sc. import anndata as ad. adata = sc.read(filepath). ```. it turns out:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). Cell In[6], [line 8](vscode-notebook-cell:?execution_count=6&line=8). [6](vscode-notebook-cell:?execution_count=6&line=6) if filename.endswith('.h5ad'):. [7](vscode-notebook-cell:?execution_count=6&line=7) filepath = os.path.join(directory, filename). ----> [8](vscode-notebook-cell:?execution_count=6&line=8) adata = sc.read(filepath). [10](vscode-notebook-cell:?execution_count=6&line=10) # Rename columns with periods in `.obs` attribute. [11](vscode-notebook-cell:?execution_count=6&line=11) for col in adata.obs.columns:. File /data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). [77](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:77) @wraps(fn). [78](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:78) def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. [79](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:79) if len(args_all) <= n_positional:. ---> [80](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80) return fn(*args_all, **kw). [82](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:82) args_pos: P.args. [83](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:1006,testability,resourc,resource,1006," this bug too. when i run:. ```. import scanpy as sc. import anndata as ad. adata = sc.read(filepath). ```. it turns out:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). Cell In[6], [line 8](vscode-notebook-cell:?execution_count=6&line=8). [6](vscode-notebook-cell:?execution_count=6&line=6) if filename.endswith('.h5ad'):. [7](vscode-notebook-cell:?execution_count=6&line=7) filepath = os.path.join(directory, filename). ----> [8](vscode-notebook-cell:?execution_count=6&line=8) adata = sc.read(filepath). [10](vscode-notebook-cell:?execution_count=6&line=10) # Rename columns with periods in `.obs` attribute. [11](vscode-notebook-cell:?execution_count=6&line=11) for col in adata.obs.columns:. File /data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). [77](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:77) @wraps(fn). [78](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:78) def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. [79](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:79) if len(args_all) <= n_positional:. ---> [80](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80) return fn(*args_all, **kw). [82](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:82) args_pos: P.args. [83](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/da",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:1182,testability,resourc,resource,1182,"---------------------------. ValueError Traceback (most recent call last). Cell In[6], [line 8](vscode-notebook-cell:?execution_count=6&line=8). [6](vscode-notebook-cell:?execution_count=6&line=6) if filename.endswith('.h5ad'):. [7](vscode-notebook-cell:?execution_count=6&line=7) filepath = os.path.join(directory, filename). ----> [8](vscode-notebook-cell:?execution_count=6&line=8) adata = sc.read(filepath). [10](vscode-notebook-cell:?execution_count=6&line=10) # Rename columns with periods in `.obs` attribute. [11](vscode-notebook-cell:?execution_count=6&line=11) for col in adata.obs.columns:. File /data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). [77](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:77) @wraps(fn). [78](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:78) def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. [79](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:79) if len(args_all) <= n_positional:. ---> [80](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80) return fn(*args_all, **kw). [82](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:82) args_pos: P.args. [83](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:83) args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /data/LSY/venv/lib/python3",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:1406,testability,resourc,resource,1406,"'):. [7](vscode-notebook-cell:?execution_count=6&line=7) filepath = os.path.join(directory, filename). ----> [8](vscode-notebook-cell:?execution_count=6&line=8) adata = sc.read(filepath). [10](vscode-notebook-cell:?execution_count=6&line=10) # Rename columns with periods in `.obs` attribute. [11](vscode-notebook-cell:?execution_count=6&line=11) for col in adata.obs.columns:. File /data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). [77](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:77) @wraps(fn). [78](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:78) def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. [79](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:79) if len(args_all) <= n_positional:. ---> [80](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80) return fn(*args_all, **kw). [82](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:82) args_pos: P.args. [83](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:83) args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs). [122](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vsco",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:1610,testability,resourc,resource,1610,"book-cell:?execution_count=6&line=10) # Rename columns with periods in `.obs` attribute. [11](vscode-notebook-cell:?execution_count=6&line=11) for col in adata.obs.columns:. File /data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). [77](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:77) @wraps(fn). [78](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:78) def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. [79](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:79) if len(args_all) <= n_positional:. ---> [80](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80) return fn(*args_all, **kw). [82](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:82) args_pos: P.args. [83](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:83) args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs). [122](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:122) filename = Path(filename) # allow passing strings. [123](https://vscode-remote+ssh-002dremote-002baws-005fcpu",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:1802,testability,resourc,resource,1802,"v/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). [77](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:77) @wraps(fn). [78](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:78) def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. [79](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:79) if len(args_all) <= n_positional:. ---> [80](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80) return fn(*args_all, **kw). [82](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:82) args_pos: P.args. [83](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:83) args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs). [122](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:122) filename = Path(filename) # allow passing strings. [123](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005f",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:1984,testability,resourc,resource,1984,"ws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:77) @wraps(fn). [78](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:78) def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. [79](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:79) if len(args_all) <= n_positional:. ---> [80](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80) return fn(*args_all, **kw). [82](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:82) args_pos: P.args. [83](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:83) args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs). [122](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:122) filename = Path(filename) # allow passing strings. [123](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(. [125](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscod",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:2413,testability,resourc,resource,2413,"code-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:79) if len(args_all) <= n_positional:. ---> [80](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80) return fn(*args_all, **kw). [82](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:82) args_pos: P.args. [83](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:83) args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs). [122](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:122) filename = Path(filename) # allow passing strings. [123](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(. [125](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:125) filename,. [126](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:126) backed=backed,. [127](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:2622,testability,resourc,resource,2622,"cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80) return fn(*args_all, **kw). [82](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:82) args_pos: P.args. [83](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:83) args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs). [122](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:122) filename = Path(filename) # allow passing strings. [123](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(. [125](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:125) filename,. [126](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:126) backed=backed,. [127](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:127) sheet=sheet,. [128](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:128) ext=ext,. ... [65](https://vs",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:2817,testability,resourc,resource,2817,".net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:82) args_pos: P.args. [83](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:83) args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs). [122](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:122) filename = Path(filename) # allow passing strings. [123](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(. [125](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:125) filename,. [126](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:126) backed=backed,. [127](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:127) sheet=sheet,. [128](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:128) ext=ext,. ... [65](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:65) anno = anno.copy(deep=False). [66](https://vsc",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:2990,testability,resourc,resource,2990,"scode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:83) args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs). [122](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:122) filename = Path(filename) # allow passing strings. [123](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(. [125](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:125) filename,. [126](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:126) backed=backed,. [127](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:127) sheet=sheet,. [128](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:128) ext=ext,. ... [65](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:65) anno = anno.copy(deep=False). [66](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:66) if not is_string_dtype(an",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:3159,testability,resourc,resource,3159,"ta/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs). [122](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:122) filename = Path(filename) # allow passing strings. [123](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(. [125](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:125) filename,. [126](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:126) backed=backed,. [127](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:127) sheet=sheet,. [128](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:128) ext=ext,. ... [65](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:65) anno = anno.copy(deep=False). [66](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:66) if not is_string_dtype(anno.index):. ValueError: Observations annot. `var` must have as many rows as `X` has columns (1), but has 33538 rows. Error raised while reading key '' of <class 'h5py._h",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:3333,testability,resourc,resource,3333,"*kwargs). [122](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:122) filename = Path(filename) # allow passing strings. [123](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(. [125](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:125) filename,. [126](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:126) backed=backed,. [127](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:127) sheet=sheet,. [128](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:128) ext=ext,. ... [65](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:65) anno = anno.copy(deep=False). [66](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:66) if not is_string_dtype(anno.index):. ValueError: Observations annot. `var` must have as many rows as `X` has columns (1), but has 33538 rows. Error raised while reading key '' of <class 'h5py._hl.files.File'> from /. Output is truncated. View as a [scrollable element](command:cellOutput.enableScrolling?21de049a-219f-41e8-9d9d-5aabf61d8031) or open in a [text editor]",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:3505,testability,resourc,resource,3505,"name = Path(filename) # allow passing strings. [123](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(. [125](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:125) filename,. [126](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:126) backed=backed,. [127](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:127) sheet=sheet,. [128](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:128) ext=ext,. ... [65](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:65) anno = anno.copy(deep=False). [66](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:66) if not is_string_dtype(anno.index):. ValueError: Observations annot. `var` must have as many rows as `X` has columns (1), but has 33538 rows. Error raised while reading key '' of <class 'h5py._hl.files.File'> from /. Output is truncated. View as a [scrollable element](command:cellOutput.enableScrolling?21de049a-219f-41e8-9d9d-5aabf61d8031) or open in a [text editor](command:workbench.action.openLargeOutput?21de049a-219f-41e8-9d9d-5aabf61d8031). Adjust cell output [settings](command:workbench.action.openSettings?%5B%22%40tag%3Anotebook",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:3676,testability,resourc,resource,3676,"ce.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(. [125](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:125) filename,. [126](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:126) backed=backed,. [127](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:127) sheet=sheet,. [128](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:128) ext=ext,. ... [65](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:65) anno = anno.copy(deep=False). [66](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:66) if not is_string_dtype(anno.index):. ValueError: Observations annot. `var` must have as many rows as `X` has columns (1), but has 33538 rows. Error raised while reading key '' of <class 'h5py._hl.files.File'> from /. Output is truncated. View as a [scrollable element](command:cellOutput.enableScrolling?21de049a-219f-41e8-9d9d-5aabf61d8031) or open in a [text editor](command:workbench.action.openLargeOutput?21de049a-219f-41e8-9d9d-5aabf61d8031). Adjust cell output [settings](command:workbench.action.openSettings?%5B%22%40tag%3AnotebookOutputLayout%22%5D)... ```. This is my version:. ```. print(sc.__version__). print(ad.__version__). 1.10.0. 0.10.6. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:3870,testability,resourc,resource,3870,"ce.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(. [125](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:125) filename,. [126](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:126) backed=backed,. [127](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:127) sheet=sheet,. [128](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:128) ext=ext,. ... [65](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:65) anno = anno.copy(deep=False). [66](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:66) if not is_string_dtype(anno.index):. ValueError: Observations annot. `var` must have as many rows as `X` has columns (1), but has 33538 rows. Error raised while reading key '' of <class 'h5py._hl.files.File'> from /. Output is truncated. View as a [scrollable element](command:cellOutput.enableScrolling?21de049a-219f-41e8-9d9d-5aabf61d8031) or open in a [text editor](command:workbench.action.openLargeOutput?21de049a-219f-41e8-9d9d-5aabf61d8031). Adjust cell output [settings](command:workbench.action.openSettings?%5B%22%40tag%3AnotebookOutputLayout%22%5D)... ```. This is my version:. ```. print(sc.__version__). print(ad.__version__). 1.10.0. 0.10.6. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:4018,testability,Observ,Observations,4018,"ce.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(. [125](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:125) filename,. [126](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:126) backed=backed,. [127](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:127) sheet=sheet,. [128](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:128) ext=ext,. ... [65](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:65) anno = anno.copy(deep=False). [66](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:66) if not is_string_dtype(anno.index):. ValueError: Observations annot. `var` must have as many rows as `X` has columns (1), but has 33538 rows. Error raised while reading key '' of <class 'h5py._hl.files.File'> from /. Output is truncated. View as a [scrollable element](command:cellOutput.enableScrolling?21de049a-219f-41e8-9d9d-5aabf61d8031) or open in a [text editor](command:workbench.action.openLargeOutput?21de049a-219f-41e8-9d9d-5aabf61d8031). Adjust cell output [settings](command:workbench.action.openSettings?%5B%22%40tag%3AnotebookOutputLayout%22%5D)... ```. This is my version:. ```. print(sc.__version__). print(ad.__version__). 1.10.0. 0.10.6. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:4111,usability,Error,Error,4111,"ce.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(. [125](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:125) filename,. [126](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:126) backed=backed,. [127](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:127) sheet=sheet,. [128](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:128) ext=ext,. ... [65](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:65) anno = anno.copy(deep=False). [66](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:66) if not is_string_dtype(anno.index):. ValueError: Observations annot. `var` must have as many rows as `X` has columns (1), but has 33538 rows. Error raised while reading key '' of <class 'h5py._hl.files.File'> from /. Output is truncated. View as a [scrollable element](command:cellOutput.enableScrolling?21de049a-219f-41e8-9d9d-5aabf61d8031) or open in a [text editor](command:workbench.action.openLargeOutput?21de049a-219f-41e8-9d9d-5aabf61d8031). Adjust cell output [settings](command:workbench.action.openSettings?%5B%22%40tag%3AnotebookOutputLayout%22%5D)... ```. This is my version:. ```. print(sc.__version__). print(ad.__version__). 1.10.0. 0.10.6. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:4238,usability,command,command,4238,"ce.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(. [125](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:125) filename,. [126](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:126) backed=backed,. [127](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:127) sheet=sheet,. [128](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:128) ext=ext,. ... [65](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:65) anno = anno.copy(deep=False). [66](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:66) if not is_string_dtype(anno.index):. ValueError: Observations annot. `var` must have as many rows as `X` has columns (1), but has 33538 rows. Error raised while reading key '' of <class 'h5py._hl.files.File'> from /. Output is truncated. View as a [scrollable element](command:cellOutput.enableScrolling?21de049a-219f-41e8-9d9d-5aabf61d8031) or open in a [text editor](command:workbench.action.openLargeOutput?21de049a-219f-41e8-9d9d-5aabf61d8031). Adjust cell output [settings](command:workbench.action.openSettings?%5B%22%40tag%3AnotebookOutputLayout%22%5D)... ```. This is my version:. ```. print(sc.__version__). print(ad.__version__). 1.10.0. 0.10.6. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:4338,usability,command,command,4338,"ce.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(. [125](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:125) filename,. [126](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:126) backed=backed,. [127](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:127) sheet=sheet,. [128](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:128) ext=ext,. ... [65](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:65) anno = anno.copy(deep=False). [66](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:66) if not is_string_dtype(anno.index):. ValueError: Observations annot. `var` must have as many rows as `X` has columns (1), but has 33538 rows. Error raised while reading key '' of <class 'h5py._hl.files.File'> from /. Output is truncated. View as a [scrollable element](command:cellOutput.enableScrolling?21de049a-219f-41e8-9d9d-5aabf61d8031) or open in a [text editor](command:workbench.action.openLargeOutput?21de049a-219f-41e8-9d9d-5aabf61d8031). Adjust cell output [settings](command:workbench.action.openSettings?%5B%22%40tag%3AnotebookOutputLayout%22%5D)... ```. This is my version:. ```. print(sc.__version__). print(ad.__version__). 1.10.0. 0.10.6. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/323:4448,usability,command,command,4448,"ce.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):. --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(. [125](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:125) filename,. [126](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:126) backed=backed,. [127](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:127) sheet=sheet,. [128](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:128) ext=ext,. ... [65](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:65) anno = anno.copy(deep=False). [66](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/anndata/_core/aligned_df.py:66) if not is_string_dtype(anno.index):. ValueError: Observations annot. `var` must have as many rows as `X` has columns (1), but has 33538 rows. Error raised while reading key '' of <class 'h5py._hl.files.File'> from /. Output is truncated. View as a [scrollable element](command:cellOutput.enableScrolling?21de049a-219f-41e8-9d9d-5aabf61d8031) or open in a [text editor](command:workbench.action.openLargeOutput?21de049a-219f-41e8-9d9d-5aabf61d8031). Adjust cell output [settings](command:workbench.action.openSettings?%5B%22%40tag%3AnotebookOutputLayout%22%5D)... ```. This is my version:. ```. print(sc.__version__). print(ad.__version__). 1.10.0. 0.10.6. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323
https://github.com/scverse/scanpy/issues/324:129,energy efficiency,reduc,reduced,129,"So I've been sitting offline with @VolkerBergen trying to get to the bottom of this. It seems that the precision of `adata.X` is reduced after subsetting. This is the case when using either of:. ```. adata_hvg = adata_hvg[:, disp_filter['gene_subset']]. adata_hvg._inplace_subset_var(disp_filter['gene_subset']). ```. Either way `adata[:,disp_filter['gene_subset']].X` gives a higher precision than `adata_hvg.X`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/324
https://github.com/scverse/scanpy/issues/324:143,integrability,sub,subsetting,143,"So I've been sitting offline with @VolkerBergen trying to get to the bottom of this. It seems that the precision of `adata.X` is reduced after subsetting. This is the case when using either of:. ```. adata_hvg = adata_hvg[:, disp_filter['gene_subset']]. adata_hvg._inplace_subset_var(disp_filter['gene_subset']). ```. Either way `adata[:,disp_filter['gene_subset']].X` gives a higher precision than `adata_hvg.X`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/324
https://github.com/scverse/scanpy/issues/324:115,deployability,automat,automatically,115,I have not set the precision to float64 manually anywhere. It may however be the case that ComBat batch correction automatically uses float64. I will test and let you know.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/324
https://github.com/scverse/scanpy/issues/324:98,integrability,batch,batch,98,I have not set the precision to float64 manually anywhere. It may however be the case that ComBat batch correction automatically uses float64. I will test and let you know.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/324
https://github.com/scverse/scanpy/issues/324:98,performance,batch,batch,98,I have not set the precision to float64 manually anywhere. It may however be the case that ComBat batch correction automatically uses float64. I will test and let you know.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/324
https://github.com/scverse/scanpy/issues/324:150,safety,test,test,150,I have not set the precision to float64 manually anywhere. It may however be the case that ComBat batch correction automatically uses float64. I will test and let you know.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/324
https://github.com/scverse/scanpy/issues/324:115,testability,automat,automatically,115,I have not set the precision to float64 manually anywhere. It may however be the case that ComBat batch correction automatically uses float64. I will test and let you know.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/324
https://github.com/scverse/scanpy/issues/324:150,testability,test,test,150,I have not set the precision to float64 manually anywhere. It may however be the case that ComBat batch correction automatically uses float64. I will test and let you know.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/324
https://github.com/scverse/scanpy/issues/324:110,deployability,updat,update,110,So it seems ComBat outputs np.float64 . I assume with the anndata fix that should be fine now though? I will update and rerun...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/324
https://github.com/scverse/scanpy/issues/324:110,safety,updat,update,110,So it seems ComBat outputs np.float64 . I assume with the anndata fix that should be fine now though? I will update and rerun...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/324
https://github.com/scverse/scanpy/issues/324:110,security,updat,update,110,So it seems ComBat outputs np.float64 . I assume with the anndata fix that should be fine now though? I will update and rerun...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/324
https://github.com/scverse/scanpy/issues/325:535,availability,cluster,clustering,535,"1. It gives exactly the same results when rerunning on the same platform, but scikit-learn's randomized PCA sometimes fails to reproduce exactly the same results when run on different platforms. That's why since some time, the warning is output. 2. Setting `svd_solver='arpack'` resolves that problem. 3. That's probably hard, as at the time, the results were produced using the randomized version expecting that setting the same seed on a different would reproduce this also elsewhere. 4. What I uploaded for you at the time were the clustering results shown in the PAGA paper and the Scanpy paper, they are still linked in the issue in the PAGA repo: https://github.com/theislab/paga/issues/1#issuecomment-404263982; I can easily upload these clustering results also to https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells, that's no problem at all. There was one issue with right? The vector is shifted by one cell or something? Can I change something so that the problem you had at the time doesn't come up again? Sorry about the whole issue!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:745,availability,cluster,clustering,745,"1. It gives exactly the same results when rerunning on the same platform, but scikit-learn's randomized PCA sometimes fails to reproduce exactly the same results when run on different platforms. That's why since some time, the warning is output. 2. Setting `svd_solver='arpack'` resolves that problem. 3. That's probably hard, as at the time, the results were produced using the randomized version expecting that setting the same seed on a different would reproduce this also elsewhere. 4. What I uploaded for you at the time were the clustering results shown in the PAGA paper and the Scanpy paper, they are still linked in the issue in the PAGA repo: https://github.com/theislab/paga/issues/1#issuecomment-404263982; I can easily upload these clustering results also to https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells, that's no problem at all. There was one issue with right? The vector is shifted by one cell or something? Can I change something so that the problem you had at the time doesn't come up again? Sorry about the whole issue!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:118,deployability,fail,fails,118,"1. It gives exactly the same results when rerunning on the same platform, but scikit-learn's randomized PCA sometimes fails to reproduce exactly the same results when run on different platforms. That's why since some time, the warning is output. 2. Setting `svd_solver='arpack'` resolves that problem. 3. That's probably hard, as at the time, the results were produced using the randomized version expecting that setting the same seed on a different would reproduce this also elsewhere. 4. What I uploaded for you at the time were the clustering results shown in the PAGA paper and the Scanpy paper, they are still linked in the issue in the PAGA repo: https://github.com/theislab/paga/issues/1#issuecomment-404263982; I can easily upload these clustering results also to https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells, that's no problem at all. There was one issue with right? The vector is shifted by one cell or something? Can I change something so that the problem you had at the time doesn't come up again? Sorry about the whole issue!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:390,deployability,version,version,390,"1. It gives exactly the same results when rerunning on the same platform, but scikit-learn's randomized PCA sometimes fails to reproduce exactly the same results when run on different platforms. That's why since some time, the warning is output. 2. Setting `svd_solver='arpack'` resolves that problem. 3. That's probably hard, as at the time, the results were produced using the randomized version expecting that setting the same seed on a different would reproduce this also elsewhere. 4. What I uploaded for you at the time were the clustering results shown in the PAGA paper and the Scanpy paper, they are still linked in the issue in the PAGA repo: https://github.com/theislab/paga/issues/1#issuecomment-404263982; I can easily upload these clustering results also to https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells, that's no problem at all. There was one issue with right? The vector is shifted by one cell or something? Can I change something so that the problem you had at the time doesn't come up again? Sorry about the whole issue!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:535,deployability,cluster,clustering,535,"1. It gives exactly the same results when rerunning on the same platform, but scikit-learn's randomized PCA sometimes fails to reproduce exactly the same results when run on different platforms. That's why since some time, the warning is output. 2. Setting `svd_solver='arpack'` resolves that problem. 3. That's probably hard, as at the time, the results were produced using the randomized version expecting that setting the same seed on a different would reproduce this also elsewhere. 4. What I uploaded for you at the time were the clustering results shown in the PAGA paper and the Scanpy paper, they are still linked in the issue in the PAGA repo: https://github.com/theislab/paga/issues/1#issuecomment-404263982; I can easily upload these clustering results also to https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells, that's no problem at all. There was one issue with right? The vector is shifted by one cell or something? Can I change something so that the problem you had at the time doesn't come up again? Sorry about the whole issue!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:745,deployability,cluster,clustering,745,"1. It gives exactly the same results when rerunning on the same platform, but scikit-learn's randomized PCA sometimes fails to reproduce exactly the same results when run on different platforms. That's why since some time, the warning is output. 2. Setting `svd_solver='arpack'` resolves that problem. 3. That's probably hard, as at the time, the results were produced using the randomized version expecting that setting the same seed on a different would reproduce this also elsewhere. 4. What I uploaded for you at the time were the clustering results shown in the PAGA paper and the Scanpy paper, they are still linked in the issue in the PAGA repo: https://github.com/theislab/paga/issues/1#issuecomment-404263982; I can easily upload these clustering results also to https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells, that's no problem at all. There was one issue with right? The vector is shifted by one cell or something? Can I change something so that the problem you had at the time doesn't come up again? Sorry about the whole issue!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:390,integrability,version,version,390,"1. It gives exactly the same results when rerunning on the same platform, but scikit-learn's randomized PCA sometimes fails to reproduce exactly the same results when run on different platforms. That's why since some time, the warning is output. 2. Setting `svd_solver='arpack'` resolves that problem. 3. That's probably hard, as at the time, the results were produced using the randomized version expecting that setting the same seed on a different would reproduce this also elsewhere. 4. What I uploaded for you at the time were the clustering results shown in the PAGA paper and the Scanpy paper, they are still linked in the issue in the PAGA repo: https://github.com/theislab/paga/issues/1#issuecomment-404263982; I can easily upload these clustering results also to https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells, that's no problem at all. There was one issue with right? The vector is shifted by one cell or something? Can I change something so that the problem you had at the time doesn't come up again? Sorry about the whole issue!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:64,interoperability,platform,platform,64,"1. It gives exactly the same results when rerunning on the same platform, but scikit-learn's randomized PCA sometimes fails to reproduce exactly the same results when run on different platforms. That's why since some time, the warning is output. 2. Setting `svd_solver='arpack'` resolves that problem. 3. That's probably hard, as at the time, the results were produced using the randomized version expecting that setting the same seed on a different would reproduce this also elsewhere. 4. What I uploaded for you at the time were the clustering results shown in the PAGA paper and the Scanpy paper, they are still linked in the issue in the PAGA repo: https://github.com/theislab/paga/issues/1#issuecomment-404263982; I can easily upload these clustering results also to https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells, that's no problem at all. There was one issue with right? The vector is shifted by one cell or something? Can I change something so that the problem you had at the time doesn't come up again? Sorry about the whole issue!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:184,interoperability,platform,platforms,184,"1. It gives exactly the same results when rerunning on the same platform, but scikit-learn's randomized PCA sometimes fails to reproduce exactly the same results when run on different platforms. That's why since some time, the warning is output. 2. Setting `svd_solver='arpack'` resolves that problem. 3. That's probably hard, as at the time, the results were produced using the randomized version expecting that setting the same seed on a different would reproduce this also elsewhere. 4. What I uploaded for you at the time were the clustering results shown in the PAGA paper and the Scanpy paper, they are still linked in the issue in the PAGA repo: https://github.com/theislab/paga/issues/1#issuecomment-404263982; I can easily upload these clustering results also to https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells, that's no problem at all. There was one issue with right? The vector is shifted by one cell or something? Can I change something so that the problem you had at the time doesn't come up again? Sorry about the whole issue!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:390,modifiability,version,version,390,"1. It gives exactly the same results when rerunning on the same platform, but scikit-learn's randomized PCA sometimes fails to reproduce exactly the same results when run on different platforms. That's why since some time, the warning is output. 2. Setting `svd_solver='arpack'` resolves that problem. 3. That's probably hard, as at the time, the results were produced using the randomized version expecting that setting the same seed on a different would reproduce this also elsewhere. 4. What I uploaded for you at the time were the clustering results shown in the PAGA paper and the Scanpy paper, they are still linked in the issue in the PAGA repo: https://github.com/theislab/paga/issues/1#issuecomment-404263982; I can easily upload these clustering results also to https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells, that's no problem at all. There was one issue with right? The vector is shifted by one cell or something? Can I change something so that the problem you had at the time doesn't come up again? Sorry about the whole issue!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:217,performance,time,time,217,"1. It gives exactly the same results when rerunning on the same platform, but scikit-learn's randomized PCA sometimes fails to reproduce exactly the same results when run on different platforms. That's why since some time, the warning is output. 2. Setting `svd_solver='arpack'` resolves that problem. 3. That's probably hard, as at the time, the results were produced using the randomized version expecting that setting the same seed on a different would reproduce this also elsewhere. 4. What I uploaded for you at the time were the clustering results shown in the PAGA paper and the Scanpy paper, they are still linked in the issue in the PAGA repo: https://github.com/theislab/paga/issues/1#issuecomment-404263982; I can easily upload these clustering results also to https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells, that's no problem at all. There was one issue with right? The vector is shifted by one cell or something? Can I change something so that the problem you had at the time doesn't come up again? Sorry about the whole issue!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:337,performance,time,time,337,"1. It gives exactly the same results when rerunning on the same platform, but scikit-learn's randomized PCA sometimes fails to reproduce exactly the same results when run on different platforms. That's why since some time, the warning is output. 2. Setting `svd_solver='arpack'` resolves that problem. 3. That's probably hard, as at the time, the results were produced using the randomized version expecting that setting the same seed on a different would reproduce this also elsewhere. 4. What I uploaded for you at the time were the clustering results shown in the PAGA paper and the Scanpy paper, they are still linked in the issue in the PAGA repo: https://github.com/theislab/paga/issues/1#issuecomment-404263982; I can easily upload these clustering results also to https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells, that's no problem at all. There was one issue with right? The vector is shifted by one cell or something? Can I change something so that the problem you had at the time doesn't come up again? Sorry about the whole issue!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:521,performance,time,time,521,"1. It gives exactly the same results when rerunning on the same platform, but scikit-learn's randomized PCA sometimes fails to reproduce exactly the same results when run on different platforms. That's why since some time, the warning is output. 2. Setting `svd_solver='arpack'` resolves that problem. 3. That's probably hard, as at the time, the results were produced using the randomized version expecting that setting the same seed on a different would reproduce this also elsewhere. 4. What I uploaded for you at the time were the clustering results shown in the PAGA paper and the Scanpy paper, they are still linked in the issue in the PAGA repo: https://github.com/theislab/paga/issues/1#issuecomment-404263982; I can easily upload these clustering results also to https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells, that's no problem at all. There was one issue with right? The vector is shifted by one cell or something? Can I change something so that the problem you had at the time doesn't come up again? Sorry about the whole issue!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:1027,performance,time,time,1027,"1. It gives exactly the same results when rerunning on the same platform, but scikit-learn's randomized PCA sometimes fails to reproduce exactly the same results when run on different platforms. That's why since some time, the warning is output. 2. Setting `svd_solver='arpack'` resolves that problem. 3. That's probably hard, as at the time, the results were produced using the randomized version expecting that setting the same seed on a different would reproduce this also elsewhere. 4. What I uploaded for you at the time were the clustering results shown in the PAGA paper and the Scanpy paper, they are still linked in the issue in the PAGA repo: https://github.com/theislab/paga/issues/1#issuecomment-404263982; I can easily upload these clustering results also to https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells, that's no problem at all. There was one issue with right? The vector is shifted by one cell or something? Can I change something so that the problem you had at the time doesn't come up again? Sorry about the whole issue!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:118,reliability,fail,fails,118,"1. It gives exactly the same results when rerunning on the same platform, but scikit-learn's randomized PCA sometimes fails to reproduce exactly the same results when run on different platforms. That's why since some time, the warning is output. 2. Setting `svd_solver='arpack'` resolves that problem. 3. That's probably hard, as at the time, the results were produced using the randomized version expecting that setting the same seed on a different would reproduce this also elsewhere. 4. What I uploaded for you at the time were the clustering results shown in the PAGA paper and the Scanpy paper, they are still linked in the issue in the PAGA repo: https://github.com/theislab/paga/issues/1#issuecomment-404263982; I can easily upload these clustering results also to https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells, that's no problem at all. There was one issue with right? The vector is shifted by one cell or something? Can I change something so that the problem you had at the time doesn't come up again? Sorry about the whole issue!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:1032,reliability,doe,doesn,1032,"1. It gives exactly the same results when rerunning on the same platform, but scikit-learn's randomized PCA sometimes fails to reproduce exactly the same results when run on different platforms. That's why since some time, the warning is output. 2. Setting `svd_solver='arpack'` resolves that problem. 3. That's probably hard, as at the time, the results were produced using the randomized version expecting that setting the same seed on a different would reproduce this also elsewhere. 4. What I uploaded for you at the time were the clustering results shown in the PAGA paper and the Scanpy paper, they are still linked in the issue in the PAGA repo: https://github.com/theislab/paga/issues/1#issuecomment-404263982; I can easily upload these clustering results also to https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells, that's no problem at all. There was one issue with right? The vector is shifted by one cell or something? Can I change something so that the problem you had at the time doesn't come up again? Sorry about the whole issue!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:85,usability,learn,learn,85,"1. It gives exactly the same results when rerunning on the same platform, but scikit-learn's randomized PCA sometimes fails to reproduce exactly the same results when run on different platforms. That's why since some time, the warning is output. 2. Setting `svd_solver='arpack'` resolves that problem. 3. That's probably hard, as at the time, the results were produced using the randomized version expecting that setting the same seed on a different would reproduce this also elsewhere. 4. What I uploaded for you at the time were the clustering results shown in the PAGA paper and the Scanpy paper, they are still linked in the issue in the PAGA repo: https://github.com/theislab/paga/issues/1#issuecomment-404263982; I can easily upload these clustering results also to https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells, that's no problem at all. There was one issue with right? The vector is shifted by one cell or something? Can I change something so that the problem you had at the time doesn't come up again? Sorry about the whole issue!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:55,availability,cluster,clustering,55,"Starting from the end: I think if you could upload the clustering results from the Scanpy paper / PAGA preprint to the scanpy github repo, it would be great. I still have the dropbox link of course, but I guess in the long run it's better if that file was located here and linked from the https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells page. The issue with 1 cell missing was because I did not specify `header=None` when loading it with Pandas :) So my error, not yours. The file is correct as is. That said, I am worried about the influence the random seed in randomized PCA seems to give in this case. Let me show you how it looks:. ![mln-tsne-clustering-comparison](https://user-images.githubusercontent.com/8970231/47555195-71af9480-d90b-11e8-85fb-a3e8dcb7a66f.png). I would be fine with some cells getting into other clusters depending on the random seed, and it would even be okay if small clusters changed their identities, but what we see here is a very drastic change of the cluster structure. Are you sure that the only difference is the randomized PCA outcome? Can it be that some of the default parameters in `sc.pp.recipe_zheng17`, `sc.pp.neighbors`, or `sc.tl.louvain` changed since when you ran the clustering? The scanpy code I posted above is the full code I used, and I ran it yesterday after updating scanpy via pip. BTW, the visualization above is taken from https://www.biorxiv.org/content/early/2018/10/25/453449 which we posted yesterday. Any comments very welcome! I hope you don't mind being thanked in the acknowledgements!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:496,availability,error,error,496,"Starting from the end: I think if you could upload the clustering results from the Scanpy paper / PAGA preprint to the scanpy github repo, it would be great. I still have the dropbox link of course, but I guess in the long run it's better if that file was located here and linked from the https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells page. The issue with 1 cell missing was because I did not specify `header=None` when loading it with Pandas :) So my error, not yours. The file is correct as is. That said, I am worried about the influence the random seed in randomized PCA seems to give in this case. Let me show you how it looks:. ![mln-tsne-clustering-comparison](https://user-images.githubusercontent.com/8970231/47555195-71af9480-d90b-11e8-85fb-a3e8dcb7a66f.png). I would be fine with some cells getting into other clusters depending on the random seed, and it would even be okay if small clusters changed their identities, but what we see here is a very drastic change of the cluster structure. Are you sure that the only difference is the randomized PCA outcome? Can it be that some of the default parameters in `sc.pp.recipe_zheng17`, `sc.pp.neighbors`, or `sc.tl.louvain` changed since when you ran the clustering? The scanpy code I posted above is the full code I used, and I ran it yesterday after updating scanpy via pip. BTW, the visualization above is taken from https://www.biorxiv.org/content/early/2018/10/25/453449 which we posted yesterday. Any comments very welcome! I hope you don't mind being thanked in the acknowledgements!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:689,availability,cluster,clustering-comparison,689,"Starting from the end: I think if you could upload the clustering results from the Scanpy paper / PAGA preprint to the scanpy github repo, it would be great. I still have the dropbox link of course, but I guess in the long run it's better if that file was located here and linked from the https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells page. The issue with 1 cell missing was because I did not specify `header=None` when loading it with Pandas :) So my error, not yours. The file is correct as is. That said, I am worried about the influence the random seed in randomized PCA seems to give in this case. Let me show you how it looks:. ![mln-tsne-clustering-comparison](https://user-images.githubusercontent.com/8970231/47555195-71af9480-d90b-11e8-85fb-a3e8dcb7a66f.png). I would be fine with some cells getting into other clusters depending on the random seed, and it would even be okay if small clusters changed their identities, but what we see here is a very drastic change of the cluster structure. Are you sure that the only difference is the randomized PCA outcome? Can it be that some of the default parameters in `sc.pp.recipe_zheng17`, `sc.pp.neighbors`, or `sc.tl.louvain` changed since when you ran the clustering? The scanpy code I posted above is the full code I used, and I ran it yesterday after updating scanpy via pip. BTW, the visualization above is taken from https://www.biorxiv.org/content/early/2018/10/25/453449 which we posted yesterday. Any comments very welcome! I hope you don't mind being thanked in the acknowledgements!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:865,availability,cluster,clusters,865,"Starting from the end: I think if you could upload the clustering results from the Scanpy paper / PAGA preprint to the scanpy github repo, it would be great. I still have the dropbox link of course, but I guess in the long run it's better if that file was located here and linked from the https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells page. The issue with 1 cell missing was because I did not specify `header=None` when loading it with Pandas :) So my error, not yours. The file is correct as is. That said, I am worried about the influence the random seed in randomized PCA seems to give in this case. Let me show you how it looks:. ![mln-tsne-clustering-comparison](https://user-images.githubusercontent.com/8970231/47555195-71af9480-d90b-11e8-85fb-a3e8dcb7a66f.png). I would be fine with some cells getting into other clusters depending on the random seed, and it would even be okay if small clusters changed their identities, but what we see here is a very drastic change of the cluster structure. Are you sure that the only difference is the randomized PCA outcome? Can it be that some of the default parameters in `sc.pp.recipe_zheng17`, `sc.pp.neighbors`, or `sc.tl.louvain` changed since when you ran the clustering? The scanpy code I posted above is the full code I used, and I ran it yesterday after updating scanpy via pip. BTW, the visualization above is taken from https://www.biorxiv.org/content/early/2018/10/25/453449 which we posted yesterday. Any comments very welcome! I hope you don't mind being thanked in the acknowledgements!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:939,availability,cluster,clusters,939,"Starting from the end: I think if you could upload the clustering results from the Scanpy paper / PAGA preprint to the scanpy github repo, it would be great. I still have the dropbox link of course, but I guess in the long run it's better if that file was located here and linked from the https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells page. The issue with 1 cell missing was because I did not specify `header=None` when loading it with Pandas :) So my error, not yours. The file is correct as is. That said, I am worried about the influence the random seed in randomized PCA seems to give in this case. Let me show you how it looks:. ![mln-tsne-clustering-comparison](https://user-images.githubusercontent.com/8970231/47555195-71af9480-d90b-11e8-85fb-a3e8dcb7a66f.png). I would be fine with some cells getting into other clusters depending on the random seed, and it would even be okay if small clusters changed their identities, but what we see here is a very drastic change of the cluster structure. Are you sure that the only difference is the randomized PCA outcome? Can it be that some of the default parameters in `sc.pp.recipe_zheng17`, `sc.pp.neighbors`, or `sc.tl.louvain` changed since when you ran the clustering? The scanpy code I posted above is the full code I used, and I ran it yesterday after updating scanpy via pip. BTW, the visualization above is taken from https://www.biorxiv.org/content/early/2018/10/25/453449 which we posted yesterday. Any comments very welcome! I hope you don't mind being thanked in the acknowledgements!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:1027,availability,cluster,cluster,1027,"Starting from the end: I think if you could upload the clustering results from the Scanpy paper / PAGA preprint to the scanpy github repo, it would be great. I still have the dropbox link of course, but I guess in the long run it's better if that file was located here and linked from the https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells page. The issue with 1 cell missing was because I did not specify `header=None` when loading it with Pandas :) So my error, not yours. The file is correct as is. That said, I am worried about the influence the random seed in randomized PCA seems to give in this case. Let me show you how it looks:. ![mln-tsne-clustering-comparison](https://user-images.githubusercontent.com/8970231/47555195-71af9480-d90b-11e8-85fb-a3e8dcb7a66f.png). I would be fine with some cells getting into other clusters depending on the random seed, and it would even be okay if small clusters changed their identities, but what we see here is a very drastic change of the cluster structure. Are you sure that the only difference is the randomized PCA outcome? Can it be that some of the default parameters in `sc.pp.recipe_zheng17`, `sc.pp.neighbors`, or `sc.tl.louvain` changed since when you ran the clustering? The scanpy code I posted above is the full code I used, and I ran it yesterday after updating scanpy via pip. BTW, the visualization above is taken from https://www.biorxiv.org/content/early/2018/10/25/453449 which we posted yesterday. Any comments very welcome! I hope you don't mind being thanked in the acknowledgements!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:1257,availability,cluster,clustering,1257,"Starting from the end: I think if you could upload the clustering results from the Scanpy paper / PAGA preprint to the scanpy github repo, it would be great. I still have the dropbox link of course, but I guess in the long run it's better if that file was located here and linked from the https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells page. The issue with 1 cell missing was because I did not specify `header=None` when loading it with Pandas :) So my error, not yours. The file is correct as is. That said, I am worried about the influence the random seed in randomized PCA seems to give in this case. Let me show you how it looks:. ![mln-tsne-clustering-comparison](https://user-images.githubusercontent.com/8970231/47555195-71af9480-d90b-11e8-85fb-a3e8dcb7a66f.png). I would be fine with some cells getting into other clusters depending on the random seed, and it would even be okay if small clusters changed their identities, but what we see here is a very drastic change of the cluster structure. Are you sure that the only difference is the randomized PCA outcome? Can it be that some of the default parameters in `sc.pp.recipe_zheng17`, `sc.pp.neighbors`, or `sc.tl.louvain` changed since when you ran the clustering? The scanpy code I posted above is the full code I used, and I ran it yesterday after updating scanpy via pip. BTW, the visualization above is taken from https://www.biorxiv.org/content/early/2018/10/25/453449 which we posted yesterday. Any comments very welcome! I hope you don't mind being thanked in the acknowledgements!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:55,deployability,cluster,clustering,55,"Starting from the end: I think if you could upload the clustering results from the Scanpy paper / PAGA preprint to the scanpy github repo, it would be great. I still have the dropbox link of course, but I guess in the long run it's better if that file was located here and linked from the https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells page. The issue with 1 cell missing was because I did not specify `header=None` when loading it with Pandas :) So my error, not yours. The file is correct as is. That said, I am worried about the influence the random seed in randomized PCA seems to give in this case. Let me show you how it looks:. ![mln-tsne-clustering-comparison](https://user-images.githubusercontent.com/8970231/47555195-71af9480-d90b-11e8-85fb-a3e8dcb7a66f.png). I would be fine with some cells getting into other clusters depending on the random seed, and it would even be okay if small clusters changed their identities, but what we see here is a very drastic change of the cluster structure. Are you sure that the only difference is the randomized PCA outcome? Can it be that some of the default parameters in `sc.pp.recipe_zheng17`, `sc.pp.neighbors`, or `sc.tl.louvain` changed since when you ran the clustering? The scanpy code I posted above is the full code I used, and I ran it yesterday after updating scanpy via pip. BTW, the visualization above is taken from https://www.biorxiv.org/content/early/2018/10/25/453449 which we posted yesterday. Any comments very welcome! I hope you don't mind being thanked in the acknowledgements!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:689,deployability,cluster,clustering-comparison,689,"Starting from the end: I think if you could upload the clustering results from the Scanpy paper / PAGA preprint to the scanpy github repo, it would be great. I still have the dropbox link of course, but I guess in the long run it's better if that file was located here and linked from the https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells page. The issue with 1 cell missing was because I did not specify `header=None` when loading it with Pandas :) So my error, not yours. The file is correct as is. That said, I am worried about the influence the random seed in randomized PCA seems to give in this case. Let me show you how it looks:. ![mln-tsne-clustering-comparison](https://user-images.githubusercontent.com/8970231/47555195-71af9480-d90b-11e8-85fb-a3e8dcb7a66f.png). I would be fine with some cells getting into other clusters depending on the random seed, and it would even be okay if small clusters changed their identities, but what we see here is a very drastic change of the cluster structure. Are you sure that the only difference is the randomized PCA outcome? Can it be that some of the default parameters in `sc.pp.recipe_zheng17`, `sc.pp.neighbors`, or `sc.tl.louvain` changed since when you ran the clustering? The scanpy code I posted above is the full code I used, and I ran it yesterday after updating scanpy via pip. BTW, the visualization above is taken from https://www.biorxiv.org/content/early/2018/10/25/453449 which we posted yesterday. Any comments very welcome! I hope you don't mind being thanked in the acknowledgements!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:865,deployability,cluster,clusters,865,"Starting from the end: I think if you could upload the clustering results from the Scanpy paper / PAGA preprint to the scanpy github repo, it would be great. I still have the dropbox link of course, but I guess in the long run it's better if that file was located here and linked from the https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells page. The issue with 1 cell missing was because I did not specify `header=None` when loading it with Pandas :) So my error, not yours. The file is correct as is. That said, I am worried about the influence the random seed in randomized PCA seems to give in this case. Let me show you how it looks:. ![mln-tsne-clustering-comparison](https://user-images.githubusercontent.com/8970231/47555195-71af9480-d90b-11e8-85fb-a3e8dcb7a66f.png). I would be fine with some cells getting into other clusters depending on the random seed, and it would even be okay if small clusters changed their identities, but what we see here is a very drastic change of the cluster structure. Are you sure that the only difference is the randomized PCA outcome? Can it be that some of the default parameters in `sc.pp.recipe_zheng17`, `sc.pp.neighbors`, or `sc.tl.louvain` changed since when you ran the clustering? The scanpy code I posted above is the full code I used, and I ran it yesterday after updating scanpy via pip. BTW, the visualization above is taken from https://www.biorxiv.org/content/early/2018/10/25/453449 which we posted yesterday. Any comments very welcome! I hope you don't mind being thanked in the acknowledgements!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:874,deployability,depend,depending,874,"Starting from the end: I think if you could upload the clustering results from the Scanpy paper / PAGA preprint to the scanpy github repo, it would be great. I still have the dropbox link of course, but I guess in the long run it's better if that file was located here and linked from the https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells page. The issue with 1 cell missing was because I did not specify `header=None` when loading it with Pandas :) So my error, not yours. The file is correct as is. That said, I am worried about the influence the random seed in randomized PCA seems to give in this case. Let me show you how it looks:. ![mln-tsne-clustering-comparison](https://user-images.githubusercontent.com/8970231/47555195-71af9480-d90b-11e8-85fb-a3e8dcb7a66f.png). I would be fine with some cells getting into other clusters depending on the random seed, and it would even be okay if small clusters changed their identities, but what we see here is a very drastic change of the cluster structure. Are you sure that the only difference is the randomized PCA outcome? Can it be that some of the default parameters in `sc.pp.recipe_zheng17`, `sc.pp.neighbors`, or `sc.tl.louvain` changed since when you ran the clustering? The scanpy code I posted above is the full code I used, and I ran it yesterday after updating scanpy via pip. BTW, the visualization above is taken from https://www.biorxiv.org/content/early/2018/10/25/453449 which we posted yesterday. Any comments very welcome! I hope you don't mind being thanked in the acknowledgements!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:939,deployability,cluster,clusters,939,"Starting from the end: I think if you could upload the clustering results from the Scanpy paper / PAGA preprint to the scanpy github repo, it would be great. I still have the dropbox link of course, but I guess in the long run it's better if that file was located here and linked from the https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells page. The issue with 1 cell missing was because I did not specify `header=None` when loading it with Pandas :) So my error, not yours. The file is correct as is. That said, I am worried about the influence the random seed in randomized PCA seems to give in this case. Let me show you how it looks:. ![mln-tsne-clustering-comparison](https://user-images.githubusercontent.com/8970231/47555195-71af9480-d90b-11e8-85fb-a3e8dcb7a66f.png). I would be fine with some cells getting into other clusters depending on the random seed, and it would even be okay if small clusters changed their identities, but what we see here is a very drastic change of the cluster structure. Are you sure that the only difference is the randomized PCA outcome? Can it be that some of the default parameters in `sc.pp.recipe_zheng17`, `sc.pp.neighbors`, or `sc.tl.louvain` changed since when you ran the clustering? The scanpy code I posted above is the full code I used, and I ran it yesterday after updating scanpy via pip. BTW, the visualization above is taken from https://www.biorxiv.org/content/early/2018/10/25/453449 which we posted yesterday. Any comments very welcome! I hope you don't mind being thanked in the acknowledgements!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:1027,deployability,cluster,cluster,1027,"Starting from the end: I think if you could upload the clustering results from the Scanpy paper / PAGA preprint to the scanpy github repo, it would be great. I still have the dropbox link of course, but I guess in the long run it's better if that file was located here and linked from the https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells page. The issue with 1 cell missing was because I did not specify `header=None` when loading it with Pandas :) So my error, not yours. The file is correct as is. That said, I am worried about the influence the random seed in randomized PCA seems to give in this case. Let me show you how it looks:. ![mln-tsne-clustering-comparison](https://user-images.githubusercontent.com/8970231/47555195-71af9480-d90b-11e8-85fb-a3e8dcb7a66f.png). I would be fine with some cells getting into other clusters depending on the random seed, and it would even be okay if small clusters changed their identities, but what we see here is a very drastic change of the cluster structure. Are you sure that the only difference is the randomized PCA outcome? Can it be that some of the default parameters in `sc.pp.recipe_zheng17`, `sc.pp.neighbors`, or `sc.tl.louvain` changed since when you ran the clustering? The scanpy code I posted above is the full code I used, and I ran it yesterday after updating scanpy via pip. BTW, the visualization above is taken from https://www.biorxiv.org/content/early/2018/10/25/453449 which we posted yesterday. Any comments very welcome! I hope you don't mind being thanked in the acknowledgements!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:1257,deployability,cluster,clustering,1257,"Starting from the end: I think if you could upload the clustering results from the Scanpy paper / PAGA preprint to the scanpy github repo, it would be great. I still have the dropbox link of course, but I guess in the long run it's better if that file was located here and linked from the https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells page. The issue with 1 cell missing was because I did not specify `header=None` when loading it with Pandas :) So my error, not yours. The file is correct as is. That said, I am worried about the influence the random seed in randomized PCA seems to give in this case. Let me show you how it looks:. ![mln-tsne-clustering-comparison](https://user-images.githubusercontent.com/8970231/47555195-71af9480-d90b-11e8-85fb-a3e8dcb7a66f.png). I would be fine with some cells getting into other clusters depending on the random seed, and it would even be okay if small clusters changed their identities, but what we see here is a very drastic change of the cluster structure. Are you sure that the only difference is the randomized PCA outcome? Can it be that some of the default parameters in `sc.pp.recipe_zheng17`, `sc.pp.neighbors`, or `sc.tl.louvain` changed since when you ran the clustering? The scanpy code I posted above is the full code I used, and I ran it yesterday after updating scanpy via pip. BTW, the visualization above is taken from https://www.biorxiv.org/content/early/2018/10/25/453449 which we posted yesterday. Any comments very welcome! I hope you don't mind being thanked in the acknowledgements!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:1354,deployability,updat,updating,1354,"Starting from the end: I think if you could upload the clustering results from the Scanpy paper / PAGA preprint to the scanpy github repo, it would be great. I still have the dropbox link of course, but I guess in the long run it's better if that file was located here and linked from the https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells page. The issue with 1 cell missing was because I did not specify `header=None` when loading it with Pandas :) So my error, not yours. The file is correct as is. That said, I am worried about the influence the random seed in randomized PCA seems to give in this case. Let me show you how it looks:. ![mln-tsne-clustering-comparison](https://user-images.githubusercontent.com/8970231/47555195-71af9480-d90b-11e8-85fb-a3e8dcb7a66f.png). I would be fine with some cells getting into other clusters depending on the random seed, and it would even be okay if small clusters changed their identities, but what we see here is a very drastic change of the cluster structure. Are you sure that the only difference is the randomized PCA outcome? Can it be that some of the default parameters in `sc.pp.recipe_zheng17`, `sc.pp.neighbors`, or `sc.tl.louvain` changed since when you ran the clustering? The scanpy code I posted above is the full code I used, and I ran it yesterday after updating scanpy via pip. BTW, the visualization above is taken from https://www.biorxiv.org/content/early/2018/10/25/453449 which we posted yesterday. Any comments very welcome! I hope you don't mind being thanked in the acknowledgements!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:464,energy efficiency,load,loading,464,"Starting from the end: I think if you could upload the clustering results from the Scanpy paper / PAGA preprint to the scanpy github repo, it would be great. I still have the dropbox link of course, but I guess in the long run it's better if that file was located here and linked from the https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells page. The issue with 1 cell missing was because I did not specify `header=None` when loading it with Pandas :) So my error, not yours. The file is correct as is. That said, I am worried about the influence the random seed in randomized PCA seems to give in this case. Let me show you how it looks:. ![mln-tsne-clustering-comparison](https://user-images.githubusercontent.com/8970231/47555195-71af9480-d90b-11e8-85fb-a3e8dcb7a66f.png). I would be fine with some cells getting into other clusters depending on the random seed, and it would even be okay if small clusters changed their identities, but what we see here is a very drastic change of the cluster structure. Are you sure that the only difference is the randomized PCA outcome? Can it be that some of the default parameters in `sc.pp.recipe_zheng17`, `sc.pp.neighbors`, or `sc.tl.louvain` changed since when you ran the clustering? The scanpy code I posted above is the full code I used, and I ran it yesterday after updating scanpy via pip. BTW, the visualization above is taken from https://www.biorxiv.org/content/early/2018/10/25/453449 which we posted yesterday. Any comments very welcome! I hope you don't mind being thanked in the acknowledgements!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:874,integrability,depend,depending,874,"Starting from the end: I think if you could upload the clustering results from the Scanpy paper / PAGA preprint to the scanpy github repo, it would be great. I still have the dropbox link of course, but I guess in the long run it's better if that file was located here and linked from the https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells page. The issue with 1 cell missing was because I did not specify `header=None` when loading it with Pandas :) So my error, not yours. The file is correct as is. That said, I am worried about the influence the random seed in randomized PCA seems to give in this case. Let me show you how it looks:. ![mln-tsne-clustering-comparison](https://user-images.githubusercontent.com/8970231/47555195-71af9480-d90b-11e8-85fb-a3e8dcb7a66f.png). I would be fine with some cells getting into other clusters depending on the random seed, and it would even be okay if small clusters changed their identities, but what we see here is a very drastic change of the cluster structure. Are you sure that the only difference is the randomized PCA outcome? Can it be that some of the default parameters in `sc.pp.recipe_zheng17`, `sc.pp.neighbors`, or `sc.tl.louvain` changed since when you ran the clustering? The scanpy code I posted above is the full code I used, and I ran it yesterday after updating scanpy via pip. BTW, the visualization above is taken from https://www.biorxiv.org/content/early/2018/10/25/453449 which we posted yesterday. Any comments very welcome! I hope you don't mind being thanked in the acknowledgements!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:437,interoperability,specif,specify,437,"Starting from the end: I think if you could upload the clustering results from the Scanpy paper / PAGA preprint to the scanpy github repo, it would be great. I still have the dropbox link of course, but I guess in the long run it's better if that file was located here and linked from the https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells page. The issue with 1 cell missing was because I did not specify `header=None` when loading it with Pandas :) So my error, not yours. The file is correct as is. That said, I am worried about the influence the random seed in randomized PCA seems to give in this case. Let me show you how it looks:. ![mln-tsne-clustering-comparison](https://user-images.githubusercontent.com/8970231/47555195-71af9480-d90b-11e8-85fb-a3e8dcb7a66f.png). I would be fine with some cells getting into other clusters depending on the random seed, and it would even be okay if small clusters changed their identities, but what we see here is a very drastic change of the cluster structure. Are you sure that the only difference is the randomized PCA outcome? Can it be that some of the default parameters in `sc.pp.recipe_zheng17`, `sc.pp.neighbors`, or `sc.tl.louvain` changed since when you ran the clustering? The scanpy code I posted above is the full code I used, and I ran it yesterday after updating scanpy via pip. BTW, the visualization above is taken from https://www.biorxiv.org/content/early/2018/10/25/453449 which we posted yesterday. Any comments very welcome! I hope you don't mind being thanked in the acknowledgements!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:874,modifiability,depend,depending,874,"Starting from the end: I think if you could upload the clustering results from the Scanpy paper / PAGA preprint to the scanpy github repo, it would be great. I still have the dropbox link of course, but I guess in the long run it's better if that file was located here and linked from the https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells page. The issue with 1 cell missing was because I did not specify `header=None` when loading it with Pandas :) So my error, not yours. The file is correct as is. That said, I am worried about the influence the random seed in randomized PCA seems to give in this case. Let me show you how it looks:. ![mln-tsne-clustering-comparison](https://user-images.githubusercontent.com/8970231/47555195-71af9480-d90b-11e8-85fb-a3e8dcb7a66f.png). I would be fine with some cells getting into other clusters depending on the random seed, and it would even be okay if small clusters changed their identities, but what we see here is a very drastic change of the cluster structure. Are you sure that the only difference is the randomized PCA outcome? Can it be that some of the default parameters in `sc.pp.recipe_zheng17`, `sc.pp.neighbors`, or `sc.tl.louvain` changed since when you ran the clustering? The scanpy code I posted above is the full code I used, and I ran it yesterday after updating scanpy via pip. BTW, the visualization above is taken from https://www.biorxiv.org/content/early/2018/10/25/453449 which we posted yesterday. Any comments very welcome! I hope you don't mind being thanked in the acknowledgements!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:1150,modifiability,paramet,parameters,1150,"Starting from the end: I think if you could upload the clustering results from the Scanpy paper / PAGA preprint to the scanpy github repo, it would be great. I still have the dropbox link of course, but I guess in the long run it's better if that file was located here and linked from the https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells page. The issue with 1 cell missing was because I did not specify `header=None` when loading it with Pandas :) So my error, not yours. The file is correct as is. That said, I am worried about the influence the random seed in randomized PCA seems to give in this case. Let me show you how it looks:. ![mln-tsne-clustering-comparison](https://user-images.githubusercontent.com/8970231/47555195-71af9480-d90b-11e8-85fb-a3e8dcb7a66f.png). I would be fine with some cells getting into other clusters depending on the random seed, and it would even be okay if small clusters changed their identities, but what we see here is a very drastic change of the cluster structure. Are you sure that the only difference is the randomized PCA outcome? Can it be that some of the default parameters in `sc.pp.recipe_zheng17`, `sc.pp.neighbors`, or `sc.tl.louvain` changed since when you ran the clustering? The scanpy code I posted above is the full code I used, and I ran it yesterday after updating scanpy via pip. BTW, the visualization above is taken from https://www.biorxiv.org/content/early/2018/10/25/453449 which we posted yesterday. Any comments very welcome! I hope you don't mind being thanked in the acknowledgements!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:464,performance,load,loading,464,"Starting from the end: I think if you could upload the clustering results from the Scanpy paper / PAGA preprint to the scanpy github repo, it would be great. I still have the dropbox link of course, but I guess in the long run it's better if that file was located here and linked from the https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells page. The issue with 1 cell missing was because I did not specify `header=None` when loading it with Pandas :) So my error, not yours. The file is correct as is. That said, I am worried about the influence the random seed in randomized PCA seems to give in this case. Let me show you how it looks:. ![mln-tsne-clustering-comparison](https://user-images.githubusercontent.com/8970231/47555195-71af9480-d90b-11e8-85fb-a3e8dcb7a66f.png). I would be fine with some cells getting into other clusters depending on the random seed, and it would even be okay if small clusters changed their identities, but what we see here is a very drastic change of the cluster structure. Are you sure that the only difference is the randomized PCA outcome? Can it be that some of the default parameters in `sc.pp.recipe_zheng17`, `sc.pp.neighbors`, or `sc.tl.louvain` changed since when you ran the clustering? The scanpy code I posted above is the full code I used, and I ran it yesterday after updating scanpy via pip. BTW, the visualization above is taken from https://www.biorxiv.org/content/early/2018/10/25/453449 which we posted yesterday. Any comments very welcome! I hope you don't mind being thanked in the acknowledgements!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:496,performance,error,error,496,"Starting from the end: I think if you could upload the clustering results from the Scanpy paper / PAGA preprint to the scanpy github repo, it would be great. I still have the dropbox link of course, but I guess in the long run it's better if that file was located here and linked from the https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells page. The issue with 1 cell missing was because I did not specify `header=None` when loading it with Pandas :) So my error, not yours. The file is correct as is. That said, I am worried about the influence the random seed in randomized PCA seems to give in this case. Let me show you how it looks:. ![mln-tsne-clustering-comparison](https://user-images.githubusercontent.com/8970231/47555195-71af9480-d90b-11e8-85fb-a3e8dcb7a66f.png). I would be fine with some cells getting into other clusters depending on the random seed, and it would even be okay if small clusters changed their identities, but what we see here is a very drastic change of the cluster structure. Are you sure that the only difference is the randomized PCA outcome? Can it be that some of the default parameters in `sc.pp.recipe_zheng17`, `sc.pp.neighbors`, or `sc.tl.louvain` changed since when you ran the clustering? The scanpy code I posted above is the full code I used, and I ran it yesterday after updating scanpy via pip. BTW, the visualization above is taken from https://www.biorxiv.org/content/early/2018/10/25/453449 which we posted yesterday. Any comments very welcome! I hope you don't mind being thanked in the acknowledgements!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:1446,performance,content,content,1446,"Starting from the end: I think if you could upload the clustering results from the Scanpy paper / PAGA preprint to the scanpy github repo, it would be great. I still have the dropbox link of course, but I guess in the long run it's better if that file was located here and linked from the https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells page. The issue with 1 cell missing was because I did not specify `header=None` when loading it with Pandas :) So my error, not yours. The file is correct as is. That said, I am worried about the influence the random seed in randomized PCA seems to give in this case. Let me show you how it looks:. ![mln-tsne-clustering-comparison](https://user-images.githubusercontent.com/8970231/47555195-71af9480-d90b-11e8-85fb-a3e8dcb7a66f.png). I would be fine with some cells getting into other clusters depending on the random seed, and it would even be okay if small clusters changed their identities, but what we see here is a very drastic change of the cluster structure. Are you sure that the only difference is the randomized PCA outcome? Can it be that some of the default parameters in `sc.pp.recipe_zheng17`, `sc.pp.neighbors`, or `sc.tl.louvain` changed since when you ran the clustering? The scanpy code I posted above is the full code I used, and I ran it yesterday after updating scanpy via pip. BTW, the visualization above is taken from https://www.biorxiv.org/content/early/2018/10/25/453449 which we posted yesterday. Any comments very welcome! I hope you don't mind being thanked in the acknowledgements!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:496,safety,error,error,496,"Starting from the end: I think if you could upload the clustering results from the Scanpy paper / PAGA preprint to the scanpy github repo, it would be great. I still have the dropbox link of course, but I guess in the long run it's better if that file was located here and linked from the https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells page. The issue with 1 cell missing was because I did not specify `header=None` when loading it with Pandas :) So my error, not yours. The file is correct as is. That said, I am worried about the influence the random seed in randomized PCA seems to give in this case. Let me show you how it looks:. ![mln-tsne-clustering-comparison](https://user-images.githubusercontent.com/8970231/47555195-71af9480-d90b-11e8-85fb-a3e8dcb7a66f.png). I would be fine with some cells getting into other clusters depending on the random seed, and it would even be okay if small clusters changed their identities, but what we see here is a very drastic change of the cluster structure. Are you sure that the only difference is the randomized PCA outcome? Can it be that some of the default parameters in `sc.pp.recipe_zheng17`, `sc.pp.neighbors`, or `sc.tl.louvain` changed since when you ran the clustering? The scanpy code I posted above is the full code I used, and I ran it yesterday after updating scanpy via pip. BTW, the visualization above is taken from https://www.biorxiv.org/content/early/2018/10/25/453449 which we posted yesterday. Any comments very welcome! I hope you don't mind being thanked in the acknowledgements!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:874,safety,depend,depending,874,"Starting from the end: I think if you could upload the clustering results from the Scanpy paper / PAGA preprint to the scanpy github repo, it would be great. I still have the dropbox link of course, but I guess in the long run it's better if that file was located here and linked from the https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells page. The issue with 1 cell missing was because I did not specify `header=None` when loading it with Pandas :) So my error, not yours. The file is correct as is. That said, I am worried about the influence the random seed in randomized PCA seems to give in this case. Let me show you how it looks:. ![mln-tsne-clustering-comparison](https://user-images.githubusercontent.com/8970231/47555195-71af9480-d90b-11e8-85fb-a3e8dcb7a66f.png). I would be fine with some cells getting into other clusters depending on the random seed, and it would even be okay if small clusters changed their identities, but what we see here is a very drastic change of the cluster structure. Are you sure that the only difference is the randomized PCA outcome? Can it be that some of the default parameters in `sc.pp.recipe_zheng17`, `sc.pp.neighbors`, or `sc.tl.louvain` changed since when you ran the clustering? The scanpy code I posted above is the full code I used, and I ran it yesterday after updating scanpy via pip. BTW, the visualization above is taken from https://www.biorxiv.org/content/early/2018/10/25/453449 which we posted yesterday. Any comments very welcome! I hope you don't mind being thanked in the acknowledgements!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:1354,safety,updat,updating,1354,"Starting from the end: I think if you could upload the clustering results from the Scanpy paper / PAGA preprint to the scanpy github repo, it would be great. I still have the dropbox link of course, but I guess in the long run it's better if that file was located here and linked from the https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells page. The issue with 1 cell missing was because I did not specify `header=None` when loading it with Pandas :) So my error, not yours. The file is correct as is. That said, I am worried about the influence the random seed in randomized PCA seems to give in this case. Let me show you how it looks:. ![mln-tsne-clustering-comparison](https://user-images.githubusercontent.com/8970231/47555195-71af9480-d90b-11e8-85fb-a3e8dcb7a66f.png). I would be fine with some cells getting into other clusters depending on the random seed, and it would even be okay if small clusters changed their identities, but what we see here is a very drastic change of the cluster structure. Are you sure that the only difference is the randomized PCA outcome? Can it be that some of the default parameters in `sc.pp.recipe_zheng17`, `sc.pp.neighbors`, or `sc.tl.louvain` changed since when you ran the clustering? The scanpy code I posted above is the full code I used, and I ran it yesterday after updating scanpy via pip. BTW, the visualization above is taken from https://www.biorxiv.org/content/early/2018/10/25/453449 which we posted yesterday. Any comments very welcome! I hope you don't mind being thanked in the acknowledgements!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:962,security,ident,identities,962,"Starting from the end: I think if you could upload the clustering results from the Scanpy paper / PAGA preprint to the scanpy github repo, it would be great. I still have the dropbox link of course, but I guess in the long run it's better if that file was located here and linked from the https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells page. The issue with 1 cell missing was because I did not specify `header=None` when loading it with Pandas :) So my error, not yours. The file is correct as is. That said, I am worried about the influence the random seed in randomized PCA seems to give in this case. Let me show you how it looks:. ![mln-tsne-clustering-comparison](https://user-images.githubusercontent.com/8970231/47555195-71af9480-d90b-11e8-85fb-a3e8dcb7a66f.png). I would be fine with some cells getting into other clusters depending on the random seed, and it would even be okay if small clusters changed their identities, but what we see here is a very drastic change of the cluster structure. Are you sure that the only difference is the randomized PCA outcome? Can it be that some of the default parameters in `sc.pp.recipe_zheng17`, `sc.pp.neighbors`, or `sc.tl.louvain` changed since when you ran the clustering? The scanpy code I posted above is the full code I used, and I ran it yesterday after updating scanpy via pip. BTW, the visualization above is taken from https://www.biorxiv.org/content/early/2018/10/25/453449 which we posted yesterday. Any comments very welcome! I hope you don't mind being thanked in the acknowledgements!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:1354,security,updat,updating,1354,"Starting from the end: I think if you could upload the clustering results from the Scanpy paper / PAGA preprint to the scanpy github repo, it would be great. I still have the dropbox link of course, but I guess in the long run it's better if that file was located here and linked from the https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells page. The issue with 1 cell missing was because I did not specify `header=None` when loading it with Pandas :) So my error, not yours. The file is correct as is. That said, I am worried about the influence the random seed in randomized PCA seems to give in this case. Let me show you how it looks:. ![mln-tsne-clustering-comparison](https://user-images.githubusercontent.com/8970231/47555195-71af9480-d90b-11e8-85fb-a3e8dcb7a66f.png). I would be fine with some cells getting into other clusters depending on the random seed, and it would even be okay if small clusters changed their identities, but what we see here is a very drastic change of the cluster structure. Are you sure that the only difference is the randomized PCA outcome? Can it be that some of the default parameters in `sc.pp.recipe_zheng17`, `sc.pp.neighbors`, or `sc.tl.louvain` changed since when you ran the clustering? The scanpy code I posted above is the full code I used, and I ran it yesterday after updating scanpy via pip. BTW, the visualization above is taken from https://www.biorxiv.org/content/early/2018/10/25/453449 which we posted yesterday. Any comments very welcome! I hope you don't mind being thanked in the acknowledgements!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:874,testability,depend,depending,874,"Starting from the end: I think if you could upload the clustering results from the Scanpy paper / PAGA preprint to the scanpy github repo, it would be great. I still have the dropbox link of course, but I guess in the long run it's better if that file was located here and linked from the https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells page. The issue with 1 cell missing was because I did not specify `header=None` when loading it with Pandas :) So my error, not yours. The file is correct as is. That said, I am worried about the influence the random seed in randomized PCA seems to give in this case. Let me show you how it looks:. ![mln-tsne-clustering-comparison](https://user-images.githubusercontent.com/8970231/47555195-71af9480-d90b-11e8-85fb-a3e8dcb7a66f.png). I would be fine with some cells getting into other clusters depending on the random seed, and it would even be okay if small clusters changed their identities, but what we see here is a very drastic change of the cluster structure. Are you sure that the only difference is the randomized PCA outcome? Can it be that some of the default parameters in `sc.pp.recipe_zheng17`, `sc.pp.neighbors`, or `sc.tl.louvain` changed since when you ran the clustering? The scanpy code I posted above is the full code I used, and I ran it yesterday after updating scanpy via pip. BTW, the visualization above is taken from https://www.biorxiv.org/content/early/2018/10/25/453449 which we posted yesterday. Any comments very welcome! I hope you don't mind being thanked in the acknowledgements!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:496,usability,error,error,496,"Starting from the end: I think if you could upload the clustering results from the Scanpy paper / PAGA preprint to the scanpy github repo, it would be great. I still have the dropbox link of course, but I guess in the long run it's better if that file was located here and linked from the https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells page. The issue with 1 cell missing was because I did not specify `header=None` when loading it with Pandas :) So my error, not yours. The file is correct as is. That said, I am worried about the influence the random seed in randomized PCA seems to give in this case. Let me show you how it looks:. ![mln-tsne-clustering-comparison](https://user-images.githubusercontent.com/8970231/47555195-71af9480-d90b-11e8-85fb-a3e8dcb7a66f.png). I would be fine with some cells getting into other clusters depending on the random seed, and it would even be okay if small clusters changed their identities, but what we see here is a very drastic change of the cluster structure. Are you sure that the only difference is the randomized PCA outcome? Can it be that some of the default parameters in `sc.pp.recipe_zheng17`, `sc.pp.neighbors`, or `sc.tl.louvain` changed since when you ran the clustering? The scanpy code I posted above is the full code I used, and I ran it yesterday after updating scanpy via pip. BTW, the visualization above is taken from https://www.biorxiv.org/content/early/2018/10/25/453449 which we posted yesterday. Any comments very welcome! I hope you don't mind being thanked in the acknowledgements!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:720,usability,user,user-images,720,"Starting from the end: I think if you could upload the clustering results from the Scanpy paper / PAGA preprint to the scanpy github repo, it would be great. I still have the dropbox link of course, but I guess in the long run it's better if that file was located here and linked from the https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells page. The issue with 1 cell missing was because I did not specify `header=None` when loading it with Pandas :) So my error, not yours. The file is correct as is. That said, I am worried about the influence the random seed in randomized PCA seems to give in this case. Let me show you how it looks:. ![mln-tsne-clustering-comparison](https://user-images.githubusercontent.com/8970231/47555195-71af9480-d90b-11e8-85fb-a3e8dcb7a66f.png). I would be fine with some cells getting into other clusters depending on the random seed, and it would even be okay if small clusters changed their identities, but what we see here is a very drastic change of the cluster structure. Are you sure that the only difference is the randomized PCA outcome? Can it be that some of the default parameters in `sc.pp.recipe_zheng17`, `sc.pp.neighbors`, or `sc.tl.louvain` changed since when you ran the clustering? The scanpy code I posted above is the full code I used, and I ran it yesterday after updating scanpy via pip. BTW, the visualization above is taken from https://www.biorxiv.org/content/early/2018/10/25/453449 which we posted yesterday. Any comments very welcome! I hope you don't mind being thanked in the acknowledgements!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:1388,usability,visual,visualization,1388,"Starting from the end: I think if you could upload the clustering results from the Scanpy paper / PAGA preprint to the scanpy github repo, it would be great. I still have the dropbox link of course, but I guess in the long run it's better if that file was located here and linked from the https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells page. The issue with 1 cell missing was because I did not specify `header=None` when loading it with Pandas :) So my error, not yours. The file is correct as is. That said, I am worried about the influence the random seed in randomized PCA seems to give in this case. Let me show you how it looks:. ![mln-tsne-clustering-comparison](https://user-images.githubusercontent.com/8970231/47555195-71af9480-d90b-11e8-85fb-a3e8dcb7a66f.png). I would be fine with some cells getting into other clusters depending on the random seed, and it would even be okay if small clusters changed their identities, but what we see here is a very drastic change of the cluster structure. Are you sure that the only difference is the randomized PCA outcome? Can it be that some of the default parameters in `sc.pp.recipe_zheng17`, `sc.pp.neighbors`, or `sc.tl.louvain` changed since when you ran the clustering? The scanpy code I posted above is the full code I used, and I ran it yesterday after updating scanpy via pip. BTW, the visualization above is taken from https://www.biorxiv.org/content/early/2018/10/25/453449 which we posted yesterday. Any comments very welcome! I hope you don't mind being thanked in the acknowledgements!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:14,availability,slo,slow,14,Sorry for the slow response! Congrats on the very nice preprint and thanks for thanking me! :wink:. I'll upload the file from dropbox to `scanpy_usage`! Regarding the result: the high PCs can change drastically depending on the platform and the random seed. I've seen clustering results changing completely after I became aware of it. I'm very sure that non of the preprocessing and other code changed at any point. What do you think?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:268,availability,cluster,clustering,268,Sorry for the slow response! Congrats on the very nice preprint and thanks for thanking me! :wink:. I'll upload the file from dropbox to `scanpy_usage`! Regarding the result: the high PCs can change drastically depending on the platform and the random seed. I've seen clustering results changing completely after I became aware of it. I'm very sure that non of the preprocessing and other code changed at any point. What do you think?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:211,deployability,depend,depending,211,Sorry for the slow response! Congrats on the very nice preprint and thanks for thanking me! :wink:. I'll upload the file from dropbox to `scanpy_usage`! Regarding the result: the high PCs can change drastically depending on the platform and the random seed. I've seen clustering results changing completely after I became aware of it. I'm very sure that non of the preprocessing and other code changed at any point. What do you think?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:268,deployability,cluster,clustering,268,Sorry for the slow response! Congrats on the very nice preprint and thanks for thanking me! :wink:. I'll upload the file from dropbox to `scanpy_usage`! Regarding the result: the high PCs can change drastically depending on the platform and the random seed. I've seen clustering results changing completely after I became aware of it. I'm very sure that non of the preprocessing and other code changed at any point. What do you think?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:211,integrability,depend,depending,211,Sorry for the slow response! Congrats on the very nice preprint and thanks for thanking me! :wink:. I'll upload the file from dropbox to `scanpy_usage`! Regarding the result: the high PCs can change drastically depending on the platform and the random seed. I've seen clustering results changing completely after I became aware of it. I'm very sure that non of the preprocessing and other code changed at any point. What do you think?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:228,interoperability,platform,platform,228,Sorry for the slow response! Congrats on the very nice preprint and thanks for thanking me! :wink:. I'll upload the file from dropbox to `scanpy_usage`! Regarding the result: the high PCs can change drastically depending on the platform and the random seed. I've seen clustering results changing completely after I became aware of it. I'm very sure that non of the preprocessing and other code changed at any point. What do you think?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:211,modifiability,depend,depending,211,Sorry for the slow response! Congrats on the very nice preprint and thanks for thanking me! :wink:. I'll upload the file from dropbox to `scanpy_usage`! Regarding the result: the high PCs can change drastically depending on the platform and the random seed. I've seen clustering results changing completely after I became aware of it. I'm very sure that non of the preprocessing and other code changed at any point. What do you think?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:14,reliability,slo,slow,14,Sorry for the slow response! Congrats on the very nice preprint and thanks for thanking me! :wink:. I'll upload the file from dropbox to `scanpy_usage`! Regarding the result: the high PCs can change drastically depending on the platform and the random seed. I've seen clustering results changing completely after I became aware of it. I'm very sure that non of the preprocessing and other code changed at any point. What do you think?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:211,safety,depend,depending,211,Sorry for the slow response! Congrats on the very nice preprint and thanks for thanking me! :wink:. I'll upload the file from dropbox to `scanpy_usage`! Regarding the result: the high PCs can change drastically depending on the platform and the random seed. I've seen clustering results changing completely after I became aware of it. I'm very sure that non of the preprocessing and other code changed at any point. What do you think?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:296,safety,compl,completely,296,Sorry for the slow response! Congrats on the very nice preprint and thanks for thanking me! :wink:. I'll upload the file from dropbox to `scanpy_usage`! Regarding the result: the high PCs can change drastically depending on the platform and the random seed. I've seen clustering results changing completely after I became aware of it. I'm very sure that non of the preprocessing and other code changed at any point. What do you think?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:296,security,compl,completely,296,Sorry for the slow response! Congrats on the very nice preprint and thanks for thanking me! :wink:. I'll upload the file from dropbox to `scanpy_usage`! Regarding the result: the high PCs can change drastically depending on the platform and the random seed. I've seen clustering results changing completely after I became aware of it. I'm very sure that non of the preprocessing and other code changed at any point. What do you think?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:211,testability,depend,depending,211,Sorry for the slow response! Congrats on the very nice preprint and thanks for thanking me! :wink:. I'll upload the file from dropbox to `scanpy_usage`! Regarding the result: the high PCs can change drastically depending on the platform and the random seed. I've seen clustering results changing completely after I became aware of it. I'm very sure that non of the preprocessing and other code changed at any point. What do you think?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:14,availability,ping,ping,14,"Great. Please ping me here when you upload the file to `scanpy_usage` and feel free to close the issue then. I'll update my script to link directly to `scanpy_usage`. > Regarding the result: the high PCs can change drastically depending on the platform and the random seed. I've seen clustering results changing completely after I became aware of it. . I don't have much experience with randomized PCA, but this is very disturbing, no? Was your feeling that the PCs themselves changed strongly (as measured, I don't know, by the %% of total captured variance, or maybe angle between subspaces, etc.), or is it rather that clustering outcome is dangerously sensitive to small changes in the data? I think this is something worth investigating.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:284,availability,cluster,clustering,284,"Great. Please ping me here when you upload the file to `scanpy_usage` and feel free to close the issue then. I'll update my script to link directly to `scanpy_usage`. > Regarding the result: the high PCs can change drastically depending on the platform and the random seed. I've seen clustering results changing completely after I became aware of it. . I don't have much experience with randomized PCA, but this is very disturbing, no? Was your feeling that the PCs themselves changed strongly (as measured, I don't know, by the %% of total captured variance, or maybe angle between subspaces, etc.), or is it rather that clustering outcome is dangerously sensitive to small changes in the data? I think this is something worth investigating.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:622,availability,cluster,clustering,622,"Great. Please ping me here when you upload the file to `scanpy_usage` and feel free to close the issue then. I'll update my script to link directly to `scanpy_usage`. > Regarding the result: the high PCs can change drastically depending on the platform and the random seed. I've seen clustering results changing completely after I became aware of it. . I don't have much experience with randomized PCA, but this is very disturbing, no? Was your feeling that the PCs themselves changed strongly (as measured, I don't know, by the %% of total captured variance, or maybe angle between subspaces, etc.), or is it rather that clustering outcome is dangerously sensitive to small changes in the data? I think this is something worth investigating.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:114,deployability,updat,update,114,"Great. Please ping me here when you upload the file to `scanpy_usage` and feel free to close the issue then. I'll update my script to link directly to `scanpy_usage`. > Regarding the result: the high PCs can change drastically depending on the platform and the random seed. I've seen clustering results changing completely after I became aware of it. . I don't have much experience with randomized PCA, but this is very disturbing, no? Was your feeling that the PCs themselves changed strongly (as measured, I don't know, by the %% of total captured variance, or maybe angle between subspaces, etc.), or is it rather that clustering outcome is dangerously sensitive to small changes in the data? I think this is something worth investigating.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:227,deployability,depend,depending,227,"Great. Please ping me here when you upload the file to `scanpy_usage` and feel free to close the issue then. I'll update my script to link directly to `scanpy_usage`. > Regarding the result: the high PCs can change drastically depending on the platform and the random seed. I've seen clustering results changing completely after I became aware of it. . I don't have much experience with randomized PCA, but this is very disturbing, no? Was your feeling that the PCs themselves changed strongly (as measured, I don't know, by the %% of total captured variance, or maybe angle between subspaces, etc.), or is it rather that clustering outcome is dangerously sensitive to small changes in the data? I think this is something worth investigating.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:284,deployability,cluster,clustering,284,"Great. Please ping me here when you upload the file to `scanpy_usage` and feel free to close the issue then. I'll update my script to link directly to `scanpy_usage`. > Regarding the result: the high PCs can change drastically depending on the platform and the random seed. I've seen clustering results changing completely after I became aware of it. . I don't have much experience with randomized PCA, but this is very disturbing, no? Was your feeling that the PCs themselves changed strongly (as measured, I don't know, by the %% of total captured variance, or maybe angle between subspaces, etc.), or is it rather that clustering outcome is dangerously sensitive to small changes in the data? I think this is something worth investigating.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:622,deployability,cluster,clustering,622,"Great. Please ping me here when you upload the file to `scanpy_usage` and feel free to close the issue then. I'll update my script to link directly to `scanpy_usage`. > Regarding the result: the high PCs can change drastically depending on the platform and the random seed. I've seen clustering results changing completely after I became aware of it. . I don't have much experience with randomized PCA, but this is very disturbing, no? Was your feeling that the PCs themselves changed strongly (as measured, I don't know, by the %% of total captured variance, or maybe angle between subspaces, etc.), or is it rather that clustering outcome is dangerously sensitive to small changes in the data? I think this is something worth investigating.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:498,energy efficiency,measur,measured,498,"Great. Please ping me here when you upload the file to `scanpy_usage` and feel free to close the issue then. I'll update my script to link directly to `scanpy_usage`. > Regarding the result: the high PCs can change drastically depending on the platform and the random seed. I've seen clustering results changing completely after I became aware of it. . I don't have much experience with randomized PCA, but this is very disturbing, no? Was your feeling that the PCs themselves changed strongly (as measured, I don't know, by the %% of total captured variance, or maybe angle between subspaces, etc.), or is it rather that clustering outcome is dangerously sensitive to small changes in the data? I think this is something worth investigating.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:227,integrability,depend,depending,227,"Great. Please ping me here when you upload the file to `scanpy_usage` and feel free to close the issue then. I'll update my script to link directly to `scanpy_usage`. > Regarding the result: the high PCs can change drastically depending on the platform and the random seed. I've seen clustering results changing completely after I became aware of it. . I don't have much experience with randomized PCA, but this is very disturbing, no? Was your feeling that the PCs themselves changed strongly (as measured, I don't know, by the %% of total captured variance, or maybe angle between subspaces, etc.), or is it rather that clustering outcome is dangerously sensitive to small changes in the data? I think this is something worth investigating.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:583,integrability,sub,subspaces,583,"Great. Please ping me here when you upload the file to `scanpy_usage` and feel free to close the issue then. I'll update my script to link directly to `scanpy_usage`. > Regarding the result: the high PCs can change drastically depending on the platform and the random seed. I've seen clustering results changing completely after I became aware of it. . I don't have much experience with randomized PCA, but this is very disturbing, no? Was your feeling that the PCs themselves changed strongly (as measured, I don't know, by the %% of total captured variance, or maybe angle between subspaces, etc.), or is it rather that clustering outcome is dangerously sensitive to small changes in the data? I think this is something worth investigating.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:244,interoperability,platform,platform,244,"Great. Please ping me here when you upload the file to `scanpy_usage` and feel free to close the issue then. I'll update my script to link directly to `scanpy_usage`. > Regarding the result: the high PCs can change drastically depending on the platform and the random seed. I've seen clustering results changing completely after I became aware of it. . I don't have much experience with randomized PCA, but this is very disturbing, no? Was your feeling that the PCs themselves changed strongly (as measured, I don't know, by the %% of total captured variance, or maybe angle between subspaces, etc.), or is it rather that clustering outcome is dangerously sensitive to small changes in the data? I think this is something worth investigating.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:227,modifiability,depend,depending,227,"Great. Please ping me here when you upload the file to `scanpy_usage` and feel free to close the issue then. I'll update my script to link directly to `scanpy_usage`. > Regarding the result: the high PCs can change drastically depending on the platform and the random seed. I've seen clustering results changing completely after I became aware of it. . I don't have much experience with randomized PCA, but this is very disturbing, no? Was your feeling that the PCs themselves changed strongly (as measured, I don't know, by the %% of total captured variance, or maybe angle between subspaces, etc.), or is it rather that clustering outcome is dangerously sensitive to small changes in the data? I think this is something worth investigating.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:114,safety,updat,update,114,"Great. Please ping me here when you upload the file to `scanpy_usage` and feel free to close the issue then. I'll update my script to link directly to `scanpy_usage`. > Regarding the result: the high PCs can change drastically depending on the platform and the random seed. I've seen clustering results changing completely after I became aware of it. . I don't have much experience with randomized PCA, but this is very disturbing, no? Was your feeling that the PCs themselves changed strongly (as measured, I don't know, by the %% of total captured variance, or maybe angle between subspaces, etc.), or is it rather that clustering outcome is dangerously sensitive to small changes in the data? I think this is something worth investigating.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:227,safety,depend,depending,227,"Great. Please ping me here when you upload the file to `scanpy_usage` and feel free to close the issue then. I'll update my script to link directly to `scanpy_usage`. > Regarding the result: the high PCs can change drastically depending on the platform and the random seed. I've seen clustering results changing completely after I became aware of it. . I don't have much experience with randomized PCA, but this is very disturbing, no? Was your feeling that the PCs themselves changed strongly (as measured, I don't know, by the %% of total captured variance, or maybe angle between subspaces, etc.), or is it rather that clustering outcome is dangerously sensitive to small changes in the data? I think this is something worth investigating.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:312,safety,compl,completely,312,"Great. Please ping me here when you upload the file to `scanpy_usage` and feel free to close the issue then. I'll update my script to link directly to `scanpy_usage`. > Regarding the result: the high PCs can change drastically depending on the platform and the random seed. I've seen clustering results changing completely after I became aware of it. . I don't have much experience with randomized PCA, but this is very disturbing, no? Was your feeling that the PCs themselves changed strongly (as measured, I don't know, by the %% of total captured variance, or maybe angle between subspaces, etc.), or is it rather that clustering outcome is dangerously sensitive to small changes in the data? I think this is something worth investigating.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:114,security,updat,update,114,"Great. Please ping me here when you upload the file to `scanpy_usage` and feel free to close the issue then. I'll update my script to link directly to `scanpy_usage`. > Regarding the result: the high PCs can change drastically depending on the platform and the random seed. I've seen clustering results changing completely after I became aware of it. . I don't have much experience with randomized PCA, but this is very disturbing, no? Was your feeling that the PCs themselves changed strongly (as measured, I don't know, by the %% of total captured variance, or maybe angle between subspaces, etc.), or is it rather that clustering outcome is dangerously sensitive to small changes in the data? I think this is something worth investigating.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:312,security,compl,completely,312,"Great. Please ping me here when you upload the file to `scanpy_usage` and feel free to close the issue then. I'll update my script to link directly to `scanpy_usage`. > Regarding the result: the high PCs can change drastically depending on the platform and the random seed. I've seen clustering results changing completely after I became aware of it. . I don't have much experience with randomized PCA, but this is very disturbing, no? Was your feeling that the PCs themselves changed strongly (as measured, I don't know, by the %% of total captured variance, or maybe angle between subspaces, etc.), or is it rather that clustering outcome is dangerously sensitive to small changes in the data? I think this is something worth investigating.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:227,testability,depend,depending,227,"Great. Please ping me here when you upload the file to `scanpy_usage` and feel free to close the issue then. I'll update my script to link directly to `scanpy_usage`. > Regarding the result: the high PCs can change drastically depending on the platform and the random seed. I've seen clustering results changing completely after I became aware of it. . I don't have much experience with randomized PCA, but this is very disturbing, no? Was your feeling that the PCs themselves changed strongly (as measured, I don't know, by the %% of total captured variance, or maybe angle between subspaces, etc.), or is it rather that clustering outcome is dangerously sensitive to small changes in the data? I think this is something worth investigating.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:87,usability,close,close,87,"Great. Please ping me here when you upload the file to `scanpy_usage` and feel free to close the issue then. I'll update my script to link directly to `scanpy_usage`. > Regarding the result: the high PCs can change drastically depending on the platform and the random seed. I've seen clustering results changing completely after I became aware of it. . I don't have much experience with randomized PCA, but this is very disturbing, no? Was your feeling that the PCs themselves changed strongly (as measured, I don't know, by the %% of total captured variance, or maybe angle between subspaces, etc.), or is it rather that clustering outcome is dangerously sensitive to small changes in the data? I think this is something worth investigating.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:371,usability,experien,experience,371,"Great. Please ping me here when you upload the file to `scanpy_usage` and feel free to close the issue then. I'll update my script to link directly to `scanpy_usage`. > Regarding the result: the high PCs can change drastically depending on the platform and the random seed. I've seen clustering results changing completely after I became aware of it. . I don't have much experience with randomized PCA, but this is very disturbing, no? Was your feeling that the PCs themselves changed strongly (as measured, I don't know, by the %% of total captured variance, or maybe angle between subspaces, etc.), or is it rather that clustering outcome is dangerously sensitive to small changes in the data? I think this is something worth investigating.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:78,availability,cluster,clustering,78,"I have observed the latter. Converting between float64 and float32 changed my clustering quite a bit where cluster boundaries fell into more densely sampled regions of transcriptome space. PCs were similar to within a couple of decimal points, but the KNN graph connectivities changed more drastically.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:107,availability,cluster,cluster,107,"I have observed the latter. Converting between float64 and float32 changed my clustering quite a bit where cluster boundaries fell into more densely sampled regions of transcriptome space. PCs were similar to within a couple of decimal points, but the KNN graph connectivities changed more drastically.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:7,deployability,observ,observed,7,"I have observed the latter. Converting between float64 and float32 changed my clustering quite a bit where cluster boundaries fell into more densely sampled regions of transcriptome space. PCs were similar to within a couple of decimal points, but the KNN graph connectivities changed more drastically.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:78,deployability,cluster,clustering,78,"I have observed the latter. Converting between float64 and float32 changed my clustering quite a bit where cluster boundaries fell into more densely sampled regions of transcriptome space. PCs were similar to within a couple of decimal points, but the KNN graph connectivities changed more drastically.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:107,deployability,cluster,cluster,107,"I have observed the latter. Converting between float64 and float32 changed my clustering quite a bit where cluster boundaries fell into more densely sampled regions of transcriptome space. PCs were similar to within a couple of decimal points, but the KNN graph connectivities changed more drastically.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:218,integrability,coupl,couple,218,"I have observed the latter. Converting between float64 and float32 changed my clustering quite a bit where cluster boundaries fell into more densely sampled regions of transcriptome space. PCs were similar to within a couple of decimal points, but the KNN graph connectivities changed more drastically.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:218,modifiability,coupl,couple,218,"I have observed the latter. Converting between float64 and float32 changed my clustering quite a bit where cluster boundaries fell into more densely sampled regions of transcriptome space. PCs were similar to within a couple of decimal points, but the KNN graph connectivities changed more drastically.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:7,testability,observ,observed,7,"I have observed the latter. Converting between float64 and float32 changed my clustering quite a bit where cluster boundaries fell into more densely sampled regions of transcriptome space. PCs were similar to within a couple of decimal points, but the KNN graph connectivities changed more drastically.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:218,testability,coupl,couple,218,"I have observed the latter. Converting between float64 and float32 changed my clustering quite a bit where cluster boundaries fell into more densely sampled regions of transcriptome space. PCs were similar to within a couple of decimal points, but the KNN graph connectivities changed more drastically.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:108,availability,down,downstream,108,"@LuckyMD Thanks. This agrees with what I suspected: that randomized PCA itself should be pretty stable, but downstream clustering procedures can be very unstable. I have little experience with clustering so I cannot really comment further, but this certainly should be a big red flag for taking a clustering result seriously. It's especially impressive that float32 vs float64 can cause such a difference. Did you observe this influencing t-SNE/UMAP/etc equally drastically, or did it only affect clustering so strongly?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:119,availability,cluster,clustering,119,"@LuckyMD Thanks. This agrees with what I suspected: that randomized PCA itself should be pretty stable, but downstream clustering procedures can be very unstable. I have little experience with clustering so I cannot really comment further, but this certainly should be a big red flag for taking a clustering result seriously. It's especially impressive that float32 vs float64 can cause such a difference. Did you observe this influencing t-SNE/UMAP/etc equally drastically, or did it only affect clustering so strongly?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:193,availability,cluster,clustering,193,"@LuckyMD Thanks. This agrees with what I suspected: that randomized PCA itself should be pretty stable, but downstream clustering procedures can be very unstable. I have little experience with clustering so I cannot really comment further, but this certainly should be a big red flag for taking a clustering result seriously. It's especially impressive that float32 vs float64 can cause such a difference. Did you observe this influencing t-SNE/UMAP/etc equally drastically, or did it only affect clustering so strongly?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:297,availability,cluster,clustering,297,"@LuckyMD Thanks. This agrees with what I suspected: that randomized PCA itself should be pretty stable, but downstream clustering procedures can be very unstable. I have little experience with clustering so I cannot really comment further, but this certainly should be a big red flag for taking a clustering result seriously. It's especially impressive that float32 vs float64 can cause such a difference. Did you observe this influencing t-SNE/UMAP/etc equally drastically, or did it only affect clustering so strongly?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:497,availability,cluster,clustering,497,"@LuckyMD Thanks. This agrees with what I suspected: that randomized PCA itself should be pretty stable, but downstream clustering procedures can be very unstable. I have little experience with clustering so I cannot really comment further, but this certainly should be a big red flag for taking a clustering result seriously. It's especially impressive that float32 vs float64 can cause such a difference. Did you observe this influencing t-SNE/UMAP/etc equally drastically, or did it only affect clustering so strongly?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:119,deployability,cluster,clustering,119,"@LuckyMD Thanks. This agrees with what I suspected: that randomized PCA itself should be pretty stable, but downstream clustering procedures can be very unstable. I have little experience with clustering so I cannot really comment further, but this certainly should be a big red flag for taking a clustering result seriously. It's especially impressive that float32 vs float64 can cause such a difference. Did you observe this influencing t-SNE/UMAP/etc equally drastically, or did it only affect clustering so strongly?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:193,deployability,cluster,clustering,193,"@LuckyMD Thanks. This agrees with what I suspected: that randomized PCA itself should be pretty stable, but downstream clustering procedures can be very unstable. I have little experience with clustering so I cannot really comment further, but this certainly should be a big red flag for taking a clustering result seriously. It's especially impressive that float32 vs float64 can cause such a difference. Did you observe this influencing t-SNE/UMAP/etc equally drastically, or did it only affect clustering so strongly?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:297,deployability,cluster,clustering,297,"@LuckyMD Thanks. This agrees with what I suspected: that randomized PCA itself should be pretty stable, but downstream clustering procedures can be very unstable. I have little experience with clustering so I cannot really comment further, but this certainly should be a big red flag for taking a clustering result seriously. It's especially impressive that float32 vs float64 can cause such a difference. Did you observe this influencing t-SNE/UMAP/etc equally drastically, or did it only affect clustering so strongly?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:414,deployability,observ,observe,414,"@LuckyMD Thanks. This agrees with what I suspected: that randomized PCA itself should be pretty stable, but downstream clustering procedures can be very unstable. I have little experience with clustering so I cannot really comment further, but this certainly should be a big red flag for taking a clustering result seriously. It's especially impressive that float32 vs float64 can cause such a difference. Did you observe this influencing t-SNE/UMAP/etc equally drastically, or did it only affect clustering so strongly?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:497,deployability,cluster,clustering,497,"@LuckyMD Thanks. This agrees with what I suspected: that randomized PCA itself should be pretty stable, but downstream clustering procedures can be very unstable. I have little experience with clustering so I cannot really comment further, but this certainly should be a big red flag for taking a clustering result seriously. It's especially impressive that float32 vs float64 can cause such a difference. Did you observe this influencing t-SNE/UMAP/etc equally drastically, or did it only affect clustering so strongly?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:414,testability,observ,observe,414,"@LuckyMD Thanks. This agrees with what I suspected: that randomized PCA itself should be pretty stable, but downstream clustering procedures can be very unstable. I have little experience with clustering so I cannot really comment further, but this certainly should be a big red flag for taking a clustering result seriously. It's especially impressive that float32 vs float64 can cause such a difference. Did you observe this influencing t-SNE/UMAP/etc equally drastically, or did it only affect clustering so strongly?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:177,usability,experien,experience,177,"@LuckyMD Thanks. This agrees with what I suspected: that randomized PCA itself should be pretty stable, but downstream clustering procedures can be very unstable. I have little experience with clustering so I cannot really comment further, but this certainly should be a big red flag for taking a clustering result seriously. It's especially impressive that float32 vs float64 can cause such a difference. Did you observe this influencing t-SNE/UMAP/etc equally drastically, or did it only affect clustering so strongly?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:196,availability,cluster,clustering,196,"The reason I actually noticed it in the first place was a difference in t-SNE and UMAP... the difference wasn't huge, but noticeable. Here's the issue where you can see the UMAP plots (#324). The clustering differences were actually rather small... the more worrying thing was the UMAP at the time as it appeared to show a bridge between two clusters that I didn't really expect to be there and didn't see in the float64 data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:342,availability,cluster,clusters,342,"The reason I actually noticed it in the first place was a difference in t-SNE and UMAP... the difference wasn't huge, but noticeable. Here's the issue where you can see the UMAP plots (#324). The clustering differences were actually rather small... the more worrying thing was the UMAP at the time as it appeared to show a bridge between two clusters that I didn't really expect to be there and didn't see in the float64 data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:196,deployability,cluster,clustering,196,"The reason I actually noticed it in the first place was a difference in t-SNE and UMAP... the difference wasn't huge, but noticeable. Here's the issue where you can see the UMAP plots (#324). The clustering differences were actually rather small... the more worrying thing was the UMAP at the time as it appeared to show a bridge between two clusters that I didn't really expect to be there and didn't see in the float64 data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:342,deployability,cluster,clusters,342,"The reason I actually noticed it in the first place was a difference in t-SNE and UMAP... the difference wasn't huge, but noticeable. Here's the issue where you can see the UMAP plots (#324). The clustering differences were actually rather small... the more worrying thing was the UMAP at the time as it appeared to show a bridge between two clusters that I didn't really expect to be there and didn't see in the float64 data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:323,integrability,bridg,bridge,323,"The reason I actually noticed it in the first place was a difference in t-SNE and UMAP... the difference wasn't huge, but noticeable. Here's the issue where you can see the UMAP plots (#324). The clustering differences were actually rather small... the more worrying thing was the UMAP at the time as it appeared to show a bridge between two clusters that I didn't really expect to be there and didn't see in the float64 data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:323,interoperability,bridg,bridge,323,"The reason I actually noticed it in the first place was a difference in t-SNE and UMAP... the difference wasn't huge, but noticeable. Here's the issue where you can see the UMAP plots (#324). The clustering differences were actually rather small... the more worrying thing was the UMAP at the time as it appeared to show a bridge between two clusters that I didn't really expect to be there and didn't see in the float64 data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:293,performance,time,time,293,"The reason I actually noticed it in the first place was a difference in t-SNE and UMAP... the difference wasn't huge, but noticeable. Here's the issue where you can see the UMAP plots (#324). The clustering differences were actually rather small... the more worrying thing was the UMAP at the time as it appeared to show a bridge between two clusters that I didn't really expect to be there and didn't see in the float64 data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:16,availability,ping,ping,16,> Great. Please ping me here when you upload the file to... I uploaded the file.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:462,availability,robust,robustly,462,"> This agrees with what I suspected: that randomized PCA itself should be pretty stable, . No, if you look into how higher PCs vary, you see that they vary drastically depending on the seed or computational platform. That also makes sense, it's a power-method that does the computation, that runs into some unstable stuff. > PCs were similar to within a couple of decimal points,. I'm very sure that you only observed this for the first couple of PCs, which you robustly estimate. Going higher, you end up in some local minima for a subsapce; I believe that it doesn't mean it doesn't capture important variation; it just means that it's a local minimum that the algorithm converges into... something we observe all the time when training models... in the context of Lanzcos and other algorithms powering SVD, PCA etc., it's usually a nuisance that you have that instability when you go higher in iterations; but also there, people just use the results even if they know they don't get the *exact* 50th eigen vector...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:168,deployability,depend,depending,168,"> This agrees with what I suspected: that randomized PCA itself should be pretty stable, . No, if you look into how higher PCs vary, you see that they vary drastically depending on the seed or computational platform. That also makes sense, it's a power-method that does the computation, that runs into some unstable stuff. > PCs were similar to within a couple of decimal points,. I'm very sure that you only observed this for the first couple of PCs, which you robustly estimate. Going higher, you end up in some local minima for a subsapce; I believe that it doesn't mean it doesn't capture important variation; it just means that it's a local minimum that the algorithm converges into... something we observe all the time when training models... in the context of Lanzcos and other algorithms powering SVD, PCA etc., it's usually a nuisance that you have that instability when you go higher in iterations; but also there, people just use the results even if they know they don't get the *exact* 50th eigen vector...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:409,deployability,observ,observed,409,"> This agrees with what I suspected: that randomized PCA itself should be pretty stable, . No, if you look into how higher PCs vary, you see that they vary drastically depending on the seed or computational platform. That also makes sense, it's a power-method that does the computation, that runs into some unstable stuff. > PCs were similar to within a couple of decimal points,. I'm very sure that you only observed this for the first couple of PCs, which you robustly estimate. Going higher, you end up in some local minima for a subsapce; I believe that it doesn't mean it doesn't capture important variation; it just means that it's a local minimum that the algorithm converges into... something we observe all the time when training models... in the context of Lanzcos and other algorithms powering SVD, PCA etc., it's usually a nuisance that you have that instability when you go higher in iterations; but also there, people just use the results even if they know they don't get the *exact* 50th eigen vector...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:704,deployability,observ,observe,704,"> This agrees with what I suspected: that randomized PCA itself should be pretty stable, . No, if you look into how higher PCs vary, you see that they vary drastically depending on the seed or computational platform. That also makes sense, it's a power-method that does the computation, that runs into some unstable stuff. > PCs were similar to within a couple of decimal points,. I'm very sure that you only observed this for the first couple of PCs, which you robustly estimate. Going higher, you end up in some local minima for a subsapce; I believe that it doesn't mean it doesn't capture important variation; it just means that it's a local minimum that the algorithm converges into... something we observe all the time when training models... in the context of Lanzcos and other algorithms powering SVD, PCA etc., it's usually a nuisance that you have that instability when you go higher in iterations; but also there, people just use the results even if they know they don't get the *exact* 50th eigen vector...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:247,energy efficiency,power,power-method,247,"> This agrees with what I suspected: that randomized PCA itself should be pretty stable, . No, if you look into how higher PCs vary, you see that they vary drastically depending on the seed or computational platform. That also makes sense, it's a power-method that does the computation, that runs into some unstable stuff. > PCs were similar to within a couple of decimal points,. I'm very sure that you only observed this for the first couple of PCs, which you robustly estimate. Going higher, you end up in some local minima for a subsapce; I believe that it doesn't mean it doesn't capture important variation; it just means that it's a local minimum that the algorithm converges into... something we observe all the time when training models... in the context of Lanzcos and other algorithms powering SVD, PCA etc., it's usually a nuisance that you have that instability when you go higher in iterations; but also there, people just use the results even if they know they don't get the *exact* 50th eigen vector...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:471,energy efficiency,estimat,estimate,471,"> This agrees with what I suspected: that randomized PCA itself should be pretty stable, . No, if you look into how higher PCs vary, you see that they vary drastically depending on the seed or computational platform. That also makes sense, it's a power-method that does the computation, that runs into some unstable stuff. > PCs were similar to within a couple of decimal points,. I'm very sure that you only observed this for the first couple of PCs, which you robustly estimate. Going higher, you end up in some local minima for a subsapce; I believe that it doesn't mean it doesn't capture important variation; it just means that it's a local minimum that the algorithm converges into... something we observe all the time when training models... in the context of Lanzcos and other algorithms powering SVD, PCA etc., it's usually a nuisance that you have that instability when you go higher in iterations; but also there, people just use the results even if they know they don't get the *exact* 50th eigen vector...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:739,energy efficiency,model,models,739,"> This agrees with what I suspected: that randomized PCA itself should be pretty stable, . No, if you look into how higher PCs vary, you see that they vary drastically depending on the seed or computational platform. That also makes sense, it's a power-method that does the computation, that runs into some unstable stuff. > PCs were similar to within a couple of decimal points,. I'm very sure that you only observed this for the first couple of PCs, which you robustly estimate. Going higher, you end up in some local minima for a subsapce; I believe that it doesn't mean it doesn't capture important variation; it just means that it's a local minimum that the algorithm converges into... something we observe all the time when training models... in the context of Lanzcos and other algorithms powering SVD, PCA etc., it's usually a nuisance that you have that instability when you go higher in iterations; but also there, people just use the results even if they know they don't get the *exact* 50th eigen vector...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:796,energy efficiency,power,powering,796,"> This agrees with what I suspected: that randomized PCA itself should be pretty stable, . No, if you look into how higher PCs vary, you see that they vary drastically depending on the seed or computational platform. That also makes sense, it's a power-method that does the computation, that runs into some unstable stuff. > PCs were similar to within a couple of decimal points,. I'm very sure that you only observed this for the first couple of PCs, which you robustly estimate. Going higher, you end up in some local minima for a subsapce; I believe that it doesn't mean it doesn't capture important variation; it just means that it's a local minimum that the algorithm converges into... something we observe all the time when training models... in the context of Lanzcos and other algorithms powering SVD, PCA etc., it's usually a nuisance that you have that instability when you go higher in iterations; but also there, people just use the results even if they know they don't get the *exact* 50th eigen vector...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:168,integrability,depend,depending,168,"> This agrees with what I suspected: that randomized PCA itself should be pretty stable, . No, if you look into how higher PCs vary, you see that they vary drastically depending on the seed or computational platform. That also makes sense, it's a power-method that does the computation, that runs into some unstable stuff. > PCs were similar to within a couple of decimal points,. I'm very sure that you only observed this for the first couple of PCs, which you robustly estimate. Going higher, you end up in some local minima for a subsapce; I believe that it doesn't mean it doesn't capture important variation; it just means that it's a local minimum that the algorithm converges into... something we observe all the time when training models... in the context of Lanzcos and other algorithms powering SVD, PCA etc., it's usually a nuisance that you have that instability when you go higher in iterations; but also there, people just use the results even if they know they don't get the *exact* 50th eigen vector...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:354,integrability,coupl,couple,354,"> This agrees with what I suspected: that randomized PCA itself should be pretty stable, . No, if you look into how higher PCs vary, you see that they vary drastically depending on the seed or computational platform. That also makes sense, it's a power-method that does the computation, that runs into some unstable stuff. > PCs were similar to within a couple of decimal points,. I'm very sure that you only observed this for the first couple of PCs, which you robustly estimate. Going higher, you end up in some local minima for a subsapce; I believe that it doesn't mean it doesn't capture important variation; it just means that it's a local minimum that the algorithm converges into... something we observe all the time when training models... in the context of Lanzcos and other algorithms powering SVD, PCA etc., it's usually a nuisance that you have that instability when you go higher in iterations; but also there, people just use the results even if they know they don't get the *exact* 50th eigen vector...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:437,integrability,coupl,couple,437,"> This agrees with what I suspected: that randomized PCA itself should be pretty stable, . No, if you look into how higher PCs vary, you see that they vary drastically depending on the seed or computational platform. That also makes sense, it's a power-method that does the computation, that runs into some unstable stuff. > PCs were similar to within a couple of decimal points,. I'm very sure that you only observed this for the first couple of PCs, which you robustly estimate. Going higher, you end up in some local minima for a subsapce; I believe that it doesn't mean it doesn't capture important variation; it just means that it's a local minimum that the algorithm converges into... something we observe all the time when training models... in the context of Lanzcos and other algorithms powering SVD, PCA etc., it's usually a nuisance that you have that instability when you go higher in iterations; but also there, people just use the results even if they know they don't get the *exact* 50th eigen vector...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:533,integrability,sub,subsapce,533,"> This agrees with what I suspected: that randomized PCA itself should be pretty stable, . No, if you look into how higher PCs vary, you see that they vary drastically depending on the seed or computational platform. That also makes sense, it's a power-method that does the computation, that runs into some unstable stuff. > PCs were similar to within a couple of decimal points,. I'm very sure that you only observed this for the first couple of PCs, which you robustly estimate. Going higher, you end up in some local minima for a subsapce; I believe that it doesn't mean it doesn't capture important variation; it just means that it's a local minimum that the algorithm converges into... something we observe all the time when training models... in the context of Lanzcos and other algorithms powering SVD, PCA etc., it's usually a nuisance that you have that instability when you go higher in iterations; but also there, people just use the results even if they know they don't get the *exact* 50th eigen vector...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:207,interoperability,platform,platform,207,"> This agrees with what I suspected: that randomized PCA itself should be pretty stable, . No, if you look into how higher PCs vary, you see that they vary drastically depending on the seed or computational platform. That also makes sense, it's a power-method that does the computation, that runs into some unstable stuff. > PCs were similar to within a couple of decimal points,. I'm very sure that you only observed this for the first couple of PCs, which you robustly estimate. Going higher, you end up in some local minima for a subsapce; I believe that it doesn't mean it doesn't capture important variation; it just means that it's a local minimum that the algorithm converges into... something we observe all the time when training models... in the context of Lanzcos and other algorithms powering SVD, PCA etc., it's usually a nuisance that you have that instability when you go higher in iterations; but also there, people just use the results even if they know they don't get the *exact* 50th eigen vector...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:168,modifiability,depend,depending,168,"> This agrees with what I suspected: that randomized PCA itself should be pretty stable, . No, if you look into how higher PCs vary, you see that they vary drastically depending on the seed or computational platform. That also makes sense, it's a power-method that does the computation, that runs into some unstable stuff. > PCs were similar to within a couple of decimal points,. I'm very sure that you only observed this for the first couple of PCs, which you robustly estimate. Going higher, you end up in some local minima for a subsapce; I believe that it doesn't mean it doesn't capture important variation; it just means that it's a local minimum that the algorithm converges into... something we observe all the time when training models... in the context of Lanzcos and other algorithms powering SVD, PCA etc., it's usually a nuisance that you have that instability when you go higher in iterations; but also there, people just use the results even if they know they don't get the *exact* 50th eigen vector...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:354,modifiability,coupl,couple,354,"> This agrees with what I suspected: that randomized PCA itself should be pretty stable, . No, if you look into how higher PCs vary, you see that they vary drastically depending on the seed or computational platform. That also makes sense, it's a power-method that does the computation, that runs into some unstable stuff. > PCs were similar to within a couple of decimal points,. I'm very sure that you only observed this for the first couple of PCs, which you robustly estimate. Going higher, you end up in some local minima for a subsapce; I believe that it doesn't mean it doesn't capture important variation; it just means that it's a local minimum that the algorithm converges into... something we observe all the time when training models... in the context of Lanzcos and other algorithms powering SVD, PCA etc., it's usually a nuisance that you have that instability when you go higher in iterations; but also there, people just use the results even if they know they don't get the *exact* 50th eigen vector...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:437,modifiability,coupl,couple,437,"> This agrees with what I suspected: that randomized PCA itself should be pretty stable, . No, if you look into how higher PCs vary, you see that they vary drastically depending on the seed or computational platform. That also makes sense, it's a power-method that does the computation, that runs into some unstable stuff. > PCs were similar to within a couple of decimal points,. I'm very sure that you only observed this for the first couple of PCs, which you robustly estimate. Going higher, you end up in some local minima for a subsapce; I believe that it doesn't mean it doesn't capture important variation; it just means that it's a local minimum that the algorithm converges into... something we observe all the time when training models... in the context of Lanzcos and other algorithms powering SVD, PCA etc., it's usually a nuisance that you have that instability when you go higher in iterations; but also there, people just use the results even if they know they don't get the *exact* 50th eigen vector...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:720,performance,time,time,720,"> This agrees with what I suspected: that randomized PCA itself should be pretty stable, . No, if you look into how higher PCs vary, you see that they vary drastically depending on the seed or computational platform. That also makes sense, it's a power-method that does the computation, that runs into some unstable stuff. > PCs were similar to within a couple of decimal points,. I'm very sure that you only observed this for the first couple of PCs, which you robustly estimate. Going higher, you end up in some local minima for a subsapce; I believe that it doesn't mean it doesn't capture important variation; it just means that it's a local minimum that the algorithm converges into... something we observe all the time when training models... in the context of Lanzcos and other algorithms powering SVD, PCA etc., it's usually a nuisance that you have that instability when you go higher in iterations; but also there, people just use the results even if they know they don't get the *exact* 50th eigen vector...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:265,reliability,doe,does,265,"> This agrees with what I suspected: that randomized PCA itself should be pretty stable, . No, if you look into how higher PCs vary, you see that they vary drastically depending on the seed or computational platform. That also makes sense, it's a power-method that does the computation, that runs into some unstable stuff. > PCs were similar to within a couple of decimal points,. I'm very sure that you only observed this for the first couple of PCs, which you robustly estimate. Going higher, you end up in some local minima for a subsapce; I believe that it doesn't mean it doesn't capture important variation; it just means that it's a local minimum that the algorithm converges into... something we observe all the time when training models... in the context of Lanzcos and other algorithms powering SVD, PCA etc., it's usually a nuisance that you have that instability when you go higher in iterations; but also there, people just use the results even if they know they don't get the *exact* 50th eigen vector...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:462,reliability,robust,robustly,462,"> This agrees with what I suspected: that randomized PCA itself should be pretty stable, . No, if you look into how higher PCs vary, you see that they vary drastically depending on the seed or computational platform. That also makes sense, it's a power-method that does the computation, that runs into some unstable stuff. > PCs were similar to within a couple of decimal points,. I'm very sure that you only observed this for the first couple of PCs, which you robustly estimate. Going higher, you end up in some local minima for a subsapce; I believe that it doesn't mean it doesn't capture important variation; it just means that it's a local minimum that the algorithm converges into... something we observe all the time when training models... in the context of Lanzcos and other algorithms powering SVD, PCA etc., it's usually a nuisance that you have that instability when you go higher in iterations; but also there, people just use the results even if they know they don't get the *exact* 50th eigen vector...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:561,reliability,doe,doesn,561,"> This agrees with what I suspected: that randomized PCA itself should be pretty stable, . No, if you look into how higher PCs vary, you see that they vary drastically depending on the seed or computational platform. That also makes sense, it's a power-method that does the computation, that runs into some unstable stuff. > PCs were similar to within a couple of decimal points,. I'm very sure that you only observed this for the first couple of PCs, which you robustly estimate. Going higher, you end up in some local minima for a subsapce; I believe that it doesn't mean it doesn't capture important variation; it just means that it's a local minimum that the algorithm converges into... something we observe all the time when training models... in the context of Lanzcos and other algorithms powering SVD, PCA etc., it's usually a nuisance that you have that instability when you go higher in iterations; but also there, people just use the results even if they know they don't get the *exact* 50th eigen vector...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:577,reliability,doe,doesn,577,"> This agrees with what I suspected: that randomized PCA itself should be pretty stable, . No, if you look into how higher PCs vary, you see that they vary drastically depending on the seed or computational platform. That also makes sense, it's a power-method that does the computation, that runs into some unstable stuff. > PCs were similar to within a couple of decimal points,. I'm very sure that you only observed this for the first couple of PCs, which you robustly estimate. Going higher, you end up in some local minima for a subsapce; I believe that it doesn't mean it doesn't capture important variation; it just means that it's a local minimum that the algorithm converges into... something we observe all the time when training models... in the context of Lanzcos and other algorithms powering SVD, PCA etc., it's usually a nuisance that you have that instability when you go higher in iterations; but also there, people just use the results even if they know they don't get the *exact* 50th eigen vector...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:168,safety,depend,depending,168,"> This agrees with what I suspected: that randomized PCA itself should be pretty stable, . No, if you look into how higher PCs vary, you see that they vary drastically depending on the seed or computational platform. That also makes sense, it's a power-method that does the computation, that runs into some unstable stuff. > PCs were similar to within a couple of decimal points,. I'm very sure that you only observed this for the first couple of PCs, which you robustly estimate. Going higher, you end up in some local minima for a subsapce; I believe that it doesn't mean it doesn't capture important variation; it just means that it's a local minimum that the algorithm converges into... something we observe all the time when training models... in the context of Lanzcos and other algorithms powering SVD, PCA etc., it's usually a nuisance that you have that instability when you go higher in iterations; but also there, people just use the results even if they know they don't get the *exact* 50th eigen vector...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:462,safety,robust,robustly,462,"> This agrees with what I suspected: that randomized PCA itself should be pretty stable, . No, if you look into how higher PCs vary, you see that they vary drastically depending on the seed or computational platform. That also makes sense, it's a power-method that does the computation, that runs into some unstable stuff. > PCs were similar to within a couple of decimal points,. I'm very sure that you only observed this for the first couple of PCs, which you robustly estimate. Going higher, you end up in some local minima for a subsapce; I believe that it doesn't mean it doesn't capture important variation; it just means that it's a local minimum that the algorithm converges into... something we observe all the time when training models... in the context of Lanzcos and other algorithms powering SVD, PCA etc., it's usually a nuisance that you have that instability when you go higher in iterations; but also there, people just use the results even if they know they don't get the *exact* 50th eigen vector...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:739,security,model,models,739,"> This agrees with what I suspected: that randomized PCA itself should be pretty stable, . No, if you look into how higher PCs vary, you see that they vary drastically depending on the seed or computational platform. That also makes sense, it's a power-method that does the computation, that runs into some unstable stuff. > PCs were similar to within a couple of decimal points,. I'm very sure that you only observed this for the first couple of PCs, which you robustly estimate. Going higher, you end up in some local minima for a subsapce; I believe that it doesn't mean it doesn't capture important variation; it just means that it's a local minimum that the algorithm converges into... something we observe all the time when training models... in the context of Lanzcos and other algorithms powering SVD, PCA etc., it's usually a nuisance that you have that instability when you go higher in iterations; but also there, people just use the results even if they know they don't get the *exact* 50th eigen vector...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:168,testability,depend,depending,168,"> This agrees with what I suspected: that randomized PCA itself should be pretty stable, . No, if you look into how higher PCs vary, you see that they vary drastically depending on the seed or computational platform. That also makes sense, it's a power-method that does the computation, that runs into some unstable stuff. > PCs were similar to within a couple of decimal points,. I'm very sure that you only observed this for the first couple of PCs, which you robustly estimate. Going higher, you end up in some local minima for a subsapce; I believe that it doesn't mean it doesn't capture important variation; it just means that it's a local minimum that the algorithm converges into... something we observe all the time when training models... in the context of Lanzcos and other algorithms powering SVD, PCA etc., it's usually a nuisance that you have that instability when you go higher in iterations; but also there, people just use the results even if they know they don't get the *exact* 50th eigen vector...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:354,testability,coupl,couple,354,"> This agrees with what I suspected: that randomized PCA itself should be pretty stable, . No, if you look into how higher PCs vary, you see that they vary drastically depending on the seed or computational platform. That also makes sense, it's a power-method that does the computation, that runs into some unstable stuff. > PCs were similar to within a couple of decimal points,. I'm very sure that you only observed this for the first couple of PCs, which you robustly estimate. Going higher, you end up in some local minima for a subsapce; I believe that it doesn't mean it doesn't capture important variation; it just means that it's a local minimum that the algorithm converges into... something we observe all the time when training models... in the context of Lanzcos and other algorithms powering SVD, PCA etc., it's usually a nuisance that you have that instability when you go higher in iterations; but also there, people just use the results even if they know they don't get the *exact* 50th eigen vector...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:409,testability,observ,observed,409,"> This agrees with what I suspected: that randomized PCA itself should be pretty stable, . No, if you look into how higher PCs vary, you see that they vary drastically depending on the seed or computational platform. That also makes sense, it's a power-method that does the computation, that runs into some unstable stuff. > PCs were similar to within a couple of decimal points,. I'm very sure that you only observed this for the first couple of PCs, which you robustly estimate. Going higher, you end up in some local minima for a subsapce; I believe that it doesn't mean it doesn't capture important variation; it just means that it's a local minimum that the algorithm converges into... something we observe all the time when training models... in the context of Lanzcos and other algorithms powering SVD, PCA etc., it's usually a nuisance that you have that instability when you go higher in iterations; but also there, people just use the results even if they know they don't get the *exact* 50th eigen vector...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:437,testability,coupl,couple,437,"> This agrees with what I suspected: that randomized PCA itself should be pretty stable, . No, if you look into how higher PCs vary, you see that they vary drastically depending on the seed or computational platform. That also makes sense, it's a power-method that does the computation, that runs into some unstable stuff. > PCs were similar to within a couple of decimal points,. I'm very sure that you only observed this for the first couple of PCs, which you robustly estimate. Going higher, you end up in some local minima for a subsapce; I believe that it doesn't mean it doesn't capture important variation; it just means that it's a local minimum that the algorithm converges into... something we observe all the time when training models... in the context of Lanzcos and other algorithms powering SVD, PCA etc., it's usually a nuisance that you have that instability when you go higher in iterations; but also there, people just use the results even if they know they don't get the *exact* 50th eigen vector...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:704,testability,observ,observe,704,"> This agrees with what I suspected: that randomized PCA itself should be pretty stable, . No, if you look into how higher PCs vary, you see that they vary drastically depending on the seed or computational platform. That also makes sense, it's a power-method that does the computation, that runs into some unstable stuff. > PCs were similar to within a couple of decimal points,. I'm very sure that you only observed this for the first couple of PCs, which you robustly estimate. Going higher, you end up in some local minima for a subsapce; I believe that it doesn't mean it doesn't capture important variation; it just means that it's a local minimum that the algorithm converges into... something we observe all the time when training models... in the context of Lanzcos and other algorithms powering SVD, PCA etc., it's usually a nuisance that you have that instability when you go higher in iterations; but also there, people just use the results even if they know they don't get the *exact* 50th eigen vector...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:756,testability,context,context,756,"> This agrees with what I suspected: that randomized PCA itself should be pretty stable, . No, if you look into how higher PCs vary, you see that they vary drastically depending on the seed or computational platform. That also makes sense, it's a power-method that does the computation, that runs into some unstable stuff. > PCs were similar to within a couple of decimal points,. I'm very sure that you only observed this for the first couple of PCs, which you robustly estimate. Going higher, you end up in some local minima for a subsapce; I believe that it doesn't mean it doesn't capture important variation; it just means that it's a local minimum that the algorithm converges into... something we observe all the time when training models... in the context of Lanzcos and other algorithms powering SVD, PCA etc., it's usually a nuisance that you have that instability when you go higher in iterations; but also there, people just use the results even if they know they don't get the *exact* 50th eigen vector...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:520,usability,minim,minima,520,"> This agrees with what I suspected: that randomized PCA itself should be pretty stable, . No, if you look into how higher PCs vary, you see that they vary drastically depending on the seed or computational platform. That also makes sense, it's a power-method that does the computation, that runs into some unstable stuff. > PCs were similar to within a couple of decimal points,. I'm very sure that you only observed this for the first couple of PCs, which you robustly estimate. Going higher, you end up in some local minima for a subsapce; I believe that it doesn't mean it doesn't capture important variation; it just means that it's a local minimum that the algorithm converges into... something we observe all the time when training models... in the context of Lanzcos and other algorithms powering SVD, PCA etc., it's usually a nuisance that you have that instability when you go higher in iterations; but also there, people just use the results even if they know they don't get the *exact* 50th eigen vector...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:646,usability,minim,minimum,646,"> This agrees with what I suspected: that randomized PCA itself should be pretty stable, . No, if you look into how higher PCs vary, you see that they vary drastically depending on the seed or computational platform. That also makes sense, it's a power-method that does the computation, that runs into some unstable stuff. > PCs were similar to within a couple of decimal points,. I'm very sure that you only observed this for the first couple of PCs, which you robustly estimate. Going higher, you end up in some local minima for a subsapce; I believe that it doesn't mean it doesn't capture important variation; it just means that it's a local minimum that the algorithm converges into... something we observe all the time when training models... in the context of Lanzcos and other algorithms powering SVD, PCA etc., it's usually a nuisance that you have that instability when you go higher in iterations; but also there, people just use the results even if they know they don't get the *exact* 50th eigen vector...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:214,availability,state,statement,214,"True, I only checked the first couple of PCs. I mainly noticed that while the top PCs varied in the 3rd or 4th decimal place, the connectivities varies within the first decimal place or even more. I can't make any statement about the later PCs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:31,integrability,coupl,couple,31,"True, I only checked the first couple of PCs. I mainly noticed that while the top PCs varied in the 3rd or 4th decimal place, the connectivities varies within the first decimal place or even more. I can't make any statement about the later PCs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:214,integrability,state,statement,214,"True, I only checked the first couple of PCs. I mainly noticed that while the top PCs varied in the 3rd or 4th decimal place, the connectivities varies within the first decimal place or even more. I can't make any statement about the later PCs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:31,modifiability,coupl,couple,31,"True, I only checked the first couple of PCs. I mainly noticed that while the top PCs varied in the 3rd or 4th decimal place, the connectivities varies within the first decimal place or even more. I can't make any statement about the later PCs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/issues/325:31,testability,coupl,couple,31,"True, I only checked the first couple of PCs. I mainly noticed that while the top PCs varied in the 3rd or 4th decimal place, the connectivities varies within the first decimal place or even more. I can't make any statement about the later PCs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325
https://github.com/scverse/scanpy/pull/327:0,energy efficiency,Cool,Cool,0,Cool!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/327
https://github.com/scverse/scanpy/issues/328:299,availability,error,error,299,"Sorry for the super-late response! I just worked through almost 60 issues starting with the most recent, this is the last one... Sorry about that. `paga_path` requires computing a pseudotime before-hand as one needs to order cells at single-cell resolution along the path. I added a more meaningful error message stating that. PS: Now, there is also a test for PAGA [here](https://github.com/theislab/scanpy/blob/master/scanpy/tests/notebooks/test_paga_paul15_subsampled.py), making sure that the canonical use ([here](https://nbviewer.jupyter.org/github/theislab/paga/blob/master/blood/paul15/paul15.ipynb)) remains unchanged.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/328
https://github.com/scverse/scanpy/issues/328:305,integrability,messag,message,305,"Sorry for the super-late response! I just worked through almost 60 issues starting with the most recent, this is the last one... Sorry about that. `paga_path` requires computing a pseudotime before-hand as one needs to order cells at single-cell resolution along the path. I added a more meaningful error message stating that. PS: Now, there is also a test for PAGA [here](https://github.com/theislab/scanpy/blob/master/scanpy/tests/notebooks/test_paga_paul15_subsampled.py), making sure that the canonical use ([here](https://nbviewer.jupyter.org/github/theislab/paga/blob/master/blood/paul15/paul15.ipynb)) remains unchanged.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/328
https://github.com/scverse/scanpy/issues/328:305,interoperability,messag,message,305,"Sorry for the super-late response! I just worked through almost 60 issues starting with the most recent, this is the last one... Sorry about that. `paga_path` requires computing a pseudotime before-hand as one needs to order cells at single-cell resolution along the path. I added a more meaningful error message stating that. PS: Now, there is also a test for PAGA [here](https://github.com/theislab/scanpy/blob/master/scanpy/tests/notebooks/test_paga_paul15_subsampled.py), making sure that the canonical use ([here](https://nbviewer.jupyter.org/github/theislab/paga/blob/master/blood/paul15/paul15.ipynb)) remains unchanged.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/328
https://github.com/scverse/scanpy/issues/328:299,performance,error,error,299,"Sorry for the super-late response! I just worked through almost 60 issues starting with the most recent, this is the last one... Sorry about that. `paga_path` requires computing a pseudotime before-hand as one needs to order cells at single-cell resolution along the path. I added a more meaningful error message stating that. PS: Now, there is also a test for PAGA [here](https://github.com/theislab/scanpy/blob/master/scanpy/tests/notebooks/test_paga_paul15_subsampled.py), making sure that the canonical use ([here](https://nbviewer.jupyter.org/github/theislab/paga/blob/master/blood/paul15/paul15.ipynb)) remains unchanged.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/328
https://github.com/scverse/scanpy/issues/328:299,safety,error,error,299,"Sorry for the super-late response! I just worked through almost 60 issues starting with the most recent, this is the last one... Sorry about that. `paga_path` requires computing a pseudotime before-hand as one needs to order cells at single-cell resolution along the path. I added a more meaningful error message stating that. PS: Now, there is also a test for PAGA [here](https://github.com/theislab/scanpy/blob/master/scanpy/tests/notebooks/test_paga_paul15_subsampled.py), making sure that the canonical use ([here](https://nbviewer.jupyter.org/github/theislab/paga/blob/master/blood/paul15/paul15.ipynb)) remains unchanged.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/328
https://github.com/scverse/scanpy/issues/328:352,safety,test,test,352,"Sorry for the super-late response! I just worked through almost 60 issues starting with the most recent, this is the last one... Sorry about that. `paga_path` requires computing a pseudotime before-hand as one needs to order cells at single-cell resolution along the path. I added a more meaningful error message stating that. PS: Now, there is also a test for PAGA [here](https://github.com/theislab/scanpy/blob/master/scanpy/tests/notebooks/test_paga_paul15_subsampled.py), making sure that the canonical use ([here](https://nbviewer.jupyter.org/github/theislab/paga/blob/master/blood/paul15/paul15.ipynb)) remains unchanged.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/328
https://github.com/scverse/scanpy/issues/328:427,safety,test,tests,427,"Sorry for the super-late response! I just worked through almost 60 issues starting with the most recent, this is the last one... Sorry about that. `paga_path` requires computing a pseudotime before-hand as one needs to order cells at single-cell resolution along the path. I added a more meaningful error message stating that. PS: Now, there is also a test for PAGA [here](https://github.com/theislab/scanpy/blob/master/scanpy/tests/notebooks/test_paga_paul15_subsampled.py), making sure that the canonical use ([here](https://nbviewer.jupyter.org/github/theislab/paga/blob/master/blood/paul15/paul15.ipynb)) remains unchanged.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/328
https://github.com/scverse/scanpy/issues/328:352,testability,test,test,352,"Sorry for the super-late response! I just worked through almost 60 issues starting with the most recent, this is the last one... Sorry about that. `paga_path` requires computing a pseudotime before-hand as one needs to order cells at single-cell resolution along the path. I added a more meaningful error message stating that. PS: Now, there is also a test for PAGA [here](https://github.com/theislab/scanpy/blob/master/scanpy/tests/notebooks/test_paga_paul15_subsampled.py), making sure that the canonical use ([here](https://nbviewer.jupyter.org/github/theislab/paga/blob/master/blood/paul15/paul15.ipynb)) remains unchanged.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/328
https://github.com/scverse/scanpy/issues/328:427,testability,test,tests,427,"Sorry for the super-late response! I just worked through almost 60 issues starting with the most recent, this is the last one... Sorry about that. `paga_path` requires computing a pseudotime before-hand as one needs to order cells at single-cell resolution along the path. I added a more meaningful error message stating that. PS: Now, there is also a test for PAGA [here](https://github.com/theislab/scanpy/blob/master/scanpy/tests/notebooks/test_paga_paul15_subsampled.py), making sure that the canonical use ([here](https://nbviewer.jupyter.org/github/theislab/paga/blob/master/blood/paul15/paul15.ipynb)) remains unchanged.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/328
https://github.com/scverse/scanpy/issues/328:299,usability,error,error,299,"Sorry for the super-late response! I just worked through almost 60 issues starting with the most recent, this is the last one... Sorry about that. `paga_path` requires computing a pseudotime before-hand as one needs to order cells at single-cell resolution along the path. I added a more meaningful error message stating that. PS: Now, there is also a test for PAGA [here](https://github.com/theislab/scanpy/blob/master/scanpy/tests/notebooks/test_paga_paul15_subsampled.py), making sure that the canonical use ([here](https://nbviewer.jupyter.org/github/theislab/paga/blob/master/blood/paul15/paul15.ipynb)) remains unchanged.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/328
https://github.com/scverse/scanpy/pull/330:200,deployability,releas,release,200,"The whole PR is pretty awesome already! I wrote some comments... Can you also add the function to the docs, cross reference the deprecated and the new function in the Notes section and add it to the [release notes](https://github.com/theislab/scanpy/blob/master/docs/release_notes.rst)?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/330
https://github.com/scverse/scanpy/pull/330:31,availability,slo,slowly,31,"I'm merging this now, as we're slowly running in danger of getting conflicts with other people working on this. Please move forward with the remaining issues in a new PR. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/330
https://github.com/scverse/scanpy/pull/330:67,interoperability,conflict,conflicts,67,"I'm merging this now, as we're slowly running in danger of getting conflicts with other people working on this. Please move forward with the remaining issues in a new PR. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/330
https://github.com/scverse/scanpy/pull/330:31,reliability,slo,slowly,31,"I'm merging this now, as we're slowly running in danger of getting conflicts with other people working on this. Please move forward with the remaining issues in a new PR. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/330
https://github.com/scverse/scanpy/issues/331:212,deployability,observ,observations,212,"Sorry for the late response, Joshua! Could it be that your dataset has less than 50 cells or variables or something like this? I believe that you're stating this. Computing a 50 dimensional PCA with less than 50 observations is probably not possible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/331
https://github.com/scverse/scanpy/issues/331:93,modifiability,variab,variables,93,"Sorry for the late response, Joshua! Could it be that your dataset has less than 50 cells or variables or something like this? I believe that you're stating this. Computing a 50 dimensional PCA with less than 50 observations is probably not possible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/331
https://github.com/scverse/scanpy/issues/331:212,testability,observ,observations,212,"Sorry for the late response, Joshua! Could it be that your dataset has less than 50 cells or variables or something like this? I believe that you're stating this. Computing a 50 dimensional PCA with less than 50 observations is probably not possible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/331
https://github.com/scverse/scanpy/issues/332:20,availability,error,errors,20,"Addendum: different errors are generated depending on which axis is first sliced. The data set I'm loading is a dense matrix. ```. >>> data.X.dtype. dtype('<f4'). >>> data[:,0][0,:]. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1303, in __getitem__. return self._getitem_view(index). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1307, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 669, in __init__. self._init_as_view(X, oidx, vidx). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 694, in _init_as_view. uns_new = deepcopy(self._adata_ref._uns). File ""/usr/lib/python3.6/copy.py"", line 180, in deepcopy. y = _reconstruct(x, memo, *rv). File ""/usr/lib/python3.6/copy.py"", line 307, in _reconstruct. y[key] = value. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 444, in __setitem__. _init_actual_AnnData(adata_view). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 367, in _init_actual_AnnData. adata_view._init_as_actual(adata_view.copy()). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 880, in _init_as_actual. self._check_dimensions(). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1879, in _check_dimensions. .format(self._n_obs, self._obs.shape[0])). ValueError: Observations annot. `obs` must have number of rows of `X` (1), but has 2638 rows. >>> data[0,:][:,0]. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1303, in __getitem__. return self._getitem_view(index). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1307, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cellxg",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332
https://github.com/scverse/scanpy/issues/332:74,availability,sli,sliced,74,"Addendum: different errors are generated depending on which axis is first sliced. The data set I'm loading is a dense matrix. ```. >>> data.X.dtype. dtype('<f4'). >>> data[:,0][0,:]. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1303, in __getitem__. return self._getitem_view(index). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1307, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 669, in __init__. self._init_as_view(X, oidx, vidx). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 694, in _init_as_view. uns_new = deepcopy(self._adata_ref._uns). File ""/usr/lib/python3.6/copy.py"", line 180, in deepcopy. y = _reconstruct(x, memo, *rv). File ""/usr/lib/python3.6/copy.py"", line 307, in _reconstruct. y[key] = value. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 444, in __setitem__. _init_actual_AnnData(adata_view). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 367, in _init_actual_AnnData. adata_view._init_as_actual(adata_view.copy()). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 880, in _init_as_actual. self._check_dimensions(). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1879, in _check_dimensions. .format(self._n_obs, self._obs.shape[0])). ValueError: Observations annot. `obs` must have number of rows of `X` (1), but has 2638 rows. >>> data[0,:][:,0]. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1303, in __getitem__. return self._getitem_view(index). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1307, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cellxg",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332
https://github.com/scverse/scanpy/issues/332:41,deployability,depend,depending,41,"Addendum: different errors are generated depending on which axis is first sliced. The data set I'm loading is a dense matrix. ```. >>> data.X.dtype. dtype('<f4'). >>> data[:,0][0,:]. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1303, in __getitem__. return self._getitem_view(index). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1307, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 669, in __init__. self._init_as_view(X, oidx, vidx). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 694, in _init_as_view. uns_new = deepcopy(self._adata_ref._uns). File ""/usr/lib/python3.6/copy.py"", line 180, in deepcopy. y = _reconstruct(x, memo, *rv). File ""/usr/lib/python3.6/copy.py"", line 307, in _reconstruct. y[key] = value. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 444, in __setitem__. _init_actual_AnnData(adata_view). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 367, in _init_actual_AnnData. adata_view._init_as_actual(adata_view.copy()). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 880, in _init_as_actual. self._check_dimensions(). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1879, in _check_dimensions. .format(self._n_obs, self._obs.shape[0])). ValueError: Observations annot. `obs` must have number of rows of `X` (1), but has 2638 rows. >>> data[0,:][:,0]. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1303, in __getitem__. return self._getitem_view(index). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1307, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cellxg",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332
https://github.com/scverse/scanpy/issues/332:247,deployability,modul,module,247,"Addendum: different errors are generated depending on which axis is first sliced. The data set I'm loading is a dense matrix. ```. >>> data.X.dtype. dtype('<f4'). >>> data[:,0][0,:]. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1303, in __getitem__. return self._getitem_view(index). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1307, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 669, in __init__. self._init_as_view(X, oidx, vidx). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 694, in _init_as_view. uns_new = deepcopy(self._adata_ref._uns). File ""/usr/lib/python3.6/copy.py"", line 180, in deepcopy. y = _reconstruct(x, memo, *rv). File ""/usr/lib/python3.6/copy.py"", line 307, in _reconstruct. y[key] = value. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 444, in __setitem__. _init_actual_AnnData(adata_view). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 367, in _init_actual_AnnData. adata_view._init_as_actual(adata_view.copy()). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 880, in _init_as_actual. self._check_dimensions(). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1879, in _check_dimensions. .format(self._n_obs, self._obs.shape[0])). ValueError: Observations annot. `obs` must have number of rows of `X` (1), but has 2638 rows. >>> data[0,:][:,0]. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1303, in __getitem__. return self._getitem_view(index). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1307, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cellxg",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332
https://github.com/scverse/scanpy/issues/332:1529,deployability,Observ,Observations,1529,"site-packages/anndata/base.py"", line 1307, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 669, in __init__. self._init_as_view(X, oidx, vidx). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 694, in _init_as_view. uns_new = deepcopy(self._adata_ref._uns). File ""/usr/lib/python3.6/copy.py"", line 180, in deepcopy. y = _reconstruct(x, memo, *rv). File ""/usr/lib/python3.6/copy.py"", line 307, in _reconstruct. y[key] = value. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 444, in __setitem__. _init_actual_AnnData(adata_view). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 367, in _init_actual_AnnData. adata_view._init_as_actual(adata_view.copy()). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 880, in _init_as_actual. self._check_dimensions(). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1879, in _check_dimensions. .format(self._n_obs, self._obs.shape[0])). ValueError: Observations annot. `obs` must have number of rows of `X` (1), but has 2638 rows. >>> data[0,:][:,0]. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1303, in __getitem__. return self._getitem_view(index). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1307, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 669, in __init__. self._init_as_view(X, oidx, vidx). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 726, in _init_as_view. self._init_X_as_view(). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 741, in _init_X_as_view. X = self._adata_ref.X[self._oidx, self._vidx]. IndexError: too many indices for array. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332
https://github.com/scverse/scanpy/issues/332:1695,deployability,modul,module,1695,"site-packages/anndata/base.py"", line 1307, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 669, in __init__. self._init_as_view(X, oidx, vidx). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 694, in _init_as_view. uns_new = deepcopy(self._adata_ref._uns). File ""/usr/lib/python3.6/copy.py"", line 180, in deepcopy. y = _reconstruct(x, memo, *rv). File ""/usr/lib/python3.6/copy.py"", line 307, in _reconstruct. y[key] = value. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 444, in __setitem__. _init_actual_AnnData(adata_view). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 367, in _init_actual_AnnData. adata_view._init_as_actual(adata_view.copy()). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 880, in _init_as_actual. self._check_dimensions(). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1879, in _check_dimensions. .format(self._n_obs, self._obs.shape[0])). ValueError: Observations annot. `obs` must have number of rows of `X` (1), but has 2638 rows. >>> data[0,:][:,0]. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1303, in __getitem__. return self._getitem_view(index). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1307, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 669, in __init__. self._init_as_view(X, oidx, vidx). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 726, in _init_as_view. self._init_X_as_view(). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 741, in _init_X_as_view. X = self._adata_ref.X[self._oidx, self._vidx]. IndexError: too many indices for array. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332
https://github.com/scverse/scanpy/issues/332:99,energy efficiency,load,loading,99,"Addendum: different errors are generated depending on which axis is first sliced. The data set I'm loading is a dense matrix. ```. >>> data.X.dtype. dtype('<f4'). >>> data[:,0][0,:]. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1303, in __getitem__. return self._getitem_view(index). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1307, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 669, in __init__. self._init_as_view(X, oidx, vidx). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 694, in _init_as_view. uns_new = deepcopy(self._adata_ref._uns). File ""/usr/lib/python3.6/copy.py"", line 180, in deepcopy. y = _reconstruct(x, memo, *rv). File ""/usr/lib/python3.6/copy.py"", line 307, in _reconstruct. y[key] = value. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 444, in __setitem__. _init_actual_AnnData(adata_view). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 367, in _init_actual_AnnData. adata_view._init_as_actual(adata_view.copy()). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 880, in _init_as_actual. self._check_dimensions(). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1879, in _check_dimensions. .format(self._n_obs, self._obs.shape[0])). ValueError: Observations annot. `obs` must have number of rows of `X` (1), but has 2638 rows. >>> data[0,:][:,0]. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1303, in __getitem__. return self._getitem_view(index). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1307, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cellxg",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332
https://github.com/scverse/scanpy/issues/332:41,integrability,depend,depending,41,"Addendum: different errors are generated depending on which axis is first sliced. The data set I'm loading is a dense matrix. ```. >>> data.X.dtype. dtype('<f4'). >>> data[:,0][0,:]. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1303, in __getitem__. return self._getitem_view(index). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1307, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 669, in __init__. self._init_as_view(X, oidx, vidx). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 694, in _init_as_view. uns_new = deepcopy(self._adata_ref._uns). File ""/usr/lib/python3.6/copy.py"", line 180, in deepcopy. y = _reconstruct(x, memo, *rv). File ""/usr/lib/python3.6/copy.py"", line 307, in _reconstruct. y[key] = value. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 444, in __setitem__. _init_actual_AnnData(adata_view). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 367, in _init_actual_AnnData. adata_view._init_as_actual(adata_view.copy()). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 880, in _init_as_actual. self._check_dimensions(). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1879, in _check_dimensions. .format(self._n_obs, self._obs.shape[0])). ValueError: Observations annot. `obs` must have number of rows of `X` (1), but has 2638 rows. >>> data[0,:][:,0]. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1303, in __getitem__. return self._getitem_view(index). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1307, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cellxg",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332
https://github.com/scverse/scanpy/issues/332:1475,interoperability,format,format,1475,"site-packages/anndata/base.py"", line 1307, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 669, in __init__. self._init_as_view(X, oidx, vidx). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 694, in _init_as_view. uns_new = deepcopy(self._adata_ref._uns). File ""/usr/lib/python3.6/copy.py"", line 180, in deepcopy. y = _reconstruct(x, memo, *rv). File ""/usr/lib/python3.6/copy.py"", line 307, in _reconstruct. y[key] = value. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 444, in __setitem__. _init_actual_AnnData(adata_view). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 367, in _init_actual_AnnData. adata_view._init_as_actual(adata_view.copy()). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 880, in _init_as_actual. self._check_dimensions(). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1879, in _check_dimensions. .format(self._n_obs, self._obs.shape[0])). ValueError: Observations annot. `obs` must have number of rows of `X` (1), but has 2638 rows. >>> data[0,:][:,0]. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1303, in __getitem__. return self._getitem_view(index). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1307, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 669, in __init__. self._init_as_view(X, oidx, vidx). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 726, in _init_as_view. self._init_X_as_view(). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 741, in _init_X_as_view. X = self._adata_ref.X[self._oidx, self._vidx]. IndexError: too many indices for array. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332
https://github.com/scverse/scanpy/issues/332:41,modifiability,depend,depending,41,"Addendum: different errors are generated depending on which axis is first sliced. The data set I'm loading is a dense matrix. ```. >>> data.X.dtype. dtype('<f4'). >>> data[:,0][0,:]. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1303, in __getitem__. return self._getitem_view(index). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1307, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 669, in __init__. self._init_as_view(X, oidx, vidx). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 694, in _init_as_view. uns_new = deepcopy(self._adata_ref._uns). File ""/usr/lib/python3.6/copy.py"", line 180, in deepcopy. y = _reconstruct(x, memo, *rv). File ""/usr/lib/python3.6/copy.py"", line 307, in _reconstruct. y[key] = value. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 444, in __setitem__. _init_actual_AnnData(adata_view). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 367, in _init_actual_AnnData. adata_view._init_as_actual(adata_view.copy()). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 880, in _init_as_actual. self._check_dimensions(). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1879, in _check_dimensions. .format(self._n_obs, self._obs.shape[0])). ValueError: Observations annot. `obs` must have number of rows of `X` (1), but has 2638 rows. >>> data[0,:][:,0]. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1303, in __getitem__. return self._getitem_view(index). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1307, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cellxg",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332
https://github.com/scverse/scanpy/issues/332:247,modifiability,modul,module,247,"Addendum: different errors are generated depending on which axis is first sliced. The data set I'm loading is a dense matrix. ```. >>> data.X.dtype. dtype('<f4'). >>> data[:,0][0,:]. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1303, in __getitem__. return self._getitem_view(index). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1307, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 669, in __init__. self._init_as_view(X, oidx, vidx). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 694, in _init_as_view. uns_new = deepcopy(self._adata_ref._uns). File ""/usr/lib/python3.6/copy.py"", line 180, in deepcopy. y = _reconstruct(x, memo, *rv). File ""/usr/lib/python3.6/copy.py"", line 307, in _reconstruct. y[key] = value. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 444, in __setitem__. _init_actual_AnnData(adata_view). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 367, in _init_actual_AnnData. adata_view._init_as_actual(adata_view.copy()). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 880, in _init_as_actual. self._check_dimensions(). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1879, in _check_dimensions. .format(self._n_obs, self._obs.shape[0])). ValueError: Observations annot. `obs` must have number of rows of `X` (1), but has 2638 rows. >>> data[0,:][:,0]. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1303, in __getitem__. return self._getitem_view(index). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1307, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cellxg",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332
https://github.com/scverse/scanpy/issues/332:297,modifiability,pac,packages,297,"Addendum: different errors are generated depending on which axis is first sliced. The data set I'm loading is a dense matrix. ```. >>> data.X.dtype. dtype('<f4'). >>> data[:,0][0,:]. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1303, in __getitem__. return self._getitem_view(index). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1307, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 669, in __init__. self._init_as_view(X, oidx, vidx). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 694, in _init_as_view. uns_new = deepcopy(self._adata_ref._uns). File ""/usr/lib/python3.6/copy.py"", line 180, in deepcopy. y = _reconstruct(x, memo, *rv). File ""/usr/lib/python3.6/copy.py"", line 307, in _reconstruct. y[key] = value. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 444, in __setitem__. _init_actual_AnnData(adata_view). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 367, in _init_actual_AnnData. adata_view._init_as_actual(adata_view.copy()). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 880, in _init_as_actual. self._check_dimensions(). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1879, in _check_dimensions. .format(self._n_obs, self._obs.shape[0])). ValueError: Observations annot. `obs` must have number of rows of `X` (1), but has 2638 rows. >>> data[0,:][:,0]. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1303, in __getitem__. return self._getitem_view(index). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1307, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cellxg",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332
https://github.com/scverse/scanpy/issues/332:426,modifiability,pac,packages,426,"Addendum: different errors are generated depending on which axis is first sliced. The data set I'm loading is a dense matrix. ```. >>> data.X.dtype. dtype('<f4'). >>> data[:,0][0,:]. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1303, in __getitem__. return self._getitem_view(index). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1307, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 669, in __init__. self._init_as_view(X, oidx, vidx). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 694, in _init_as_view. uns_new = deepcopy(self._adata_ref._uns). File ""/usr/lib/python3.6/copy.py"", line 180, in deepcopy. y = _reconstruct(x, memo, *rv). File ""/usr/lib/python3.6/copy.py"", line 307, in _reconstruct. y[key] = value. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 444, in __setitem__. _init_actual_AnnData(adata_view). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 367, in _init_actual_AnnData. adata_view._init_as_actual(adata_view.copy()). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 880, in _init_as_actual. self._check_dimensions(). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1879, in _check_dimensions. .format(self._n_obs, self._obs.shape[0])). ValueError: Observations annot. `obs` must have number of rows of `X` (1), but has 2638 rows. >>> data[0,:][:,0]. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1303, in __getitem__. return self._getitem_view(index). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1307, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cellxg",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332
https://github.com/scverse/scanpy/issues/332:580,modifiability,pac,packages,580,"Addendum: different errors are generated depending on which axis is first sliced. The data set I'm loading is a dense matrix. ```. >>> data.X.dtype. dtype('<f4'). >>> data[:,0][0,:]. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1303, in __getitem__. return self._getitem_view(index). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1307, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 669, in __init__. self._init_as_view(X, oidx, vidx). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 694, in _init_as_view. uns_new = deepcopy(self._adata_ref._uns). File ""/usr/lib/python3.6/copy.py"", line 180, in deepcopy. y = _reconstruct(x, memo, *rv). File ""/usr/lib/python3.6/copy.py"", line 307, in _reconstruct. y[key] = value. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 444, in __setitem__. _init_actual_AnnData(adata_view). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 367, in _init_actual_AnnData. adata_view._init_as_actual(adata_view.copy()). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 880, in _init_as_actual. self._check_dimensions(). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1879, in _check_dimensions. .format(self._n_obs, self._obs.shape[0])). ValueError: Observations annot. `obs` must have number of rows of `X` (1), but has 2638 rows. >>> data[0,:][:,0]. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1303, in __getitem__. return self._getitem_view(index). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1307, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cellxg",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332
https://github.com/scverse/scanpy/issues/332:706,modifiability,pac,packages,706,"Addendum: different errors are generated depending on which axis is first sliced. The data set I'm loading is a dense matrix. ```. >>> data.X.dtype. dtype('<f4'). >>> data[:,0][0,:]. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1303, in __getitem__. return self._getitem_view(index). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1307, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 669, in __init__. self._init_as_view(X, oidx, vidx). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 694, in _init_as_view. uns_new = deepcopy(self._adata_ref._uns). File ""/usr/lib/python3.6/copy.py"", line 180, in deepcopy. y = _reconstruct(x, memo, *rv). File ""/usr/lib/python3.6/copy.py"", line 307, in _reconstruct. y[key] = value. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 444, in __setitem__. _init_actual_AnnData(adata_view). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 367, in _init_actual_AnnData. adata_view._init_as_actual(adata_view.copy()). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 880, in _init_as_actual. self._check_dimensions(). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1879, in _check_dimensions. .format(self._n_obs, self._obs.shape[0])). ValueError: Observations annot. `obs` must have number of rows of `X` (1), but has 2638 rows. >>> data[0,:][:,0]. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1303, in __getitem__. return self._getitem_view(index). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1307, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cellxg",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332
https://github.com/scverse/scanpy/issues/332:1012,modifiability,pac,packages,1012,"ent errors are generated depending on which axis is first sliced. The data set I'm loading is a dense matrix. ```. >>> data.X.dtype. dtype('<f4'). >>> data[:,0][0,:]. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1303, in __getitem__. return self._getitem_view(index). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1307, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 669, in __init__. self._init_as_view(X, oidx, vidx). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 694, in _init_as_view. uns_new = deepcopy(self._adata_ref._uns). File ""/usr/lib/python3.6/copy.py"", line 180, in deepcopy. y = _reconstruct(x, memo, *rv). File ""/usr/lib/python3.6/copy.py"", line 307, in _reconstruct. y[key] = value. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 444, in __setitem__. _init_actual_AnnData(adata_view). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 367, in _init_actual_AnnData. adata_view._init_as_actual(adata_view.copy()). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 880, in _init_as_actual. self._check_dimensions(). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1879, in _check_dimensions. .format(self._n_obs, self._obs.shape[0])). ValueError: Observations annot. `obs` must have number of rows of `X` (1), but has 2638 rows. >>> data[0,:][:,0]. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1303, in __getitem__. return self._getitem_view(index). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1307, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cellxgene/venv/lib/pyt",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332
https://github.com/scverse/scanpy/issues/332:1140,modifiability,pac,packages,1140,"ype. dtype('<f4'). >>> data[:,0][0,:]. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1303, in __getitem__. return self._getitem_view(index). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1307, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 669, in __init__. self._init_as_view(X, oidx, vidx). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 694, in _init_as_view. uns_new = deepcopy(self._adata_ref._uns). File ""/usr/lib/python3.6/copy.py"", line 180, in deepcopy. y = _reconstruct(x, memo, *rv). File ""/usr/lib/python3.6/copy.py"", line 307, in _reconstruct. y[key] = value. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 444, in __setitem__. _init_actual_AnnData(adata_view). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 367, in _init_actual_AnnData. adata_view._init_as_actual(adata_view.copy()). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 880, in _init_as_actual. self._check_dimensions(). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1879, in _check_dimensions. .format(self._n_obs, self._obs.shape[0])). ValueError: Observations annot. `obs` must have number of rows of `X` (1), but has 2638 rows. >>> data[0,:][:,0]. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1303, in __getitem__. return self._getitem_view(index). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1307, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 669, in __init__. self._init_as_view(X, oidx, vidx). File ""/cellxgene/venv/lib/pytho",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332
https://github.com/scverse/scanpy/issues/332:1290,modifiability,pac,packages,1290,"te-packages/anndata/base.py"", line 1303, in __getitem__. return self._getitem_view(index). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1307, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 669, in __init__. self._init_as_view(X, oidx, vidx). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 694, in _init_as_view. uns_new = deepcopy(self._adata_ref._uns). File ""/usr/lib/python3.6/copy.py"", line 180, in deepcopy. y = _reconstruct(x, memo, *rv). File ""/usr/lib/python3.6/copy.py"", line 307, in _reconstruct. y[key] = value. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 444, in __setitem__. _init_actual_AnnData(adata_view). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 367, in _init_actual_AnnData. adata_view._init_as_actual(adata_view.copy()). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 880, in _init_as_actual. self._check_dimensions(). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1879, in _check_dimensions. .format(self._n_obs, self._obs.shape[0])). ValueError: Observations annot. `obs` must have number of rows of `X` (1), but has 2638 rows. >>> data[0,:][:,0]. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1303, in __getitem__. return self._getitem_view(index). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1307, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 669, in __init__. self._init_as_view(X, oidx, vidx). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 726, in _init_as_view. self._init_X_as_view(). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/bas",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332
https://github.com/scverse/scanpy/issues/332:1414,modifiability,pac,packages,1414,".6/site-packages/anndata/base.py"", line 1307, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 669, in __init__. self._init_as_view(X, oidx, vidx). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 694, in _init_as_view. uns_new = deepcopy(self._adata_ref._uns). File ""/usr/lib/python3.6/copy.py"", line 180, in deepcopy. y = _reconstruct(x, memo, *rv). File ""/usr/lib/python3.6/copy.py"", line 307, in _reconstruct. y[key] = value. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 444, in __setitem__. _init_actual_AnnData(adata_view). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 367, in _init_actual_AnnData. adata_view._init_as_actual(adata_view.copy()). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 880, in _init_as_actual. self._check_dimensions(). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1879, in _check_dimensions. .format(self._n_obs, self._obs.shape[0])). ValueError: Observations annot. `obs` must have number of rows of `X` (1), but has 2638 rows. >>> data[0,:][:,0]. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1303, in __getitem__. return self._getitem_view(index). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1307, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 669, in __init__. self._init_as_view(X, oidx, vidx). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 726, in _init_as_view. self._init_X_as_view(). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 741, in _init_X_as_view. X = self._adata_ref.X[self._oidx, self._vidx]. IndexError: too many indices for array. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332
https://github.com/scverse/scanpy/issues/332:1695,modifiability,modul,module,1695,"site-packages/anndata/base.py"", line 1307, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 669, in __init__. self._init_as_view(X, oidx, vidx). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 694, in _init_as_view. uns_new = deepcopy(self._adata_ref._uns). File ""/usr/lib/python3.6/copy.py"", line 180, in deepcopy. y = _reconstruct(x, memo, *rv). File ""/usr/lib/python3.6/copy.py"", line 307, in _reconstruct. y[key] = value. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 444, in __setitem__. _init_actual_AnnData(adata_view). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 367, in _init_actual_AnnData. adata_view._init_as_actual(adata_view.copy()). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 880, in _init_as_actual. self._check_dimensions(). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1879, in _check_dimensions. .format(self._n_obs, self._obs.shape[0])). ValueError: Observations annot. `obs` must have number of rows of `X` (1), but has 2638 rows. >>> data[0,:][:,0]. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1303, in __getitem__. return self._getitem_view(index). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1307, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 669, in __init__. self._init_as_view(X, oidx, vidx). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 726, in _init_as_view. self._init_X_as_view(). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 741, in _init_X_as_view. X = self._adata_ref.X[self._oidx, self._vidx]. IndexError: too many indices for array. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332
https://github.com/scverse/scanpy/issues/332:1745,modifiability,pac,packages,1745,"site-packages/anndata/base.py"", line 1307, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 669, in __init__. self._init_as_view(X, oidx, vidx). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 694, in _init_as_view. uns_new = deepcopy(self._adata_ref._uns). File ""/usr/lib/python3.6/copy.py"", line 180, in deepcopy. y = _reconstruct(x, memo, *rv). File ""/usr/lib/python3.6/copy.py"", line 307, in _reconstruct. y[key] = value. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 444, in __setitem__. _init_actual_AnnData(adata_view). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 367, in _init_actual_AnnData. adata_view._init_as_actual(adata_view.copy()). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 880, in _init_as_actual. self._check_dimensions(). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1879, in _check_dimensions. .format(self._n_obs, self._obs.shape[0])). ValueError: Observations annot. `obs` must have number of rows of `X` (1), but has 2638 rows. >>> data[0,:][:,0]. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1303, in __getitem__. return self._getitem_view(index). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1307, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 669, in __init__. self._init_as_view(X, oidx, vidx). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 726, in _init_as_view. self._init_X_as_view(). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 741, in _init_X_as_view. X = self._adata_ref.X[self._oidx, self._vidx]. IndexError: too many indices for array. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332
https://github.com/scverse/scanpy/issues/332:1874,modifiability,pac,packages,1874,"site-packages/anndata/base.py"", line 1307, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 669, in __init__. self._init_as_view(X, oidx, vidx). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 694, in _init_as_view. uns_new = deepcopy(self._adata_ref._uns). File ""/usr/lib/python3.6/copy.py"", line 180, in deepcopy. y = _reconstruct(x, memo, *rv). File ""/usr/lib/python3.6/copy.py"", line 307, in _reconstruct. y[key] = value. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 444, in __setitem__. _init_actual_AnnData(adata_view). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 367, in _init_actual_AnnData. adata_view._init_as_actual(adata_view.copy()). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 880, in _init_as_actual. self._check_dimensions(). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1879, in _check_dimensions. .format(self._n_obs, self._obs.shape[0])). ValueError: Observations annot. `obs` must have number of rows of `X` (1), but has 2638 rows. >>> data[0,:][:,0]. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1303, in __getitem__. return self._getitem_view(index). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1307, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 669, in __init__. self._init_as_view(X, oidx, vidx). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 726, in _init_as_view. self._init_X_as_view(). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 741, in _init_X_as_view. X = self._adata_ref.X[self._oidx, self._vidx]. IndexError: too many indices for array. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332
https://github.com/scverse/scanpy/issues/332:2028,modifiability,pac,packages,2028,"site-packages/anndata/base.py"", line 1307, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 669, in __init__. self._init_as_view(X, oidx, vidx). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 694, in _init_as_view. uns_new = deepcopy(self._adata_ref._uns). File ""/usr/lib/python3.6/copy.py"", line 180, in deepcopy. y = _reconstruct(x, memo, *rv). File ""/usr/lib/python3.6/copy.py"", line 307, in _reconstruct. y[key] = value. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 444, in __setitem__. _init_actual_AnnData(adata_view). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 367, in _init_actual_AnnData. adata_view._init_as_actual(adata_view.copy()). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 880, in _init_as_actual. self._check_dimensions(). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1879, in _check_dimensions. .format(self._n_obs, self._obs.shape[0])). ValueError: Observations annot. `obs` must have number of rows of `X` (1), but has 2638 rows. >>> data[0,:][:,0]. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1303, in __getitem__. return self._getitem_view(index). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1307, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 669, in __init__. self._init_as_view(X, oidx, vidx). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 726, in _init_as_view. self._init_X_as_view(). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 741, in _init_X_as_view. X = self._adata_ref.X[self._oidx, self._vidx]. IndexError: too many indices for array. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332
https://github.com/scverse/scanpy/issues/332:2154,modifiability,pac,packages,2154,"site-packages/anndata/base.py"", line 1307, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 669, in __init__. self._init_as_view(X, oidx, vidx). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 694, in _init_as_view. uns_new = deepcopy(self._adata_ref._uns). File ""/usr/lib/python3.6/copy.py"", line 180, in deepcopy. y = _reconstruct(x, memo, *rv). File ""/usr/lib/python3.6/copy.py"", line 307, in _reconstruct. y[key] = value. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 444, in __setitem__. _init_actual_AnnData(adata_view). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 367, in _init_actual_AnnData. adata_view._init_as_actual(adata_view.copy()). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 880, in _init_as_actual. self._check_dimensions(). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1879, in _check_dimensions. .format(self._n_obs, self._obs.shape[0])). ValueError: Observations annot. `obs` must have number of rows of `X` (1), but has 2638 rows. >>> data[0,:][:,0]. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1303, in __getitem__. return self._getitem_view(index). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1307, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 669, in __init__. self._init_as_view(X, oidx, vidx). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 726, in _init_as_view. self._init_X_as_view(). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 741, in _init_X_as_view. X = self._adata_ref.X[self._oidx, self._vidx]. IndexError: too many indices for array. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332
https://github.com/scverse/scanpy/issues/332:2274,modifiability,pac,packages,2274,"site-packages/anndata/base.py"", line 1307, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 669, in __init__. self._init_as_view(X, oidx, vidx). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 694, in _init_as_view. uns_new = deepcopy(self._adata_ref._uns). File ""/usr/lib/python3.6/copy.py"", line 180, in deepcopy. y = _reconstruct(x, memo, *rv). File ""/usr/lib/python3.6/copy.py"", line 307, in _reconstruct. y[key] = value. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 444, in __setitem__. _init_actual_AnnData(adata_view). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 367, in _init_actual_AnnData. adata_view._init_as_actual(adata_view.copy()). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 880, in _init_as_actual. self._check_dimensions(). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1879, in _check_dimensions. .format(self._n_obs, self._obs.shape[0])). ValueError: Observations annot. `obs` must have number of rows of `X` (1), but has 2638 rows. >>> data[0,:][:,0]. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1303, in __getitem__. return self._getitem_view(index). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1307, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 669, in __init__. self._init_as_view(X, oidx, vidx). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 726, in _init_as_view. self._init_X_as_view(). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 741, in _init_X_as_view. X = self._adata_ref.X[self._oidx, self._vidx]. IndexError: too many indices for array. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332
https://github.com/scverse/scanpy/issues/332:20,performance,error,errors,20,"Addendum: different errors are generated depending on which axis is first sliced. The data set I'm loading is a dense matrix. ```. >>> data.X.dtype. dtype('<f4'). >>> data[:,0][0,:]. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1303, in __getitem__. return self._getitem_view(index). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1307, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 669, in __init__. self._init_as_view(X, oidx, vidx). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 694, in _init_as_view. uns_new = deepcopy(self._adata_ref._uns). File ""/usr/lib/python3.6/copy.py"", line 180, in deepcopy. y = _reconstruct(x, memo, *rv). File ""/usr/lib/python3.6/copy.py"", line 307, in _reconstruct. y[key] = value. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 444, in __setitem__. _init_actual_AnnData(adata_view). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 367, in _init_actual_AnnData. adata_view._init_as_actual(adata_view.copy()). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 880, in _init_as_actual. self._check_dimensions(). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1879, in _check_dimensions. .format(self._n_obs, self._obs.shape[0])). ValueError: Observations annot. `obs` must have number of rows of `X` (1), but has 2638 rows. >>> data[0,:][:,0]. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1303, in __getitem__. return self._getitem_view(index). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1307, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cellxg",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332
https://github.com/scverse/scanpy/issues/332:99,performance,load,loading,99,"Addendum: different errors are generated depending on which axis is first sliced. The data set I'm loading is a dense matrix. ```. >>> data.X.dtype. dtype('<f4'). >>> data[:,0][0,:]. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1303, in __getitem__. return self._getitem_view(index). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1307, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 669, in __init__. self._init_as_view(X, oidx, vidx). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 694, in _init_as_view. uns_new = deepcopy(self._adata_ref._uns). File ""/usr/lib/python3.6/copy.py"", line 180, in deepcopy. y = _reconstruct(x, memo, *rv). File ""/usr/lib/python3.6/copy.py"", line 307, in _reconstruct. y[key] = value. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 444, in __setitem__. _init_actual_AnnData(adata_view). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 367, in _init_actual_AnnData. adata_view._init_as_actual(adata_view.copy()). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 880, in _init_as_actual. self._check_dimensions(). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1879, in _check_dimensions. .format(self._n_obs, self._obs.shape[0])). ValueError: Observations annot. `obs` must have number of rows of `X` (1), but has 2638 rows. >>> data[0,:][:,0]. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1303, in __getitem__. return self._getitem_view(index). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1307, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cellxg",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332
https://github.com/scverse/scanpy/issues/332:74,reliability,sli,sliced,74,"Addendum: different errors are generated depending on which axis is first sliced. The data set I'm loading is a dense matrix. ```. >>> data.X.dtype. dtype('<f4'). >>> data[:,0][0,:]. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1303, in __getitem__. return self._getitem_view(index). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1307, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 669, in __init__. self._init_as_view(X, oidx, vidx). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 694, in _init_as_view. uns_new = deepcopy(self._adata_ref._uns). File ""/usr/lib/python3.6/copy.py"", line 180, in deepcopy. y = _reconstruct(x, memo, *rv). File ""/usr/lib/python3.6/copy.py"", line 307, in _reconstruct. y[key] = value. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 444, in __setitem__. _init_actual_AnnData(adata_view). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 367, in _init_actual_AnnData. adata_view._init_as_actual(adata_view.copy()). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 880, in _init_as_actual. self._check_dimensions(). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1879, in _check_dimensions. .format(self._n_obs, self._obs.shape[0])). ValueError: Observations annot. `obs` must have number of rows of `X` (1), but has 2638 rows. >>> data[0,:][:,0]. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1303, in __getitem__. return self._getitem_view(index). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1307, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cellxg",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332
https://github.com/scverse/scanpy/issues/332:20,safety,error,errors,20,"Addendum: different errors are generated depending on which axis is first sliced. The data set I'm loading is a dense matrix. ```. >>> data.X.dtype. dtype('<f4'). >>> data[:,0][0,:]. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1303, in __getitem__. return self._getitem_view(index). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1307, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 669, in __init__. self._init_as_view(X, oidx, vidx). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 694, in _init_as_view. uns_new = deepcopy(self._adata_ref._uns). File ""/usr/lib/python3.6/copy.py"", line 180, in deepcopy. y = _reconstruct(x, memo, *rv). File ""/usr/lib/python3.6/copy.py"", line 307, in _reconstruct. y[key] = value. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 444, in __setitem__. _init_actual_AnnData(adata_view). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 367, in _init_actual_AnnData. adata_view._init_as_actual(adata_view.copy()). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 880, in _init_as_actual. self._check_dimensions(). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1879, in _check_dimensions. .format(self._n_obs, self._obs.shape[0])). ValueError: Observations annot. `obs` must have number of rows of `X` (1), but has 2638 rows. >>> data[0,:][:,0]. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1303, in __getitem__. return self._getitem_view(index). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1307, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cellxg",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332
https://github.com/scverse/scanpy/issues/332:41,safety,depend,depending,41,"Addendum: different errors are generated depending on which axis is first sliced. The data set I'm loading is a dense matrix. ```. >>> data.X.dtype. dtype('<f4'). >>> data[:,0][0,:]. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1303, in __getitem__. return self._getitem_view(index). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1307, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 669, in __init__. self._init_as_view(X, oidx, vidx). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 694, in _init_as_view. uns_new = deepcopy(self._adata_ref._uns). File ""/usr/lib/python3.6/copy.py"", line 180, in deepcopy. y = _reconstruct(x, memo, *rv). File ""/usr/lib/python3.6/copy.py"", line 307, in _reconstruct. y[key] = value. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 444, in __setitem__. _init_actual_AnnData(adata_view). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 367, in _init_actual_AnnData. adata_view._init_as_actual(adata_view.copy()). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 880, in _init_as_actual. self._check_dimensions(). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1879, in _check_dimensions. .format(self._n_obs, self._obs.shape[0])). ValueError: Observations annot. `obs` must have number of rows of `X` (1), but has 2638 rows. >>> data[0,:][:,0]. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1303, in __getitem__. return self._getitem_view(index). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1307, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cellxg",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332
https://github.com/scverse/scanpy/issues/332:247,safety,modul,module,247,"Addendum: different errors are generated depending on which axis is first sliced. The data set I'm loading is a dense matrix. ```. >>> data.X.dtype. dtype('<f4'). >>> data[:,0][0,:]. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1303, in __getitem__. return self._getitem_view(index). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1307, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 669, in __init__. self._init_as_view(X, oidx, vidx). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 694, in _init_as_view. uns_new = deepcopy(self._adata_ref._uns). File ""/usr/lib/python3.6/copy.py"", line 180, in deepcopy. y = _reconstruct(x, memo, *rv). File ""/usr/lib/python3.6/copy.py"", line 307, in _reconstruct. y[key] = value. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 444, in __setitem__. _init_actual_AnnData(adata_view). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 367, in _init_actual_AnnData. adata_view._init_as_actual(adata_view.copy()). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 880, in _init_as_actual. self._check_dimensions(). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1879, in _check_dimensions. .format(self._n_obs, self._obs.shape[0])). ValueError: Observations annot. `obs` must have number of rows of `X` (1), but has 2638 rows. >>> data[0,:][:,0]. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1303, in __getitem__. return self._getitem_view(index). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1307, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cellxg",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332
https://github.com/scverse/scanpy/issues/332:1695,safety,modul,module,1695,"site-packages/anndata/base.py"", line 1307, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 669, in __init__. self._init_as_view(X, oidx, vidx). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 694, in _init_as_view. uns_new = deepcopy(self._adata_ref._uns). File ""/usr/lib/python3.6/copy.py"", line 180, in deepcopy. y = _reconstruct(x, memo, *rv). File ""/usr/lib/python3.6/copy.py"", line 307, in _reconstruct. y[key] = value. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 444, in __setitem__. _init_actual_AnnData(adata_view). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 367, in _init_actual_AnnData. adata_view._init_as_actual(adata_view.copy()). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 880, in _init_as_actual. self._check_dimensions(). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1879, in _check_dimensions. .format(self._n_obs, self._obs.shape[0])). ValueError: Observations annot. `obs` must have number of rows of `X` (1), but has 2638 rows. >>> data[0,:][:,0]. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1303, in __getitem__. return self._getitem_view(index). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1307, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 669, in __init__. self._init_as_view(X, oidx, vidx). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 726, in _init_as_view. self._init_X_as_view(). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 741, in _init_X_as_view. X = self._adata_ref.X[self._oidx, self._vidx]. IndexError: too many indices for array. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332
https://github.com/scverse/scanpy/issues/332:41,testability,depend,depending,41,"Addendum: different errors are generated depending on which axis is first sliced. The data set I'm loading is a dense matrix. ```. >>> data.X.dtype. dtype('<f4'). >>> data[:,0][0,:]. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1303, in __getitem__. return self._getitem_view(index). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1307, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 669, in __init__. self._init_as_view(X, oidx, vidx). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 694, in _init_as_view. uns_new = deepcopy(self._adata_ref._uns). File ""/usr/lib/python3.6/copy.py"", line 180, in deepcopy. y = _reconstruct(x, memo, *rv). File ""/usr/lib/python3.6/copy.py"", line 307, in _reconstruct. y[key] = value. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 444, in __setitem__. _init_actual_AnnData(adata_view). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 367, in _init_actual_AnnData. adata_view._init_as_actual(adata_view.copy()). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 880, in _init_as_actual. self._check_dimensions(). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1879, in _check_dimensions. .format(self._n_obs, self._obs.shape[0])). ValueError: Observations annot. `obs` must have number of rows of `X` (1), but has 2638 rows. >>> data[0,:][:,0]. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1303, in __getitem__. return self._getitem_view(index). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1307, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cellxg",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332
https://github.com/scverse/scanpy/issues/332:183,testability,Trace,Traceback,183,"Addendum: different errors are generated depending on which axis is first sliced. The data set I'm loading is a dense matrix. ```. >>> data.X.dtype. dtype('<f4'). >>> data[:,0][0,:]. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1303, in __getitem__. return self._getitem_view(index). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1307, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 669, in __init__. self._init_as_view(X, oidx, vidx). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 694, in _init_as_view. uns_new = deepcopy(self._adata_ref._uns). File ""/usr/lib/python3.6/copy.py"", line 180, in deepcopy. y = _reconstruct(x, memo, *rv). File ""/usr/lib/python3.6/copy.py"", line 307, in _reconstruct. y[key] = value. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 444, in __setitem__. _init_actual_AnnData(adata_view). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 367, in _init_actual_AnnData. adata_view._init_as_actual(adata_view.copy()). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 880, in _init_as_actual. self._check_dimensions(). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1879, in _check_dimensions. .format(self._n_obs, self._obs.shape[0])). ValueError: Observations annot. `obs` must have number of rows of `X` (1), but has 2638 rows. >>> data[0,:][:,0]. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1303, in __getitem__. return self._getitem_view(index). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1307, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cellxg",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332
https://github.com/scverse/scanpy/issues/332:1529,testability,Observ,Observations,1529,"site-packages/anndata/base.py"", line 1307, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 669, in __init__. self._init_as_view(X, oidx, vidx). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 694, in _init_as_view. uns_new = deepcopy(self._adata_ref._uns). File ""/usr/lib/python3.6/copy.py"", line 180, in deepcopy. y = _reconstruct(x, memo, *rv). File ""/usr/lib/python3.6/copy.py"", line 307, in _reconstruct. y[key] = value. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 444, in __setitem__. _init_actual_AnnData(adata_view). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 367, in _init_actual_AnnData. adata_view._init_as_actual(adata_view.copy()). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 880, in _init_as_actual. self._check_dimensions(). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1879, in _check_dimensions. .format(self._n_obs, self._obs.shape[0])). ValueError: Observations annot. `obs` must have number of rows of `X` (1), but has 2638 rows. >>> data[0,:][:,0]. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1303, in __getitem__. return self._getitem_view(index). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1307, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 669, in __init__. self._init_as_view(X, oidx, vidx). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 726, in _init_as_view. self._init_X_as_view(). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 741, in _init_X_as_view. X = self._adata_ref.X[self._oidx, self._vidx]. IndexError: too many indices for array. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332
https://github.com/scverse/scanpy/issues/332:1631,testability,Trace,Traceback,1631,"site-packages/anndata/base.py"", line 1307, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 669, in __init__. self._init_as_view(X, oidx, vidx). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 694, in _init_as_view. uns_new = deepcopy(self._adata_ref._uns). File ""/usr/lib/python3.6/copy.py"", line 180, in deepcopy. y = _reconstruct(x, memo, *rv). File ""/usr/lib/python3.6/copy.py"", line 307, in _reconstruct. y[key] = value. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 444, in __setitem__. _init_actual_AnnData(adata_view). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 367, in _init_actual_AnnData. adata_view._init_as_actual(adata_view.copy()). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 880, in _init_as_actual. self._check_dimensions(). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1879, in _check_dimensions. .format(self._n_obs, self._obs.shape[0])). ValueError: Observations annot. `obs` must have number of rows of `X` (1), but has 2638 rows. >>> data[0,:][:,0]. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1303, in __getitem__. return self._getitem_view(index). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1307, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 669, in __init__. self._init_as_view(X, oidx, vidx). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 726, in _init_as_view. self._init_X_as_view(). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 741, in _init_X_as_view. X = self._adata_ref.X[self._oidx, self._vidx]. IndexError: too many indices for array. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332
https://github.com/scverse/scanpy/issues/332:20,usability,error,errors,20,"Addendum: different errors are generated depending on which axis is first sliced. The data set I'm loading is a dense matrix. ```. >>> data.X.dtype. dtype('<f4'). >>> data[:,0][0,:]. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1303, in __getitem__. return self._getitem_view(index). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1307, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 669, in __init__. self._init_as_view(X, oidx, vidx). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 694, in _init_as_view. uns_new = deepcopy(self._adata_ref._uns). File ""/usr/lib/python3.6/copy.py"", line 180, in deepcopy. y = _reconstruct(x, memo, *rv). File ""/usr/lib/python3.6/copy.py"", line 307, in _reconstruct. y[key] = value. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 444, in __setitem__. _init_actual_AnnData(adata_view). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 367, in _init_actual_AnnData. adata_view._init_as_actual(adata_view.copy()). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 880, in _init_as_actual. self._check_dimensions(). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1879, in _check_dimensions. .format(self._n_obs, self._obs.shape[0])). ValueError: Observations annot. `obs` must have number of rows of `X` (1), but has 2638 rows. >>> data[0,:][:,0]. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1303, in __getitem__. return self._getitem_view(index). File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1307, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cellxg",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332
https://github.com/scverse/scanpy/issues/332:56,availability,sli,slicing,56,"More comprehensive test, similar to the current AnnData slicing tests (no use of external data). Each try/except block will (currently) fail on its last assertion. ```. import sys, traceback. import numpy as np. import scanpy.api as sc. adata = sc.AnnData(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])). # integer indexing. print(""\n>>> integer indexing, obs first""). try:. assert adata[0:2, :][:, 0:2].X.tolist() == [[1,2], [4,5]]. assert adata[0, 0].X.tolist() == 1. assert adata[0:1, 0:1].X.tolist() == 1. assert adata[0, :][:, 0].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). print(""\n>>> integer indexing, vars first""). try:. assert adata[:, 0:2][0:2, :].X.tolist() == [[1,2], [4,5]]. assert adata[0, 0].X.tolist() == 1. assert adata[0:1, 0:1].X.tolist() == 1. assert adata[:, 0][0, :].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). # boolean indexing. print(""\n>>> boolean indexing, obs first""). try:. obs_selector = np.zeros(len(adata.obs), dtype=bool). vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]. vars_selector[:] = [True, True, False]. assert adata[obs_selector, :][:, vars_selector].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]. vars_selector[:] = [True, False, False]. assert adata[obs_selector, :][:, vars_selector].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). print(""\n>>> boolean indexing, vars first""). try:. obs_selector = np.zeros(len(adata.obs), dtype=bool). vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]. vars_selector[:] = [True, True, False]. assert adata[:, vars_selector][obs_selector, :].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]. vars_selector[:] = [True, False, False]. assert adata[:, vars_selector][obs_selector, :].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332
https://github.com/scverse/scanpy/issues/332:136,deployability,fail,fail,136,"More comprehensive test, similar to the current AnnData slicing tests (no use of external data). Each try/except block will (currently) fail on its last assertion. ```. import sys, traceback. import numpy as np. import scanpy.api as sc. adata = sc.AnnData(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])). # integer indexing. print(""\n>>> integer indexing, obs first""). try:. assert adata[0:2, :][:, 0:2].X.tolist() == [[1,2], [4,5]]. assert adata[0, 0].X.tolist() == 1. assert adata[0:1, 0:1].X.tolist() == 1. assert adata[0, :][:, 0].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). print(""\n>>> integer indexing, vars first""). try:. assert adata[:, 0:2][0:2, :].X.tolist() == [[1,2], [4,5]]. assert adata[0, 0].X.tolist() == 1. assert adata[0:1, 0:1].X.tolist() == 1. assert adata[:, 0][0, :].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). # boolean indexing. print(""\n>>> boolean indexing, obs first""). try:. obs_selector = np.zeros(len(adata.obs), dtype=bool). vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]. vars_selector[:] = [True, True, False]. assert adata[obs_selector, :][:, vars_selector].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]. vars_selector[:] = [True, False, False]. assert adata[obs_selector, :][:, vars_selector].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). print(""\n>>> boolean indexing, vars first""). try:. obs_selector = np.zeros(len(adata.obs), dtype=bool). vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]. vars_selector[:] = [True, True, False]. assert adata[:, vars_selector][obs_selector, :].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]. vars_selector[:] = [True, False, False]. assert adata[:, vars_selector][obs_selector, :].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332
https://github.com/scverse/scanpy/issues/332:226,deployability,api,api,226,"More comprehensive test, similar to the current AnnData slicing tests (no use of external data). Each try/except block will (currently) fail on its last assertion. ```. import sys, traceback. import numpy as np. import scanpy.api as sc. adata = sc.AnnData(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])). # integer indexing. print(""\n>>> integer indexing, obs first""). try:. assert adata[0:2, :][:, 0:2].X.tolist() == [[1,2], [4,5]]. assert adata[0, 0].X.tolist() == 1. assert adata[0:1, 0:1].X.tolist() == 1. assert adata[0, :][:, 0].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). print(""\n>>> integer indexing, vars first""). try:. assert adata[:, 0:2][0:2, :].X.tolist() == [[1,2], [4,5]]. assert adata[0, 0].X.tolist() == 1. assert adata[0:1, 0:1].X.tolist() == 1. assert adata[:, 0][0, :].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). # boolean indexing. print(""\n>>> boolean indexing, obs first""). try:. obs_selector = np.zeros(len(adata.obs), dtype=bool). vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]. vars_selector[:] = [True, True, False]. assert adata[obs_selector, :][:, vars_selector].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]. vars_selector[:] = [True, False, False]. assert adata[obs_selector, :][:, vars_selector].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). print(""\n>>> boolean indexing, vars first""). try:. obs_selector = np.zeros(len(adata.obs), dtype=bool). vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]. vars_selector[:] = [True, True, False]. assert adata[:, vars_selector][obs_selector, :].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]. vars_selector[:] = [True, False, False]. assert adata[:, vars_selector][obs_selector, :].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332
https://github.com/scverse/scanpy/issues/332:40,energy efficiency,current,current,40,"More comprehensive test, similar to the current AnnData slicing tests (no use of external data). Each try/except block will (currently) fail on its last assertion. ```. import sys, traceback. import numpy as np. import scanpy.api as sc. adata = sc.AnnData(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])). # integer indexing. print(""\n>>> integer indexing, obs first""). try:. assert adata[0:2, :][:, 0:2].X.tolist() == [[1,2], [4,5]]. assert adata[0, 0].X.tolist() == 1. assert adata[0:1, 0:1].X.tolist() == 1. assert adata[0, :][:, 0].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). print(""\n>>> integer indexing, vars first""). try:. assert adata[:, 0:2][0:2, :].X.tolist() == [[1,2], [4,5]]. assert adata[0, 0].X.tolist() == 1. assert adata[0:1, 0:1].X.tolist() == 1. assert adata[:, 0][0, :].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). # boolean indexing. print(""\n>>> boolean indexing, obs first""). try:. obs_selector = np.zeros(len(adata.obs), dtype=bool). vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]. vars_selector[:] = [True, True, False]. assert adata[obs_selector, :][:, vars_selector].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]. vars_selector[:] = [True, False, False]. assert adata[obs_selector, :][:, vars_selector].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). print(""\n>>> boolean indexing, vars first""). try:. obs_selector = np.zeros(len(adata.obs), dtype=bool). vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]. vars_selector[:] = [True, True, False]. assert adata[:, vars_selector][obs_selector, :].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]. vars_selector[:] = [True, False, False]. assert adata[:, vars_selector][obs_selector, :].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332
https://github.com/scverse/scanpy/issues/332:125,energy efficiency,current,currently,125,"More comprehensive test, similar to the current AnnData slicing tests (no use of external data). Each try/except block will (currently) fail on its last assertion. ```. import sys, traceback. import numpy as np. import scanpy.api as sc. adata = sc.AnnData(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])). # integer indexing. print(""\n>>> integer indexing, obs first""). try:. assert adata[0:2, :][:, 0:2].X.tolist() == [[1,2], [4,5]]. assert adata[0, 0].X.tolist() == 1. assert adata[0:1, 0:1].X.tolist() == 1. assert adata[0, :][:, 0].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). print(""\n>>> integer indexing, vars first""). try:. assert adata[:, 0:2][0:2, :].X.tolist() == [[1,2], [4,5]]. assert adata[0, 0].X.tolist() == 1. assert adata[0:1, 0:1].X.tolist() == 1. assert adata[:, 0][0, :].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). # boolean indexing. print(""\n>>> boolean indexing, obs first""). try:. obs_selector = np.zeros(len(adata.obs), dtype=bool). vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]. vars_selector[:] = [True, True, False]. assert adata[obs_selector, :][:, vars_selector].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]. vars_selector[:] = [True, False, False]. assert adata[obs_selector, :][:, vars_selector].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). print(""\n>>> boolean indexing, vars first""). try:. obs_selector = np.zeros(len(adata.obs), dtype=bool). vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]. vars_selector[:] = [True, True, False]. assert adata[:, vars_selector][obs_selector, :].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]. vars_selector[:] = [True, False, False]. assert adata[:, vars_selector][obs_selector, :].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332
https://github.com/scverse/scanpy/issues/332:226,integrability,api,api,226,"More comprehensive test, similar to the current AnnData slicing tests (no use of external data). Each try/except block will (currently) fail on its last assertion. ```. import sys, traceback. import numpy as np. import scanpy.api as sc. adata = sc.AnnData(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])). # integer indexing. print(""\n>>> integer indexing, obs first""). try:. assert adata[0:2, :][:, 0:2].X.tolist() == [[1,2], [4,5]]. assert adata[0, 0].X.tolist() == 1. assert adata[0:1, 0:1].X.tolist() == 1. assert adata[0, :][:, 0].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). print(""\n>>> integer indexing, vars first""). try:. assert adata[:, 0:2][0:2, :].X.tolist() == [[1,2], [4,5]]. assert adata[0, 0].X.tolist() == 1. assert adata[0:1, 0:1].X.tolist() == 1. assert adata[:, 0][0, :].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). # boolean indexing. print(""\n>>> boolean indexing, obs first""). try:. obs_selector = np.zeros(len(adata.obs), dtype=bool). vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]. vars_selector[:] = [True, True, False]. assert adata[obs_selector, :][:, vars_selector].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]. vars_selector[:] = [True, False, False]. assert adata[obs_selector, :][:, vars_selector].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). print(""\n>>> boolean indexing, vars first""). try:. obs_selector = np.zeros(len(adata.obs), dtype=bool). vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]. vars_selector[:] = [True, True, False]. assert adata[:, vars_selector][obs_selector, :].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]. vars_selector[:] = [True, False, False]. assert adata[:, vars_selector][obs_selector, :].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332
https://github.com/scverse/scanpy/issues/332:226,interoperability,api,api,226,"More comprehensive test, similar to the current AnnData slicing tests (no use of external data). Each try/except block will (currently) fail on its last assertion. ```. import sys, traceback. import numpy as np. import scanpy.api as sc. adata = sc.AnnData(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])). # integer indexing. print(""\n>>> integer indexing, obs first""). try:. assert adata[0:2, :][:, 0:2].X.tolist() == [[1,2], [4,5]]. assert adata[0, 0].X.tolist() == 1. assert adata[0:1, 0:1].X.tolist() == 1. assert adata[0, :][:, 0].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). print(""\n>>> integer indexing, vars first""). try:. assert adata[:, 0:2][0:2, :].X.tolist() == [[1,2], [4,5]]. assert adata[0, 0].X.tolist() == 1. assert adata[0:1, 0:1].X.tolist() == 1. assert adata[:, 0][0, :].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). # boolean indexing. print(""\n>>> boolean indexing, obs first""). try:. obs_selector = np.zeros(len(adata.obs), dtype=bool). vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]. vars_selector[:] = [True, True, False]. assert adata[obs_selector, :][:, vars_selector].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]. vars_selector[:] = [True, False, False]. assert adata[obs_selector, :][:, vars_selector].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). print(""\n>>> boolean indexing, vars first""). try:. obs_selector = np.zeros(len(adata.obs), dtype=bool). vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]. vars_selector[:] = [True, True, False]. assert adata[:, vars_selector][obs_selector, :].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]. vars_selector[:] = [True, False, False]. assert adata[:, vars_selector][obs_selector, :].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332
https://github.com/scverse/scanpy/issues/332:56,reliability,sli,slicing,56,"More comprehensive test, similar to the current AnnData slicing tests (no use of external data). Each try/except block will (currently) fail on its last assertion. ```. import sys, traceback. import numpy as np. import scanpy.api as sc. adata = sc.AnnData(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])). # integer indexing. print(""\n>>> integer indexing, obs first""). try:. assert adata[0:2, :][:, 0:2].X.tolist() == [[1,2], [4,5]]. assert adata[0, 0].X.tolist() == 1. assert adata[0:1, 0:1].X.tolist() == 1. assert adata[0, :][:, 0].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). print(""\n>>> integer indexing, vars first""). try:. assert adata[:, 0:2][0:2, :].X.tolist() == [[1,2], [4,5]]. assert adata[0, 0].X.tolist() == 1. assert adata[0:1, 0:1].X.tolist() == 1. assert adata[:, 0][0, :].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). # boolean indexing. print(""\n>>> boolean indexing, obs first""). try:. obs_selector = np.zeros(len(adata.obs), dtype=bool). vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]. vars_selector[:] = [True, True, False]. assert adata[obs_selector, :][:, vars_selector].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]. vars_selector[:] = [True, False, False]. assert adata[obs_selector, :][:, vars_selector].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). print(""\n>>> boolean indexing, vars first""). try:. obs_selector = np.zeros(len(adata.obs), dtype=bool). vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]. vars_selector[:] = [True, True, False]. assert adata[:, vars_selector][obs_selector, :].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]. vars_selector[:] = [True, False, False]. assert adata[:, vars_selector][obs_selector, :].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332
https://github.com/scverse/scanpy/issues/332:136,reliability,fail,fail,136,"More comprehensive test, similar to the current AnnData slicing tests (no use of external data). Each try/except block will (currently) fail on its last assertion. ```. import sys, traceback. import numpy as np. import scanpy.api as sc. adata = sc.AnnData(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])). # integer indexing. print(""\n>>> integer indexing, obs first""). try:. assert adata[0:2, :][:, 0:2].X.tolist() == [[1,2], [4,5]]. assert adata[0, 0].X.tolist() == 1. assert adata[0:1, 0:1].X.tolist() == 1. assert adata[0, :][:, 0].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). print(""\n>>> integer indexing, vars first""). try:. assert adata[:, 0:2][0:2, :].X.tolist() == [[1,2], [4,5]]. assert adata[0, 0].X.tolist() == 1. assert adata[0:1, 0:1].X.tolist() == 1. assert adata[:, 0][0, :].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). # boolean indexing. print(""\n>>> boolean indexing, obs first""). try:. obs_selector = np.zeros(len(adata.obs), dtype=bool). vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]. vars_selector[:] = [True, True, False]. assert adata[obs_selector, :][:, vars_selector].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]. vars_selector[:] = [True, False, False]. assert adata[obs_selector, :][:, vars_selector].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). print(""\n>>> boolean indexing, vars first""). try:. obs_selector = np.zeros(len(adata.obs), dtype=bool). vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]. vars_selector[:] = [True, True, False]. assert adata[:, vars_selector][obs_selector, :].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]. vars_selector[:] = [True, False, False]. assert adata[:, vars_selector][obs_selector, :].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332
https://github.com/scverse/scanpy/issues/332:19,safety,test,test,19,"More comprehensive test, similar to the current AnnData slicing tests (no use of external data). Each try/except block will (currently) fail on its last assertion. ```. import sys, traceback. import numpy as np. import scanpy.api as sc. adata = sc.AnnData(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])). # integer indexing. print(""\n>>> integer indexing, obs first""). try:. assert adata[0:2, :][:, 0:2].X.tolist() == [[1,2], [4,5]]. assert adata[0, 0].X.tolist() == 1. assert adata[0:1, 0:1].X.tolist() == 1. assert adata[0, :][:, 0].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). print(""\n>>> integer indexing, vars first""). try:. assert adata[:, 0:2][0:2, :].X.tolist() == [[1,2], [4,5]]. assert adata[0, 0].X.tolist() == 1. assert adata[0:1, 0:1].X.tolist() == 1. assert adata[:, 0][0, :].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). # boolean indexing. print(""\n>>> boolean indexing, obs first""). try:. obs_selector = np.zeros(len(adata.obs), dtype=bool). vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]. vars_selector[:] = [True, True, False]. assert adata[obs_selector, :][:, vars_selector].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]. vars_selector[:] = [True, False, False]. assert adata[obs_selector, :][:, vars_selector].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). print(""\n>>> boolean indexing, vars first""). try:. obs_selector = np.zeros(len(adata.obs), dtype=bool). vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]. vars_selector[:] = [True, True, False]. assert adata[:, vars_selector][obs_selector, :].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]. vars_selector[:] = [True, False, False]. assert adata[:, vars_selector][obs_selector, :].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332
https://github.com/scverse/scanpy/issues/332:64,safety,test,tests,64,"More comprehensive test, similar to the current AnnData slicing tests (no use of external data). Each try/except block will (currently) fail on its last assertion. ```. import sys, traceback. import numpy as np. import scanpy.api as sc. adata = sc.AnnData(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])). # integer indexing. print(""\n>>> integer indexing, obs first""). try:. assert adata[0:2, :][:, 0:2].X.tolist() == [[1,2], [4,5]]. assert adata[0, 0].X.tolist() == 1. assert adata[0:1, 0:1].X.tolist() == 1. assert adata[0, :][:, 0].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). print(""\n>>> integer indexing, vars first""). try:. assert adata[:, 0:2][0:2, :].X.tolist() == [[1,2], [4,5]]. assert adata[0, 0].X.tolist() == 1. assert adata[0:1, 0:1].X.tolist() == 1. assert adata[:, 0][0, :].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). # boolean indexing. print(""\n>>> boolean indexing, obs first""). try:. obs_selector = np.zeros(len(adata.obs), dtype=bool). vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]. vars_selector[:] = [True, True, False]. assert adata[obs_selector, :][:, vars_selector].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]. vars_selector[:] = [True, False, False]. assert adata[obs_selector, :][:, vars_selector].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). print(""\n>>> boolean indexing, vars first""). try:. obs_selector = np.zeros(len(adata.obs), dtype=bool). vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]. vars_selector[:] = [True, True, False]. assert adata[:, vars_selector][obs_selector, :].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]. vars_selector[:] = [True, False, False]. assert adata[:, vars_selector][obs_selector, :].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332
https://github.com/scverse/scanpy/issues/332:106,safety,except,except,106,"More comprehensive test, similar to the current AnnData slicing tests (no use of external data). Each try/except block will (currently) fail on its last assertion. ```. import sys, traceback. import numpy as np. import scanpy.api as sc. adata = sc.AnnData(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])). # integer indexing. print(""\n>>> integer indexing, obs first""). try:. assert adata[0:2, :][:, 0:2].X.tolist() == [[1,2], [4,5]]. assert adata[0, 0].X.tolist() == 1. assert adata[0:1, 0:1].X.tolist() == 1. assert adata[0, :][:, 0].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). print(""\n>>> integer indexing, vars first""). try:. assert adata[:, 0:2][0:2, :].X.tolist() == [[1,2], [4,5]]. assert adata[0, 0].X.tolist() == 1. assert adata[0:1, 0:1].X.tolist() == 1. assert adata[:, 0][0, :].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). # boolean indexing. print(""\n>>> boolean indexing, obs first""). try:. obs_selector = np.zeros(len(adata.obs), dtype=bool). vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]. vars_selector[:] = [True, True, False]. assert adata[obs_selector, :][:, vars_selector].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]. vars_selector[:] = [True, False, False]. assert adata[obs_selector, :][:, vars_selector].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). print(""\n>>> boolean indexing, vars first""). try:. obs_selector = np.zeros(len(adata.obs), dtype=bool). vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]. vars_selector[:] = [True, True, False]. assert adata[:, vars_selector][obs_selector, :].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]. vars_selector[:] = [True, False, False]. assert adata[:, vars_selector][obs_selector, :].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332
https://github.com/scverse/scanpy/issues/332:549,safety,except,except,549,"More comprehensive test, similar to the current AnnData slicing tests (no use of external data). Each try/except block will (currently) fail on its last assertion. ```. import sys, traceback. import numpy as np. import scanpy.api as sc. adata = sc.AnnData(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])). # integer indexing. print(""\n>>> integer indexing, obs first""). try:. assert adata[0:2, :][:, 0:2].X.tolist() == [[1,2], [4,5]]. assert adata[0, 0].X.tolist() == 1. assert adata[0:1, 0:1].X.tolist() == 1. assert adata[0, :][:, 0].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). print(""\n>>> integer indexing, vars first""). try:. assert adata[:, 0:2][0:2, :].X.tolist() == [[1,2], [4,5]]. assert adata[0, 0].X.tolist() == 1. assert adata[0:1, 0:1].X.tolist() == 1. assert adata[:, 0][0, :].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). # boolean indexing. print(""\n>>> boolean indexing, obs first""). try:. obs_selector = np.zeros(len(adata.obs), dtype=bool). vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]. vars_selector[:] = [True, True, False]. assert adata[obs_selector, :][:, vars_selector].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]. vars_selector[:] = [True, False, False]. assert adata[obs_selector, :][:, vars_selector].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). print(""\n>>> boolean indexing, vars first""). try:. obs_selector = np.zeros(len(adata.obs), dtype=bool). vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]. vars_selector[:] = [True, True, False]. assert adata[:, vars_selector][obs_selector, :].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]. vars_selector[:] = [True, False, False]. assert adata[:, vars_selector][obs_selector, :].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332
https://github.com/scverse/scanpy/issues/332:556,safety,Except,Exception,556,"More comprehensive test, similar to the current AnnData slicing tests (no use of external data). Each try/except block will (currently) fail on its last assertion. ```. import sys, traceback. import numpy as np. import scanpy.api as sc. adata = sc.AnnData(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])). # integer indexing. print(""\n>>> integer indexing, obs first""). try:. assert adata[0:2, :][:, 0:2].X.tolist() == [[1,2], [4,5]]. assert adata[0, 0].X.tolist() == 1. assert adata[0:1, 0:1].X.tolist() == 1. assert adata[0, :][:, 0].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). print(""\n>>> integer indexing, vars first""). try:. assert adata[:, 0:2][0:2, :].X.tolist() == [[1,2], [4,5]]. assert adata[0, 0].X.tolist() == 1. assert adata[0:1, 0:1].X.tolist() == 1. assert adata[:, 0][0, :].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). # boolean indexing. print(""\n>>> boolean indexing, obs first""). try:. obs_selector = np.zeros(len(adata.obs), dtype=bool). vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]. vars_selector[:] = [True, True, False]. assert adata[obs_selector, :][:, vars_selector].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]. vars_selector[:] = [True, False, False]. assert adata[obs_selector, :][:, vars_selector].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). print(""\n>>> boolean indexing, vars first""). try:. obs_selector = np.zeros(len(adata.obs), dtype=bool). vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]. vars_selector[:] = [True, True, False]. assert adata[:, vars_selector][obs_selector, :].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]. vars_selector[:] = [True, False, False]. assert adata[:, vars_selector][obs_selector, :].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332
https://github.com/scverse/scanpy/issues/332:839,safety,except,except,839,"More comprehensive test, similar to the current AnnData slicing tests (no use of external data). Each try/except block will (currently) fail on its last assertion. ```. import sys, traceback. import numpy as np. import scanpy.api as sc. adata = sc.AnnData(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])). # integer indexing. print(""\n>>> integer indexing, obs first""). try:. assert adata[0:2, :][:, 0:2].X.tolist() == [[1,2], [4,5]]. assert adata[0, 0].X.tolist() == 1. assert adata[0:1, 0:1].X.tolist() == 1. assert adata[0, :][:, 0].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). print(""\n>>> integer indexing, vars first""). try:. assert adata[:, 0:2][0:2, :].X.tolist() == [[1,2], [4,5]]. assert adata[0, 0].X.tolist() == 1. assert adata[0:1, 0:1].X.tolist() == 1. assert adata[:, 0][0, :].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). # boolean indexing. print(""\n>>> boolean indexing, obs first""). try:. obs_selector = np.zeros(len(adata.obs), dtype=bool). vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]. vars_selector[:] = [True, True, False]. assert adata[obs_selector, :][:, vars_selector].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]. vars_selector[:] = [True, False, False]. assert adata[obs_selector, :][:, vars_selector].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). print(""\n>>> boolean indexing, vars first""). try:. obs_selector = np.zeros(len(adata.obs), dtype=bool). vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]. vars_selector[:] = [True, True, False]. assert adata[:, vars_selector][obs_selector, :].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]. vars_selector[:] = [True, False, False]. assert adata[:, vars_selector][obs_selector, :].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332
https://github.com/scverse/scanpy/issues/332:846,safety,Except,Exception,846,"More comprehensive test, similar to the current AnnData slicing tests (no use of external data). Each try/except block will (currently) fail on its last assertion. ```. import sys, traceback. import numpy as np. import scanpy.api as sc. adata = sc.AnnData(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])). # integer indexing. print(""\n>>> integer indexing, obs first""). try:. assert adata[0:2, :][:, 0:2].X.tolist() == [[1,2], [4,5]]. assert adata[0, 0].X.tolist() == 1. assert adata[0:1, 0:1].X.tolist() == 1. assert adata[0, :][:, 0].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). print(""\n>>> integer indexing, vars first""). try:. assert adata[:, 0:2][0:2, :].X.tolist() == [[1,2], [4,5]]. assert adata[0, 0].X.tolist() == 1. assert adata[0:1, 0:1].X.tolist() == 1. assert adata[:, 0][0, :].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). # boolean indexing. print(""\n>>> boolean indexing, obs first""). try:. obs_selector = np.zeros(len(adata.obs), dtype=bool). vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]. vars_selector[:] = [True, True, False]. assert adata[obs_selector, :][:, vars_selector].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]. vars_selector[:] = [True, False, False]. assert adata[obs_selector, :][:, vars_selector].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). print(""\n>>> boolean indexing, vars first""). try:. obs_selector = np.zeros(len(adata.obs), dtype=bool). vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]. vars_selector[:] = [True, True, False]. assert adata[:, vars_selector][obs_selector, :].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]. vars_selector[:] = [True, False, False]. assert adata[:, vars_selector][obs_selector, :].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332
https://github.com/scverse/scanpy/issues/332:1381,safety,except,except,1381,"More comprehensive test, similar to the current AnnData slicing tests (no use of external data). Each try/except block will (currently) fail on its last assertion. ```. import sys, traceback. import numpy as np. import scanpy.api as sc. adata = sc.AnnData(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])). # integer indexing. print(""\n>>> integer indexing, obs first""). try:. assert adata[0:2, :][:, 0:2].X.tolist() == [[1,2], [4,5]]. assert adata[0, 0].X.tolist() == 1. assert adata[0:1, 0:1].X.tolist() == 1. assert adata[0, :][:, 0].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). print(""\n>>> integer indexing, vars first""). try:. assert adata[:, 0:2][0:2, :].X.tolist() == [[1,2], [4,5]]. assert adata[0, 0].X.tolist() == 1. assert adata[0:1, 0:1].X.tolist() == 1. assert adata[:, 0][0, :].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). # boolean indexing. print(""\n>>> boolean indexing, obs first""). try:. obs_selector = np.zeros(len(adata.obs), dtype=bool). vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]. vars_selector[:] = [True, True, False]. assert adata[obs_selector, :][:, vars_selector].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]. vars_selector[:] = [True, False, False]. assert adata[obs_selector, :][:, vars_selector].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). print(""\n>>> boolean indexing, vars first""). try:. obs_selector = np.zeros(len(adata.obs), dtype=bool). vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]. vars_selector[:] = [True, True, False]. assert adata[:, vars_selector][obs_selector, :].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]. vars_selector[:] = [True, False, False]. assert adata[:, vars_selector][obs_selector, :].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332
https://github.com/scverse/scanpy/issues/332:1388,safety,Except,Exception,1388,"More comprehensive test, similar to the current AnnData slicing tests (no use of external data). Each try/except block will (currently) fail on its last assertion. ```. import sys, traceback. import numpy as np. import scanpy.api as sc. adata = sc.AnnData(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])). # integer indexing. print(""\n>>> integer indexing, obs first""). try:. assert adata[0:2, :][:, 0:2].X.tolist() == [[1,2], [4,5]]. assert adata[0, 0].X.tolist() == 1. assert adata[0:1, 0:1].X.tolist() == 1. assert adata[0, :][:, 0].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). print(""\n>>> integer indexing, vars first""). try:. assert adata[:, 0:2][0:2, :].X.tolist() == [[1,2], [4,5]]. assert adata[0, 0].X.tolist() == 1. assert adata[0:1, 0:1].X.tolist() == 1. assert adata[:, 0][0, :].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). # boolean indexing. print(""\n>>> boolean indexing, obs first""). try:. obs_selector = np.zeros(len(adata.obs), dtype=bool). vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]. vars_selector[:] = [True, True, False]. assert adata[obs_selector, :][:, vars_selector].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]. vars_selector[:] = [True, False, False]. assert adata[obs_selector, :][:, vars_selector].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). print(""\n>>> boolean indexing, vars first""). try:. obs_selector = np.zeros(len(adata.obs), dtype=bool). vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]. vars_selector[:] = [True, True, False]. assert adata[:, vars_selector][obs_selector, :].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]. vars_selector[:] = [True, False, False]. assert adata[:, vars_selector][obs_selector, :].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332
https://github.com/scverse/scanpy/issues/332:1904,safety,except,except,1904,"More comprehensive test, similar to the current AnnData slicing tests (no use of external data). Each try/except block will (currently) fail on its last assertion. ```. import sys, traceback. import numpy as np. import scanpy.api as sc. adata = sc.AnnData(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])). # integer indexing. print(""\n>>> integer indexing, obs first""). try:. assert adata[0:2, :][:, 0:2].X.tolist() == [[1,2], [4,5]]. assert adata[0, 0].X.tolist() == 1. assert adata[0:1, 0:1].X.tolist() == 1. assert adata[0, :][:, 0].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). print(""\n>>> integer indexing, vars first""). try:. assert adata[:, 0:2][0:2, :].X.tolist() == [[1,2], [4,5]]. assert adata[0, 0].X.tolist() == 1. assert adata[0:1, 0:1].X.tolist() == 1. assert adata[:, 0][0, :].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). # boolean indexing. print(""\n>>> boolean indexing, obs first""). try:. obs_selector = np.zeros(len(adata.obs), dtype=bool). vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]. vars_selector[:] = [True, True, False]. assert adata[obs_selector, :][:, vars_selector].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]. vars_selector[:] = [True, False, False]. assert adata[obs_selector, :][:, vars_selector].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). print(""\n>>> boolean indexing, vars first""). try:. obs_selector = np.zeros(len(adata.obs), dtype=bool). vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]. vars_selector[:] = [True, True, False]. assert adata[:, vars_selector][obs_selector, :].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]. vars_selector[:] = [True, False, False]. assert adata[:, vars_selector][obs_selector, :].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332
https://github.com/scverse/scanpy/issues/332:1911,safety,Except,Exception,1911,"More comprehensive test, similar to the current AnnData slicing tests (no use of external data). Each try/except block will (currently) fail on its last assertion. ```. import sys, traceback. import numpy as np. import scanpy.api as sc. adata = sc.AnnData(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])). # integer indexing. print(""\n>>> integer indexing, obs first""). try:. assert adata[0:2, :][:, 0:2].X.tolist() == [[1,2], [4,5]]. assert adata[0, 0].X.tolist() == 1. assert adata[0:1, 0:1].X.tolist() == 1. assert adata[0, :][:, 0].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). print(""\n>>> integer indexing, vars first""). try:. assert adata[:, 0:2][0:2, :].X.tolist() == [[1,2], [4,5]]. assert adata[0, 0].X.tolist() == 1. assert adata[0:1, 0:1].X.tolist() == 1. assert adata[:, 0][0, :].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). # boolean indexing. print(""\n>>> boolean indexing, obs first""). try:. obs_selector = np.zeros(len(adata.obs), dtype=bool). vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]. vars_selector[:] = [True, True, False]. assert adata[obs_selector, :][:, vars_selector].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]. vars_selector[:] = [True, False, False]. assert adata[obs_selector, :][:, vars_selector].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). print(""\n>>> boolean indexing, vars first""). try:. obs_selector = np.zeros(len(adata.obs), dtype=bool). vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]. vars_selector[:] = [True, True, False]. assert adata[:, vars_selector][obs_selector, :].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]. vars_selector[:] = [True, False, False]. assert adata[:, vars_selector][obs_selector, :].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332
https://github.com/scverse/scanpy/issues/332:19,testability,test,test,19,"More comprehensive test, similar to the current AnnData slicing tests (no use of external data). Each try/except block will (currently) fail on its last assertion. ```. import sys, traceback. import numpy as np. import scanpy.api as sc. adata = sc.AnnData(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])). # integer indexing. print(""\n>>> integer indexing, obs first""). try:. assert adata[0:2, :][:, 0:2].X.tolist() == [[1,2], [4,5]]. assert adata[0, 0].X.tolist() == 1. assert adata[0:1, 0:1].X.tolist() == 1. assert adata[0, :][:, 0].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). print(""\n>>> integer indexing, vars first""). try:. assert adata[:, 0:2][0:2, :].X.tolist() == [[1,2], [4,5]]. assert adata[0, 0].X.tolist() == 1. assert adata[0:1, 0:1].X.tolist() == 1. assert adata[:, 0][0, :].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). # boolean indexing. print(""\n>>> boolean indexing, obs first""). try:. obs_selector = np.zeros(len(adata.obs), dtype=bool). vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]. vars_selector[:] = [True, True, False]. assert adata[obs_selector, :][:, vars_selector].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]. vars_selector[:] = [True, False, False]. assert adata[obs_selector, :][:, vars_selector].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). print(""\n>>> boolean indexing, vars first""). try:. obs_selector = np.zeros(len(adata.obs), dtype=bool). vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]. vars_selector[:] = [True, True, False]. assert adata[:, vars_selector][obs_selector, :].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]. vars_selector[:] = [True, False, False]. assert adata[:, vars_selector][obs_selector, :].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332
https://github.com/scverse/scanpy/issues/332:64,testability,test,tests,64,"More comprehensive test, similar to the current AnnData slicing tests (no use of external data). Each try/except block will (currently) fail on its last assertion. ```. import sys, traceback. import numpy as np. import scanpy.api as sc. adata = sc.AnnData(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])). # integer indexing. print(""\n>>> integer indexing, obs first""). try:. assert adata[0:2, :][:, 0:2].X.tolist() == [[1,2], [4,5]]. assert adata[0, 0].X.tolist() == 1. assert adata[0:1, 0:1].X.tolist() == 1. assert adata[0, :][:, 0].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). print(""\n>>> integer indexing, vars first""). try:. assert adata[:, 0:2][0:2, :].X.tolist() == [[1,2], [4,5]]. assert adata[0, 0].X.tolist() == 1. assert adata[0:1, 0:1].X.tolist() == 1. assert adata[:, 0][0, :].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). # boolean indexing. print(""\n>>> boolean indexing, obs first""). try:. obs_selector = np.zeros(len(adata.obs), dtype=bool). vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]. vars_selector[:] = [True, True, False]. assert adata[obs_selector, :][:, vars_selector].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]. vars_selector[:] = [True, False, False]. assert adata[obs_selector, :][:, vars_selector].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). print(""\n>>> boolean indexing, vars first""). try:. obs_selector = np.zeros(len(adata.obs), dtype=bool). vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]. vars_selector[:] = [True, True, False]. assert adata[:, vars_selector][obs_selector, :].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]. vars_selector[:] = [True, False, False]. assert adata[:, vars_selector][obs_selector, :].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332
https://github.com/scverse/scanpy/issues/332:153,testability,assert,assertion,153,"More comprehensive test, similar to the current AnnData slicing tests (no use of external data). Each try/except block will (currently) fail on its last assertion. ```. import sys, traceback. import numpy as np. import scanpy.api as sc. adata = sc.AnnData(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])). # integer indexing. print(""\n>>> integer indexing, obs first""). try:. assert adata[0:2, :][:, 0:2].X.tolist() == [[1,2], [4,5]]. assert adata[0, 0].X.tolist() == 1. assert adata[0:1, 0:1].X.tolist() == 1. assert adata[0, :][:, 0].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). print(""\n>>> integer indexing, vars first""). try:. assert adata[:, 0:2][0:2, :].X.tolist() == [[1,2], [4,5]]. assert adata[0, 0].X.tolist() == 1. assert adata[0:1, 0:1].X.tolist() == 1. assert adata[:, 0][0, :].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). # boolean indexing. print(""\n>>> boolean indexing, obs first""). try:. obs_selector = np.zeros(len(adata.obs), dtype=bool). vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]. vars_selector[:] = [True, True, False]. assert adata[obs_selector, :][:, vars_selector].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]. vars_selector[:] = [True, False, False]. assert adata[obs_selector, :][:, vars_selector].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). print(""\n>>> boolean indexing, vars first""). try:. obs_selector = np.zeros(len(adata.obs), dtype=bool). vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]. vars_selector[:] = [True, True, False]. assert adata[:, vars_selector][obs_selector, :].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]. vars_selector[:] = [True, False, False]. assert adata[:, vars_selector][obs_selector, :].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332
https://github.com/scverse/scanpy/issues/332:181,testability,trace,traceback,181,"More comprehensive test, similar to the current AnnData slicing tests (no use of external data). Each try/except block will (currently) fail on its last assertion. ```. import sys, traceback. import numpy as np. import scanpy.api as sc. adata = sc.AnnData(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])). # integer indexing. print(""\n>>> integer indexing, obs first""). try:. assert adata[0:2, :][:, 0:2].X.tolist() == [[1,2], [4,5]]. assert adata[0, 0].X.tolist() == 1. assert adata[0:1, 0:1].X.tolist() == 1. assert adata[0, :][:, 0].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). print(""\n>>> integer indexing, vars first""). try:. assert adata[:, 0:2][0:2, :].X.tolist() == [[1,2], [4,5]]. assert adata[0, 0].X.tolist() == 1. assert adata[0:1, 0:1].X.tolist() == 1. assert adata[:, 0][0, :].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). # boolean indexing. print(""\n>>> boolean indexing, obs first""). try:. obs_selector = np.zeros(len(adata.obs), dtype=bool). vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]. vars_selector[:] = [True, True, False]. assert adata[obs_selector, :][:, vars_selector].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]. vars_selector[:] = [True, False, False]. assert adata[obs_selector, :][:, vars_selector].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). print(""\n>>> boolean indexing, vars first""). try:. obs_selector = np.zeros(len(adata.obs), dtype=bool). vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]. vars_selector[:] = [True, True, False]. assert adata[:, vars_selector][obs_selector, :].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]. vars_selector[:] = [True, False, False]. assert adata[:, vars_selector][obs_selector, :].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332
https://github.com/scverse/scanpy/issues/332:372,testability,assert,assert,372,"More comprehensive test, similar to the current AnnData slicing tests (no use of external data). Each try/except block will (currently) fail on its last assertion. ```. import sys, traceback. import numpy as np. import scanpy.api as sc. adata = sc.AnnData(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])). # integer indexing. print(""\n>>> integer indexing, obs first""). try:. assert adata[0:2, :][:, 0:2].X.tolist() == [[1,2], [4,5]]. assert adata[0, 0].X.tolist() == 1. assert adata[0:1, 0:1].X.tolist() == 1. assert adata[0, :][:, 0].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). print(""\n>>> integer indexing, vars first""). try:. assert adata[:, 0:2][0:2, :].X.tolist() == [[1,2], [4,5]]. assert adata[0, 0].X.tolist() == 1. assert adata[0:1, 0:1].X.tolist() == 1. assert adata[:, 0][0, :].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). # boolean indexing. print(""\n>>> boolean indexing, obs first""). try:. obs_selector = np.zeros(len(adata.obs), dtype=bool). vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]. vars_selector[:] = [True, True, False]. assert adata[obs_selector, :][:, vars_selector].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]. vars_selector[:] = [True, False, False]. assert adata[obs_selector, :][:, vars_selector].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). print(""\n>>> boolean indexing, vars first""). try:. obs_selector = np.zeros(len(adata.obs), dtype=bool). vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]. vars_selector[:] = [True, True, False]. assert adata[:, vars_selector][obs_selector, :].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]. vars_selector[:] = [True, False, False]. assert adata[:, vars_selector][obs_selector, :].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332
https://github.com/scverse/scanpy/issues/332:431,testability,assert,assert,431,"More comprehensive test, similar to the current AnnData slicing tests (no use of external data). Each try/except block will (currently) fail on its last assertion. ```. import sys, traceback. import numpy as np. import scanpy.api as sc. adata = sc.AnnData(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])). # integer indexing. print(""\n>>> integer indexing, obs first""). try:. assert adata[0:2, :][:, 0:2].X.tolist() == [[1,2], [4,5]]. assert adata[0, 0].X.tolist() == 1. assert adata[0:1, 0:1].X.tolist() == 1. assert adata[0, :][:, 0].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). print(""\n>>> integer indexing, vars first""). try:. assert adata[:, 0:2][0:2, :].X.tolist() == [[1,2], [4,5]]. assert adata[0, 0].X.tolist() == 1. assert adata[0:1, 0:1].X.tolist() == 1. assert adata[:, 0][0, :].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). # boolean indexing. print(""\n>>> boolean indexing, obs first""). try:. obs_selector = np.zeros(len(adata.obs), dtype=bool). vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]. vars_selector[:] = [True, True, False]. assert adata[obs_selector, :][:, vars_selector].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]. vars_selector[:] = [True, False, False]. assert adata[obs_selector, :][:, vars_selector].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). print(""\n>>> boolean indexing, vars first""). try:. obs_selector = np.zeros(len(adata.obs), dtype=bool). vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]. vars_selector[:] = [True, True, False]. assert adata[:, vars_selector][obs_selector, :].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]. vars_selector[:] = [True, False, False]. assert adata[:, vars_selector][obs_selector, :].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332
https://github.com/scverse/scanpy/issues/332:467,testability,assert,assert,467,"More comprehensive test, similar to the current AnnData slicing tests (no use of external data). Each try/except block will (currently) fail on its last assertion. ```. import sys, traceback. import numpy as np. import scanpy.api as sc. adata = sc.AnnData(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])). # integer indexing. print(""\n>>> integer indexing, obs first""). try:. assert adata[0:2, :][:, 0:2].X.tolist() == [[1,2], [4,5]]. assert adata[0, 0].X.tolist() == 1. assert adata[0:1, 0:1].X.tolist() == 1. assert adata[0, :][:, 0].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). print(""\n>>> integer indexing, vars first""). try:. assert adata[:, 0:2][0:2, :].X.tolist() == [[1,2], [4,5]]. assert adata[0, 0].X.tolist() == 1. assert adata[0:1, 0:1].X.tolist() == 1. assert adata[:, 0][0, :].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). # boolean indexing. print(""\n>>> boolean indexing, obs first""). try:. obs_selector = np.zeros(len(adata.obs), dtype=bool). vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]. vars_selector[:] = [True, True, False]. assert adata[obs_selector, :][:, vars_selector].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]. vars_selector[:] = [True, False, False]. assert adata[obs_selector, :][:, vars_selector].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). print(""\n>>> boolean indexing, vars first""). try:. obs_selector = np.zeros(len(adata.obs), dtype=bool). vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]. vars_selector[:] = [True, True, False]. assert adata[:, vars_selector][obs_selector, :].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]. vars_selector[:] = [True, False, False]. assert adata[:, vars_selector][obs_selector, :].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332
https://github.com/scverse/scanpy/issues/332:507,testability,assert,assert,507,"More comprehensive test, similar to the current AnnData slicing tests (no use of external data). Each try/except block will (currently) fail on its last assertion. ```. import sys, traceback. import numpy as np. import scanpy.api as sc. adata = sc.AnnData(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])). # integer indexing. print(""\n>>> integer indexing, obs first""). try:. assert adata[0:2, :][:, 0:2].X.tolist() == [[1,2], [4,5]]. assert adata[0, 0].X.tolist() == 1. assert adata[0:1, 0:1].X.tolist() == 1. assert adata[0, :][:, 0].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). print(""\n>>> integer indexing, vars first""). try:. assert adata[:, 0:2][0:2, :].X.tolist() == [[1,2], [4,5]]. assert adata[0, 0].X.tolist() == 1. assert adata[0:1, 0:1].X.tolist() == 1. assert adata[:, 0][0, :].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). # boolean indexing. print(""\n>>> boolean indexing, obs first""). try:. obs_selector = np.zeros(len(adata.obs), dtype=bool). vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]. vars_selector[:] = [True, True, False]. assert adata[obs_selector, :][:, vars_selector].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]. vars_selector[:] = [True, False, False]. assert adata[obs_selector, :][:, vars_selector].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). print(""\n>>> boolean indexing, vars first""). try:. obs_selector = np.zeros(len(adata.obs), dtype=bool). vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]. vars_selector[:] = [True, True, False]. assert adata[:, vars_selector][obs_selector, :].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]. vars_selector[:] = [True, False, False]. assert adata[:, vars_selector][obs_selector, :].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332
https://github.com/scverse/scanpy/issues/332:573,testability,trace,traceback,573,"More comprehensive test, similar to the current AnnData slicing tests (no use of external data). Each try/except block will (currently) fail on its last assertion. ```. import sys, traceback. import numpy as np. import scanpy.api as sc. adata = sc.AnnData(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])). # integer indexing. print(""\n>>> integer indexing, obs first""). try:. assert adata[0:2, :][:, 0:2].X.tolist() == [[1,2], [4,5]]. assert adata[0, 0].X.tolist() == 1. assert adata[0:1, 0:1].X.tolist() == 1. assert adata[0, :][:, 0].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). print(""\n>>> integer indexing, vars first""). try:. assert adata[:, 0:2][0:2, :].X.tolist() == [[1,2], [4,5]]. assert adata[0, 0].X.tolist() == 1. assert adata[0:1, 0:1].X.tolist() == 1. assert adata[:, 0][0, :].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). # boolean indexing. print(""\n>>> boolean indexing, obs first""). try:. obs_selector = np.zeros(len(adata.obs), dtype=bool). vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]. vars_selector[:] = [True, True, False]. assert adata[obs_selector, :][:, vars_selector].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]. vars_selector[:] = [True, False, False]. assert adata[obs_selector, :][:, vars_selector].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). print(""\n>>> boolean indexing, vars first""). try:. obs_selector = np.zeros(len(adata.obs), dtype=bool). vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]. vars_selector[:] = [True, True, False]. assert adata[:, vars_selector][obs_selector, :].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]. vars_selector[:] = [True, False, False]. assert adata[:, vars_selector][obs_selector, :].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332
https://github.com/scverse/scanpy/issues/332:662,testability,assert,assert,662,"More comprehensive test, similar to the current AnnData slicing tests (no use of external data). Each try/except block will (currently) fail on its last assertion. ```. import sys, traceback. import numpy as np. import scanpy.api as sc. adata = sc.AnnData(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])). # integer indexing. print(""\n>>> integer indexing, obs first""). try:. assert adata[0:2, :][:, 0:2].X.tolist() == [[1,2], [4,5]]. assert adata[0, 0].X.tolist() == 1. assert adata[0:1, 0:1].X.tolist() == 1. assert adata[0, :][:, 0].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). print(""\n>>> integer indexing, vars first""). try:. assert adata[:, 0:2][0:2, :].X.tolist() == [[1,2], [4,5]]. assert adata[0, 0].X.tolist() == 1. assert adata[0:1, 0:1].X.tolist() == 1. assert adata[:, 0][0, :].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). # boolean indexing. print(""\n>>> boolean indexing, obs first""). try:. obs_selector = np.zeros(len(adata.obs), dtype=bool). vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]. vars_selector[:] = [True, True, False]. assert adata[obs_selector, :][:, vars_selector].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]. vars_selector[:] = [True, False, False]. assert adata[obs_selector, :][:, vars_selector].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). print(""\n>>> boolean indexing, vars first""). try:. obs_selector = np.zeros(len(adata.obs), dtype=bool). vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]. vars_selector[:] = [True, True, False]. assert adata[:, vars_selector][obs_selector, :].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]. vars_selector[:] = [True, False, False]. assert adata[:, vars_selector][obs_selector, :].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332
https://github.com/scverse/scanpy/issues/332:721,testability,assert,assert,721,"More comprehensive test, similar to the current AnnData slicing tests (no use of external data). Each try/except block will (currently) fail on its last assertion. ```. import sys, traceback. import numpy as np. import scanpy.api as sc. adata = sc.AnnData(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])). # integer indexing. print(""\n>>> integer indexing, obs first""). try:. assert adata[0:2, :][:, 0:2].X.tolist() == [[1,2], [4,5]]. assert adata[0, 0].X.tolist() == 1. assert adata[0:1, 0:1].X.tolist() == 1. assert adata[0, :][:, 0].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). print(""\n>>> integer indexing, vars first""). try:. assert adata[:, 0:2][0:2, :].X.tolist() == [[1,2], [4,5]]. assert adata[0, 0].X.tolist() == 1. assert adata[0:1, 0:1].X.tolist() == 1. assert adata[:, 0][0, :].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). # boolean indexing. print(""\n>>> boolean indexing, obs first""). try:. obs_selector = np.zeros(len(adata.obs), dtype=bool). vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]. vars_selector[:] = [True, True, False]. assert adata[obs_selector, :][:, vars_selector].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]. vars_selector[:] = [True, False, False]. assert adata[obs_selector, :][:, vars_selector].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). print(""\n>>> boolean indexing, vars first""). try:. obs_selector = np.zeros(len(adata.obs), dtype=bool). vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]. vars_selector[:] = [True, True, False]. assert adata[:, vars_selector][obs_selector, :].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]. vars_selector[:] = [True, False, False]. assert adata[:, vars_selector][obs_selector, :].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332
https://github.com/scverse/scanpy/issues/332:757,testability,assert,assert,757,"More comprehensive test, similar to the current AnnData slicing tests (no use of external data). Each try/except block will (currently) fail on its last assertion. ```. import sys, traceback. import numpy as np. import scanpy.api as sc. adata = sc.AnnData(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])). # integer indexing. print(""\n>>> integer indexing, obs first""). try:. assert adata[0:2, :][:, 0:2].X.tolist() == [[1,2], [4,5]]. assert adata[0, 0].X.tolist() == 1. assert adata[0:1, 0:1].X.tolist() == 1. assert adata[0, :][:, 0].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). print(""\n>>> integer indexing, vars first""). try:. assert adata[:, 0:2][0:2, :].X.tolist() == [[1,2], [4,5]]. assert adata[0, 0].X.tolist() == 1. assert adata[0:1, 0:1].X.tolist() == 1. assert adata[:, 0][0, :].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). # boolean indexing. print(""\n>>> boolean indexing, obs first""). try:. obs_selector = np.zeros(len(adata.obs), dtype=bool). vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]. vars_selector[:] = [True, True, False]. assert adata[obs_selector, :][:, vars_selector].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]. vars_selector[:] = [True, False, False]. assert adata[obs_selector, :][:, vars_selector].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). print(""\n>>> boolean indexing, vars first""). try:. obs_selector = np.zeros(len(adata.obs), dtype=bool). vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]. vars_selector[:] = [True, True, False]. assert adata[:, vars_selector][obs_selector, :].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]. vars_selector[:] = [True, False, False]. assert adata[:, vars_selector][obs_selector, :].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332
https://github.com/scverse/scanpy/issues/332:797,testability,assert,assert,797,"More comprehensive test, similar to the current AnnData slicing tests (no use of external data). Each try/except block will (currently) fail on its last assertion. ```. import sys, traceback. import numpy as np. import scanpy.api as sc. adata = sc.AnnData(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])). # integer indexing. print(""\n>>> integer indexing, obs first""). try:. assert adata[0:2, :][:, 0:2].X.tolist() == [[1,2], [4,5]]. assert adata[0, 0].X.tolist() == 1. assert adata[0:1, 0:1].X.tolist() == 1. assert adata[0, :][:, 0].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). print(""\n>>> integer indexing, vars first""). try:. assert adata[:, 0:2][0:2, :].X.tolist() == [[1,2], [4,5]]. assert adata[0, 0].X.tolist() == 1. assert adata[0:1, 0:1].X.tolist() == 1. assert adata[:, 0][0, :].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). # boolean indexing. print(""\n>>> boolean indexing, obs first""). try:. obs_selector = np.zeros(len(adata.obs), dtype=bool). vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]. vars_selector[:] = [True, True, False]. assert adata[obs_selector, :][:, vars_selector].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]. vars_selector[:] = [True, False, False]. assert adata[obs_selector, :][:, vars_selector].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). print(""\n>>> boolean indexing, vars first""). try:. obs_selector = np.zeros(len(adata.obs), dtype=bool). vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]. vars_selector[:] = [True, True, False]. assert adata[:, vars_selector][obs_selector, :].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]. vars_selector[:] = [True, False, False]. assert adata[:, vars_selector][obs_selector, :].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332
https://github.com/scverse/scanpy/issues/332:863,testability,trace,traceback,863,"More comprehensive test, similar to the current AnnData slicing tests (no use of external data). Each try/except block will (currently) fail on its last assertion. ```. import sys, traceback. import numpy as np. import scanpy.api as sc. adata = sc.AnnData(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])). # integer indexing. print(""\n>>> integer indexing, obs first""). try:. assert adata[0:2, :][:, 0:2].X.tolist() == [[1,2], [4,5]]. assert adata[0, 0].X.tolist() == 1. assert adata[0:1, 0:1].X.tolist() == 1. assert adata[0, :][:, 0].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). print(""\n>>> integer indexing, vars first""). try:. assert adata[:, 0:2][0:2, :].X.tolist() == [[1,2], [4,5]]. assert adata[0, 0].X.tolist() == 1. assert adata[0:1, 0:1].X.tolist() == 1. assert adata[:, 0][0, :].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). # boolean indexing. print(""\n>>> boolean indexing, obs first""). try:. obs_selector = np.zeros(len(adata.obs), dtype=bool). vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]. vars_selector[:] = [True, True, False]. assert adata[obs_selector, :][:, vars_selector].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]. vars_selector[:] = [True, False, False]. assert adata[obs_selector, :][:, vars_selector].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). print(""\n>>> boolean indexing, vars first""). try:. obs_selector = np.zeros(len(adata.obs), dtype=bool). vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]. vars_selector[:] = [True, True, False]. assert adata[:, vars_selector][obs_selector, :].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]. vars_selector[:] = [True, False, False]. assert adata[:, vars_selector][obs_selector, :].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332
https://github.com/scverse/scanpy/issues/332:1157,testability,assert,assert,1157,"More comprehensive test, similar to the current AnnData slicing tests (no use of external data). Each try/except block will (currently) fail on its last assertion. ```. import sys, traceback. import numpy as np. import scanpy.api as sc. adata = sc.AnnData(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])). # integer indexing. print(""\n>>> integer indexing, obs first""). try:. assert adata[0:2, :][:, 0:2].X.tolist() == [[1,2], [4,5]]. assert adata[0, 0].X.tolist() == 1. assert adata[0:1, 0:1].X.tolist() == 1. assert adata[0, :][:, 0].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). print(""\n>>> integer indexing, vars first""). try:. assert adata[:, 0:2][0:2, :].X.tolist() == [[1,2], [4,5]]. assert adata[0, 0].X.tolist() == 1. assert adata[0:1, 0:1].X.tolist() == 1. assert adata[:, 0][0, :].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). # boolean indexing. print(""\n>>> boolean indexing, obs first""). try:. obs_selector = np.zeros(len(adata.obs), dtype=bool). vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]. vars_selector[:] = [True, True, False]. assert adata[obs_selector, :][:, vars_selector].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]. vars_selector[:] = [True, False, False]. assert adata[obs_selector, :][:, vars_selector].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). print(""\n>>> boolean indexing, vars first""). try:. obs_selector = np.zeros(len(adata.obs), dtype=bool). vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]. vars_selector[:] = [True, True, False]. assert adata[:, vars_selector][obs_selector, :].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]. vars_selector[:] = [True, False, False]. assert adata[:, vars_selector][obs_selector, :].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332
https://github.com/scverse/scanpy/issues/332:1316,testability,assert,assert,1316,"More comprehensive test, similar to the current AnnData slicing tests (no use of external data). Each try/except block will (currently) fail on its last assertion. ```. import sys, traceback. import numpy as np. import scanpy.api as sc. adata = sc.AnnData(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])). # integer indexing. print(""\n>>> integer indexing, obs first""). try:. assert adata[0:2, :][:, 0:2].X.tolist() == [[1,2], [4,5]]. assert adata[0, 0].X.tolist() == 1. assert adata[0:1, 0:1].X.tolist() == 1. assert adata[0, :][:, 0].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). print(""\n>>> integer indexing, vars first""). try:. assert adata[:, 0:2][0:2, :].X.tolist() == [[1,2], [4,5]]. assert adata[0, 0].X.tolist() == 1. assert adata[0:1, 0:1].X.tolist() == 1. assert adata[:, 0][0, :].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). # boolean indexing. print(""\n>>> boolean indexing, obs first""). try:. obs_selector = np.zeros(len(adata.obs), dtype=bool). vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]. vars_selector[:] = [True, True, False]. assert adata[obs_selector, :][:, vars_selector].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]. vars_selector[:] = [True, False, False]. assert adata[obs_selector, :][:, vars_selector].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). print(""\n>>> boolean indexing, vars first""). try:. obs_selector = np.zeros(len(adata.obs), dtype=bool). vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]. vars_selector[:] = [True, True, False]. assert adata[:, vars_selector][obs_selector, :].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]. vars_selector[:] = [True, False, False]. assert adata[:, vars_selector][obs_selector, :].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332
https://github.com/scverse/scanpy/issues/332:1405,testability,trace,traceback,1405,"More comprehensive test, similar to the current AnnData slicing tests (no use of external data). Each try/except block will (currently) fail on its last assertion. ```. import sys, traceback. import numpy as np. import scanpy.api as sc. adata = sc.AnnData(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])). # integer indexing. print(""\n>>> integer indexing, obs first""). try:. assert adata[0:2, :][:, 0:2].X.tolist() == [[1,2], [4,5]]. assert adata[0, 0].X.tolist() == 1. assert adata[0:1, 0:1].X.tolist() == 1. assert adata[0, :][:, 0].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). print(""\n>>> integer indexing, vars first""). try:. assert adata[:, 0:2][0:2, :].X.tolist() == [[1,2], [4,5]]. assert adata[0, 0].X.tolist() == 1. assert adata[0:1, 0:1].X.tolist() == 1. assert adata[:, 0][0, :].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). # boolean indexing. print(""\n>>> boolean indexing, obs first""). try:. obs_selector = np.zeros(len(adata.obs), dtype=bool). vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]. vars_selector[:] = [True, True, False]. assert adata[obs_selector, :][:, vars_selector].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]. vars_selector[:] = [True, False, False]. assert adata[obs_selector, :][:, vars_selector].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). print(""\n>>> boolean indexing, vars first""). try:. obs_selector = np.zeros(len(adata.obs), dtype=bool). vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]. vars_selector[:] = [True, True, False]. assert adata[:, vars_selector][obs_selector, :].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]. vars_selector[:] = [True, False, False]. assert adata[:, vars_selector][obs_selector, :].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332
https://github.com/scverse/scanpy/issues/332:1680,testability,assert,assert,1680,"More comprehensive test, similar to the current AnnData slicing tests (no use of external data). Each try/except block will (currently) fail on its last assertion. ```. import sys, traceback. import numpy as np. import scanpy.api as sc. adata = sc.AnnData(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])). # integer indexing. print(""\n>>> integer indexing, obs first""). try:. assert adata[0:2, :][:, 0:2].X.tolist() == [[1,2], [4,5]]. assert adata[0, 0].X.tolist() == 1. assert adata[0:1, 0:1].X.tolist() == 1. assert adata[0, :][:, 0].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). print(""\n>>> integer indexing, vars first""). try:. assert adata[:, 0:2][0:2, :].X.tolist() == [[1,2], [4,5]]. assert adata[0, 0].X.tolist() == 1. assert adata[0:1, 0:1].X.tolist() == 1. assert adata[:, 0][0, :].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). # boolean indexing. print(""\n>>> boolean indexing, obs first""). try:. obs_selector = np.zeros(len(adata.obs), dtype=bool). vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]. vars_selector[:] = [True, True, False]. assert adata[obs_selector, :][:, vars_selector].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]. vars_selector[:] = [True, False, False]. assert adata[obs_selector, :][:, vars_selector].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). print(""\n>>> boolean indexing, vars first""). try:. obs_selector = np.zeros(len(adata.obs), dtype=bool). vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]. vars_selector[:] = [True, True, False]. assert adata[:, vars_selector][obs_selector, :].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]. vars_selector[:] = [True, False, False]. assert adata[:, vars_selector][obs_selector, :].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332
https://github.com/scverse/scanpy/issues/332:1839,testability,assert,assert,1839,"More comprehensive test, similar to the current AnnData slicing tests (no use of external data). Each try/except block will (currently) fail on its last assertion. ```. import sys, traceback. import numpy as np. import scanpy.api as sc. adata = sc.AnnData(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])). # integer indexing. print(""\n>>> integer indexing, obs first""). try:. assert adata[0:2, :][:, 0:2].X.tolist() == [[1,2], [4,5]]. assert adata[0, 0].X.tolist() == 1. assert adata[0:1, 0:1].X.tolist() == 1. assert adata[0, :][:, 0].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). print(""\n>>> integer indexing, vars first""). try:. assert adata[:, 0:2][0:2, :].X.tolist() == [[1,2], [4,5]]. assert adata[0, 0].X.tolist() == 1. assert adata[0:1, 0:1].X.tolist() == 1. assert adata[:, 0][0, :].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). # boolean indexing. print(""\n>>> boolean indexing, obs first""). try:. obs_selector = np.zeros(len(adata.obs), dtype=bool). vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]. vars_selector[:] = [True, True, False]. assert adata[obs_selector, :][:, vars_selector].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]. vars_selector[:] = [True, False, False]. assert adata[obs_selector, :][:, vars_selector].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). print(""\n>>> boolean indexing, vars first""). try:. obs_selector = np.zeros(len(adata.obs), dtype=bool). vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]. vars_selector[:] = [True, True, False]. assert adata[:, vars_selector][obs_selector, :].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]. vars_selector[:] = [True, False, False]. assert adata[:, vars_selector][obs_selector, :].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332
https://github.com/scverse/scanpy/issues/332:1928,testability,trace,traceback,1928,"More comprehensive test, similar to the current AnnData slicing tests (no use of external data). Each try/except block will (currently) fail on its last assertion. ```. import sys, traceback. import numpy as np. import scanpy.api as sc. adata = sc.AnnData(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])). # integer indexing. print(""\n>>> integer indexing, obs first""). try:. assert adata[0:2, :][:, 0:2].X.tolist() == [[1,2], [4,5]]. assert adata[0, 0].X.tolist() == 1. assert adata[0:1, 0:1].X.tolist() == 1. assert adata[0, :][:, 0].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). print(""\n>>> integer indexing, vars first""). try:. assert adata[:, 0:2][0:2, :].X.tolist() == [[1,2], [4,5]]. assert adata[0, 0].X.tolist() == 1. assert adata[0:1, 0:1].X.tolist() == 1. assert adata[:, 0][0, :].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). # boolean indexing. print(""\n>>> boolean indexing, obs first""). try:. obs_selector = np.zeros(len(adata.obs), dtype=bool). vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]. vars_selector[:] = [True, True, False]. assert adata[obs_selector, :][:, vars_selector].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]. vars_selector[:] = [True, False, False]. assert adata[obs_selector, :][:, vars_selector].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). print(""\n>>> boolean indexing, vars first""). try:. obs_selector = np.zeros(len(adata.obs), dtype=bool). vars_selector = np.zeros(len(adata.var), dtype=bool). obs_selector[:] = [True, True, False]. vars_selector[:] = [True, True, False]. assert adata[:, vars_selector][obs_selector, :].X.tolist() == [[1,2], [4,5]]. obs_selector[:] = [True, False, False]. vars_selector[:] = [True, False, False]. assert adata[:, vars_selector][obs_selector, :].X.tolist() == 1. except Exception as e:. traceback.print_exc(file=sys.stdout). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332
https://github.com/scverse/scanpy/issues/333:22,availability,error,error,22,"I am getting the same error in a much more basic setting:. ```. paul=sc.datasets.paul15(). sc.pl.scatter(paul, x=paul.var_names[0], y=paul.var_names[1]). ```. ... > TypeError: object of type 'numpy.int64' has no len()",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/333
https://github.com/scverse/scanpy/issues/333:22,performance,error,error,22,"I am getting the same error in a much more basic setting:. ```. paul=sc.datasets.paul15(). sc.pl.scatter(paul, x=paul.var_names[0], y=paul.var_names[1]). ```. ... > TypeError: object of type 'numpy.int64' has no len()",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/333
https://github.com/scverse/scanpy/issues/333:22,safety,error,error,22,"I am getting the same error in a much more basic setting:. ```. paul=sc.datasets.paul15(). sc.pl.scatter(paul, x=paul.var_names[0], y=paul.var_names[1]). ```. ... > TypeError: object of type 'numpy.int64' has no len()",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/333
https://github.com/scverse/scanpy/issues/333:22,usability,error,error,22,"I am getting the same error in a much more basic setting:. ```. paul=sc.datasets.paul15(). sc.pl.scatter(paul, x=paul.var_names[0], y=paul.var_names[1]). ```. ... > TypeError: object of type 'numpy.int64' has no len()",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/333
https://github.com/scverse/scanpy/issues/333:21,deployability,version,versions,21,Forgot to mention my versions: . > scanpy==1.3.2 anndata==0.6.11 numpy==1.14.6 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/333
https://github.com/scverse/scanpy/issues/333:21,integrability,version,versions,21,Forgot to mention my versions: . > scanpy==1.3.2 anndata==0.6.11 numpy==1.14.6 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/333
https://github.com/scverse/scanpy/issues/333:21,modifiability,version,versions,21,Forgot to mention my versions: . > scanpy==1.3.2 anndata==0.6.11 numpy==1.14.6 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/333
https://github.com/scverse/scanpy/issues/333:114,usability,learn,learn,114,Forgot to mention my versions: . > scanpy==1.3.2 anndata==0.6.11 numpy==1.14.6 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/333
https://github.com/scverse/scanpy/issues/333:74,deployability,version,version,74,"Everything runs fine on the current master branch, I uploaded the current version of the notebook: https://nbviewer.jupyter.org/github/theislab/paga/blob/master/blood/paul15/paul15.ipynb. I'll release either 1.3.3 or 1.4 very soon and if there should have been a bug at some point, it seems to have been fixed at some point. Finally, PAGA is also in the continuous integration tests, so no bugs in the future anymore for this. ;)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/333
https://github.com/scverse/scanpy/issues/333:193,deployability,releas,release,193,"Everything runs fine on the current master branch, I uploaded the current version of the notebook: https://nbviewer.jupyter.org/github/theislab/paga/blob/master/blood/paul15/paul15.ipynb. I'll release either 1.3.3 or 1.4 very soon and if there should have been a bug at some point, it seems to have been fixed at some point. Finally, PAGA is also in the continuous integration tests, so no bugs in the future anymore for this. ;)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/333
https://github.com/scverse/scanpy/issues/333:354,deployability,continu,continuous,354,"Everything runs fine on the current master branch, I uploaded the current version of the notebook: https://nbviewer.jupyter.org/github/theislab/paga/blob/master/blood/paul15/paul15.ipynb. I'll release either 1.3.3 or 1.4 very soon and if there should have been a bug at some point, it seems to have been fixed at some point. Finally, PAGA is also in the continuous integration tests, so no bugs in the future anymore for this. ;)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/333
https://github.com/scverse/scanpy/issues/333:365,deployability,integr,integration,365,"Everything runs fine on the current master branch, I uploaded the current version of the notebook: https://nbviewer.jupyter.org/github/theislab/paga/blob/master/blood/paul15/paul15.ipynb. I'll release either 1.3.3 or 1.4 very soon and if there should have been a bug at some point, it seems to have been fixed at some point. Finally, PAGA is also in the continuous integration tests, so no bugs in the future anymore for this. ;)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/333
https://github.com/scverse/scanpy/issues/333:28,energy efficiency,current,current,28,"Everything runs fine on the current master branch, I uploaded the current version of the notebook: https://nbviewer.jupyter.org/github/theislab/paga/blob/master/blood/paul15/paul15.ipynb. I'll release either 1.3.3 or 1.4 very soon and if there should have been a bug at some point, it seems to have been fixed at some point. Finally, PAGA is also in the continuous integration tests, so no bugs in the future anymore for this. ;)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/333
https://github.com/scverse/scanpy/issues/333:66,energy efficiency,current,current,66,"Everything runs fine on the current master branch, I uploaded the current version of the notebook: https://nbviewer.jupyter.org/github/theislab/paga/blob/master/blood/paul15/paul15.ipynb. I'll release either 1.3.3 or 1.4 very soon and if there should have been a bug at some point, it seems to have been fixed at some point. Finally, PAGA is also in the continuous integration tests, so no bugs in the future anymore for this. ;)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/333
https://github.com/scverse/scanpy/issues/333:74,integrability,version,version,74,"Everything runs fine on the current master branch, I uploaded the current version of the notebook: https://nbviewer.jupyter.org/github/theislab/paga/blob/master/blood/paul15/paul15.ipynb. I'll release either 1.3.3 or 1.4 very soon and if there should have been a bug at some point, it seems to have been fixed at some point. Finally, PAGA is also in the continuous integration tests, so no bugs in the future anymore for this. ;)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/333
https://github.com/scverse/scanpy/issues/333:365,integrability,integration test,integration tests,365,"Everything runs fine on the current master branch, I uploaded the current version of the notebook: https://nbviewer.jupyter.org/github/theislab/paga/blob/master/blood/paul15/paul15.ipynb. I'll release either 1.3.3 or 1.4 very soon and if there should have been a bug at some point, it seems to have been fixed at some point. Finally, PAGA is also in the continuous integration tests, so no bugs in the future anymore for this. ;)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/333
https://github.com/scverse/scanpy/issues/333:365,interoperability,integr,integration,365,"Everything runs fine on the current master branch, I uploaded the current version of the notebook: https://nbviewer.jupyter.org/github/theislab/paga/blob/master/blood/paul15/paul15.ipynb. I'll release either 1.3.3 or 1.4 very soon and if there should have been a bug at some point, it seems to have been fixed at some point. Finally, PAGA is also in the continuous integration tests, so no bugs in the future anymore for this. ;)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/333
https://github.com/scverse/scanpy/issues/333:74,modifiability,version,version,74,"Everything runs fine on the current master branch, I uploaded the current version of the notebook: https://nbviewer.jupyter.org/github/theislab/paga/blob/master/blood/paul15/paul15.ipynb. I'll release either 1.3.3 or 1.4 very soon and if there should have been a bug at some point, it seems to have been fixed at some point. Finally, PAGA is also in the continuous integration tests, so no bugs in the future anymore for this. ;)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/333
https://github.com/scverse/scanpy/issues/333:365,modifiability,integr,integration,365,"Everything runs fine on the current master branch, I uploaded the current version of the notebook: https://nbviewer.jupyter.org/github/theislab/paga/blob/master/blood/paul15/paul15.ipynb. I'll release either 1.3.3 or 1.4 very soon and if there should have been a bug at some point, it seems to have been fixed at some point. Finally, PAGA is also in the continuous integration tests, so no bugs in the future anymore for this. ;)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/333
https://github.com/scverse/scanpy/issues/333:365,reliability,integr,integration,365,"Everything runs fine on the current master branch, I uploaded the current version of the notebook: https://nbviewer.jupyter.org/github/theislab/paga/blob/master/blood/paul15/paul15.ipynb. I'll release either 1.3.3 or 1.4 very soon and if there should have been a bug at some point, it seems to have been fixed at some point. Finally, PAGA is also in the continuous integration tests, so no bugs in the future anymore for this. ;)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/333
https://github.com/scverse/scanpy/issues/333:377,safety,test,tests,377,"Everything runs fine on the current master branch, I uploaded the current version of the notebook: https://nbviewer.jupyter.org/github/theislab/paga/blob/master/blood/paul15/paul15.ipynb. I'll release either 1.3.3 or 1.4 very soon and if there should have been a bug at some point, it seems to have been fixed at some point. Finally, PAGA is also in the continuous integration tests, so no bugs in the future anymore for this. ;)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/333
https://github.com/scverse/scanpy/issues/333:365,security,integr,integration,365,"Everything runs fine on the current master branch, I uploaded the current version of the notebook: https://nbviewer.jupyter.org/github/theislab/paga/blob/master/blood/paul15/paul15.ipynb. I'll release either 1.3.3 or 1.4 very soon and if there should have been a bug at some point, it seems to have been fixed at some point. Finally, PAGA is also in the continuous integration tests, so no bugs in the future anymore for this. ;)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/333
https://github.com/scverse/scanpy/issues/333:365,testability,integr,integration,365,"Everything runs fine on the current master branch, I uploaded the current version of the notebook: https://nbviewer.jupyter.org/github/theislab/paga/blob/master/blood/paul15/paul15.ipynb. I'll release either 1.3.3 or 1.4 very soon and if there should have been a bug at some point, it seems to have been fixed at some point. Finally, PAGA is also in the continuous integration tests, so no bugs in the future anymore for this. ;)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/333
https://github.com/scverse/scanpy/issues/333:377,testability,test,tests,377,"Everything runs fine on the current master branch, I uploaded the current version of the notebook: https://nbviewer.jupyter.org/github/theislab/paga/blob/master/blood/paul15/paul15.ipynb. I'll release either 1.3.3 or 1.4 very soon and if there should have been a bug at some point, it seems to have been fixed at some point. Finally, PAGA is also in the continuous integration tests, so no bugs in the future anymore for this. ;)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/333
https://github.com/scverse/scanpy/issues/333:27,availability,error,error,27,I updated to 1.3.3 but the error still persists. One important thing I didnt mention before: I am running python/scanpy on a Windows machine. @Donovan-CG do you also use Windows?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/333
https://github.com/scverse/scanpy/issues/333:2,deployability,updat,updated,2,I updated to 1.3.3 but the error still persists. One important thing I didnt mention before: I am running python/scanpy on a Windows machine. @Donovan-CG do you also use Windows?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/333
https://github.com/scverse/scanpy/issues/333:27,performance,error,error,27,I updated to 1.3.3 but the error still persists. One important thing I didnt mention before: I am running python/scanpy on a Windows machine. @Donovan-CG do you also use Windows?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/333
https://github.com/scverse/scanpy/issues/333:2,safety,updat,updated,2,I updated to 1.3.3 but the error still persists. One important thing I didnt mention before: I am running python/scanpy on a Windows machine. @Donovan-CG do you also use Windows?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/333
https://github.com/scverse/scanpy/issues/333:27,safety,error,error,27,I updated to 1.3.3 but the error still persists. One important thing I didnt mention before: I am running python/scanpy on a Windows machine. @Donovan-CG do you also use Windows?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/333
https://github.com/scverse/scanpy/issues/333:2,security,updat,updated,2,I updated to 1.3.3 but the error still persists. One important thing I didnt mention before: I am running python/scanpy on a Windows machine. @Donovan-CG do you also use Windows?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/333
https://github.com/scverse/scanpy/issues/333:27,usability,error,error,27,I updated to 1.3.3 but the error still persists. One important thing I didnt mention before: I am running python/scanpy on a Windows machine. @Donovan-CG do you also use Windows?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/333
https://github.com/scverse/scanpy/issues/333:80,deployability,stack,stackoverflow,80,"This is probably related to some Windows ""weirdness"" as discussed here: https://stackoverflow.com/questions/36278590/numpy-array-dtype-is-coming-as-int32-by-default-in-a-windows-10-64-bit-machine",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/333
https://github.com/scverse/scanpy/issues/335:107,reliability,doe,doesn,107,"Yes, a few people manually overlay different images and need the label positions for that. But I agree, it doesn't seem like a canonical case and we don't want the previous hackish solution for that. Maybe one could add the label positions in an unstructured annotation, though: `.uns['umap']['pos_louvain`]`. Wouldn't have a actual disadvantages, I guess.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/335
https://github.com/scverse/scanpy/issues/335:173,security,hack,hackish,173,"Yes, a few people manually overlay different images and need the label positions for that. But I agree, it doesn't seem like a canonical case and we don't want the previous hackish solution for that. Maybe one could add the label positions in an unstructured annotation, though: `.uns['umap']['pos_louvain`]`. Wouldn't have a actual disadvantages, I guess.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/335
https://github.com/scverse/scanpy/issues/336:253,deployability,api,api,253,@fidelram are you calling an implicit function `summarize_categorical` or something that could be exposed to the user as a tool? . @wangjiawen2013 `sc.set_figure_params(vector_friendly=False)` does what you want: https://scanpy.readthedocs.io/en/latest/api/index.html#settings,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/336
https://github.com/scverse/scanpy/issues/336:253,integrability,api,api,253,@fidelram are you calling an implicit function `summarize_categorical` or something that could be exposed to the user as a tool? . @wangjiawen2013 `sc.set_figure_params(vector_friendly=False)` does what you want: https://scanpy.readthedocs.io/en/latest/api/index.html#settings,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/336
https://github.com/scverse/scanpy/issues/336:253,interoperability,api,api,253,@fidelram are you calling an implicit function `summarize_categorical` or something that could be exposed to the user as a tool? . @wangjiawen2013 `sc.set_figure_params(vector_friendly=False)` does what you want: https://scanpy.readthedocs.io/en/latest/api/index.html#settings,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/336
https://github.com/scverse/scanpy/issues/336:193,reliability,doe,does,193,@fidelram are you calling an implicit function `summarize_categorical` or something that could be exposed to the user as a tool? . @wangjiawen2013 `sc.set_figure_params(vector_friendly=False)` does what you want: https://scanpy.readthedocs.io/en/latest/api/index.html#settings,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/336
https://github.com/scverse/scanpy/issues/336:98,security,expos,exposed,98,@fidelram are you calling an implicit function `summarize_categorical` or something that could be exposed to the user as a tool? . @wangjiawen2013 `sc.set_figure_params(vector_friendly=False)` does what you want: https://scanpy.readthedocs.io/en/latest/api/index.html#settings,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/336
https://github.com/scverse/scanpy/issues/336:113,usability,user,user,113,@fidelram are you calling an implicit function `summarize_categorical` or something that could be exposed to the user as a tool? . @wangjiawen2013 `sc.set_figure_params(vector_friendly=False)` does what you want: https://scanpy.readthedocs.io/en/latest/api/index.html#settings,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/336
https://github.com/scverse/scanpy/issues/336:123,usability,tool,tool,123,@fidelram are you calling an implicit function `summarize_categorical` or something that could be exposed to the user as a tool? . @wangjiawen2013 `sc.set_figure_params(vector_friendly=False)` does what you want: https://scanpy.readthedocs.io/en/latest/api/index.html#settings,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/336
https://github.com/scverse/scanpy/issues/336:111,availability,cluster,clusters,111,"I have got what I want with the following code adapted from dotplot():. gene_ids = adata.raw.var.index.values. clusters = adata.obs['louvain'].cat.categories. obs = adata.raw[:,gene_ids].X.toarray(). obs = pd.DataFrame(obs,columns=gene_ids,index=adata.obs['louvain']). average_obs = obs.groupby(level=0).mean(). obs_bool = obs.astype(bool). fraction_obs = obs_bool.groupby(level=0).sum()/obs_bool.groupby(level=0).count(). average_obs.T.to_csv(""average.csv""). fraction_obs.T.to_csv(""fraction.csv"")",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/336
https://github.com/scverse/scanpy/issues/336:111,deployability,cluster,clusters,111,"I have got what I want with the following code adapted from dotplot():. gene_ids = adata.raw.var.index.values. clusters = adata.obs['louvain'].cat.categories. obs = adata.raw[:,gene_ids].X.toarray(). obs = pd.DataFrame(obs,columns=gene_ids,index=adata.obs['louvain']). average_obs = obs.groupby(level=0).mean(). obs_bool = obs.astype(bool). fraction_obs = obs_bool.groupby(level=0).sum()/obs_bool.groupby(level=0).count(). average_obs.T.to_csv(""average.csv""). fraction_obs.T.to_csv(""fraction.csv"")",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/336
https://github.com/scverse/scanpy/issues/336:47,energy efficiency,adapt,adapted,47,"I have got what I want with the following code adapted from dotplot():. gene_ids = adata.raw.var.index.values. clusters = adata.obs['louvain'].cat.categories. obs = adata.raw[:,gene_ids].X.toarray(). obs = pd.DataFrame(obs,columns=gene_ids,index=adata.obs['louvain']). average_obs = obs.groupby(level=0).mean(). obs_bool = obs.astype(bool). fraction_obs = obs_bool.groupby(level=0).sum()/obs_bool.groupby(level=0).count(). average_obs.T.to_csv(""average.csv""). fraction_obs.T.to_csv(""fraction.csv"")",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/336
https://github.com/scverse/scanpy/issues/336:47,integrability,adapt,adapted,47,"I have got what I want with the following code adapted from dotplot():. gene_ids = adata.raw.var.index.values. clusters = adata.obs['louvain'].cat.categories. obs = adata.raw[:,gene_ids].X.toarray(). obs = pd.DataFrame(obs,columns=gene_ids,index=adata.obs['louvain']). average_obs = obs.groupby(level=0).mean(). obs_bool = obs.astype(bool). fraction_obs = obs_bool.groupby(level=0).sum()/obs_bool.groupby(level=0).count(). average_obs.T.to_csv(""average.csv""). fraction_obs.T.to_csv(""fraction.csv"")",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/336
https://github.com/scverse/scanpy/issues/336:47,interoperability,adapt,adapted,47,"I have got what I want with the following code adapted from dotplot():. gene_ids = adata.raw.var.index.values. clusters = adata.obs['louvain'].cat.categories. obs = adata.raw[:,gene_ids].X.toarray(). obs = pd.DataFrame(obs,columns=gene_ids,index=adata.obs['louvain']). average_obs = obs.groupby(level=0).mean(). obs_bool = obs.astype(bool). fraction_obs = obs_bool.groupby(level=0).sum()/obs_bool.groupby(level=0).count(). average_obs.T.to_csv(""average.csv""). fraction_obs.T.to_csv(""fraction.csv"")",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/336
https://github.com/scverse/scanpy/issues/336:47,modifiability,adapt,adapted,47,"I have got what I want with the following code adapted from dotplot():. gene_ids = adata.raw.var.index.values. clusters = adata.obs['louvain'].cat.categories. obs = adata.raw[:,gene_ids].X.toarray(). obs = pd.DataFrame(obs,columns=gene_ids,index=adata.obs['louvain']). average_obs = obs.groupby(level=0).mean(). obs_bool = obs.astype(bool). fraction_obs = obs_bool.groupby(level=0).sum()/obs_bool.groupby(level=0).count(). average_obs.T.to_csv(""average.csv""). fraction_obs.T.to_csv(""fraction.csv"")",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/336
https://github.com/scverse/scanpy/issues/336:331,availability,cluster,cluster,331,"I could modify dotplot to return this information. Initially, I thought that the data used by dot plot was too *ad hoc* because the percentage (size of dot) is based on the dropouts, which only is meaningful on the raw matrix. However, I keep finding this information useful to eyeball potential markers expressed only on a single cluster.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/336
https://github.com/scverse/scanpy/issues/336:331,deployability,cluster,cluster,331,"I could modify dotplot to return this information. Initially, I thought that the data used by dot plot was too *ad hoc* because the percentage (size of dot) is based on the dropouts, which only is meaningful on the raw matrix. However, I keep finding this information useful to eyeball potential markers expressed only on a single cluster.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/336
https://github.com/scverse/scanpy/issues/336:8,security,modif,modify,8,"I could modify dotplot to return this information. Initially, I thought that the data used by dot plot was too *ad hoc* because the percentage (size of dot) is based on the dropouts, which only is meaningful on the raw matrix. However, I keep finding this information useful to eyeball potential markers expressed only on a single cluster.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/336
https://github.com/scverse/scanpy/issues/336:133,availability,cluster,cluster,133,I would also be interested in a version which delivers the information shown in the dotplot! Would be extremely useful for automatic cluster annotation.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/336
https://github.com/scverse/scanpy/issues/336:32,deployability,version,version,32,I would also be interested in a version which delivers the information shown in the dotplot! Would be extremely useful for automatic cluster annotation.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/336
https://github.com/scverse/scanpy/issues/336:123,deployability,automat,automatic,123,I would also be interested in a version which delivers the information shown in the dotplot! Would be extremely useful for automatic cluster annotation.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/336
https://github.com/scverse/scanpy/issues/336:133,deployability,cluster,cluster,133,I would also be interested in a version which delivers the information shown in the dotplot! Would be extremely useful for automatic cluster annotation.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/336
https://github.com/scverse/scanpy/issues/336:32,integrability,version,version,32,I would also be interested in a version which delivers the information shown in the dotplot! Would be extremely useful for automatic cluster annotation.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/336
https://github.com/scverse/scanpy/issues/336:32,modifiability,version,version,32,I would also be interested in a version which delivers the information shown in the dotplot! Would be extremely useful for automatic cluster annotation.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/336
https://github.com/scverse/scanpy/issues/336:123,testability,automat,automatic,123,I would also be interested in a version which delivers the information shown in the dotplot! Would be extremely useful for automatic cluster annotation.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/336
https://github.com/scverse/scanpy/issues/336:99,availability,cluster,cluster,99,Agree. Adding specialized function returning mean expression and percentage of given genes in each cluster will be very useful.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/336
https://github.com/scverse/scanpy/issues/336:99,deployability,cluster,cluster,99,Agree. Adding specialized function returning mean expression and percentage of given genes in each cluster will be very useful.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/336
https://github.com/scverse/scanpy/issues/336:118,availability,cluster,clusters,118,"> I have got what I want with the following code adapted from dotplot():. > . > gene_ids = adata.raw.var.index.values clusters = adata.obs['louvain'].cat.categories obs = adata.raw[:,gene_ids].X.toarray() obs = pd.DataFrame(obs,columns=gene_ids,index=adata.obs['louvain']) average_obs = obs.groupby(level=0).mean() obs_bool = obs.astype(bool) fraction_obs = obs_bool.groupby(level=0).sum()/obs_bool.groupby(level=0).count() average_obs.T.to_csv(""average.csv"") fraction_obs.T.to_csv(""fraction.csv""). Love this! Thanks a lot!! . Just one question, is there a way to get the average expression in different cell types (cluster label 1 ) in different sample (cluster label 2 ) from an integrated object?? . to get something roughly like this:. Gene 1 Gene 2 . sample1 sample2 sample3 sample1 sample2 sample3 ..... ....... .... T-cell. B-cell . ..... ..... I am not sure if this makes sense, but I have been trying to do this for a while and nothing worked!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/336
https://github.com/scverse/scanpy/issues/336:616,availability,cluster,cluster,616,"> I have got what I want with the following code adapted from dotplot():. > . > gene_ids = adata.raw.var.index.values clusters = adata.obs['louvain'].cat.categories obs = adata.raw[:,gene_ids].X.toarray() obs = pd.DataFrame(obs,columns=gene_ids,index=adata.obs['louvain']) average_obs = obs.groupby(level=0).mean() obs_bool = obs.astype(bool) fraction_obs = obs_bool.groupby(level=0).sum()/obs_bool.groupby(level=0).count() average_obs.T.to_csv(""average.csv"") fraction_obs.T.to_csv(""fraction.csv""). Love this! Thanks a lot!! . Just one question, is there a way to get the average expression in different cell types (cluster label 1 ) in different sample (cluster label 2 ) from an integrated object?? . to get something roughly like this:. Gene 1 Gene 2 . sample1 sample2 sample3 sample1 sample2 sample3 ..... ....... .... T-cell. B-cell . ..... ..... I am not sure if this makes sense, but I have been trying to do this for a while and nothing worked!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/336
https://github.com/scverse/scanpy/issues/336:655,availability,cluster,cluster,655,"> I have got what I want with the following code adapted from dotplot():. > . > gene_ids = adata.raw.var.index.values clusters = adata.obs['louvain'].cat.categories obs = adata.raw[:,gene_ids].X.toarray() obs = pd.DataFrame(obs,columns=gene_ids,index=adata.obs['louvain']) average_obs = obs.groupby(level=0).mean() obs_bool = obs.astype(bool) fraction_obs = obs_bool.groupby(level=0).sum()/obs_bool.groupby(level=0).count() average_obs.T.to_csv(""average.csv"") fraction_obs.T.to_csv(""fraction.csv""). Love this! Thanks a lot!! . Just one question, is there a way to get the average expression in different cell types (cluster label 1 ) in different sample (cluster label 2 ) from an integrated object?? . to get something roughly like this:. Gene 1 Gene 2 . sample1 sample2 sample3 sample1 sample2 sample3 ..... ....... .... T-cell. B-cell . ..... ..... I am not sure if this makes sense, but I have been trying to do this for a while and nothing worked!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/336
https://github.com/scverse/scanpy/issues/336:118,deployability,cluster,clusters,118,"> I have got what I want with the following code adapted from dotplot():. > . > gene_ids = adata.raw.var.index.values clusters = adata.obs['louvain'].cat.categories obs = adata.raw[:,gene_ids].X.toarray() obs = pd.DataFrame(obs,columns=gene_ids,index=adata.obs['louvain']) average_obs = obs.groupby(level=0).mean() obs_bool = obs.astype(bool) fraction_obs = obs_bool.groupby(level=0).sum()/obs_bool.groupby(level=0).count() average_obs.T.to_csv(""average.csv"") fraction_obs.T.to_csv(""fraction.csv""). Love this! Thanks a lot!! . Just one question, is there a way to get the average expression in different cell types (cluster label 1 ) in different sample (cluster label 2 ) from an integrated object?? . to get something roughly like this:. Gene 1 Gene 2 . sample1 sample2 sample3 sample1 sample2 sample3 ..... ....... .... T-cell. B-cell . ..... ..... I am not sure if this makes sense, but I have been trying to do this for a while and nothing worked!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/336
https://github.com/scverse/scanpy/issues/336:616,deployability,cluster,cluster,616,"> I have got what I want with the following code adapted from dotplot():. > . > gene_ids = adata.raw.var.index.values clusters = adata.obs['louvain'].cat.categories obs = adata.raw[:,gene_ids].X.toarray() obs = pd.DataFrame(obs,columns=gene_ids,index=adata.obs['louvain']) average_obs = obs.groupby(level=0).mean() obs_bool = obs.astype(bool) fraction_obs = obs_bool.groupby(level=0).sum()/obs_bool.groupby(level=0).count() average_obs.T.to_csv(""average.csv"") fraction_obs.T.to_csv(""fraction.csv""). Love this! Thanks a lot!! . Just one question, is there a way to get the average expression in different cell types (cluster label 1 ) in different sample (cluster label 2 ) from an integrated object?? . to get something roughly like this:. Gene 1 Gene 2 . sample1 sample2 sample3 sample1 sample2 sample3 ..... ....... .... T-cell. B-cell . ..... ..... I am not sure if this makes sense, but I have been trying to do this for a while and nothing worked!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/336
https://github.com/scverse/scanpy/issues/336:655,deployability,cluster,cluster,655,"> I have got what I want with the following code adapted from dotplot():. > . > gene_ids = adata.raw.var.index.values clusters = adata.obs['louvain'].cat.categories obs = adata.raw[:,gene_ids].X.toarray() obs = pd.DataFrame(obs,columns=gene_ids,index=adata.obs['louvain']) average_obs = obs.groupby(level=0).mean() obs_bool = obs.astype(bool) fraction_obs = obs_bool.groupby(level=0).sum()/obs_bool.groupby(level=0).count() average_obs.T.to_csv(""average.csv"") fraction_obs.T.to_csv(""fraction.csv""). Love this! Thanks a lot!! . Just one question, is there a way to get the average expression in different cell types (cluster label 1 ) in different sample (cluster label 2 ) from an integrated object?? . to get something roughly like this:. Gene 1 Gene 2 . sample1 sample2 sample3 sample1 sample2 sample3 ..... ....... .... T-cell. B-cell . ..... ..... I am not sure if this makes sense, but I have been trying to do this for a while and nothing worked!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/336
https://github.com/scverse/scanpy/issues/336:681,deployability,integr,integrated,681,"> I have got what I want with the following code adapted from dotplot():. > . > gene_ids = adata.raw.var.index.values clusters = adata.obs['louvain'].cat.categories obs = adata.raw[:,gene_ids].X.toarray() obs = pd.DataFrame(obs,columns=gene_ids,index=adata.obs['louvain']) average_obs = obs.groupby(level=0).mean() obs_bool = obs.astype(bool) fraction_obs = obs_bool.groupby(level=0).sum()/obs_bool.groupby(level=0).count() average_obs.T.to_csv(""average.csv"") fraction_obs.T.to_csv(""fraction.csv""). Love this! Thanks a lot!! . Just one question, is there a way to get the average expression in different cell types (cluster label 1 ) in different sample (cluster label 2 ) from an integrated object?? . to get something roughly like this:. Gene 1 Gene 2 . sample1 sample2 sample3 sample1 sample2 sample3 ..... ....... .... T-cell. B-cell . ..... ..... I am not sure if this makes sense, but I have been trying to do this for a while and nothing worked!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/336
https://github.com/scverse/scanpy/issues/336:49,energy efficiency,adapt,adapted,49,"> I have got what I want with the following code adapted from dotplot():. > . > gene_ids = adata.raw.var.index.values clusters = adata.obs['louvain'].cat.categories obs = adata.raw[:,gene_ids].X.toarray() obs = pd.DataFrame(obs,columns=gene_ids,index=adata.obs['louvain']) average_obs = obs.groupby(level=0).mean() obs_bool = obs.astype(bool) fraction_obs = obs_bool.groupby(level=0).sum()/obs_bool.groupby(level=0).count() average_obs.T.to_csv(""average.csv"") fraction_obs.T.to_csv(""fraction.csv""). Love this! Thanks a lot!! . Just one question, is there a way to get the average expression in different cell types (cluster label 1 ) in different sample (cluster label 2 ) from an integrated object?? . to get something roughly like this:. Gene 1 Gene 2 . sample1 sample2 sample3 sample1 sample2 sample3 ..... ....... .... T-cell. B-cell . ..... ..... I am not sure if this makes sense, but I have been trying to do this for a while and nothing worked!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/336
https://github.com/scverse/scanpy/issues/336:49,integrability,adapt,adapted,49,"> I have got what I want with the following code adapted from dotplot():. > . > gene_ids = adata.raw.var.index.values clusters = adata.obs['louvain'].cat.categories obs = adata.raw[:,gene_ids].X.toarray() obs = pd.DataFrame(obs,columns=gene_ids,index=adata.obs['louvain']) average_obs = obs.groupby(level=0).mean() obs_bool = obs.astype(bool) fraction_obs = obs_bool.groupby(level=0).sum()/obs_bool.groupby(level=0).count() average_obs.T.to_csv(""average.csv"") fraction_obs.T.to_csv(""fraction.csv""). Love this! Thanks a lot!! . Just one question, is there a way to get the average expression in different cell types (cluster label 1 ) in different sample (cluster label 2 ) from an integrated object?? . to get something roughly like this:. Gene 1 Gene 2 . sample1 sample2 sample3 sample1 sample2 sample3 ..... ....... .... T-cell. B-cell . ..... ..... I am not sure if this makes sense, but I have been trying to do this for a while and nothing worked!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/336
https://github.com/scverse/scanpy/issues/336:681,integrability,integr,integrated,681,"> I have got what I want with the following code adapted from dotplot():. > . > gene_ids = adata.raw.var.index.values clusters = adata.obs['louvain'].cat.categories obs = adata.raw[:,gene_ids].X.toarray() obs = pd.DataFrame(obs,columns=gene_ids,index=adata.obs['louvain']) average_obs = obs.groupby(level=0).mean() obs_bool = obs.astype(bool) fraction_obs = obs_bool.groupby(level=0).sum()/obs_bool.groupby(level=0).count() average_obs.T.to_csv(""average.csv"") fraction_obs.T.to_csv(""fraction.csv""). Love this! Thanks a lot!! . Just one question, is there a way to get the average expression in different cell types (cluster label 1 ) in different sample (cluster label 2 ) from an integrated object?? . to get something roughly like this:. Gene 1 Gene 2 . sample1 sample2 sample3 sample1 sample2 sample3 ..... ....... .... T-cell. B-cell . ..... ..... I am not sure if this makes sense, but I have been trying to do this for a while and nothing worked!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/336
https://github.com/scverse/scanpy/issues/336:49,interoperability,adapt,adapted,49,"> I have got what I want with the following code adapted from dotplot():. > . > gene_ids = adata.raw.var.index.values clusters = adata.obs['louvain'].cat.categories obs = adata.raw[:,gene_ids].X.toarray() obs = pd.DataFrame(obs,columns=gene_ids,index=adata.obs['louvain']) average_obs = obs.groupby(level=0).mean() obs_bool = obs.astype(bool) fraction_obs = obs_bool.groupby(level=0).sum()/obs_bool.groupby(level=0).count() average_obs.T.to_csv(""average.csv"") fraction_obs.T.to_csv(""fraction.csv""). Love this! Thanks a lot!! . Just one question, is there a way to get the average expression in different cell types (cluster label 1 ) in different sample (cluster label 2 ) from an integrated object?? . to get something roughly like this:. Gene 1 Gene 2 . sample1 sample2 sample3 sample1 sample2 sample3 ..... ....... .... T-cell. B-cell . ..... ..... I am not sure if this makes sense, but I have been trying to do this for a while and nothing worked!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/336
https://github.com/scverse/scanpy/issues/336:681,interoperability,integr,integrated,681,"> I have got what I want with the following code adapted from dotplot():. > . > gene_ids = adata.raw.var.index.values clusters = adata.obs['louvain'].cat.categories obs = adata.raw[:,gene_ids].X.toarray() obs = pd.DataFrame(obs,columns=gene_ids,index=adata.obs['louvain']) average_obs = obs.groupby(level=0).mean() obs_bool = obs.astype(bool) fraction_obs = obs_bool.groupby(level=0).sum()/obs_bool.groupby(level=0).count() average_obs.T.to_csv(""average.csv"") fraction_obs.T.to_csv(""fraction.csv""). Love this! Thanks a lot!! . Just one question, is there a way to get the average expression in different cell types (cluster label 1 ) in different sample (cluster label 2 ) from an integrated object?? . to get something roughly like this:. Gene 1 Gene 2 . sample1 sample2 sample3 sample1 sample2 sample3 ..... ....... .... T-cell. B-cell . ..... ..... I am not sure if this makes sense, but I have been trying to do this for a while and nothing worked!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/336
https://github.com/scverse/scanpy/issues/336:49,modifiability,adapt,adapted,49,"> I have got what I want with the following code adapted from dotplot():. > . > gene_ids = adata.raw.var.index.values clusters = adata.obs['louvain'].cat.categories obs = adata.raw[:,gene_ids].X.toarray() obs = pd.DataFrame(obs,columns=gene_ids,index=adata.obs['louvain']) average_obs = obs.groupby(level=0).mean() obs_bool = obs.astype(bool) fraction_obs = obs_bool.groupby(level=0).sum()/obs_bool.groupby(level=0).count() average_obs.T.to_csv(""average.csv"") fraction_obs.T.to_csv(""fraction.csv""). Love this! Thanks a lot!! . Just one question, is there a way to get the average expression in different cell types (cluster label 1 ) in different sample (cluster label 2 ) from an integrated object?? . to get something roughly like this:. Gene 1 Gene 2 . sample1 sample2 sample3 sample1 sample2 sample3 ..... ....... .... T-cell. B-cell . ..... ..... I am not sure if this makes sense, but I have been trying to do this for a while and nothing worked!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/336
https://github.com/scverse/scanpy/issues/336:681,modifiability,integr,integrated,681,"> I have got what I want with the following code adapted from dotplot():. > . > gene_ids = adata.raw.var.index.values clusters = adata.obs['louvain'].cat.categories obs = adata.raw[:,gene_ids].X.toarray() obs = pd.DataFrame(obs,columns=gene_ids,index=adata.obs['louvain']) average_obs = obs.groupby(level=0).mean() obs_bool = obs.astype(bool) fraction_obs = obs_bool.groupby(level=0).sum()/obs_bool.groupby(level=0).count() average_obs.T.to_csv(""average.csv"") fraction_obs.T.to_csv(""fraction.csv""). Love this! Thanks a lot!! . Just one question, is there a way to get the average expression in different cell types (cluster label 1 ) in different sample (cluster label 2 ) from an integrated object?? . to get something roughly like this:. Gene 1 Gene 2 . sample1 sample2 sample3 sample1 sample2 sample3 ..... ....... .... T-cell. B-cell . ..... ..... I am not sure if this makes sense, but I have been trying to do this for a while and nothing worked!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/336
https://github.com/scverse/scanpy/issues/336:681,reliability,integr,integrated,681,"> I have got what I want with the following code adapted from dotplot():. > . > gene_ids = adata.raw.var.index.values clusters = adata.obs['louvain'].cat.categories obs = adata.raw[:,gene_ids].X.toarray() obs = pd.DataFrame(obs,columns=gene_ids,index=adata.obs['louvain']) average_obs = obs.groupby(level=0).mean() obs_bool = obs.astype(bool) fraction_obs = obs_bool.groupby(level=0).sum()/obs_bool.groupby(level=0).count() average_obs.T.to_csv(""average.csv"") fraction_obs.T.to_csv(""fraction.csv""). Love this! Thanks a lot!! . Just one question, is there a way to get the average expression in different cell types (cluster label 1 ) in different sample (cluster label 2 ) from an integrated object?? . to get something roughly like this:. Gene 1 Gene 2 . sample1 sample2 sample3 sample1 sample2 sample3 ..... ....... .... T-cell. B-cell . ..... ..... I am not sure if this makes sense, but I have been trying to do this for a while and nothing worked!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/336
https://github.com/scverse/scanpy/issues/336:681,security,integr,integrated,681,"> I have got what I want with the following code adapted from dotplot():. > . > gene_ids = adata.raw.var.index.values clusters = adata.obs['louvain'].cat.categories obs = adata.raw[:,gene_ids].X.toarray() obs = pd.DataFrame(obs,columns=gene_ids,index=adata.obs['louvain']) average_obs = obs.groupby(level=0).mean() obs_bool = obs.astype(bool) fraction_obs = obs_bool.groupby(level=0).sum()/obs_bool.groupby(level=0).count() average_obs.T.to_csv(""average.csv"") fraction_obs.T.to_csv(""fraction.csv""). Love this! Thanks a lot!! . Just one question, is there a way to get the average expression in different cell types (cluster label 1 ) in different sample (cluster label 2 ) from an integrated object?? . to get something roughly like this:. Gene 1 Gene 2 . sample1 sample2 sample3 sample1 sample2 sample3 ..... ....... .... T-cell. B-cell . ..... ..... I am not sure if this makes sense, but I have been trying to do this for a while and nothing worked!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/336
https://github.com/scverse/scanpy/issues/336:681,testability,integr,integrated,681,"> I have got what I want with the following code adapted from dotplot():. > . > gene_ids = adata.raw.var.index.values clusters = adata.obs['louvain'].cat.categories obs = adata.raw[:,gene_ids].X.toarray() obs = pd.DataFrame(obs,columns=gene_ids,index=adata.obs['louvain']) average_obs = obs.groupby(level=0).mean() obs_bool = obs.astype(bool) fraction_obs = obs_bool.groupby(level=0).sum()/obs_bool.groupby(level=0).count() average_obs.T.to_csv(""average.csv"") fraction_obs.T.to_csv(""fraction.csv""). Love this! Thanks a lot!! . Just one question, is there a way to get the average expression in different cell types (cluster label 1 ) in different sample (cluster label 2 ) from an integrated object?? . to get something roughly like this:. Gene 1 Gene 2 . sample1 sample2 sample3 sample1 sample2 sample3 ..... ....... .... T-cell. B-cell . ..... ..... I am not sure if this makes sense, but I have been trying to do this for a while and nothing worked!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/336
https://github.com/scverse/scanpy/issues/337:374,energy efficiency,current,currently,374,"Maybe changing the figure size can result in what you want? (rcParams['figure.figsize']). On the top of my head I don't remember how to change the the color bar thick labels, but I think that what you are trying to do will affect the main plot but not the color bar. . One option is to try to get the color bar axe using some pyplot function. Try searching for this. . I am currently traveling, otherwise I could give you a better solution . . > Am 30.10.2018 um 13:40 schrieb MalteDLuecken <notifications@github.com>:. > . > Hi,. > . > I can't seem to figure out how to change the size of the colorbar tick labels. I've tried. > . > p1 = sc.pl.scatter(adata, 'n_counts', 'n_genes', color='mt_frac', show=False, legend_fontweight=50). > p1.tick_params(labelsize=tick_size). > and a few other options, but I only seem to be able to change the axis label sizes, but never the colormap labels. Any idea? @fidelram? > . > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/337
https://github.com/scverse/scanpy/issues/337:120,safety,reme,remember,120,"Maybe changing the figure size can result in what you want? (rcParams['figure.figsize']). On the top of my head I don't remember how to change the the color bar thick labels, but I think that what you are trying to do will affect the main plot but not the color bar. . One option is to try to get the color bar axe using some pyplot function. Try searching for this. . I am currently traveling, otherwise I could give you a better solution . . > Am 30.10.2018 um 13:40 schrieb MalteDLuecken <notifications@github.com>:. > . > Hi,. > . > I can't seem to figure out how to change the size of the colorbar tick labels. I've tried. > . > p1 = sc.pl.scatter(adata, 'n_counts', 'n_genes', color='mt_frac', show=False, legend_fontweight=50). > p1.tick_params(labelsize=tick_size). > and a few other options, but I only seem to be able to change the axis label sizes, but never the colormap labels. Any idea? @fidelram? > . > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/337
https://github.com/scverse/scanpy/issues/337:19,usability,help,helped,19,"Thanks a lot! This helped to solve my issue. Knowing what to search for is usually gets you there... for future reference, the way to change axis and colorbar ticks is:. ```. tick_size = 15. ax = sc.pl.scatter(adata, 'n_counts', 'n_genes', color='mt_frac', show=False). ax.tick_params(labelsize=tick_size). fig = plt.gcf(). cbar_ax = fig.axes[-1]. cbar_ax.tick_params(labelsize=tick_size). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/337
https://github.com/scverse/scanpy/issues/337:21,usability,help,helped,21,"> Thanks a lot! This helped to solve my issue. > . > Knowing what to search for is usually gets you there... for future reference, the way to change axis and colorbar ticks is:. > . > ```. > tick_size = 15. > ax = sc.pl.scatter(adata, 'n_counts', 'n_genes', color='mt_frac', show=False). > ax.tick_params(labelsize=tick_size). > fig = plt.gcf(). > cbar_ax = fig.axes[-1]. > cbar_ax.tick_params(labelsize=tick_size). > ```. Thank you very muck. It works well",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/337
https://github.com/scverse/scanpy/issues/337:272,availability,error,error,272,"Hi I am having a similar issue where I would like to set the tick locations on the colorbar. Using similar code as above. ```. ax = sc.pl.tsne(adata, color = 'gene', show=False). fig = plt.gcf(). cbar_ax = fig.axes[-1]. cbar_ax.set_yticks([0,1]). ```. I get the following error 'UserWarning: Use the colorbar set_ticks() method instead'. How can I access the colorbar specifically? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/337
https://github.com/scverse/scanpy/issues/337:368,interoperability,specif,specifically,368,"Hi I am having a similar issue where I would like to set the tick locations on the colorbar. Using similar code as above. ```. ax = sc.pl.tsne(adata, color = 'gene', show=False). fig = plt.gcf(). cbar_ax = fig.axes[-1]. cbar_ax.set_yticks([0,1]). ```. I get the following error 'UserWarning: Use the colorbar set_ticks() method instead'. How can I access the colorbar specifically? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/337
https://github.com/scverse/scanpy/issues/337:272,performance,error,error,272,"Hi I am having a similar issue where I would like to set the tick locations on the colorbar. Using similar code as above. ```. ax = sc.pl.tsne(adata, color = 'gene', show=False). fig = plt.gcf(). cbar_ax = fig.axes[-1]. cbar_ax.set_yticks([0,1]). ```. I get the following error 'UserWarning: Use the colorbar set_ticks() method instead'. How can I access the colorbar specifically? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/337
https://github.com/scverse/scanpy/issues/337:272,safety,error,error,272,"Hi I am having a similar issue where I would like to set the tick locations on the colorbar. Using similar code as above. ```. ax = sc.pl.tsne(adata, color = 'gene', show=False). fig = plt.gcf(). cbar_ax = fig.axes[-1]. cbar_ax.set_yticks([0,1]). ```. I get the following error 'UserWarning: Use the colorbar set_ticks() method instead'. How can I access the colorbar specifically? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/337
https://github.com/scverse/scanpy/issues/337:348,security,access,access,348,"Hi I am having a similar issue where I would like to set the tick locations on the colorbar. Using similar code as above. ```. ax = sc.pl.tsne(adata, color = 'gene', show=False). fig = plt.gcf(). cbar_ax = fig.axes[-1]. cbar_ax.set_yticks([0,1]). ```. I get the following error 'UserWarning: Use the colorbar set_ticks() method instead'. How can I access the colorbar specifically? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/337
https://github.com/scverse/scanpy/issues/337:272,usability,error,error,272,"Hi I am having a similar issue where I would like to set the tick locations on the colorbar. Using similar code as above. ```. ax = sc.pl.tsne(adata, color = 'gene', show=False). fig = plt.gcf(). cbar_ax = fig.axes[-1]. cbar_ax.set_yticks([0,1]). ```. I get the following error 'UserWarning: Use the colorbar set_ticks() method instead'. How can I access the colorbar specifically? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/337
https://github.com/scverse/scanpy/issues/337:279,usability,User,UserWarning,279,"Hi I am having a similar issue where I would like to set the tick locations on the colorbar. Using similar code as above. ```. ax = sc.pl.tsne(adata, color = 'gene', show=False). fig = plt.gcf(). cbar_ax = fig.axes[-1]. cbar_ax.set_yticks([0,1]). ```. I get the following error 'UserWarning: Use the colorbar set_ticks() method instead'. How can I access the colorbar specifically? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/337
https://github.com/scverse/scanpy/issues/338:14,testability,simpl,simply,14,"Hm, how about simply ranking things yourself, like . ```. sort_idcs = np.argsort(adata.var['PCs'][:, 0]). genes_ranked_by_loading_in_PC1 = adata.var_names[sort_idcs]. ```. This is what the plotting functions do internally.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/338
https://github.com/scverse/scanpy/issues/338:14,usability,simpl,simply,14,"Hm, how about simply ranking things yourself, like . ```. sort_idcs = np.argsort(adata.var['PCs'][:, 0]). genes_ranked_by_loading_in_PC1 = adata.var_names[sort_idcs]. ```. This is what the plotting functions do internally.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/338
https://github.com/scverse/scanpy/issues/339:106,deployability,api,api,106,`set_figure_params` does a whole bunch of stuff is `scanpy=True`: https://scanpy.readthedocs.io/en/latest/api/scanpy.api.set_figure_params.html#scanpy.api.set_figure_params. Simply set your favorite `matplotlib.rcParams` directly after calling the function; or don't call the function at all (or with `scanpy=False`) and set all the rcParams yourself.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/339
https://github.com/scverse/scanpy/issues/339:117,deployability,api,api,117,`set_figure_params` does a whole bunch of stuff is `scanpy=True`: https://scanpy.readthedocs.io/en/latest/api/scanpy.api.set_figure_params.html#scanpy.api.set_figure_params. Simply set your favorite `matplotlib.rcParams` directly after calling the function; or don't call the function at all (or with `scanpy=False`) and set all the rcParams yourself.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/339
https://github.com/scverse/scanpy/issues/339:151,deployability,api,api,151,`set_figure_params` does a whole bunch of stuff is `scanpy=True`: https://scanpy.readthedocs.io/en/latest/api/scanpy.api.set_figure_params.html#scanpy.api.set_figure_params. Simply set your favorite `matplotlib.rcParams` directly after calling the function; or don't call the function at all (or with `scanpy=False`) and set all the rcParams yourself.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/339
https://github.com/scverse/scanpy/issues/339:106,integrability,api,api,106,`set_figure_params` does a whole bunch of stuff is `scanpy=True`: https://scanpy.readthedocs.io/en/latest/api/scanpy.api.set_figure_params.html#scanpy.api.set_figure_params. Simply set your favorite `matplotlib.rcParams` directly after calling the function; or don't call the function at all (or with `scanpy=False`) and set all the rcParams yourself.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/339
https://github.com/scverse/scanpy/issues/339:117,integrability,api,api,117,`set_figure_params` does a whole bunch of stuff is `scanpy=True`: https://scanpy.readthedocs.io/en/latest/api/scanpy.api.set_figure_params.html#scanpy.api.set_figure_params. Simply set your favorite `matplotlib.rcParams` directly after calling the function; or don't call the function at all (or with `scanpy=False`) and set all the rcParams yourself.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/339
https://github.com/scverse/scanpy/issues/339:151,integrability,api,api,151,`set_figure_params` does a whole bunch of stuff is `scanpy=True`: https://scanpy.readthedocs.io/en/latest/api/scanpy.api.set_figure_params.html#scanpy.api.set_figure_params. Simply set your favorite `matplotlib.rcParams` directly after calling the function; or don't call the function at all (or with `scanpy=False`) and set all the rcParams yourself.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/339
https://github.com/scverse/scanpy/issues/339:106,interoperability,api,api,106,`set_figure_params` does a whole bunch of stuff is `scanpy=True`: https://scanpy.readthedocs.io/en/latest/api/scanpy.api.set_figure_params.html#scanpy.api.set_figure_params. Simply set your favorite `matplotlib.rcParams` directly after calling the function; or don't call the function at all (or with `scanpy=False`) and set all the rcParams yourself.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/339
https://github.com/scverse/scanpy/issues/339:117,interoperability,api,api,117,`set_figure_params` does a whole bunch of stuff is `scanpy=True`: https://scanpy.readthedocs.io/en/latest/api/scanpy.api.set_figure_params.html#scanpy.api.set_figure_params. Simply set your favorite `matplotlib.rcParams` directly after calling the function; or don't call the function at all (or with `scanpy=False`) and set all the rcParams yourself.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/339
https://github.com/scverse/scanpy/issues/339:151,interoperability,api,api,151,`set_figure_params` does a whole bunch of stuff is `scanpy=True`: https://scanpy.readthedocs.io/en/latest/api/scanpy.api.set_figure_params.html#scanpy.api.set_figure_params. Simply set your favorite `matplotlib.rcParams` directly after calling the function; or don't call the function at all (or with `scanpy=False`) and set all the rcParams yourself.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/339
https://github.com/scverse/scanpy/issues/339:20,reliability,doe,does,20,`set_figure_params` does a whole bunch of stuff is `scanpy=True`: https://scanpy.readthedocs.io/en/latest/api/scanpy.api.set_figure_params.html#scanpy.api.set_figure_params. Simply set your favorite `matplotlib.rcParams` directly after calling the function; or don't call the function at all (or with `scanpy=False`) and set all the rcParams yourself.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/339
https://github.com/scverse/scanpy/issues/339:174,testability,Simpl,Simply,174,`set_figure_params` does a whole bunch of stuff is `scanpy=True`: https://scanpy.readthedocs.io/en/latest/api/scanpy.api.set_figure_params.html#scanpy.api.set_figure_params. Simply set your favorite `matplotlib.rcParams` directly after calling the function; or don't call the function at all (or with `scanpy=False`) and set all the rcParams yourself.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/339
https://github.com/scverse/scanpy/issues/339:174,usability,Simpl,Simply,174,`set_figure_params` does a whole bunch of stuff is `scanpy=True`: https://scanpy.readthedocs.io/en/latest/api/scanpy.api.set_figure_params.html#scanpy.api.set_figure_params. Simply set your favorite `matplotlib.rcParams` directly after calling the function; or don't call the function at all (or with `scanpy=False`) and set all the rcParams yourself.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/339
https://github.com/scverse/scanpy/pull/340:175,availability,down,downsampling,175,"Um, it wasn't me. ```. CondaHTTPError: HTTP 000 CONNECTION FAILED for url <https://conda.anaconda.org/conda-forge/linux-64/xorg-libxdmcp-1.1.2-h470a237_7.tar.bz2>. ```. Also, downsampling from 3785143 finished after an hour, but definitely had the wrong answer (all counts in one gene). I'm not sure what to make of this, since it's given reasonable results for smaller tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/340
https://github.com/scverse/scanpy/pull/340:59,deployability,FAIL,FAILED,59,"Um, it wasn't me. ```. CondaHTTPError: HTTP 000 CONNECTION FAILED for url <https://conda.anaconda.org/conda-forge/linux-64/xorg-libxdmcp-1.1.2-h470a237_7.tar.bz2>. ```. Also, downsampling from 3785143 finished after an hour, but definitely had the wrong answer (all counts in one gene). I'm not sure what to make of this, since it's given reasonable results for smaller tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/340
https://github.com/scverse/scanpy/pull/340:59,reliability,FAIL,FAILED,59,"Um, it wasn't me. ```. CondaHTTPError: HTTP 000 CONNECTION FAILED for url <https://conda.anaconda.org/conda-forge/linux-64/xorg-libxdmcp-1.1.2-h470a237_7.tar.bz2>. ```. Also, downsampling from 3785143 finished after an hour, but definitely had the wrong answer (all counts in one gene). I'm not sure what to make of this, since it's given reasonable results for smaller tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/340
https://github.com/scverse/scanpy/pull/340:370,safety,test,tests,370,"Um, it wasn't me. ```. CondaHTTPError: HTTP 000 CONNECTION FAILED for url <https://conda.anaconda.org/conda-forge/linux-64/xorg-libxdmcp-1.1.2-h470a237_7.tar.bz2>. ```. Also, downsampling from 3785143 finished after an hour, but definitely had the wrong answer (all counts in one gene). I'm not sure what to make of this, since it's given reasonable results for smaller tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/340
https://github.com/scverse/scanpy/pull/340:370,testability,test,tests,370,"Um, it wasn't me. ```. CondaHTTPError: HTTP 000 CONNECTION FAILED for url <https://conda.anaconda.org/conda-forge/linux-64/xorg-libxdmcp-1.1.2-h470a237_7.tar.bz2>. ```. Also, downsampling from 3785143 finished after an hour, but definitely had the wrong answer (all counts in one gene). I'm not sure what to make of this, since it's given reasonable results for smaller tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/340
https://github.com/scverse/scanpy/pull/340:156,performance,time,time,156,"Hey! I wrote this function a while ago... it was definitely not the cleanest or quickest implementation. And it did take a while to run on ~5k cells at the time, but I thought it would be useful to have this functionality in scanpy. Just wanted to note that the intention was definitely to implement this without resampling. I clearly missed that the default was to use resampling in `np.random.choice`. Thanks for spotting this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/340
https://github.com/scverse/scanpy/pull/340:327,usability,clear,clearly,327,"Hey! I wrote this function a while ago... it was definitely not the cleanest or quickest implementation. And it did take a while to run on ~5k cells at the time, but I thought it would be useful to have this functionality in scanpy. Just wanted to note that the intention was definitely to implement this without resampling. I clearly missed that the default was to use resampling in `np.random.choice`. Thanks for spotting this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/340
https://github.com/scverse/scanpy/pull/340:42,deployability,log,logging,42,"That's really cool, thank you! I'll add a logging output about that `replace=False` is the more natural choice and we'll make it the default in the next major release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/340
https://github.com/scverse/scanpy/pull/340:159,deployability,releas,release,159,"That's really cool, thank you! I'll add a logging output about that `replace=False` is the more natural choice and we'll make it the default in the next major release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/340
https://github.com/scverse/scanpy/pull/340:14,energy efficiency,cool,cool,14,"That's really cool, thank you! I'll add a logging output about that `replace=False` is the more natural choice and we'll make it the default in the next major release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/340
https://github.com/scverse/scanpy/pull/340:42,safety,log,logging,42,"That's really cool, thank you! I'll add a logging output about that `replace=False` is the more natural choice and we'll make it the default in the next major release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/340
https://github.com/scverse/scanpy/pull/340:42,security,log,logging,42,"That's really cool, thank you! I'll add a logging output about that `replace=False` is the more natural choice and we'll make it the default in the next major release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/340
https://github.com/scverse/scanpy/pull/340:42,testability,log,logging,42,"That's really cool, thank you! I'll add a logging output about that `replace=False` is the more natural choice and we'll make it the default in the next major release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/340
https://github.com/scverse/scanpy/issues/341:178,deployability,observ,observation,178,"You can do this via `X.obs['PC2'] = X.obsm['X_pca'][:, 1]`, when you don't like PC 2 (when the first component is ""PC1"" and not ""PC0""). `regress_out` accepts as many uni-variate observation annotations as you want, so you can combine this with anything else and as much PCs as you like.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/341
https://github.com/scverse/scanpy/issues/341:101,integrability,compon,component,101,"You can do this via `X.obs['PC2'] = X.obsm['X_pca'][:, 1]`, when you don't like PC 2 (when the first component is ""PC1"" and not ""PC0""). `regress_out` accepts as many uni-variate observation annotations as you want, so you can combine this with anything else and as much PCs as you like.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/341
https://github.com/scverse/scanpy/issues/341:101,interoperability,compon,component,101,"You can do this via `X.obs['PC2'] = X.obsm['X_pca'][:, 1]`, when you don't like PC 2 (when the first component is ""PC1"" and not ""PC0""). `regress_out` accepts as many uni-variate observation annotations as you want, so you can combine this with anything else and as much PCs as you like.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/341
https://github.com/scverse/scanpy/issues/341:101,modifiability,compon,component,101,"You can do this via `X.obs['PC2'] = X.obsm['X_pca'][:, 1]`, when you don't like PC 2 (when the first component is ""PC1"" and not ""PC0""). `regress_out` accepts as many uni-variate observation annotations as you want, so you can combine this with anything else and as much PCs as you like.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/341
https://github.com/scverse/scanpy/issues/341:178,testability,observ,observation,178,"You can do this via `X.obs['PC2'] = X.obsm['X_pca'][:, 1]`, when you don't like PC 2 (when the first component is ""PC1"" and not ""PC0""). `regress_out` accepts as many uni-variate observation annotations as you want, so you can combine this with anything else and as much PCs as you like.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/341
https://github.com/scverse/scanpy/pull/343:26,deployability,build,build,26,Just don't understand why build fails...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/343
https://github.com/scverse/scanpy/pull/343:32,deployability,fail,fails,32,Just don't understand why build fails...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/343
https://github.com/scverse/scanpy/pull/343:32,reliability,fail,fails,32,Just don't understand why build fails...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/343
https://github.com/scverse/scanpy/pull/343:11,testability,understand,understand,11,Just don't understand why build fails...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/343
https://github.com/scverse/scanpy/pull/343:25,deployability,build,build,25,"It seems to be a stalled build in CI. Something to do with a URL request... if the tests run through on your end, everything should be fine. Do they?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/343
https://github.com/scverse/scanpy/pull/343:83,safety,test,tests,83,"It seems to be a stalled build in CI. Something to do with a URL request... if the tests run through on your end, everything should be fine. Do they?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/343
https://github.com/scverse/scanpy/pull/343:83,testability,test,tests,83,"It seems to be a stalled build in CI. Something to do with a URL request... if the tests run through on your end, everything should be fine. Do they?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/343
https://github.com/scverse/scanpy/issues/344:16,deployability,instal,install,16,You may need to install from github rather than wait for new pypi versions.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/344
https://github.com/scverse/scanpy/issues/344:66,deployability,version,versions,66,You may need to install from github rather than wait for new pypi versions.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/344
https://github.com/scverse/scanpy/issues/344:66,integrability,version,versions,66,You may need to install from github rather than wait for new pypi versions.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/344
https://github.com/scverse/scanpy/issues/344:66,modifiability,version,versions,66,You may need to install from github rather than wait for new pypi versions.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/344
https://github.com/scverse/scanpy/issues/344:63,deployability,version,version,63,"It's in 1.3.3, which is out on PyPI. The docs shows the latest version from GitHub by default (`latest`).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/344
https://github.com/scverse/scanpy/issues/344:63,integrability,version,version,63,"It's in 1.3.3, which is out on PyPI. The docs shows the latest version from GitHub by default (`latest`).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/344
https://github.com/scverse/scanpy/issues/344:63,modifiability,version,version,63,"It's in 1.3.3, which is out on PyPI. The docs shows the latest version from GitHub by default (`latest`).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/344
https://github.com/scverse/scanpy/issues/345:268,deployability,scale,scale,268,"I'm sorry but I'm not going to investigate this, it seems that it's highly related to usage `MulticoreTSNE`. Quite generally: UMAP and graph drawing should are the only recommended visualization algorithms these days, given that tSNE tears apart your data and doesn't scale well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/345
https://github.com/scverse/scanpy/issues/345:141,energy efficiency,draw,drawing,141,"I'm sorry but I'm not going to investigate this, it seems that it's highly related to usage `MulticoreTSNE`. Quite generally: UMAP and graph drawing should are the only recommended visualization algorithms these days, given that tSNE tears apart your data and doesn't scale well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/345
https://github.com/scverse/scanpy/issues/345:268,energy efficiency,scale,scale,268,"I'm sorry but I'm not going to investigate this, it seems that it's highly related to usage `MulticoreTSNE`. Quite generally: UMAP and graph drawing should are the only recommended visualization algorithms these days, given that tSNE tears apart your data and doesn't scale well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/345
https://github.com/scverse/scanpy/issues/345:268,modifiability,scal,scale,268,"I'm sorry but I'm not going to investigate this, it seems that it's highly related to usage `MulticoreTSNE`. Quite generally: UMAP and graph drawing should are the only recommended visualization algorithms these days, given that tSNE tears apart your data and doesn't scale well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/345
https://github.com/scverse/scanpy/issues/345:268,performance,scale,scale,268,"I'm sorry but I'm not going to investigate this, it seems that it's highly related to usage `MulticoreTSNE`. Quite generally: UMAP and graph drawing should are the only recommended visualization algorithms these days, given that tSNE tears apart your data and doesn't scale well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/345
https://github.com/scverse/scanpy/issues/345:260,reliability,doe,doesn,260,"I'm sorry but I'm not going to investigate this, it seems that it's highly related to usage `MulticoreTSNE`. Quite generally: UMAP and graph drawing should are the only recommended visualization algorithms these days, given that tSNE tears apart your data and doesn't scale well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/345
https://github.com/scverse/scanpy/issues/345:181,usability,visual,visualization,181,"I'm sorry but I'm not going to investigate this, it seems that it's highly related to usage `MulticoreTSNE`. Quite generally: UMAP and graph drawing should are the only recommended visualization algorithms these days, given that tSNE tears apart your data and doesn't scale well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/345
https://github.com/scverse/scanpy/issues/346:8,testability,simpl,simply,8,"Why not simply as in the [tutorial](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb)? ```. sc.tl.rank_genes_groups(adata, 'louvain', groups=['0'], reference='1'). ```. Or am I missing your problem? A few lines of code documenting your call wouldn't hurt.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/346
https://github.com/scverse/scanpy/issues/346:8,usability,simpl,simply,8,"Why not simply as in the [tutorial](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb)? ```. sc.tl.rank_genes_groups(adata, 'louvain', groups=['0'], reference='1'). ```. Or am I missing your problem? A few lines of code documenting your call wouldn't hurt.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/346
https://github.com/scverse/scanpy/issues/346:267,usability,document,documenting,267,"Why not simply as in the [tutorial](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb)? ```. sc.tl.rank_genes_groups(adata, 'louvain', groups=['0'], reference='1'). ```. Or am I missing your problem? A few lines of code documenting your call wouldn't hurt.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/346
https://github.com/scverse/scanpy/issues/346:208,availability,Cluster,ClusterName,208,"Hi Alex! Sorry for this long delay, I just forgot completely. Maybe I wasn't clear enough in my original post, here is where the issue lies:. . When I run . ```py. sc.tl.rank_genes_groups(adata_f, groupby = 'ClusterName', groups = 'all'). ```. everything is fine and I get my desired result. However, when I just change the reference setting. ```py. sc.tl.rank_genes_groups(adata_f, groupby = 'ClusterName', reference='CA', groups = 'all'). ```. then I get the following error. ```pytb. 100 groups_order = [str(n) for n in groups_order]. 101 if reference != 'rest' and reference not in set(groups_order):. --> 102 groups_order += [reference]. 103 if (reference != 'rest'. 104 and reference not in set(adata.obs[groupby].cat.categories)):. TypeError: must be str, not list. ```. I absolutely understand how to solve this - as you said, I can just use the tutorial call and select groups explicilty. ```py. sc.tl.rank_genes_groups(adata_f, groupby = 'ClusterName', reference='CA', groups = ['OPC', 'Granule']). ```. and then it works again. I was just wondering whether this is the desired behavior - most users will leave the `groups` attribute at it's default setting when they change the `reference` attribute and wonder why it does not work - at least that was my idea. Maybe I am wrong. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/346
https://github.com/scverse/scanpy/issues/346:394,availability,Cluster,ClusterName,394,"Hi Alex! Sorry for this long delay, I just forgot completely. Maybe I wasn't clear enough in my original post, here is where the issue lies:. . When I run . ```py. sc.tl.rank_genes_groups(adata_f, groupby = 'ClusterName', groups = 'all'). ```. everything is fine and I get my desired result. However, when I just change the reference setting. ```py. sc.tl.rank_genes_groups(adata_f, groupby = 'ClusterName', reference='CA', groups = 'all'). ```. then I get the following error. ```pytb. 100 groups_order = [str(n) for n in groups_order]. 101 if reference != 'rest' and reference not in set(groups_order):. --> 102 groups_order += [reference]. 103 if (reference != 'rest'. 104 and reference not in set(adata.obs[groupby].cat.categories)):. TypeError: must be str, not list. ```. I absolutely understand how to solve this - as you said, I can just use the tutorial call and select groups explicilty. ```py. sc.tl.rank_genes_groups(adata_f, groupby = 'ClusterName', reference='CA', groups = ['OPC', 'Granule']). ```. and then it works again. I was just wondering whether this is the desired behavior - most users will leave the `groups` attribute at it's default setting when they change the `reference` attribute and wonder why it does not work - at least that was my idea. Maybe I am wrong. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/346
https://github.com/scverse/scanpy/issues/346:471,availability,error,error,471,"Hi Alex! Sorry for this long delay, I just forgot completely. Maybe I wasn't clear enough in my original post, here is where the issue lies:. . When I run . ```py. sc.tl.rank_genes_groups(adata_f, groupby = 'ClusterName', groups = 'all'). ```. everything is fine and I get my desired result. However, when I just change the reference setting. ```py. sc.tl.rank_genes_groups(adata_f, groupby = 'ClusterName', reference='CA', groups = 'all'). ```. then I get the following error. ```pytb. 100 groups_order = [str(n) for n in groups_order]. 101 if reference != 'rest' and reference not in set(groups_order):. --> 102 groups_order += [reference]. 103 if (reference != 'rest'. 104 and reference not in set(adata.obs[groupby].cat.categories)):. TypeError: must be str, not list. ```. I absolutely understand how to solve this - as you said, I can just use the tutorial call and select groups explicilty. ```py. sc.tl.rank_genes_groups(adata_f, groupby = 'ClusterName', reference='CA', groups = ['OPC', 'Granule']). ```. and then it works again. I was just wondering whether this is the desired behavior - most users will leave the `groups` attribute at it's default setting when they change the `reference` attribute and wonder why it does not work - at least that was my idea. Maybe I am wrong. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/346
https://github.com/scverse/scanpy/issues/346:949,availability,Cluster,ClusterName,949,"Hi Alex! Sorry for this long delay, I just forgot completely. Maybe I wasn't clear enough in my original post, here is where the issue lies:. . When I run . ```py. sc.tl.rank_genes_groups(adata_f, groupby = 'ClusterName', groups = 'all'). ```. everything is fine and I get my desired result. However, when I just change the reference setting. ```py. sc.tl.rank_genes_groups(adata_f, groupby = 'ClusterName', reference='CA', groups = 'all'). ```. then I get the following error. ```pytb. 100 groups_order = [str(n) for n in groups_order]. 101 if reference != 'rest' and reference not in set(groups_order):. --> 102 groups_order += [reference]. 103 if (reference != 'rest'. 104 and reference not in set(adata.obs[groupby].cat.categories)):. TypeError: must be str, not list. ```. I absolutely understand how to solve this - as you said, I can just use the tutorial call and select groups explicilty. ```py. sc.tl.rank_genes_groups(adata_f, groupby = 'ClusterName', reference='CA', groups = ['OPC', 'Granule']). ```. and then it works again. I was just wondering whether this is the desired behavior - most users will leave the `groups` attribute at it's default setting when they change the `reference` attribute and wonder why it does not work - at least that was my idea. Maybe I am wrong. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/346
https://github.com/scverse/scanpy/issues/346:208,deployability,Cluster,ClusterName,208,"Hi Alex! Sorry for this long delay, I just forgot completely. Maybe I wasn't clear enough in my original post, here is where the issue lies:. . When I run . ```py. sc.tl.rank_genes_groups(adata_f, groupby = 'ClusterName', groups = 'all'). ```. everything is fine and I get my desired result. However, when I just change the reference setting. ```py. sc.tl.rank_genes_groups(adata_f, groupby = 'ClusterName', reference='CA', groups = 'all'). ```. then I get the following error. ```pytb. 100 groups_order = [str(n) for n in groups_order]. 101 if reference != 'rest' and reference not in set(groups_order):. --> 102 groups_order += [reference]. 103 if (reference != 'rest'. 104 and reference not in set(adata.obs[groupby].cat.categories)):. TypeError: must be str, not list. ```. I absolutely understand how to solve this - as you said, I can just use the tutorial call and select groups explicilty. ```py. sc.tl.rank_genes_groups(adata_f, groupby = 'ClusterName', reference='CA', groups = ['OPC', 'Granule']). ```. and then it works again. I was just wondering whether this is the desired behavior - most users will leave the `groups` attribute at it's default setting when they change the `reference` attribute and wonder why it does not work - at least that was my idea. Maybe I am wrong. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/346
https://github.com/scverse/scanpy/issues/346:394,deployability,Cluster,ClusterName,394,"Hi Alex! Sorry for this long delay, I just forgot completely. Maybe I wasn't clear enough in my original post, here is where the issue lies:. . When I run . ```py. sc.tl.rank_genes_groups(adata_f, groupby = 'ClusterName', groups = 'all'). ```. everything is fine and I get my desired result. However, when I just change the reference setting. ```py. sc.tl.rank_genes_groups(adata_f, groupby = 'ClusterName', reference='CA', groups = 'all'). ```. then I get the following error. ```pytb. 100 groups_order = [str(n) for n in groups_order]. 101 if reference != 'rest' and reference not in set(groups_order):. --> 102 groups_order += [reference]. 103 if (reference != 'rest'. 104 and reference not in set(adata.obs[groupby].cat.categories)):. TypeError: must be str, not list. ```. I absolutely understand how to solve this - as you said, I can just use the tutorial call and select groups explicilty. ```py. sc.tl.rank_genes_groups(adata_f, groupby = 'ClusterName', reference='CA', groups = ['OPC', 'Granule']). ```. and then it works again. I was just wondering whether this is the desired behavior - most users will leave the `groups` attribute at it's default setting when they change the `reference` attribute and wonder why it does not work - at least that was my idea. Maybe I am wrong. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/346
https://github.com/scverse/scanpy/issues/346:949,deployability,Cluster,ClusterName,949,"Hi Alex! Sorry for this long delay, I just forgot completely. Maybe I wasn't clear enough in my original post, here is where the issue lies:. . When I run . ```py. sc.tl.rank_genes_groups(adata_f, groupby = 'ClusterName', groups = 'all'). ```. everything is fine and I get my desired result. However, when I just change the reference setting. ```py. sc.tl.rank_genes_groups(adata_f, groupby = 'ClusterName', reference='CA', groups = 'all'). ```. then I get the following error. ```pytb. 100 groups_order = [str(n) for n in groups_order]. 101 if reference != 'rest' and reference not in set(groups_order):. --> 102 groups_order += [reference]. 103 if (reference != 'rest'. 104 and reference not in set(adata.obs[groupby].cat.categories)):. TypeError: must be str, not list. ```. I absolutely understand how to solve this - as you said, I can just use the tutorial call and select groups explicilty. ```py. sc.tl.rank_genes_groups(adata_f, groupby = 'ClusterName', reference='CA', groups = ['OPC', 'Granule']). ```. and then it works again. I was just wondering whether this is the desired behavior - most users will leave the `groups` attribute at it's default setting when they change the `reference` attribute and wonder why it does not work - at least that was my idea. Maybe I am wrong. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/346
https://github.com/scverse/scanpy/issues/346:471,performance,error,error,471,"Hi Alex! Sorry for this long delay, I just forgot completely. Maybe I wasn't clear enough in my original post, here is where the issue lies:. . When I run . ```py. sc.tl.rank_genes_groups(adata_f, groupby = 'ClusterName', groups = 'all'). ```. everything is fine and I get my desired result. However, when I just change the reference setting. ```py. sc.tl.rank_genes_groups(adata_f, groupby = 'ClusterName', reference='CA', groups = 'all'). ```. then I get the following error. ```pytb. 100 groups_order = [str(n) for n in groups_order]. 101 if reference != 'rest' and reference not in set(groups_order):. --> 102 groups_order += [reference]. 103 if (reference != 'rest'. 104 and reference not in set(adata.obs[groupby].cat.categories)):. TypeError: must be str, not list. ```. I absolutely understand how to solve this - as you said, I can just use the tutorial call and select groups explicilty. ```py. sc.tl.rank_genes_groups(adata_f, groupby = 'ClusterName', reference='CA', groups = ['OPC', 'Granule']). ```. and then it works again. I was just wondering whether this is the desired behavior - most users will leave the `groups` attribute at it's default setting when they change the `reference` attribute and wonder why it does not work - at least that was my idea. Maybe I am wrong. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/346
https://github.com/scverse/scanpy/issues/346:1229,reliability,doe,does,1229,"Hi Alex! Sorry for this long delay, I just forgot completely. Maybe I wasn't clear enough in my original post, here is where the issue lies:. . When I run . ```py. sc.tl.rank_genes_groups(adata_f, groupby = 'ClusterName', groups = 'all'). ```. everything is fine and I get my desired result. However, when I just change the reference setting. ```py. sc.tl.rank_genes_groups(adata_f, groupby = 'ClusterName', reference='CA', groups = 'all'). ```. then I get the following error. ```pytb. 100 groups_order = [str(n) for n in groups_order]. 101 if reference != 'rest' and reference not in set(groups_order):. --> 102 groups_order += [reference]. 103 if (reference != 'rest'. 104 and reference not in set(adata.obs[groupby].cat.categories)):. TypeError: must be str, not list. ```. I absolutely understand how to solve this - as you said, I can just use the tutorial call and select groups explicilty. ```py. sc.tl.rank_genes_groups(adata_f, groupby = 'ClusterName', reference='CA', groups = ['OPC', 'Granule']). ```. and then it works again. I was just wondering whether this is the desired behavior - most users will leave the `groups` attribute at it's default setting when they change the `reference` attribute and wonder why it does not work - at least that was my idea. Maybe I am wrong. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/346
https://github.com/scverse/scanpy/issues/346:50,safety,compl,completely,50,"Hi Alex! Sorry for this long delay, I just forgot completely. Maybe I wasn't clear enough in my original post, here is where the issue lies:. . When I run . ```py. sc.tl.rank_genes_groups(adata_f, groupby = 'ClusterName', groups = 'all'). ```. everything is fine and I get my desired result. However, when I just change the reference setting. ```py. sc.tl.rank_genes_groups(adata_f, groupby = 'ClusterName', reference='CA', groups = 'all'). ```. then I get the following error. ```pytb. 100 groups_order = [str(n) for n in groups_order]. 101 if reference != 'rest' and reference not in set(groups_order):. --> 102 groups_order += [reference]. 103 if (reference != 'rest'. 104 and reference not in set(adata.obs[groupby].cat.categories)):. TypeError: must be str, not list. ```. I absolutely understand how to solve this - as you said, I can just use the tutorial call and select groups explicilty. ```py. sc.tl.rank_genes_groups(adata_f, groupby = 'ClusterName', reference='CA', groups = ['OPC', 'Granule']). ```. and then it works again. I was just wondering whether this is the desired behavior - most users will leave the `groups` attribute at it's default setting when they change the `reference` attribute and wonder why it does not work - at least that was my idea. Maybe I am wrong. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/346
https://github.com/scverse/scanpy/issues/346:471,safety,error,error,471,"Hi Alex! Sorry for this long delay, I just forgot completely. Maybe I wasn't clear enough in my original post, here is where the issue lies:. . When I run . ```py. sc.tl.rank_genes_groups(adata_f, groupby = 'ClusterName', groups = 'all'). ```. everything is fine and I get my desired result. However, when I just change the reference setting. ```py. sc.tl.rank_genes_groups(adata_f, groupby = 'ClusterName', reference='CA', groups = 'all'). ```. then I get the following error. ```pytb. 100 groups_order = [str(n) for n in groups_order]. 101 if reference != 'rest' and reference not in set(groups_order):. --> 102 groups_order += [reference]. 103 if (reference != 'rest'. 104 and reference not in set(adata.obs[groupby].cat.categories)):. TypeError: must be str, not list. ```. I absolutely understand how to solve this - as you said, I can just use the tutorial call and select groups explicilty. ```py. sc.tl.rank_genes_groups(adata_f, groupby = 'ClusterName', reference='CA', groups = ['OPC', 'Granule']). ```. and then it works again. I was just wondering whether this is the desired behavior - most users will leave the `groups` attribute at it's default setting when they change the `reference` attribute and wonder why it does not work - at least that was my idea. Maybe I am wrong. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/346
https://github.com/scverse/scanpy/issues/346:50,security,compl,completely,50,"Hi Alex! Sorry for this long delay, I just forgot completely. Maybe I wasn't clear enough in my original post, here is where the issue lies:. . When I run . ```py. sc.tl.rank_genes_groups(adata_f, groupby = 'ClusterName', groups = 'all'). ```. everything is fine and I get my desired result. However, when I just change the reference setting. ```py. sc.tl.rank_genes_groups(adata_f, groupby = 'ClusterName', reference='CA', groups = 'all'). ```. then I get the following error. ```pytb. 100 groups_order = [str(n) for n in groups_order]. 101 if reference != 'rest' and reference not in set(groups_order):. --> 102 groups_order += [reference]. 103 if (reference != 'rest'. 104 and reference not in set(adata.obs[groupby].cat.categories)):. TypeError: must be str, not list. ```. I absolutely understand how to solve this - as you said, I can just use the tutorial call and select groups explicilty. ```py. sc.tl.rank_genes_groups(adata_f, groupby = 'ClusterName', reference='CA', groups = ['OPC', 'Granule']). ```. and then it works again. I was just wondering whether this is the desired behavior - most users will leave the `groups` attribute at it's default setting when they change the `reference` attribute and wonder why it does not work - at least that was my idea. Maybe I am wrong. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/346
https://github.com/scverse/scanpy/issues/346:791,testability,understand,understand,791,"Hi Alex! Sorry for this long delay, I just forgot completely. Maybe I wasn't clear enough in my original post, here is where the issue lies:. . When I run . ```py. sc.tl.rank_genes_groups(adata_f, groupby = 'ClusterName', groups = 'all'). ```. everything is fine and I get my desired result. However, when I just change the reference setting. ```py. sc.tl.rank_genes_groups(adata_f, groupby = 'ClusterName', reference='CA', groups = 'all'). ```. then I get the following error. ```pytb. 100 groups_order = [str(n) for n in groups_order]. 101 if reference != 'rest' and reference not in set(groups_order):. --> 102 groups_order += [reference]. 103 if (reference != 'rest'. 104 and reference not in set(adata.obs[groupby].cat.categories)):. TypeError: must be str, not list. ```. I absolutely understand how to solve this - as you said, I can just use the tutorial call and select groups explicilty. ```py. sc.tl.rank_genes_groups(adata_f, groupby = 'ClusterName', reference='CA', groups = ['OPC', 'Granule']). ```. and then it works again. I was just wondering whether this is the desired behavior - most users will leave the `groups` attribute at it's default setting when they change the `reference` attribute and wonder why it does not work - at least that was my idea. Maybe I am wrong. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/346
https://github.com/scverse/scanpy/issues/346:77,usability,clear,clear,77,"Hi Alex! Sorry for this long delay, I just forgot completely. Maybe I wasn't clear enough in my original post, here is where the issue lies:. . When I run . ```py. sc.tl.rank_genes_groups(adata_f, groupby = 'ClusterName', groups = 'all'). ```. everything is fine and I get my desired result. However, when I just change the reference setting. ```py. sc.tl.rank_genes_groups(adata_f, groupby = 'ClusterName', reference='CA', groups = 'all'). ```. then I get the following error. ```pytb. 100 groups_order = [str(n) for n in groups_order]. 101 if reference != 'rest' and reference not in set(groups_order):. --> 102 groups_order += [reference]. 103 if (reference != 'rest'. 104 and reference not in set(adata.obs[groupby].cat.categories)):. TypeError: must be str, not list. ```. I absolutely understand how to solve this - as you said, I can just use the tutorial call and select groups explicilty. ```py. sc.tl.rank_genes_groups(adata_f, groupby = 'ClusterName', reference='CA', groups = ['OPC', 'Granule']). ```. and then it works again. I was just wondering whether this is the desired behavior - most users will leave the `groups` attribute at it's default setting when they change the `reference` attribute and wonder why it does not work - at least that was my idea. Maybe I am wrong. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/346
https://github.com/scverse/scanpy/issues/346:471,usability,error,error,471,"Hi Alex! Sorry for this long delay, I just forgot completely. Maybe I wasn't clear enough in my original post, here is where the issue lies:. . When I run . ```py. sc.tl.rank_genes_groups(adata_f, groupby = 'ClusterName', groups = 'all'). ```. everything is fine and I get my desired result. However, when I just change the reference setting. ```py. sc.tl.rank_genes_groups(adata_f, groupby = 'ClusterName', reference='CA', groups = 'all'). ```. then I get the following error. ```pytb. 100 groups_order = [str(n) for n in groups_order]. 101 if reference != 'rest' and reference not in set(groups_order):. --> 102 groups_order += [reference]. 103 if (reference != 'rest'. 104 and reference not in set(adata.obs[groupby].cat.categories)):. TypeError: must be str, not list. ```. I absolutely understand how to solve this - as you said, I can just use the tutorial call and select groups explicilty. ```py. sc.tl.rank_genes_groups(adata_f, groupby = 'ClusterName', reference='CA', groups = ['OPC', 'Granule']). ```. and then it works again. I was just wondering whether this is the desired behavior - most users will leave the `groups` attribute at it's default setting when they change the `reference` attribute and wonder why it does not work - at least that was my idea. Maybe I am wrong. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/346
https://github.com/scverse/scanpy/issues/346:1088,usability,behavi,behavior,1088,"Hi Alex! Sorry for this long delay, I just forgot completely. Maybe I wasn't clear enough in my original post, here is where the issue lies:. . When I run . ```py. sc.tl.rank_genes_groups(adata_f, groupby = 'ClusterName', groups = 'all'). ```. everything is fine and I get my desired result. However, when I just change the reference setting. ```py. sc.tl.rank_genes_groups(adata_f, groupby = 'ClusterName', reference='CA', groups = 'all'). ```. then I get the following error. ```pytb. 100 groups_order = [str(n) for n in groups_order]. 101 if reference != 'rest' and reference not in set(groups_order):. --> 102 groups_order += [reference]. 103 if (reference != 'rest'. 104 and reference not in set(adata.obs[groupby].cat.categories)):. TypeError: must be str, not list. ```. I absolutely understand how to solve this - as you said, I can just use the tutorial call and select groups explicilty. ```py. sc.tl.rank_genes_groups(adata_f, groupby = 'ClusterName', reference='CA', groups = ['OPC', 'Granule']). ```. and then it works again. I was just wondering whether this is the desired behavior - most users will leave the `groups` attribute at it's default setting when they change the `reference` attribute and wonder why it does not work - at least that was my idea. Maybe I am wrong. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/346
https://github.com/scverse/scanpy/issues/346:1104,usability,user,users,1104,"Hi Alex! Sorry for this long delay, I just forgot completely. Maybe I wasn't clear enough in my original post, here is where the issue lies:. . When I run . ```py. sc.tl.rank_genes_groups(adata_f, groupby = 'ClusterName', groups = 'all'). ```. everything is fine and I get my desired result. However, when I just change the reference setting. ```py. sc.tl.rank_genes_groups(adata_f, groupby = 'ClusterName', reference='CA', groups = 'all'). ```. then I get the following error. ```pytb. 100 groups_order = [str(n) for n in groups_order]. 101 if reference != 'rest' and reference not in set(groups_order):. --> 102 groups_order += [reference]. 103 if (reference != 'rest'. 104 and reference not in set(adata.obs[groupby].cat.categories)):. TypeError: must be str, not list. ```. I absolutely understand how to solve this - as you said, I can just use the tutorial call and select groups explicilty. ```py. sc.tl.rank_genes_groups(adata_f, groupby = 'ClusterName', reference='CA', groups = ['OPC', 'Granule']). ```. and then it works again. I was just wondering whether this is the desired behavior - most users will leave the `groups` attribute at it's default setting when they change the `reference` attribute and wonder why it does not work - at least that was my idea. Maybe I am wrong. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/346
https://github.com/scverse/scanpy/issues/346:129,usability,behavi,behavior,129,"I also had this issue with scanpy 1.6.0, which is fixable in the way mentioned by Marius1311, but I agree it is not the expected behavior",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/346
https://github.com/scverse/scanpy/issues/347:102,availability,cluster,clustering,102,"Hey! So you used `restrict_to = ('louvain', ['1'])` if you wanted to recluster '1'? You'll have a new clustering variable `louvain_R` in this case, which you can rename anything you like... checkout the documentation: https://icb-scanpy.readthedocs-hosted.com/en/latest/api/scanpy.api.tl.louvain.html. I guess that this is what you want.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/347
https://github.com/scverse/scanpy/issues/347:102,deployability,cluster,clustering,102,"Hey! So you used `restrict_to = ('louvain', ['1'])` if you wanted to recluster '1'? You'll have a new clustering variable `louvain_R` in this case, which you can rename anything you like... checkout the documentation: https://icb-scanpy.readthedocs-hosted.com/en/latest/api/scanpy.api.tl.louvain.html. I guess that this is what you want.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/347
https://github.com/scverse/scanpy/issues/347:270,deployability,api,api,270,"Hey! So you used `restrict_to = ('louvain', ['1'])` if you wanted to recluster '1'? You'll have a new clustering variable `louvain_R` in this case, which you can rename anything you like... checkout the documentation: https://icb-scanpy.readthedocs-hosted.com/en/latest/api/scanpy.api.tl.louvain.html. I guess that this is what you want.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/347
https://github.com/scverse/scanpy/issues/347:281,deployability,api,api,281,"Hey! So you used `restrict_to = ('louvain', ['1'])` if you wanted to recluster '1'? You'll have a new clustering variable `louvain_R` in this case, which you can rename anything you like... checkout the documentation: https://icb-scanpy.readthedocs-hosted.com/en/latest/api/scanpy.api.tl.louvain.html. I guess that this is what you want.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/347
https://github.com/scverse/scanpy/issues/347:270,integrability,api,api,270,"Hey! So you used `restrict_to = ('louvain', ['1'])` if you wanted to recluster '1'? You'll have a new clustering variable `louvain_R` in this case, which you can rename anything you like... checkout the documentation: https://icb-scanpy.readthedocs-hosted.com/en/latest/api/scanpy.api.tl.louvain.html. I guess that this is what you want.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/347
https://github.com/scverse/scanpy/issues/347:281,integrability,api,api,281,"Hey! So you used `restrict_to = ('louvain', ['1'])` if you wanted to recluster '1'? You'll have a new clustering variable `louvain_R` in this case, which you can rename anything you like... checkout the documentation: https://icb-scanpy.readthedocs-hosted.com/en/latest/api/scanpy.api.tl.louvain.html. I guess that this is what you want.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/347
https://github.com/scverse/scanpy/issues/347:270,interoperability,api,api,270,"Hey! So you used `restrict_to = ('louvain', ['1'])` if you wanted to recluster '1'? You'll have a new clustering variable `louvain_R` in this case, which you can rename anything you like... checkout the documentation: https://icb-scanpy.readthedocs-hosted.com/en/latest/api/scanpy.api.tl.louvain.html. I guess that this is what you want.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/347
https://github.com/scverse/scanpy/issues/347:281,interoperability,api,api,281,"Hey! So you used `restrict_to = ('louvain', ['1'])` if you wanted to recluster '1'? You'll have a new clustering variable `louvain_R` in this case, which you can rename anything you like... checkout the documentation: https://icb-scanpy.readthedocs-hosted.com/en/latest/api/scanpy.api.tl.louvain.html. I guess that this is what you want.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/347
https://github.com/scverse/scanpy/issues/347:113,modifiability,variab,variable,113,"Hey! So you used `restrict_to = ('louvain', ['1'])` if you wanted to recluster '1'? You'll have a new clustering variable `louvain_R` in this case, which you can rename anything you like... checkout the documentation: https://icb-scanpy.readthedocs-hosted.com/en/latest/api/scanpy.api.tl.louvain.html. I guess that this is what you want.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/347
https://github.com/scverse/scanpy/issues/347:203,usability,document,documentation,203,"Hey! So you used `restrict_to = ('louvain', ['1'])` if you wanted to recluster '1'? You'll have a new clustering variable `louvain_R` in this case, which you can rename anything you like... checkout the documentation: https://icb-scanpy.readthedocs-hosted.com/en/latest/api/scanpy.api.tl.louvain.html. I guess that this is what you want.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/347
https://github.com/scverse/scanpy/issues/347:56,usability,document,documentation,56,"It works perfectly, don't know how I missed that in the documentation...thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/347
https://github.com/scverse/scanpy/issues/348:92,deployability,contain,contained,92,Can you help me with a quick example to reproduce the problem? You can use example datasets contained in scanpy.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/348
https://github.com/scverse/scanpy/issues/348:8,usability,help,help,8,Can you help me with a quick example to reproduce the problem? You can use example datasets contained in scanpy.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/348
https://github.com/scverse/scanpy/issues/348:63,availability,avail,available,63,"Can we reopen this issue? I still don't see this functionality available. The ask is to be able to specify the number of rows or columns for the arrangement of the output panels from `sc.pl.violin`. Right now if I plot 8 genes, for example, they all show up on one row, yielding tiny plots. It would be nice to be able to pass in something like `ncols=4` so that the 8 panels will be arranged as a 2x4 instead of a 1x8.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/348
https://github.com/scverse/scanpy/issues/348:99,interoperability,specif,specify,99,"Can we reopen this issue? I still don't see this functionality available. The ask is to be able to specify the number of rows or columns for the arrangement of the output panels from `sc.pl.violin`. Right now if I plot 8 genes, for example, they all show up on one row, yielding tiny plots. It would be nice to be able to pass in something like `ncols=4` so that the 8 panels will be arranged as a 2x4 instead of a 1x8.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/348
https://github.com/scverse/scanpy/issues/348:63,reliability,availab,available,63,"Can we reopen this issue? I still don't see this functionality available. The ask is to be able to specify the number of rows or columns for the arrangement of the output panels from `sc.pl.violin`. Right now if I plot 8 genes, for example, they all show up on one row, yielding tiny plots. It would be nice to be able to pass in something like `ncols=4` so that the 8 panels will be arranged as a 2x4 instead of a 1x8.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/348
https://github.com/scverse/scanpy/issues/348:63,safety,avail,available,63,"Can we reopen this issue? I still don't see this functionality available. The ask is to be able to specify the number of rows or columns for the arrangement of the output panels from `sc.pl.violin`. Right now if I plot 8 genes, for example, they all show up on one row, yielding tiny plots. It would be nice to be able to pass in something like `ncols=4` so that the 8 panels will be arranged as a 2x4 instead of a 1x8.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/348
https://github.com/scverse/scanpy/issues/348:63,security,availab,available,63,"Can we reopen this issue? I still don't see this functionality available. The ask is to be able to specify the number of rows or columns for the arrangement of the output panels from `sc.pl.violin`. Right now if I plot 8 genes, for example, they all show up on one row, yielding tiny plots. It would be nice to be able to pass in something like `ncols=4` so that the 8 panels will be arranged as a 2x4 instead of a 1x8.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/348
https://github.com/scverse/scanpy/issues/348:84,usability,close,closed,84,@rpeys I think that might be better as a new feature request. I'd like to keep this closed since the bug that was reported here has been fixed.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/348
https://github.com/scverse/scanpy/issues/349:107,availability,avail,available,107,"With respect to the heatmap, indeed it is possible to transpose the matrix. Currently, this option is only available for `stacked_violin`. I thought. about adding this option to other plots like heatmap, matrixplot and. dotplot but I have not find the time and it is always possible to save the. figure and rotate it so it has low priority for me. The changes are not as. trivial as simply rotating the matrix as all other elements need to be. adjusted. On Wed, Nov 7, 2018 at 3:03 AM Alex Wolf <notifications@github.com> wrote:. > @fidelram <https://github.com/fidelram> should be the expert for this... > . >. > . > You are receiving this because you were mentioned. >. >. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/349#issuecomment-436477272>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1b_dlMN1mihuJIbXg2lPmMJvgqGgks5usj-FgaJpZM4YRQ7g>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/349
https://github.com/scverse/scanpy/issues/349:20,energy efficiency,heat,heatmap,20,"With respect to the heatmap, indeed it is possible to transpose the matrix. Currently, this option is only available for `stacked_violin`. I thought. about adding this option to other plots like heatmap, matrixplot and. dotplot but I have not find the time and it is always possible to save the. figure and rotate it so it has low priority for me. The changes are not as. trivial as simply rotating the matrix as all other elements need to be. adjusted. On Wed, Nov 7, 2018 at 3:03 AM Alex Wolf <notifications@github.com> wrote:. > @fidelram <https://github.com/fidelram> should be the expert for this... > . >. > . > You are receiving this because you were mentioned. >. >. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/349#issuecomment-436477272>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1b_dlMN1mihuJIbXg2lPmMJvgqGgks5usj-FgaJpZM4YRQ7g>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/349
https://github.com/scverse/scanpy/issues/349:76,energy efficiency,Current,Currently,76,"With respect to the heatmap, indeed it is possible to transpose the matrix. Currently, this option is only available for `stacked_violin`. I thought. about adding this option to other plots like heatmap, matrixplot and. dotplot but I have not find the time and it is always possible to save the. figure and rotate it so it has low priority for me. The changes are not as. trivial as simply rotating the matrix as all other elements need to be. adjusted. On Wed, Nov 7, 2018 at 3:03 AM Alex Wolf <notifications@github.com> wrote:. > @fidelram <https://github.com/fidelram> should be the expert for this... > . >. > . > You are receiving this because you were mentioned. >. >. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/349#issuecomment-436477272>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1b_dlMN1mihuJIbXg2lPmMJvgqGgks5usj-FgaJpZM4YRQ7g>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/349
https://github.com/scverse/scanpy/issues/349:195,energy efficiency,heat,heatmap,195,"With respect to the heatmap, indeed it is possible to transpose the matrix. Currently, this option is only available for `stacked_violin`. I thought. about adding this option to other plots like heatmap, matrixplot and. dotplot but I have not find the time and it is always possible to save the. figure and rotate it so it has low priority for me. The changes are not as. trivial as simply rotating the matrix as all other elements need to be. adjusted. On Wed, Nov 7, 2018 at 3:03 AM Alex Wolf <notifications@github.com> wrote:. > @fidelram <https://github.com/fidelram> should be the expert for this... > . >. > . > You are receiving this because you were mentioned. >. >. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/349#issuecomment-436477272>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1b_dlMN1mihuJIbXg2lPmMJvgqGgks5usj-FgaJpZM4YRQ7g>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/349
https://github.com/scverse/scanpy/issues/349:252,performance,time,time,252,"With respect to the heatmap, indeed it is possible to transpose the matrix. Currently, this option is only available for `stacked_violin`. I thought. about adding this option to other plots like heatmap, matrixplot and. dotplot but I have not find the time and it is always possible to save the. figure and rotate it so it has low priority for me. The changes are not as. trivial as simply rotating the matrix as all other elements need to be. adjusted. On Wed, Nov 7, 2018 at 3:03 AM Alex Wolf <notifications@github.com> wrote:. > @fidelram <https://github.com/fidelram> should be the expert for this... > . >. > . > You are receiving this because you were mentioned. >. >. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/349#issuecomment-436477272>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1b_dlMN1mihuJIbXg2lPmMJvgqGgks5usj-FgaJpZM4YRQ7g>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/349
https://github.com/scverse/scanpy/issues/349:107,reliability,availab,available,107,"With respect to the heatmap, indeed it is possible to transpose the matrix. Currently, this option is only available for `stacked_violin`. I thought. about adding this option to other plots like heatmap, matrixplot and. dotplot but I have not find the time and it is always possible to save the. figure and rotate it so it has low priority for me. The changes are not as. trivial as simply rotating the matrix as all other elements need to be. adjusted. On Wed, Nov 7, 2018 at 3:03 AM Alex Wolf <notifications@github.com> wrote:. > @fidelram <https://github.com/fidelram> should be the expert for this... > . >. > . > You are receiving this because you were mentioned. >. >. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/349#issuecomment-436477272>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1b_dlMN1mihuJIbXg2lPmMJvgqGgks5usj-FgaJpZM4YRQ7g>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/349
https://github.com/scverse/scanpy/issues/349:107,safety,avail,available,107,"With respect to the heatmap, indeed it is possible to transpose the matrix. Currently, this option is only available for `stacked_violin`. I thought. about adding this option to other plots like heatmap, matrixplot and. dotplot but I have not find the time and it is always possible to save the. figure and rotate it so it has low priority for me. The changes are not as. trivial as simply rotating the matrix as all other elements need to be. adjusted. On Wed, Nov 7, 2018 at 3:03 AM Alex Wolf <notifications@github.com> wrote:. > @fidelram <https://github.com/fidelram> should be the expert for this... > . >. > . > You are receiving this because you were mentioned. >. >. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/349#issuecomment-436477272>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1b_dlMN1mihuJIbXg2lPmMJvgqGgks5usj-FgaJpZM4YRQ7g>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/349
https://github.com/scverse/scanpy/issues/349:107,security,availab,available,107,"With respect to the heatmap, indeed it is possible to transpose the matrix. Currently, this option is only available for `stacked_violin`. I thought. about adding this option to other plots like heatmap, matrixplot and. dotplot but I have not find the time and it is always possible to save the. figure and rotate it so it has low priority for me. The changes are not as. trivial as simply rotating the matrix as all other elements need to be. adjusted. On Wed, Nov 7, 2018 at 3:03 AM Alex Wolf <notifications@github.com> wrote:. > @fidelram <https://github.com/fidelram> should be the expert for this... > . >. > . > You are receiving this because you were mentioned. >. >. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/349#issuecomment-436477272>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1b_dlMN1mihuJIbXg2lPmMJvgqGgks5usj-FgaJpZM4YRQ7g>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/349
https://github.com/scverse/scanpy/issues/349:307,security,rotat,rotate,307,"With respect to the heatmap, indeed it is possible to transpose the matrix. Currently, this option is only available for `stacked_violin`. I thought. about adding this option to other plots like heatmap, matrixplot and. dotplot but I have not find the time and it is always possible to save the. figure and rotate it so it has low priority for me. The changes are not as. trivial as simply rotating the matrix as all other elements need to be. adjusted. On Wed, Nov 7, 2018 at 3:03 AM Alex Wolf <notifications@github.com> wrote:. > @fidelram <https://github.com/fidelram> should be the expert for this... > . >. > . > You are receiving this because you were mentioned. >. >. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/349#issuecomment-436477272>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1b_dlMN1mihuJIbXg2lPmMJvgqGgks5usj-FgaJpZM4YRQ7g>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/349
https://github.com/scverse/scanpy/issues/349:390,security,rotat,rotating,390,"With respect to the heatmap, indeed it is possible to transpose the matrix. Currently, this option is only available for `stacked_violin`. I thought. about adding this option to other plots like heatmap, matrixplot and. dotplot but I have not find the time and it is always possible to save the. figure and rotate it so it has low priority for me. The changes are not as. trivial as simply rotating the matrix as all other elements need to be. adjusted. On Wed, Nov 7, 2018 at 3:03 AM Alex Wolf <notifications@github.com> wrote:. > @fidelram <https://github.com/fidelram> should be the expert for this... > . >. > . > You are receiving this because you were mentioned. >. >. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/349#issuecomment-436477272>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1b_dlMN1mihuJIbXg2lPmMJvgqGgks5usj-FgaJpZM4YRQ7g>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/349
https://github.com/scverse/scanpy/issues/349:873,security,auth,auth,873,"With respect to the heatmap, indeed it is possible to transpose the matrix. Currently, this option is only available for `stacked_violin`. I thought. about adding this option to other plots like heatmap, matrixplot and. dotplot but I have not find the time and it is always possible to save the. figure and rotate it so it has low priority for me. The changes are not as. trivial as simply rotating the matrix as all other elements need to be. adjusted. On Wed, Nov 7, 2018 at 3:03 AM Alex Wolf <notifications@github.com> wrote:. > @fidelram <https://github.com/fidelram> should be the expert for this... > . >. > . > You are receiving this because you were mentioned. >. >. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/349#issuecomment-436477272>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1b_dlMN1mihuJIbXg2lPmMJvgqGgks5usj-FgaJpZM4YRQ7g>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/349
https://github.com/scverse/scanpy/issues/349:383,testability,simpl,simply,383,"With respect to the heatmap, indeed it is possible to transpose the matrix. Currently, this option is only available for `stacked_violin`. I thought. about adding this option to other plots like heatmap, matrixplot and. dotplot but I have not find the time and it is always possible to save the. figure and rotate it so it has low priority for me. The changes are not as. trivial as simply rotating the matrix as all other elements need to be. adjusted. On Wed, Nov 7, 2018 at 3:03 AM Alex Wolf <notifications@github.com> wrote:. > @fidelram <https://github.com/fidelram> should be the expert for this... > . >. > . > You are receiving this because you were mentioned. >. >. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/349#issuecomment-436477272>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1b_dlMN1mihuJIbXg2lPmMJvgqGgks5usj-FgaJpZM4YRQ7g>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/349
https://github.com/scverse/scanpy/issues/349:383,usability,simpl,simply,383,"With respect to the heatmap, indeed it is possible to transpose the matrix. Currently, this option is only available for `stacked_violin`. I thought. about adding this option to other plots like heatmap, matrixplot and. dotplot but I have not find the time and it is always possible to save the. figure and rotate it so it has low priority for me. The changes are not as. trivial as simply rotating the matrix as all other elements need to be. adjusted. On Wed, Nov 7, 2018 at 3:03 AM Alex Wolf <notifications@github.com> wrote:. > @fidelram <https://github.com/fidelram> should be the expert for this... > . >. > . > You are receiving this because you were mentioned. >. >. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/349#issuecomment-436477272>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1b_dlMN1mihuJIbXg2lPmMJvgqGgks5usj-FgaJpZM4YRQ7g>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/349
https://github.com/scverse/scanpy/issues/349:215,safety,input,input,215,I made a hack solution for this to have something that would work for me. I put it here: https://gist.github.com/Xparx/33026da63dabb1c200b2602bbae0b95c. It's not defensively coded so it might bug out without proper input.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/349
https://github.com/scverse/scanpy/issues/349:9,security,hack,hack,9,I made a hack solution for this to have something that would work for me. I put it here: https://gist.github.com/Xparx/33026da63dabb1c200b2602bbae0b95c. It's not defensively coded so it might bug out without proper input.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/349
https://github.com/scverse/scanpy/issues/349:215,usability,input,input,215,I made a hack solution for this to have something that would work for me. I put it here: https://gist.github.com/Xparx/33026da63dabb1c200b2602bbae0b95c. It's not defensively coded so it might bug out without proper input.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/349
https://github.com/scverse/scanpy/issues/349:62,usability,user,user-images,62,Sure. Here is one example of my output image. ![4hpf](https://user-images.githubusercontent.com/715716/48497219-b68b6480-e801-11e8-9d80-2cf9219af59e.png).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/349
https://github.com/scverse/scanpy/issues/349:45,energy efficiency,heat,heatmap,45,Thanks. I will do a PR soon which allows the heatmap and matrixplot functions to be rotated.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/349
https://github.com/scverse/scanpy/issues/349:84,security,rotat,rotated,84,Thanks. I will do a PR soon which allows the heatmap and matrixplot functions to be rotated.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/349
https://github.com/scverse/scanpy/issues/349:197,availability,avail,availible,197,"As this issue is not closed I'll add a question here. . Is it possible, or if not could it be added, that the cells in the heatmaps are sorted within the groupby variables. Either by pseudotime if availible or just clustered simply by hierarchical clustering. This could add a more visually and intuitive pleasing ordering of cells. For example as in my figure above the groupby miss some property of the data with pattern over cells. If the cell ordering was random or default this pattern could not be seen.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/349
https://github.com/scverse/scanpy/issues/349:215,availability,cluster,clustered,215,"As this issue is not closed I'll add a question here. . Is it possible, or if not could it be added, that the cells in the heatmaps are sorted within the groupby variables. Either by pseudotime if availible or just clustered simply by hierarchical clustering. This could add a more visually and intuitive pleasing ordering of cells. For example as in my figure above the groupby miss some property of the data with pattern over cells. If the cell ordering was random or default this pattern could not be seen.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/349
https://github.com/scverse/scanpy/issues/349:248,availability,cluster,clustering,248,"As this issue is not closed I'll add a question here. . Is it possible, or if not could it be added, that the cells in the heatmaps are sorted within the groupby variables. Either by pseudotime if availible or just clustered simply by hierarchical clustering. This could add a more visually and intuitive pleasing ordering of cells. For example as in my figure above the groupby miss some property of the data with pattern over cells. If the cell ordering was random or default this pattern could not be seen.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/349
https://github.com/scverse/scanpy/issues/349:215,deployability,cluster,clustered,215,"As this issue is not closed I'll add a question here. . Is it possible, or if not could it be added, that the cells in the heatmaps are sorted within the groupby variables. Either by pseudotime if availible or just clustered simply by hierarchical clustering. This could add a more visually and intuitive pleasing ordering of cells. For example as in my figure above the groupby miss some property of the data with pattern over cells. If the cell ordering was random or default this pattern could not be seen.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/349
https://github.com/scverse/scanpy/issues/349:248,deployability,cluster,clustering,248,"As this issue is not closed I'll add a question here. . Is it possible, or if not could it be added, that the cells in the heatmaps are sorted within the groupby variables. Either by pseudotime if availible or just clustered simply by hierarchical clustering. This could add a more visually and intuitive pleasing ordering of cells. For example as in my figure above the groupby miss some property of the data with pattern over cells. If the cell ordering was random or default this pattern could not be seen.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/349
https://github.com/scverse/scanpy/issues/349:123,energy efficiency,heat,heatmaps,123,"As this issue is not closed I'll add a question here. . Is it possible, or if not could it be added, that the cells in the heatmaps are sorted within the groupby variables. Either by pseudotime if availible or just clustered simply by hierarchical clustering. This could add a more visually and intuitive pleasing ordering of cells. For example as in my figure above the groupby miss some property of the data with pattern over cells. If the cell ordering was random or default this pattern could not be seen.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/349
https://github.com/scverse/scanpy/issues/349:162,modifiability,variab,variables,162,"As this issue is not closed I'll add a question here. . Is it possible, or if not could it be added, that the cells in the heatmaps are sorted within the groupby variables. Either by pseudotime if availible or just clustered simply by hierarchical clustering. This could add a more visually and intuitive pleasing ordering of cells. For example as in my figure above the groupby miss some property of the data with pattern over cells. If the cell ordering was random or default this pattern could not be seen.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/349
https://github.com/scverse/scanpy/issues/349:197,safety,avail,availible,197,"As this issue is not closed I'll add a question here. . Is it possible, or if not could it be added, that the cells in the heatmaps are sorted within the groupby variables. Either by pseudotime if availible or just clustered simply by hierarchical clustering. This could add a more visually and intuitive pleasing ordering of cells. For example as in my figure above the groupby miss some property of the data with pattern over cells. If the cell ordering was random or default this pattern could not be seen.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/349
https://github.com/scverse/scanpy/issues/349:225,testability,simpl,simply,225,"As this issue is not closed I'll add a question here. . Is it possible, or if not could it be added, that the cells in the heatmaps are sorted within the groupby variables. Either by pseudotime if availible or just clustered simply by hierarchical clustering. This could add a more visually and intuitive pleasing ordering of cells. For example as in my figure above the groupby miss some property of the data with pattern over cells. If the cell ordering was random or default this pattern could not be seen.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/349
https://github.com/scverse/scanpy/issues/349:21,usability,close,closed,21,"As this issue is not closed I'll add a question here. . Is it possible, or if not could it be added, that the cells in the heatmaps are sorted within the groupby variables. Either by pseudotime if availible or just clustered simply by hierarchical clustering. This could add a more visually and intuitive pleasing ordering of cells. For example as in my figure above the groupby miss some property of the data with pattern over cells. If the cell ordering was random or default this pattern could not be seen.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/349
https://github.com/scverse/scanpy/issues/349:225,usability,simpl,simply,225,"As this issue is not closed I'll add a question here. . Is it possible, or if not could it be added, that the cells in the heatmaps are sorted within the groupby variables. Either by pseudotime if availible or just clustered simply by hierarchical clustering. This could add a more visually and intuitive pleasing ordering of cells. For example as in my figure above the groupby miss some property of the data with pattern over cells. If the cell ordering was random or default this pattern could not be seen.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/349
https://github.com/scverse/scanpy/issues/349:282,usability,visual,visually,282,"As this issue is not closed I'll add a question here. . Is it possible, or if not could it be added, that the cells in the heatmaps are sorted within the groupby variables. Either by pseudotime if availible or just clustered simply by hierarchical clustering. This could add a more visually and intuitive pleasing ordering of cells. For example as in my figure above the groupby miss some property of the data with pattern over cells. If the cell ordering was random or default this pattern could not be seen.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/349
https://github.com/scverse/scanpy/issues/349:295,usability,intuit,intuitive,295,"As this issue is not closed I'll add a question here. . Is it possible, or if not could it be added, that the cells in the heatmaps are sorted within the groupby variables. Either by pseudotime if availible or just clustered simply by hierarchical clustering. This could add a more visually and intuitive pleasing ordering of cells. For example as in my figure above the groupby miss some property of the data with pattern over cells. If the cell ordering was random or default this pattern could not be seen.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/349
https://github.com/scverse/scanpy/issues/349:47,energy efficiency,heat,heatmap,47,"Should be possible to do what you want. `sc.pl.heatmap` basically plots a pandas DataFrame that has as columns the selected gene names, and as rows the cells with an index based on the categories. . You can modify this dataframe (called `obs_tidy`) to have the ordering that you want. This is the line in the current code that returns the dataframe: https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_anndata.py#L1098",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/349
https://github.com/scverse/scanpy/issues/349:309,energy efficiency,current,current,309,"Should be possible to do what you want. `sc.pl.heatmap` basically plots a pandas DataFrame that has as columns the selected gene names, and as rows the cells with an index based on the categories. . You can modify this dataframe (called `obs_tidy`) to have the ordering that you want. This is the line in the current code that returns the dataframe: https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_anndata.py#L1098",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/349
https://github.com/scverse/scanpy/issues/349:207,security,modif,modify,207,"Should be possible to do what you want. `sc.pl.heatmap` basically plots a pandas DataFrame that has as columns the selected gene names, and as rows the cells with an index based on the categories. . You can modify this dataframe (called `obs_tidy`) to have the ordering that you want. This is the line in the current code that returns the dataframe: https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_anndata.py#L1098",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/349
https://github.com/scverse/scanpy/issues/349:71,availability,cluster,clustering,71,"Again, as this issue is not closed,. is it possible to do hierarchical clustering on the genes while plotting the heamap? By grouping the genes according their expression in the clusters would be a lot informative.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/349
https://github.com/scverse/scanpy/issues/349:178,availability,cluster,clusters,178,"Again, as this issue is not closed,. is it possible to do hierarchical clustering on the genes while plotting the heamap? By grouping the genes according their expression in the clusters would be a lot informative.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/349
https://github.com/scverse/scanpy/issues/349:71,deployability,cluster,clustering,71,"Again, as this issue is not closed,. is it possible to do hierarchical clustering on the genes while plotting the heamap? By grouping the genes according their expression in the clusters would be a lot informative.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/349
https://github.com/scverse/scanpy/issues/349:178,deployability,cluster,clusters,178,"Again, as this issue is not closed,. is it possible to do hierarchical clustering on the genes while plotting the heamap? By grouping the genes according their expression in the clusters would be a lot informative.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/349
https://github.com/scverse/scanpy/issues/349:28,usability,close,closed,28,"Again, as this issue is not closed,. is it possible to do hierarchical clustering on the genes while plotting the heamap? By grouping the genes according their expression in the clusters would be a lot informative.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/349
https://github.com/scverse/scanpy/issues/350:239,availability,cluster,clustering,239,"Thanks for the information. Very interesting read. From the paper it seems that the Leiden method indeed is superior to the louvain method. However, instead of replacing the louvain function I think is better to explicitly create a leiden clustering function as you have done and you can add the n_iterations parameter. . I would suggest to add a PR with this function as part of tools, but because of the new dependency required I am not sure. @falexwolf any suggestions? Meanwhile I will give it a try.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:239,deployability,cluster,clustering,239,"Thanks for the information. Very interesting read. From the paper it seems that the Leiden method indeed is superior to the louvain method. However, instead of replacing the louvain function I think is better to explicitly create a leiden clustering function as you have done and you can add the n_iterations parameter. . I would suggest to add a PR with this function as part of tools, but because of the new dependency required I am not sure. @falexwolf any suggestions? Meanwhile I will give it a try.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:410,deployability,depend,dependency,410,"Thanks for the information. Very interesting read. From the paper it seems that the Leiden method indeed is superior to the louvain method. However, instead of replacing the louvain function I think is better to explicitly create a leiden clustering function as you have done and you can add the n_iterations parameter. . I would suggest to add a PR with this function as part of tools, but because of the new dependency required I am not sure. @falexwolf any suggestions? Meanwhile I will give it a try.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:410,integrability,depend,dependency,410,"Thanks for the information. Very interesting read. From the paper it seems that the Leiden method indeed is superior to the louvain method. However, instead of replacing the louvain function I think is better to explicitly create a leiden clustering function as you have done and you can add the n_iterations parameter. . I would suggest to add a PR with this function as part of tools, but because of the new dependency required I am not sure. @falexwolf any suggestions? Meanwhile I will give it a try.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:309,modifiability,paramet,parameter,309,"Thanks for the information. Very interesting read. From the paper it seems that the Leiden method indeed is superior to the louvain method. However, instead of replacing the louvain function I think is better to explicitly create a leiden clustering function as you have done and you can add the n_iterations parameter. . I would suggest to add a PR with this function as part of tools, but because of the new dependency required I am not sure. @falexwolf any suggestions? Meanwhile I will give it a try.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:410,modifiability,depend,dependency,410,"Thanks for the information. Very interesting read. From the paper it seems that the Leiden method indeed is superior to the louvain method. However, instead of replacing the louvain function I think is better to explicitly create a leiden clustering function as you have done and you can add the n_iterations parameter. . I would suggest to add a PR with this function as part of tools, but because of the new dependency required I am not sure. @falexwolf any suggestions? Meanwhile I will give it a try.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:410,safety,depend,dependency,410,"Thanks for the information. Very interesting read. From the paper it seems that the Leiden method indeed is superior to the louvain method. However, instead of replacing the louvain function I think is better to explicitly create a leiden clustering function as you have done and you can add the n_iterations parameter. . I would suggest to add a PR with this function as part of tools, but because of the new dependency required I am not sure. @falexwolf any suggestions? Meanwhile I will give it a try.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:410,testability,depend,dependency,410,"Thanks for the information. Very interesting read. From the paper it seems that the Leiden method indeed is superior to the louvain method. However, instead of replacing the louvain function I think is better to explicitly create a leiden clustering function as you have done and you can add the n_iterations parameter. . I would suggest to add a PR with this function as part of tools, but because of the new dependency required I am not sure. @falexwolf any suggestions? Meanwhile I will give it a try.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:380,usability,tool,tools,380,"Thanks for the information. Very interesting read. From the paper it seems that the Leiden method indeed is superior to the louvain method. However, instead of replacing the louvain function I think is better to explicitly create a leiden clustering function as you have done and you can add the n_iterations parameter. . I would suggest to add a PR with this function as part of tools, but because of the new dependency required I am not sure. @falexwolf any suggestions? Meanwhile I will give it a try.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:236,modifiability,pac,package,236,"In that case, it'd probably be easiest to just add a `'leiden'` flavour to `sc.tl.louvain()`. Due to the overlap of the syntax, you may even get away with explicitly recycling the existing Louvain code in the function and just swap the package behind the scenes:. 	if flavor == 'vtraag':. 		import louvain. 	elif flavor == 'leiden':. 		import leidenalg as louvain",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:550,availability,error,errors,550,"I mean, @vtraag is is the person Id believe when asked which algorithm is superior, so we could. 1. add `sc.tl.leiden` as an alternative that doesnt have a flavour argument. 2. make `leidenalg` a dependency and `louvain-igraph` an optional one. 3. when calling `sc.tl.louvain` (no matter the flavor used), emit a ``DeprecationWarning('We recommend to use `sc.tool.leiden` instead. Refer to its documentation for details')``. This meets the following goals:. - education: people will learn why we recommend the new function. - ease of use: no weird errors pop up suddenly. - reproducibility: If `louvain-igraph` is installed, the code works exactly as before (with an added warning), else it crashes. we could do the following within `sc.tl.louvain` to help users:. ```py. try:. import louvain. except ImportError:. raise ImportError(. 'The package louvain-igraph is not installed. '. 'Try using `sc.tl.leiden` in case you do not need '. 'to reproduce results produced using `sc.tl.louvain`'. ). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:198,deployability,depend,dependency,198,"I mean, @vtraag is is the person Id believe when asked which algorithm is superior, so we could. 1. add `sc.tl.leiden` as an alternative that doesnt have a flavour argument. 2. make `leidenalg` a dependency and `louvain-igraph` an optional one. 3. when calling `sc.tl.louvain` (no matter the flavor used), emit a ``DeprecationWarning('We recommend to use `sc.tool.leiden` instead. Refer to its documentation for details')``. This meets the following goals:. - education: people will learn why we recommend the new function. - ease of use: no weird errors pop up suddenly. - reproducibility: If `louvain-igraph` is installed, the code works exactly as before (with an added warning), else it crashes. we could do the following within `sc.tl.louvain` to help users:. ```py. try:. import louvain. except ImportError:. raise ImportError(. 'The package louvain-igraph is not installed. '. 'Try using `sc.tl.leiden` in case you do not need '. 'to reproduce results produced using `sc.tl.louvain`'. ). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:616,deployability,instal,installed,616,"I mean, @vtraag is is the person Id believe when asked which algorithm is superior, so we could. 1. add `sc.tl.leiden` as an alternative that doesnt have a flavour argument. 2. make `leidenalg` a dependency and `louvain-igraph` an optional one. 3. when calling `sc.tl.louvain` (no matter the flavor used), emit a ``DeprecationWarning('We recommend to use `sc.tool.leiden` instead. Refer to its documentation for details')``. This meets the following goals:. - education: people will learn why we recommend the new function. - ease of use: no weird errors pop up suddenly. - reproducibility: If `louvain-igraph` is installed, the code works exactly as before (with an added warning), else it crashes. we could do the following within `sc.tl.louvain` to help users:. ```py. try:. import louvain. except ImportError:. raise ImportError(. 'The package louvain-igraph is not installed. '. 'Try using `sc.tl.leiden` in case you do not need '. 'to reproduce results produced using `sc.tl.louvain`'. ). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:874,deployability,instal,installed,874,"I mean, @vtraag is is the person Id believe when asked which algorithm is superior, so we could. 1. add `sc.tl.leiden` as an alternative that doesnt have a flavour argument. 2. make `leidenalg` a dependency and `louvain-igraph` an optional one. 3. when calling `sc.tl.louvain` (no matter the flavor used), emit a ``DeprecationWarning('We recommend to use `sc.tool.leiden` instead. Refer to its documentation for details')``. This meets the following goals:. - education: people will learn why we recommend the new function. - ease of use: no weird errors pop up suddenly. - reproducibility: If `louvain-igraph` is installed, the code works exactly as before (with an added warning), else it crashes. we could do the following within `sc.tl.louvain` to help users:. ```py. try:. import louvain. except ImportError:. raise ImportError(. 'The package louvain-igraph is not installed. '. 'Try using `sc.tl.leiden` in case you do not need '. 'to reproduce results produced using `sc.tl.louvain`'. ). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:198,integrability,depend,dependency,198,"I mean, @vtraag is is the person Id believe when asked which algorithm is superior, so we could. 1. add `sc.tl.leiden` as an alternative that doesnt have a flavour argument. 2. make `leidenalg` a dependency and `louvain-igraph` an optional one. 3. when calling `sc.tl.louvain` (no matter the flavor used), emit a ``DeprecationWarning('We recommend to use `sc.tool.leiden` instead. Refer to its documentation for details')``. This meets the following goals:. - education: people will learn why we recommend the new function. - ease of use: no weird errors pop up suddenly. - reproducibility: If `louvain-igraph` is installed, the code works exactly as before (with an added warning), else it crashes. we could do the following within `sc.tl.louvain` to help users:. ```py. try:. import louvain. except ImportError:. raise ImportError(. 'The package louvain-igraph is not installed. '. 'Try using `sc.tl.leiden` in case you do not need '. 'to reproduce results produced using `sc.tl.louvain`'. ). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:198,modifiability,depend,dependency,198,"I mean, @vtraag is is the person Id believe when asked which algorithm is superior, so we could. 1. add `sc.tl.leiden` as an alternative that doesnt have a flavour argument. 2. make `leidenalg` a dependency and `louvain-igraph` an optional one. 3. when calling `sc.tl.louvain` (no matter the flavor used), emit a ``DeprecationWarning('We recommend to use `sc.tool.leiden` instead. Refer to its documentation for details')``. This meets the following goals:. - education: people will learn why we recommend the new function. - ease of use: no weird errors pop up suddenly. - reproducibility: If `louvain-igraph` is installed, the code works exactly as before (with an added warning), else it crashes. we could do the following within `sc.tl.louvain` to help users:. ```py. try:. import louvain. except ImportError:. raise ImportError(. 'The package louvain-igraph is not installed. '. 'Try using `sc.tl.leiden` in case you do not need '. 'to reproduce results produced using `sc.tl.louvain`'. ). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:842,modifiability,pac,package,842,"I mean, @vtraag is is the person Id believe when asked which algorithm is superior, so we could. 1. add `sc.tl.leiden` as an alternative that doesnt have a flavour argument. 2. make `leidenalg` a dependency and `louvain-igraph` an optional one. 3. when calling `sc.tl.louvain` (no matter the flavor used), emit a ``DeprecationWarning('We recommend to use `sc.tool.leiden` instead. Refer to its documentation for details')``. This meets the following goals:. - education: people will learn why we recommend the new function. - ease of use: no weird errors pop up suddenly. - reproducibility: If `louvain-igraph` is installed, the code works exactly as before (with an added warning), else it crashes. we could do the following within `sc.tl.louvain` to help users:. ```py. try:. import louvain. except ImportError:. raise ImportError(. 'The package louvain-igraph is not installed. '. 'Try using `sc.tl.leiden` in case you do not need '. 'to reproduce results produced using `sc.tl.louvain`'. ). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:550,performance,error,errors,550,"I mean, @vtraag is is the person Id believe when asked which algorithm is superior, so we could. 1. add `sc.tl.leiden` as an alternative that doesnt have a flavour argument. 2. make `leidenalg` a dependency and `louvain-igraph` an optional one. 3. when calling `sc.tl.louvain` (no matter the flavor used), emit a ``DeprecationWarning('We recommend to use `sc.tool.leiden` instead. Refer to its documentation for details')``. This meets the following goals:. - education: people will learn why we recommend the new function. - ease of use: no weird errors pop up suddenly. - reproducibility: If `louvain-igraph` is installed, the code works exactly as before (with an added warning), else it crashes. we could do the following within `sc.tl.louvain` to help users:. ```py. try:. import louvain. except ImportError:. raise ImportError(. 'The package louvain-igraph is not installed. '. 'Try using `sc.tl.leiden` in case you do not need '. 'to reproduce results produced using `sc.tl.louvain`'. ). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:143,reliability,doe,doesn,143,"I mean, @vtraag is is the person Id believe when asked which algorithm is superior, so we could. 1. add `sc.tl.leiden` as an alternative that doesnt have a flavour argument. 2. make `leidenalg` a dependency and `louvain-igraph` an optional one. 3. when calling `sc.tl.louvain` (no matter the flavor used), emit a ``DeprecationWarning('We recommend to use `sc.tool.leiden` instead. Refer to its documentation for details')``. This meets the following goals:. - education: people will learn why we recommend the new function. - ease of use: no weird errors pop up suddenly. - reproducibility: If `louvain-igraph` is installed, the code works exactly as before (with an added warning), else it crashes. we could do the following within `sc.tl.louvain` to help users:. ```py. try:. import louvain. except ImportError:. raise ImportError(. 'The package louvain-igraph is not installed. '. 'Try using `sc.tl.leiden` in case you do not need '. 'to reproduce results produced using `sc.tl.louvain`'. ). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:198,safety,depend,dependency,198,"I mean, @vtraag is is the person Id believe when asked which algorithm is superior, so we could. 1. add `sc.tl.leiden` as an alternative that doesnt have a flavour argument. 2. make `leidenalg` a dependency and `louvain-igraph` an optional one. 3. when calling `sc.tl.louvain` (no matter the flavor used), emit a ``DeprecationWarning('We recommend to use `sc.tool.leiden` instead. Refer to its documentation for details')``. This meets the following goals:. - education: people will learn why we recommend the new function. - ease of use: no weird errors pop up suddenly. - reproducibility: If `louvain-igraph` is installed, the code works exactly as before (with an added warning), else it crashes. we could do the following within `sc.tl.louvain` to help users:. ```py. try:. import louvain. except ImportError:. raise ImportError(. 'The package louvain-igraph is not installed. '. 'Try using `sc.tl.leiden` in case you do not need '. 'to reproduce results produced using `sc.tl.louvain`'. ). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:550,safety,error,errors,550,"I mean, @vtraag is is the person Id believe when asked which algorithm is superior, so we could. 1. add `sc.tl.leiden` as an alternative that doesnt have a flavour argument. 2. make `leidenalg` a dependency and `louvain-igraph` an optional one. 3. when calling `sc.tl.louvain` (no matter the flavor used), emit a ``DeprecationWarning('We recommend to use `sc.tool.leiden` instead. Refer to its documentation for details')``. This meets the following goals:. - education: people will learn why we recommend the new function. - ease of use: no weird errors pop up suddenly. - reproducibility: If `louvain-igraph` is installed, the code works exactly as before (with an added warning), else it crashes. we could do the following within `sc.tl.louvain` to help users:. ```py. try:. import louvain. except ImportError:. raise ImportError(. 'The package louvain-igraph is not installed. '. 'Try using `sc.tl.leiden` in case you do not need '. 'to reproduce results produced using `sc.tl.louvain`'. ). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:796,safety,except,except,796,"I mean, @vtraag is is the person Id believe when asked which algorithm is superior, so we could. 1. add `sc.tl.leiden` as an alternative that doesnt have a flavour argument. 2. make `leidenalg` a dependency and `louvain-igraph` an optional one. 3. when calling `sc.tl.louvain` (no matter the flavor used), emit a ``DeprecationWarning('We recommend to use `sc.tool.leiden` instead. Refer to its documentation for details')``. This meets the following goals:. - education: people will learn why we recommend the new function. - ease of use: no weird errors pop up suddenly. - reproducibility: If `louvain-igraph` is installed, the code works exactly as before (with an added warning), else it crashes. we could do the following within `sc.tl.louvain` to help users:. ```py. try:. import louvain. except ImportError:. raise ImportError(. 'The package louvain-igraph is not installed. '. 'Try using `sc.tl.leiden` in case you do not need '. 'to reproduce results produced using `sc.tl.louvain`'. ). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:198,testability,depend,dependency,198,"I mean, @vtraag is is the person Id believe when asked which algorithm is superior, so we could. 1. add `sc.tl.leiden` as an alternative that doesnt have a flavour argument. 2. make `leidenalg` a dependency and `louvain-igraph` an optional one. 3. when calling `sc.tl.louvain` (no matter the flavor used), emit a ``DeprecationWarning('We recommend to use `sc.tool.leiden` instead. Refer to its documentation for details')``. This meets the following goals:. - education: people will learn why we recommend the new function. - ease of use: no weird errors pop up suddenly. - reproducibility: If `louvain-igraph` is installed, the code works exactly as before (with an added warning), else it crashes. we could do the following within `sc.tl.louvain` to help users:. ```py. try:. import louvain. except ImportError:. raise ImportError(. 'The package louvain-igraph is not installed. '. 'Try using `sc.tl.leiden` in case you do not need '. 'to reproduce results produced using `sc.tl.louvain`'. ). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:26,usability,person,person,26,"I mean, @vtraag is is the person Id believe when asked which algorithm is superior, so we could. 1. add `sc.tl.leiden` as an alternative that doesnt have a flavour argument. 2. make `leidenalg` a dependency and `louvain-igraph` an optional one. 3. when calling `sc.tl.louvain` (no matter the flavor used), emit a ``DeprecationWarning('We recommend to use `sc.tool.leiden` instead. Refer to its documentation for details')``. This meets the following goals:. - education: people will learn why we recommend the new function. - ease of use: no weird errors pop up suddenly. - reproducibility: If `louvain-igraph` is installed, the code works exactly as before (with an added warning), else it crashes. we could do the following within `sc.tl.louvain` to help users:. ```py. try:. import louvain. except ImportError:. raise ImportError(. 'The package louvain-igraph is not installed. '. 'Try using `sc.tl.leiden` in case you do not need '. 'to reproduce results produced using `sc.tl.louvain`'. ). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:361,usability,tool,tool,361,"I mean, @vtraag is is the person Id believe when asked which algorithm is superior, so we could. 1. add `sc.tl.leiden` as an alternative that doesnt have a flavour argument. 2. make `leidenalg` a dependency and `louvain-igraph` an optional one. 3. when calling `sc.tl.louvain` (no matter the flavor used), emit a ``DeprecationWarning('We recommend to use `sc.tool.leiden` instead. Refer to its documentation for details')``. This meets the following goals:. - education: people will learn why we recommend the new function. - ease of use: no weird errors pop up suddenly. - reproducibility: If `louvain-igraph` is installed, the code works exactly as before (with an added warning), else it crashes. we could do the following within `sc.tl.louvain` to help users:. ```py. try:. import louvain. except ImportError:. raise ImportError(. 'The package louvain-igraph is not installed. '. 'Try using `sc.tl.leiden` in case you do not need '. 'to reproduce results produced using `sc.tl.louvain`'. ). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:396,usability,document,documentation,396,"I mean, @vtraag is is the person Id believe when asked which algorithm is superior, so we could. 1. add `sc.tl.leiden` as an alternative that doesnt have a flavour argument. 2. make `leidenalg` a dependency and `louvain-igraph` an optional one. 3. when calling `sc.tl.louvain` (no matter the flavor used), emit a ``DeprecationWarning('We recommend to use `sc.tool.leiden` instead. Refer to its documentation for details')``. This meets the following goals:. - education: people will learn why we recommend the new function. - ease of use: no weird errors pop up suddenly. - reproducibility: If `louvain-igraph` is installed, the code works exactly as before (with an added warning), else it crashes. we could do the following within `sc.tl.louvain` to help users:. ```py. try:. import louvain. except ImportError:. raise ImportError(. 'The package louvain-igraph is not installed. '. 'Try using `sc.tl.leiden` in case you do not need '. 'to reproduce results produced using `sc.tl.louvain`'. ). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:485,usability,learn,learn,485,"I mean, @vtraag is is the person Id believe when asked which algorithm is superior, so we could. 1. add `sc.tl.leiden` as an alternative that doesnt have a flavour argument. 2. make `leidenalg` a dependency and `louvain-igraph` an optional one. 3. when calling `sc.tl.louvain` (no matter the flavor used), emit a ``DeprecationWarning('We recommend to use `sc.tool.leiden` instead. Refer to its documentation for details')``. This meets the following goals:. - education: people will learn why we recommend the new function. - ease of use: no weird errors pop up suddenly. - reproducibility: If `louvain-igraph` is installed, the code works exactly as before (with an added warning), else it crashes. we could do the following within `sc.tl.louvain` to help users:. ```py. try:. import louvain. except ImportError:. raise ImportError(. 'The package louvain-igraph is not installed. '. 'Try using `sc.tl.leiden` in case you do not need '. 'to reproduce results produced using `sc.tl.louvain`'. ). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:550,usability,error,errors,550,"I mean, @vtraag is is the person Id believe when asked which algorithm is superior, so we could. 1. add `sc.tl.leiden` as an alternative that doesnt have a flavour argument. 2. make `leidenalg` a dependency and `louvain-igraph` an optional one. 3. when calling `sc.tl.louvain` (no matter the flavor used), emit a ``DeprecationWarning('We recommend to use `sc.tool.leiden` instead. Refer to its documentation for details')``. This meets the following goals:. - education: people will learn why we recommend the new function. - ease of use: no weird errors pop up suddenly. - reproducibility: If `louvain-igraph` is installed, the code works exactly as before (with an added warning), else it crashes. we could do the following within `sc.tl.louvain` to help users:. ```py. try:. import louvain. except ImportError:. raise ImportError(. 'The package louvain-igraph is not installed. '. 'Try using `sc.tl.leiden` in case you do not need '. 'to reproduce results produced using `sc.tl.louvain`'. ). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:754,usability,help,help,754,"I mean, @vtraag is is the person Id believe when asked which algorithm is superior, so we could. 1. add `sc.tl.leiden` as an alternative that doesnt have a flavour argument. 2. make `leidenalg` a dependency and `louvain-igraph` an optional one. 3. when calling `sc.tl.louvain` (no matter the flavor used), emit a ``DeprecationWarning('We recommend to use `sc.tool.leiden` instead. Refer to its documentation for details')``. This meets the following goals:. - education: people will learn why we recommend the new function. - ease of use: no weird errors pop up suddenly. - reproducibility: If `louvain-igraph` is installed, the code works exactly as before (with an added warning), else it crashes. we could do the following within `sc.tl.louvain` to help users:. ```py. try:. import louvain. except ImportError:. raise ImportError(. 'The package louvain-igraph is not installed. '. 'Try using `sc.tl.leiden` in case you do not need '. 'to reproduce results produced using `sc.tl.louvain`'. ). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:759,usability,user,users,759,"I mean, @vtraag is is the person Id believe when asked which algorithm is superior, so we could. 1. add `sc.tl.leiden` as an alternative that doesnt have a flavour argument. 2. make `leidenalg` a dependency and `louvain-igraph` an optional one. 3. when calling `sc.tl.louvain` (no matter the flavor used), emit a ``DeprecationWarning('We recommend to use `sc.tool.leiden` instead. Refer to its documentation for details')``. This meets the following goals:. - education: people will learn why we recommend the new function. - ease of use: no weird errors pop up suddenly. - reproducibility: If `louvain-igraph` is installed, the code works exactly as before (with an added warning), else it crashes. we could do the following within `sc.tl.louvain` to help users:. ```py. try:. import louvain. except ImportError:. raise ImportError(. 'The package louvain-igraph is not installed. '. 'Try using `sc.tl.leiden` in case you do not need '. 'to reproduce results produced using `sc.tl.louvain`'. ). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:546,availability,cluster,cluster,546,"I just did a quick comparison between louvain and leiden algorithms using the pbmc68k_reduced dataset:. ```PYTHON. adata = sc.datasets.pbmc68k_reduced(). sc.pp.neighbors(adata). leiden(adata, use_weights=True). sc.tl.louvain(adata, use_weights=True). sc.pl.umap(adata, color=['louvain', 'leiden'], s=50, alpha=0.6, ncols=2). ```. ![image](https://user-images.githubusercontent.com/4964309/48210096-fb814800-e376-11e8-9cbc-b16490c9ead9.png). The results are almost identical. However, while in the `louvain` results some cells appear in the wrong cluster (red circle) this is not the case for the `leiden` method. I should note that the installation of `leidenalg` didn't go smooth and took me a while to set it up.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:546,deployability,cluster,cluster,546,"I just did a quick comparison between louvain and leiden algorithms using the pbmc68k_reduced dataset:. ```PYTHON. adata = sc.datasets.pbmc68k_reduced(). sc.pp.neighbors(adata). leiden(adata, use_weights=True). sc.tl.louvain(adata, use_weights=True). sc.pl.umap(adata, color=['louvain', 'leiden'], s=50, alpha=0.6, ncols=2). ```. ![image](https://user-images.githubusercontent.com/4964309/48210096-fb814800-e376-11e8-9cbc-b16490c9ead9.png). The results are almost identical. However, while in the `louvain` results some cells appear in the wrong cluster (red circle) this is not the case for the `leiden` method. I should note that the installation of `leidenalg` didn't go smooth and took me a while to set it up.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:636,deployability,instal,installation,636,"I just did a quick comparison between louvain and leiden algorithms using the pbmc68k_reduced dataset:. ```PYTHON. adata = sc.datasets.pbmc68k_reduced(). sc.pp.neighbors(adata). leiden(adata, use_weights=True). sc.tl.louvain(adata, use_weights=True). sc.pl.umap(adata, color=['louvain', 'leiden'], s=50, alpha=0.6, ncols=2). ```. ![image](https://user-images.githubusercontent.com/4964309/48210096-fb814800-e376-11e8-9cbc-b16490c9ead9.png). The results are almost identical. However, while in the `louvain` results some cells appear in the wrong cluster (red circle) this is not the case for the `leiden` method. I should note that the installation of `leidenalg` didn't go smooth and took me a while to set it up.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:464,security,ident,identical,464,"I just did a quick comparison between louvain and leiden algorithms using the pbmc68k_reduced dataset:. ```PYTHON. adata = sc.datasets.pbmc68k_reduced(). sc.pp.neighbors(adata). leiden(adata, use_weights=True). sc.tl.louvain(adata, use_weights=True). sc.pl.umap(adata, color=['louvain', 'leiden'], s=50, alpha=0.6, ncols=2). ```. ![image](https://user-images.githubusercontent.com/4964309/48210096-fb814800-e376-11e8-9cbc-b16490c9ead9.png). The results are almost identical. However, while in the `louvain` results some cells appear in the wrong cluster (red circle) this is not the case for the `leiden` method. I should note that the installation of `leidenalg` didn't go smooth and took me a while to set it up.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:347,usability,user,user-images,347,"I just did a quick comparison between louvain and leiden algorithms using the pbmc68k_reduced dataset:. ```PYTHON. adata = sc.datasets.pbmc68k_reduced(). sc.pp.neighbors(adata). leiden(adata, use_weights=True). sc.tl.louvain(adata, use_weights=True). sc.pl.umap(adata, color=['louvain', 'leiden'], s=50, alpha=0.6, ncols=2). ```. ![image](https://user-images.githubusercontent.com/4964309/48210096-fb814800-e376-11e8-9cbc-b16490c9ead9.png). The results are almost identical. However, while in the `louvain` results some cells appear in the wrong cluster (red circle) this is not the case for the `leiden` method. I should note that the installation of `leidenalg` didn't go smooth and took me a while to set it up.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:130,deployability,stage,staged-recipes,130,"Sorry to hear it took you some time to set it up. I've created a recipe for `conda-forge` channel (https://github.com/conda-forge/staged-recipes/pull/6911), once merged that should hopefully simplify some of the installation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:212,deployability,instal,installation,212,"Sorry to hear it took you some time to set it up. I've created a recipe for `conda-forge` channel (https://github.com/conda-forge/staged-recipes/pull/6911), once merged that should hopefully simplify some of the installation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:31,performance,time,time,31,"Sorry to hear it took you some time to set it up. I've created a recipe for `conda-forge` channel (https://github.com/conda-forge/staged-recipes/pull/6911), once merged that should hopefully simplify some of the installation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:191,testability,simpl,simplify,191,"Sorry to hear it took you some time to set it up. I've created a recipe for `conda-forge` channel (https://github.com/conda-forge/staged-recipes/pull/6911), once merged that should hopefully simplify some of the installation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:191,usability,simpl,simplify,191,"Sorry to hear it took you some time to set it up. I've created a recipe for `conda-forge` channel (https://github.com/conda-forge/staged-recipes/pull/6911), once merged that should hopefully simplify some of the installation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:19,deployability,instal,install,19,"@vtraag FWIW, `pip install leidenalg` worked without a hitch for me (CentOS 6.5, Python 3.6.6 in a relatively empty conda env: scanpy + scikit stack).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:143,deployability,stack,stack,143,"@vtraag FWIW, `pip install leidenalg` worked without a hitch for me (CentOS 6.5, Python 3.6.6 in a relatively empty conda env: scanpy + scikit stack).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:26,deployability,instal,install,26,"Likewise, I just ran `pip install leidenalg` on an OSX machine which already had scanpy and louvain on it, and it set up effortlessly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:27,deployability,instal,installation,27,Good to hear also positive installation results!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:718,availability,slo,slowly,718,"I'd definitely make a new function `tl.leiden`; then @vtraag 's package can also be taken credit of in an appropriate way, using the reference to the recent arxiv. . Also, backwards compat is guaranteed and @flying-sheep's two other points (eduction and ease-of-use) are met, too. So, @ktpolanski, would you make a PR for a function `tl.leiden`? Of course, it would be nice if didn't duplicate all code in the louvain function, but that's up to you. I would not yet make the `leidenalg` package a at this stage, but transition to that either in Scanpy `1.4` or later. People can definitely achieve decent results with the current setup and we don't want everyone to change everything. After the PR, those who want can slowly transition to the new clustering algorithm. After a major release, we can broadly advertise the package. @vtraag: Great to see your new preprint and package. I thought that your Louvain implementation already yielded very well-connected communities and even made a remark on that [here](https://doi.org/10.1101/208819) in the first version more than a year ago. But great, in hard cases, I'd expect better results using your new algorithm for partitioning the graph...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:747,availability,cluster,clustering,747,"I'd definitely make a new function `tl.leiden`; then @vtraag 's package can also be taken credit of in an appropriate way, using the reference to the recent arxiv. . Also, backwards compat is guaranteed and @flying-sheep's two other points (eduction and ease-of-use) are met, too. So, @ktpolanski, would you make a PR for a function `tl.leiden`? Of course, it would be nice if didn't duplicate all code in the louvain function, but that's up to you. I would not yet make the `leidenalg` package a at this stage, but transition to that either in Scanpy `1.4` or later. People can definitely achieve decent results with the current setup and we don't want everyone to change everything. After the PR, those who want can slowly transition to the new clustering algorithm. After a major release, we can broadly advertise the package. @vtraag: Great to see your new preprint and package. I thought that your Louvain implementation already yielded very well-connected communities and even made a remark on that [here](https://doi.org/10.1101/208819) in the first version more than a year ago. But great, in hard cases, I'd expect better results using your new algorithm for partitioning the graph...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:505,deployability,stage,stage,505,"I'd definitely make a new function `tl.leiden`; then @vtraag 's package can also be taken credit of in an appropriate way, using the reference to the recent arxiv. . Also, backwards compat is guaranteed and @flying-sheep's two other points (eduction and ease-of-use) are met, too. So, @ktpolanski, would you make a PR for a function `tl.leiden`? Of course, it would be nice if didn't duplicate all code in the louvain function, but that's up to you. I would not yet make the `leidenalg` package a at this stage, but transition to that either in Scanpy `1.4` or later. People can definitely achieve decent results with the current setup and we don't want everyone to change everything. After the PR, those who want can slowly transition to the new clustering algorithm. After a major release, we can broadly advertise the package. @vtraag: Great to see your new preprint and package. I thought that your Louvain implementation already yielded very well-connected communities and even made a remark on that [here](https://doi.org/10.1101/208819) in the first version more than a year ago. But great, in hard cases, I'd expect better results using your new algorithm for partitioning the graph...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:747,deployability,cluster,clustering,747,"I'd definitely make a new function `tl.leiden`; then @vtraag 's package can also be taken credit of in an appropriate way, using the reference to the recent arxiv. . Also, backwards compat is guaranteed and @flying-sheep's two other points (eduction and ease-of-use) are met, too. So, @ktpolanski, would you make a PR for a function `tl.leiden`? Of course, it would be nice if didn't duplicate all code in the louvain function, but that's up to you. I would not yet make the `leidenalg` package a at this stage, but transition to that either in Scanpy `1.4` or later. People can definitely achieve decent results with the current setup and we don't want everyone to change everything. After the PR, those who want can slowly transition to the new clustering algorithm. After a major release, we can broadly advertise the package. @vtraag: Great to see your new preprint and package. I thought that your Louvain implementation already yielded very well-connected communities and even made a remark on that [here](https://doi.org/10.1101/208819) in the first version more than a year ago. But great, in hard cases, I'd expect better results using your new algorithm for partitioning the graph...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:783,deployability,releas,release,783,"I'd definitely make a new function `tl.leiden`; then @vtraag 's package can also be taken credit of in an appropriate way, using the reference to the recent arxiv. . Also, backwards compat is guaranteed and @flying-sheep's two other points (eduction and ease-of-use) are met, too. So, @ktpolanski, would you make a PR for a function `tl.leiden`? Of course, it would be nice if didn't duplicate all code in the louvain function, but that's up to you. I would not yet make the `leidenalg` package a at this stage, but transition to that either in Scanpy `1.4` or later. People can definitely achieve decent results with the current setup and we don't want everyone to change everything. After the PR, those who want can slowly transition to the new clustering algorithm. After a major release, we can broadly advertise the package. @vtraag: Great to see your new preprint and package. I thought that your Louvain implementation already yielded very well-connected communities and even made a remark on that [here](https://doi.org/10.1101/208819) in the first version more than a year ago. But great, in hard cases, I'd expect better results using your new algorithm for partitioning the graph...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:1057,deployability,version,version,1057,"I'd definitely make a new function `tl.leiden`; then @vtraag 's package can also be taken credit of in an appropriate way, using the reference to the recent arxiv. . Also, backwards compat is guaranteed and @flying-sheep's two other points (eduction and ease-of-use) are met, too. So, @ktpolanski, would you make a PR for a function `tl.leiden`? Of course, it would be nice if didn't duplicate all code in the louvain function, but that's up to you. I would not yet make the `leidenalg` package a at this stage, but transition to that either in Scanpy `1.4` or later. People can definitely achieve decent results with the current setup and we don't want everyone to change everything. After the PR, those who want can slowly transition to the new clustering algorithm. After a major release, we can broadly advertise the package. @vtraag: Great to see your new preprint and package. I thought that your Louvain implementation already yielded very well-connected communities and even made a remark on that [here](https://doi.org/10.1101/208819) in the first version more than a year ago. But great, in hard cases, I'd expect better results using your new algorithm for partitioning the graph...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:622,energy efficiency,current,current,622,"I'd definitely make a new function `tl.leiden`; then @vtraag 's package can also be taken credit of in an appropriate way, using the reference to the recent arxiv. . Also, backwards compat is guaranteed and @flying-sheep's two other points (eduction and ease-of-use) are met, too. So, @ktpolanski, would you make a PR for a function `tl.leiden`? Of course, it would be nice if didn't duplicate all code in the louvain function, but that's up to you. I would not yet make the `leidenalg` package a at this stage, but transition to that either in Scanpy `1.4` or later. People can definitely achieve decent results with the current setup and we don't want everyone to change everything. After the PR, those who want can slowly transition to the new clustering algorithm. After a major release, we can broadly advertise the package. @vtraag: Great to see your new preprint and package. I thought that your Louvain implementation already yielded very well-connected communities and even made a remark on that [here](https://doi.org/10.1101/208819) in the first version more than a year ago. But great, in hard cases, I'd expect better results using your new algorithm for partitioning the graph...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:1057,integrability,version,version,1057,"I'd definitely make a new function `tl.leiden`; then @vtraag 's package can also be taken credit of in an appropriate way, using the reference to the recent arxiv. . Also, backwards compat is guaranteed and @flying-sheep's two other points (eduction and ease-of-use) are met, too. So, @ktpolanski, would you make a PR for a function `tl.leiden`? Of course, it would be nice if didn't duplicate all code in the louvain function, but that's up to you. I would not yet make the `leidenalg` package a at this stage, but transition to that either in Scanpy `1.4` or later. People can definitely achieve decent results with the current setup and we don't want everyone to change everything. After the PR, those who want can slowly transition to the new clustering algorithm. After a major release, we can broadly advertise the package. @vtraag: Great to see your new preprint and package. I thought that your Louvain implementation already yielded very well-connected communities and even made a remark on that [here](https://doi.org/10.1101/208819) in the first version more than a year ago. But great, in hard cases, I'd expect better results using your new algorithm for partitioning the graph...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:64,modifiability,pac,package,64,"I'd definitely make a new function `tl.leiden`; then @vtraag 's package can also be taken credit of in an appropriate way, using the reference to the recent arxiv. . Also, backwards compat is guaranteed and @flying-sheep's two other points (eduction and ease-of-use) are met, too. So, @ktpolanski, would you make a PR for a function `tl.leiden`? Of course, it would be nice if didn't duplicate all code in the louvain function, but that's up to you. I would not yet make the `leidenalg` package a at this stage, but transition to that either in Scanpy `1.4` or later. People can definitely achieve decent results with the current setup and we don't want everyone to change everything. After the PR, those who want can slowly transition to the new clustering algorithm. After a major release, we can broadly advertise the package. @vtraag: Great to see your new preprint and package. I thought that your Louvain implementation already yielded very well-connected communities and even made a remark on that [here](https://doi.org/10.1101/208819) in the first version more than a year ago. But great, in hard cases, I'd expect better results using your new algorithm for partitioning the graph...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:487,modifiability,pac,package,487,"I'd definitely make a new function `tl.leiden`; then @vtraag 's package can also be taken credit of in an appropriate way, using the reference to the recent arxiv. . Also, backwards compat is guaranteed and @flying-sheep's two other points (eduction and ease-of-use) are met, too. So, @ktpolanski, would you make a PR for a function `tl.leiden`? Of course, it would be nice if didn't duplicate all code in the louvain function, but that's up to you. I would not yet make the `leidenalg` package a at this stage, but transition to that either in Scanpy `1.4` or later. People can definitely achieve decent results with the current setup and we don't want everyone to change everything. After the PR, those who want can slowly transition to the new clustering algorithm. After a major release, we can broadly advertise the package. @vtraag: Great to see your new preprint and package. I thought that your Louvain implementation already yielded very well-connected communities and even made a remark on that [here](https://doi.org/10.1101/208819) in the first version more than a year ago. But great, in hard cases, I'd expect better results using your new algorithm for partitioning the graph...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:821,modifiability,pac,package,821,"I'd definitely make a new function `tl.leiden`; then @vtraag 's package can also be taken credit of in an appropriate way, using the reference to the recent arxiv. . Also, backwards compat is guaranteed and @flying-sheep's two other points (eduction and ease-of-use) are met, too. So, @ktpolanski, would you make a PR for a function `tl.leiden`? Of course, it would be nice if didn't duplicate all code in the louvain function, but that's up to you. I would not yet make the `leidenalg` package a at this stage, but transition to that either in Scanpy `1.4` or later. People can definitely achieve decent results with the current setup and we don't want everyone to change everything. After the PR, those who want can slowly transition to the new clustering algorithm. After a major release, we can broadly advertise the package. @vtraag: Great to see your new preprint and package. I thought that your Louvain implementation already yielded very well-connected communities and even made a remark on that [here](https://doi.org/10.1101/208819) in the first version more than a year ago. But great, in hard cases, I'd expect better results using your new algorithm for partitioning the graph...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:874,modifiability,pac,package,874,"I'd definitely make a new function `tl.leiden`; then @vtraag 's package can also be taken credit of in an appropriate way, using the reference to the recent arxiv. . Also, backwards compat is guaranteed and @flying-sheep's two other points (eduction and ease-of-use) are met, too. So, @ktpolanski, would you make a PR for a function `tl.leiden`? Of course, it would be nice if didn't duplicate all code in the louvain function, but that's up to you. I would not yet make the `leidenalg` package a at this stage, but transition to that either in Scanpy `1.4` or later. People can definitely achieve decent results with the current setup and we don't want everyone to change everything. After the PR, those who want can slowly transition to the new clustering algorithm. After a major release, we can broadly advertise the package. @vtraag: Great to see your new preprint and package. I thought that your Louvain implementation already yielded very well-connected communities and even made a remark on that [here](https://doi.org/10.1101/208819) in the first version more than a year ago. But great, in hard cases, I'd expect better results using your new algorithm for partitioning the graph...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:1057,modifiability,version,version,1057,"I'd definitely make a new function `tl.leiden`; then @vtraag 's package can also be taken credit of in an appropriate way, using the reference to the recent arxiv. . Also, backwards compat is guaranteed and @flying-sheep's two other points (eduction and ease-of-use) are met, too. So, @ktpolanski, would you make a PR for a function `tl.leiden`? Of course, it would be nice if didn't duplicate all code in the louvain function, but that's up to you. I would not yet make the `leidenalg` package a at this stage, but transition to that either in Scanpy `1.4` or later. People can definitely achieve decent results with the current setup and we don't want everyone to change everything. After the PR, those who want can slowly transition to the new clustering algorithm. After a major release, we can broadly advertise the package. @vtraag: Great to see your new preprint and package. I thought that your Louvain implementation already yielded very well-connected communities and even made a remark on that [here](https://doi.org/10.1101/208819) in the first version more than a year ago. But great, in hard cases, I'd expect better results using your new algorithm for partitioning the graph...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:718,reliability,slo,slowly,718,"I'd definitely make a new function `tl.leiden`; then @vtraag 's package can also be taken credit of in an appropriate way, using the reference to the recent arxiv. . Also, backwards compat is guaranteed and @flying-sheep's two other points (eduction and ease-of-use) are met, too. So, @ktpolanski, would you make a PR for a function `tl.leiden`? Of course, it would be nice if didn't duplicate all code in the louvain function, but that's up to you. I would not yet make the `leidenalg` package a at this stage, but transition to that either in Scanpy `1.4` or later. People can definitely achieve decent results with the current setup and we don't want everyone to change everything. After the PR, those who want can slowly transition to the new clustering algorithm. After a major release, we can broadly advertise the package. @vtraag: Great to see your new preprint and package. I thought that your Louvain implementation already yielded very well-connected communities and even made a remark on that [here](https://doi.org/10.1101/208819) in the first version more than a year ago. But great, in hard cases, I'd expect better results using your new algorithm for partitioning the graph...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:334,deployability,upgrad,upgrade,334,"I think the disconnected communities in Louvain should have less of an effect in KNN graphs as the degree distribution is a lot more regular. This issue appears to occur a lot more frequently when the node degrees in a community are quite different (or at least this is what I found on PPI networks). Nonetheless, it's a good idea to upgrade I reckon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:106,interoperability,distribut,distribution,106,"I think the disconnected communities in Louvain should have less of an effect in KNN graphs as the degree distribution is a lot more regular. This issue appears to occur a lot more frequently when the node degrees in a community are quite different (or at least this is what I found on PPI networks). Nonetheless, it's a good idea to upgrade I reckon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:334,modifiability,upgrad,upgrade,334,"I think the disconnected communities in Louvain should have less of an effect in KNN graphs as the degree distribution is a lot more regular. This issue appears to occur a lot more frequently when the node degrees in a community are quite different (or at least this is what I found on PPI networks). Nonetheless, it's a good idea to upgrade I reckon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:290,performance,network,networks,290,"I think the disconnected communities in Louvain should have less of an effect in KNN graphs as the degree distribution is a lot more regular. This issue appears to occur a lot more frequently when the node degrees in a community are quite different (or at least this is what I found on PPI networks). Nonetheless, it's a good idea to upgrade I reckon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:290,security,network,networks,290,"I think the disconnected communities in Louvain should have less of an effect in KNN graphs as the degree distribution is a lot more regular. This issue appears to occur a lot more frequently when the node degrees in a community are quite different (or at least this is what I found on PPI networks). Nonetheless, it's a good idea to upgrade I reckon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:271,deployability,continu,continue,271,"@LuckyMD Yes, I was recently reminded of that by Mason Porter. I saw you also wrote a small piece on it in your [thesis](https://ora.ox.ac.uk/objects/uuid:b49187be-8203-4aa0-abbd-bff1a507ff6f). I had already forgotten about our email exchange again. But it's nice we can continue the conversation 4 years later here :). The approach I took back then was quite straightforward: simply separate the different connected components. This of course ignores the larger issue that even when communities are not completely disconnected, they may still be quite badly connected. The approach taken in the Leiden algorithm is quite different, and has much nicer properties I think.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:417,integrability,compon,components,417,"@LuckyMD Yes, I was recently reminded of that by Mason Porter. I saw you also wrote a small piece on it in your [thesis](https://ora.ox.ac.uk/objects/uuid:b49187be-8203-4aa0-abbd-bff1a507ff6f). I had already forgotten about our email exchange again. But it's nice we can continue the conversation 4 years later here :). The approach I took back then was quite straightforward: simply separate the different connected components. This of course ignores the larger issue that even when communities are not completely disconnected, they may still be quite badly connected. The approach taken in the Leiden algorithm is quite different, and has much nicer properties I think.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:234,interoperability,exchang,exchange,234,"@LuckyMD Yes, I was recently reminded of that by Mason Porter. I saw you also wrote a small piece on it in your [thesis](https://ora.ox.ac.uk/objects/uuid:b49187be-8203-4aa0-abbd-bff1a507ff6f). I had already forgotten about our email exchange again. But it's nice we can continue the conversation 4 years later here :). The approach I took back then was quite straightforward: simply separate the different connected components. This of course ignores the larger issue that even when communities are not completely disconnected, they may still be quite badly connected. The approach taken in the Leiden algorithm is quite different, and has much nicer properties I think.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:284,interoperability,convers,conversation,284,"@LuckyMD Yes, I was recently reminded of that by Mason Porter. I saw you also wrote a small piece on it in your [thesis](https://ora.ox.ac.uk/objects/uuid:b49187be-8203-4aa0-abbd-bff1a507ff6f). I had already forgotten about our email exchange again. But it's nice we can continue the conversation 4 years later here :). The approach I took back then was quite straightforward: simply separate the different connected components. This of course ignores the larger issue that even when communities are not completely disconnected, they may still be quite badly connected. The approach taken in the Leiden algorithm is quite different, and has much nicer properties I think.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:417,interoperability,compon,components,417,"@LuckyMD Yes, I was recently reminded of that by Mason Porter. I saw you also wrote a small piece on it in your [thesis](https://ora.ox.ac.uk/objects/uuid:b49187be-8203-4aa0-abbd-bff1a507ff6f). I had already forgotten about our email exchange again. But it's nice we can continue the conversation 4 years later here :). The approach I took back then was quite straightforward: simply separate the different connected components. This of course ignores the larger issue that even when communities are not completely disconnected, they may still be quite badly connected. The approach taken in the Leiden algorithm is quite different, and has much nicer properties I think.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:417,modifiability,compon,components,417,"@LuckyMD Yes, I was recently reminded of that by Mason Porter. I saw you also wrote a small piece on it in your [thesis](https://ora.ox.ac.uk/objects/uuid:b49187be-8203-4aa0-abbd-bff1a507ff6f). I had already forgotten about our email exchange again. But it's nice we can continue the conversation 4 years later here :). The approach I took back then was quite straightforward: simply separate the different connected components. This of course ignores the larger issue that even when communities are not completely disconnected, they may still be quite badly connected. The approach taken in the Leiden algorithm is quite different, and has much nicer properties I think.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:504,safety,compl,completely,504,"@LuckyMD Yes, I was recently reminded of that by Mason Porter. I saw you also wrote a small piece on it in your [thesis](https://ora.ox.ac.uk/objects/uuid:b49187be-8203-4aa0-abbd-bff1a507ff6f). I had already forgotten about our email exchange again. But it's nice we can continue the conversation 4 years later here :). The approach I took back then was quite straightforward: simply separate the different connected components. This of course ignores the larger issue that even when communities are not completely disconnected, they may still be quite badly connected. The approach taken in the Leiden algorithm is quite different, and has much nicer properties I think.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:504,security,compl,completely,504,"@LuckyMD Yes, I was recently reminded of that by Mason Porter. I saw you also wrote a small piece on it in your [thesis](https://ora.ox.ac.uk/objects/uuid:b49187be-8203-4aa0-abbd-bff1a507ff6f). I had already forgotten about our email exchange again. But it's nice we can continue the conversation 4 years later here :). The approach I took back then was quite straightforward: simply separate the different connected components. This of course ignores the larger issue that even when communities are not completely disconnected, they may still be quite badly connected. The approach taken in the Leiden algorithm is quite different, and has much nicer properties I think.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:377,testability,simpl,simply,377,"@LuckyMD Yes, I was recently reminded of that by Mason Porter. I saw you also wrote a small piece on it in your [thesis](https://ora.ox.ac.uk/objects/uuid:b49187be-8203-4aa0-abbd-bff1a507ff6f). I had already forgotten about our email exchange again. But it's nice we can continue the conversation 4 years later here :). The approach I took back then was quite straightforward: simply separate the different connected components. This of course ignores the larger issue that even when communities are not completely disconnected, they may still be quite badly connected. The approach taken in the Leiden algorithm is quite different, and has much nicer properties I think.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:377,usability,simpl,simply,377,"@LuckyMD Yes, I was recently reminded of that by Mason Porter. I saw you also wrote a small piece on it in your [thesis](https://ora.ox.ac.uk/objects/uuid:b49187be-8203-4aa0-abbd-bff1a507ff6f). I had already forgotten about our email exchange again. But it's nice we can continue the conversation 4 years later here :). The approach I took back then was quite straightforward: simply separate the different connected components. This of course ignores the larger issue that even when communities are not completely disconnected, they may still be quite badly connected. The approach taken in the Leiden algorithm is quite different, and has much nicer properties I think.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:436,integrability,topic,topic,436,"@vtraag The Leiden algorithm is definitely a much better solution and the preprint explains nicely how it's not just about disconnecting communities but a more general problem. I needed a quick fix at the time, which worked well enough. Thanks again for that ;). I did spend some more time on trying to understand the process of getting these disconnected communities back in 2014, and hence put a bit in my thesis. As it was a bit off-topic for me, that section was shortened by revisions unfortunately. I'm surprised you found it actually.... although I'm glad someone read it :). Also, I thought I saw an abstract of yours in some conference proceedings a couple years ago about the Leiden algorithm already. It seems to have been in the workings for quite some time. Congratulations on getting it out!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:608,integrability,abstract,abstract,608,"@vtraag The Leiden algorithm is definitely a much better solution and the preprint explains nicely how it's not just about disconnecting communities but a more general problem. I needed a quick fix at the time, which worked well enough. Thanks again for that ;). I did spend some more time on trying to understand the process of getting these disconnected communities back in 2014, and hence put a bit in my thesis. As it was a bit off-topic for me, that section was shortened by revisions unfortunately. I'm surprised you found it actually.... although I'm glad someone read it :). Also, I thought I saw an abstract of yours in some conference proceedings a couple years ago about the Leiden algorithm already. It seems to have been in the workings for quite some time. Congratulations on getting it out!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:659,integrability,coupl,couple,659,"@vtraag The Leiden algorithm is definitely a much better solution and the preprint explains nicely how it's not just about disconnecting communities but a more general problem. I needed a quick fix at the time, which worked well enough. Thanks again for that ;). I did spend some more time on trying to understand the process of getting these disconnected communities back in 2014, and hence put a bit in my thesis. As it was a bit off-topic for me, that section was shortened by revisions unfortunately. I'm surprised you found it actually.... although I'm glad someone read it :). Also, I thought I saw an abstract of yours in some conference proceedings a couple years ago about the Leiden algorithm already. It seems to have been in the workings for quite some time. Congratulations on getting it out!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:608,modifiability,abstract,abstract,608,"@vtraag The Leiden algorithm is definitely a much better solution and the preprint explains nicely how it's not just about disconnecting communities but a more general problem. I needed a quick fix at the time, which worked well enough. Thanks again for that ;). I did spend some more time on trying to understand the process of getting these disconnected communities back in 2014, and hence put a bit in my thesis. As it was a bit off-topic for me, that section was shortened by revisions unfortunately. I'm surprised you found it actually.... although I'm glad someone read it :). Also, I thought I saw an abstract of yours in some conference proceedings a couple years ago about the Leiden algorithm already. It seems to have been in the workings for quite some time. Congratulations on getting it out!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:659,modifiability,coupl,couple,659,"@vtraag The Leiden algorithm is definitely a much better solution and the preprint explains nicely how it's not just about disconnecting communities but a more general problem. I needed a quick fix at the time, which worked well enough. Thanks again for that ;). I did spend some more time on trying to understand the process of getting these disconnected communities back in 2014, and hence put a bit in my thesis. As it was a bit off-topic for me, that section was shortened by revisions unfortunately. I'm surprised you found it actually.... although I'm glad someone read it :). Also, I thought I saw an abstract of yours in some conference proceedings a couple years ago about the Leiden algorithm already. It seems to have been in the workings for quite some time. Congratulations on getting it out!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:205,performance,time,time,205,"@vtraag The Leiden algorithm is definitely a much better solution and the preprint explains nicely how it's not just about disconnecting communities but a more general problem. I needed a quick fix at the time, which worked well enough. Thanks again for that ;). I did spend some more time on trying to understand the process of getting these disconnected communities back in 2014, and hence put a bit in my thesis. As it was a bit off-topic for me, that section was shortened by revisions unfortunately. I'm surprised you found it actually.... although I'm glad someone read it :). Also, I thought I saw an abstract of yours in some conference proceedings a couple years ago about the Leiden algorithm already. It seems to have been in the workings for quite some time. Congratulations on getting it out!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:285,performance,time,time,285,"@vtraag The Leiden algorithm is definitely a much better solution and the preprint explains nicely how it's not just about disconnecting communities but a more general problem. I needed a quick fix at the time, which worked well enough. Thanks again for that ;). I did spend some more time on trying to understand the process of getting these disconnected communities back in 2014, and hence put a bit in my thesis. As it was a bit off-topic for me, that section was shortened by revisions unfortunately. I'm surprised you found it actually.... although I'm glad someone read it :). Also, I thought I saw an abstract of yours in some conference proceedings a couple years ago about the Leiden algorithm already. It seems to have been in the workings for quite some time. Congratulations on getting it out!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:765,performance,time,time,765,"@vtraag The Leiden algorithm is definitely a much better solution and the preprint explains nicely how it's not just about disconnecting communities but a more general problem. I needed a quick fix at the time, which worked well enough. Thanks again for that ;). I did spend some more time on trying to understand the process of getting these disconnected communities back in 2014, and hence put a bit in my thesis. As it was a bit off-topic for me, that section was shortened by revisions unfortunately. I'm surprised you found it actually.... although I'm glad someone read it :). Also, I thought I saw an abstract of yours in some conference proceedings a couple years ago about the Leiden algorithm already. It seems to have been in the workings for quite some time. Congratulations on getting it out!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:303,testability,understand,understand,303,"@vtraag The Leiden algorithm is definitely a much better solution and the preprint explains nicely how it's not just about disconnecting communities but a more general problem. I needed a quick fix at the time, which worked well enough. Thanks again for that ;). I did spend some more time on trying to understand the process of getting these disconnected communities back in 2014, and hence put a bit in my thesis. As it was a bit off-topic for me, that section was shortened by revisions unfortunately. I'm surprised you found it actually.... although I'm glad someone read it :). Also, I thought I saw an abstract of yours in some conference proceedings a couple years ago about the Leiden algorithm already. It seems to have been in the workings for quite some time. Congratulations on getting it out!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:659,testability,coupl,couple,659,"@vtraag The Leiden algorithm is definitely a much better solution and the preprint explains nicely how it's not just about disconnecting communities but a more general problem. I needed a quick fix at the time, which worked well enough. Thanks again for that ;). I did spend some more time on trying to understand the process of getting these disconnected communities back in 2014, and hence put a bit in my thesis. As it was a bit off-topic for me, that section was shortened by revisions unfortunately. I'm surprised you found it actually.... although I'm glad someone read it :). Also, I thought I saw an abstract of yours in some conference proceedings a couple years ago about the Leiden algorithm already. It seems to have been in the workings for quite some time. Congratulations on getting it out!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:359,availability,down,down,359,"@LuckyMD Thanks! Yes, the algorithm has been in development for quite some time. I presented it already in 2016 at a conference in Amsterdam, and after that in several other places. It kept changing in relatively minor ways, although that also affected the exact guarantees it could offer. Unfortunate that the section on disconnected communities got trimmed down! Would you have more extensive results described somewhere else?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:385,modifiability,extens,extensive,385,"@LuckyMD Thanks! Yes, the algorithm has been in development for quite some time. I presented it already in 2016 at a conference in Amsterdam, and after that in several other places. It kept changing in relatively minor ways, although that also affected the exact guarantees it could offer. Unfortunate that the section on disconnected communities got trimmed down! Would you have more extensive results described somewhere else?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:75,performance,time,time,75,"@LuckyMD Thanks! Yes, the algorithm has been in development for quite some time. I presented it already in 2016 at a conference in Amsterdam, and after that in several other places. It kept changing in relatively minor ways, although that also affected the exact guarantees it could offer. Unfortunate that the section on disconnected communities got trimmed down! Would you have more extensive results described somewhere else?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:568,deployability,version,versions,568,"@vtraag I have quite a few more examples of communities that were disconnected that I went through in my notes (and I should have one in an earlier draft of the thesis)... including a community that can only be reconnected with 2 intermediate nodes (though this was a rare case). The general trend was that the ""removed"" nodes were always of a higher degree than any node remaining in the disconnected community. Just by the number of alternative local merger options for hub nodes that would make sense I guess. If you're interested I can see if I can find the older versions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:568,integrability,version,versions,568,"@vtraag I have quite a few more examples of communities that were disconnected that I went through in my notes (and I should have one in an earlier draft of the thesis)... including a community that can only be reconnected with 2 intermediate nodes (though this was a rare case). The general trend was that the ""removed"" nodes were always of a higher degree than any node remaining in the disconnected community. Just by the number of alternative local merger options for hub nodes that would make sense I guess. If you're interested I can see if I can find the older versions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:230,modifiability,interm,intermediate,230,"@vtraag I have quite a few more examples of communities that were disconnected that I went through in my notes (and I should have one in an earlier draft of the thesis)... including a community that can only be reconnected with 2 intermediate nodes (though this was a rare case). The general trend was that the ""removed"" nodes were always of a higher degree than any node remaining in the disconnected community. Just by the number of alternative local merger options for hub nodes that would make sense I guess. If you're interested I can see if I can find the older versions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:568,modifiability,version,versions,568,"@vtraag I have quite a few more examples of communities that were disconnected that I went through in my notes (and I should have one in an earlier draft of the thesis)... including a community that can only be reconnected with 2 intermediate nodes (though this was a rare case). The general trend was that the ""removed"" nodes were always of a higher degree than any node remaining in the disconnected community. Just by the number of alternative local merger options for hub nodes that would make sense I guess. If you're interested I can see if I can find the older versions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:121,performance,time,times,121,"These days, I can only make it by going through 70 github issues in a row after the twins finally sleep... there will be times with more thoughtful comments from my side, again, I hope...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:343,deployability,version,version,343,"Interesting example, @LuckyMD, indeed I'd be interested to reading more about it (you can email me if you want). Thank you for letting me know @falexwolf that the disconnected comunities are also mentioned in the [paper](https://doi.org/10.1101/208819). I'll be sure to point to both your thesis @LuckyMD and the paper @falexwolf in a revised version. Interestingly, in an earlier implementation of the Louvain algorithm (https://launchpad.net/louvain), I indeed included this splitting of disconnected communities in their connected components before the aggregation (although it seems I never pushed the code to launchpad and only shared with @LuckyMD). However, I had forgotten about that problem when developing the new package https://github.com/vtraag/louvain-igraph, so I never split communities in that package. You did that yourself in that implementation? Or at least, that would be my guess, if I read the paper correctly. . P.S. I sympathise @falexwolf, I never had twins, but single babies already take up quite some of your time. Good luck! Surely it will get less stressful over time...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:343,integrability,version,version,343,"Interesting example, @LuckyMD, indeed I'd be interested to reading more about it (you can email me if you want). Thank you for letting me know @falexwolf that the disconnected comunities are also mentioned in the [paper](https://doi.org/10.1101/208819). I'll be sure to point to both your thesis @LuckyMD and the paper @falexwolf in a revised version. Interestingly, in an earlier implementation of the Louvain algorithm (https://launchpad.net/louvain), I indeed included this splitting of disconnected communities in their connected components before the aggregation (although it seems I never pushed the code to launchpad and only shared with @LuckyMD). However, I had forgotten about that problem when developing the new package https://github.com/vtraag/louvain-igraph, so I never split communities in that package. You did that yourself in that implementation? Or at least, that would be my guess, if I read the paper correctly. . P.S. I sympathise @falexwolf, I never had twins, but single babies already take up quite some of your time. Good luck! Surely it will get less stressful over time...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:534,integrability,compon,components,534,"Interesting example, @LuckyMD, indeed I'd be interested to reading more about it (you can email me if you want). Thank you for letting me know @falexwolf that the disconnected comunities are also mentioned in the [paper](https://doi.org/10.1101/208819). I'll be sure to point to both your thesis @LuckyMD and the paper @falexwolf in a revised version. Interestingly, in an earlier implementation of the Louvain algorithm (https://launchpad.net/louvain), I indeed included this splitting of disconnected communities in their connected components before the aggregation (although it seems I never pushed the code to launchpad and only shared with @LuckyMD). However, I had forgotten about that problem when developing the new package https://github.com/vtraag/louvain-igraph, so I never split communities in that package. You did that yourself in that implementation? Or at least, that would be my guess, if I read the paper correctly. . P.S. I sympathise @falexwolf, I never had twins, but single babies already take up quite some of your time. Good luck! Surely it will get less stressful over time...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:534,interoperability,compon,components,534,"Interesting example, @LuckyMD, indeed I'd be interested to reading more about it (you can email me if you want). Thank you for letting me know @falexwolf that the disconnected comunities are also mentioned in the [paper](https://doi.org/10.1101/208819). I'll be sure to point to both your thesis @LuckyMD and the paper @falexwolf in a revised version. Interestingly, in an earlier implementation of the Louvain algorithm (https://launchpad.net/louvain), I indeed included this splitting of disconnected communities in their connected components before the aggregation (although it seems I never pushed the code to launchpad and only shared with @LuckyMD). However, I had forgotten about that problem when developing the new package https://github.com/vtraag/louvain-igraph, so I never split communities in that package. You did that yourself in that implementation? Or at least, that would be my guess, if I read the paper correctly. . P.S. I sympathise @falexwolf, I never had twins, but single babies already take up quite some of your time. Good luck! Surely it will get less stressful over time...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:633,interoperability,share,shared,633,"Interesting example, @LuckyMD, indeed I'd be interested to reading more about it (you can email me if you want). Thank you for letting me know @falexwolf that the disconnected comunities are also mentioned in the [paper](https://doi.org/10.1101/208819). I'll be sure to point to both your thesis @LuckyMD and the paper @falexwolf in a revised version. Interestingly, in an earlier implementation of the Louvain algorithm (https://launchpad.net/louvain), I indeed included this splitting of disconnected communities in their connected components before the aggregation (although it seems I never pushed the code to launchpad and only shared with @LuckyMD). However, I had forgotten about that problem when developing the new package https://github.com/vtraag/louvain-igraph, so I never split communities in that package. You did that yourself in that implementation? Or at least, that would be my guess, if I read the paper correctly. . P.S. I sympathise @falexwolf, I never had twins, but single babies already take up quite some of your time. Good luck! Surely it will get less stressful over time...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:343,modifiability,version,version,343,"Interesting example, @LuckyMD, indeed I'd be interested to reading more about it (you can email me if you want). Thank you for letting me know @falexwolf that the disconnected comunities are also mentioned in the [paper](https://doi.org/10.1101/208819). I'll be sure to point to both your thesis @LuckyMD and the paper @falexwolf in a revised version. Interestingly, in an earlier implementation of the Louvain algorithm (https://launchpad.net/louvain), I indeed included this splitting of disconnected communities in their connected components before the aggregation (although it seems I never pushed the code to launchpad and only shared with @LuckyMD). However, I had forgotten about that problem when developing the new package https://github.com/vtraag/louvain-igraph, so I never split communities in that package. You did that yourself in that implementation? Or at least, that would be my guess, if I read the paper correctly. . P.S. I sympathise @falexwolf, I never had twins, but single babies already take up quite some of your time. Good luck! Surely it will get less stressful over time...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:534,modifiability,compon,components,534,"Interesting example, @LuckyMD, indeed I'd be interested to reading more about it (you can email me if you want). Thank you for letting me know @falexwolf that the disconnected comunities are also mentioned in the [paper](https://doi.org/10.1101/208819). I'll be sure to point to both your thesis @LuckyMD and the paper @falexwolf in a revised version. Interestingly, in an earlier implementation of the Louvain algorithm (https://launchpad.net/louvain), I indeed included this splitting of disconnected communities in their connected components before the aggregation (although it seems I never pushed the code to launchpad and only shared with @LuckyMD). However, I had forgotten about that problem when developing the new package https://github.com/vtraag/louvain-igraph, so I never split communities in that package. You did that yourself in that implementation? Or at least, that would be my guess, if I read the paper correctly. . P.S. I sympathise @falexwolf, I never had twins, but single babies already take up quite some of your time. Good luck! Surely it will get less stressful over time...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:724,modifiability,pac,package,724,"Interesting example, @LuckyMD, indeed I'd be interested to reading more about it (you can email me if you want). Thank you for letting me know @falexwolf that the disconnected comunities are also mentioned in the [paper](https://doi.org/10.1101/208819). I'll be sure to point to both your thesis @LuckyMD and the paper @falexwolf in a revised version. Interestingly, in an earlier implementation of the Louvain algorithm (https://launchpad.net/louvain), I indeed included this splitting of disconnected communities in their connected components before the aggregation (although it seems I never pushed the code to launchpad and only shared with @LuckyMD). However, I had forgotten about that problem when developing the new package https://github.com/vtraag/louvain-igraph, so I never split communities in that package. You did that yourself in that implementation? Or at least, that would be my guess, if I read the paper correctly. . P.S. I sympathise @falexwolf, I never had twins, but single babies already take up quite some of your time. Good luck! Surely it will get less stressful over time...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:811,modifiability,pac,package,811,"Interesting example, @LuckyMD, indeed I'd be interested to reading more about it (you can email me if you want). Thank you for letting me know @falexwolf that the disconnected comunities are also mentioned in the [paper](https://doi.org/10.1101/208819). I'll be sure to point to both your thesis @LuckyMD and the paper @falexwolf in a revised version. Interestingly, in an earlier implementation of the Louvain algorithm (https://launchpad.net/louvain), I indeed included this splitting of disconnected communities in their connected components before the aggregation (although it seems I never pushed the code to launchpad and only shared with @LuckyMD). However, I had forgotten about that problem when developing the new package https://github.com/vtraag/louvain-igraph, so I never split communities in that package. You did that yourself in that implementation? Or at least, that would be my guess, if I read the paper correctly. . P.S. I sympathise @falexwolf, I never had twins, but single babies already take up quite some of your time. Good luck! Surely it will get less stressful over time...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:1038,performance,time,time,1038,"Interesting example, @LuckyMD, indeed I'd be interested to reading more about it (you can email me if you want). Thank you for letting me know @falexwolf that the disconnected comunities are also mentioned in the [paper](https://doi.org/10.1101/208819). I'll be sure to point to both your thesis @LuckyMD and the paper @falexwolf in a revised version. Interestingly, in an earlier implementation of the Louvain algorithm (https://launchpad.net/louvain), I indeed included this splitting of disconnected communities in their connected components before the aggregation (although it seems I never pushed the code to launchpad and only shared with @LuckyMD). However, I had forgotten about that problem when developing the new package https://github.com/vtraag/louvain-igraph, so I never split communities in that package. You did that yourself in that implementation? Or at least, that would be my guess, if I read the paper correctly. . P.S. I sympathise @falexwolf, I never had twins, but single babies already take up quite some of your time. Good luck! Surely it will get less stressful over time...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:1094,performance,time,time,1094,"Interesting example, @LuckyMD, indeed I'd be interested to reading more about it (you can email me if you want). Thank you for letting me know @falexwolf that the disconnected comunities are also mentioned in the [paper](https://doi.org/10.1101/208819). I'll be sure to point to both your thesis @LuckyMD and the paper @falexwolf in a revised version. Interestingly, in an earlier implementation of the Louvain algorithm (https://launchpad.net/louvain), I indeed included this splitting of disconnected communities in their connected components before the aggregation (although it seems I never pushed the code to launchpad and only shared with @LuckyMD). However, I had forgotten about that problem when developing the new package https://github.com/vtraag/louvain-igraph, so I never split communities in that package. You did that yourself in that implementation? Or at least, that would be my guess, if I read the paper correctly. . P.S. I sympathise @falexwolf, I never had twins, but single babies already take up quite some of your time. Good luck! Surely it will get less stressful over time...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:214,availability,cluster,clustering,214,"Maybe I didn't get what you wrote above and you're not talking about the PAGA paper. But in that, I mention the louvain algorithm (and cite your louvain-igraph package in particular) as the primary candidate for a clustering algorithm that produces connected clusters in the knn-graph representation of the data (which was originally represented in some feature space). I made this statement to highlight the difference to non-graph based algorithms like k-means, dbscan, etc. Already quite more than a year ago, after one of my talks, Malte highlighted the fact that your package does indeed a good job of producing connected communities and that he had had some communication with you about this. So, I thought, that's worth a note in the supplement. But it's good to know that the louvain-igraph never actually had this fix. In practice though, it definitely does a very good job for us. Still, really cool that this has been thoroughly addressed in leiden. So, we can all transition to that at some point.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:259,availability,cluster,clusters,259,"Maybe I didn't get what you wrote above and you're not talking about the PAGA paper. But in that, I mention the louvain algorithm (and cite your louvain-igraph package in particular) as the primary candidate for a clustering algorithm that produces connected clusters in the knn-graph representation of the data (which was originally represented in some feature space). I made this statement to highlight the difference to non-graph based algorithms like k-means, dbscan, etc. Already quite more than a year ago, after one of my talks, Malte highlighted the fact that your package does indeed a good job of producing connected communities and that he had had some communication with you about this. So, I thought, that's worth a note in the supplement. But it's good to know that the louvain-igraph never actually had this fix. In practice though, it definitely does a very good job for us. Still, really cool that this has been thoroughly addressed in leiden. So, we can all transition to that at some point.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:382,availability,state,statement,382,"Maybe I didn't get what you wrote above and you're not talking about the PAGA paper. But in that, I mention the louvain algorithm (and cite your louvain-igraph package in particular) as the primary candidate for a clustering algorithm that produces connected clusters in the knn-graph representation of the data (which was originally represented in some feature space). I made this statement to highlight the difference to non-graph based algorithms like k-means, dbscan, etc. Already quite more than a year ago, after one of my talks, Malte highlighted the fact that your package does indeed a good job of producing connected communities and that he had had some communication with you about this. So, I thought, that's worth a note in the supplement. But it's good to know that the louvain-igraph never actually had this fix. In practice though, it definitely does a very good job for us. Still, really cool that this has been thoroughly addressed in leiden. So, we can all transition to that at some point.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:214,deployability,cluster,clustering,214,"Maybe I didn't get what you wrote above and you're not talking about the PAGA paper. But in that, I mention the louvain algorithm (and cite your louvain-igraph package in particular) as the primary candidate for a clustering algorithm that produces connected clusters in the knn-graph representation of the data (which was originally represented in some feature space). I made this statement to highlight the difference to non-graph based algorithms like k-means, dbscan, etc. Already quite more than a year ago, after one of my talks, Malte highlighted the fact that your package does indeed a good job of producing connected communities and that he had had some communication with you about this. So, I thought, that's worth a note in the supplement. But it's good to know that the louvain-igraph never actually had this fix. In practice though, it definitely does a very good job for us. Still, really cool that this has been thoroughly addressed in leiden. So, we can all transition to that at some point.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:259,deployability,cluster,clusters,259,"Maybe I didn't get what you wrote above and you're not talking about the PAGA paper. But in that, I mention the louvain algorithm (and cite your louvain-igraph package in particular) as the primary candidate for a clustering algorithm that produces connected clusters in the knn-graph representation of the data (which was originally represented in some feature space). I made this statement to highlight the difference to non-graph based algorithms like k-means, dbscan, etc. Already quite more than a year ago, after one of my talks, Malte highlighted the fact that your package does indeed a good job of producing connected communities and that he had had some communication with you about this. So, I thought, that's worth a note in the supplement. But it's good to know that the louvain-igraph never actually had this fix. In practice though, it definitely does a very good job for us. Still, really cool that this has been thoroughly addressed in leiden. So, we can all transition to that at some point.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:905,energy efficiency,cool,cool,905,"Maybe I didn't get what you wrote above and you're not talking about the PAGA paper. But in that, I mention the louvain algorithm (and cite your louvain-igraph package in particular) as the primary candidate for a clustering algorithm that produces connected clusters in the knn-graph representation of the data (which was originally represented in some feature space). I made this statement to highlight the difference to non-graph based algorithms like k-means, dbscan, etc. Already quite more than a year ago, after one of my talks, Malte highlighted the fact that your package does indeed a good job of producing connected communities and that he had had some communication with you about this. So, I thought, that's worth a note in the supplement. But it's good to know that the louvain-igraph never actually had this fix. In practice though, it definitely does a very good job for us. Still, really cool that this has been thoroughly addressed in leiden. So, we can all transition to that at some point.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:382,integrability,state,statement,382,"Maybe I didn't get what you wrote above and you're not talking about the PAGA paper. But in that, I mention the louvain algorithm (and cite your louvain-igraph package in particular) as the primary candidate for a clustering algorithm that produces connected clusters in the knn-graph representation of the data (which was originally represented in some feature space). I made this statement to highlight the difference to non-graph based algorithms like k-means, dbscan, etc. Already quite more than a year ago, after one of my talks, Malte highlighted the fact that your package does indeed a good job of producing connected communities and that he had had some communication with you about this. So, I thought, that's worth a note in the supplement. But it's good to know that the louvain-igraph never actually had this fix. In practice though, it definitely does a very good job for us. Still, really cool that this has been thoroughly addressed in leiden. So, we can all transition to that at some point.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:160,modifiability,pac,package,160,"Maybe I didn't get what you wrote above and you're not talking about the PAGA paper. But in that, I mention the louvain algorithm (and cite your louvain-igraph package in particular) as the primary candidate for a clustering algorithm that produces connected clusters in the knn-graph representation of the data (which was originally represented in some feature space). I made this statement to highlight the difference to non-graph based algorithms like k-means, dbscan, etc. Already quite more than a year ago, after one of my talks, Malte highlighted the fact that your package does indeed a good job of producing connected communities and that he had had some communication with you about this. So, I thought, that's worth a note in the supplement. But it's good to know that the louvain-igraph never actually had this fix. In practice though, it definitely does a very good job for us. Still, really cool that this has been thoroughly addressed in leiden. So, we can all transition to that at some point.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:573,modifiability,pac,package,573,"Maybe I didn't get what you wrote above and you're not talking about the PAGA paper. But in that, I mention the louvain algorithm (and cite your louvain-igraph package in particular) as the primary candidate for a clustering algorithm that produces connected clusters in the knn-graph representation of the data (which was originally represented in some feature space). I made this statement to highlight the difference to non-graph based algorithms like k-means, dbscan, etc. Already quite more than a year ago, after one of my talks, Malte highlighted the fact that your package does indeed a good job of producing connected communities and that he had had some communication with you about this. So, I thought, that's worth a note in the supplement. But it's good to know that the louvain-igraph never actually had this fix. In practice though, it definitely does a very good job for us. Still, really cool that this has been thoroughly addressed in leiden. So, we can all transition to that at some point.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:581,reliability,doe,does,581,"Maybe I didn't get what you wrote above and you're not talking about the PAGA paper. But in that, I mention the louvain algorithm (and cite your louvain-igraph package in particular) as the primary candidate for a clustering algorithm that produces connected clusters in the knn-graph representation of the data (which was originally represented in some feature space). I made this statement to highlight the difference to non-graph based algorithms like k-means, dbscan, etc. Already quite more than a year ago, after one of my talks, Malte highlighted the fact that your package does indeed a good job of producing connected communities and that he had had some communication with you about this. So, I thought, that's worth a note in the supplement. But it's good to know that the louvain-igraph never actually had this fix. In practice though, it definitely does a very good job for us. Still, really cool that this has been thoroughly addressed in leiden. So, we can all transition to that at some point.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:831,reliability,pra,practice,831,"Maybe I didn't get what you wrote above and you're not talking about the PAGA paper. But in that, I mention the louvain algorithm (and cite your louvain-igraph package in particular) as the primary candidate for a clustering algorithm that produces connected clusters in the knn-graph representation of the data (which was originally represented in some feature space). I made this statement to highlight the difference to non-graph based algorithms like k-means, dbscan, etc. Already quite more than a year ago, after one of my talks, Malte highlighted the fact that your package does indeed a good job of producing connected communities and that he had had some communication with you about this. So, I thought, that's worth a note in the supplement. But it's good to know that the louvain-igraph never actually had this fix. In practice though, it definitely does a very good job for us. Still, really cool that this has been thoroughly addressed in leiden. So, we can all transition to that at some point.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:862,reliability,doe,does,862,"Maybe I didn't get what you wrote above and you're not talking about the PAGA paper. But in that, I mention the louvain algorithm (and cite your louvain-igraph package in particular) as the primary candidate for a clustering algorithm that produces connected clusters in the knn-graph representation of the data (which was originally represented in some feature space). I made this statement to highlight the difference to non-graph based algorithms like k-means, dbscan, etc. Already quite more than a year ago, after one of my talks, Malte highlighted the fact that your package does indeed a good job of producing connected communities and that he had had some communication with you about this. So, I thought, that's worth a note in the supplement. But it's good to know that the louvain-igraph never actually had this fix. In practice though, it definitely does a very good job for us. Still, really cool that this has been thoroughly addressed in leiden. So, we can all transition to that at some point.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:114,integrability,coupl,couple,114,"P.S. And thank you for the kind words about the babies, @vtraag! Yes, it already got a lot better after the first couple of months in the nights... but at daytime, you still don't get one single quiet moment... I expect this to go on like this for another year or so... let's see :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:114,modifiability,coupl,couple,114,"P.S. And thank you for the kind words about the babies, @vtraag! Yes, it already got a lot better after the first couple of months in the nights... but at daytime, you still don't get one single quiet moment... I expect this to go on like this for another year or so... let's see :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:114,testability,coupl,couple,114,"P.S. And thank you for the kind words about the babies, @vtraag! Yes, it already got a lot better after the first couple of months in the nights... but at daytime, you still don't get one single quiet moment... I expect this to go on like this for another year or so... let's see :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:7,interoperability,specif,specifically,7,"What I specifically referred to is this part:. > The original implementation of the Louvain algorithm could lead to disconnected communities when nodes were assigned to a common community with a single node connecting two or more parts of this community. When the central node was reconsidered by the algorithm and moved to a different community, a disconnected community remained. This unexpected behaviour could be fixed by splitting disconnected communities before each community aggregation step in the implementation of [47]. . Here [47] refers to the [`louvain-igraph`](https://github.com/vtraag/louvain-igraph) package. My point was that in that package I do not split disconnected communities before aggregation, contrary to the code that I shared with @LuckyMD at one point. Anyway, good to hear the `louvain-igraph` package already did a good job for you! Looking forward to seeing the Leiden algorithm implemented in the `scanpy` package.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:749,interoperability,share,shared,749,"What I specifically referred to is this part:. > The original implementation of the Louvain algorithm could lead to disconnected communities when nodes were assigned to a common community with a single node connecting two or more parts of this community. When the central node was reconsidered by the algorithm and moved to a different community, a disconnected community remained. This unexpected behaviour could be fixed by splitting disconnected communities before each community aggregation step in the implementation of [47]. . Here [47] refers to the [`louvain-igraph`](https://github.com/vtraag/louvain-igraph) package. My point was that in that package I do not split disconnected communities before aggregation, contrary to the code that I shared with @LuckyMD at one point. Anyway, good to hear the `louvain-igraph` package already did a good job for you! Looking forward to seeing the Leiden algorithm implemented in the `scanpy` package.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:618,modifiability,pac,package,618,"What I specifically referred to is this part:. > The original implementation of the Louvain algorithm could lead to disconnected communities when nodes were assigned to a common community with a single node connecting two or more parts of this community. When the central node was reconsidered by the algorithm and moved to a different community, a disconnected community remained. This unexpected behaviour could be fixed by splitting disconnected communities before each community aggregation step in the implementation of [47]. . Here [47] refers to the [`louvain-igraph`](https://github.com/vtraag/louvain-igraph) package. My point was that in that package I do not split disconnected communities before aggregation, contrary to the code that I shared with @LuckyMD at one point. Anyway, good to hear the `louvain-igraph` package already did a good job for you! Looking forward to seeing the Leiden algorithm implemented in the `scanpy` package.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:653,modifiability,pac,package,653,"What I specifically referred to is this part:. > The original implementation of the Louvain algorithm could lead to disconnected communities when nodes were assigned to a common community with a single node connecting two or more parts of this community. When the central node was reconsidered by the algorithm and moved to a different community, a disconnected community remained. This unexpected behaviour could be fixed by splitting disconnected communities before each community aggregation step in the implementation of [47]. . Here [47] refers to the [`louvain-igraph`](https://github.com/vtraag/louvain-igraph) package. My point was that in that package I do not split disconnected communities before aggregation, contrary to the code that I shared with @LuckyMD at one point. Anyway, good to hear the `louvain-igraph` package already did a good job for you! Looking forward to seeing the Leiden algorithm implemented in the `scanpy` package.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:826,modifiability,pac,package,826,"What I specifically referred to is this part:. > The original implementation of the Louvain algorithm could lead to disconnected communities when nodes were assigned to a common community with a single node connecting two or more parts of this community. When the central node was reconsidered by the algorithm and moved to a different community, a disconnected community remained. This unexpected behaviour could be fixed by splitting disconnected communities before each community aggregation step in the implementation of [47]. . Here [47] refers to the [`louvain-igraph`](https://github.com/vtraag/louvain-igraph) package. My point was that in that package I do not split disconnected communities before aggregation, contrary to the code that I shared with @LuckyMD at one point. Anyway, good to hear the `louvain-igraph` package already did a good job for you! Looking forward to seeing the Leiden algorithm implemented in the `scanpy` package.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:941,modifiability,pac,package,941,"What I specifically referred to is this part:. > The original implementation of the Louvain algorithm could lead to disconnected communities when nodes were assigned to a common community with a single node connecting two or more parts of this community. When the central node was reconsidered by the algorithm and moved to a different community, a disconnected community remained. This unexpected behaviour could be fixed by splitting disconnected communities before each community aggregation step in the implementation of [47]. . Here [47] refers to the [`louvain-igraph`](https://github.com/vtraag/louvain-igraph) package. My point was that in that package I do not split disconnected communities before aggregation, contrary to the code that I shared with @LuckyMD at one point. Anyway, good to hear the `louvain-igraph` package already did a good job for you! Looking forward to seeing the Leiden algorithm implemented in the `scanpy` package.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:398,usability,behavi,behaviour,398,"What I specifically referred to is this part:. > The original implementation of the Louvain algorithm could lead to disconnected communities when nodes were assigned to a common community with a single node connecting two or more parts of this community. When the central node was reconsidered by the algorithm and moved to a different community, a disconnected community remained. This unexpected behaviour could be fixed by splitting disconnected communities before each community aggregation step in the implementation of [47]. . Here [47] refers to the [`louvain-igraph`](https://github.com/vtraag/louvain-igraph) package. My point was that in that package I do not split disconnected communities before aggregation, contrary to the code that I shared with @LuckyMD at one point. Anyway, good to hear the `louvain-igraph` package already did a good job for you! Looking forward to seeing the Leiden algorithm implemented in the `scanpy` package.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:30,integrability,wrap,wrapped,30,"Well, implemented Id say wrapped.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:374,deployability,version,version,374,"> My point was that in that package I do not split disconnected communities before aggregation, contrary to the code that I shared with @LuckyMD at one point. @vtraag Yes, I understood that from your previous post, that's why wrote:. > But it's good to know that the louvain-igraph never actually had this fix. . So, yes, the note you pulled out from the paper (the revised version of it was worded by @LuckyMD) gives `louvain-igraph` wrong credit. So, if I'm able to make another revision, I'll replace it with the reference to the leiden preprint. Nonetheless, `louvain-igraph` remains an awesome package and I'll maintain all other references to it. :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:374,integrability,version,version,374,"> My point was that in that package I do not split disconnected communities before aggregation, contrary to the code that I shared with @LuckyMD at one point. @vtraag Yes, I understood that from your previous post, that's why wrote:. > But it's good to know that the louvain-igraph never actually had this fix. . So, yes, the note you pulled out from the paper (the revised version of it was worded by @LuckyMD) gives `louvain-igraph` wrong credit. So, if I'm able to make another revision, I'll replace it with the reference to the leiden preprint. Nonetheless, `louvain-igraph` remains an awesome package and I'll maintain all other references to it. :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:124,interoperability,share,shared,124,"> My point was that in that package I do not split disconnected communities before aggregation, contrary to the code that I shared with @LuckyMD at one point. @vtraag Yes, I understood that from your previous post, that's why wrote:. > But it's good to know that the louvain-igraph never actually had this fix. . So, yes, the note you pulled out from the paper (the revised version of it was worded by @LuckyMD) gives `louvain-igraph` wrong credit. So, if I'm able to make another revision, I'll replace it with the reference to the leiden preprint. Nonetheless, `louvain-igraph` remains an awesome package and I'll maintain all other references to it. :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:28,modifiability,pac,package,28,"> My point was that in that package I do not split disconnected communities before aggregation, contrary to the code that I shared with @LuckyMD at one point. @vtraag Yes, I understood that from your previous post, that's why wrote:. > But it's good to know that the louvain-igraph never actually had this fix. . So, yes, the note you pulled out from the paper (the revised version of it was worded by @LuckyMD) gives `louvain-igraph` wrong credit. So, if I'm able to make another revision, I'll replace it with the reference to the leiden preprint. Nonetheless, `louvain-igraph` remains an awesome package and I'll maintain all other references to it. :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:374,modifiability,version,version,374,"> My point was that in that package I do not split disconnected communities before aggregation, contrary to the code that I shared with @LuckyMD at one point. @vtraag Yes, I understood that from your previous post, that's why wrote:. > But it's good to know that the louvain-igraph never actually had this fix. . So, yes, the note you pulled out from the paper (the revised version of it was worded by @LuckyMD) gives `louvain-igraph` wrong credit. So, if I'm able to make another revision, I'll replace it with the reference to the leiden preprint. Nonetheless, `louvain-igraph` remains an awesome package and I'll maintain all other references to it. :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:599,modifiability,pac,package,599,"> My point was that in that package I do not split disconnected communities before aggregation, contrary to the code that I shared with @LuckyMD at one point. @vtraag Yes, I understood that from your previous post, that's why wrote:. > But it's good to know that the louvain-igraph never actually had this fix. . So, yes, the note you pulled out from the paper (the revised version of it was worded by @LuckyMD) gives `louvain-igraph` wrong credit. So, if I'm able to make another revision, I'll replace it with the reference to the leiden preprint. Nonetheless, `louvain-igraph` remains an awesome package and I'll maintain all other references to it. :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:616,modifiability,maintain,maintain,616,"> My point was that in that package I do not split disconnected communities before aggregation, contrary to the code that I shared with @LuckyMD at one point. @vtraag Yes, I understood that from your previous post, that's why wrote:. > But it's good to know that the louvain-igraph never actually had this fix. . So, yes, the note you pulled out from the paper (the revised version of it was worded by @LuckyMD) gives `louvain-igraph` wrong credit. So, if I'm able to make another revision, I'll replace it with the reference to the leiden preprint. Nonetheless, `louvain-igraph` remains an awesome package and I'll maintain all other references to it. :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:616,safety,maintain,maintain,616,"> My point was that in that package I do not split disconnected communities before aggregation, contrary to the code that I shared with @LuckyMD at one point. @vtraag Yes, I understood that from your previous post, that's why wrote:. > But it's good to know that the louvain-igraph never actually had this fix. . So, yes, the note you pulled out from the paper (the revised version of it was worded by @LuckyMD) gives `louvain-igraph` wrong credit. So, if I'm able to make another revision, I'll replace it with the reference to the leiden preprint. Nonetheless, `louvain-igraph` remains an awesome package and I'll maintain all other references to it. :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:105,deployability,version,version,105,Yeah... sorry about that @falexwolf. I assumed the the python implementation had the same fix as the C++ version I got from you @vtraag.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:105,integrability,version,version,105,Yeah... sorry about that @falexwolf. I assumed the the python implementation had the same fix as the C++ version I got from you @vtraag.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:105,modifiability,version,version,105,Yeah... sorry about that @falexwolf. I assumed the the python implementation had the same fix as the C++ version I got from you @vtraag.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:101,deployability,contain,contain,101,"@falexwolf thanks! And no worries about it, I just wanted to make clear that `louvain-igraph` didn't contain that fix. Indeed @LuckyMD, I would have assumed the same myself :smile:. But alas, the problem got lost in the mists of time (or well, my mist of time: I forgot about it), until it resurfaced in another context.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:229,performance,time,time,229,"@falexwolf thanks! And no worries about it, I just wanted to make clear that `louvain-igraph` didn't contain that fix. Indeed @LuckyMD, I would have assumed the same myself :smile:. But alas, the problem got lost in the mists of time (or well, my mist of time: I forgot about it), until it resurfaced in another context.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:255,performance,time,time,255,"@falexwolf thanks! And no worries about it, I just wanted to make clear that `louvain-igraph` didn't contain that fix. Indeed @LuckyMD, I would have assumed the same myself :smile:. But alas, the problem got lost in the mists of time (or well, my mist of time: I forgot about it), until it resurfaced in another context.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:312,testability,context,context,312,"@falexwolf thanks! And no worries about it, I just wanted to make clear that `louvain-igraph` didn't contain that fix. Indeed @LuckyMD, I would have assumed the same myself :smile:. But alas, the problem got lost in the mists of time (or well, my mist of time: I forgot about it), until it resurfaced in another context.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/350:66,usability,clear,clear,66,"@falexwolf thanks! And no worries about it, I just wanted to make clear that `louvain-igraph` didn't contain that fix. Indeed @LuckyMD, I would have assumed the same myself :smile:. But alas, the problem got lost in the mists of time (or well, my mist of time: I forgot about it), until it resurfaced in another context.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350
https://github.com/scverse/scanpy/issues/351:82,usability,learn,learn,82,"This is very interesting! It would be awesome if you linked to a small example to learn what you do exactly! I guess, a PR would then be more than welcome! ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:1014,deployability,modul,module,1014,"@falexwolf Mostly just following along with the Seurat vignettes here using scanpy:. * [Demultiplexing with cell hashing](https://satijalab.org/seurat/hashing_vignette.html). * [Multimodal analysis with CITE-seq ADTs](https://satijalab.org/seurat/multimodal_vignette.html). Mostly I'm using [CITE-seq-count](https://github.com/Hoohm/CITE-seq-Count) to generate counts matrices then storing the ADT counts in `adata.obsm[""X_adt""]` and HTO counts in `adata.obsm[""X_hto""]`. From there, I generate additional metadata about the cells which I store in either `adata.obs` (e.g, for HTOs, `adata.obs.global_classification` stores `[""singlet"", ""doublet"", ""negative""]` and `adata.obs.tag_class` stores `[""hashtag_1"", ""hashtag_2"", ...]`). I'd be happy to contribute a PR in the next couple weeks. A quick question though: the workflow involves some IO, normalization, classification, and plotting, so where would be the best entry point for this? I was thinking about stashing all related functionality in a `multimodal.py` module, but then should they be part of the `tools` api or a separate api like `sc.mm`? Or should I spread out the functionality across the existing `pp`, `tl`, `pl` apis?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:1066,deployability,api,api,1066,"@falexwolf Mostly just following along with the Seurat vignettes here using scanpy:. * [Demultiplexing with cell hashing](https://satijalab.org/seurat/hashing_vignette.html). * [Multimodal analysis with CITE-seq ADTs](https://satijalab.org/seurat/multimodal_vignette.html). Mostly I'm using [CITE-seq-count](https://github.com/Hoohm/CITE-seq-Count) to generate counts matrices then storing the ADT counts in `adata.obsm[""X_adt""]` and HTO counts in `adata.obsm[""X_hto""]`. From there, I generate additional metadata about the cells which I store in either `adata.obs` (e.g, for HTOs, `adata.obs.global_classification` stores `[""singlet"", ""doublet"", ""negative""]` and `adata.obs.tag_class` stores `[""hashtag_1"", ""hashtag_2"", ...]`). I'd be happy to contribute a PR in the next couple weeks. A quick question though: the workflow involves some IO, normalization, classification, and plotting, so where would be the best entry point for this? I was thinking about stashing all related functionality in a `multimodal.py` module, but then should they be part of the `tools` api or a separate api like `sc.mm`? Or should I spread out the functionality across the existing `pp`, `tl`, `pl` apis?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:1084,deployability,api,api,1084,"@falexwolf Mostly just following along with the Seurat vignettes here using scanpy:. * [Demultiplexing with cell hashing](https://satijalab.org/seurat/hashing_vignette.html). * [Multimodal analysis with CITE-seq ADTs](https://satijalab.org/seurat/multimodal_vignette.html). Mostly I'm using [CITE-seq-count](https://github.com/Hoohm/CITE-seq-Count) to generate counts matrices then storing the ADT counts in `adata.obsm[""X_adt""]` and HTO counts in `adata.obsm[""X_hto""]`. From there, I generate additional metadata about the cells which I store in either `adata.obs` (e.g, for HTOs, `adata.obs.global_classification` stores `[""singlet"", ""doublet"", ""negative""]` and `adata.obs.tag_class` stores `[""hashtag_1"", ""hashtag_2"", ...]`). I'd be happy to contribute a PR in the next couple weeks. A quick question though: the workflow involves some IO, normalization, classification, and plotting, so where would be the best entry point for this? I was thinking about stashing all related functionality in a `multimodal.py` module, but then should they be part of the `tools` api or a separate api like `sc.mm`? Or should I spread out the functionality across the existing `pp`, `tl`, `pl` apis?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:1180,deployability,api,apis,1180,"@falexwolf Mostly just following along with the Seurat vignettes here using scanpy:. * [Demultiplexing with cell hashing](https://satijalab.org/seurat/hashing_vignette.html). * [Multimodal analysis with CITE-seq ADTs](https://satijalab.org/seurat/multimodal_vignette.html). Mostly I'm using [CITE-seq-count](https://github.com/Hoohm/CITE-seq-Count) to generate counts matrices then storing the ADT counts in `adata.obsm[""X_adt""]` and HTO counts in `adata.obsm[""X_hto""]`. From there, I generate additional metadata about the cells which I store in either `adata.obs` (e.g, for HTOs, `adata.obs.global_classification` stores `[""singlet"", ""doublet"", ""negative""]` and `adata.obs.tag_class` stores `[""hashtag_1"", ""hashtag_2"", ...]`). I'd be happy to contribute a PR in the next couple weeks. A quick question though: the workflow involves some IO, normalization, classification, and plotting, so where would be the best entry point for this? I was thinking about stashing all related functionality in a `multimodal.py` module, but then should they be part of the `tools` api or a separate api like `sc.mm`? Or should I spread out the functionality across the existing `pp`, `tl`, `pl` apis?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:773,integrability,coupl,couple,773,"@falexwolf Mostly just following along with the Seurat vignettes here using scanpy:. * [Demultiplexing with cell hashing](https://satijalab.org/seurat/hashing_vignette.html). * [Multimodal analysis with CITE-seq ADTs](https://satijalab.org/seurat/multimodal_vignette.html). Mostly I'm using [CITE-seq-count](https://github.com/Hoohm/CITE-seq-Count) to generate counts matrices then storing the ADT counts in `adata.obsm[""X_adt""]` and HTO counts in `adata.obsm[""X_hto""]`. From there, I generate additional metadata about the cells which I store in either `adata.obs` (e.g, for HTOs, `adata.obs.global_classification` stores `[""singlet"", ""doublet"", ""negative""]` and `adata.obs.tag_class` stores `[""hashtag_1"", ""hashtag_2"", ...]`). I'd be happy to contribute a PR in the next couple weeks. A quick question though: the workflow involves some IO, normalization, classification, and plotting, so where would be the best entry point for this? I was thinking about stashing all related functionality in a `multimodal.py` module, but then should they be part of the `tools` api or a separate api like `sc.mm`? Or should I spread out the functionality across the existing `pp`, `tl`, `pl` apis?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:1066,integrability,api,api,1066,"@falexwolf Mostly just following along with the Seurat vignettes here using scanpy:. * [Demultiplexing with cell hashing](https://satijalab.org/seurat/hashing_vignette.html). * [Multimodal analysis with CITE-seq ADTs](https://satijalab.org/seurat/multimodal_vignette.html). Mostly I'm using [CITE-seq-count](https://github.com/Hoohm/CITE-seq-Count) to generate counts matrices then storing the ADT counts in `adata.obsm[""X_adt""]` and HTO counts in `adata.obsm[""X_hto""]`. From there, I generate additional metadata about the cells which I store in either `adata.obs` (e.g, for HTOs, `adata.obs.global_classification` stores `[""singlet"", ""doublet"", ""negative""]` and `adata.obs.tag_class` stores `[""hashtag_1"", ""hashtag_2"", ...]`). I'd be happy to contribute a PR in the next couple weeks. A quick question though: the workflow involves some IO, normalization, classification, and plotting, so where would be the best entry point for this? I was thinking about stashing all related functionality in a `multimodal.py` module, but then should they be part of the `tools` api or a separate api like `sc.mm`? Or should I spread out the functionality across the existing `pp`, `tl`, `pl` apis?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:1084,integrability,api,api,1084,"@falexwolf Mostly just following along with the Seurat vignettes here using scanpy:. * [Demultiplexing with cell hashing](https://satijalab.org/seurat/hashing_vignette.html). * [Multimodal analysis with CITE-seq ADTs](https://satijalab.org/seurat/multimodal_vignette.html). Mostly I'm using [CITE-seq-count](https://github.com/Hoohm/CITE-seq-Count) to generate counts matrices then storing the ADT counts in `adata.obsm[""X_adt""]` and HTO counts in `adata.obsm[""X_hto""]`. From there, I generate additional metadata about the cells which I store in either `adata.obs` (e.g, for HTOs, `adata.obs.global_classification` stores `[""singlet"", ""doublet"", ""negative""]` and `adata.obs.tag_class` stores `[""hashtag_1"", ""hashtag_2"", ...]`). I'd be happy to contribute a PR in the next couple weeks. A quick question though: the workflow involves some IO, normalization, classification, and plotting, so where would be the best entry point for this? I was thinking about stashing all related functionality in a `multimodal.py` module, but then should they be part of the `tools` api or a separate api like `sc.mm`? Or should I spread out the functionality across the existing `pp`, `tl`, `pl` apis?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:1180,integrability,api,apis,1180,"@falexwolf Mostly just following along with the Seurat vignettes here using scanpy:. * [Demultiplexing with cell hashing](https://satijalab.org/seurat/hashing_vignette.html). * [Multimodal analysis with CITE-seq ADTs](https://satijalab.org/seurat/multimodal_vignette.html). Mostly I'm using [CITE-seq-count](https://github.com/Hoohm/CITE-seq-Count) to generate counts matrices then storing the ADT counts in `adata.obsm[""X_adt""]` and HTO counts in `adata.obsm[""X_hto""]`. From there, I generate additional metadata about the cells which I store in either `adata.obs` (e.g, for HTOs, `adata.obs.global_classification` stores `[""singlet"", ""doublet"", ""negative""]` and `adata.obs.tag_class` stores `[""hashtag_1"", ""hashtag_2"", ...]`). I'd be happy to contribute a PR in the next couple weeks. A quick question though: the workflow involves some IO, normalization, classification, and plotting, so where would be the best entry point for this? I was thinking about stashing all related functionality in a `multimodal.py` module, but then should they be part of the `tools` api or a separate api like `sc.mm`? Or should I spread out the functionality across the existing `pp`, `tl`, `pl` apis?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:1066,interoperability,api,api,1066,"@falexwolf Mostly just following along with the Seurat vignettes here using scanpy:. * [Demultiplexing with cell hashing](https://satijalab.org/seurat/hashing_vignette.html). * [Multimodal analysis with CITE-seq ADTs](https://satijalab.org/seurat/multimodal_vignette.html). Mostly I'm using [CITE-seq-count](https://github.com/Hoohm/CITE-seq-Count) to generate counts matrices then storing the ADT counts in `adata.obsm[""X_adt""]` and HTO counts in `adata.obsm[""X_hto""]`. From there, I generate additional metadata about the cells which I store in either `adata.obs` (e.g, for HTOs, `adata.obs.global_classification` stores `[""singlet"", ""doublet"", ""negative""]` and `adata.obs.tag_class` stores `[""hashtag_1"", ""hashtag_2"", ...]`). I'd be happy to contribute a PR in the next couple weeks. A quick question though: the workflow involves some IO, normalization, classification, and plotting, so where would be the best entry point for this? I was thinking about stashing all related functionality in a `multimodal.py` module, but then should they be part of the `tools` api or a separate api like `sc.mm`? Or should I spread out the functionality across the existing `pp`, `tl`, `pl` apis?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:1084,interoperability,api,api,1084,"@falexwolf Mostly just following along with the Seurat vignettes here using scanpy:. * [Demultiplexing with cell hashing](https://satijalab.org/seurat/hashing_vignette.html). * [Multimodal analysis with CITE-seq ADTs](https://satijalab.org/seurat/multimodal_vignette.html). Mostly I'm using [CITE-seq-count](https://github.com/Hoohm/CITE-seq-Count) to generate counts matrices then storing the ADT counts in `adata.obsm[""X_adt""]` and HTO counts in `adata.obsm[""X_hto""]`. From there, I generate additional metadata about the cells which I store in either `adata.obs` (e.g, for HTOs, `adata.obs.global_classification` stores `[""singlet"", ""doublet"", ""negative""]` and `adata.obs.tag_class` stores `[""hashtag_1"", ""hashtag_2"", ...]`). I'd be happy to contribute a PR in the next couple weeks. A quick question though: the workflow involves some IO, normalization, classification, and plotting, so where would be the best entry point for this? I was thinking about stashing all related functionality in a `multimodal.py` module, but then should they be part of the `tools` api or a separate api like `sc.mm`? Or should I spread out the functionality across the existing `pp`, `tl`, `pl` apis?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:1180,interoperability,api,apis,1180,"@falexwolf Mostly just following along with the Seurat vignettes here using scanpy:. * [Demultiplexing with cell hashing](https://satijalab.org/seurat/hashing_vignette.html). * [Multimodal analysis with CITE-seq ADTs](https://satijalab.org/seurat/multimodal_vignette.html). Mostly I'm using [CITE-seq-count](https://github.com/Hoohm/CITE-seq-Count) to generate counts matrices then storing the ADT counts in `adata.obsm[""X_adt""]` and HTO counts in `adata.obsm[""X_hto""]`. From there, I generate additional metadata about the cells which I store in either `adata.obs` (e.g, for HTOs, `adata.obs.global_classification` stores `[""singlet"", ""doublet"", ""negative""]` and `adata.obs.tag_class` stores `[""hashtag_1"", ""hashtag_2"", ...]`). I'd be happy to contribute a PR in the next couple weeks. A quick question though: the workflow involves some IO, normalization, classification, and plotting, so where would be the best entry point for this? I was thinking about stashing all related functionality in a `multimodal.py` module, but then should they be part of the `tools` api or a separate api like `sc.mm`? Or should I spread out the functionality across the existing `pp`, `tl`, `pl` apis?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:773,modifiability,coupl,couple,773,"@falexwolf Mostly just following along with the Seurat vignettes here using scanpy:. * [Demultiplexing with cell hashing](https://satijalab.org/seurat/hashing_vignette.html). * [Multimodal analysis with CITE-seq ADTs](https://satijalab.org/seurat/multimodal_vignette.html). Mostly I'm using [CITE-seq-count](https://github.com/Hoohm/CITE-seq-Count) to generate counts matrices then storing the ADT counts in `adata.obsm[""X_adt""]` and HTO counts in `adata.obsm[""X_hto""]`. From there, I generate additional metadata about the cells which I store in either `adata.obs` (e.g, for HTOs, `adata.obs.global_classification` stores `[""singlet"", ""doublet"", ""negative""]` and `adata.obs.tag_class` stores `[""hashtag_1"", ""hashtag_2"", ...]`). I'd be happy to contribute a PR in the next couple weeks. A quick question though: the workflow involves some IO, normalization, classification, and plotting, so where would be the best entry point for this? I was thinking about stashing all related functionality in a `multimodal.py` module, but then should they be part of the `tools` api or a separate api like `sc.mm`? Or should I spread out the functionality across the existing `pp`, `tl`, `pl` apis?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:1014,modifiability,modul,module,1014,"@falexwolf Mostly just following along with the Seurat vignettes here using scanpy:. * [Demultiplexing with cell hashing](https://satijalab.org/seurat/hashing_vignette.html). * [Multimodal analysis with CITE-seq ADTs](https://satijalab.org/seurat/multimodal_vignette.html). Mostly I'm using [CITE-seq-count](https://github.com/Hoohm/CITE-seq-Count) to generate counts matrices then storing the ADT counts in `adata.obsm[""X_adt""]` and HTO counts in `adata.obsm[""X_hto""]`. From there, I generate additional metadata about the cells which I store in either `adata.obs` (e.g, for HTOs, `adata.obs.global_classification` stores `[""singlet"", ""doublet"", ""negative""]` and `adata.obs.tag_class` stores `[""hashtag_1"", ""hashtag_2"", ...]`). I'd be happy to contribute a PR in the next couple weeks. A quick question though: the workflow involves some IO, normalization, classification, and plotting, so where would be the best entry point for this? I was thinking about stashing all related functionality in a `multimodal.py` module, but then should they be part of the `tools` api or a separate api like `sc.mm`? Or should I spread out the functionality across the existing `pp`, `tl`, `pl` apis?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:1014,safety,modul,module,1014,"@falexwolf Mostly just following along with the Seurat vignettes here using scanpy:. * [Demultiplexing with cell hashing](https://satijalab.org/seurat/hashing_vignette.html). * [Multimodal analysis with CITE-seq ADTs](https://satijalab.org/seurat/multimodal_vignette.html). Mostly I'm using [CITE-seq-count](https://github.com/Hoohm/CITE-seq-Count) to generate counts matrices then storing the ADT counts in `adata.obsm[""X_adt""]` and HTO counts in `adata.obsm[""X_hto""]`. From there, I generate additional metadata about the cells which I store in either `adata.obs` (e.g, for HTOs, `adata.obs.global_classification` stores `[""singlet"", ""doublet"", ""negative""]` and `adata.obs.tag_class` stores `[""hashtag_1"", ""hashtag_2"", ...]`). I'd be happy to contribute a PR in the next couple weeks. A quick question though: the workflow involves some IO, normalization, classification, and plotting, so where would be the best entry point for this? I was thinking about stashing all related functionality in a `multimodal.py` module, but then should they be part of the `tools` api or a separate api like `sc.mm`? Or should I spread out the functionality across the existing `pp`, `tl`, `pl` apis?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:113,security,hash,hashing,113,"@falexwolf Mostly just following along with the Seurat vignettes here using scanpy:. * [Demultiplexing with cell hashing](https://satijalab.org/seurat/hashing_vignette.html). * [Multimodal analysis with CITE-seq ADTs](https://satijalab.org/seurat/multimodal_vignette.html). Mostly I'm using [CITE-seq-count](https://github.com/Hoohm/CITE-seq-Count) to generate counts matrices then storing the ADT counts in `adata.obsm[""X_adt""]` and HTO counts in `adata.obsm[""X_hto""]`. From there, I generate additional metadata about the cells which I store in either `adata.obs` (e.g, for HTOs, `adata.obs.global_classification` stores `[""singlet"", ""doublet"", ""negative""]` and `adata.obs.tag_class` stores `[""hashtag_1"", ""hashtag_2"", ...]`). I'd be happy to contribute a PR in the next couple weeks. A quick question though: the workflow involves some IO, normalization, classification, and plotting, so where would be the best entry point for this? I was thinking about stashing all related functionality in a `multimodal.py` module, but then should they be part of the `tools` api or a separate api like `sc.mm`? Or should I spread out the functionality across the existing `pp`, `tl`, `pl` apis?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:773,testability,coupl,couple,773,"@falexwolf Mostly just following along with the Seurat vignettes here using scanpy:. * [Demultiplexing with cell hashing](https://satijalab.org/seurat/hashing_vignette.html). * [Multimodal analysis with CITE-seq ADTs](https://satijalab.org/seurat/multimodal_vignette.html). Mostly I'm using [CITE-seq-count](https://github.com/Hoohm/CITE-seq-Count) to generate counts matrices then storing the ADT counts in `adata.obsm[""X_adt""]` and HTO counts in `adata.obsm[""X_hto""]`. From there, I generate additional metadata about the cells which I store in either `adata.obs` (e.g, for HTOs, `adata.obs.global_classification` stores `[""singlet"", ""doublet"", ""negative""]` and `adata.obs.tag_class` stores `[""hashtag_1"", ""hashtag_2"", ...]`). I'd be happy to contribute a PR in the next couple weeks. A quick question though: the workflow involves some IO, normalization, classification, and plotting, so where would be the best entry point for this? I was thinking about stashing all related functionality in a `multimodal.py` module, but then should they be part of the `tools` api or a separate api like `sc.mm`? Or should I spread out the functionality across the existing `pp`, `tl`, `pl` apis?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:816,usability,workflow,workflow,816,"@falexwolf Mostly just following along with the Seurat vignettes here using scanpy:. * [Demultiplexing with cell hashing](https://satijalab.org/seurat/hashing_vignette.html). * [Multimodal analysis with CITE-seq ADTs](https://satijalab.org/seurat/multimodal_vignette.html). Mostly I'm using [CITE-seq-count](https://github.com/Hoohm/CITE-seq-Count) to generate counts matrices then storing the ADT counts in `adata.obsm[""X_adt""]` and HTO counts in `adata.obsm[""X_hto""]`. From there, I generate additional metadata about the cells which I store in either `adata.obs` (e.g, for HTOs, `adata.obs.global_classification` stores `[""singlet"", ""doublet"", ""negative""]` and `adata.obs.tag_class` stores `[""hashtag_1"", ""hashtag_2"", ...]`). I'd be happy to contribute a PR in the next couple weeks. A quick question though: the workflow involves some IO, normalization, classification, and plotting, so where would be the best entry point for this? I was thinking about stashing all related functionality in a `multimodal.py` module, but then should they be part of the `tools` api or a separate api like `sc.mm`? Or should I spread out the functionality across the existing `pp`, `tl`, `pl` apis?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:1059,usability,tool,tools,1059,"@falexwolf Mostly just following along with the Seurat vignettes here using scanpy:. * [Demultiplexing with cell hashing](https://satijalab.org/seurat/hashing_vignette.html). * [Multimodal analysis with CITE-seq ADTs](https://satijalab.org/seurat/multimodal_vignette.html). Mostly I'm using [CITE-seq-count](https://github.com/Hoohm/CITE-seq-Count) to generate counts matrices then storing the ADT counts in `adata.obsm[""X_adt""]` and HTO counts in `adata.obsm[""X_hto""]`. From there, I generate additional metadata about the cells which I store in either `adata.obs` (e.g, for HTOs, `adata.obs.global_classification` stores `[""singlet"", ""doublet"", ""negative""]` and `adata.obs.tag_class` stores `[""hashtag_1"", ""hashtag_2"", ...]`). I'd be happy to contribute a PR in the next couple weeks. A quick question though: the workflow involves some IO, normalization, classification, and plotting, so where would be the best entry point for this? I was thinking about stashing all related functionality in a `multimodal.py` module, but then should they be part of the `tools` api or a separate api like `sc.mm`? Or should I spread out the functionality across the existing `pp`, `tl`, `pl` apis?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:43,integrability,sub,submit,43,I've got some code to do this. I'll try to submit a PR soon,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:839,deployability,pipelin,pipeline,839,"I'm also interested in this since I'll be analyzing some HTO data soon. . As I wrote [here](https://github.com/theislab/scanpy/pull/797/files/8bcee13537d6353399f1722bac7f60bc943a482f#r335664372), I think we should also discuss the I/O and storage procedures for ADT/HTOs. . @wflynny it makes a lot of sense to use `adata.obsm[""X_adt""]` and `adata.obsm[""X_hto""]` for ADT and HTO counts. One caveat is that we cannot store ADT/HTO barcode strings in `adata.obsm` but I don't know how important this is. For I/O, we can define a `sc.read_antibody_tags(filename)` that reads HTO/ADTs into the `adata.obsm['X_hto']`. Then a simple `sc.pp.classify_hashtags()` method can determine classes and creates new fields like HTO_class in `adata.obs`. @wflynny @njbernstein what do you think? @wflynny what else do you think is needed for a nice HTO/ADT pipeline?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:839,integrability,pipelin,pipeline,839,"I'm also interested in this since I'll be analyzing some HTO data soon. . As I wrote [here](https://github.com/theislab/scanpy/pull/797/files/8bcee13537d6353399f1722bac7f60bc943a482f#r335664372), I think we should also discuss the I/O and storage procedures for ADT/HTOs. . @wflynny it makes a lot of sense to use `adata.obsm[""X_adt""]` and `adata.obsm[""X_hto""]` for ADT and HTO counts. One caveat is that we cannot store ADT/HTO barcode strings in `adata.obsm` but I don't know how important this is. For I/O, we can define a `sc.read_antibody_tags(filename)` that reads HTO/ADTs into the `adata.obsm['X_hto']`. Then a simple `sc.pp.classify_hashtags()` method can determine classes and creates new fields like HTO_class in `adata.obs`. @wflynny @njbernstein what do you think? @wflynny what else do you think is needed for a nice HTO/ADT pipeline?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:231,performance,I/O,I/O,231,"I'm also interested in this since I'll be analyzing some HTO data soon. . As I wrote [here](https://github.com/theislab/scanpy/pull/797/files/8bcee13537d6353399f1722bac7f60bc943a482f#r335664372), I think we should also discuss the I/O and storage procedures for ADT/HTOs. . @wflynny it makes a lot of sense to use `adata.obsm[""X_adt""]` and `adata.obsm[""X_hto""]` for ADT and HTO counts. One caveat is that we cannot store ADT/HTO barcode strings in `adata.obsm` but I don't know how important this is. For I/O, we can define a `sc.read_antibody_tags(filename)` that reads HTO/ADTs into the `adata.obsm['X_hto']`. Then a simple `sc.pp.classify_hashtags()` method can determine classes and creates new fields like HTO_class in `adata.obs`. @wflynny @njbernstein what do you think? @wflynny what else do you think is needed for a nice HTO/ADT pipeline?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:505,performance,I/O,I/O,505,"I'm also interested in this since I'll be analyzing some HTO data soon. . As I wrote [here](https://github.com/theislab/scanpy/pull/797/files/8bcee13537d6353399f1722bac7f60bc943a482f#r335664372), I think we should also discuss the I/O and storage procedures for ADT/HTOs. . @wflynny it makes a lot of sense to use `adata.obsm[""X_adt""]` and `adata.obsm[""X_hto""]` for ADT and HTO counts. One caveat is that we cannot store ADT/HTO barcode strings in `adata.obsm` but I don't know how important this is. For I/O, we can define a `sc.read_antibody_tags(filename)` that reads HTO/ADTs into the `adata.obsm['X_hto']`. Then a simple `sc.pp.classify_hashtags()` method can determine classes and creates new fields like HTO_class in `adata.obs`. @wflynny @njbernstein what do you think? @wflynny what else do you think is needed for a nice HTO/ADT pipeline?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:619,testability,simpl,simple,619,"I'm also interested in this since I'll be analyzing some HTO data soon. . As I wrote [here](https://github.com/theislab/scanpy/pull/797/files/8bcee13537d6353399f1722bac7f60bc943a482f#r335664372), I think we should also discuss the I/O and storage procedures for ADT/HTOs. . @wflynny it makes a lot of sense to use `adata.obsm[""X_adt""]` and `adata.obsm[""X_hto""]` for ADT and HTO counts. One caveat is that we cannot store ADT/HTO barcode strings in `adata.obsm` but I don't know how important this is. For I/O, we can define a `sc.read_antibody_tags(filename)` that reads HTO/ADTs into the `adata.obsm['X_hto']`. Then a simple `sc.pp.classify_hashtags()` method can determine classes and creates new fields like HTO_class in `adata.obs`. @wflynny @njbernstein what do you think? @wflynny what else do you think is needed for a nice HTO/ADT pipeline?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:619,usability,simpl,simple,619,"I'm also interested in this since I'll be analyzing some HTO data soon. . As I wrote [here](https://github.com/theislab/scanpy/pull/797/files/8bcee13537d6353399f1722bac7f60bc943a482f#r335664372), I think we should also discuss the I/O and storage procedures for ADT/HTOs. . @wflynny it makes a lot of sense to use `adata.obsm[""X_adt""]` and `adata.obsm[""X_hto""]` for ADT and HTO counts. One caveat is that we cannot store ADT/HTO barcode strings in `adata.obsm` but I don't know how important this is. For I/O, we can define a `sc.read_antibody_tags(filename)` that reads HTO/ADTs into the `adata.obsm['X_hto']`. Then a simple `sc.pp.classify_hashtags()` method can determine classes and creates new fields like HTO_class in `adata.obs`. @wflynny @njbernstein what do you think? @wflynny what else do you think is needed for a nice HTO/ADT pipeline?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:189,deployability,version,versions,189,"@gokceneraslan Hey, sorry for my long silence on this. I've been using @Hoohm's [https://github.com/Hoohm/CITE-seq-Count](CITE-seq-Count) for ADT/HTO tag counting which produces (in recent versions) a 10X v3 style `mtx` directory for both reads and UMIs. In some cases, I'll load these in as their own AnnData object with reads and counts as different `layers` which is helpful in computing per-cell or per-tag ""sequencing saturation"" and other metrics involving both reads and counts. This is especially helpful for investigating some pilot experiments (lipid tags, cholesterol tags, etc.) we've been doing. However, most of the time I'll just load the tags matrix in as a pandas dataframe and run them through a demuxing function that'll modify `adata.obs`. A couple challenges/ideas to consider:. * at our facility, we're typically building the same Illumina i7 index (`ATTACTCG`) into all tag libraries. This leads to some tricky situations when using a NovaSeq for sequencing since the multiple tag libraries (with disjoint sets of tags) may be run on the same sequencing flowcell lane. This results in a single set of FASTQ files and thus a single barcode-tag matrix for all tag libraries on that lane. Therefore, the mapping between transcriptome AnnData objects <-> tag library matrices is not always 1-to-1. * in my experience, HTO libraries have a large variance in quality, so for the most part I've been using the transcriptome as my ""ground truth"" as to what is a cell. However, I imagine others use HTOs to ""rescue"" cells that were not called by their pipeline of choice (and I hope to do this once I build enough trust in the data). In that case, one would want to intersect the HTO classifications with the raw cell-gene matrix. * not all tags are antibody based, so I'd vote for naming all related functions `*hashtags()`. I'd therefore vote for something like the following design:. ```{python}. # htos is a AnnData object. htos = sc.read_hashtags(filename) . # classify_hashtags ad",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:835,deployability,build,building,835,"@gokceneraslan Hey, sorry for my long silence on this. I've been using @Hoohm's [https://github.com/Hoohm/CITE-seq-Count](CITE-seq-Count) for ADT/HTO tag counting which produces (in recent versions) a 10X v3 style `mtx` directory for both reads and UMIs. In some cases, I'll load these in as their own AnnData object with reads and counts as different `layers` which is helpful in computing per-cell or per-tag ""sequencing saturation"" and other metrics involving both reads and counts. This is especially helpful for investigating some pilot experiments (lipid tags, cholesterol tags, etc.) we've been doing. However, most of the time I'll just load the tags matrix in as a pandas dataframe and run them through a demuxing function that'll modify `adata.obs`. A couple challenges/ideas to consider:. * at our facility, we're typically building the same Illumina i7 index (`ATTACTCG`) into all tag libraries. This leads to some tricky situations when using a NovaSeq for sequencing since the multiple tag libraries (with disjoint sets of tags) may be run on the same sequencing flowcell lane. This results in a single set of FASTQ files and thus a single barcode-tag matrix for all tag libraries on that lane. Therefore, the mapping between transcriptome AnnData objects <-> tag library matrices is not always 1-to-1. * in my experience, HTO libraries have a large variance in quality, so for the most part I've been using the transcriptome as my ""ground truth"" as to what is a cell. However, I imagine others use HTOs to ""rescue"" cells that were not called by their pipeline of choice (and I hope to do this once I build enough trust in the data). In that case, one would want to intersect the HTO classifications with the raw cell-gene matrix. * not all tags are antibody based, so I'd vote for naming all related functions `*hashtags()`. I'd therefore vote for something like the following design:. ```{python}. # htos is a AnnData object. htos = sc.read_hashtags(filename) . # classify_hashtags ad",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:1566,deployability,pipelin,pipeline,1566,"lesterol tags, etc.) we've been doing. However, most of the time I'll just load the tags matrix in as a pandas dataframe and run them through a demuxing function that'll modify `adata.obs`. A couple challenges/ideas to consider:. * at our facility, we're typically building the same Illumina i7 index (`ATTACTCG`) into all tag libraries. This leads to some tricky situations when using a NovaSeq for sequencing since the multiple tag libraries (with disjoint sets of tags) may be run on the same sequencing flowcell lane. This results in a single set of FASTQ files and thus a single barcode-tag matrix for all tag libraries on that lane. Therefore, the mapping between transcriptome AnnData objects <-> tag library matrices is not always 1-to-1. * in my experience, HTO libraries have a large variance in quality, so for the most part I've been using the transcriptome as my ""ground truth"" as to what is a cell. However, I imagine others use HTOs to ""rescue"" cells that were not called by their pipeline of choice (and I hope to do this once I build enough trust in the data). In that case, one would want to intersect the HTO classifications with the raw cell-gene matrix. * not all tags are antibody based, so I'd vote for naming all related functions `*hashtags()`. I'd therefore vote for something like the following design:. ```{python}. # htos is a AnnData object. htos = sc.read_hashtags(filename) . # classify_hashtags adds a classification to the hto AnnData object. # kwargs might involve things like `use_tags=[""tag1"", ""tag2"", ""tag3""]`. sc.pp.classify_hashtags(htos, **kwargs). print(htos.obs.classification) . # demuxing cell-gene matrix(es) could then be done like. rna1 = sc.read_10x_h5(...). rna2 = sc.read_10x_h5(...). # sc.pp.demux_by_hashtag(adata_hto, *adata_rna, tag_groups=None, ...). sc.pp.demux_by_hashtag(. htos, . rna1, rna2, . tag_groups=[(""tag1"", ""tag3"", ""tag5""), (""tag2"", ""tag4"", ""tag6"")]. ). ```. @gokceneraslan This is more complex than what you suggested, but I think ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:1615,deployability,build,build,1615,"most of the time I'll just load the tags matrix in as a pandas dataframe and run them through a demuxing function that'll modify `adata.obs`. A couple challenges/ideas to consider:. * at our facility, we're typically building the same Illumina i7 index (`ATTACTCG`) into all tag libraries. This leads to some tricky situations when using a NovaSeq for sequencing since the multiple tag libraries (with disjoint sets of tags) may be run on the same sequencing flowcell lane. This results in a single set of FASTQ files and thus a single barcode-tag matrix for all tag libraries on that lane. Therefore, the mapping between transcriptome AnnData objects <-> tag library matrices is not always 1-to-1. * in my experience, HTO libraries have a large variance in quality, so for the most part I've been using the transcriptome as my ""ground truth"" as to what is a cell. However, I imagine others use HTOs to ""rescue"" cells that were not called by their pipeline of choice (and I hope to do this once I build enough trust in the data). In that case, one would want to intersect the HTO classifications with the raw cell-gene matrix. * not all tags are antibody based, so I'd vote for naming all related functions `*hashtags()`. I'd therefore vote for something like the following design:. ```{python}. # htos is a AnnData object. htos = sc.read_hashtags(filename) . # classify_hashtags adds a classification to the hto AnnData object. # kwargs might involve things like `use_tags=[""tag1"", ""tag2"", ""tag3""]`. sc.pp.classify_hashtags(htos, **kwargs). print(htos.obs.classification) . # demuxing cell-gene matrix(es) could then be done like. rna1 = sc.read_10x_h5(...). rna2 = sc.read_10x_h5(...). # sc.pp.demux_by_hashtag(adata_hto, *adata_rna, tag_groups=None, ...). sc.pp.demux_by_hashtag(. htos, . rna1, rna2, . tag_groups=[(""tag1"", ""tag3"", ""tag5""), (""tag2"", ""tag4"", ""tag6"")]. ). ```. @gokceneraslan This is more complex than what you suggested, but I think is sufficiently general to cover my needs as lis",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:275,energy efficiency,load,load,275,"@gokceneraslan Hey, sorry for my long silence on this. I've been using @Hoohm's [https://github.com/Hoohm/CITE-seq-Count](CITE-seq-Count) for ADT/HTO tag counting which produces (in recent versions) a 10X v3 style `mtx` directory for both reads and UMIs. In some cases, I'll load these in as their own AnnData object with reads and counts as different `layers` which is helpful in computing per-cell or per-tag ""sequencing saturation"" and other metrics involving both reads and counts. This is especially helpful for investigating some pilot experiments (lipid tags, cholesterol tags, etc.) we've been doing. However, most of the time I'll just load the tags matrix in as a pandas dataframe and run them through a demuxing function that'll modify `adata.obs`. A couple challenges/ideas to consider:. * at our facility, we're typically building the same Illumina i7 index (`ATTACTCG`) into all tag libraries. This leads to some tricky situations when using a NovaSeq for sequencing since the multiple tag libraries (with disjoint sets of tags) may be run on the same sequencing flowcell lane. This results in a single set of FASTQ files and thus a single barcode-tag matrix for all tag libraries on that lane. Therefore, the mapping between transcriptome AnnData objects <-> tag library matrices is not always 1-to-1. * in my experience, HTO libraries have a large variance in quality, so for the most part I've been using the transcriptome as my ""ground truth"" as to what is a cell. However, I imagine others use HTOs to ""rescue"" cells that were not called by their pipeline of choice (and I hope to do this once I build enough trust in the data). In that case, one would want to intersect the HTO classifications with the raw cell-gene matrix. * not all tags are antibody based, so I'd vote for naming all related functions `*hashtags()`. I'd therefore vote for something like the following design:. ```{python}. # htos is a AnnData object. htos = sc.read_hashtags(filename) . # classify_hashtags ad",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:645,energy efficiency,load,load,645,"@gokceneraslan Hey, sorry for my long silence on this. I've been using @Hoohm's [https://github.com/Hoohm/CITE-seq-Count](CITE-seq-Count) for ADT/HTO tag counting which produces (in recent versions) a 10X v3 style `mtx` directory for both reads and UMIs. In some cases, I'll load these in as their own AnnData object with reads and counts as different `layers` which is helpful in computing per-cell or per-tag ""sequencing saturation"" and other metrics involving both reads and counts. This is especially helpful for investigating some pilot experiments (lipid tags, cholesterol tags, etc.) we've been doing. However, most of the time I'll just load the tags matrix in as a pandas dataframe and run them through a demuxing function that'll modify `adata.obs`. A couple challenges/ideas to consider:. * at our facility, we're typically building the same Illumina i7 index (`ATTACTCG`) into all tag libraries. This leads to some tricky situations when using a NovaSeq for sequencing since the multiple tag libraries (with disjoint sets of tags) may be run on the same sequencing flowcell lane. This results in a single set of FASTQ files and thus a single barcode-tag matrix for all tag libraries on that lane. Therefore, the mapping between transcriptome AnnData objects <-> tag library matrices is not always 1-to-1. * in my experience, HTO libraries have a large variance in quality, so for the most part I've been using the transcriptome as my ""ground truth"" as to what is a cell. However, I imagine others use HTOs to ""rescue"" cells that were not called by their pipeline of choice (and I hope to do this once I build enough trust in the data). In that case, one would want to intersect the HTO classifications with the raw cell-gene matrix. * not all tags are antibody based, so I'd vote for naming all related functions `*hashtags()`. I'd therefore vote for something like the following design:. ```{python}. # htos is a AnnData object. htos = sc.read_hashtags(filename) . # classify_hashtags ad",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:189,integrability,version,versions,189,"@gokceneraslan Hey, sorry for my long silence on this. I've been using @Hoohm's [https://github.com/Hoohm/CITE-seq-Count](CITE-seq-Count) for ADT/HTO tag counting which produces (in recent versions) a 10X v3 style `mtx` directory for both reads and UMIs. In some cases, I'll load these in as their own AnnData object with reads and counts as different `layers` which is helpful in computing per-cell or per-tag ""sequencing saturation"" and other metrics involving both reads and counts. This is especially helpful for investigating some pilot experiments (lipid tags, cholesterol tags, etc.) we've been doing. However, most of the time I'll just load the tags matrix in as a pandas dataframe and run them through a demuxing function that'll modify `adata.obs`. A couple challenges/ideas to consider:. * at our facility, we're typically building the same Illumina i7 index (`ATTACTCG`) into all tag libraries. This leads to some tricky situations when using a NovaSeq for sequencing since the multiple tag libraries (with disjoint sets of tags) may be run on the same sequencing flowcell lane. This results in a single set of FASTQ files and thus a single barcode-tag matrix for all tag libraries on that lane. Therefore, the mapping between transcriptome AnnData objects <-> tag library matrices is not always 1-to-1. * in my experience, HTO libraries have a large variance in quality, so for the most part I've been using the transcriptome as my ""ground truth"" as to what is a cell. However, I imagine others use HTOs to ""rescue"" cells that were not called by their pipeline of choice (and I hope to do this once I build enough trust in the data). In that case, one would want to intersect the HTO classifications with the raw cell-gene matrix. * not all tags are antibody based, so I'd vote for naming all related functions `*hashtags()`. I'd therefore vote for something like the following design:. ```{python}. # htos is a AnnData object. htos = sc.read_hashtags(filename) . # classify_hashtags ad",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:762,integrability,coupl,couple,762,"@gokceneraslan Hey, sorry for my long silence on this. I've been using @Hoohm's [https://github.com/Hoohm/CITE-seq-Count](CITE-seq-Count) for ADT/HTO tag counting which produces (in recent versions) a 10X v3 style `mtx` directory for both reads and UMIs. In some cases, I'll load these in as their own AnnData object with reads and counts as different `layers` which is helpful in computing per-cell or per-tag ""sequencing saturation"" and other metrics involving both reads and counts. This is especially helpful for investigating some pilot experiments (lipid tags, cholesterol tags, etc.) we've been doing. However, most of the time I'll just load the tags matrix in as a pandas dataframe and run them through a demuxing function that'll modify `adata.obs`. A couple challenges/ideas to consider:. * at our facility, we're typically building the same Illumina i7 index (`ATTACTCG`) into all tag libraries. This leads to some tricky situations when using a NovaSeq for sequencing since the multiple tag libraries (with disjoint sets of tags) may be run on the same sequencing flowcell lane. This results in a single set of FASTQ files and thus a single barcode-tag matrix for all tag libraries on that lane. Therefore, the mapping between transcriptome AnnData objects <-> tag library matrices is not always 1-to-1. * in my experience, HTO libraries have a large variance in quality, so for the most part I've been using the transcriptome as my ""ground truth"" as to what is a cell. However, I imagine others use HTOs to ""rescue"" cells that were not called by their pipeline of choice (and I hope to do this once I build enough trust in the data). In that case, one would want to intersect the HTO classifications with the raw cell-gene matrix. * not all tags are antibody based, so I'd vote for naming all related functions `*hashtags()`. I'd therefore vote for something like the following design:. ```{python}. # htos is a AnnData object. htos = sc.read_hashtags(filename) . # classify_hashtags ad",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:1566,integrability,pipelin,pipeline,1566,"lesterol tags, etc.) we've been doing. However, most of the time I'll just load the tags matrix in as a pandas dataframe and run them through a demuxing function that'll modify `adata.obs`. A couple challenges/ideas to consider:. * at our facility, we're typically building the same Illumina i7 index (`ATTACTCG`) into all tag libraries. This leads to some tricky situations when using a NovaSeq for sequencing since the multiple tag libraries (with disjoint sets of tags) may be run on the same sequencing flowcell lane. This results in a single set of FASTQ files and thus a single barcode-tag matrix for all tag libraries on that lane. Therefore, the mapping between transcriptome AnnData objects <-> tag library matrices is not always 1-to-1. * in my experience, HTO libraries have a large variance in quality, so for the most part I've been using the transcriptome as my ""ground truth"" as to what is a cell. However, I imagine others use HTOs to ""rescue"" cells that were not called by their pipeline of choice (and I hope to do this once I build enough trust in the data). In that case, one would want to intersect the HTO classifications with the raw cell-gene matrix. * not all tags are antibody based, so I'd vote for naming all related functions `*hashtags()`. I'd therefore vote for something like the following design:. ```{python}. # htos is a AnnData object. htos = sc.read_hashtags(filename) . # classify_hashtags adds a classification to the hto AnnData object. # kwargs might involve things like `use_tags=[""tag1"", ""tag2"", ""tag3""]`. sc.pp.classify_hashtags(htos, **kwargs). print(htos.obs.classification) . # demuxing cell-gene matrix(es) could then be done like. rna1 = sc.read_10x_h5(...). rna2 = sc.read_10x_h5(...). # sc.pp.demux_by_hashtag(adata_hto, *adata_rna, tag_groups=None, ...). sc.pp.demux_by_hashtag(. htos, . rna1, rna2, . tag_groups=[(""tag1"", ""tag3"", ""tag5""), (""tag2"", ""tag4"", ""tag6"")]. ). ```. @gokceneraslan This is more complex than what you suggested, but I think ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:189,modifiability,version,versions,189,"@gokceneraslan Hey, sorry for my long silence on this. I've been using @Hoohm's [https://github.com/Hoohm/CITE-seq-Count](CITE-seq-Count) for ADT/HTO tag counting which produces (in recent versions) a 10X v3 style `mtx` directory for both reads and UMIs. In some cases, I'll load these in as their own AnnData object with reads and counts as different `layers` which is helpful in computing per-cell or per-tag ""sequencing saturation"" and other metrics involving both reads and counts. This is especially helpful for investigating some pilot experiments (lipid tags, cholesterol tags, etc.) we've been doing. However, most of the time I'll just load the tags matrix in as a pandas dataframe and run them through a demuxing function that'll modify `adata.obs`. A couple challenges/ideas to consider:. * at our facility, we're typically building the same Illumina i7 index (`ATTACTCG`) into all tag libraries. This leads to some tricky situations when using a NovaSeq for sequencing since the multiple tag libraries (with disjoint sets of tags) may be run on the same sequencing flowcell lane. This results in a single set of FASTQ files and thus a single barcode-tag matrix for all tag libraries on that lane. Therefore, the mapping between transcriptome AnnData objects <-> tag library matrices is not always 1-to-1. * in my experience, HTO libraries have a large variance in quality, so for the most part I've been using the transcriptome as my ""ground truth"" as to what is a cell. However, I imagine others use HTOs to ""rescue"" cells that were not called by their pipeline of choice (and I hope to do this once I build enough trust in the data). In that case, one would want to intersect the HTO classifications with the raw cell-gene matrix. * not all tags are antibody based, so I'd vote for naming all related functions `*hashtags()`. I'd therefore vote for something like the following design:. ```{python}. # htos is a AnnData object. htos = sc.read_hashtags(filename) . # classify_hashtags ad",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:353,modifiability,layer,layers,353,"@gokceneraslan Hey, sorry for my long silence on this. I've been using @Hoohm's [https://github.com/Hoohm/CITE-seq-Count](CITE-seq-Count) for ADT/HTO tag counting which produces (in recent versions) a 10X v3 style `mtx` directory for both reads and UMIs. In some cases, I'll load these in as their own AnnData object with reads and counts as different `layers` which is helpful in computing per-cell or per-tag ""sequencing saturation"" and other metrics involving both reads and counts. This is especially helpful for investigating some pilot experiments (lipid tags, cholesterol tags, etc.) we've been doing. However, most of the time I'll just load the tags matrix in as a pandas dataframe and run them through a demuxing function that'll modify `adata.obs`. A couple challenges/ideas to consider:. * at our facility, we're typically building the same Illumina i7 index (`ATTACTCG`) into all tag libraries. This leads to some tricky situations when using a NovaSeq for sequencing since the multiple tag libraries (with disjoint sets of tags) may be run on the same sequencing flowcell lane. This results in a single set of FASTQ files and thus a single barcode-tag matrix for all tag libraries on that lane. Therefore, the mapping between transcriptome AnnData objects <-> tag library matrices is not always 1-to-1. * in my experience, HTO libraries have a large variance in quality, so for the most part I've been using the transcriptome as my ""ground truth"" as to what is a cell. However, I imagine others use HTOs to ""rescue"" cells that were not called by their pipeline of choice (and I hope to do this once I build enough trust in the data). In that case, one would want to intersect the HTO classifications with the raw cell-gene matrix. * not all tags are antibody based, so I'd vote for naming all related functions `*hashtags()`. I'd therefore vote for something like the following design:. ```{python}. # htos is a AnnData object. htos = sc.read_hashtags(filename) . # classify_hashtags ad",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:762,modifiability,coupl,couple,762,"@gokceneraslan Hey, sorry for my long silence on this. I've been using @Hoohm's [https://github.com/Hoohm/CITE-seq-Count](CITE-seq-Count) for ADT/HTO tag counting which produces (in recent versions) a 10X v3 style `mtx` directory for both reads and UMIs. In some cases, I'll load these in as their own AnnData object with reads and counts as different `layers` which is helpful in computing per-cell or per-tag ""sequencing saturation"" and other metrics involving both reads and counts. This is especially helpful for investigating some pilot experiments (lipid tags, cholesterol tags, etc.) we've been doing. However, most of the time I'll just load the tags matrix in as a pandas dataframe and run them through a demuxing function that'll modify `adata.obs`. A couple challenges/ideas to consider:. * at our facility, we're typically building the same Illumina i7 index (`ATTACTCG`) into all tag libraries. This leads to some tricky situations when using a NovaSeq for sequencing since the multiple tag libraries (with disjoint sets of tags) may be run on the same sequencing flowcell lane. This results in a single set of FASTQ files and thus a single barcode-tag matrix for all tag libraries on that lane. Therefore, the mapping between transcriptome AnnData objects <-> tag library matrices is not always 1-to-1. * in my experience, HTO libraries have a large variance in quality, so for the most part I've been using the transcriptome as my ""ground truth"" as to what is a cell. However, I imagine others use HTOs to ""rescue"" cells that were not called by their pipeline of choice (and I hope to do this once I build enough trust in the data). In that case, one would want to intersect the HTO classifications with the raw cell-gene matrix. * not all tags are antibody based, so I'd vote for naming all related functions `*hashtags()`. I'd therefore vote for something like the following design:. ```{python}. # htos is a AnnData object. htos = sc.read_hashtags(filename) . # classify_hashtags ad",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:275,performance,load,load,275,"@gokceneraslan Hey, sorry for my long silence on this. I've been using @Hoohm's [https://github.com/Hoohm/CITE-seq-Count](CITE-seq-Count) for ADT/HTO tag counting which produces (in recent versions) a 10X v3 style `mtx` directory for both reads and UMIs. In some cases, I'll load these in as their own AnnData object with reads and counts as different `layers` which is helpful in computing per-cell or per-tag ""sequencing saturation"" and other metrics involving both reads and counts. This is especially helpful for investigating some pilot experiments (lipid tags, cholesterol tags, etc.) we've been doing. However, most of the time I'll just load the tags matrix in as a pandas dataframe and run them through a demuxing function that'll modify `adata.obs`. A couple challenges/ideas to consider:. * at our facility, we're typically building the same Illumina i7 index (`ATTACTCG`) into all tag libraries. This leads to some tricky situations when using a NovaSeq for sequencing since the multiple tag libraries (with disjoint sets of tags) may be run on the same sequencing flowcell lane. This results in a single set of FASTQ files and thus a single barcode-tag matrix for all tag libraries on that lane. Therefore, the mapping between transcriptome AnnData objects <-> tag library matrices is not always 1-to-1. * in my experience, HTO libraries have a large variance in quality, so for the most part I've been using the transcriptome as my ""ground truth"" as to what is a cell. However, I imagine others use HTOs to ""rescue"" cells that were not called by their pipeline of choice (and I hope to do this once I build enough trust in the data). In that case, one would want to intersect the HTO classifications with the raw cell-gene matrix. * not all tags are antibody based, so I'd vote for naming all related functions `*hashtags()`. I'd therefore vote for something like the following design:. ```{python}. # htos is a AnnData object. htos = sc.read_hashtags(filename) . # classify_hashtags ad",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:630,performance,time,time,630,"@gokceneraslan Hey, sorry for my long silence on this. I've been using @Hoohm's [https://github.com/Hoohm/CITE-seq-Count](CITE-seq-Count) for ADT/HTO tag counting which produces (in recent versions) a 10X v3 style `mtx` directory for both reads and UMIs. In some cases, I'll load these in as their own AnnData object with reads and counts as different `layers` which is helpful in computing per-cell or per-tag ""sequencing saturation"" and other metrics involving both reads and counts. This is especially helpful for investigating some pilot experiments (lipid tags, cholesterol tags, etc.) we've been doing. However, most of the time I'll just load the tags matrix in as a pandas dataframe and run them through a demuxing function that'll modify `adata.obs`. A couple challenges/ideas to consider:. * at our facility, we're typically building the same Illumina i7 index (`ATTACTCG`) into all tag libraries. This leads to some tricky situations when using a NovaSeq for sequencing since the multiple tag libraries (with disjoint sets of tags) may be run on the same sequencing flowcell lane. This results in a single set of FASTQ files and thus a single barcode-tag matrix for all tag libraries on that lane. Therefore, the mapping between transcriptome AnnData objects <-> tag library matrices is not always 1-to-1. * in my experience, HTO libraries have a large variance in quality, so for the most part I've been using the transcriptome as my ""ground truth"" as to what is a cell. However, I imagine others use HTOs to ""rescue"" cells that were not called by their pipeline of choice (and I hope to do this once I build enough trust in the data). In that case, one would want to intersect the HTO classifications with the raw cell-gene matrix. * not all tags are antibody based, so I'd vote for naming all related functions `*hashtags()`. I'd therefore vote for something like the following design:. ```{python}. # htos is a AnnData object. htos = sc.read_hashtags(filename) . # classify_hashtags ad",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:645,performance,load,load,645,"@gokceneraslan Hey, sorry for my long silence on this. I've been using @Hoohm's [https://github.com/Hoohm/CITE-seq-Count](CITE-seq-Count) for ADT/HTO tag counting which produces (in recent versions) a 10X v3 style `mtx` directory for both reads and UMIs. In some cases, I'll load these in as their own AnnData object with reads and counts as different `layers` which is helpful in computing per-cell or per-tag ""sequencing saturation"" and other metrics involving both reads and counts. This is especially helpful for investigating some pilot experiments (lipid tags, cholesterol tags, etc.) we've been doing. However, most of the time I'll just load the tags matrix in as a pandas dataframe and run them through a demuxing function that'll modify `adata.obs`. A couple challenges/ideas to consider:. * at our facility, we're typically building the same Illumina i7 index (`ATTACTCG`) into all tag libraries. This leads to some tricky situations when using a NovaSeq for sequencing since the multiple tag libraries (with disjoint sets of tags) may be run on the same sequencing flowcell lane. This results in a single set of FASTQ files and thus a single barcode-tag matrix for all tag libraries on that lane. Therefore, the mapping between transcriptome AnnData objects <-> tag library matrices is not always 1-to-1. * in my experience, HTO libraries have a large variance in quality, so for the most part I've been using the transcriptome as my ""ground truth"" as to what is a cell. However, I imagine others use HTOs to ""rescue"" cells that were not called by their pipeline of choice (and I hope to do this once I build enough trust in the data). In that case, one would want to intersect the HTO classifications with the raw cell-gene matrix. * not all tags are antibody based, so I'd vote for naming all related functions `*hashtags()`. I'd therefore vote for something like the following design:. ```{python}. # htos is a AnnData object. htos = sc.read_hashtags(filename) . # classify_hashtags ad",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:2685,performance,time,time,2685," that'll modify `adata.obs`. A couple challenges/ideas to consider:. * at our facility, we're typically building the same Illumina i7 index (`ATTACTCG`) into all tag libraries. This leads to some tricky situations when using a NovaSeq for sequencing since the multiple tag libraries (with disjoint sets of tags) may be run on the same sequencing flowcell lane. This results in a single set of FASTQ files and thus a single barcode-tag matrix for all tag libraries on that lane. Therefore, the mapping between transcriptome AnnData objects <-> tag library matrices is not always 1-to-1. * in my experience, HTO libraries have a large variance in quality, so for the most part I've been using the transcriptome as my ""ground truth"" as to what is a cell. However, I imagine others use HTOs to ""rescue"" cells that were not called by their pipeline of choice (and I hope to do this once I build enough trust in the data). In that case, one would want to intersect the HTO classifications with the raw cell-gene matrix. * not all tags are antibody based, so I'd vote for naming all related functions `*hashtags()`. I'd therefore vote for something like the following design:. ```{python}. # htos is a AnnData object. htos = sc.read_hashtags(filename) . # classify_hashtags adds a classification to the hto AnnData object. # kwargs might involve things like `use_tags=[""tag1"", ""tag2"", ""tag3""]`. sc.pp.classify_hashtags(htos, **kwargs). print(htos.obs.classification) . # demuxing cell-gene matrix(es) could then be done like. rna1 = sc.read_10x_h5(...). rna2 = sc.read_10x_h5(...). # sc.pp.demux_by_hashtag(adata_hto, *adata_rna, tag_groups=None, ...). sc.pp.demux_by_hashtag(. htos, . rna1, rna2, . tag_groups=[(""tag1"", ""tag3"", ""tag5""), (""tag2"", ""tag4"", ""tag6"")]. ). ```. @gokceneraslan This is more complex than what you suggested, but I think is sufficiently general to cover my needs as listed above. Let me know what you think---I'll have some development time next week to possible contribute to this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:2525,safety,compl,complex,2525," that'll modify `adata.obs`. A couple challenges/ideas to consider:. * at our facility, we're typically building the same Illumina i7 index (`ATTACTCG`) into all tag libraries. This leads to some tricky situations when using a NovaSeq for sequencing since the multiple tag libraries (with disjoint sets of tags) may be run on the same sequencing flowcell lane. This results in a single set of FASTQ files and thus a single barcode-tag matrix for all tag libraries on that lane. Therefore, the mapping between transcriptome AnnData objects <-> tag library matrices is not always 1-to-1. * in my experience, HTO libraries have a large variance in quality, so for the most part I've been using the transcriptome as my ""ground truth"" as to what is a cell. However, I imagine others use HTOs to ""rescue"" cells that were not called by their pipeline of choice (and I hope to do this once I build enough trust in the data). In that case, one would want to intersect the HTO classifications with the raw cell-gene matrix. * not all tags are antibody based, so I'd vote for naming all related functions `*hashtags()`. I'd therefore vote for something like the following design:. ```{python}. # htos is a AnnData object. htos = sc.read_hashtags(filename) . # classify_hashtags adds a classification to the hto AnnData object. # kwargs might involve things like `use_tags=[""tag1"", ""tag2"", ""tag3""]`. sc.pp.classify_hashtags(htos, **kwargs). print(htos.obs.classification) . # demuxing cell-gene matrix(es) could then be done like. rna1 = sc.read_10x_h5(...). rna2 = sc.read_10x_h5(...). # sc.pp.demux_by_hashtag(adata_hto, *adata_rna, tag_groups=None, ...). sc.pp.demux_by_hashtag(. htos, . rna1, rna2, . tag_groups=[(""tag1"", ""tag3"", ""tag5""), (""tag2"", ""tag4"", ""tag6"")]. ). ```. @gokceneraslan This is more complex than what you suggested, but I think is sufficiently general to cover my needs as listed above. Let me know what you think---I'll have some development time next week to possible contribute to this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:740,security,modif,modify,740,"@gokceneraslan Hey, sorry for my long silence on this. I've been using @Hoohm's [https://github.com/Hoohm/CITE-seq-Count](CITE-seq-Count) for ADT/HTO tag counting which produces (in recent versions) a 10X v3 style `mtx` directory for both reads and UMIs. In some cases, I'll load these in as their own AnnData object with reads and counts as different `layers` which is helpful in computing per-cell or per-tag ""sequencing saturation"" and other metrics involving both reads and counts. This is especially helpful for investigating some pilot experiments (lipid tags, cholesterol tags, etc.) we've been doing. However, most of the time I'll just load the tags matrix in as a pandas dataframe and run them through a demuxing function that'll modify `adata.obs`. A couple challenges/ideas to consider:. * at our facility, we're typically building the same Illumina i7 index (`ATTACTCG`) into all tag libraries. This leads to some tricky situations when using a NovaSeq for sequencing since the multiple tag libraries (with disjoint sets of tags) may be run on the same sequencing flowcell lane. This results in a single set of FASTQ files and thus a single barcode-tag matrix for all tag libraries on that lane. Therefore, the mapping between transcriptome AnnData objects <-> tag library matrices is not always 1-to-1. * in my experience, HTO libraries have a large variance in quality, so for the most part I've been using the transcriptome as my ""ground truth"" as to what is a cell. However, I imagine others use HTOs to ""rescue"" cells that were not called by their pipeline of choice (and I hope to do this once I build enough trust in the data). In that case, one would want to intersect the HTO classifications with the raw cell-gene matrix. * not all tags are antibody based, so I'd vote for naming all related functions `*hashtags()`. I'd therefore vote for something like the following design:. ```{python}. # htos is a AnnData object. htos = sc.read_hashtags(filename) . # classify_hashtags ad",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:1628,security,trust,trust,1628,"ime I'll just load the tags matrix in as a pandas dataframe and run them through a demuxing function that'll modify `adata.obs`. A couple challenges/ideas to consider:. * at our facility, we're typically building the same Illumina i7 index (`ATTACTCG`) into all tag libraries. This leads to some tricky situations when using a NovaSeq for sequencing since the multiple tag libraries (with disjoint sets of tags) may be run on the same sequencing flowcell lane. This results in a single set of FASTQ files and thus a single barcode-tag matrix for all tag libraries on that lane. Therefore, the mapping between transcriptome AnnData objects <-> tag library matrices is not always 1-to-1. * in my experience, HTO libraries have a large variance in quality, so for the most part I've been using the transcriptome as my ""ground truth"" as to what is a cell. However, I imagine others use HTOs to ""rescue"" cells that were not called by their pipeline of choice (and I hope to do this once I build enough trust in the data). In that case, one would want to intersect the HTO classifications with the raw cell-gene matrix. * not all tags are antibody based, so I'd vote for naming all related functions `*hashtags()`. I'd therefore vote for something like the following design:. ```{python}. # htos is a AnnData object. htos = sc.read_hashtags(filename) . # classify_hashtags adds a classification to the hto AnnData object. # kwargs might involve things like `use_tags=[""tag1"", ""tag2"", ""tag3""]`. sc.pp.classify_hashtags(htos, **kwargs). print(htos.obs.classification) . # demuxing cell-gene matrix(es) could then be done like. rna1 = sc.read_10x_h5(...). rna2 = sc.read_10x_h5(...). # sc.pp.demux_by_hashtag(adata_hto, *adata_rna, tag_groups=None, ...). sc.pp.demux_by_hashtag(. htos, . rna1, rna2, . tag_groups=[(""tag1"", ""tag3"", ""tag5""), (""tag2"", ""tag4"", ""tag6"")]. ). ```. @gokceneraslan This is more complex than what you suggested, but I think is sufficiently general to cover my needs as listed above. Le",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:1827,security,hash,hashtags,1827," that'll modify `adata.obs`. A couple challenges/ideas to consider:. * at our facility, we're typically building the same Illumina i7 index (`ATTACTCG`) into all tag libraries. This leads to some tricky situations when using a NovaSeq for sequencing since the multiple tag libraries (with disjoint sets of tags) may be run on the same sequencing flowcell lane. This results in a single set of FASTQ files and thus a single barcode-tag matrix for all tag libraries on that lane. Therefore, the mapping between transcriptome AnnData objects <-> tag library matrices is not always 1-to-1. * in my experience, HTO libraries have a large variance in quality, so for the most part I've been using the transcriptome as my ""ground truth"" as to what is a cell. However, I imagine others use HTOs to ""rescue"" cells that were not called by their pipeline of choice (and I hope to do this once I build enough trust in the data). In that case, one would want to intersect the HTO classifications with the raw cell-gene matrix. * not all tags are antibody based, so I'd vote for naming all related functions `*hashtags()`. I'd therefore vote for something like the following design:. ```{python}. # htos is a AnnData object. htos = sc.read_hashtags(filename) . # classify_hashtags adds a classification to the hto AnnData object. # kwargs might involve things like `use_tags=[""tag1"", ""tag2"", ""tag3""]`. sc.pp.classify_hashtags(htos, **kwargs). print(htos.obs.classification) . # demuxing cell-gene matrix(es) could then be done like. rna1 = sc.read_10x_h5(...). rna2 = sc.read_10x_h5(...). # sc.pp.demux_by_hashtag(adata_hto, *adata_rna, tag_groups=None, ...). sc.pp.demux_by_hashtag(. htos, . rna1, rna2, . tag_groups=[(""tag1"", ""tag3"", ""tag5""), (""tag2"", ""tag4"", ""tag6"")]. ). ```. @gokceneraslan This is more complex than what you suggested, but I think is sufficiently general to cover my needs as listed above. Let me know what you think---I'll have some development time next week to possible contribute to this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:2525,security,compl,complex,2525," that'll modify `adata.obs`. A couple challenges/ideas to consider:. * at our facility, we're typically building the same Illumina i7 index (`ATTACTCG`) into all tag libraries. This leads to some tricky situations when using a NovaSeq for sequencing since the multiple tag libraries (with disjoint sets of tags) may be run on the same sequencing flowcell lane. This results in a single set of FASTQ files and thus a single barcode-tag matrix for all tag libraries on that lane. Therefore, the mapping between transcriptome AnnData objects <-> tag library matrices is not always 1-to-1. * in my experience, HTO libraries have a large variance in quality, so for the most part I've been using the transcriptome as my ""ground truth"" as to what is a cell. However, I imagine others use HTOs to ""rescue"" cells that were not called by their pipeline of choice (and I hope to do this once I build enough trust in the data). In that case, one would want to intersect the HTO classifications with the raw cell-gene matrix. * not all tags are antibody based, so I'd vote for naming all related functions `*hashtags()`. I'd therefore vote for something like the following design:. ```{python}. # htos is a AnnData object. htos = sc.read_hashtags(filename) . # classify_hashtags adds a classification to the hto AnnData object. # kwargs might involve things like `use_tags=[""tag1"", ""tag2"", ""tag3""]`. sc.pp.classify_hashtags(htos, **kwargs). print(htos.obs.classification) . # demuxing cell-gene matrix(es) could then be done like. rna1 = sc.read_10x_h5(...). rna2 = sc.read_10x_h5(...). # sc.pp.demux_by_hashtag(adata_hto, *adata_rna, tag_groups=None, ...). sc.pp.demux_by_hashtag(. htos, . rna1, rna2, . tag_groups=[(""tag1"", ""tag3"", ""tag5""), (""tag2"", ""tag4"", ""tag6"")]. ). ```. @gokceneraslan This is more complex than what you suggested, but I think is sufficiently general to cover my needs as listed above. Let me know what you think---I'll have some development time next week to possible contribute to this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:762,testability,coupl,couple,762,"@gokceneraslan Hey, sorry for my long silence on this. I've been using @Hoohm's [https://github.com/Hoohm/CITE-seq-Count](CITE-seq-Count) for ADT/HTO tag counting which produces (in recent versions) a 10X v3 style `mtx` directory for both reads and UMIs. In some cases, I'll load these in as their own AnnData object with reads and counts as different `layers` which is helpful in computing per-cell or per-tag ""sequencing saturation"" and other metrics involving both reads and counts. This is especially helpful for investigating some pilot experiments (lipid tags, cholesterol tags, etc.) we've been doing. However, most of the time I'll just load the tags matrix in as a pandas dataframe and run them through a demuxing function that'll modify `adata.obs`. A couple challenges/ideas to consider:. * at our facility, we're typically building the same Illumina i7 index (`ATTACTCG`) into all tag libraries. This leads to some tricky situations when using a NovaSeq for sequencing since the multiple tag libraries (with disjoint sets of tags) may be run on the same sequencing flowcell lane. This results in a single set of FASTQ files and thus a single barcode-tag matrix for all tag libraries on that lane. Therefore, the mapping between transcriptome AnnData objects <-> tag library matrices is not always 1-to-1. * in my experience, HTO libraries have a large variance in quality, so for the most part I've been using the transcriptome as my ""ground truth"" as to what is a cell. However, I imagine others use HTOs to ""rescue"" cells that were not called by their pipeline of choice (and I hope to do this once I build enough trust in the data). In that case, one would want to intersect the HTO classifications with the raw cell-gene matrix. * not all tags are antibody based, so I'd vote for naming all related functions `*hashtags()`. I'd therefore vote for something like the following design:. ```{python}. # htos is a AnnData object. htos = sc.read_hashtags(filename) . # classify_hashtags ad",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:370,usability,help,helpful,370,"@gokceneraslan Hey, sorry for my long silence on this. I've been using @Hoohm's [https://github.com/Hoohm/CITE-seq-Count](CITE-seq-Count) for ADT/HTO tag counting which produces (in recent versions) a 10X v3 style `mtx` directory for both reads and UMIs. In some cases, I'll load these in as their own AnnData object with reads and counts as different `layers` which is helpful in computing per-cell or per-tag ""sequencing saturation"" and other metrics involving both reads and counts. This is especially helpful for investigating some pilot experiments (lipid tags, cholesterol tags, etc.) we've been doing. However, most of the time I'll just load the tags matrix in as a pandas dataframe and run them through a demuxing function that'll modify `adata.obs`. A couple challenges/ideas to consider:. * at our facility, we're typically building the same Illumina i7 index (`ATTACTCG`) into all tag libraries. This leads to some tricky situations when using a NovaSeq for sequencing since the multiple tag libraries (with disjoint sets of tags) may be run on the same sequencing flowcell lane. This results in a single set of FASTQ files and thus a single barcode-tag matrix for all tag libraries on that lane. Therefore, the mapping between transcriptome AnnData objects <-> tag library matrices is not always 1-to-1. * in my experience, HTO libraries have a large variance in quality, so for the most part I've been using the transcriptome as my ""ground truth"" as to what is a cell. However, I imagine others use HTOs to ""rescue"" cells that were not called by their pipeline of choice (and I hope to do this once I build enough trust in the data). In that case, one would want to intersect the HTO classifications with the raw cell-gene matrix. * not all tags are antibody based, so I'd vote for naming all related functions `*hashtags()`. I'd therefore vote for something like the following design:. ```{python}. # htos is a AnnData object. htos = sc.read_hashtags(filename) . # classify_hashtags ad",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:505,usability,help,helpful,505,"@gokceneraslan Hey, sorry for my long silence on this. I've been using @Hoohm's [https://github.com/Hoohm/CITE-seq-Count](CITE-seq-Count) for ADT/HTO tag counting which produces (in recent versions) a 10X v3 style `mtx` directory for both reads and UMIs. In some cases, I'll load these in as their own AnnData object with reads and counts as different `layers` which is helpful in computing per-cell or per-tag ""sequencing saturation"" and other metrics involving both reads and counts. This is especially helpful for investigating some pilot experiments (lipid tags, cholesterol tags, etc.) we've been doing. However, most of the time I'll just load the tags matrix in as a pandas dataframe and run them through a demuxing function that'll modify `adata.obs`. A couple challenges/ideas to consider:. * at our facility, we're typically building the same Illumina i7 index (`ATTACTCG`) into all tag libraries. This leads to some tricky situations when using a NovaSeq for sequencing since the multiple tag libraries (with disjoint sets of tags) may be run on the same sequencing flowcell lane. This results in a single set of FASTQ files and thus a single barcode-tag matrix for all tag libraries on that lane. Therefore, the mapping between transcriptome AnnData objects <-> tag library matrices is not always 1-to-1. * in my experience, HTO libraries have a large variance in quality, so for the most part I've been using the transcriptome as my ""ground truth"" as to what is a cell. However, I imagine others use HTOs to ""rescue"" cells that were not called by their pipeline of choice (and I hope to do this once I build enough trust in the data). In that case, one would want to intersect the HTO classifications with the raw cell-gene matrix. * not all tags are antibody based, so I'd vote for naming all related functions `*hashtags()`. I'd therefore vote for something like the following design:. ```{python}. # htos is a AnnData object. htos = sc.read_hashtags(filename) . # classify_hashtags ad",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:1325,usability,experien,experience,1325,"d counts as different `layers` which is helpful in computing per-cell or per-tag ""sequencing saturation"" and other metrics involving both reads and counts. This is especially helpful for investigating some pilot experiments (lipid tags, cholesterol tags, etc.) we've been doing. However, most of the time I'll just load the tags matrix in as a pandas dataframe and run them through a demuxing function that'll modify `adata.obs`. A couple challenges/ideas to consider:. * at our facility, we're typically building the same Illumina i7 index (`ATTACTCG`) into all tag libraries. This leads to some tricky situations when using a NovaSeq for sequencing since the multiple tag libraries (with disjoint sets of tags) may be run on the same sequencing flowcell lane. This results in a single set of FASTQ files and thus a single barcode-tag matrix for all tag libraries on that lane. Therefore, the mapping between transcriptome AnnData objects <-> tag library matrices is not always 1-to-1. * in my experience, HTO libraries have a large variance in quality, so for the most part I've been using the transcriptome as my ""ground truth"" as to what is a cell. However, I imagine others use HTOs to ""rescue"" cells that were not called by their pipeline of choice (and I hope to do this once I build enough trust in the data). In that case, one would want to intersect the HTO classifications with the raw cell-gene matrix. * not all tags are antibody based, so I'd vote for naming all related functions `*hashtags()`. I'd therefore vote for something like the following design:. ```{python}. # htos is a AnnData object. htos = sc.read_hashtags(filename) . # classify_hashtags adds a classification to the hto AnnData object. # kwargs might involve things like `use_tags=[""tag1"", ""tag2"", ""tag3""]`. sc.pp.classify_hashtags(htos, **kwargs). print(htos.obs.classification) . # demuxing cell-gene matrix(es) could then be done like. rna1 = sc.read_10x_h5(...). rna2 = sc.read_10x_h5(...). # sc.pp.demux_by_hashtag",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:145,performance,content,content,145,"I'm happy to implement my method, when we have a consensus. https://github.com/calico/solo/blob/master/solo/hashsolo.py. https://www.biorxiv.org/content/10.1101/841981v1",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:108,security,hash,hashsolo,108,"I'm happy to implement my method, when we have a consensus. https://github.com/calico/solo/blob/master/solo/hashsolo.py. https://www.biorxiv.org/content/10.1101/841981v1",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:113,usability,document,documentation,113,"Hello all,. Has any functionality discussed above and in #797 implemented already ? If yes, where can I find the documentation on the methods ? Thanks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:81,deployability,instal,install,81,@aditisk I'm the author of this method https://github.com/calico/solo. it should install relatively easily if you have any issues I'm happy to help. The main functionality it doesn't have is `tag_groups` so you'd have t manually create that if you have used multiple hashes per group.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:175,reliability,doe,doesn,175,@aditisk I'm the author of this method https://github.com/calico/solo. it should install relatively easily if you have any issues I'm happy to help. The main functionality it doesn't have is `tag_groups` so you'd have t manually create that if you have used multiple hashes per group.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:17,security,auth,author,17,@aditisk I'm the author of this method https://github.com/calico/solo. it should install relatively easily if you have any issues I'm happy to help. The main functionality it doesn't have is `tag_groups` so you'd have t manually create that if you have used multiple hashes per group.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:267,security,hash,hashes,267,@aditisk I'm the author of this method https://github.com/calico/solo. it should install relatively easily if you have any issues I'm happy to help. The main functionality it doesn't have is `tag_groups` so you'd have t manually create that if you have used multiple hashes per group.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:143,usability,help,help,143,@aditisk I'm the author of this method https://github.com/calico/solo. it should install relatively easily if you have any issues I'm happy to help. The main functionality it doesn't have is `tag_groups` so you'd have t manually create that if you have used multiple hashes per group.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:66,security,hash,hashing,66,I'd like to tackle this. Can someone tell me how we want to store hashing data in an `anndata` object? @flying-sheep @fidelram . I'll take back up porting hashsolo to scanpy,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:155,security,hash,hashsolo,155,I'd like to tackle this. Can someone tell me how we want to store hashing data in an `anndata` object? @flying-sheep @fidelram . I'll take back up porting hashsolo to scanpy,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:100,performance,multiplex,multiplexed,100,"I've been using a rough python implementation of Chris McGinnis's MULTIseq demuxing code for all my multiplexed experiments. This algorithm has been incorporated into Seurat as an alternative to their default `HTODemux` function. This [recent preprint](https://www.biorxiv.org/content/10.1101/2020.11.16.384222v1) suggests it's one of the better algorithms for sample demultiplexing. I recently put my implementation on GitHub here: https://github.com/wflynny/multiseq-py. Is there interest in including this in `scanpy.external` in addition to solo? If so, I can invest effort into cleaning up the implementation, adding tests, etc.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:277,performance,content,content,277,"I've been using a rough python implementation of Chris McGinnis's MULTIseq demuxing code for all my multiplexed experiments. This algorithm has been incorporated into Seurat as an alternative to their default `HTODemux` function. This [recent preprint](https://www.biorxiv.org/content/10.1101/2020.11.16.384222v1) suggests it's one of the better algorithms for sample demultiplexing. I recently put my implementation on GitHub here: https://github.com/wflynny/multiseq-py. Is there interest in including this in `scanpy.external` in addition to solo? If so, I can invest effort into cleaning up the implementation, adding tests, etc.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:622,safety,test,tests,622,"I've been using a rough python implementation of Chris McGinnis's MULTIseq demuxing code for all my multiplexed experiments. This algorithm has been incorporated into Seurat as an alternative to their default `HTODemux` function. This [recent preprint](https://www.biorxiv.org/content/10.1101/2020.11.16.384222v1) suggests it's one of the better algorithms for sample demultiplexing. I recently put my implementation on GitHub here: https://github.com/wflynny/multiseq-py. Is there interest in including this in `scanpy.external` in addition to solo? If so, I can invest effort into cleaning up the implementation, adding tests, etc.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:622,testability,test,tests,622,"I've been using a rough python implementation of Chris McGinnis's MULTIseq demuxing code for all my multiplexed experiments. This algorithm has been incorporated into Seurat as an alternative to their default `HTODemux` function. This [recent preprint](https://www.biorxiv.org/content/10.1101/2020.11.16.384222v1) suggests it's one of the better algorithms for sample demultiplexing. I recently put my implementation on GitHub here: https://github.com/wflynny/multiseq-py. Is there interest in including this in `scanpy.external` in addition to solo? If so, I can invest effort into cleaning up the implementation, adding tests, etc.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:418,deployability,log,log,418,"If your implementation already uses scanpy, the best is to keep it in your repository and we can link to it from scanpy (see https://scanpy.readthedocs.io/en/stable/ecosystem.html). I did some work on HTOs in the past and for me what worked best was to fit a gaussian mixture but I had not followed the new methods. Something that helped was to visualize the results as follows (each row a different barcode, x axis = log HTO):. ![image](https://user-images.githubusercontent.com/4964309/104469555-edfc2400-55b8-11eb-9f47-580395b255a7.png). If you are interested I can share the code with you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:75,integrability,repositor,repository,75,"If your implementation already uses scanpy, the best is to keep it in your repository and we can link to it from scanpy (see https://scanpy.readthedocs.io/en/stable/ecosystem.html). I did some work on HTOs in the past and for me what worked best was to fit a gaussian mixture but I had not followed the new methods. Something that helped was to visualize the results as follows (each row a different barcode, x axis = log HTO):. ![image](https://user-images.githubusercontent.com/4964309/104469555-edfc2400-55b8-11eb-9f47-580395b255a7.png). If you are interested I can share the code with you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:75,interoperability,repositor,repository,75,"If your implementation already uses scanpy, the best is to keep it in your repository and we can link to it from scanpy (see https://scanpy.readthedocs.io/en/stable/ecosystem.html). I did some work on HTOs in the past and for me what worked best was to fit a gaussian mixture but I had not followed the new methods. Something that helped was to visualize the results as follows (each row a different barcode, x axis = log HTO):. ![image](https://user-images.githubusercontent.com/4964309/104469555-edfc2400-55b8-11eb-9f47-580395b255a7.png). If you are interested I can share the code with you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:569,interoperability,share,share,569,"If your implementation already uses scanpy, the best is to keep it in your repository and we can link to it from scanpy (see https://scanpy.readthedocs.io/en/stable/ecosystem.html). I did some work on HTOs in the past and for me what worked best was to fit a gaussian mixture but I had not followed the new methods. Something that helped was to visualize the results as follows (each row a different barcode, x axis = log HTO):. ![image](https://user-images.githubusercontent.com/4964309/104469555-edfc2400-55b8-11eb-9f47-580395b255a7.png). If you are interested I can share the code with you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:418,safety,log,log,418,"If your implementation already uses scanpy, the best is to keep it in your repository and we can link to it from scanpy (see https://scanpy.readthedocs.io/en/stable/ecosystem.html). I did some work on HTOs in the past and for me what worked best was to fit a gaussian mixture but I had not followed the new methods. Something that helped was to visualize the results as follows (each row a different barcode, x axis = log HTO):. ![image](https://user-images.githubusercontent.com/4964309/104469555-edfc2400-55b8-11eb-9f47-580395b255a7.png). If you are interested I can share the code with you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:418,security,log,log,418,"If your implementation already uses scanpy, the best is to keep it in your repository and we can link to it from scanpy (see https://scanpy.readthedocs.io/en/stable/ecosystem.html). I did some work on HTOs in the past and for me what worked best was to fit a gaussian mixture but I had not followed the new methods. Something that helped was to visualize the results as follows (each row a different barcode, x axis = log HTO):. ![image](https://user-images.githubusercontent.com/4964309/104469555-edfc2400-55b8-11eb-9f47-580395b255a7.png). If you are interested I can share the code with you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:418,testability,log,log,418,"If your implementation already uses scanpy, the best is to keep it in your repository and we can link to it from scanpy (see https://scanpy.readthedocs.io/en/stable/ecosystem.html). I did some work on HTOs in the past and for me what worked best was to fit a gaussian mixture but I had not followed the new methods. Something that helped was to visualize the results as follows (each row a different barcode, x axis = log HTO):. ![image](https://user-images.githubusercontent.com/4964309/104469555-edfc2400-55b8-11eb-9f47-580395b255a7.png). If you are interested I can share the code with you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:331,usability,help,helped,331,"If your implementation already uses scanpy, the best is to keep it in your repository and we can link to it from scanpy (see https://scanpy.readthedocs.io/en/stable/ecosystem.html). I did some work on HTOs in the past and for me what worked best was to fit a gaussian mixture but I had not followed the new methods. Something that helped was to visualize the results as follows (each row a different barcode, x axis = log HTO):. ![image](https://user-images.githubusercontent.com/4964309/104469555-edfc2400-55b8-11eb-9f47-580395b255a7.png). If you are interested I can share the code with you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:345,usability,visual,visualize,345,"If your implementation already uses scanpy, the best is to keep it in your repository and we can link to it from scanpy (see https://scanpy.readthedocs.io/en/stable/ecosystem.html). I did some work on HTOs in the past and for me what worked best was to fit a gaussian mixture but I had not followed the new methods. Something that helped was to visualize the results as follows (each row a different barcode, x axis = log HTO):. ![image](https://user-images.githubusercontent.com/4964309/104469555-edfc2400-55b8-11eb-9f47-580395b255a7.png). If you are interested I can share the code with you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:446,usability,user,user-images,446,"If your implementation already uses scanpy, the best is to keep it in your repository and we can link to it from scanpy (see https://scanpy.readthedocs.io/en/stable/ecosystem.html). I did some work on HTOs in the past and for me what worked best was to fit a gaussian mixture but I had not followed the new methods. Something that helped was to visualize the results as follows (each row a different barcode, x axis = log HTO):. ![image](https://user-images.githubusercontent.com/4964309/104469555-edfc2400-55b8-11eb-9f47-580395b255a7.png). If you are interested I can share the code with you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:29,deployability,updat,update,29,"@fidelram Sounds good---I'll update the code and then open a PR to get it added to the ecosystem docs. In my experience with HTOs (and now LMOs & CMOs), GMMs and even poisson/negative binomial mixture models don't work particularly well for all experiments as they tend to only call 50-70% of cells as singlets/multiplets. The remaining ""negatives"" or uncalled cells can really hamper some experimental designs (like when tags correspond to different conditions/perturbations). Anecdotally, multiplexing seems substantially more difficult to get right for tissues rather than blood or cell/organoid lines. . That said, I'd be interested in any plotting code you could share :). I very much appreciate all the plotting functionality you've implemented in scanpy!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:201,energy efficiency,model,models,201,"@fidelram Sounds good---I'll update the code and then open a PR to get it added to the ecosystem docs. In my experience with HTOs (and now LMOs & CMOs), GMMs and even poisson/negative binomial mixture models don't work particularly well for all experiments as they tend to only call 50-70% of cells as singlets/multiplets. The remaining ""negatives"" or uncalled cells can really hamper some experimental designs (like when tags correspond to different conditions/perturbations). Anecdotally, multiplexing seems substantially more difficult to get right for tissues rather than blood or cell/organoid lines. . That said, I'd be interested in any plotting code you could share :). I very much appreciate all the plotting functionality you've implemented in scanpy!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:510,integrability,sub,substantially,510,"@fidelram Sounds good---I'll update the code and then open a PR to get it added to the ecosystem docs. In my experience with HTOs (and now LMOs & CMOs), GMMs and even poisson/negative binomial mixture models don't work particularly well for all experiments as they tend to only call 50-70% of cells as singlets/multiplets. The remaining ""negatives"" or uncalled cells can really hamper some experimental designs (like when tags correspond to different conditions/perturbations). Anecdotally, multiplexing seems substantially more difficult to get right for tissues rather than blood or cell/organoid lines. . That said, I'd be interested in any plotting code you could share :). I very much appreciate all the plotting functionality you've implemented in scanpy!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:668,interoperability,share,share,668,"@fidelram Sounds good---I'll update the code and then open a PR to get it added to the ecosystem docs. In my experience with HTOs (and now LMOs & CMOs), GMMs and even poisson/negative binomial mixture models don't work particularly well for all experiments as they tend to only call 50-70% of cells as singlets/multiplets. The remaining ""negatives"" or uncalled cells can really hamper some experimental designs (like when tags correspond to different conditions/perturbations). Anecdotally, multiplexing seems substantially more difficult to get right for tissues rather than blood or cell/organoid lines. . That said, I'd be interested in any plotting code you could share :). I very much appreciate all the plotting functionality you've implemented in scanpy!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:491,performance,multiplex,multiplexing,491,"@fidelram Sounds good---I'll update the code and then open a PR to get it added to the ecosystem docs. In my experience with HTOs (and now LMOs & CMOs), GMMs and even poisson/negative binomial mixture models don't work particularly well for all experiments as they tend to only call 50-70% of cells as singlets/multiplets. The remaining ""negatives"" or uncalled cells can really hamper some experimental designs (like when tags correspond to different conditions/perturbations). Anecdotally, multiplexing seems substantially more difficult to get right for tissues rather than blood or cell/organoid lines. . That said, I'd be interested in any plotting code you could share :). I very much appreciate all the plotting functionality you've implemented in scanpy!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:167,reliability,poisson,poisson,167,"@fidelram Sounds good---I'll update the code and then open a PR to get it added to the ecosystem docs. In my experience with HTOs (and now LMOs & CMOs), GMMs and even poisson/negative binomial mixture models don't work particularly well for all experiments as they tend to only call 50-70% of cells as singlets/multiplets. The remaining ""negatives"" or uncalled cells can really hamper some experimental designs (like when tags correspond to different conditions/perturbations). Anecdotally, multiplexing seems substantially more difficult to get right for tissues rather than blood or cell/organoid lines. . That said, I'd be interested in any plotting code you could share :). I very much appreciate all the plotting functionality you've implemented in scanpy!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:29,safety,updat,update,29,"@fidelram Sounds good---I'll update the code and then open a PR to get it added to the ecosystem docs. In my experience with HTOs (and now LMOs & CMOs), GMMs and even poisson/negative binomial mixture models don't work particularly well for all experiments as they tend to only call 50-70% of cells as singlets/multiplets. The remaining ""negatives"" or uncalled cells can really hamper some experimental designs (like when tags correspond to different conditions/perturbations). Anecdotally, multiplexing seems substantially more difficult to get right for tissues rather than blood or cell/organoid lines. . That said, I'd be interested in any plotting code you could share :). I very much appreciate all the plotting functionality you've implemented in scanpy!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:29,security,updat,update,29,"@fidelram Sounds good---I'll update the code and then open a PR to get it added to the ecosystem docs. In my experience with HTOs (and now LMOs & CMOs), GMMs and even poisson/negative binomial mixture models don't work particularly well for all experiments as they tend to only call 50-70% of cells as singlets/multiplets. The remaining ""negatives"" or uncalled cells can really hamper some experimental designs (like when tags correspond to different conditions/perturbations). Anecdotally, multiplexing seems substantially more difficult to get right for tissues rather than blood or cell/organoid lines. . That said, I'd be interested in any plotting code you could share :). I very much appreciate all the plotting functionality you've implemented in scanpy!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:201,security,model,models,201,"@fidelram Sounds good---I'll update the code and then open a PR to get it added to the ecosystem docs. In my experience with HTOs (and now LMOs & CMOs), GMMs and even poisson/negative binomial mixture models don't work particularly well for all experiments as they tend to only call 50-70% of cells as singlets/multiplets. The remaining ""negatives"" or uncalled cells can really hamper some experimental designs (like when tags correspond to different conditions/perturbations). Anecdotally, multiplexing seems substantially more difficult to get right for tissues rather than blood or cell/organoid lines. . That said, I'd be interested in any plotting code you could share :). I very much appreciate all the plotting functionality you've implemented in scanpy!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:109,usability,experien,experience,109,"@fidelram Sounds good---I'll update the code and then open a PR to get it added to the ecosystem docs. In my experience with HTOs (and now LMOs & CMOs), GMMs and even poisson/negative binomial mixture models don't work particularly well for all experiments as they tend to only call 50-70% of cells as singlets/multiplets. The remaining ""negatives"" or uncalled cells can really hamper some experimental designs (like when tags correspond to different conditions/perturbations). Anecdotally, multiplexing seems substantially more difficult to get right for tissues rather than blood or cell/organoid lines. . That said, I'd be interested in any plotting code you could share :). I very much appreciate all the plotting functionality you've implemented in scanpy!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:312,availability,cluster,clustering,312,"@wflynny `hashsolo` allows you to set a prior for your expected rate negatives, singlets, and doublets, which helps quite a bit with the issue you described despite modeling the log CMO counts as a normal distribution. Additionally, you can also add cell types if you've done cell-type annotation or even leiden clustering labels to help with cell type variability with CMO counts. This helped me quite a bit in kidney where NK cells had far fewer CMO counts than other cells despite being apparently live cells, e.g. good gene diversity and low mitochondrial gene percentage. @fidelram I'd be happy to add a visualization tool like you suggested if you have the code laying around.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:178,deployability,log,log,178,"@wflynny `hashsolo` allows you to set a prior for your expected rate negatives, singlets, and doublets, which helps quite a bit with the issue you described despite modeling the log CMO counts as a normal distribution. Additionally, you can also add cell types if you've done cell-type annotation or even leiden clustering labels to help with cell type variability with CMO counts. This helped me quite a bit in kidney where NK cells had far fewer CMO counts than other cells despite being apparently live cells, e.g. good gene diversity and low mitochondrial gene percentage. @fidelram I'd be happy to add a visualization tool like you suggested if you have the code laying around.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:312,deployability,cluster,clustering,312,"@wflynny `hashsolo` allows you to set a prior for your expected rate negatives, singlets, and doublets, which helps quite a bit with the issue you described despite modeling the log CMO counts as a normal distribution. Additionally, you can also add cell types if you've done cell-type annotation or even leiden clustering labels to help with cell type variability with CMO counts. This helped me quite a bit in kidney where NK cells had far fewer CMO counts than other cells despite being apparently live cells, e.g. good gene diversity and low mitochondrial gene percentage. @fidelram I'd be happy to add a visualization tool like you suggested if you have the code laying around.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:165,energy efficiency,model,modeling,165,"@wflynny `hashsolo` allows you to set a prior for your expected rate negatives, singlets, and doublets, which helps quite a bit with the issue you described despite modeling the log CMO counts as a normal distribution. Additionally, you can also add cell types if you've done cell-type annotation or even leiden clustering labels to help with cell type variability with CMO counts. This helped me quite a bit in kidney where NK cells had far fewer CMO counts than other cells despite being apparently live cells, e.g. good gene diversity and low mitochondrial gene percentage. @fidelram I'd be happy to add a visualization tool like you suggested if you have the code laying around.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:205,interoperability,distribut,distribution,205,"@wflynny `hashsolo` allows you to set a prior for your expected rate negatives, singlets, and doublets, which helps quite a bit with the issue you described despite modeling the log CMO counts as a normal distribution. Additionally, you can also add cell types if you've done cell-type annotation or even leiden clustering labels to help with cell type variability with CMO counts. This helped me quite a bit in kidney where NK cells had far fewer CMO counts than other cells despite being apparently live cells, e.g. good gene diversity and low mitochondrial gene percentage. @fidelram I'd be happy to add a visualization tool like you suggested if you have the code laying around.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:353,modifiability,variab,variability,353,"@wflynny `hashsolo` allows you to set a prior for your expected rate negatives, singlets, and doublets, which helps quite a bit with the issue you described despite modeling the log CMO counts as a normal distribution. Additionally, you can also add cell types if you've done cell-type annotation or even leiden clustering labels to help with cell type variability with CMO counts. This helped me quite a bit in kidney where NK cells had far fewer CMO counts than other cells despite being apparently live cells, e.g. good gene diversity and low mitochondrial gene percentage. @fidelram I'd be happy to add a visualization tool like you suggested if you have the code laying around.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:178,safety,log,log,178,"@wflynny `hashsolo` allows you to set a prior for your expected rate negatives, singlets, and doublets, which helps quite a bit with the issue you described despite modeling the log CMO counts as a normal distribution. Additionally, you can also add cell types if you've done cell-type annotation or even leiden clustering labels to help with cell type variability with CMO counts. This helped me quite a bit in kidney where NK cells had far fewer CMO counts than other cells despite being apparently live cells, e.g. good gene diversity and low mitochondrial gene percentage. @fidelram I'd be happy to add a visualization tool like you suggested if you have the code laying around.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:10,security,hash,hashsolo,10,"@wflynny `hashsolo` allows you to set a prior for your expected rate negatives, singlets, and doublets, which helps quite a bit with the issue you described despite modeling the log CMO counts as a normal distribution. Additionally, you can also add cell types if you've done cell-type annotation or even leiden clustering labels to help with cell type variability with CMO counts. This helped me quite a bit in kidney where NK cells had far fewer CMO counts than other cells despite being apparently live cells, e.g. good gene diversity and low mitochondrial gene percentage. @fidelram I'd be happy to add a visualization tool like you suggested if you have the code laying around.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:165,security,model,modeling,165,"@wflynny `hashsolo` allows you to set a prior for your expected rate negatives, singlets, and doublets, which helps quite a bit with the issue you described despite modeling the log CMO counts as a normal distribution. Additionally, you can also add cell types if you've done cell-type annotation or even leiden clustering labels to help with cell type variability with CMO counts. This helped me quite a bit in kidney where NK cells had far fewer CMO counts than other cells despite being apparently live cells, e.g. good gene diversity and low mitochondrial gene percentage. @fidelram I'd be happy to add a visualization tool like you suggested if you have the code laying around.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:178,security,log,log,178,"@wflynny `hashsolo` allows you to set a prior for your expected rate negatives, singlets, and doublets, which helps quite a bit with the issue you described despite modeling the log CMO counts as a normal distribution. Additionally, you can also add cell types if you've done cell-type annotation or even leiden clustering labels to help with cell type variability with CMO counts. This helped me quite a bit in kidney where NK cells had far fewer CMO counts than other cells despite being apparently live cells, e.g. good gene diversity and low mitochondrial gene percentage. @fidelram I'd be happy to add a visualization tool like you suggested if you have the code laying around.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:178,testability,log,log,178,"@wflynny `hashsolo` allows you to set a prior for your expected rate negatives, singlets, and doublets, which helps quite a bit with the issue you described despite modeling the log CMO counts as a normal distribution. Additionally, you can also add cell types if you've done cell-type annotation or even leiden clustering labels to help with cell type variability with CMO counts. This helped me quite a bit in kidney where NK cells had far fewer CMO counts than other cells despite being apparently live cells, e.g. good gene diversity and low mitochondrial gene percentage. @fidelram I'd be happy to add a visualization tool like you suggested if you have the code laying around.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:110,usability,help,helps,110,"@wflynny `hashsolo` allows you to set a prior for your expected rate negatives, singlets, and doublets, which helps quite a bit with the issue you described despite modeling the log CMO counts as a normal distribution. Additionally, you can also add cell types if you've done cell-type annotation or even leiden clustering labels to help with cell type variability with CMO counts. This helped me quite a bit in kidney where NK cells had far fewer CMO counts than other cells despite being apparently live cells, e.g. good gene diversity and low mitochondrial gene percentage. @fidelram I'd be happy to add a visualization tool like you suggested if you have the code laying around.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:333,usability,help,help,333,"@wflynny `hashsolo` allows you to set a prior for your expected rate negatives, singlets, and doublets, which helps quite a bit with the issue you described despite modeling the log CMO counts as a normal distribution. Additionally, you can also add cell types if you've done cell-type annotation or even leiden clustering labels to help with cell type variability with CMO counts. This helped me quite a bit in kidney where NK cells had far fewer CMO counts than other cells despite being apparently live cells, e.g. good gene diversity and low mitochondrial gene percentage. @fidelram I'd be happy to add a visualization tool like you suggested if you have the code laying around.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:387,usability,help,helped,387,"@wflynny `hashsolo` allows you to set a prior for your expected rate negatives, singlets, and doublets, which helps quite a bit with the issue you described despite modeling the log CMO counts as a normal distribution. Additionally, you can also add cell types if you've done cell-type annotation or even leiden clustering labels to help with cell type variability with CMO counts. This helped me quite a bit in kidney where NK cells had far fewer CMO counts than other cells despite being apparently live cells, e.g. good gene diversity and low mitochondrial gene percentage. @fidelram I'd be happy to add a visualization tool like you suggested if you have the code laying around.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:609,usability,visual,visualization,609,"@wflynny `hashsolo` allows you to set a prior for your expected rate negatives, singlets, and doublets, which helps quite a bit with the issue you described despite modeling the log CMO counts as a normal distribution. Additionally, you can also add cell types if you've done cell-type annotation or even leiden clustering labels to help with cell type variability with CMO counts. This helped me quite a bit in kidney where NK cells had far fewer CMO counts than other cells despite being apparently live cells, e.g. good gene diversity and low mitochondrial gene percentage. @fidelram I'd be happy to add a visualization tool like you suggested if you have the code laying around.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:623,usability,tool,tool,623,"@wflynny `hashsolo` allows you to set a prior for your expected rate negatives, singlets, and doublets, which helps quite a bit with the issue you described despite modeling the log CMO counts as a normal distribution. Additionally, you can also add cell types if you've done cell-type annotation or even leiden clustering labels to help with cell type variability with CMO counts. This helped me quite a bit in kidney where NK cells had far fewer CMO counts than other cells despite being apparently live cells, e.g. good gene diversity and low mitochondrial gene percentage. @fidelram I'd be happy to add a visualization tool like you suggested if you have the code laying around.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:580,deployability,contain,containing,580,"@njbernstein Probably not the right place for this discussion, but a couple of follow-up questions for you:. - Do you happen to have a benchmark of `hashsolo` vs the other demuxing algos? It's been on my todo list for ..a while.. but still haven't gotten around to doing it. I've seen the benchmarks of the doublet finding capabilities of `solo` and they look good. As a user of scrublet, it'd be nice to have one tool/codebase that handles both transcriptomic and tag multiplets. - Are you open to PRs? I'd at least like to have functionality to generate the initial h5ad object containing the tag counts from the output of `CITE-seq-Count`. - Regarding non-antibody tags, have you noticed celltype-specific preferential binding? I've had problems with LMO/CMOs where not tagging particular celltypes (like some epithelial subtypes where we had 2-3 orders of magnitude lower tag counts).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:69,integrability,coupl,couple,69,"@njbernstein Probably not the right place for this discussion, but a couple of follow-up questions for you:. - Do you happen to have a benchmark of `hashsolo` vs the other demuxing algos? It's been on my todo list for ..a while.. but still haven't gotten around to doing it. I've seen the benchmarks of the doublet finding capabilities of `solo` and they look good. As a user of scrublet, it'd be nice to have one tool/codebase that handles both transcriptomic and tag multiplets. - Are you open to PRs? I'd at least like to have functionality to generate the initial h5ad object containing the tag counts from the output of `CITE-seq-Count`. - Regarding non-antibody tags, have you noticed celltype-specific preferential binding? I've had problems with LMO/CMOs where not tagging particular celltypes (like some epithelial subtypes where we had 2-3 orders of magnitude lower tag counts).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:824,integrability,sub,subtypes,824,"@njbernstein Probably not the right place for this discussion, but a couple of follow-up questions for you:. - Do you happen to have a benchmark of `hashsolo` vs the other demuxing algos? It's been on my todo list for ..a while.. but still haven't gotten around to doing it. I've seen the benchmarks of the doublet finding capabilities of `solo` and they look good. As a user of scrublet, it'd be nice to have one tool/codebase that handles both transcriptomic and tag multiplets. - Are you open to PRs? I'd at least like to have functionality to generate the initial h5ad object containing the tag counts from the output of `CITE-seq-Count`. - Regarding non-antibody tags, have you noticed celltype-specific preferential binding? I've had problems with LMO/CMOs where not tagging particular celltypes (like some epithelial subtypes where we had 2-3 orders of magnitude lower tag counts).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:700,interoperability,specif,specific,700,"@njbernstein Probably not the right place for this discussion, but a couple of follow-up questions for you:. - Do you happen to have a benchmark of `hashsolo` vs the other demuxing algos? It's been on my todo list for ..a while.. but still haven't gotten around to doing it. I've seen the benchmarks of the doublet finding capabilities of `solo` and they look good. As a user of scrublet, it'd be nice to have one tool/codebase that handles both transcriptomic and tag multiplets. - Are you open to PRs? I'd at least like to have functionality to generate the initial h5ad object containing the tag counts from the output of `CITE-seq-Count`. - Regarding non-antibody tags, have you noticed celltype-specific preferential binding? I've had problems with LMO/CMOs where not tagging particular celltypes (like some epithelial subtypes where we had 2-3 orders of magnitude lower tag counts).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:722,interoperability,bind,binding,722,"@njbernstein Probably not the right place for this discussion, but a couple of follow-up questions for you:. - Do you happen to have a benchmark of `hashsolo` vs the other demuxing algos? It's been on my todo list for ..a while.. but still haven't gotten around to doing it. I've seen the benchmarks of the doublet finding capabilities of `solo` and they look good. As a user of scrublet, it'd be nice to have one tool/codebase that handles both transcriptomic and tag multiplets. - Are you open to PRs? I'd at least like to have functionality to generate the initial h5ad object containing the tag counts from the output of `CITE-seq-Count`. - Regarding non-antibody tags, have you noticed celltype-specific preferential binding? I've had problems with LMO/CMOs where not tagging particular celltypes (like some epithelial subtypes where we had 2-3 orders of magnitude lower tag counts).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:69,modifiability,coupl,couple,69,"@njbernstein Probably not the right place for this discussion, but a couple of follow-up questions for you:. - Do you happen to have a benchmark of `hashsolo` vs the other demuxing algos? It's been on my todo list for ..a while.. but still haven't gotten around to doing it. I've seen the benchmarks of the doublet finding capabilities of `solo` and they look good. As a user of scrublet, it'd be nice to have one tool/codebase that handles both transcriptomic and tag multiplets. - Are you open to PRs? I'd at least like to have functionality to generate the initial h5ad object containing the tag counts from the output of `CITE-seq-Count`. - Regarding non-antibody tags, have you noticed celltype-specific preferential binding? I've had problems with LMO/CMOs where not tagging particular celltypes (like some epithelial subtypes where we had 2-3 orders of magnitude lower tag counts).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:722,modifiability,bind,binding,722,"@njbernstein Probably not the right place for this discussion, but a couple of follow-up questions for you:. - Do you happen to have a benchmark of `hashsolo` vs the other demuxing algos? It's been on my todo list for ..a while.. but still haven't gotten around to doing it. I've seen the benchmarks of the doublet finding capabilities of `solo` and they look good. As a user of scrublet, it'd be nice to have one tool/codebase that handles both transcriptomic and tag multiplets. - Are you open to PRs? I'd at least like to have functionality to generate the initial h5ad object containing the tag counts from the output of `CITE-seq-Count`. - Regarding non-antibody tags, have you noticed celltype-specific preferential binding? I've had problems with LMO/CMOs where not tagging particular celltypes (like some epithelial subtypes where we had 2-3 orders of magnitude lower tag counts).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:149,security,hash,hashsolo,149,"@njbernstein Probably not the right place for this discussion, but a couple of follow-up questions for you:. - Do you happen to have a benchmark of `hashsolo` vs the other demuxing algos? It's been on my todo list for ..a while.. but still haven't gotten around to doing it. I've seen the benchmarks of the doublet finding capabilities of `solo` and they look good. As a user of scrublet, it'd be nice to have one tool/codebase that handles both transcriptomic and tag multiplets. - Are you open to PRs? I'd at least like to have functionality to generate the initial h5ad object containing the tag counts from the output of `CITE-seq-Count`. - Regarding non-antibody tags, have you noticed celltype-specific preferential binding? I've had problems with LMO/CMOs where not tagging particular celltypes (like some epithelial subtypes where we had 2-3 orders of magnitude lower tag counts).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:69,testability,coupl,couple,69,"@njbernstein Probably not the right place for this discussion, but a couple of follow-up questions for you:. - Do you happen to have a benchmark of `hashsolo` vs the other demuxing algos? It's been on my todo list for ..a while.. but still haven't gotten around to doing it. I've seen the benchmarks of the doublet finding capabilities of `solo` and they look good. As a user of scrublet, it'd be nice to have one tool/codebase that handles both transcriptomic and tag multiplets. - Are you open to PRs? I'd at least like to have functionality to generate the initial h5ad object containing the tag counts from the output of `CITE-seq-Count`. - Regarding non-antibody tags, have you noticed celltype-specific preferential binding? I've had problems with LMO/CMOs where not tagging particular celltypes (like some epithelial subtypes where we had 2-3 orders of magnitude lower tag counts).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:371,usability,user,user,371,"@njbernstein Probably not the right place for this discussion, but a couple of follow-up questions for you:. - Do you happen to have a benchmark of `hashsolo` vs the other demuxing algos? It's been on my todo list for ..a while.. but still haven't gotten around to doing it. I've seen the benchmarks of the doublet finding capabilities of `solo` and they look good. As a user of scrublet, it'd be nice to have one tool/codebase that handles both transcriptomic and tag multiplets. - Are you open to PRs? I'd at least like to have functionality to generate the initial h5ad object containing the tag counts from the output of `CITE-seq-Count`. - Regarding non-antibody tags, have you noticed celltype-specific preferential binding? I've had problems with LMO/CMOs where not tagging particular celltypes (like some epithelial subtypes where we had 2-3 orders of magnitude lower tag counts).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:414,usability,tool,tool,414,"@njbernstein Probably not the right place for this discussion, but a couple of follow-up questions for you:. - Do you happen to have a benchmark of `hashsolo` vs the other demuxing algos? It's been on my todo list for ..a while.. but still haven't gotten around to doing it. I've seen the benchmarks of the doublet finding capabilities of `solo` and they look good. As a user of scrublet, it'd be nice to have one tool/codebase that handles both transcriptomic and tag multiplets. - Are you open to PRs? I'd at least like to have functionality to generate the initial h5ad object containing the tag counts from the output of `CITE-seq-Count`. - Regarding non-antibody tags, have you noticed celltype-specific preferential binding? I've had problems with LMO/CMOs where not tagging particular celltypes (like some epithelial subtypes where we had 2-3 orders of magnitude lower tag counts).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:709,usability,prefer,preferential,709,"@njbernstein Probably not the right place for this discussion, but a couple of follow-up questions for you:. - Do you happen to have a benchmark of `hashsolo` vs the other demuxing algos? It's been on my todo list for ..a while.. but still haven't gotten around to doing it. I've seen the benchmarks of the doublet finding capabilities of `solo` and they look good. As a user of scrublet, it'd be nice to have one tool/codebase that handles both transcriptomic and tag multiplets. - Are you open to PRs? I'd at least like to have functionality to generate the initial h5ad object containing the tag counts from the output of `CITE-seq-Count`. - Regarding non-antibody tags, have you noticed celltype-specific preferential binding? I've had problems with LMO/CMOs where not tagging particular celltypes (like some epithelial subtypes where we had 2-3 orders of magnitude lower tag counts).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:385,availability,recov,recover,385,"In supplementary figure 9 of our paper, I did a light comparison of tools using the demuxlet data as ground truth: https://www.cell.com/cms/10.1016/j.cels.2020.05.010/attachment/040c239d-1e70-42a4-8974-9fbd75c65551/mmc1.pdf. Which I think is a fine first stab at getting at this comparison, but it could be better. Hashsolo performance was comparable with other methods but is able to recover cell types with lower CMO counts. . I think that sounds great. That's an issue we had as well, but I noticed it occurring for NK cells in kidney. ![Screen Shot 2021-01-13 at 9 18 30 AM](https://user-images.githubusercontent.com/6864886/104486266-5d095680-5580-11eb-971e-c882063f2a45.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:385,deployability,recov,recover,385,"In supplementary figure 9 of our paper, I did a light comparison of tools using the demuxlet data as ground truth: https://www.cell.com/cms/10.1016/j.cels.2020.05.010/attachment/040c239d-1e70-42a4-8974-9fbd75c65551/mmc1.pdf. Which I think is a fine first stab at getting at this comparison, but it could be better. Hashsolo performance was comparable with other methods but is able to recover cell types with lower CMO counts. . I think that sounds great. That's an issue we had as well, but I noticed it occurring for NK cells in kidney. ![Screen Shot 2021-01-13 at 9 18 30 AM](https://user-images.githubusercontent.com/6864886/104486266-5d095680-5580-11eb-971e-c882063f2a45.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:324,performance,perform,performance,324,"In supplementary figure 9 of our paper, I did a light comparison of tools using the demuxlet data as ground truth: https://www.cell.com/cms/10.1016/j.cels.2020.05.010/attachment/040c239d-1e70-42a4-8974-9fbd75c65551/mmc1.pdf. Which I think is a fine first stab at getting at this comparison, but it could be better. Hashsolo performance was comparable with other methods but is able to recover cell types with lower CMO counts. . I think that sounds great. That's an issue we had as well, but I noticed it occurring for NK cells in kidney. ![Screen Shot 2021-01-13 at 9 18 30 AM](https://user-images.githubusercontent.com/6864886/104486266-5d095680-5580-11eb-971e-c882063f2a45.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:385,reliability,recov,recover,385,"In supplementary figure 9 of our paper, I did a light comparison of tools using the demuxlet data as ground truth: https://www.cell.com/cms/10.1016/j.cels.2020.05.010/attachment/040c239d-1e70-42a4-8974-9fbd75c65551/mmc1.pdf. Which I think is a fine first stab at getting at this comparison, but it could be better. Hashsolo performance was comparable with other methods but is able to recover cell types with lower CMO counts. . I think that sounds great. That's an issue we had as well, but I noticed it occurring for NK cells in kidney. ![Screen Shot 2021-01-13 at 9 18 30 AM](https://user-images.githubusercontent.com/6864886/104486266-5d095680-5580-11eb-971e-c882063f2a45.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:385,safety,recov,recover,385,"In supplementary figure 9 of our paper, I did a light comparison of tools using the demuxlet data as ground truth: https://www.cell.com/cms/10.1016/j.cels.2020.05.010/attachment/040c239d-1e70-42a4-8974-9fbd75c65551/mmc1.pdf. Which I think is a fine first stab at getting at this comparison, but it could be better. Hashsolo performance was comparable with other methods but is able to recover cell types with lower CMO counts. . I think that sounds great. That's an issue we had as well, but I noticed it occurring for NK cells in kidney. ![Screen Shot 2021-01-13 at 9 18 30 AM](https://user-images.githubusercontent.com/6864886/104486266-5d095680-5580-11eb-971e-c882063f2a45.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:315,security,Hash,Hashsolo,315,"In supplementary figure 9 of our paper, I did a light comparison of tools using the demuxlet data as ground truth: https://www.cell.com/cms/10.1016/j.cels.2020.05.010/attachment/040c239d-1e70-42a4-8974-9fbd75c65551/mmc1.pdf. Which I think is a fine first stab at getting at this comparison, but it could be better. Hashsolo performance was comparable with other methods but is able to recover cell types with lower CMO counts. . I think that sounds great. That's an issue we had as well, but I noticed it occurring for NK cells in kidney. ![Screen Shot 2021-01-13 at 9 18 30 AM](https://user-images.githubusercontent.com/6864886/104486266-5d095680-5580-11eb-971e-c882063f2a45.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:385,security,recov,recover,385,"In supplementary figure 9 of our paper, I did a light comparison of tools using the demuxlet data as ground truth: https://www.cell.com/cms/10.1016/j.cels.2020.05.010/attachment/040c239d-1e70-42a4-8974-9fbd75c65551/mmc1.pdf. Which I think is a fine first stab at getting at this comparison, but it could be better. Hashsolo performance was comparable with other methods but is able to recover cell types with lower CMO counts. . I think that sounds great. That's an issue we had as well, but I noticed it occurring for NK cells in kidney. ![Screen Shot 2021-01-13 at 9 18 30 AM](https://user-images.githubusercontent.com/6864886/104486266-5d095680-5580-11eb-971e-c882063f2a45.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:68,usability,tool,tools,68,"In supplementary figure 9 of our paper, I did a light comparison of tools using the demuxlet data as ground truth: https://www.cell.com/cms/10.1016/j.cels.2020.05.010/attachment/040c239d-1e70-42a4-8974-9fbd75c65551/mmc1.pdf. Which I think is a fine first stab at getting at this comparison, but it could be better. Hashsolo performance was comparable with other methods but is able to recover cell types with lower CMO counts. . I think that sounds great. That's an issue we had as well, but I noticed it occurring for NK cells in kidney. ![Screen Shot 2021-01-13 at 9 18 30 AM](https://user-images.githubusercontent.com/6864886/104486266-5d095680-5580-11eb-971e-c882063f2a45.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:324,usability,perform,performance,324,"In supplementary figure 9 of our paper, I did a light comparison of tools using the demuxlet data as ground truth: https://www.cell.com/cms/10.1016/j.cels.2020.05.010/attachment/040c239d-1e70-42a4-8974-9fbd75c65551/mmc1.pdf. Which I think is a fine first stab at getting at this comparison, but it could be better. Hashsolo performance was comparable with other methods but is able to recover cell types with lower CMO counts. . I think that sounds great. That's an issue we had as well, but I noticed it occurring for NK cells in kidney. ![Screen Shot 2021-01-13 at 9 18 30 AM](https://user-images.githubusercontent.com/6864886/104486266-5d095680-5580-11eb-971e-c882063f2a45.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:587,usability,user,user-images,587,"In supplementary figure 9 of our paper, I did a light comparison of tools using the demuxlet data as ground truth: https://www.cell.com/cms/10.1016/j.cels.2020.05.010/attachment/040c239d-1e70-42a4-8974-9fbd75c65551/mmc1.pdf. Which I think is a fine first stab at getting at this comparison, but it could be better. Hashsolo performance was comparable with other methods but is able to recover cell types with lower CMO counts. . I think that sounds great. That's an issue we had as well, but I noticed it occurring for NK cells in kidney. ![Screen Shot 2021-01-13 at 9 18 30 AM](https://user-images.githubusercontent.com/6864886/104486266-5d095680-5580-11eb-971e-c882063f2a45.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:4,deployability,updat,updates,4,Any updates on this thread?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:4,safety,updat,updates,4,Any updates on this thread?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:4,security,updat,updates,4,Any updates on this thread?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:13,security,hash,hashsolo,13,@brianpenghe hashsolo is implementer in scanpy now,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:83,deployability,version,version,83,> @brianpenghe hashsolo is implementer in scanpy now. That would be awesome. Which version of Scanpy includes hashsolo? Any Scanpy tutorials?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:83,integrability,version,version,83,> @brianpenghe hashsolo is implementer in scanpy now. That would be awesome. Which version of Scanpy includes hashsolo? Any Scanpy tutorials?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:83,modifiability,version,version,83,> @brianpenghe hashsolo is implementer in scanpy now. That would be awesome. Which version of Scanpy includes hashsolo? Any Scanpy tutorials?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:15,security,hash,hashsolo,15,> @brianpenghe hashsolo is implementer in scanpy now. That would be awesome. Which version of Scanpy includes hashsolo? Any Scanpy tutorials?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:110,security,hash,hashsolo,110,> @brianpenghe hashsolo is implementer in scanpy now. That would be awesome. Which version of Scanpy includes hashsolo? Any Scanpy tutorials?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:74,deployability,api,api,74,@brianpenghe solo is here at least: https://docs.scvi-tools.org/en/stable/api/reference/scvi.external.SOLO.html Don't think hashsolo is anywhere yet though,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:74,integrability,api,api,74,@brianpenghe solo is here at least: https://docs.scvi-tools.org/en/stable/api/reference/scvi.external.SOLO.html Don't think hashsolo is anywhere yet though,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:74,interoperability,api,api,74,@brianpenghe solo is here at least: https://docs.scvi-tools.org/en/stable/api/reference/scvi.external.SOLO.html Don't think hashsolo is anywhere yet though,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:124,security,hash,hashsolo,124,@brianpenghe solo is here at least: https://docs.scvi-tools.org/en/stable/api/reference/scvi.external.SOLO.html Don't think hashsolo is anywhere yet though,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:54,usability,tool,tools,54,@brianpenghe solo is here at least: https://docs.scvi-tools.org/en/stable/api/reference/scvi.external.SOLO.html Don't think hashsolo is anywhere yet though,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:55,deployability,api,api,55,@Zethson hashsolo is in scanpy already in the external api. Let me know if you have any questions about it !,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:55,integrability,api,api,55,@Zethson hashsolo is in scanpy already in the external api. Let me know if you have any questions about it !,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:55,interoperability,api,api,55,@Zethson hashsolo is in scanpy already in the external api. Let me know if you have any questions about it !,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:9,security,hash,hashsolo,9,@Zethson hashsolo is in scanpy already in the external api. Let me know if you have any questions about it !,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:35,deployability,version,version,35,Hi @njbernstein . I'm using scanpy version 1.10.2 and when I tried to run hashsholo but it's not working and I'm not sure why. ![image](https://github.com/user-attachments/assets/3a542123-f7b1-432d-9898-cb25ed52bd51). Any suggestions? Thank you. Edit: htos is just the list with he two hashtag names shown in the obs,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:35,integrability,version,version,35,Hi @njbernstein . I'm using scanpy version 1.10.2 and when I tried to run hashsholo but it's not working and I'm not sure why. ![image](https://github.com/user-attachments/assets/3a542123-f7b1-432d-9898-cb25ed52bd51). Any suggestions? Thank you. Edit: htos is just the list with he two hashtag names shown in the obs,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:35,modifiability,version,version,35,Hi @njbernstein . I'm using scanpy version 1.10.2 and when I tried to run hashsholo but it's not working and I'm not sure why. ![image](https://github.com/user-attachments/assets/3a542123-f7b1-432d-9898-cb25ed52bd51). Any suggestions? Thank you. Edit: htos is just the list with he two hashtag names shown in the obs,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:74,security,hash,hashsholo,74,Hi @njbernstein . I'm using scanpy version 1.10.2 and when I tried to run hashsholo but it's not working and I'm not sure why. ![image](https://github.com/user-attachments/assets/3a542123-f7b1-432d-9898-cb25ed52bd51). Any suggestions? Thank you. Edit: htos is just the list with he two hashtag names shown in the obs,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:286,security,hash,hashtag,286,Hi @njbernstein . I'm using scanpy version 1.10.2 and when I tried to run hashsholo but it's not working and I'm not sure why. ![image](https://github.com/user-attachments/assets/3a542123-f7b1-432d-9898-cb25ed52bd51). Any suggestions? Thank you. Edit: htos is just the list with he two hashtag names shown in the obs,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:155,usability,user,user-attachments,155,Hi @njbernstein . I'm using scanpy version 1.10.2 and when I tried to run hashsholo but it's not working and I'm not sure why. ![image](https://github.com/user-attachments/assets/3a542123-f7b1-432d-9898-cb25ed52bd51). Any suggestions? Thank you. Edit: htos is just the list with he two hashtag names shown in the obs,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:99,security,hash,hashsolo,99,"@Lucas-Maciel . Can you try setting the `number_of_noise_barcodes = 1`? . e.g. `scanpy.external.pp.hashsolo(adata, cell_hashing_columns, *, priors=(0.01, 0.8, 0.19), pre_existing_clusters=None, number_of_noise_barcodes=1)`. https://scanpy.readthedocs.io/en/stable/generated/scanpy.external.pp.hashsolo.html.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/351:293,security,hash,hashsolo,293,"@Lucas-Maciel . Can you try setting the `number_of_noise_barcodes = 1`? . e.g. `scanpy.external.pp.hashsolo(adata, cell_hashing_columns, *, priors=(0.01, 0.8, 0.19), pre_existing_clusters=None, number_of_noise_barcodes=1)`. https://scanpy.readthedocs.io/en/stable/generated/scanpy.external.pp.hashsolo.html.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351
https://github.com/scverse/scanpy/issues/353:20,availability,sli,slicing,20,"Simply use standard slicing. The most elegant way in this case would be something like [this](https://github.com/theislab/scanpy/blob/7de1f5159c91d6d2243bb7866d9495ee6747c750/scanpy/datasets/__init__.py#L108-L109), I guess. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/353
https://github.com/scverse/scanpy/issues/353:11,interoperability,standard,standard,11,"Simply use standard slicing. The most elegant way in this case would be something like [this](https://github.com/theislab/scanpy/blob/7de1f5159c91d6d2243bb7866d9495ee6747c750/scanpy/datasets/__init__.py#L108-L109), I guess. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/353
https://github.com/scverse/scanpy/issues/353:20,reliability,sli,slicing,20,"Simply use standard slicing. The most elegant way in this case would be something like [this](https://github.com/theislab/scanpy/blob/7de1f5159c91d6d2243bb7866d9495ee6747c750/scanpy/datasets/__init__.py#L108-L109), I guess. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/353
https://github.com/scverse/scanpy/issues/353:0,testability,Simpl,Simply,0,"Simply use standard slicing. The most elegant way in this case would be something like [this](https://github.com/theislab/scanpy/blob/7de1f5159c91d6d2243bb7866d9495ee6747c750/scanpy/datasets/__init__.py#L108-L109), I guess. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/353
https://github.com/scverse/scanpy/issues/353:0,usability,Simpl,Simply,0,"Simply use standard slicing. The most elegant way in this case would be something like [this](https://github.com/theislab/scanpy/blob/7de1f5159c91d6d2243bb7866d9495ee6747c750/scanpy/datasets/__init__.py#L108-L109), I guess. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/353
https://github.com/scverse/scanpy/issues/355:0,deployability,Updat,Update,0,"Update: the issue is that `apt-get install -y python3-pip` installs Python version 3.5.2. You would need to upgrade to version 3.6, as the readme suggests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/355
https://github.com/scverse/scanpy/issues/355:35,deployability,instal,install,35,"Update: the issue is that `apt-get install -y python3-pip` installs Python version 3.5.2. You would need to upgrade to version 3.6, as the readme suggests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/355
https://github.com/scverse/scanpy/issues/355:59,deployability,instal,installs,59,"Update: the issue is that `apt-get install -y python3-pip` installs Python version 3.5.2. You would need to upgrade to version 3.6, as the readme suggests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/355
https://github.com/scverse/scanpy/issues/355:75,deployability,version,version,75,"Update: the issue is that `apt-get install -y python3-pip` installs Python version 3.5.2. You would need to upgrade to version 3.6, as the readme suggests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/355
https://github.com/scverse/scanpy/issues/355:108,deployability,upgrad,upgrade,108,"Update: the issue is that `apt-get install -y python3-pip` installs Python version 3.5.2. You would need to upgrade to version 3.6, as the readme suggests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/355
https://github.com/scverse/scanpy/issues/355:119,deployability,version,version,119,"Update: the issue is that `apt-get install -y python3-pip` installs Python version 3.5.2. You would need to upgrade to version 3.6, as the readme suggests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/355
https://github.com/scverse/scanpy/issues/355:75,integrability,version,version,75,"Update: the issue is that `apt-get install -y python3-pip` installs Python version 3.5.2. You would need to upgrade to version 3.6, as the readme suggests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/355
https://github.com/scverse/scanpy/issues/355:119,integrability,version,version,119,"Update: the issue is that `apt-get install -y python3-pip` installs Python version 3.5.2. You would need to upgrade to version 3.6, as the readme suggests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/355
https://github.com/scverse/scanpy/issues/355:75,modifiability,version,version,75,"Update: the issue is that `apt-get install -y python3-pip` installs Python version 3.5.2. You would need to upgrade to version 3.6, as the readme suggests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/355
https://github.com/scverse/scanpy/issues/355:108,modifiability,upgrad,upgrade,108,"Update: the issue is that `apt-get install -y python3-pip` installs Python version 3.5.2. You would need to upgrade to version 3.6, as the readme suggests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/355
https://github.com/scverse/scanpy/issues/355:119,modifiability,version,version,119,"Update: the issue is that `apt-get install -y python3-pip` installs Python version 3.5.2. You would need to upgrade to version 3.6, as the readme suggests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/355
https://github.com/scverse/scanpy/issues/355:0,safety,Updat,Update,0,"Update: the issue is that `apt-get install -y python3-pip` installs Python version 3.5.2. You would need to upgrade to version 3.6, as the readme suggests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/355
https://github.com/scverse/scanpy/issues/355:0,security,Updat,Update,0,"Update: the issue is that `apt-get install -y python3-pip` installs Python version 3.5.2. You would need to upgrade to version 3.6, as the readme suggests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/355
https://github.com/scverse/scanpy/issues/355:27,security,apt,apt-get,27,"Update: the issue is that `apt-get install -y python3-pip` installs Python version 3.5.2. You would need to upgrade to version 3.6, as the readme suggests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/355
https://github.com/scverse/scanpy/pull/358:96,deployability,updat,update,96,"PS: Don't worry about the tutorial, I'll move that into the Scanpy docs without images soon and update it there. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/358
https://github.com/scverse/scanpy/pull/358:96,safety,updat,update,96,"PS: Don't worry about the tutorial, I'll move that into the Scanpy docs without images soon and update it there. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/358
https://github.com/scverse/scanpy/pull/358:96,security,updat,update,96,"PS: Don't worry about the tutorial, I'll move that into the Scanpy docs without images soon and update it there. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/358
https://github.com/scverse/scanpy/pull/360:102,deployability,instal,installation,102,"Hm, yes it's nice that things are simpler now, but the point of the script before was to use the fast installation of the conda binaries... . Before your commit: 3 min 46 s test time (https://travis-ci.org/theislab/scanpy/builds/454438531?utm_source=github_status&utm_medium=notification). After your commit: 6 min 46 s test time (https://travis-ci.org/theislab/scanpy/builds/454487170?utm_source=github_status&utm_medium=notification). While the 3 min 46 s are way too long, there is still a good chance that you realize that your commit broke everything. After almost 7 min, you're almost always doing something else already. I also feel kind of bad about travis's servers. ;)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/360
https://github.com/scverse/scanpy/pull/360:222,deployability,build,builds,222,"Hm, yes it's nice that things are simpler now, but the point of the script before was to use the fast installation of the conda binaries... . Before your commit: 3 min 46 s test time (https://travis-ci.org/theislab/scanpy/builds/454438531?utm_source=github_status&utm_medium=notification). After your commit: 6 min 46 s test time (https://travis-ci.org/theislab/scanpy/builds/454487170?utm_source=github_status&utm_medium=notification). While the 3 min 46 s are way too long, there is still a good chance that you realize that your commit broke everything. After almost 7 min, you're almost always doing something else already. I also feel kind of bad about travis's servers. ;)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/360
https://github.com/scverse/scanpy/pull/360:369,deployability,build,builds,369,"Hm, yes it's nice that things are simpler now, but the point of the script before was to use the fast installation of the conda binaries... . Before your commit: 3 min 46 s test time (https://travis-ci.org/theislab/scanpy/builds/454438531?utm_source=github_status&utm_medium=notification). After your commit: 6 min 46 s test time (https://travis-ci.org/theislab/scanpy/builds/454487170?utm_source=github_status&utm_medium=notification). While the 3 min 46 s are way too long, there is still a good chance that you realize that your commit broke everything. After almost 7 min, you're almost always doing something else already. I also feel kind of bad about travis's servers. ;)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/360
https://github.com/scverse/scanpy/pull/360:178,performance,time,time,178,"Hm, yes it's nice that things are simpler now, but the point of the script before was to use the fast installation of the conda binaries... . Before your commit: 3 min 46 s test time (https://travis-ci.org/theislab/scanpy/builds/454438531?utm_source=github_status&utm_medium=notification). After your commit: 6 min 46 s test time (https://travis-ci.org/theislab/scanpy/builds/454487170?utm_source=github_status&utm_medium=notification). While the 3 min 46 s are way too long, there is still a good chance that you realize that your commit broke everything. After almost 7 min, you're almost always doing something else already. I also feel kind of bad about travis's servers. ;)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/360
https://github.com/scverse/scanpy/pull/360:325,performance,time,time,325,"Hm, yes it's nice that things are simpler now, but the point of the script before was to use the fast installation of the conda binaries... . Before your commit: 3 min 46 s test time (https://travis-ci.org/theislab/scanpy/builds/454438531?utm_source=github_status&utm_medium=notification). After your commit: 6 min 46 s test time (https://travis-ci.org/theislab/scanpy/builds/454487170?utm_source=github_status&utm_medium=notification). While the 3 min 46 s are way too long, there is still a good chance that you realize that your commit broke everything. After almost 7 min, you're almost always doing something else already. I also feel kind of bad about travis's servers. ;)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/360
https://github.com/scverse/scanpy/pull/360:173,safety,test,test,173,"Hm, yes it's nice that things are simpler now, but the point of the script before was to use the fast installation of the conda binaries... . Before your commit: 3 min 46 s test time (https://travis-ci.org/theislab/scanpy/builds/454438531?utm_source=github_status&utm_medium=notification). After your commit: 6 min 46 s test time (https://travis-ci.org/theislab/scanpy/builds/454487170?utm_source=github_status&utm_medium=notification). While the 3 min 46 s are way too long, there is still a good chance that you realize that your commit broke everything. After almost 7 min, you're almost always doing something else already. I also feel kind of bad about travis's servers. ;)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/360
https://github.com/scverse/scanpy/pull/360:320,safety,test,test,320,"Hm, yes it's nice that things are simpler now, but the point of the script before was to use the fast installation of the conda binaries... . Before your commit: 3 min 46 s test time (https://travis-ci.org/theislab/scanpy/builds/454438531?utm_source=github_status&utm_medium=notification). After your commit: 6 min 46 s test time (https://travis-ci.org/theislab/scanpy/builds/454487170?utm_source=github_status&utm_medium=notification). While the 3 min 46 s are way too long, there is still a good chance that you realize that your commit broke everything. After almost 7 min, you're almost always doing something else already. I also feel kind of bad about travis's servers. ;)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/360
https://github.com/scverse/scanpy/pull/360:34,testability,simpl,simpler,34,"Hm, yes it's nice that things are simpler now, but the point of the script before was to use the fast installation of the conda binaries... . Before your commit: 3 min 46 s test time (https://travis-ci.org/theislab/scanpy/builds/454438531?utm_source=github_status&utm_medium=notification). After your commit: 6 min 46 s test time (https://travis-ci.org/theislab/scanpy/builds/454487170?utm_source=github_status&utm_medium=notification). While the 3 min 46 s are way too long, there is still a good chance that you realize that your commit broke everything. After almost 7 min, you're almost always doing something else already. I also feel kind of bad about travis's servers. ;)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/360
https://github.com/scverse/scanpy/pull/360:173,testability,test,test,173,"Hm, yes it's nice that things are simpler now, but the point of the script before was to use the fast installation of the conda binaries... . Before your commit: 3 min 46 s test time (https://travis-ci.org/theislab/scanpy/builds/454438531?utm_source=github_status&utm_medium=notification). After your commit: 6 min 46 s test time (https://travis-ci.org/theislab/scanpy/builds/454487170?utm_source=github_status&utm_medium=notification). While the 3 min 46 s are way too long, there is still a good chance that you realize that your commit broke everything. After almost 7 min, you're almost always doing something else already. I also feel kind of bad about travis's servers. ;)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/360
https://github.com/scverse/scanpy/pull/360:320,testability,test,test,320,"Hm, yes it's nice that things are simpler now, but the point of the script before was to use the fast installation of the conda binaries... . Before your commit: 3 min 46 s test time (https://travis-ci.org/theislab/scanpy/builds/454438531?utm_source=github_status&utm_medium=notification). After your commit: 6 min 46 s test time (https://travis-ci.org/theislab/scanpy/builds/454487170?utm_source=github_status&utm_medium=notification). While the 3 min 46 s are way too long, there is still a good chance that you realize that your commit broke everything. After almost 7 min, you're almost always doing something else already. I also feel kind of bad about travis's servers. ;)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/360
https://github.com/scverse/scanpy/pull/360:34,usability,simpl,simpler,34,"Hm, yes it's nice that things are simpler now, but the point of the script before was to use the fast installation of the conda binaries... . Before your commit: 3 min 46 s test time (https://travis-ci.org/theislab/scanpy/builds/454438531?utm_source=github_status&utm_medium=notification). After your commit: 6 min 46 s test time (https://travis-ci.org/theislab/scanpy/builds/454487170?utm_source=github_status&utm_medium=notification). While the 3 min 46 s are way too long, there is still a good chance that you realize that your commit broke everything. After almost 7 min, you're almost always doing something else already. I also feel kind of bad about travis's servers. ;)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/360
https://github.com/scverse/scanpy/pull/360:203,deployability,instal,install,203,"Well, so essentially, this PR reversed what I did quite some time ago to speed up the CI... But, let's leave it like this. Hopefully, at some point, we'll have a less hackish way than the previous conda install script of dealing with this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/360
https://github.com/scverse/scanpy/pull/360:61,performance,time,time,61,"Well, so essentially, this PR reversed what I did quite some time ago to speed up the CI... But, let's leave it like this. Hopefully, at some point, we'll have a less hackish way than the previous conda install script of dealing with this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/360
https://github.com/scverse/scanpy/pull/360:167,security,hack,hackish,167,"Well, so essentially, this PR reversed what I did quite some time ago to speed up the CI... But, let's leave it like this. Hopefully, at some point, we'll have a less hackish way than the previous conda install script of dealing with this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/360
https://github.com/scverse/scanpy/pull/360:152,deployability,build,build,152,"> Well, so essentially, this PR reversed what I did quite some time ago to speed up the CI... Exactly! With the crucial difference that after the first build, binary packages for everything are being cached by pip, *and* now we dont have to install conda every build. The one you linked to was just this one initial build. The real numbers are now:. &nbsp; | Runtime | Total | Link. --- | --- | --- | ---. Before (conda) | 4m47s | 8m33s | https://travis-ci.org/theislab/scanpy/builds/454438531 . After (pip) | 3m34s | 5m 2s | https://travis-ci.org/theislab/scanpy/builds/456855724. > But, let's leave it like this. Hopefully, at some point, we'll have a less hackish way than the previous conda install script of dealing with this. The time is now, wheee!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/360
https://github.com/scverse/scanpy/pull/360:242,deployability,instal,install,242,"> Well, so essentially, this PR reversed what I did quite some time ago to speed up the CI... Exactly! With the crucial difference that after the first build, binary packages for everything are being cached by pip, *and* now we dont have to install conda every build. The one you linked to was just this one initial build. The real numbers are now:. &nbsp; | Runtime | Total | Link. --- | --- | --- | ---. Before (conda) | 4m47s | 8m33s | https://travis-ci.org/theislab/scanpy/builds/454438531 . After (pip) | 3m34s | 5m 2s | https://travis-ci.org/theislab/scanpy/builds/456855724. > But, let's leave it like this. Hopefully, at some point, we'll have a less hackish way than the previous conda install script of dealing with this. The time is now, wheee!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/360
https://github.com/scverse/scanpy/pull/360:262,deployability,build,build,262,"> Well, so essentially, this PR reversed what I did quite some time ago to speed up the CI... Exactly! With the crucial difference that after the first build, binary packages for everything are being cached by pip, *and* now we dont have to install conda every build. The one you linked to was just this one initial build. The real numbers are now:. &nbsp; | Runtime | Total | Link. --- | --- | --- | ---. Before (conda) | 4m47s | 8m33s | https://travis-ci.org/theislab/scanpy/builds/454438531 . After (pip) | 3m34s | 5m 2s | https://travis-ci.org/theislab/scanpy/builds/456855724. > But, let's leave it like this. Hopefully, at some point, we'll have a less hackish way than the previous conda install script of dealing with this. The time is now, wheee!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/360
https://github.com/scverse/scanpy/pull/360:317,deployability,build,build,317,"> Well, so essentially, this PR reversed what I did quite some time ago to speed up the CI... Exactly! With the crucial difference that after the first build, binary packages for everything are being cached by pip, *and* now we dont have to install conda every build. The one you linked to was just this one initial build. The real numbers are now:. &nbsp; | Runtime | Total | Link. --- | --- | --- | ---. Before (conda) | 4m47s | 8m33s | https://travis-ci.org/theislab/scanpy/builds/454438531 . After (pip) | 3m34s | 5m 2s | https://travis-ci.org/theislab/scanpy/builds/456855724. > But, let's leave it like this. Hopefully, at some point, we'll have a less hackish way than the previous conda install script of dealing with this. The time is now, wheee!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/360
https://github.com/scverse/scanpy/pull/360:478,deployability,build,builds,478,"> Well, so essentially, this PR reversed what I did quite some time ago to speed up the CI... Exactly! With the crucial difference that after the first build, binary packages for everything are being cached by pip, *and* now we dont have to install conda every build. The one you linked to was just this one initial build. The real numbers are now:. &nbsp; | Runtime | Total | Link. --- | --- | --- | ---. Before (conda) | 4m47s | 8m33s | https://travis-ci.org/theislab/scanpy/builds/454438531 . After (pip) | 3m34s | 5m 2s | https://travis-ci.org/theislab/scanpy/builds/456855724. > But, let's leave it like this. Hopefully, at some point, we'll have a less hackish way than the previous conda install script of dealing with this. The time is now, wheee!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/360
https://github.com/scverse/scanpy/pull/360:565,deployability,build,builds,565,"> Well, so essentially, this PR reversed what I did quite some time ago to speed up the CI... Exactly! With the crucial difference that after the first build, binary packages for everything are being cached by pip, *and* now we dont have to install conda every build. The one you linked to was just this one initial build. The real numbers are now:. &nbsp; | Runtime | Total | Link. --- | --- | --- | ---. Before (conda) | 4m47s | 8m33s | https://travis-ci.org/theislab/scanpy/builds/454438531 . After (pip) | 3m34s | 5m 2s | https://travis-ci.org/theislab/scanpy/builds/456855724. > But, let's leave it like this. Hopefully, at some point, we'll have a less hackish way than the previous conda install script of dealing with this. The time is now, wheee!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/360
https://github.com/scverse/scanpy/pull/360:696,deployability,instal,install,696,"> Well, so essentially, this PR reversed what I did quite some time ago to speed up the CI... Exactly! With the crucial difference that after the first build, binary packages for everything are being cached by pip, *and* now we dont have to install conda every build. The one you linked to was just this one initial build. The real numbers are now:. &nbsp; | Runtime | Total | Link. --- | --- | --- | ---. Before (conda) | 4m47s | 8m33s | https://travis-ci.org/theislab/scanpy/builds/454438531 . After (pip) | 3m34s | 5m 2s | https://travis-ci.org/theislab/scanpy/builds/456855724. > But, let's leave it like this. Hopefully, at some point, we'll have a less hackish way than the previous conda install script of dealing with this. The time is now, wheee!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/360
https://github.com/scverse/scanpy/pull/360:166,modifiability,pac,packages,166,"> Well, so essentially, this PR reversed what I did quite some time ago to speed up the CI... Exactly! With the crucial difference that after the first build, binary packages for everything are being cached by pip, *and* now we dont have to install conda every build. The one you linked to was just this one initial build. The real numbers are now:. &nbsp; | Runtime | Total | Link. --- | --- | --- | ---. Before (conda) | 4m47s | 8m33s | https://travis-ci.org/theislab/scanpy/builds/454438531 . After (pip) | 3m34s | 5m 2s | https://travis-ci.org/theislab/scanpy/builds/456855724. > But, let's leave it like this. Hopefully, at some point, we'll have a less hackish way than the previous conda install script of dealing with this. The time is now, wheee!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/360
https://github.com/scverse/scanpy/pull/360:63,performance,time,time,63,"> Well, so essentially, this PR reversed what I did quite some time ago to speed up the CI... Exactly! With the crucial difference that after the first build, binary packages for everything are being cached by pip, *and* now we dont have to install conda every build. The one you linked to was just this one initial build. The real numbers are now:. &nbsp; | Runtime | Total | Link. --- | --- | --- | ---. Before (conda) | 4m47s | 8m33s | https://travis-ci.org/theislab/scanpy/builds/454438531 . After (pip) | 3m34s | 5m 2s | https://travis-ci.org/theislab/scanpy/builds/456855724. > But, let's leave it like this. Hopefully, at some point, we'll have a less hackish way than the previous conda install script of dealing with this. The time is now, wheee!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/360
https://github.com/scverse/scanpy/pull/360:200,performance,cach,cached,200,"> Well, so essentially, this PR reversed what I did quite some time ago to speed up the CI... Exactly! With the crucial difference that after the first build, binary packages for everything are being cached by pip, *and* now we dont have to install conda every build. The one you linked to was just this one initial build. The real numbers are now:. &nbsp; | Runtime | Total | Link. --- | --- | --- | ---. Before (conda) | 4m47s | 8m33s | https://travis-ci.org/theislab/scanpy/builds/454438531 . After (pip) | 3m34s | 5m 2s | https://travis-ci.org/theislab/scanpy/builds/456855724. > But, let's leave it like this. Hopefully, at some point, we'll have a less hackish way than the previous conda install script of dealing with this. The time is now, wheee!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/360
https://github.com/scverse/scanpy/pull/360:737,performance,time,time,737,"> Well, so essentially, this PR reversed what I did quite some time ago to speed up the CI... Exactly! With the crucial difference that after the first build, binary packages for everything are being cached by pip, *and* now we dont have to install conda every build. The one you linked to was just this one initial build. The real numbers are now:. &nbsp; | Runtime | Total | Link. --- | --- | --- | ---. Before (conda) | 4m47s | 8m33s | https://travis-ci.org/theislab/scanpy/builds/454438531 . After (pip) | 3m34s | 5m 2s | https://travis-ci.org/theislab/scanpy/builds/456855724. > But, let's leave it like this. Hopefully, at some point, we'll have a less hackish way than the previous conda install script of dealing with this. The time is now, wheee!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/360
https://github.com/scverse/scanpy/pull/360:660,security,hack,hackish,660,"> Well, so essentially, this PR reversed what I did quite some time ago to speed up the CI... Exactly! With the crucial difference that after the first build, binary packages for everything are being cached by pip, *and* now we dont have to install conda every build. The one you linked to was just this one initial build. The real numbers are now:. &nbsp; | Runtime | Total | Link. --- | --- | --- | ---. Before (conda) | 4m47s | 8m33s | https://travis-ci.org/theislab/scanpy/builds/454438531 . After (pip) | 3m34s | 5m 2s | https://travis-ci.org/theislab/scanpy/builds/456855724. > But, let's leave it like this. Hopefully, at some point, we'll have a less hackish way than the previous conda install script of dealing with this. The time is now, wheee!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/360
https://github.com/scverse/scanpy/pull/360:345,energy efficiency,cool,cool,345,"Oh, that's wonderful and exactly what I had hoped pip on the travis server would do! :smile: You mentioned that you might look into it at some point. I just didn't notice the . ```. cache: pip. ```. line in the commit... Great that you figured this out! Test times now are really nice, in particular, as I can easily speed them up further... So cool! :smile:.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/360
https://github.com/scverse/scanpy/pull/360:182,performance,cach,cache,182,"Oh, that's wonderful and exactly what I had hoped pip on the travis server would do! :smile: You mentioned that you might look into it at some point. I just didn't notice the . ```. cache: pip. ```. line in the commit... Great that you figured this out! Test times now are really nice, in particular, as I can easily speed them up further... So cool! :smile:.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/360
https://github.com/scverse/scanpy/pull/360:259,performance,time,times,259,"Oh, that's wonderful and exactly what I had hoped pip on the travis server would do! :smile: You mentioned that you might look into it at some point. I just didn't notice the . ```. cache: pip. ```. line in the commit... Great that you figured this out! Test times now are really nice, in particular, as I can easily speed them up further... So cool! :smile:.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/360
https://github.com/scverse/scanpy/pull/360:254,safety,Test,Test,254,"Oh, that's wonderful and exactly what I had hoped pip on the travis server would do! :smile: You mentioned that you might look into it at some point. I just didn't notice the . ```. cache: pip. ```. line in the commit... Great that you figured this out! Test times now are really nice, in particular, as I can easily speed them up further... So cool! :smile:.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/360
https://github.com/scverse/scanpy/pull/360:254,testability,Test,Test,254,"Oh, that's wonderful and exactly what I had hoped pip on the travis server would do! :smile: You mentioned that you might look into it at some point. I just didn't notice the . ```. cache: pip. ```. line in the commit... Great that you figured this out! Test times now are really nice, in particular, as I can easily speed them up further... So cool! :smile:.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/360
https://github.com/scverse/scanpy/pull/361:23,deployability,fail,failing,23,Think the Travis thing failing is because of the new dependencies,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361
https://github.com/scverse/scanpy/pull/361:53,deployability,depend,dependencies,53,Think the Travis thing failing is because of the new dependencies,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361
https://github.com/scverse/scanpy/pull/361:53,integrability,depend,dependencies,53,Think the Travis thing failing is because of the new dependencies,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361
https://github.com/scverse/scanpy/pull/361:53,modifiability,depend,dependencies,53,Think the Travis thing failing is because of the new dependencies,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361
https://github.com/scverse/scanpy/pull/361:23,reliability,fail,failing,23,Think the Travis thing failing is because of the new dependencies,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361
https://github.com/scverse/scanpy/pull/361:53,safety,depend,dependencies,53,Think the Travis thing failing is because of the new dependencies,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361
https://github.com/scverse/scanpy/pull/361:53,testability,depend,dependencies,53,Think the Travis thing failing is because of the new dependencies,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361
https://github.com/scverse/scanpy/pull/361:16,deployability,fail,fail,16,"The tests dont fail, but you should still add the extra to setup.py",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361
https://github.com/scverse/scanpy/pull/361:16,reliability,fail,fail,16,"The tests dont fail, but you should still add the extra to setup.py",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361
https://github.com/scverse/scanpy/pull/361:4,safety,test,tests,4,"The tests dont fail, but you should still add the extra to setup.py",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361
https://github.com/scverse/scanpy/pull/361:4,testability,test,tests,4,"The tests dont fail, but you should still add the extra to setup.py",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361
https://github.com/scverse/scanpy/pull/361:18,deployability,fail,fail,18,"> The tests dont fail, but you should still add the extra to setup.py. Is the fact that you don't list `'docs'` in your pip thing (`pip install -e .[louvain,leiden,test]`) purposeful?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361
https://github.com/scverse/scanpy/pull/361:137,deployability,instal,install,137,"> The tests dont fail, but you should still add the extra to setup.py. Is the fact that you don't list `'docs'` in your pip thing (`pip install -e .[louvain,leiden,test]`) purposeful?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361
https://github.com/scverse/scanpy/pull/361:18,reliability,fail,fail,18,"> The tests dont fail, but you should still add the extra to setup.py. Is the fact that you don't list `'docs'` in your pip thing (`pip install -e .[louvain,leiden,test]`) purposeful?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361
https://github.com/scverse/scanpy/pull/361:6,safety,test,tests,6,"> The tests dont fail, but you should still add the extra to setup.py. Is the fact that you don't list `'docs'` in your pip thing (`pip install -e .[louvain,leiden,test]`) purposeful?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361
https://github.com/scverse/scanpy/pull/361:165,safety,test,test,165,"> The tests dont fail, but you should still add the extra to setup.py. Is the fact that you don't list `'docs'` in your pip thing (`pip install -e .[louvain,leiden,test]`) purposeful?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361
https://github.com/scverse/scanpy/pull/361:6,testability,test,tests,6,"> The tests dont fail, but you should still add the extra to setup.py. Is the fact that you don't list `'docs'` in your pip thing (`pip install -e .[louvain,leiden,test]`) purposeful?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361
https://github.com/scverse/scanpy/pull/361:165,testability,test,test,165,"> The tests dont fail, but you should still add the extra to setup.py. Is the fact that you don't list `'docs'` in your pip thing (`pip install -e .[louvain,leiden,test]`) purposeful?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361
https://github.com/scverse/scanpy/pull/361:14,deployability,build,build,14,"Yes. We dont build the docs, we just check the readme. so we only need docutils and not sphinx and so on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361
https://github.com/scverse/scanpy/pull/361:58,usability,command,command,58,"Ok, I've added the thing to `setup.py`, and did the `pip` command with all non-docs extras, but apparently the travis file didn't change.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361
https://github.com/scverse/scanpy/pull/361:0,energy efficiency,Current,Currently,0,"Currently there are no tests, so those packages aren't actually needed. Looks good to me!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361
https://github.com/scverse/scanpy/pull/361:39,modifiability,pac,packages,39,"Currently there are no tests, so those packages aren't actually needed. Looks good to me!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361
https://github.com/scverse/scanpy/pull/361:23,safety,test,tests,23,"Currently there are no tests, so those packages aren't actually needed. Looks good to me!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361
https://github.com/scverse/scanpy/pull/361:23,testability,test,tests,23,"Currently there are no tests, so those packages aren't actually needed. Looks good to me!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361
https://github.com/scverse/scanpy/pull/361:23,deployability,fail,fails,23,"So apparently this now fails on the typing, on a line you provided as an example. Great. Can I please revert to how it was?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361
https://github.com/scverse/scanpy/pull/361:23,reliability,fail,fails,23,"So apparently this now fails on the typing, on a line you provided as an example. Great. Can I please revert to how it was?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361
https://github.com/scverse/scanpy/pull/361:107,testability,simpl,simply,107,"Hi! Sorry for frustrating you :( if you want I can fix and merge it manually. You're doing great work! You simply need to import the things you're using in the annotations, then it'll work!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361
https://github.com/scverse/scanpy/pull/361:107,usability,simpl,simply,107,"Hi! Sorry for frustrating you :( if you want I can fix and merge it manually. You're doing great work! You simply need to import the things you're using in the annotations, then it'll work!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361
https://github.com/scverse/scanpy/pull/361:17,usability,Help,Help,17,"Ok, I'm stumped. Help please.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361
https://github.com/scverse/scanpy/pull/361:20,modifiability,paramet,parameter,20,"just leave that one parameters annotation out for now, and Ill pull it :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361
https://github.com/scverse/scanpy/pull/361:57,performance,time,time,57,"Ok, did that. Sorry about the trouble, that was my first time trying to type stuff and I was a bit in the dark on this. Thanks for your patience.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361
https://github.com/scverse/scanpy/pull/361:262,availability,cluster,clustering-and-trajectory-inference,262,"I had to fix a few issues, if you want you can check it out: 66e64b40870035c3ee869e3baf34cf7110508d85. - I unified the parameter order with `louvain`. - I actually import it in `sc.tl`. - I added it to the docs here: https://scanpy.readthedocs.io/en/latest/api/#clustering-and-trajectory-inference. - I added a test. - I fixed the references (you had typos there: 2018 instead of 18 and a missing L). - You did this:. ```py. partition_kwargs['weights'] = None. if use_weights:. weights = np.array(g.es['weight']).astype(np.float64). # weights is never used then. ```. But I assume you meant this. Am I correct? ```py. if use_weights:. partition_kwargs['weights'] = np.array(g.es['weight']).astype(np.float64). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361
https://github.com/scverse/scanpy/pull/361:257,deployability,api,api,257,"I had to fix a few issues, if you want you can check it out: 66e64b40870035c3ee869e3baf34cf7110508d85. - I unified the parameter order with `louvain`. - I actually import it in `sc.tl`. - I added it to the docs here: https://scanpy.readthedocs.io/en/latest/api/#clustering-and-trajectory-inference. - I added a test. - I fixed the references (you had typos there: 2018 instead of 18 and a missing L). - You did this:. ```py. partition_kwargs['weights'] = None. if use_weights:. weights = np.array(g.es['weight']).astype(np.float64). # weights is never used then. ```. But I assume you meant this. Am I correct? ```py. if use_weights:. partition_kwargs['weights'] = np.array(g.es['weight']).astype(np.float64). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361
https://github.com/scverse/scanpy/pull/361:262,deployability,cluster,clustering-and-trajectory-inference,262,"I had to fix a few issues, if you want you can check it out: 66e64b40870035c3ee869e3baf34cf7110508d85. - I unified the parameter order with `louvain`. - I actually import it in `sc.tl`. - I added it to the docs here: https://scanpy.readthedocs.io/en/latest/api/#clustering-and-trajectory-inference. - I added a test. - I fixed the references (you had typos there: 2018 instead of 18 and a missing L). - You did this:. ```py. partition_kwargs['weights'] = None. if use_weights:. weights = np.array(g.es['weight']).astype(np.float64). # weights is never used then. ```. But I assume you meant this. Am I correct? ```py. if use_weights:. partition_kwargs['weights'] = np.array(g.es['weight']).astype(np.float64). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361
https://github.com/scverse/scanpy/pull/361:257,integrability,api,api,257,"I had to fix a few issues, if you want you can check it out: 66e64b40870035c3ee869e3baf34cf7110508d85. - I unified the parameter order with `louvain`. - I actually import it in `sc.tl`. - I added it to the docs here: https://scanpy.readthedocs.io/en/latest/api/#clustering-and-trajectory-inference. - I added a test. - I fixed the references (you had typos there: 2018 instead of 18 and a missing L). - You did this:. ```py. partition_kwargs['weights'] = None. if use_weights:. weights = np.array(g.es['weight']).astype(np.float64). # weights is never used then. ```. But I assume you meant this. Am I correct? ```py. if use_weights:. partition_kwargs['weights'] = np.array(g.es['weight']).astype(np.float64). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361
https://github.com/scverse/scanpy/pull/361:257,interoperability,api,api,257,"I had to fix a few issues, if you want you can check it out: 66e64b40870035c3ee869e3baf34cf7110508d85. - I unified the parameter order with `louvain`. - I actually import it in `sc.tl`. - I added it to the docs here: https://scanpy.readthedocs.io/en/latest/api/#clustering-and-trajectory-inference. - I added a test. - I fixed the references (you had typos there: 2018 instead of 18 and a missing L). - You did this:. ```py. partition_kwargs['weights'] = None. if use_weights:. weights = np.array(g.es['weight']).astype(np.float64). # weights is never used then. ```. But I assume you meant this. Am I correct? ```py. if use_weights:. partition_kwargs['weights'] = np.array(g.es['weight']).astype(np.float64). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361
https://github.com/scverse/scanpy/pull/361:119,modifiability,paramet,parameter,119,"I had to fix a few issues, if you want you can check it out: 66e64b40870035c3ee869e3baf34cf7110508d85. - I unified the parameter order with `louvain`. - I actually import it in `sc.tl`. - I added it to the docs here: https://scanpy.readthedocs.io/en/latest/api/#clustering-and-trajectory-inference. - I added a test. - I fixed the references (you had typos there: 2018 instead of 18 and a missing L). - You did this:. ```py. partition_kwargs['weights'] = None. if use_weights:. weights = np.array(g.es['weight']).astype(np.float64). # weights is never used then. ```. But I assume you meant this. Am I correct? ```py. if use_weights:. partition_kwargs['weights'] = np.array(g.es['weight']).astype(np.float64). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361
https://github.com/scverse/scanpy/pull/361:311,safety,test,test,311,"I had to fix a few issues, if you want you can check it out: 66e64b40870035c3ee869e3baf34cf7110508d85. - I unified the parameter order with `louvain`. - I actually import it in `sc.tl`. - I added it to the docs here: https://scanpy.readthedocs.io/en/latest/api/#clustering-and-trajectory-inference. - I added a test. - I fixed the references (you had typos there: 2018 instead of 18 and a missing L). - You did this:. ```py. partition_kwargs['weights'] = None. if use_weights:. weights = np.array(g.es['weight']).astype(np.float64). # weights is never used then. ```. But I assume you meant this. Am I correct? ```py. if use_weights:. partition_kwargs['weights'] = np.array(g.es['weight']).astype(np.float64). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361
https://github.com/scverse/scanpy/pull/361:311,testability,test,test,311,"I had to fix a few issues, if you want you can check it out: 66e64b40870035c3ee869e3baf34cf7110508d85. - I unified the parameter order with `louvain`. - I actually import it in `sc.tl`. - I added it to the docs here: https://scanpy.readthedocs.io/en/latest/api/#clustering-and-trajectory-inference. - I added a test. - I fixed the references (you had typos there: 2018 instead of 18 and a missing L). - You did this:. ```py. partition_kwargs['weights'] = None. if use_weights:. weights = np.array(g.es['weight']).astype(np.float64). # weights is never used then. ```. But I assume you meant this. Am I correct? ```py. if use_weights:. partition_kwargs['weights'] = np.array(g.es['weight']).astype(np.float64). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361
https://github.com/scverse/scanpy/pull/361:153,testability,hook,hook,153,"Good catch, sorry! I added the whole `partition_kwargs` dictionary late into the process and forgot to change that syntax. In that case, could you also ""hook up"" `pp.bbknn`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361
https://github.com/scverse/scanpy/pull/361:142,usability,help,help,142,"Thank you, @flying-sheep for fielding all the questions! Thank you for the PR, @ktpolanski! Is there anything still unclear where I can be of help?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361
https://github.com/scverse/scanpy/pull/361:77,deployability,api,api,77,"Well, the bbknn docs look like this: https://scanpy.readthedocs.io/en/latest/api/index.html#batch-effect-correction. I went that way since I didnt want to make it look like we coded it (with the docs hosted on our page and so on). Do you think thats a good solution or would you like it to be done differently?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361
https://github.com/scverse/scanpy/pull/361:77,integrability,api,api,77,"Well, the bbknn docs look like this: https://scanpy.readthedocs.io/en/latest/api/index.html#batch-effect-correction. I went that way since I didnt want to make it look like we coded it (with the docs hosted on our page and so on). Do you think thats a good solution or would you like it to be done differently?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361
https://github.com/scverse/scanpy/pull/361:92,integrability,batch,batch-effect-correction,92,"Well, the bbknn docs look like this: https://scanpy.readthedocs.io/en/latest/api/index.html#batch-effect-correction. I went that way since I didnt want to make it look like we coded it (with the docs hosted on our page and so on). Do you think thats a good solution or would you like it to be done differently?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361
https://github.com/scverse/scanpy/pull/361:77,interoperability,api,api,77,"Well, the bbknn docs look like this: https://scanpy.readthedocs.io/en/latest/api/index.html#batch-effect-correction. I went that way since I didnt want to make it look like we coded it (with the docs hosted on our page and so on). Do you think thats a good solution or would you like it to be done differently?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361
https://github.com/scverse/scanpy/pull/361:92,performance,batch,batch-effect-correction,92,"Well, the bbknn docs look like this: https://scanpy.readthedocs.io/en/latest/api/index.html#batch-effect-correction. I went that way since I didnt want to make it look like we coded it (with the docs hosted on our page and so on). Do you think thats a good solution or would you like it to be done differently?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361
https://github.com/scverse/scanpy/pull/361:122,usability,visual,visually,122,"I mirrored the scanpy docstring style to the best of my ability, but for some reason it turned out a little bit different visually than the ones you have. Hope that's not too much of a problem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361
https://github.com/scverse/scanpy/pull/361:50,reliability,doe,doesn,50,"Surely not! And since were linking it anyway, it doesnt need to match ours. I like that youre using ``` ``code`` ``` instead of ``` `code` ```, that looks much better than the way many many docstrings in scanpy do. I suggest that you add some lines to your intersphinx mapping and use more links instead of code though, that makes the docs easier to use, since people can just click on things:. https://github.com/theislab/scanpy/blob/7b97d0d734970527230cf7f25ab15df874a143b3/docs/conf.py#L60-L69. I also added this hack to make links to `AnnData` and `csr_matrix` work:. https://github.com/theislab/scanpy/blob/7b97d0d734970527230cf7f25ab15df874a143b3/docs/conf.py#L236-L253",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361
https://github.com/scverse/scanpy/pull/361:519,security,hack,hack,519,"Surely not! And since were linking it anyway, it doesnt need to match ours. I like that youre using ``` ``code`` ``` instead of ``` `code` ```, that looks much better than the way many many docstrings in scanpy do. I suggest that you add some lines to your intersphinx mapping and use more links instead of code though, that makes the docs easier to use, since people can just click on things:. https://github.com/theislab/scanpy/blob/7b97d0d734970527230cf7f25ab15df874a143b3/docs/conf.py#L60-L69. I also added this hack to make links to `AnnData` and `csr_matrix` work:. https://github.com/theislab/scanpy/blob/7b97d0d734970527230cf7f25ab15df874a143b3/docs/conf.py#L236-L253",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361
https://github.com/scverse/scanpy/pull/361:834,availability,state,statement,834,"```. try:. from bbknn import bbknn. except ImportError:. def bbknn(*args, **kwargs):. raise ImportError('Please install BBKNN: `pip3 install bbknn`'). ```. > I went that way since I didnt want to make it look like we coded it (with the docs hosted on our page and so on). Do you think thats a good solution or would you like it to be done differently? This is great! https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.bbknn.html#scanpy.api.pp.bbknn. But I actually don't see any docs there, I don't know why it doesn't find the original docstring... We'd like to have the reference to @ktpolanski preprint in the docstring in the first line together with a short summary of what it does and how it does it, just as for any other function. As this directly uses the implementation from @ktpolanski, we'd also want an explicit statement about that. > pp.bbknn is just an alias for bbknn.bbknn(). Refer to it for the documentation. ... can be removed. That should be evident... In the case of the `tl.leiden` wrapper, I'd also add an explicit statement that this wraps the leiden package of Traag (2018). Otherwise, all of this is great!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361
https://github.com/scverse/scanpy/pull/361:1049,availability,state,statement,1049,"```. try:. from bbknn import bbknn. except ImportError:. def bbknn(*args, **kwargs):. raise ImportError('Please install BBKNN: `pip3 install bbknn`'). ```. > I went that way since I didnt want to make it look like we coded it (with the docs hosted on our page and so on). Do you think thats a good solution or would you like it to be done differently? This is great! https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.bbknn.html#scanpy.api.pp.bbknn. But I actually don't see any docs there, I don't know why it doesn't find the original docstring... We'd like to have the reference to @ktpolanski preprint in the docstring in the first line together with a short summary of what it does and how it does it, just as for any other function. As this directly uses the implementation from @ktpolanski, we'd also want an explicit statement about that. > pp.bbknn is just an alias for bbknn.bbknn(). Refer to it for the documentation. ... can be removed. That should be evident... In the case of the `tl.leiden` wrapper, I'd also add an explicit statement that this wraps the leiden package of Traag (2018). Otherwise, all of this is great!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361
https://github.com/scverse/scanpy/pull/361:112,deployability,instal,install,112,"```. try:. from bbknn import bbknn. except ImportError:. def bbknn(*args, **kwargs):. raise ImportError('Please install BBKNN: `pip3 install bbknn`'). ```. > I went that way since I didnt want to make it look like we coded it (with the docs hosted on our page and so on). Do you think thats a good solution or would you like it to be done differently? This is great! https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.bbknn.html#scanpy.api.pp.bbknn. But I actually don't see any docs there, I don't know why it doesn't find the original docstring... We'd like to have the reference to @ktpolanski preprint in the docstring in the first line together with a short summary of what it does and how it does it, just as for any other function. As this directly uses the implementation from @ktpolanski, we'd also want an explicit statement about that. > pp.bbknn is just an alias for bbknn.bbknn(). Refer to it for the documentation. ... can be removed. That should be evident... In the case of the `tl.leiden` wrapper, I'd also add an explicit statement that this wraps the leiden package of Traag (2018). Otherwise, all of this is great!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361
https://github.com/scverse/scanpy/pull/361:133,deployability,instal,install,133,"```. try:. from bbknn import bbknn. except ImportError:. def bbknn(*args, **kwargs):. raise ImportError('Please install BBKNN: `pip3 install bbknn`'). ```. > I went that way since I didnt want to make it look like we coded it (with the docs hosted on our page and so on). Do you think thats a good solution or would you like it to be done differently? This is great! https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.bbknn.html#scanpy.api.pp.bbknn. But I actually don't see any docs there, I don't know why it doesn't find the original docstring... We'd like to have the reference to @ktpolanski preprint in the docstring in the first line together with a short summary of what it does and how it does it, just as for any other function. As this directly uses the implementation from @ktpolanski, we'd also want an explicit statement about that. > pp.bbknn is just an alias for bbknn.bbknn(). Refer to it for the documentation. ... can be removed. That should be evident... In the case of the `tl.leiden` wrapper, I'd also add an explicit statement that this wraps the leiden package of Traag (2018). Otherwise, all of this is great!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361
https://github.com/scverse/scanpy/pull/361:409,deployability,api,api,409,"```. try:. from bbknn import bbknn. except ImportError:. def bbknn(*args, **kwargs):. raise ImportError('Please install BBKNN: `pip3 install bbknn`'). ```. > I went that way since I didnt want to make it look like we coded it (with the docs hosted on our page and so on). Do you think thats a good solution or would you like it to be done differently? This is great! https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.bbknn.html#scanpy.api.pp.bbknn. But I actually don't see any docs there, I don't know why it doesn't find the original docstring... We'd like to have the reference to @ktpolanski preprint in the docstring in the first line together with a short summary of what it does and how it does it, just as for any other function. As this directly uses the implementation from @ktpolanski, we'd also want an explicit statement about that. > pp.bbknn is just an alias for bbknn.bbknn(). Refer to it for the documentation. ... can be removed. That should be evident... In the case of the `tl.leiden` wrapper, I'd also add an explicit statement that this wraps the leiden package of Traag (2018). Otherwise, all of this is great!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361
https://github.com/scverse/scanpy/pull/361:420,deployability,api,api,420,"```. try:. from bbknn import bbknn. except ImportError:. def bbknn(*args, **kwargs):. raise ImportError('Please install BBKNN: `pip3 install bbknn`'). ```. > I went that way since I didnt want to make it look like we coded it (with the docs hosted on our page and so on). Do you think thats a good solution or would you like it to be done differently? This is great! https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.bbknn.html#scanpy.api.pp.bbknn. But I actually don't see any docs there, I don't know why it doesn't find the original docstring... We'd like to have the reference to @ktpolanski preprint in the docstring in the first line together with a short summary of what it does and how it does it, just as for any other function. As this directly uses the implementation from @ktpolanski, we'd also want an explicit statement about that. > pp.bbknn is just an alias for bbknn.bbknn(). Refer to it for the documentation. ... can be removed. That should be evident... In the case of the `tl.leiden` wrapper, I'd also add an explicit statement that this wraps the leiden package of Traag (2018). Otherwise, all of this is great!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361
https://github.com/scverse/scanpy/pull/361:445,deployability,api,api,445,"```. try:. from bbknn import bbknn. except ImportError:. def bbknn(*args, **kwargs):. raise ImportError('Please install BBKNN: `pip3 install bbknn`'). ```. > I went that way since I didnt want to make it look like we coded it (with the docs hosted on our page and so on). Do you think thats a good solution or would you like it to be done differently? This is great! https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.bbknn.html#scanpy.api.pp.bbknn. But I actually don't see any docs there, I don't know why it doesn't find the original docstring... We'd like to have the reference to @ktpolanski preprint in the docstring in the first line together with a short summary of what it does and how it does it, just as for any other function. As this directly uses the implementation from @ktpolanski, we'd also want an explicit statement about that. > pp.bbknn is just an alias for bbknn.bbknn(). Refer to it for the documentation. ... can be removed. That should be evident... In the case of the `tl.leiden` wrapper, I'd also add an explicit statement that this wraps the leiden package of Traag (2018). Otherwise, all of this is great!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361
https://github.com/scverse/scanpy/pull/361:409,integrability,api,api,409,"```. try:. from bbknn import bbknn. except ImportError:. def bbknn(*args, **kwargs):. raise ImportError('Please install BBKNN: `pip3 install bbknn`'). ```. > I went that way since I didnt want to make it look like we coded it (with the docs hosted on our page and so on). Do you think thats a good solution or would you like it to be done differently? This is great! https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.bbknn.html#scanpy.api.pp.bbknn. But I actually don't see any docs there, I don't know why it doesn't find the original docstring... We'd like to have the reference to @ktpolanski preprint in the docstring in the first line together with a short summary of what it does and how it does it, just as for any other function. As this directly uses the implementation from @ktpolanski, we'd also want an explicit statement about that. > pp.bbknn is just an alias for bbknn.bbknn(). Refer to it for the documentation. ... can be removed. That should be evident... In the case of the `tl.leiden` wrapper, I'd also add an explicit statement that this wraps the leiden package of Traag (2018). Otherwise, all of this is great!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361
https://github.com/scverse/scanpy/pull/361:420,integrability,api,api,420,"```. try:. from bbknn import bbknn. except ImportError:. def bbknn(*args, **kwargs):. raise ImportError('Please install BBKNN: `pip3 install bbknn`'). ```. > I went that way since I didnt want to make it look like we coded it (with the docs hosted on our page and so on). Do you think thats a good solution or would you like it to be done differently? This is great! https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.bbknn.html#scanpy.api.pp.bbknn. But I actually don't see any docs there, I don't know why it doesn't find the original docstring... We'd like to have the reference to @ktpolanski preprint in the docstring in the first line together with a short summary of what it does and how it does it, just as for any other function. As this directly uses the implementation from @ktpolanski, we'd also want an explicit statement about that. > pp.bbknn is just an alias for bbknn.bbknn(). Refer to it for the documentation. ... can be removed. That should be evident... In the case of the `tl.leiden` wrapper, I'd also add an explicit statement that this wraps the leiden package of Traag (2018). Otherwise, all of this is great!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361
https://github.com/scverse/scanpy/pull/361:445,integrability,api,api,445,"```. try:. from bbknn import bbknn. except ImportError:. def bbknn(*args, **kwargs):. raise ImportError('Please install BBKNN: `pip3 install bbknn`'). ```. > I went that way since I didnt want to make it look like we coded it (with the docs hosted on our page and so on). Do you think thats a good solution or would you like it to be done differently? This is great! https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.bbknn.html#scanpy.api.pp.bbknn. But I actually don't see any docs there, I don't know why it doesn't find the original docstring... We'd like to have the reference to @ktpolanski preprint in the docstring in the first line together with a short summary of what it does and how it does it, just as for any other function. As this directly uses the implementation from @ktpolanski, we'd also want an explicit statement about that. > pp.bbknn is just an alias for bbknn.bbknn(). Refer to it for the documentation. ... can be removed. That should be evident... In the case of the `tl.leiden` wrapper, I'd also add an explicit statement that this wraps the leiden package of Traag (2018). Otherwise, all of this is great!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361
https://github.com/scverse/scanpy/pull/361:834,integrability,state,statement,834,"```. try:. from bbknn import bbknn. except ImportError:. def bbknn(*args, **kwargs):. raise ImportError('Please install BBKNN: `pip3 install bbknn`'). ```. > I went that way since I didnt want to make it look like we coded it (with the docs hosted on our page and so on). Do you think thats a good solution or would you like it to be done differently? This is great! https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.bbknn.html#scanpy.api.pp.bbknn. But I actually don't see any docs there, I don't know why it doesn't find the original docstring... We'd like to have the reference to @ktpolanski preprint in the docstring in the first line together with a short summary of what it does and how it does it, just as for any other function. As this directly uses the implementation from @ktpolanski, we'd also want an explicit statement about that. > pp.bbknn is just an alias for bbknn.bbknn(). Refer to it for the documentation. ... can be removed. That should be evident... In the case of the `tl.leiden` wrapper, I'd also add an explicit statement that this wraps the leiden package of Traag (2018). Otherwise, all of this is great!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361
https://github.com/scverse/scanpy/pull/361:1015,integrability,wrap,wrapper,1015,"```. try:. from bbknn import bbknn. except ImportError:. def bbknn(*args, **kwargs):. raise ImportError('Please install BBKNN: `pip3 install bbknn`'). ```. > I went that way since I didnt want to make it look like we coded it (with the docs hosted on our page and so on). Do you think thats a good solution or would you like it to be done differently? This is great! https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.bbknn.html#scanpy.api.pp.bbknn. But I actually don't see any docs there, I don't know why it doesn't find the original docstring... We'd like to have the reference to @ktpolanski preprint in the docstring in the first line together with a short summary of what it does and how it does it, just as for any other function. As this directly uses the implementation from @ktpolanski, we'd also want an explicit statement about that. > pp.bbknn is just an alias for bbknn.bbknn(). Refer to it for the documentation. ... can be removed. That should be evident... In the case of the `tl.leiden` wrapper, I'd also add an explicit statement that this wraps the leiden package of Traag (2018). Otherwise, all of this is great!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361
https://github.com/scverse/scanpy/pull/361:1049,integrability,state,statement,1049,"```. try:. from bbknn import bbknn. except ImportError:. def bbknn(*args, **kwargs):. raise ImportError('Please install BBKNN: `pip3 install bbknn`'). ```. > I went that way since I didnt want to make it look like we coded it (with the docs hosted on our page and so on). Do you think thats a good solution or would you like it to be done differently? This is great! https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.bbknn.html#scanpy.api.pp.bbknn. But I actually don't see any docs there, I don't know why it doesn't find the original docstring... We'd like to have the reference to @ktpolanski preprint in the docstring in the first line together with a short summary of what it does and how it does it, just as for any other function. As this directly uses the implementation from @ktpolanski, we'd also want an explicit statement about that. > pp.bbknn is just an alias for bbknn.bbknn(). Refer to it for the documentation. ... can be removed. That should be evident... In the case of the `tl.leiden` wrapper, I'd also add an explicit statement that this wraps the leiden package of Traag (2018). Otherwise, all of this is great!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361
https://github.com/scverse/scanpy/pull/361:1069,integrability,wrap,wraps,1069,"```. try:. from bbknn import bbknn. except ImportError:. def bbknn(*args, **kwargs):. raise ImportError('Please install BBKNN: `pip3 install bbknn`'). ```. > I went that way since I didnt want to make it look like we coded it (with the docs hosted on our page and so on). Do you think thats a good solution or would you like it to be done differently? This is great! https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.bbknn.html#scanpy.api.pp.bbknn. But I actually don't see any docs there, I don't know why it doesn't find the original docstring... We'd like to have the reference to @ktpolanski preprint in the docstring in the first line together with a short summary of what it does and how it does it, just as for any other function. As this directly uses the implementation from @ktpolanski, we'd also want an explicit statement about that. > pp.bbknn is just an alias for bbknn.bbknn(). Refer to it for the documentation. ... can be removed. That should be evident... In the case of the `tl.leiden` wrapper, I'd also add an explicit statement that this wraps the leiden package of Traag (2018). Otherwise, all of this is great!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361
https://github.com/scverse/scanpy/pull/361:409,interoperability,api,api,409,"```. try:. from bbknn import bbknn. except ImportError:. def bbknn(*args, **kwargs):. raise ImportError('Please install BBKNN: `pip3 install bbknn`'). ```. > I went that way since I didnt want to make it look like we coded it (with the docs hosted on our page and so on). Do you think thats a good solution or would you like it to be done differently? This is great! https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.bbknn.html#scanpy.api.pp.bbknn. But I actually don't see any docs there, I don't know why it doesn't find the original docstring... We'd like to have the reference to @ktpolanski preprint in the docstring in the first line together with a short summary of what it does and how it does it, just as for any other function. As this directly uses the implementation from @ktpolanski, we'd also want an explicit statement about that. > pp.bbknn is just an alias for bbknn.bbknn(). Refer to it for the documentation. ... can be removed. That should be evident... In the case of the `tl.leiden` wrapper, I'd also add an explicit statement that this wraps the leiden package of Traag (2018). Otherwise, all of this is great!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361
https://github.com/scverse/scanpy/pull/361:420,interoperability,api,api,420,"```. try:. from bbknn import bbknn. except ImportError:. def bbknn(*args, **kwargs):. raise ImportError('Please install BBKNN: `pip3 install bbknn`'). ```. > I went that way since I didnt want to make it look like we coded it (with the docs hosted on our page and so on). Do you think thats a good solution or would you like it to be done differently? This is great! https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.bbknn.html#scanpy.api.pp.bbknn. But I actually don't see any docs there, I don't know why it doesn't find the original docstring... We'd like to have the reference to @ktpolanski preprint in the docstring in the first line together with a short summary of what it does and how it does it, just as for any other function. As this directly uses the implementation from @ktpolanski, we'd also want an explicit statement about that. > pp.bbknn is just an alias for bbknn.bbknn(). Refer to it for the documentation. ... can be removed. That should be evident... In the case of the `tl.leiden` wrapper, I'd also add an explicit statement that this wraps the leiden package of Traag (2018). Otherwise, all of this is great!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361
https://github.com/scverse/scanpy/pull/361:445,interoperability,api,api,445,"```. try:. from bbknn import bbknn. except ImportError:. def bbknn(*args, **kwargs):. raise ImportError('Please install BBKNN: `pip3 install bbknn`'). ```. > I went that way since I didnt want to make it look like we coded it (with the docs hosted on our page and so on). Do you think thats a good solution or would you like it to be done differently? This is great! https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.bbknn.html#scanpy.api.pp.bbknn. But I actually don't see any docs there, I don't know why it doesn't find the original docstring... We'd like to have the reference to @ktpolanski preprint in the docstring in the first line together with a short summary of what it does and how it does it, just as for any other function. As this directly uses the implementation from @ktpolanski, we'd also want an explicit statement about that. > pp.bbknn is just an alias for bbknn.bbknn(). Refer to it for the documentation. ... can be removed. That should be evident... In the case of the `tl.leiden` wrapper, I'd also add an explicit statement that this wraps the leiden package of Traag (2018). Otherwise, all of this is great!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361
https://github.com/scverse/scanpy/pull/361:1015,interoperability,wrapper,wrapper,1015,"```. try:. from bbknn import bbknn. except ImportError:. def bbknn(*args, **kwargs):. raise ImportError('Please install BBKNN: `pip3 install bbknn`'). ```. > I went that way since I didnt want to make it look like we coded it (with the docs hosted on our page and so on). Do you think thats a good solution or would you like it to be done differently? This is great! https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.bbknn.html#scanpy.api.pp.bbknn. But I actually don't see any docs there, I don't know why it doesn't find the original docstring... We'd like to have the reference to @ktpolanski preprint in the docstring in the first line together with a short summary of what it does and how it does it, just as for any other function. As this directly uses the implementation from @ktpolanski, we'd also want an explicit statement about that. > pp.bbknn is just an alias for bbknn.bbknn(). Refer to it for the documentation. ... can be removed. That should be evident... In the case of the `tl.leiden` wrapper, I'd also add an explicit statement that this wraps the leiden package of Traag (2018). Otherwise, all of this is great!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361
https://github.com/scverse/scanpy/pull/361:1086,modifiability,pac,package,1086,"```. try:. from bbknn import bbknn. except ImportError:. def bbknn(*args, **kwargs):. raise ImportError('Please install BBKNN: `pip3 install bbknn`'). ```. > I went that way since I didnt want to make it look like we coded it (with the docs hosted on our page and so on). Do you think thats a good solution or would you like it to be done differently? This is great! https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.bbknn.html#scanpy.api.pp.bbknn. But I actually don't see any docs there, I don't know why it doesn't find the original docstring... We'd like to have the reference to @ktpolanski preprint in the docstring in the first line together with a short summary of what it does and how it does it, just as for any other function. As this directly uses the implementation from @ktpolanski, we'd also want an explicit statement about that. > pp.bbknn is just an alias for bbknn.bbknn(). Refer to it for the documentation. ... can be removed. That should be evident... In the case of the `tl.leiden` wrapper, I'd also add an explicit statement that this wraps the leiden package of Traag (2018). Otherwise, all of this is great!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361
https://github.com/scverse/scanpy/pull/361:520,reliability,doe,doesn,520,"```. try:. from bbknn import bbknn. except ImportError:. def bbknn(*args, **kwargs):. raise ImportError('Please install BBKNN: `pip3 install bbknn`'). ```. > I went that way since I didnt want to make it look like we coded it (with the docs hosted on our page and so on). Do you think thats a good solution or would you like it to be done differently? This is great! https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.bbknn.html#scanpy.api.pp.bbknn. But I actually don't see any docs there, I don't know why it doesn't find the original docstring... We'd like to have the reference to @ktpolanski preprint in the docstring in the first line together with a short summary of what it does and how it does it, just as for any other function. As this directly uses the implementation from @ktpolanski, we'd also want an explicit statement about that. > pp.bbknn is just an alias for bbknn.bbknn(). Refer to it for the documentation. ... can be removed. That should be evident... In the case of the `tl.leiden` wrapper, I'd also add an explicit statement that this wraps the leiden package of Traag (2018). Otherwise, all of this is great!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361
https://github.com/scverse/scanpy/pull/361:691,reliability,doe,does,691,"```. try:. from bbknn import bbknn. except ImportError:. def bbknn(*args, **kwargs):. raise ImportError('Please install BBKNN: `pip3 install bbknn`'). ```. > I went that way since I didnt want to make it look like we coded it (with the docs hosted on our page and so on). Do you think thats a good solution or would you like it to be done differently? This is great! https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.bbknn.html#scanpy.api.pp.bbknn. But I actually don't see any docs there, I don't know why it doesn't find the original docstring... We'd like to have the reference to @ktpolanski preprint in the docstring in the first line together with a short summary of what it does and how it does it, just as for any other function. As this directly uses the implementation from @ktpolanski, we'd also want an explicit statement about that. > pp.bbknn is just an alias for bbknn.bbknn(). Refer to it for the documentation. ... can be removed. That should be evident... In the case of the `tl.leiden` wrapper, I'd also add an explicit statement that this wraps the leiden package of Traag (2018). Otherwise, all of this is great!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361
https://github.com/scverse/scanpy/pull/361:707,reliability,doe,does,707,"```. try:. from bbknn import bbknn. except ImportError:. def bbknn(*args, **kwargs):. raise ImportError('Please install BBKNN: `pip3 install bbknn`'). ```. > I went that way since I didnt want to make it look like we coded it (with the docs hosted on our page and so on). Do you think thats a good solution or would you like it to be done differently? This is great! https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.bbknn.html#scanpy.api.pp.bbknn. But I actually don't see any docs there, I don't know why it doesn't find the original docstring... We'd like to have the reference to @ktpolanski preprint in the docstring in the first line together with a short summary of what it does and how it does it, just as for any other function. As this directly uses the implementation from @ktpolanski, we'd also want an explicit statement about that. > pp.bbknn is just an alias for bbknn.bbknn(). Refer to it for the documentation. ... can be removed. That should be evident... In the case of the `tl.leiden` wrapper, I'd also add an explicit statement that this wraps the leiden package of Traag (2018). Otherwise, all of this is great!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361
https://github.com/scverse/scanpy/pull/361:36,safety,except,except,36,"```. try:. from bbknn import bbknn. except ImportError:. def bbknn(*args, **kwargs):. raise ImportError('Please install BBKNN: `pip3 install bbknn`'). ```. > I went that way since I didnt want to make it look like we coded it (with the docs hosted on our page and so on). Do you think thats a good solution or would you like it to be done differently? This is great! https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.bbknn.html#scanpy.api.pp.bbknn. But I actually don't see any docs there, I don't know why it doesn't find the original docstring... We'd like to have the reference to @ktpolanski preprint in the docstring in the first line together with a short summary of what it does and how it does it, just as for any other function. As this directly uses the implementation from @ktpolanski, we'd also want an explicit statement about that. > pp.bbknn is just an alias for bbknn.bbknn(). Refer to it for the documentation. ... can be removed. That should be evident... In the case of the `tl.leiden` wrapper, I'd also add an explicit statement that this wraps the leiden package of Traag (2018). Otherwise, all of this is great!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361
https://github.com/scverse/scanpy/pull/361:923,usability,document,documentation,923,"```. try:. from bbknn import bbknn. except ImportError:. def bbknn(*args, **kwargs):. raise ImportError('Please install BBKNN: `pip3 install bbknn`'). ```. > I went that way since I didnt want to make it look like we coded it (with the docs hosted on our page and so on). Do you think thats a good solution or would you like it to be done differently? This is great! https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.bbknn.html#scanpy.api.pp.bbknn. But I actually don't see any docs there, I don't know why it doesn't find the original docstring... We'd like to have the reference to @ktpolanski preprint in the docstring in the first line together with a short summary of what it does and how it does it, just as for any other function. As this directly uses the implementation from @ktpolanski, we'd also want an explicit statement about that. > pp.bbknn is just an alias for bbknn.bbknn(). Refer to it for the documentation. ... can be removed. That should be evident... In the case of the `tl.leiden` wrapper, I'd also add an explicit statement that this wraps the leiden package of Traag (2018). Otherwise, all of this is great!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361
https://github.com/scverse/scanpy/pull/361:145,deployability,instal,installed,145,"> But I actually don't see any docs there, I don't know why it doesn't find the original docstring... Thats because on readthedocs, bbknn isnt installed, so it uses the dummy version. > We'd like to have the reference to @ktpolanski preprint in the docstring in the first line together with a short summary of what it does and how it does it, just as for any other function. We could do that either by adding a docstring to the dummy version that links to https://bbknn.rtfd.io or by installing bbknn on rtd, and modifying the docstring programmatically. something like:. ```py. try:. from bbknn import bbknn. first_para, rest = bbknn.__doc__.split('\n\n', 1). bbknn.__doc__ =. '{}\n\nFor a graphical explanation, visit '. '`The bbknn project <https://github.com/Teichlab/bbknn>`__-\n\n{}'. .format(first_para, rest). except ImportError:. ...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361
https://github.com/scverse/scanpy/pull/361:177,deployability,version,version,177,"> But I actually don't see any docs there, I don't know why it doesn't find the original docstring... Thats because on readthedocs, bbknn isnt installed, so it uses the dummy version. > We'd like to have the reference to @ktpolanski preprint in the docstring in the first line together with a short summary of what it does and how it does it, just as for any other function. We could do that either by adding a docstring to the dummy version that links to https://bbknn.rtfd.io or by installing bbknn on rtd, and modifying the docstring programmatically. something like:. ```py. try:. from bbknn import bbknn. first_para, rest = bbknn.__doc__.split('\n\n', 1). bbknn.__doc__ =. '{}\n\nFor a graphical explanation, visit '. '`The bbknn project <https://github.com/Teichlab/bbknn>`__-\n\n{}'. .format(first_para, rest). except ImportError:. ...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361
https://github.com/scverse/scanpy/pull/361:436,deployability,version,version,436,"> But I actually don't see any docs there, I don't know why it doesn't find the original docstring... Thats because on readthedocs, bbknn isnt installed, so it uses the dummy version. > We'd like to have the reference to @ktpolanski preprint in the docstring in the first line together with a short summary of what it does and how it does it, just as for any other function. We could do that either by adding a docstring to the dummy version that links to https://bbknn.rtfd.io or by installing bbknn on rtd, and modifying the docstring programmatically. something like:. ```py. try:. from bbknn import bbknn. first_para, rest = bbknn.__doc__.split('\n\n', 1). bbknn.__doc__ =. '{}\n\nFor a graphical explanation, visit '. '`The bbknn project <https://github.com/Teichlab/bbknn>`__-\n\n{}'. .format(first_para, rest). except ImportError:. ...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361
https://github.com/scverse/scanpy/pull/361:486,deployability,instal,installing,486,"> But I actually don't see any docs there, I don't know why it doesn't find the original docstring... Thats because on readthedocs, bbknn isnt installed, so it uses the dummy version. > We'd like to have the reference to @ktpolanski preprint in the docstring in the first line together with a short summary of what it does and how it does it, just as for any other function. We could do that either by adding a docstring to the dummy version that links to https://bbknn.rtfd.io or by installing bbknn on rtd, and modifying the docstring programmatically. something like:. ```py. try:. from bbknn import bbknn. first_para, rest = bbknn.__doc__.split('\n\n', 1). bbknn.__doc__ =. '{}\n\nFor a graphical explanation, visit '. '`The bbknn project <https://github.com/Teichlab/bbknn>`__-\n\n{}'. .format(first_para, rest). except ImportError:. ...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361
https://github.com/scverse/scanpy/pull/361:177,integrability,version,version,177,"> But I actually don't see any docs there, I don't know why it doesn't find the original docstring... Thats because on readthedocs, bbknn isnt installed, so it uses the dummy version. > We'd like to have the reference to @ktpolanski preprint in the docstring in the first line together with a short summary of what it does and how it does it, just as for any other function. We could do that either by adding a docstring to the dummy version that links to https://bbknn.rtfd.io or by installing bbknn on rtd, and modifying the docstring programmatically. something like:. ```py. try:. from bbknn import bbknn. first_para, rest = bbknn.__doc__.split('\n\n', 1). bbknn.__doc__ =. '{}\n\nFor a graphical explanation, visit '. '`The bbknn project <https://github.com/Teichlab/bbknn>`__-\n\n{}'. .format(first_para, rest). except ImportError:. ...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361
https://github.com/scverse/scanpy/pull/361:436,integrability,version,version,436,"> But I actually don't see any docs there, I don't know why it doesn't find the original docstring... Thats because on readthedocs, bbknn isnt installed, so it uses the dummy version. > We'd like to have the reference to @ktpolanski preprint in the docstring in the first line together with a short summary of what it does and how it does it, just as for any other function. We could do that either by adding a docstring to the dummy version that links to https://bbknn.rtfd.io or by installing bbknn on rtd, and modifying the docstring programmatically. something like:. ```py. try:. from bbknn import bbknn. first_para, rest = bbknn.__doc__.split('\n\n', 1). bbknn.__doc__ =. '{}\n\nFor a graphical explanation, visit '. '`The bbknn project <https://github.com/Teichlab/bbknn>`__-\n\n{}'. .format(first_para, rest). except ImportError:. ...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361
https://github.com/scverse/scanpy/pull/361:794,interoperability,format,format,794,"> But I actually don't see any docs there, I don't know why it doesn't find the original docstring... Thats because on readthedocs, bbknn isnt installed, so it uses the dummy version. > We'd like to have the reference to @ktpolanski preprint in the docstring in the first line together with a short summary of what it does and how it does it, just as for any other function. We could do that either by adding a docstring to the dummy version that links to https://bbknn.rtfd.io or by installing bbknn on rtd, and modifying the docstring programmatically. something like:. ```py. try:. from bbknn import bbknn. first_para, rest = bbknn.__doc__.split('\n\n', 1). bbknn.__doc__ =. '{}\n\nFor a graphical explanation, visit '. '`The bbknn project <https://github.com/Teichlab/bbknn>`__-\n\n{}'. .format(first_para, rest). except ImportError:. ...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361
https://github.com/scverse/scanpy/pull/361:177,modifiability,version,version,177,"> But I actually don't see any docs there, I don't know why it doesn't find the original docstring... Thats because on readthedocs, bbknn isnt installed, so it uses the dummy version. > We'd like to have the reference to @ktpolanski preprint in the docstring in the first line together with a short summary of what it does and how it does it, just as for any other function. We could do that either by adding a docstring to the dummy version that links to https://bbknn.rtfd.io or by installing bbknn on rtd, and modifying the docstring programmatically. something like:. ```py. try:. from bbknn import bbknn. first_para, rest = bbknn.__doc__.split('\n\n', 1). bbknn.__doc__ =. '{}\n\nFor a graphical explanation, visit '. '`The bbknn project <https://github.com/Teichlab/bbknn>`__-\n\n{}'. .format(first_para, rest). except ImportError:. ...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361
https://github.com/scverse/scanpy/pull/361:436,modifiability,version,version,436,"> But I actually don't see any docs there, I don't know why it doesn't find the original docstring... Thats because on readthedocs, bbknn isnt installed, so it uses the dummy version. > We'd like to have the reference to @ktpolanski preprint in the docstring in the first line together with a short summary of what it does and how it does it, just as for any other function. We could do that either by adding a docstring to the dummy version that links to https://bbknn.rtfd.io or by installing bbknn on rtd, and modifying the docstring programmatically. something like:. ```py. try:. from bbknn import bbknn. first_para, rest = bbknn.__doc__.split('\n\n', 1). bbknn.__doc__ =. '{}\n\nFor a graphical explanation, visit '. '`The bbknn project <https://github.com/Teichlab/bbknn>`__-\n\n{}'. .format(first_para, rest). except ImportError:. ...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361
https://github.com/scverse/scanpy/pull/361:63,reliability,doe,doesn,63,"> But I actually don't see any docs there, I don't know why it doesn't find the original docstring... Thats because on readthedocs, bbknn isnt installed, so it uses the dummy version. > We'd like to have the reference to @ktpolanski preprint in the docstring in the first line together with a short summary of what it does and how it does it, just as for any other function. We could do that either by adding a docstring to the dummy version that links to https://bbknn.rtfd.io or by installing bbknn on rtd, and modifying the docstring programmatically. something like:. ```py. try:. from bbknn import bbknn. first_para, rest = bbknn.__doc__.split('\n\n', 1). bbknn.__doc__ =. '{}\n\nFor a graphical explanation, visit '. '`The bbknn project <https://github.com/Teichlab/bbknn>`__-\n\n{}'. .format(first_para, rest). except ImportError:. ...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361
https://github.com/scverse/scanpy/pull/361:320,reliability,doe,does,320,"> But I actually don't see any docs there, I don't know why it doesn't find the original docstring... Thats because on readthedocs, bbknn isnt installed, so it uses the dummy version. > We'd like to have the reference to @ktpolanski preprint in the docstring in the first line together with a short summary of what it does and how it does it, just as for any other function. We could do that either by adding a docstring to the dummy version that links to https://bbknn.rtfd.io or by installing bbknn on rtd, and modifying the docstring programmatically. something like:. ```py. try:. from bbknn import bbknn. first_para, rest = bbknn.__doc__.split('\n\n', 1). bbknn.__doc__ =. '{}\n\nFor a graphical explanation, visit '. '`The bbknn project <https://github.com/Teichlab/bbknn>`__-\n\n{}'. .format(first_para, rest). except ImportError:. ...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361
https://github.com/scverse/scanpy/pull/361:336,reliability,doe,does,336,"> But I actually don't see any docs there, I don't know why it doesn't find the original docstring... Thats because on readthedocs, bbknn isnt installed, so it uses the dummy version. > We'd like to have the reference to @ktpolanski preprint in the docstring in the first line together with a short summary of what it does and how it does it, just as for any other function. We could do that either by adding a docstring to the dummy version that links to https://bbknn.rtfd.io or by installing bbknn on rtd, and modifying the docstring programmatically. something like:. ```py. try:. from bbknn import bbknn. first_para, rest = bbknn.__doc__.split('\n\n', 1). bbknn.__doc__ =. '{}\n\nFor a graphical explanation, visit '. '`The bbknn project <https://github.com/Teichlab/bbknn>`__-\n\n{}'. .format(first_para, rest). except ImportError:. ...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361
https://github.com/scverse/scanpy/pull/361:820,safety,except,except,820,"> But I actually don't see any docs there, I don't know why it doesn't find the original docstring... Thats because on readthedocs, bbknn isnt installed, so it uses the dummy version. > We'd like to have the reference to @ktpolanski preprint in the docstring in the first line together with a short summary of what it does and how it does it, just as for any other function. We could do that either by adding a docstring to the dummy version that links to https://bbknn.rtfd.io or by installing bbknn on rtd, and modifying the docstring programmatically. something like:. ```py. try:. from bbknn import bbknn. first_para, rest = bbknn.__doc__.split('\n\n', 1). bbknn.__doc__ =. '{}\n\nFor a graphical explanation, visit '. '`The bbknn project <https://github.com/Teichlab/bbknn>`__-\n\n{}'. .format(first_para, rest). except ImportError:. ...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361
https://github.com/scverse/scanpy/pull/361:515,security,modif,modifying,515,"> But I actually don't see any docs there, I don't know why it doesn't find the original docstring... Thats because on readthedocs, bbknn isnt installed, so it uses the dummy version. > We'd like to have the reference to @ktpolanski preprint in the docstring in the first line together with a short summary of what it does and how it does it, just as for any other function. We could do that either by adding a docstring to the dummy version that links to https://bbknn.rtfd.io or by installing bbknn on rtd, and modifying the docstring programmatically. something like:. ```py. try:. from bbknn import bbknn. first_para, rest = bbknn.__doc__.split('\n\n', 1). bbknn.__doc__ =. '{}\n\nFor a graphical explanation, visit '. '`The bbknn project <https://github.com/Teichlab/bbknn>`__-\n\n{}'. .format(first_para, rest). except ImportError:. ...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361
https://github.com/scverse/scanpy/issues/362:1044,deployability,version,version,1044,"It is possible to do what you want. Here is a working example:. ```PYTHON. from scanpy.plotting.anndata import _plot_dendrogram, _compute_dendrogram. adata = sc.datasets.pbmc68k_reduced(). res = _compute_dendrogram(adata, 'bulk_labels', use_raw=False, cor_method='pearson', linkage_method='ward'). fig, dendro_ax = plt.subplots(1, 1). _plot_dendrogram(dendro_ax, adata, orientation='top', remove_labels=False). labels = [adata.obs.bulk_labels.cat.categories[x] for x in adata.uns['dendrogram']['categories_idx_ordered']]. ```. ![image](https://user-images.githubusercontent.com/4964309/48540327-180f0a00-e8ba-11e8-8ff9-5817278834c0.png). If this is useful for other people we may consider adding a dedicated dendrogram function that follows the scanpy convention of a tool to compute the dendrogram and a tool to plot the dendrogram. Furthermore, the current dendrogram uses either all genes in adata.var_names (or adata.raw.var_names), then averages the values by the chosen category and computes the correlation matrix. I think that Seurat's version has a different approach which we could also implement if someone is familiar on how it works.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/362
https://github.com/scverse/scanpy/issues/362:851,energy efficiency,current,current,851,"It is possible to do what you want. Here is a working example:. ```PYTHON. from scanpy.plotting.anndata import _plot_dendrogram, _compute_dendrogram. adata = sc.datasets.pbmc68k_reduced(). res = _compute_dendrogram(adata, 'bulk_labels', use_raw=False, cor_method='pearson', linkage_method='ward'). fig, dendro_ax = plt.subplots(1, 1). _plot_dendrogram(dendro_ax, adata, orientation='top', remove_labels=False). labels = [adata.obs.bulk_labels.cat.categories[x] for x in adata.uns['dendrogram']['categories_idx_ordered']]. ```. ![image](https://user-images.githubusercontent.com/4964309/48540327-180f0a00-e8ba-11e8-8ff9-5817278834c0.png). If this is useful for other people we may consider adding a dedicated dendrogram function that follows the scanpy convention of a tool to compute the dendrogram and a tool to plot the dendrogram. Furthermore, the current dendrogram uses either all genes in adata.var_names (or adata.raw.var_names), then averages the values by the chosen category and computes the correlation matrix. I think that Seurat's version has a different approach which we could also implement if someone is familiar on how it works.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/362
https://github.com/scverse/scanpy/issues/362:319,integrability,sub,subplots,319,"It is possible to do what you want. Here is a working example:. ```PYTHON. from scanpy.plotting.anndata import _plot_dendrogram, _compute_dendrogram. adata = sc.datasets.pbmc68k_reduced(). res = _compute_dendrogram(adata, 'bulk_labels', use_raw=False, cor_method='pearson', linkage_method='ward'). fig, dendro_ax = plt.subplots(1, 1). _plot_dendrogram(dendro_ax, adata, orientation='top', remove_labels=False). labels = [adata.obs.bulk_labels.cat.categories[x] for x in adata.uns['dendrogram']['categories_idx_ordered']]. ```. ![image](https://user-images.githubusercontent.com/4964309/48540327-180f0a00-e8ba-11e8-8ff9-5817278834c0.png). If this is useful for other people we may consider adding a dedicated dendrogram function that follows the scanpy convention of a tool to compute the dendrogram and a tool to plot the dendrogram. Furthermore, the current dendrogram uses either all genes in adata.var_names (or adata.raw.var_names), then averages the values by the chosen category and computes the correlation matrix. I think that Seurat's version has a different approach which we could also implement if someone is familiar on how it works.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/362
https://github.com/scverse/scanpy/issues/362:1044,integrability,version,version,1044,"It is possible to do what you want. Here is a working example:. ```PYTHON. from scanpy.plotting.anndata import _plot_dendrogram, _compute_dendrogram. adata = sc.datasets.pbmc68k_reduced(). res = _compute_dendrogram(adata, 'bulk_labels', use_raw=False, cor_method='pearson', linkage_method='ward'). fig, dendro_ax = plt.subplots(1, 1). _plot_dendrogram(dendro_ax, adata, orientation='top', remove_labels=False). labels = [adata.obs.bulk_labels.cat.categories[x] for x in adata.uns['dendrogram']['categories_idx_ordered']]. ```. ![image](https://user-images.githubusercontent.com/4964309/48540327-180f0a00-e8ba-11e8-8ff9-5817278834c0.png). If this is useful for other people we may consider adding a dedicated dendrogram function that follows the scanpy convention of a tool to compute the dendrogram and a tool to plot the dendrogram. Furthermore, the current dendrogram uses either all genes in adata.var_names (or adata.raw.var_names), then averages the values by the chosen category and computes the correlation matrix. I think that Seurat's version has a different approach which we could also implement if someone is familiar on how it works.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/362
https://github.com/scverse/scanpy/issues/362:1044,modifiability,version,version,1044,"It is possible to do what you want. Here is a working example:. ```PYTHON. from scanpy.plotting.anndata import _plot_dendrogram, _compute_dendrogram. adata = sc.datasets.pbmc68k_reduced(). res = _compute_dendrogram(adata, 'bulk_labels', use_raw=False, cor_method='pearson', linkage_method='ward'). fig, dendro_ax = plt.subplots(1, 1). _plot_dendrogram(dendro_ax, adata, orientation='top', remove_labels=False). labels = [adata.obs.bulk_labels.cat.categories[x] for x in adata.uns['dendrogram']['categories_idx_ordered']]. ```. ![image](https://user-images.githubusercontent.com/4964309/48540327-180f0a00-e8ba-11e8-8ff9-5817278834c0.png). If this is useful for other people we may consider adding a dedicated dendrogram function that follows the scanpy convention of a tool to compute the dendrogram and a tool to plot the dendrogram. Furthermore, the current dendrogram uses either all genes in adata.var_names (or adata.raw.var_names), then averages the values by the chosen category and computes the correlation matrix. I think that Seurat's version has a different approach which we could also implement if someone is familiar on how it works.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/362
https://github.com/scverse/scanpy/issues/362:544,usability,user,user-images,544,"It is possible to do what you want. Here is a working example:. ```PYTHON. from scanpy.plotting.anndata import _plot_dendrogram, _compute_dendrogram. adata = sc.datasets.pbmc68k_reduced(). res = _compute_dendrogram(adata, 'bulk_labels', use_raw=False, cor_method='pearson', linkage_method='ward'). fig, dendro_ax = plt.subplots(1, 1). _plot_dendrogram(dendro_ax, adata, orientation='top', remove_labels=False). labels = [adata.obs.bulk_labels.cat.categories[x] for x in adata.uns['dendrogram']['categories_idx_ordered']]. ```. ![image](https://user-images.githubusercontent.com/4964309/48540327-180f0a00-e8ba-11e8-8ff9-5817278834c0.png). If this is useful for other people we may consider adding a dedicated dendrogram function that follows the scanpy convention of a tool to compute the dendrogram and a tool to plot the dendrogram. Furthermore, the current dendrogram uses either all genes in adata.var_names (or adata.raw.var_names), then averages the values by the chosen category and computes the correlation matrix. I think that Seurat's version has a different approach which we could also implement if someone is familiar on how it works.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/362
https://github.com/scverse/scanpy/issues/362:768,usability,tool,tool,768,"It is possible to do what you want. Here is a working example:. ```PYTHON. from scanpy.plotting.anndata import _plot_dendrogram, _compute_dendrogram. adata = sc.datasets.pbmc68k_reduced(). res = _compute_dendrogram(adata, 'bulk_labels', use_raw=False, cor_method='pearson', linkage_method='ward'). fig, dendro_ax = plt.subplots(1, 1). _plot_dendrogram(dendro_ax, adata, orientation='top', remove_labels=False). labels = [adata.obs.bulk_labels.cat.categories[x] for x in adata.uns['dendrogram']['categories_idx_ordered']]. ```. ![image](https://user-images.githubusercontent.com/4964309/48540327-180f0a00-e8ba-11e8-8ff9-5817278834c0.png). If this is useful for other people we may consider adding a dedicated dendrogram function that follows the scanpy convention of a tool to compute the dendrogram and a tool to plot the dendrogram. Furthermore, the current dendrogram uses either all genes in adata.var_names (or adata.raw.var_names), then averages the values by the chosen category and computes the correlation matrix. I think that Seurat's version has a different approach which we could also implement if someone is familiar on how it works.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/362
https://github.com/scverse/scanpy/issues/362:805,usability,tool,tool,805,"It is possible to do what you want. Here is a working example:. ```PYTHON. from scanpy.plotting.anndata import _plot_dendrogram, _compute_dendrogram. adata = sc.datasets.pbmc68k_reduced(). res = _compute_dendrogram(adata, 'bulk_labels', use_raw=False, cor_method='pearson', linkage_method='ward'). fig, dendro_ax = plt.subplots(1, 1). _plot_dendrogram(dendro_ax, adata, orientation='top', remove_labels=False). labels = [adata.obs.bulk_labels.cat.categories[x] for x in adata.uns['dendrogram']['categories_idx_ordered']]. ```. ![image](https://user-images.githubusercontent.com/4964309/48540327-180f0a00-e8ba-11e8-8ff9-5817278834c0.png). If this is useful for other people we may consider adding a dedicated dendrogram function that follows the scanpy convention of a tool to compute the dendrogram and a tool to plot the dendrogram. Furthermore, the current dendrogram uses either all genes in adata.var_names (or adata.raw.var_names), then averages the values by the chosen category and computes the correlation matrix. I think that Seurat's version has a different approach which we could also implement if someone is familiar on how it works.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/362
https://github.com/scverse/scanpy/issues/362:141,availability,cluster,cluster,141,"That is a wonderful solution! I will give it a shot shortly. Thank you so much for your help! Here's some example code of how Seurat handles cluster scoring and merging with random forests and OOBE:. ```. pbmc <- ValidateClusters(pbmc, pc.use = 1:30, top.genes = 30). pbmc <- BuildClusterTree(pbmc, . do.reorder = T, . reorder.numeric = T). node.scores <- AssessNodes(pbmc). node.scores[order(node.scores$oobe,decreasing = T),] -> node.scores. nodes.merge <- node.scores[which(node.scores[,2] > 0.1),]. nodes.to.merge <- sort(nodes.merge$node) . pbmc.merged <- pbmc. for (n in nodes.to.merge). {. pbmc.merged <- MergeNode(pbmc.merged, n). }. ```. Here's an explanation, as this code was derived from this recent (and awesome) publication:. . From page 6 of the Supplementary Methods of Plass et al 2018: http://science.sciencemag.org/content/early/2018/04/18/science.aaq1723. To prevent obtaining spurious clusters result of overclustering, the robustness of the clusters was calculated using the function AssessNodes from Seurat. For each cluster, the average expression of all variable genes (4910) is computed and a phylogenetic tree based on the distance matrix in gene expression space is computed. Next, it computes an Out of Bag Error for a random forest classifier trained on each internal node split of the tree. We recursively build a tree and assessed all its nodes, merging all clusters with an out of bag error bigger than 0.1 until no such nodes were found.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/362
https://github.com/scverse/scanpy/issues/362:906,availability,cluster,clusters,906,"That is a wonderful solution! I will give it a shot shortly. Thank you so much for your help! Here's some example code of how Seurat handles cluster scoring and merging with random forests and OOBE:. ```. pbmc <- ValidateClusters(pbmc, pc.use = 1:30, top.genes = 30). pbmc <- BuildClusterTree(pbmc, . do.reorder = T, . reorder.numeric = T). node.scores <- AssessNodes(pbmc). node.scores[order(node.scores$oobe,decreasing = T),] -> node.scores. nodes.merge <- node.scores[which(node.scores[,2] > 0.1),]. nodes.to.merge <- sort(nodes.merge$node) . pbmc.merged <- pbmc. for (n in nodes.to.merge). {. pbmc.merged <- MergeNode(pbmc.merged, n). }. ```. Here's an explanation, as this code was derived from this recent (and awesome) publication:. . From page 6 of the Supplementary Methods of Plass et al 2018: http://science.sciencemag.org/content/early/2018/04/18/science.aaq1723. To prevent obtaining spurious clusters result of overclustering, the robustness of the clusters was calculated using the function AssessNodes from Seurat. For each cluster, the average expression of all variable genes (4910) is computed and a phylogenetic tree based on the distance matrix in gene expression space is computed. Next, it computes an Out of Bag Error for a random forest classifier trained on each internal node split of the tree. We recursively build a tree and assessed all its nodes, merging all clusters with an out of bag error bigger than 0.1 until no such nodes were found.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/362
https://github.com/scverse/scanpy/issues/362:945,availability,robust,robustness,945,"That is a wonderful solution! I will give it a shot shortly. Thank you so much for your help! Here's some example code of how Seurat handles cluster scoring and merging with random forests and OOBE:. ```. pbmc <- ValidateClusters(pbmc, pc.use = 1:30, top.genes = 30). pbmc <- BuildClusterTree(pbmc, . do.reorder = T, . reorder.numeric = T). node.scores <- AssessNodes(pbmc). node.scores[order(node.scores$oobe,decreasing = T),] -> node.scores. nodes.merge <- node.scores[which(node.scores[,2] > 0.1),]. nodes.to.merge <- sort(nodes.merge$node) . pbmc.merged <- pbmc. for (n in nodes.to.merge). {. pbmc.merged <- MergeNode(pbmc.merged, n). }. ```. Here's an explanation, as this code was derived from this recent (and awesome) publication:. . From page 6 of the Supplementary Methods of Plass et al 2018: http://science.sciencemag.org/content/early/2018/04/18/science.aaq1723. To prevent obtaining spurious clusters result of overclustering, the robustness of the clusters was calculated using the function AssessNodes from Seurat. For each cluster, the average expression of all variable genes (4910) is computed and a phylogenetic tree based on the distance matrix in gene expression space is computed. Next, it computes an Out of Bag Error for a random forest classifier trained on each internal node split of the tree. We recursively build a tree and assessed all its nodes, merging all clusters with an out of bag error bigger than 0.1 until no such nodes were found.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/362
https://github.com/scverse/scanpy/issues/362:963,availability,cluster,clusters,963,"That is a wonderful solution! I will give it a shot shortly. Thank you so much for your help! Here's some example code of how Seurat handles cluster scoring and merging with random forests and OOBE:. ```. pbmc <- ValidateClusters(pbmc, pc.use = 1:30, top.genes = 30). pbmc <- BuildClusterTree(pbmc, . do.reorder = T, . reorder.numeric = T). node.scores <- AssessNodes(pbmc). node.scores[order(node.scores$oobe,decreasing = T),] -> node.scores. nodes.merge <- node.scores[which(node.scores[,2] > 0.1),]. nodes.to.merge <- sort(nodes.merge$node) . pbmc.merged <- pbmc. for (n in nodes.to.merge). {. pbmc.merged <- MergeNode(pbmc.merged, n). }. ```. Here's an explanation, as this code was derived from this recent (and awesome) publication:. . From page 6 of the Supplementary Methods of Plass et al 2018: http://science.sciencemag.org/content/early/2018/04/18/science.aaq1723. To prevent obtaining spurious clusters result of overclustering, the robustness of the clusters was calculated using the function AssessNodes from Seurat. For each cluster, the average expression of all variable genes (4910) is computed and a phylogenetic tree based on the distance matrix in gene expression space is computed. Next, it computes an Out of Bag Error for a random forest classifier trained on each internal node split of the tree. We recursively build a tree and assessed all its nodes, merging all clusters with an out of bag error bigger than 0.1 until no such nodes were found.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/362
https://github.com/scverse/scanpy/issues/362:1040,availability,cluster,cluster,1040,"That is a wonderful solution! I will give it a shot shortly. Thank you so much for your help! Here's some example code of how Seurat handles cluster scoring and merging with random forests and OOBE:. ```. pbmc <- ValidateClusters(pbmc, pc.use = 1:30, top.genes = 30). pbmc <- BuildClusterTree(pbmc, . do.reorder = T, . reorder.numeric = T). node.scores <- AssessNodes(pbmc). node.scores[order(node.scores$oobe,decreasing = T),] -> node.scores. nodes.merge <- node.scores[which(node.scores[,2] > 0.1),]. nodes.to.merge <- sort(nodes.merge$node) . pbmc.merged <- pbmc. for (n in nodes.to.merge). {. pbmc.merged <- MergeNode(pbmc.merged, n). }. ```. Here's an explanation, as this code was derived from this recent (and awesome) publication:. . From page 6 of the Supplementary Methods of Plass et al 2018: http://science.sciencemag.org/content/early/2018/04/18/science.aaq1723. To prevent obtaining spurious clusters result of overclustering, the robustness of the clusters was calculated using the function AssessNodes from Seurat. For each cluster, the average expression of all variable genes (4910) is computed and a phylogenetic tree based on the distance matrix in gene expression space is computed. Next, it computes an Out of Bag Error for a random forest classifier trained on each internal node split of the tree. We recursively build a tree and assessed all its nodes, merging all clusters with an out of bag error bigger than 0.1 until no such nodes were found.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/362
https://github.com/scverse/scanpy/issues/362:1236,availability,Error,Error,1236,"That is a wonderful solution! I will give it a shot shortly. Thank you so much for your help! Here's some example code of how Seurat handles cluster scoring and merging with random forests and OOBE:. ```. pbmc <- ValidateClusters(pbmc, pc.use = 1:30, top.genes = 30). pbmc <- BuildClusterTree(pbmc, . do.reorder = T, . reorder.numeric = T). node.scores <- AssessNodes(pbmc). node.scores[order(node.scores$oobe,decreasing = T),] -> node.scores. nodes.merge <- node.scores[which(node.scores[,2] > 0.1),]. nodes.to.merge <- sort(nodes.merge$node) . pbmc.merged <- pbmc. for (n in nodes.to.merge). {. pbmc.merged <- MergeNode(pbmc.merged, n). }. ```. Here's an explanation, as this code was derived from this recent (and awesome) publication:. . From page 6 of the Supplementary Methods of Plass et al 2018: http://science.sciencemag.org/content/early/2018/04/18/science.aaq1723. To prevent obtaining spurious clusters result of overclustering, the robustness of the clusters was calculated using the function AssessNodes from Seurat. For each cluster, the average expression of all variable genes (4910) is computed and a phylogenetic tree based on the distance matrix in gene expression space is computed. Next, it computes an Out of Bag Error for a random forest classifier trained on each internal node split of the tree. We recursively build a tree and assessed all its nodes, merging all clusters with an out of bag error bigger than 0.1 until no such nodes were found.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/362
https://github.com/scverse/scanpy/issues/362:1390,availability,cluster,clusters,1390,"That is a wonderful solution! I will give it a shot shortly. Thank you so much for your help! Here's some example code of how Seurat handles cluster scoring and merging with random forests and OOBE:. ```. pbmc <- ValidateClusters(pbmc, pc.use = 1:30, top.genes = 30). pbmc <- BuildClusterTree(pbmc, . do.reorder = T, . reorder.numeric = T). node.scores <- AssessNodes(pbmc). node.scores[order(node.scores$oobe,decreasing = T),] -> node.scores. nodes.merge <- node.scores[which(node.scores[,2] > 0.1),]. nodes.to.merge <- sort(nodes.merge$node) . pbmc.merged <- pbmc. for (n in nodes.to.merge). {. pbmc.merged <- MergeNode(pbmc.merged, n). }. ```. Here's an explanation, as this code was derived from this recent (and awesome) publication:. . From page 6 of the Supplementary Methods of Plass et al 2018: http://science.sciencemag.org/content/early/2018/04/18/science.aaq1723. To prevent obtaining spurious clusters result of overclustering, the robustness of the clusters was calculated using the function AssessNodes from Seurat. For each cluster, the average expression of all variable genes (4910) is computed and a phylogenetic tree based on the distance matrix in gene expression space is computed. Next, it computes an Out of Bag Error for a random forest classifier trained on each internal node split of the tree. We recursively build a tree and assessed all its nodes, merging all clusters with an out of bag error bigger than 0.1 until no such nodes were found.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/362
https://github.com/scverse/scanpy/issues/362:1418,availability,error,error,1418,"That is a wonderful solution! I will give it a shot shortly. Thank you so much for your help! Here's some example code of how Seurat handles cluster scoring and merging with random forests and OOBE:. ```. pbmc <- ValidateClusters(pbmc, pc.use = 1:30, top.genes = 30). pbmc <- BuildClusterTree(pbmc, . do.reorder = T, . reorder.numeric = T). node.scores <- AssessNodes(pbmc). node.scores[order(node.scores$oobe,decreasing = T),] -> node.scores. nodes.merge <- node.scores[which(node.scores[,2] > 0.1),]. nodes.to.merge <- sort(nodes.merge$node) . pbmc.merged <- pbmc. for (n in nodes.to.merge). {. pbmc.merged <- MergeNode(pbmc.merged, n). }. ```. Here's an explanation, as this code was derived from this recent (and awesome) publication:. . From page 6 of the Supplementary Methods of Plass et al 2018: http://science.sciencemag.org/content/early/2018/04/18/science.aaq1723. To prevent obtaining spurious clusters result of overclustering, the robustness of the clusters was calculated using the function AssessNodes from Seurat. For each cluster, the average expression of all variable genes (4910) is computed and a phylogenetic tree based on the distance matrix in gene expression space is computed. Next, it computes an Out of Bag Error for a random forest classifier trained on each internal node split of the tree. We recursively build a tree and assessed all its nodes, merging all clusters with an out of bag error bigger than 0.1 until no such nodes were found.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/362
https://github.com/scverse/scanpy/issues/362:141,deployability,cluster,cluster,141,"That is a wonderful solution! I will give it a shot shortly. Thank you so much for your help! Here's some example code of how Seurat handles cluster scoring and merging with random forests and OOBE:. ```. pbmc <- ValidateClusters(pbmc, pc.use = 1:30, top.genes = 30). pbmc <- BuildClusterTree(pbmc, . do.reorder = T, . reorder.numeric = T). node.scores <- AssessNodes(pbmc). node.scores[order(node.scores$oobe,decreasing = T),] -> node.scores. nodes.merge <- node.scores[which(node.scores[,2] > 0.1),]. nodes.to.merge <- sort(nodes.merge$node) . pbmc.merged <- pbmc. for (n in nodes.to.merge). {. pbmc.merged <- MergeNode(pbmc.merged, n). }. ```. Here's an explanation, as this code was derived from this recent (and awesome) publication:. . From page 6 of the Supplementary Methods of Plass et al 2018: http://science.sciencemag.org/content/early/2018/04/18/science.aaq1723. To prevent obtaining spurious clusters result of overclustering, the robustness of the clusters was calculated using the function AssessNodes from Seurat. For each cluster, the average expression of all variable genes (4910) is computed and a phylogenetic tree based on the distance matrix in gene expression space is computed. Next, it computes an Out of Bag Error for a random forest classifier trained on each internal node split of the tree. We recursively build a tree and assessed all its nodes, merging all clusters with an out of bag error bigger than 0.1 until no such nodes were found.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/362
https://github.com/scverse/scanpy/issues/362:276,deployability,Build,BuildClusterTree,276,"That is a wonderful solution! I will give it a shot shortly. Thank you so much for your help! Here's some example code of how Seurat handles cluster scoring and merging with random forests and OOBE:. ```. pbmc <- ValidateClusters(pbmc, pc.use = 1:30, top.genes = 30). pbmc <- BuildClusterTree(pbmc, . do.reorder = T, . reorder.numeric = T). node.scores <- AssessNodes(pbmc). node.scores[order(node.scores$oobe,decreasing = T),] -> node.scores. nodes.merge <- node.scores[which(node.scores[,2] > 0.1),]. nodes.to.merge <- sort(nodes.merge$node) . pbmc.merged <- pbmc. for (n in nodes.to.merge). {. pbmc.merged <- MergeNode(pbmc.merged, n). }. ```. Here's an explanation, as this code was derived from this recent (and awesome) publication:. . From page 6 of the Supplementary Methods of Plass et al 2018: http://science.sciencemag.org/content/early/2018/04/18/science.aaq1723. To prevent obtaining spurious clusters result of overclustering, the robustness of the clusters was calculated using the function AssessNodes from Seurat. For each cluster, the average expression of all variable genes (4910) is computed and a phylogenetic tree based on the distance matrix in gene expression space is computed. Next, it computes an Out of Bag Error for a random forest classifier trained on each internal node split of the tree. We recursively build a tree and assessed all its nodes, merging all clusters with an out of bag error bigger than 0.1 until no such nodes were found.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/362
https://github.com/scverse/scanpy/issues/362:906,deployability,cluster,clusters,906,"That is a wonderful solution! I will give it a shot shortly. Thank you so much for your help! Here's some example code of how Seurat handles cluster scoring and merging with random forests and OOBE:. ```. pbmc <- ValidateClusters(pbmc, pc.use = 1:30, top.genes = 30). pbmc <- BuildClusterTree(pbmc, . do.reorder = T, . reorder.numeric = T). node.scores <- AssessNodes(pbmc). node.scores[order(node.scores$oobe,decreasing = T),] -> node.scores. nodes.merge <- node.scores[which(node.scores[,2] > 0.1),]. nodes.to.merge <- sort(nodes.merge$node) . pbmc.merged <- pbmc. for (n in nodes.to.merge). {. pbmc.merged <- MergeNode(pbmc.merged, n). }. ```. Here's an explanation, as this code was derived from this recent (and awesome) publication:. . From page 6 of the Supplementary Methods of Plass et al 2018: http://science.sciencemag.org/content/early/2018/04/18/science.aaq1723. To prevent obtaining spurious clusters result of overclustering, the robustness of the clusters was calculated using the function AssessNodes from Seurat. For each cluster, the average expression of all variable genes (4910) is computed and a phylogenetic tree based on the distance matrix in gene expression space is computed. Next, it computes an Out of Bag Error for a random forest classifier trained on each internal node split of the tree. We recursively build a tree and assessed all its nodes, merging all clusters with an out of bag error bigger than 0.1 until no such nodes were found.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/362
https://github.com/scverse/scanpy/issues/362:963,deployability,cluster,clusters,963,"That is a wonderful solution! I will give it a shot shortly. Thank you so much for your help! Here's some example code of how Seurat handles cluster scoring and merging with random forests and OOBE:. ```. pbmc <- ValidateClusters(pbmc, pc.use = 1:30, top.genes = 30). pbmc <- BuildClusterTree(pbmc, . do.reorder = T, . reorder.numeric = T). node.scores <- AssessNodes(pbmc). node.scores[order(node.scores$oobe,decreasing = T),] -> node.scores. nodes.merge <- node.scores[which(node.scores[,2] > 0.1),]. nodes.to.merge <- sort(nodes.merge$node) . pbmc.merged <- pbmc. for (n in nodes.to.merge). {. pbmc.merged <- MergeNode(pbmc.merged, n). }. ```. Here's an explanation, as this code was derived from this recent (and awesome) publication:. . From page 6 of the Supplementary Methods of Plass et al 2018: http://science.sciencemag.org/content/early/2018/04/18/science.aaq1723. To prevent obtaining spurious clusters result of overclustering, the robustness of the clusters was calculated using the function AssessNodes from Seurat. For each cluster, the average expression of all variable genes (4910) is computed and a phylogenetic tree based on the distance matrix in gene expression space is computed. Next, it computes an Out of Bag Error for a random forest classifier trained on each internal node split of the tree. We recursively build a tree and assessed all its nodes, merging all clusters with an out of bag error bigger than 0.1 until no such nodes were found.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/362
https://github.com/scverse/scanpy/issues/362:1040,deployability,cluster,cluster,1040,"That is a wonderful solution! I will give it a shot shortly. Thank you so much for your help! Here's some example code of how Seurat handles cluster scoring and merging with random forests and OOBE:. ```. pbmc <- ValidateClusters(pbmc, pc.use = 1:30, top.genes = 30). pbmc <- BuildClusterTree(pbmc, . do.reorder = T, . reorder.numeric = T). node.scores <- AssessNodes(pbmc). node.scores[order(node.scores$oobe,decreasing = T),] -> node.scores. nodes.merge <- node.scores[which(node.scores[,2] > 0.1),]. nodes.to.merge <- sort(nodes.merge$node) . pbmc.merged <- pbmc. for (n in nodes.to.merge). {. pbmc.merged <- MergeNode(pbmc.merged, n). }. ```. Here's an explanation, as this code was derived from this recent (and awesome) publication:. . From page 6 of the Supplementary Methods of Plass et al 2018: http://science.sciencemag.org/content/early/2018/04/18/science.aaq1723. To prevent obtaining spurious clusters result of overclustering, the robustness of the clusters was calculated using the function AssessNodes from Seurat. For each cluster, the average expression of all variable genes (4910) is computed and a phylogenetic tree based on the distance matrix in gene expression space is computed. Next, it computes an Out of Bag Error for a random forest classifier trained on each internal node split of the tree. We recursively build a tree and assessed all its nodes, merging all clusters with an out of bag error bigger than 0.1 until no such nodes were found.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/362
https://github.com/scverse/scanpy/issues/362:1337,deployability,build,build,1337,"That is a wonderful solution! I will give it a shot shortly. Thank you so much for your help! Here's some example code of how Seurat handles cluster scoring and merging with random forests and OOBE:. ```. pbmc <- ValidateClusters(pbmc, pc.use = 1:30, top.genes = 30). pbmc <- BuildClusterTree(pbmc, . do.reorder = T, . reorder.numeric = T). node.scores <- AssessNodes(pbmc). node.scores[order(node.scores$oobe,decreasing = T),] -> node.scores. nodes.merge <- node.scores[which(node.scores[,2] > 0.1),]. nodes.to.merge <- sort(nodes.merge$node) . pbmc.merged <- pbmc. for (n in nodes.to.merge). {. pbmc.merged <- MergeNode(pbmc.merged, n). }. ```. Here's an explanation, as this code was derived from this recent (and awesome) publication:. . From page 6 of the Supplementary Methods of Plass et al 2018: http://science.sciencemag.org/content/early/2018/04/18/science.aaq1723. To prevent obtaining spurious clusters result of overclustering, the robustness of the clusters was calculated using the function AssessNodes from Seurat. For each cluster, the average expression of all variable genes (4910) is computed and a phylogenetic tree based on the distance matrix in gene expression space is computed. Next, it computes an Out of Bag Error for a random forest classifier trained on each internal node split of the tree. We recursively build a tree and assessed all its nodes, merging all clusters with an out of bag error bigger than 0.1 until no such nodes were found.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/362
https://github.com/scverse/scanpy/issues/362:1390,deployability,cluster,clusters,1390,"That is a wonderful solution! I will give it a shot shortly. Thank you so much for your help! Here's some example code of how Seurat handles cluster scoring and merging with random forests and OOBE:. ```. pbmc <- ValidateClusters(pbmc, pc.use = 1:30, top.genes = 30). pbmc <- BuildClusterTree(pbmc, . do.reorder = T, . reorder.numeric = T). node.scores <- AssessNodes(pbmc). node.scores[order(node.scores$oobe,decreasing = T),] -> node.scores. nodes.merge <- node.scores[which(node.scores[,2] > 0.1),]. nodes.to.merge <- sort(nodes.merge$node) . pbmc.merged <- pbmc. for (n in nodes.to.merge). {. pbmc.merged <- MergeNode(pbmc.merged, n). }. ```. Here's an explanation, as this code was derived from this recent (and awesome) publication:. . From page 6 of the Supplementary Methods of Plass et al 2018: http://science.sciencemag.org/content/early/2018/04/18/science.aaq1723. To prevent obtaining spurious clusters result of overclustering, the robustness of the clusters was calculated using the function AssessNodes from Seurat. For each cluster, the average expression of all variable genes (4910) is computed and a phylogenetic tree based on the distance matrix in gene expression space is computed. Next, it computes an Out of Bag Error for a random forest classifier trained on each internal node split of the tree. We recursively build a tree and assessed all its nodes, merging all clusters with an out of bag error bigger than 0.1 until no such nodes were found.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/362
https://github.com/scverse/scanpy/issues/362:726,integrability,pub,publication,726,"That is a wonderful solution! I will give it a shot shortly. Thank you so much for your help! Here's some example code of how Seurat handles cluster scoring and merging with random forests and OOBE:. ```. pbmc <- ValidateClusters(pbmc, pc.use = 1:30, top.genes = 30). pbmc <- BuildClusterTree(pbmc, . do.reorder = T, . reorder.numeric = T). node.scores <- AssessNodes(pbmc). node.scores[order(node.scores$oobe,decreasing = T),] -> node.scores. nodes.merge <- node.scores[which(node.scores[,2] > 0.1),]. nodes.to.merge <- sort(nodes.merge$node) . pbmc.merged <- pbmc. for (n in nodes.to.merge). {. pbmc.merged <- MergeNode(pbmc.merged, n). }. ```. Here's an explanation, as this code was derived from this recent (and awesome) publication:. . From page 6 of the Supplementary Methods of Plass et al 2018: http://science.sciencemag.org/content/early/2018/04/18/science.aaq1723. To prevent obtaining spurious clusters result of overclustering, the robustness of the clusters was calculated using the function AssessNodes from Seurat. For each cluster, the average expression of all variable genes (4910) is computed and a phylogenetic tree based on the distance matrix in gene expression space is computed. Next, it computes an Out of Bag Error for a random forest classifier trained on each internal node split of the tree. We recursively build a tree and assessed all its nodes, merging all clusters with an out of bag error bigger than 0.1 until no such nodes were found.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/362
https://github.com/scverse/scanpy/issues/362:1079,modifiability,variab,variable,1079,"That is a wonderful solution! I will give it a shot shortly. Thank you so much for your help! Here's some example code of how Seurat handles cluster scoring and merging with random forests and OOBE:. ```. pbmc <- ValidateClusters(pbmc, pc.use = 1:30, top.genes = 30). pbmc <- BuildClusterTree(pbmc, . do.reorder = T, . reorder.numeric = T). node.scores <- AssessNodes(pbmc). node.scores[order(node.scores$oobe,decreasing = T),] -> node.scores. nodes.merge <- node.scores[which(node.scores[,2] > 0.1),]. nodes.to.merge <- sort(nodes.merge$node) . pbmc.merged <- pbmc. for (n in nodes.to.merge). {. pbmc.merged <- MergeNode(pbmc.merged, n). }. ```. Here's an explanation, as this code was derived from this recent (and awesome) publication:. . From page 6 of the Supplementary Methods of Plass et al 2018: http://science.sciencemag.org/content/early/2018/04/18/science.aaq1723. To prevent obtaining spurious clusters result of overclustering, the robustness of the clusters was calculated using the function AssessNodes from Seurat. For each cluster, the average expression of all variable genes (4910) is computed and a phylogenetic tree based on the distance matrix in gene expression space is computed. Next, it computes an Out of Bag Error for a random forest classifier trained on each internal node split of the tree. We recursively build a tree and assessed all its nodes, merging all clusters with an out of bag error bigger than 0.1 until no such nodes were found.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/362
https://github.com/scverse/scanpy/issues/362:834,performance,content,content,834,"That is a wonderful solution! I will give it a shot shortly. Thank you so much for your help! Here's some example code of how Seurat handles cluster scoring and merging with random forests and OOBE:. ```. pbmc <- ValidateClusters(pbmc, pc.use = 1:30, top.genes = 30). pbmc <- BuildClusterTree(pbmc, . do.reorder = T, . reorder.numeric = T). node.scores <- AssessNodes(pbmc). node.scores[order(node.scores$oobe,decreasing = T),] -> node.scores. nodes.merge <- node.scores[which(node.scores[,2] > 0.1),]. nodes.to.merge <- sort(nodes.merge$node) . pbmc.merged <- pbmc. for (n in nodes.to.merge). {. pbmc.merged <- MergeNode(pbmc.merged, n). }. ```. Here's an explanation, as this code was derived from this recent (and awesome) publication:. . From page 6 of the Supplementary Methods of Plass et al 2018: http://science.sciencemag.org/content/early/2018/04/18/science.aaq1723. To prevent obtaining spurious clusters result of overclustering, the robustness of the clusters was calculated using the function AssessNodes from Seurat. For each cluster, the average expression of all variable genes (4910) is computed and a phylogenetic tree based on the distance matrix in gene expression space is computed. Next, it computes an Out of Bag Error for a random forest classifier trained on each internal node split of the tree. We recursively build a tree and assessed all its nodes, merging all clusters with an out of bag error bigger than 0.1 until no such nodes were found.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/362
https://github.com/scverse/scanpy/issues/362:1236,performance,Error,Error,1236,"That is a wonderful solution! I will give it a shot shortly. Thank you so much for your help! Here's some example code of how Seurat handles cluster scoring and merging with random forests and OOBE:. ```. pbmc <- ValidateClusters(pbmc, pc.use = 1:30, top.genes = 30). pbmc <- BuildClusterTree(pbmc, . do.reorder = T, . reorder.numeric = T). node.scores <- AssessNodes(pbmc). node.scores[order(node.scores$oobe,decreasing = T),] -> node.scores. nodes.merge <- node.scores[which(node.scores[,2] > 0.1),]. nodes.to.merge <- sort(nodes.merge$node) . pbmc.merged <- pbmc. for (n in nodes.to.merge). {. pbmc.merged <- MergeNode(pbmc.merged, n). }. ```. Here's an explanation, as this code was derived from this recent (and awesome) publication:. . From page 6 of the Supplementary Methods of Plass et al 2018: http://science.sciencemag.org/content/early/2018/04/18/science.aaq1723. To prevent obtaining spurious clusters result of overclustering, the robustness of the clusters was calculated using the function AssessNodes from Seurat. For each cluster, the average expression of all variable genes (4910) is computed and a phylogenetic tree based on the distance matrix in gene expression space is computed. Next, it computes an Out of Bag Error for a random forest classifier trained on each internal node split of the tree. We recursively build a tree and assessed all its nodes, merging all clusters with an out of bag error bigger than 0.1 until no such nodes were found.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/362
https://github.com/scverse/scanpy/issues/362:1418,performance,error,error,1418,"That is a wonderful solution! I will give it a shot shortly. Thank you so much for your help! Here's some example code of how Seurat handles cluster scoring and merging with random forests and OOBE:. ```. pbmc <- ValidateClusters(pbmc, pc.use = 1:30, top.genes = 30). pbmc <- BuildClusterTree(pbmc, . do.reorder = T, . reorder.numeric = T). node.scores <- AssessNodes(pbmc). node.scores[order(node.scores$oobe,decreasing = T),] -> node.scores. nodes.merge <- node.scores[which(node.scores[,2] > 0.1),]. nodes.to.merge <- sort(nodes.merge$node) . pbmc.merged <- pbmc. for (n in nodes.to.merge). {. pbmc.merged <- MergeNode(pbmc.merged, n). }. ```. Here's an explanation, as this code was derived from this recent (and awesome) publication:. . From page 6 of the Supplementary Methods of Plass et al 2018: http://science.sciencemag.org/content/early/2018/04/18/science.aaq1723. To prevent obtaining spurious clusters result of overclustering, the robustness of the clusters was calculated using the function AssessNodes from Seurat. For each cluster, the average expression of all variable genes (4910) is computed and a phylogenetic tree based on the distance matrix in gene expression space is computed. Next, it computes an Out of Bag Error for a random forest classifier trained on each internal node split of the tree. We recursively build a tree and assessed all its nodes, merging all clusters with an out of bag error bigger than 0.1 until no such nodes were found.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/362
https://github.com/scverse/scanpy/issues/362:945,reliability,robust,robustness,945,"That is a wonderful solution! I will give it a shot shortly. Thank you so much for your help! Here's some example code of how Seurat handles cluster scoring and merging with random forests and OOBE:. ```. pbmc <- ValidateClusters(pbmc, pc.use = 1:30, top.genes = 30). pbmc <- BuildClusterTree(pbmc, . do.reorder = T, . reorder.numeric = T). node.scores <- AssessNodes(pbmc). node.scores[order(node.scores$oobe,decreasing = T),] -> node.scores. nodes.merge <- node.scores[which(node.scores[,2] > 0.1),]. nodes.to.merge <- sort(nodes.merge$node) . pbmc.merged <- pbmc. for (n in nodes.to.merge). {. pbmc.merged <- MergeNode(pbmc.merged, n). }. ```. Here's an explanation, as this code was derived from this recent (and awesome) publication:. . From page 6 of the Supplementary Methods of Plass et al 2018: http://science.sciencemag.org/content/early/2018/04/18/science.aaq1723. To prevent obtaining spurious clusters result of overclustering, the robustness of the clusters was calculated using the function AssessNodes from Seurat. For each cluster, the average expression of all variable genes (4910) is computed and a phylogenetic tree based on the distance matrix in gene expression space is computed. Next, it computes an Out of Bag Error for a random forest classifier trained on each internal node split of the tree. We recursively build a tree and assessed all its nodes, merging all clusters with an out of bag error bigger than 0.1 until no such nodes were found.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/362
https://github.com/scverse/scanpy/issues/362:213,safety,Valid,ValidateClusters,213,"That is a wonderful solution! I will give it a shot shortly. Thank you so much for your help! Here's some example code of how Seurat handles cluster scoring and merging with random forests and OOBE:. ```. pbmc <- ValidateClusters(pbmc, pc.use = 1:30, top.genes = 30). pbmc <- BuildClusterTree(pbmc, . do.reorder = T, . reorder.numeric = T). node.scores <- AssessNodes(pbmc). node.scores[order(node.scores$oobe,decreasing = T),] -> node.scores. nodes.merge <- node.scores[which(node.scores[,2] > 0.1),]. nodes.to.merge <- sort(nodes.merge$node) . pbmc.merged <- pbmc. for (n in nodes.to.merge). {. pbmc.merged <- MergeNode(pbmc.merged, n). }. ```. Here's an explanation, as this code was derived from this recent (and awesome) publication:. . From page 6 of the Supplementary Methods of Plass et al 2018: http://science.sciencemag.org/content/early/2018/04/18/science.aaq1723. To prevent obtaining spurious clusters result of overclustering, the robustness of the clusters was calculated using the function AssessNodes from Seurat. For each cluster, the average expression of all variable genes (4910) is computed and a phylogenetic tree based on the distance matrix in gene expression space is computed. Next, it computes an Out of Bag Error for a random forest classifier trained on each internal node split of the tree. We recursively build a tree and assessed all its nodes, merging all clusters with an out of bag error bigger than 0.1 until no such nodes were found.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/362
https://github.com/scverse/scanpy/issues/362:879,safety,prevent,prevent,879,"That is a wonderful solution! I will give it a shot shortly. Thank you so much for your help! Here's some example code of how Seurat handles cluster scoring and merging with random forests and OOBE:. ```. pbmc <- ValidateClusters(pbmc, pc.use = 1:30, top.genes = 30). pbmc <- BuildClusterTree(pbmc, . do.reorder = T, . reorder.numeric = T). node.scores <- AssessNodes(pbmc). node.scores[order(node.scores$oobe,decreasing = T),] -> node.scores. nodes.merge <- node.scores[which(node.scores[,2] > 0.1),]. nodes.to.merge <- sort(nodes.merge$node) . pbmc.merged <- pbmc. for (n in nodes.to.merge). {. pbmc.merged <- MergeNode(pbmc.merged, n). }. ```. Here's an explanation, as this code was derived from this recent (and awesome) publication:. . From page 6 of the Supplementary Methods of Plass et al 2018: http://science.sciencemag.org/content/early/2018/04/18/science.aaq1723. To prevent obtaining spurious clusters result of overclustering, the robustness of the clusters was calculated using the function AssessNodes from Seurat. For each cluster, the average expression of all variable genes (4910) is computed and a phylogenetic tree based on the distance matrix in gene expression space is computed. Next, it computes an Out of Bag Error for a random forest classifier trained on each internal node split of the tree. We recursively build a tree and assessed all its nodes, merging all clusters with an out of bag error bigger than 0.1 until no such nodes were found.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/362
https://github.com/scverse/scanpy/issues/362:945,safety,robust,robustness,945,"That is a wonderful solution! I will give it a shot shortly. Thank you so much for your help! Here's some example code of how Seurat handles cluster scoring and merging with random forests and OOBE:. ```. pbmc <- ValidateClusters(pbmc, pc.use = 1:30, top.genes = 30). pbmc <- BuildClusterTree(pbmc, . do.reorder = T, . reorder.numeric = T). node.scores <- AssessNodes(pbmc). node.scores[order(node.scores$oobe,decreasing = T),] -> node.scores. nodes.merge <- node.scores[which(node.scores[,2] > 0.1),]. nodes.to.merge <- sort(nodes.merge$node) . pbmc.merged <- pbmc. for (n in nodes.to.merge). {. pbmc.merged <- MergeNode(pbmc.merged, n). }. ```. Here's an explanation, as this code was derived from this recent (and awesome) publication:. . From page 6 of the Supplementary Methods of Plass et al 2018: http://science.sciencemag.org/content/early/2018/04/18/science.aaq1723. To prevent obtaining spurious clusters result of overclustering, the robustness of the clusters was calculated using the function AssessNodes from Seurat. For each cluster, the average expression of all variable genes (4910) is computed and a phylogenetic tree based on the distance matrix in gene expression space is computed. Next, it computes an Out of Bag Error for a random forest classifier trained on each internal node split of the tree. We recursively build a tree and assessed all its nodes, merging all clusters with an out of bag error bigger than 0.1 until no such nodes were found.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/362
https://github.com/scverse/scanpy/issues/362:1236,safety,Error,Error,1236,"That is a wonderful solution! I will give it a shot shortly. Thank you so much for your help! Here's some example code of how Seurat handles cluster scoring and merging with random forests and OOBE:. ```. pbmc <- ValidateClusters(pbmc, pc.use = 1:30, top.genes = 30). pbmc <- BuildClusterTree(pbmc, . do.reorder = T, . reorder.numeric = T). node.scores <- AssessNodes(pbmc). node.scores[order(node.scores$oobe,decreasing = T),] -> node.scores. nodes.merge <- node.scores[which(node.scores[,2] > 0.1),]. nodes.to.merge <- sort(nodes.merge$node) . pbmc.merged <- pbmc. for (n in nodes.to.merge). {. pbmc.merged <- MergeNode(pbmc.merged, n). }. ```. Here's an explanation, as this code was derived from this recent (and awesome) publication:. . From page 6 of the Supplementary Methods of Plass et al 2018: http://science.sciencemag.org/content/early/2018/04/18/science.aaq1723. To prevent obtaining spurious clusters result of overclustering, the robustness of the clusters was calculated using the function AssessNodes from Seurat. For each cluster, the average expression of all variable genes (4910) is computed and a phylogenetic tree based on the distance matrix in gene expression space is computed. Next, it computes an Out of Bag Error for a random forest classifier trained on each internal node split of the tree. We recursively build a tree and assessed all its nodes, merging all clusters with an out of bag error bigger than 0.1 until no such nodes were found.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/362
https://github.com/scverse/scanpy/issues/362:1418,safety,error,error,1418,"That is a wonderful solution! I will give it a shot shortly. Thank you so much for your help! Here's some example code of how Seurat handles cluster scoring and merging with random forests and OOBE:. ```. pbmc <- ValidateClusters(pbmc, pc.use = 1:30, top.genes = 30). pbmc <- BuildClusterTree(pbmc, . do.reorder = T, . reorder.numeric = T). node.scores <- AssessNodes(pbmc). node.scores[order(node.scores$oobe,decreasing = T),] -> node.scores. nodes.merge <- node.scores[which(node.scores[,2] > 0.1),]. nodes.to.merge <- sort(nodes.merge$node) . pbmc.merged <- pbmc. for (n in nodes.to.merge). {. pbmc.merged <- MergeNode(pbmc.merged, n). }. ```. Here's an explanation, as this code was derived from this recent (and awesome) publication:. . From page 6 of the Supplementary Methods of Plass et al 2018: http://science.sciencemag.org/content/early/2018/04/18/science.aaq1723. To prevent obtaining spurious clusters result of overclustering, the robustness of the clusters was calculated using the function AssessNodes from Seurat. For each cluster, the average expression of all variable genes (4910) is computed and a phylogenetic tree based on the distance matrix in gene expression space is computed. Next, it computes an Out of Bag Error for a random forest classifier trained on each internal node split of the tree. We recursively build a tree and assessed all its nodes, merging all clusters with an out of bag error bigger than 0.1 until no such nodes were found.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/362
https://github.com/scverse/scanpy/issues/362:213,security,Validat,ValidateClusters,213,"That is a wonderful solution! I will give it a shot shortly. Thank you so much for your help! Here's some example code of how Seurat handles cluster scoring and merging with random forests and OOBE:. ```. pbmc <- ValidateClusters(pbmc, pc.use = 1:30, top.genes = 30). pbmc <- BuildClusterTree(pbmc, . do.reorder = T, . reorder.numeric = T). node.scores <- AssessNodes(pbmc). node.scores[order(node.scores$oobe,decreasing = T),] -> node.scores. nodes.merge <- node.scores[which(node.scores[,2] > 0.1),]. nodes.to.merge <- sort(nodes.merge$node) . pbmc.merged <- pbmc. for (n in nodes.to.merge). {. pbmc.merged <- MergeNode(pbmc.merged, n). }. ```. Here's an explanation, as this code was derived from this recent (and awesome) publication:. . From page 6 of the Supplementary Methods of Plass et al 2018: http://science.sciencemag.org/content/early/2018/04/18/science.aaq1723. To prevent obtaining spurious clusters result of overclustering, the robustness of the clusters was calculated using the function AssessNodes from Seurat. For each cluster, the average expression of all variable genes (4910) is computed and a phylogenetic tree based on the distance matrix in gene expression space is computed. Next, it computes an Out of Bag Error for a random forest classifier trained on each internal node split of the tree. We recursively build a tree and assessed all its nodes, merging all clusters with an out of bag error bigger than 0.1 until no such nodes were found.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/362
https://github.com/scverse/scanpy/issues/362:356,security,Assess,AssessNodes,356,"That is a wonderful solution! I will give it a shot shortly. Thank you so much for your help! Here's some example code of how Seurat handles cluster scoring and merging with random forests and OOBE:. ```. pbmc <- ValidateClusters(pbmc, pc.use = 1:30, top.genes = 30). pbmc <- BuildClusterTree(pbmc, . do.reorder = T, . reorder.numeric = T). node.scores <- AssessNodes(pbmc). node.scores[order(node.scores$oobe,decreasing = T),] -> node.scores. nodes.merge <- node.scores[which(node.scores[,2] > 0.1),]. nodes.to.merge <- sort(nodes.merge$node) . pbmc.merged <- pbmc. for (n in nodes.to.merge). {. pbmc.merged <- MergeNode(pbmc.merged, n). }. ```. Here's an explanation, as this code was derived from this recent (and awesome) publication:. . From page 6 of the Supplementary Methods of Plass et al 2018: http://science.sciencemag.org/content/early/2018/04/18/science.aaq1723. To prevent obtaining spurious clusters result of overclustering, the robustness of the clusters was calculated using the function AssessNodes from Seurat. For each cluster, the average expression of all variable genes (4910) is computed and a phylogenetic tree based on the distance matrix in gene expression space is computed. Next, it computes an Out of Bag Error for a random forest classifier trained on each internal node split of the tree. We recursively build a tree and assessed all its nodes, merging all clusters with an out of bag error bigger than 0.1 until no such nodes were found.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/362
https://github.com/scverse/scanpy/issues/362:879,security,preven,prevent,879,"That is a wonderful solution! I will give it a shot shortly. Thank you so much for your help! Here's some example code of how Seurat handles cluster scoring and merging with random forests and OOBE:. ```. pbmc <- ValidateClusters(pbmc, pc.use = 1:30, top.genes = 30). pbmc <- BuildClusterTree(pbmc, . do.reorder = T, . reorder.numeric = T). node.scores <- AssessNodes(pbmc). node.scores[order(node.scores$oobe,decreasing = T),] -> node.scores. nodes.merge <- node.scores[which(node.scores[,2] > 0.1),]. nodes.to.merge <- sort(nodes.merge$node) . pbmc.merged <- pbmc. for (n in nodes.to.merge). {. pbmc.merged <- MergeNode(pbmc.merged, n). }. ```. Here's an explanation, as this code was derived from this recent (and awesome) publication:. . From page 6 of the Supplementary Methods of Plass et al 2018: http://science.sciencemag.org/content/early/2018/04/18/science.aaq1723. To prevent obtaining spurious clusters result of overclustering, the robustness of the clusters was calculated using the function AssessNodes from Seurat. For each cluster, the average expression of all variable genes (4910) is computed and a phylogenetic tree based on the distance matrix in gene expression space is computed. Next, it computes an Out of Bag Error for a random forest classifier trained on each internal node split of the tree. We recursively build a tree and assessed all its nodes, merging all clusters with an out of bag error bigger than 0.1 until no such nodes were found.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/362
https://github.com/scverse/scanpy/issues/362:1006,security,Assess,AssessNodes,1006,"That is a wonderful solution! I will give it a shot shortly. Thank you so much for your help! Here's some example code of how Seurat handles cluster scoring and merging with random forests and OOBE:. ```. pbmc <- ValidateClusters(pbmc, pc.use = 1:30, top.genes = 30). pbmc <- BuildClusterTree(pbmc, . do.reorder = T, . reorder.numeric = T). node.scores <- AssessNodes(pbmc). node.scores[order(node.scores$oobe,decreasing = T),] -> node.scores. nodes.merge <- node.scores[which(node.scores[,2] > 0.1),]. nodes.to.merge <- sort(nodes.merge$node) . pbmc.merged <- pbmc. for (n in nodes.to.merge). {. pbmc.merged <- MergeNode(pbmc.merged, n). }. ```. Here's an explanation, as this code was derived from this recent (and awesome) publication:. . From page 6 of the Supplementary Methods of Plass et al 2018: http://science.sciencemag.org/content/early/2018/04/18/science.aaq1723. To prevent obtaining spurious clusters result of overclustering, the robustness of the clusters was calculated using the function AssessNodes from Seurat. For each cluster, the average expression of all variable genes (4910) is computed and a phylogenetic tree based on the distance matrix in gene expression space is computed. Next, it computes an Out of Bag Error for a random forest classifier trained on each internal node split of the tree. We recursively build a tree and assessed all its nodes, merging all clusters with an out of bag error bigger than 0.1 until no such nodes were found.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/362
https://github.com/scverse/scanpy/issues/362:1354,security,assess,assessed,1354,"That is a wonderful solution! I will give it a shot shortly. Thank you so much for your help! Here's some example code of how Seurat handles cluster scoring and merging with random forests and OOBE:. ```. pbmc <- ValidateClusters(pbmc, pc.use = 1:30, top.genes = 30). pbmc <- BuildClusterTree(pbmc, . do.reorder = T, . reorder.numeric = T). node.scores <- AssessNodes(pbmc). node.scores[order(node.scores$oobe,decreasing = T),] -> node.scores. nodes.merge <- node.scores[which(node.scores[,2] > 0.1),]. nodes.to.merge <- sort(nodes.merge$node) . pbmc.merged <- pbmc. for (n in nodes.to.merge). {. pbmc.merged <- MergeNode(pbmc.merged, n). }. ```. Here's an explanation, as this code was derived from this recent (and awesome) publication:. . From page 6 of the Supplementary Methods of Plass et al 2018: http://science.sciencemag.org/content/early/2018/04/18/science.aaq1723. To prevent obtaining spurious clusters result of overclustering, the robustness of the clusters was calculated using the function AssessNodes from Seurat. For each cluster, the average expression of all variable genes (4910) is computed and a phylogenetic tree based on the distance matrix in gene expression space is computed. Next, it computes an Out of Bag Error for a random forest classifier trained on each internal node split of the tree. We recursively build a tree and assessed all its nodes, merging all clusters with an out of bag error bigger than 0.1 until no such nodes were found.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/362
https://github.com/scverse/scanpy/issues/362:88,usability,help,help,88,"That is a wonderful solution! I will give it a shot shortly. Thank you so much for your help! Here's some example code of how Seurat handles cluster scoring and merging with random forests and OOBE:. ```. pbmc <- ValidateClusters(pbmc, pc.use = 1:30, top.genes = 30). pbmc <- BuildClusterTree(pbmc, . do.reorder = T, . reorder.numeric = T). node.scores <- AssessNodes(pbmc). node.scores[order(node.scores$oobe,decreasing = T),] -> node.scores. nodes.merge <- node.scores[which(node.scores[,2] > 0.1),]. nodes.to.merge <- sort(nodes.merge$node) . pbmc.merged <- pbmc. for (n in nodes.to.merge). {. pbmc.merged <- MergeNode(pbmc.merged, n). }. ```. Here's an explanation, as this code was derived from this recent (and awesome) publication:. . From page 6 of the Supplementary Methods of Plass et al 2018: http://science.sciencemag.org/content/early/2018/04/18/science.aaq1723. To prevent obtaining spurious clusters result of overclustering, the robustness of the clusters was calculated using the function AssessNodes from Seurat. For each cluster, the average expression of all variable genes (4910) is computed and a phylogenetic tree based on the distance matrix in gene expression space is computed. Next, it computes an Out of Bag Error for a random forest classifier trained on each internal node split of the tree. We recursively build a tree and assessed all its nodes, merging all clusters with an out of bag error bigger than 0.1 until no such nodes were found.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/362
https://github.com/scverse/scanpy/issues/362:1236,usability,Error,Error,1236,"That is a wonderful solution! I will give it a shot shortly. Thank you so much for your help! Here's some example code of how Seurat handles cluster scoring and merging with random forests and OOBE:. ```. pbmc <- ValidateClusters(pbmc, pc.use = 1:30, top.genes = 30). pbmc <- BuildClusterTree(pbmc, . do.reorder = T, . reorder.numeric = T). node.scores <- AssessNodes(pbmc). node.scores[order(node.scores$oobe,decreasing = T),] -> node.scores. nodes.merge <- node.scores[which(node.scores[,2] > 0.1),]. nodes.to.merge <- sort(nodes.merge$node) . pbmc.merged <- pbmc. for (n in nodes.to.merge). {. pbmc.merged <- MergeNode(pbmc.merged, n). }. ```. Here's an explanation, as this code was derived from this recent (and awesome) publication:. . From page 6 of the Supplementary Methods of Plass et al 2018: http://science.sciencemag.org/content/early/2018/04/18/science.aaq1723. To prevent obtaining spurious clusters result of overclustering, the robustness of the clusters was calculated using the function AssessNodes from Seurat. For each cluster, the average expression of all variable genes (4910) is computed and a phylogenetic tree based on the distance matrix in gene expression space is computed. Next, it computes an Out of Bag Error for a random forest classifier trained on each internal node split of the tree. We recursively build a tree and assessed all its nodes, merging all clusters with an out of bag error bigger than 0.1 until no such nodes were found.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/362
https://github.com/scverse/scanpy/issues/362:1418,usability,error,error,1418,"That is a wonderful solution! I will give it a shot shortly. Thank you so much for your help! Here's some example code of how Seurat handles cluster scoring and merging with random forests and OOBE:. ```. pbmc <- ValidateClusters(pbmc, pc.use = 1:30, top.genes = 30). pbmc <- BuildClusterTree(pbmc, . do.reorder = T, . reorder.numeric = T). node.scores <- AssessNodes(pbmc). node.scores[order(node.scores$oobe,decreasing = T),] -> node.scores. nodes.merge <- node.scores[which(node.scores[,2] > 0.1),]. nodes.to.merge <- sort(nodes.merge$node) . pbmc.merged <- pbmc. for (n in nodes.to.merge). {. pbmc.merged <- MergeNode(pbmc.merged, n). }. ```. Here's an explanation, as this code was derived from this recent (and awesome) publication:. . From page 6 of the Supplementary Methods of Plass et al 2018: http://science.sciencemag.org/content/early/2018/04/18/science.aaq1723. To prevent obtaining spurious clusters result of overclustering, the robustness of the clusters was calculated using the function AssessNodes from Seurat. For each cluster, the average expression of all variable genes (4910) is computed and a phylogenetic tree based on the distance matrix in gene expression space is computed. Next, it computes an Out of Bag Error for a random forest classifier trained on each internal node split of the tree. We recursively build a tree and assessed all its nodes, merging all clusters with an out of bag error bigger than 0.1 until no such nodes were found.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/362
https://github.com/scverse/scanpy/issues/363:87,availability,error,error,87,"Can you call `del adata.uns['cell_ontology_class_colors']`? This should throw a better error message... I can do that soon, I wonder how you managed to produce the error... cannot be anything related to a recent update... Hm.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:164,availability,error,error,164,"Can you call `del adata.uns['cell_ontology_class_colors']`? This should throw a better error message... I can do that soon, I wonder how you managed to produce the error... cannot be anything related to a recent update... Hm.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:141,deployability,manag,managed,141,"Can you call `del adata.uns['cell_ontology_class_colors']`? This should throw a better error message... I can do that soon, I wonder how you managed to produce the error... cannot be anything related to a recent update... Hm.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:212,deployability,updat,update,212,"Can you call `del adata.uns['cell_ontology_class_colors']`? This should throw a better error message... I can do that soon, I wonder how you managed to produce the error... cannot be anything related to a recent update... Hm.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:141,energy efficiency,manag,managed,141,"Can you call `del adata.uns['cell_ontology_class_colors']`? This should throw a better error message... I can do that soon, I wonder how you managed to produce the error... cannot be anything related to a recent update... Hm.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:93,integrability,messag,message,93,"Can you call `del adata.uns['cell_ontology_class_colors']`? This should throw a better error message... I can do that soon, I wonder how you managed to produce the error... cannot be anything related to a recent update... Hm.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:93,interoperability,messag,message,93,"Can you call `del adata.uns['cell_ontology_class_colors']`? This should throw a better error message... I can do that soon, I wonder how you managed to produce the error... cannot be anything related to a recent update... Hm.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:87,performance,error,error,87,"Can you call `del adata.uns['cell_ontology_class_colors']`? This should throw a better error message... I can do that soon, I wonder how you managed to produce the error... cannot be anything related to a recent update... Hm.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:164,performance,error,error,164,"Can you call `del adata.uns['cell_ontology_class_colors']`? This should throw a better error message... I can do that soon, I wonder how you managed to produce the error... cannot be anything related to a recent update... Hm.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:87,safety,error,error,87,"Can you call `del adata.uns['cell_ontology_class_colors']`? This should throw a better error message... I can do that soon, I wonder how you managed to produce the error... cannot be anything related to a recent update... Hm.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:141,safety,manag,managed,141,"Can you call `del adata.uns['cell_ontology_class_colors']`? This should throw a better error message... I can do that soon, I wonder how you managed to produce the error... cannot be anything related to a recent update... Hm.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:164,safety,error,error,164,"Can you call `del adata.uns['cell_ontology_class_colors']`? This should throw a better error message... I can do that soon, I wonder how you managed to produce the error... cannot be anything related to a recent update... Hm.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:212,safety,updat,update,212,"Can you call `del adata.uns['cell_ontology_class_colors']`? This should throw a better error message... I can do that soon, I wonder how you managed to produce the error... cannot be anything related to a recent update... Hm.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:212,security,updat,update,212,"Can you call `del adata.uns['cell_ontology_class_colors']`? This should throw a better error message... I can do that soon, I wonder how you managed to produce the error... cannot be anything related to a recent update... Hm.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:87,usability,error,error,87,"Can you call `del adata.uns['cell_ontology_class_colors']`? This should throw a better error message... I can do that soon, I wonder how you managed to produce the error... cannot be anything related to a recent update... Hm.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:164,usability,error,error,164,"Can you call `del adata.uns['cell_ontology_class_colors']`? This should throw a better error message... I can do that soon, I wonder how you managed to produce the error... cannot be anything related to a recent update... Hm.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:34,deployability,observ,observation,34,actually what works is to set the observation to categorical or run tSNE/umap beforehand - it always happens when I'm trying to subset without having run the plots first,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:128,integrability,sub,subset,128,actually what works is to set the observation to categorical or run tSNE/umap beforehand - it always happens when I'm trying to subset without having run the plots first,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:34,testability,observ,observation,34,actually what works is to set the observation to categorical or run tSNE/umap beforehand - it always happens when I'm trying to subset without having run the plots first,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:1372,availability,sli,sliced,1372,"ection', 'batch', 'cell', 'cell_ontology_class', 'cell_ontology_id', 'cellid', 'free_annotation', 'method', 'mouse.id', 'plate', 'sex', 'subtissue', 'tissue', 'well', 'n_genes', 'n_counts', 'louvain'. var: 'n_cells', 'means', 'dispersions', 'dispersions_norm', 'highly_variable'. uns: 'free_annotation_colors', 'louvain', 'louvain_colors', 'method_colors', 'mouse.id_colors', 'neighbors', 'pca', 'rank_genes_groups', 'sex_colors', 'subtissue_colors', 'tissue_colors', 'dendrogram'. obsm: 'X_pca', 'X_umap', 'X_tsne'. varm: 'PCs'. tiss_facs[tiss_facs.obs['cell_ontology_class']=='keratinocyte stem cell']. ---------------------------------------------------------------------------. IndexError Traceback (most recent call last). <ipython-input-82-428532769794> in <module>(). ----> 1 tiss_facs[tiss_facs.obs['cell_ontology_class']=='keratinocyte stem cell']. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). 1299 def __getitem__(self, index):. 1300 """"""Returns a sliced view of the object."""""". -> 1301 return self._getitem_view(index). 1302 . 1303 def _getitem_view(self, index):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _getitem_view(self, index). 1303 def _getitem_view(self, index):. 1304 oidx, vidx = self._normalize_indices(index). -> 1305 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1306 . 1307 def _remove_unused_categories(self, df_full, df_sub, uns):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 662 if not isinstance(X, AnnData):. 663 raise ValueError('`X` has to be an AnnData object.'). --> 664 self._init_as_view(X, oidx, vidx). 665 else:. 666 self._init_as_actual(. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _init_as_view(self, adata_ref, oidx, vidx). 713 raise KeyError('Unknown Index type'). 714 # fix categories. --> 715 self._remove_unused_categories(adata_ref.obs, obs_s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:1139,deployability,modul,module,1139,"s', 'epidermal cell',. 'keratinocyte stem cell', 'leukocyte', 'stem cell of epidermis'],. dtype='object'). del tiss_facs.uns['cell_ontology_class_colors']. tiss_facs. AnnData object with n_obs  n_vars = 3468  22899 . obs: 'FACS.selection', 'batch', 'cell', 'cell_ontology_class', 'cell_ontology_id', 'cellid', 'free_annotation', 'method', 'mouse.id', 'plate', 'sex', 'subtissue', 'tissue', 'well', 'n_genes', 'n_counts', 'louvain'. var: 'n_cells', 'means', 'dispersions', 'dispersions_norm', 'highly_variable'. uns: 'free_annotation_colors', 'louvain', 'louvain_colors', 'method_colors', 'mouse.id_colors', 'neighbors', 'pca', 'rank_genes_groups', 'sex_colors', 'subtissue_colors', 'tissue_colors', 'dendrogram'. obsm: 'X_pca', 'X_umap', 'X_tsne'. varm: 'PCs'. tiss_facs[tiss_facs.obs['cell_ontology_class']=='keratinocyte stem cell']. ---------------------------------------------------------------------------. IndexError Traceback (most recent call last). <ipython-input-82-428532769794> in <module>(). ----> 1 tiss_facs[tiss_facs.obs['cell_ontology_class']=='keratinocyte stem cell']. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). 1299 def __getitem__(self, index):. 1300 """"""Returns a sliced view of the object."""""". -> 1301 return self._getitem_view(index). 1302 . 1303 def _getitem_view(self, index):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _getitem_view(self, index). 1303 def _getitem_view(self, index):. 1304 oidx, vidx = self._normalize_indices(index). -> 1305 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1306 . 1307 def _remove_unused_categories(self, df_full, df_sub, uns):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 662 if not isinstance(X, AnnData):. 663 raise ValueError('`X` has to be an AnnData object.'). --> 664 self._init_as_view(X, oidx, vidx). 665 else:. 666 self._init_as_ac",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:385,integrability,batch,batch,385,"@falexwolf now I can't really get it to work... ```. tiss_facs.obs['cell_ontology_class'].cat.categories. Index(['NA', 'basal cell of epidermis', 'epidermal cell',. 'keratinocyte stem cell', 'leukocyte', 'stem cell of epidermis'],. dtype='object'). del tiss_facs.uns['cell_ontology_class_colors']. tiss_facs. AnnData object with n_obs  n_vars = 3468  22899 . obs: 'FACS.selection', 'batch', 'cell', 'cell_ontology_class', 'cell_ontology_id', 'cellid', 'free_annotation', 'method', 'mouse.id', 'plate', 'sex', 'subtissue', 'tissue', 'well', 'n_genes', 'n_counts', 'louvain'. var: 'n_cells', 'means', 'dispersions', 'dispersions_norm', 'highly_variable'. uns: 'free_annotation_colors', 'louvain', 'louvain_colors', 'method_colors', 'mouse.id_colors', 'neighbors', 'pca', 'rank_genes_groups', 'sex_colors', 'subtissue_colors', 'tissue_colors', 'dendrogram'. obsm: 'X_pca', 'X_umap', 'X_tsne'. varm: 'PCs'. tiss_facs[tiss_facs.obs['cell_ontology_class']=='keratinocyte stem cell']. ---------------------------------------------------------------------------. IndexError Traceback (most recent call last). <ipython-input-82-428532769794> in <module>(). ----> 1 tiss_facs[tiss_facs.obs['cell_ontology_class']=='keratinocyte stem cell']. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). 1299 def __getitem__(self, index):. 1300 """"""Returns a sliced view of the object."""""". -> 1301 return self._getitem_view(index). 1302 . 1303 def _getitem_view(self, index):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _getitem_view(self, index). 1303 def _getitem_view(self, index):. 1304 oidx, vidx = self._normalize_indices(index). -> 1305 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1306 . 1307 def _remove_unused_categories(self, df_full, df_sub, uns):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 662 if not isinstance(X, A",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:512,integrability,sub,subtissue,512,"@falexwolf now I can't really get it to work... ```. tiss_facs.obs['cell_ontology_class'].cat.categories. Index(['NA', 'basal cell of epidermis', 'epidermal cell',. 'keratinocyte stem cell', 'leukocyte', 'stem cell of epidermis'],. dtype='object'). del tiss_facs.uns['cell_ontology_class_colors']. tiss_facs. AnnData object with n_obs  n_vars = 3468  22899 . obs: 'FACS.selection', 'batch', 'cell', 'cell_ontology_class', 'cell_ontology_id', 'cellid', 'free_annotation', 'method', 'mouse.id', 'plate', 'sex', 'subtissue', 'tissue', 'well', 'n_genes', 'n_counts', 'louvain'. var: 'n_cells', 'means', 'dispersions', 'dispersions_norm', 'highly_variable'. uns: 'free_annotation_colors', 'louvain', 'louvain_colors', 'method_colors', 'mouse.id_colors', 'neighbors', 'pca', 'rank_genes_groups', 'sex_colors', 'subtissue_colors', 'tissue_colors', 'dendrogram'. obsm: 'X_pca', 'X_umap', 'X_tsne'. varm: 'PCs'. tiss_facs[tiss_facs.obs['cell_ontology_class']=='keratinocyte stem cell']. ---------------------------------------------------------------------------. IndexError Traceback (most recent call last). <ipython-input-82-428532769794> in <module>(). ----> 1 tiss_facs[tiss_facs.obs['cell_ontology_class']=='keratinocyte stem cell']. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). 1299 def __getitem__(self, index):. 1300 """"""Returns a sliced view of the object."""""". -> 1301 return self._getitem_view(index). 1302 . 1303 def _getitem_view(self, index):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _getitem_view(self, index). 1303 def _getitem_view(self, index):. 1304 oidx, vidx = self._normalize_indices(index). -> 1305 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1306 . 1307 def _remove_unused_categories(self, df_full, df_sub, uns):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 662 if not isinstance(X, A",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:1139,modifiability,modul,module,1139,"s', 'epidermal cell',. 'keratinocyte stem cell', 'leukocyte', 'stem cell of epidermis'],. dtype='object'). del tiss_facs.uns['cell_ontology_class_colors']. tiss_facs. AnnData object with n_obs  n_vars = 3468  22899 . obs: 'FACS.selection', 'batch', 'cell', 'cell_ontology_class', 'cell_ontology_id', 'cellid', 'free_annotation', 'method', 'mouse.id', 'plate', 'sex', 'subtissue', 'tissue', 'well', 'n_genes', 'n_counts', 'louvain'. var: 'n_cells', 'means', 'dispersions', 'dispersions_norm', 'highly_variable'. uns: 'free_annotation_colors', 'louvain', 'louvain_colors', 'method_colors', 'mouse.id_colors', 'neighbors', 'pca', 'rank_genes_groups', 'sex_colors', 'subtissue_colors', 'tissue_colors', 'dendrogram'. obsm: 'X_pca', 'X_umap', 'X_tsne'. varm: 'PCs'. tiss_facs[tiss_facs.obs['cell_ontology_class']=='keratinocyte stem cell']. ---------------------------------------------------------------------------. IndexError Traceback (most recent call last). <ipython-input-82-428532769794> in <module>(). ----> 1 tiss_facs[tiss_facs.obs['cell_ontology_class']=='keratinocyte stem cell']. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). 1299 def __getitem__(self, index):. 1300 """"""Returns a sliced view of the object."""""". -> 1301 return self._getitem_view(index). 1302 . 1303 def _getitem_view(self, index):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _getitem_view(self, index). 1303 def _getitem_view(self, index):. 1304 oidx, vidx = self._normalize_indices(index). -> 1305 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1306 . 1307 def _remove_unused_categories(self, df_full, df_sub, uns):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 662 if not isinstance(X, AnnData):. 663 raise ValueError('`X` has to be an AnnData object.'). --> 664 self._init_as_view(X, oidx, vidx). 665 else:. 666 self._init_as_ac",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:1264,modifiability,pac,packages,1264,"cell_ontology_class_colors']. tiss_facs. AnnData object with n_obs  n_vars = 3468  22899 . obs: 'FACS.selection', 'batch', 'cell', 'cell_ontology_class', 'cell_ontology_id', 'cellid', 'free_annotation', 'method', 'mouse.id', 'plate', 'sex', 'subtissue', 'tissue', 'well', 'n_genes', 'n_counts', 'louvain'. var: 'n_cells', 'means', 'dispersions', 'dispersions_norm', 'highly_variable'. uns: 'free_annotation_colors', 'louvain', 'louvain_colors', 'method_colors', 'mouse.id_colors', 'neighbors', 'pca', 'rank_genes_groups', 'sex_colors', 'subtissue_colors', 'tissue_colors', 'dendrogram'. obsm: 'X_pca', 'X_umap', 'X_tsne'. varm: 'PCs'. tiss_facs[tiss_facs.obs['cell_ontology_class']=='keratinocyte stem cell']. ---------------------------------------------------------------------------. IndexError Traceback (most recent call last). <ipython-input-82-428532769794> in <module>(). ----> 1 tiss_facs[tiss_facs.obs['cell_ontology_class']=='keratinocyte stem cell']. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). 1299 def __getitem__(self, index):. 1300 """"""Returns a sliced view of the object."""""". -> 1301 return self._getitem_view(index). 1302 . 1303 def _getitem_view(self, index):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _getitem_view(self, index). 1303 def _getitem_view(self, index):. 1304 oidx, vidx = self._normalize_indices(index). -> 1305 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1306 . 1307 def _remove_unused_categories(self, df_full, df_sub, uns):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 662 if not isinstance(X, AnnData):. 663 raise ValueError('`X` has to be an AnnData object.'). --> 664 self._init_as_view(X, oidx, vidx). 665 else:. 666 self._init_as_actual(. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _init_as_view(self, adata_ref, oidx, vidx). 713 raise KeyErr",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:1521,modifiability,pac,packages,1521,"tissue', 'well', 'n_genes', 'n_counts', 'louvain'. var: 'n_cells', 'means', 'dispersions', 'dispersions_norm', 'highly_variable'. uns: 'free_annotation_colors', 'louvain', 'louvain_colors', 'method_colors', 'mouse.id_colors', 'neighbors', 'pca', 'rank_genes_groups', 'sex_colors', 'subtissue_colors', 'tissue_colors', 'dendrogram'. obsm: 'X_pca', 'X_umap', 'X_tsne'. varm: 'PCs'. tiss_facs[tiss_facs.obs['cell_ontology_class']=='keratinocyte stem cell']. ---------------------------------------------------------------------------. IndexError Traceback (most recent call last). <ipython-input-82-428532769794> in <module>(). ----> 1 tiss_facs[tiss_facs.obs['cell_ontology_class']=='keratinocyte stem cell']. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). 1299 def __getitem__(self, index):. 1300 """"""Returns a sliced view of the object."""""". -> 1301 return self._getitem_view(index). 1302 . 1303 def _getitem_view(self, index):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _getitem_view(self, index). 1303 def _getitem_view(self, index):. 1304 oidx, vidx = self._normalize_indices(index). -> 1305 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1306 . 1307 def _remove_unused_categories(self, df_full, df_sub, uns):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 662 if not isinstance(X, AnnData):. 663 raise ValueError('`X` has to be an AnnData object.'). --> 664 self._init_as_view(X, oidx, vidx). 665 else:. 666 self._init_as_actual(. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _init_as_view(self, adata_ref, oidx, vidx). 713 raise KeyError('Unknown Index type'). 714 # fix categories. --> 715 self._remove_unused_categories(adata_ref.obs, obs_sub, uns_new). 716 self._remove_unused_categories(adata_ref.var, var_sub, uns_new). 717 # set attributes. ~/anaconda3/lib/python3.6/site-packages/annda",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:1833,modifiability,pac,packages,1833,"ors', 'dendrogram'. obsm: 'X_pca', 'X_umap', 'X_tsne'. varm: 'PCs'. tiss_facs[tiss_facs.obs['cell_ontology_class']=='keratinocyte stem cell']. ---------------------------------------------------------------------------. IndexError Traceback (most recent call last). <ipython-input-82-428532769794> in <module>(). ----> 1 tiss_facs[tiss_facs.obs['cell_ontology_class']=='keratinocyte stem cell']. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). 1299 def __getitem__(self, index):. 1300 """"""Returns a sliced view of the object."""""". -> 1301 return self._getitem_view(index). 1302 . 1303 def _getitem_view(self, index):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _getitem_view(self, index). 1303 def _getitem_view(self, index):. 1304 oidx, vidx = self._normalize_indices(index). -> 1305 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1306 . 1307 def _remove_unused_categories(self, df_full, df_sub, uns):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 662 if not isinstance(X, AnnData):. 663 raise ValueError('`X` has to be an AnnData object.'). --> 664 self._init_as_view(X, oidx, vidx). 665 else:. 666 self._init_as_actual(. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _init_as_view(self, adata_ref, oidx, vidx). 713 raise KeyError('Unknown Index type'). 714 # fix categories. --> 715 self._remove_unused_categories(adata_ref.obs, obs_sub, uns_new). 716 self._remove_unused_categories(adata_ref.var, var_sub, uns_new). 717 # set attributes. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _remove_unused_categories(self, df_full, df_sub, uns). 1318 uns[k + '_colors'] = np.array(uns[k + '_colors'])[. 1319 np.where(np.in1d(. -> 1320 all_categories, df_sub[k].cat.categories))[0]]. 1321 . 1322 def rename_categories(self, key, categories):. IndexError: index 6 is out of bounds for axis 1",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:1906,modifiability,layer,layers,1906,"'. obsm: 'X_pca', 'X_umap', 'X_tsne'. varm: 'PCs'. tiss_facs[tiss_facs.obs['cell_ontology_class']=='keratinocyte stem cell']. ---------------------------------------------------------------------------. IndexError Traceback (most recent call last). <ipython-input-82-428532769794> in <module>(). ----> 1 tiss_facs[tiss_facs.obs['cell_ontology_class']=='keratinocyte stem cell']. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). 1299 def __getitem__(self, index):. 1300 """"""Returns a sliced view of the object."""""". -> 1301 return self._getitem_view(index). 1302 . 1303 def _getitem_view(self, index):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _getitem_view(self, index). 1303 def _getitem_view(self, index):. 1304 oidx, vidx = self._normalize_indices(index). -> 1305 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1306 . 1307 def _remove_unused_categories(self, df_full, df_sub, uns):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 662 if not isinstance(X, AnnData):. 663 raise ValueError('`X` has to be an AnnData object.'). --> 664 self._init_as_view(X, oidx, vidx). 665 else:. 666 self._init_as_actual(. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _init_as_view(self, adata_ref, oidx, vidx). 713 raise KeyError('Unknown Index type'). 714 # fix categories. --> 715 self._remove_unused_categories(adata_ref.obs, obs_sub, uns_new). 716 self._remove_unused_categories(adata_ref.var, var_sub, uns_new). 717 # set attributes. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _remove_unused_categories(self, df_full, df_sub, uns). 1318 uns[k + '_colors'] = np.array(uns[k + '_colors'])[. 1319 np.where(np.in1d(. -> 1320 all_categories, df_sub[k].cat.categories))[0]]. 1321 . 1322 def rename_categories(self, key, categories):. IndexError: index 6 is out of bounds for axis 1 with size 6. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:2180,modifiability,pac,packages,2180,"'. obsm: 'X_pca', 'X_umap', 'X_tsne'. varm: 'PCs'. tiss_facs[tiss_facs.obs['cell_ontology_class']=='keratinocyte stem cell']. ---------------------------------------------------------------------------. IndexError Traceback (most recent call last). <ipython-input-82-428532769794> in <module>(). ----> 1 tiss_facs[tiss_facs.obs['cell_ontology_class']=='keratinocyte stem cell']. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). 1299 def __getitem__(self, index):. 1300 """"""Returns a sliced view of the object."""""". -> 1301 return self._getitem_view(index). 1302 . 1303 def _getitem_view(self, index):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _getitem_view(self, index). 1303 def _getitem_view(self, index):. 1304 oidx, vidx = self._normalize_indices(index). -> 1305 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1306 . 1307 def _remove_unused_categories(self, df_full, df_sub, uns):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 662 if not isinstance(X, AnnData):. 663 raise ValueError('`X` has to be an AnnData object.'). --> 664 self._init_as_view(X, oidx, vidx). 665 else:. 666 self._init_as_actual(. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _init_as_view(self, adata_ref, oidx, vidx). 713 raise KeyError('Unknown Index type'). 714 # fix categories. --> 715 self._remove_unused_categories(adata_ref.obs, obs_sub, uns_new). 716 self._remove_unused_categories(adata_ref.var, var_sub, uns_new). 717 # set attributes. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _remove_unused_categories(self, df_full, df_sub, uns). 1318 uns[k + '_colors'] = np.array(uns[k + '_colors'])[. 1319 np.where(np.in1d(. -> 1320 all_categories, df_sub[k].cat.categories))[0]]. 1321 . 1322 def rename_categories(self, key, categories):. IndexError: index 6 is out of bounds for axis 1 with size 6. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:2511,modifiability,pac,packages,2511,"'. obsm: 'X_pca', 'X_umap', 'X_tsne'. varm: 'PCs'. tiss_facs[tiss_facs.obs['cell_ontology_class']=='keratinocyte stem cell']. ---------------------------------------------------------------------------. IndexError Traceback (most recent call last). <ipython-input-82-428532769794> in <module>(). ----> 1 tiss_facs[tiss_facs.obs['cell_ontology_class']=='keratinocyte stem cell']. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). 1299 def __getitem__(self, index):. 1300 """"""Returns a sliced view of the object."""""". -> 1301 return self._getitem_view(index). 1302 . 1303 def _getitem_view(self, index):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _getitem_view(self, index). 1303 def _getitem_view(self, index):. 1304 oidx, vidx = self._normalize_indices(index). -> 1305 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1306 . 1307 def _remove_unused_categories(self, df_full, df_sub, uns):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 662 if not isinstance(X, AnnData):. 663 raise ValueError('`X` has to be an AnnData object.'). --> 664 self._init_as_view(X, oidx, vidx). 665 else:. 666 self._init_as_actual(. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _init_as_view(self, adata_ref, oidx, vidx). 713 raise KeyError('Unknown Index type'). 714 # fix categories. --> 715 self._remove_unused_categories(adata_ref.obs, obs_sub, uns_new). 716 self._remove_unused_categories(adata_ref.var, var_sub, uns_new). 717 # set attributes. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _remove_unused_categories(self, df_full, df_sub, uns). 1318 uns[k + '_colors'] = np.array(uns[k + '_colors'])[. 1319 np.where(np.in1d(. -> 1320 all_categories, df_sub[k].cat.categories))[0]]. 1321 . 1322 def rename_categories(self, key, categories):. IndexError: index 6 is out of bounds for axis 1 with size 6. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:385,performance,batch,batch,385,"@falexwolf now I can't really get it to work... ```. tiss_facs.obs['cell_ontology_class'].cat.categories. Index(['NA', 'basal cell of epidermis', 'epidermal cell',. 'keratinocyte stem cell', 'leukocyte', 'stem cell of epidermis'],. dtype='object'). del tiss_facs.uns['cell_ontology_class_colors']. tiss_facs. AnnData object with n_obs  n_vars = 3468  22899 . obs: 'FACS.selection', 'batch', 'cell', 'cell_ontology_class', 'cell_ontology_id', 'cellid', 'free_annotation', 'method', 'mouse.id', 'plate', 'sex', 'subtissue', 'tissue', 'well', 'n_genes', 'n_counts', 'louvain'. var: 'n_cells', 'means', 'dispersions', 'dispersions_norm', 'highly_variable'. uns: 'free_annotation_colors', 'louvain', 'louvain_colors', 'method_colors', 'mouse.id_colors', 'neighbors', 'pca', 'rank_genes_groups', 'sex_colors', 'subtissue_colors', 'tissue_colors', 'dendrogram'. obsm: 'X_pca', 'X_umap', 'X_tsne'. varm: 'PCs'. tiss_facs[tiss_facs.obs['cell_ontology_class']=='keratinocyte stem cell']. ---------------------------------------------------------------------------. IndexError Traceback (most recent call last). <ipython-input-82-428532769794> in <module>(). ----> 1 tiss_facs[tiss_facs.obs['cell_ontology_class']=='keratinocyte stem cell']. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). 1299 def __getitem__(self, index):. 1300 """"""Returns a sliced view of the object."""""". -> 1301 return self._getitem_view(index). 1302 . 1303 def _getitem_view(self, index):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _getitem_view(self, index). 1303 def _getitem_view(self, index):. 1304 oidx, vidx = self._normalize_indices(index). -> 1305 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1306 . 1307 def _remove_unused_categories(self, df_full, df_sub, uns):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 662 if not isinstance(X, A",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:1372,reliability,sli,sliced,1372,"ection', 'batch', 'cell', 'cell_ontology_class', 'cell_ontology_id', 'cellid', 'free_annotation', 'method', 'mouse.id', 'plate', 'sex', 'subtissue', 'tissue', 'well', 'n_genes', 'n_counts', 'louvain'. var: 'n_cells', 'means', 'dispersions', 'dispersions_norm', 'highly_variable'. uns: 'free_annotation_colors', 'louvain', 'louvain_colors', 'method_colors', 'mouse.id_colors', 'neighbors', 'pca', 'rank_genes_groups', 'sex_colors', 'subtissue_colors', 'tissue_colors', 'dendrogram'. obsm: 'X_pca', 'X_umap', 'X_tsne'. varm: 'PCs'. tiss_facs[tiss_facs.obs['cell_ontology_class']=='keratinocyte stem cell']. ---------------------------------------------------------------------------. IndexError Traceback (most recent call last). <ipython-input-82-428532769794> in <module>(). ----> 1 tiss_facs[tiss_facs.obs['cell_ontology_class']=='keratinocyte stem cell']. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). 1299 def __getitem__(self, index):. 1300 """"""Returns a sliced view of the object."""""". -> 1301 return self._getitem_view(index). 1302 . 1303 def _getitem_view(self, index):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _getitem_view(self, index). 1303 def _getitem_view(self, index):. 1304 oidx, vidx = self._normalize_indices(index). -> 1305 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1306 . 1307 def _remove_unused_categories(self, df_full, df_sub, uns):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 662 if not isinstance(X, AnnData):. 663 raise ValueError('`X` has to be an AnnData object.'). --> 664 self._init_as_view(X, oidx, vidx). 665 else:. 666 self._init_as_actual(. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _init_as_view(self, adata_ref, oidx, vidx). 713 raise KeyError('Unknown Index type'). 714 # fix categories. --> 715 self._remove_unused_categories(adata_ref.obs, obs_s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:1112,safety,input,input-,1112,"A', 'basal cell of epidermis', 'epidermal cell',. 'keratinocyte stem cell', 'leukocyte', 'stem cell of epidermis'],. dtype='object'). del tiss_facs.uns['cell_ontology_class_colors']. tiss_facs. AnnData object with n_obs  n_vars = 3468  22899 . obs: 'FACS.selection', 'batch', 'cell', 'cell_ontology_class', 'cell_ontology_id', 'cellid', 'free_annotation', 'method', 'mouse.id', 'plate', 'sex', 'subtissue', 'tissue', 'well', 'n_genes', 'n_counts', 'louvain'. var: 'n_cells', 'means', 'dispersions', 'dispersions_norm', 'highly_variable'. uns: 'free_annotation_colors', 'louvain', 'louvain_colors', 'method_colors', 'mouse.id_colors', 'neighbors', 'pca', 'rank_genes_groups', 'sex_colors', 'subtissue_colors', 'tissue_colors', 'dendrogram'. obsm: 'X_pca', 'X_umap', 'X_tsne'. varm: 'PCs'. tiss_facs[tiss_facs.obs['cell_ontology_class']=='keratinocyte stem cell']. ---------------------------------------------------------------------------. IndexError Traceback (most recent call last). <ipython-input-82-428532769794> in <module>(). ----> 1 tiss_facs[tiss_facs.obs['cell_ontology_class']=='keratinocyte stem cell']. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). 1299 def __getitem__(self, index):. 1300 """"""Returns a sliced view of the object."""""". -> 1301 return self._getitem_view(index). 1302 . 1303 def _getitem_view(self, index):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _getitem_view(self, index). 1303 def _getitem_view(self, index):. 1304 oidx, vidx = self._normalize_indices(index). -> 1305 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1306 . 1307 def _remove_unused_categories(self, df_full, df_sub, uns):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 662 if not isinstance(X, AnnData):. 663 raise ValueError('`X` has to be an AnnData object.'). --> 664 self._init_as_view(X, oidx, vidx). 665 ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:1139,safety,modul,module,1139,"s', 'epidermal cell',. 'keratinocyte stem cell', 'leukocyte', 'stem cell of epidermis'],. dtype='object'). del tiss_facs.uns['cell_ontology_class_colors']. tiss_facs. AnnData object with n_obs  n_vars = 3468  22899 . obs: 'FACS.selection', 'batch', 'cell', 'cell_ontology_class', 'cell_ontology_id', 'cellid', 'free_annotation', 'method', 'mouse.id', 'plate', 'sex', 'subtissue', 'tissue', 'well', 'n_genes', 'n_counts', 'louvain'. var: 'n_cells', 'means', 'dispersions', 'dispersions_norm', 'highly_variable'. uns: 'free_annotation_colors', 'louvain', 'louvain_colors', 'method_colors', 'mouse.id_colors', 'neighbors', 'pca', 'rank_genes_groups', 'sex_colors', 'subtissue_colors', 'tissue_colors', 'dendrogram'. obsm: 'X_pca', 'X_umap', 'X_tsne'. varm: 'PCs'. tiss_facs[tiss_facs.obs['cell_ontology_class']=='keratinocyte stem cell']. ---------------------------------------------------------------------------. IndexError Traceback (most recent call last). <ipython-input-82-428532769794> in <module>(). ----> 1 tiss_facs[tiss_facs.obs['cell_ontology_class']=='keratinocyte stem cell']. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). 1299 def __getitem__(self, index):. 1300 """"""Returns a sliced view of the object."""""". -> 1301 return self._getitem_view(index). 1302 . 1303 def _getitem_view(self, index):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _getitem_view(self, index). 1303 def _getitem_view(self, index):. 1304 oidx, vidx = self._normalize_indices(index). -> 1305 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1306 . 1307 def _remove_unused_categories(self, df_full, df_sub, uns):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 662 if not isinstance(X, AnnData):. 663 raise ValueError('`X` has to be an AnnData object.'). --> 664 self._init_as_view(X, oidx, vidx). 665 else:. 666 self._init_as_ac",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:1068,testability,Trace,Traceback,1068,"ontology_class'].cat.categories. Index(['NA', 'basal cell of epidermis', 'epidermal cell',. 'keratinocyte stem cell', 'leukocyte', 'stem cell of epidermis'],. dtype='object'). del tiss_facs.uns['cell_ontology_class_colors']. tiss_facs. AnnData object with n_obs  n_vars = 3468  22899 . obs: 'FACS.selection', 'batch', 'cell', 'cell_ontology_class', 'cell_ontology_id', 'cellid', 'free_annotation', 'method', 'mouse.id', 'plate', 'sex', 'subtissue', 'tissue', 'well', 'n_genes', 'n_counts', 'louvain'. var: 'n_cells', 'means', 'dispersions', 'dispersions_norm', 'highly_variable'. uns: 'free_annotation_colors', 'louvain', 'louvain_colors', 'method_colors', 'mouse.id_colors', 'neighbors', 'pca', 'rank_genes_groups', 'sex_colors', 'subtissue_colors', 'tissue_colors', 'dendrogram'. obsm: 'X_pca', 'X_umap', 'X_tsne'. varm: 'PCs'. tiss_facs[tiss_facs.obs['cell_ontology_class']=='keratinocyte stem cell']. ---------------------------------------------------------------------------. IndexError Traceback (most recent call last). <ipython-input-82-428532769794> in <module>(). ----> 1 tiss_facs[tiss_facs.obs['cell_ontology_class']=='keratinocyte stem cell']. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). 1299 def __getitem__(self, index):. 1300 """"""Returns a sliced view of the object."""""". -> 1301 return self._getitem_view(index). 1302 . 1303 def _getitem_view(self, index):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _getitem_view(self, index). 1303 def _getitem_view(self, index):. 1304 oidx, vidx = self._normalize_indices(index). -> 1305 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1306 . 1307 def _remove_unused_categories(self, df_full, df_sub, uns):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 662 if not isinstance(X, AnnData):. 663 raise ValueError('`X` has to be an AnnData object.'). --> 6",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:484,usability,mous,mouse,484,"@falexwolf now I can't really get it to work... ```. tiss_facs.obs['cell_ontology_class'].cat.categories. Index(['NA', 'basal cell of epidermis', 'epidermal cell',. 'keratinocyte stem cell', 'leukocyte', 'stem cell of epidermis'],. dtype='object'). del tiss_facs.uns['cell_ontology_class_colors']. tiss_facs. AnnData object with n_obs  n_vars = 3468  22899 . obs: 'FACS.selection', 'batch', 'cell', 'cell_ontology_class', 'cell_ontology_id', 'cellid', 'free_annotation', 'method', 'mouse.id', 'plate', 'sex', 'subtissue', 'tissue', 'well', 'n_genes', 'n_counts', 'louvain'. var: 'n_cells', 'means', 'dispersions', 'dispersions_norm', 'highly_variable'. uns: 'free_annotation_colors', 'louvain', 'louvain_colors', 'method_colors', 'mouse.id_colors', 'neighbors', 'pca', 'rank_genes_groups', 'sex_colors', 'subtissue_colors', 'tissue_colors', 'dendrogram'. obsm: 'X_pca', 'X_umap', 'X_tsne'. varm: 'PCs'. tiss_facs[tiss_facs.obs['cell_ontology_class']=='keratinocyte stem cell']. ---------------------------------------------------------------------------. IndexError Traceback (most recent call last). <ipython-input-82-428532769794> in <module>(). ----> 1 tiss_facs[tiss_facs.obs['cell_ontology_class']=='keratinocyte stem cell']. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). 1299 def __getitem__(self, index):. 1300 """"""Returns a sliced view of the object."""""". -> 1301 return self._getitem_view(index). 1302 . 1303 def _getitem_view(self, index):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _getitem_view(self, index). 1303 def _getitem_view(self, index):. 1304 oidx, vidx = self._normalize_indices(index). -> 1305 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1306 . 1307 def _remove_unused_categories(self, df_full, df_sub, uns):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 662 if not isinstance(X, A",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:733,usability,mous,mouse,733,"@falexwolf now I can't really get it to work... ```. tiss_facs.obs['cell_ontology_class'].cat.categories. Index(['NA', 'basal cell of epidermis', 'epidermal cell',. 'keratinocyte stem cell', 'leukocyte', 'stem cell of epidermis'],. dtype='object'). del tiss_facs.uns['cell_ontology_class_colors']. tiss_facs. AnnData object with n_obs  n_vars = 3468  22899 . obs: 'FACS.selection', 'batch', 'cell', 'cell_ontology_class', 'cell_ontology_id', 'cellid', 'free_annotation', 'method', 'mouse.id', 'plate', 'sex', 'subtissue', 'tissue', 'well', 'n_genes', 'n_counts', 'louvain'. var: 'n_cells', 'means', 'dispersions', 'dispersions_norm', 'highly_variable'. uns: 'free_annotation_colors', 'louvain', 'louvain_colors', 'method_colors', 'mouse.id_colors', 'neighbors', 'pca', 'rank_genes_groups', 'sex_colors', 'subtissue_colors', 'tissue_colors', 'dendrogram'. obsm: 'X_pca', 'X_umap', 'X_tsne'. varm: 'PCs'. tiss_facs[tiss_facs.obs['cell_ontology_class']=='keratinocyte stem cell']. ---------------------------------------------------------------------------. IndexError Traceback (most recent call last). <ipython-input-82-428532769794> in <module>(). ----> 1 tiss_facs[tiss_facs.obs['cell_ontology_class']=='keratinocyte stem cell']. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). 1299 def __getitem__(self, index):. 1300 """"""Returns a sliced view of the object."""""". -> 1301 return self._getitem_view(index). 1302 . 1303 def _getitem_view(self, index):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _getitem_view(self, index). 1303 def _getitem_view(self, index):. 1304 oidx, vidx = self._normalize_indices(index). -> 1305 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1306 . 1307 def _remove_unused_categories(self, df_full, df_sub, uns):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 662 if not isinstance(X, A",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:1112,usability,input,input-,1112,"A', 'basal cell of epidermis', 'epidermal cell',. 'keratinocyte stem cell', 'leukocyte', 'stem cell of epidermis'],. dtype='object'). del tiss_facs.uns['cell_ontology_class_colors']. tiss_facs. AnnData object with n_obs  n_vars = 3468  22899 . obs: 'FACS.selection', 'batch', 'cell', 'cell_ontology_class', 'cell_ontology_id', 'cellid', 'free_annotation', 'method', 'mouse.id', 'plate', 'sex', 'subtissue', 'tissue', 'well', 'n_genes', 'n_counts', 'louvain'. var: 'n_cells', 'means', 'dispersions', 'dispersions_norm', 'highly_variable'. uns: 'free_annotation_colors', 'louvain', 'louvain_colors', 'method_colors', 'mouse.id_colors', 'neighbors', 'pca', 'rank_genes_groups', 'sex_colors', 'subtissue_colors', 'tissue_colors', 'dendrogram'. obsm: 'X_pca', 'X_umap', 'X_tsne'. varm: 'PCs'. tiss_facs[tiss_facs.obs['cell_ontology_class']=='keratinocyte stem cell']. ---------------------------------------------------------------------------. IndexError Traceback (most recent call last). <ipython-input-82-428532769794> in <module>(). ----> 1 tiss_facs[tiss_facs.obs['cell_ontology_class']=='keratinocyte stem cell']. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). 1299 def __getitem__(self, index):. 1300 """"""Returns a sliced view of the object."""""". -> 1301 return self._getitem_view(index). 1302 . 1303 def _getitem_view(self, index):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _getitem_view(self, index). 1303 def _getitem_view(self, index):. 1304 oidx, vidx = self._normalize_indices(index). -> 1305 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1306 . 1307 def _remove_unused_categories(self, df_full, df_sub, uns):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 662 if not isinstance(X, AnnData):. 663 raise ValueError('`X` has to be an AnnData object.'). --> 664 self._init_as_view(X, oidx, vidx). 665 ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:1117,availability,sli,sliced,1117,"ll have no success... <details>. <summary>Traceback</summary>. ```pytb. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-12-a0821f8568d8> in <module>(). 22 axs[i].set_title(c). 23 # plot_boxplot_nonzero(tiss[tiss.obs['cell_ontology_class']==cell,:],geneofinterest,'age',axs[i],show = False). ---> 24 plot_boxplot_cell_fraction(tiss[tiss.obs['auto_cell_ontology_class']==c],geneofinterest,'age',c,axs[i],show = False). 25 print(c + ' is done!'). 26 i = i+1. <ipython-input-10-40fc020a5ff4> in plot_boxplot_cell_fraction(adata, gene, label, title, ax, show). 1 def plot_boxplot_cell_fraction(adata, gene, label, title, ax, show=True):. ----> 2 gene_vals = np.asarray(adata[:, gene].X).flatten(). 3 . 4 labels = ['3m','24m']. 5 # labels = list(set(adata.obs[label])). ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). 1299 def __getitem__(self, index):. 1300 """"""Returns a sliced view of the object."""""". -> 1301 return self._getitem_view(index). 1302 . 1303 def _getitem_view(self, index):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _getitem_view(self, index). 1303 def _getitem_view(self, index):. 1304 oidx, vidx = self._normalize_indices(index). -> 1305 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1306 . 1307 def _remove_unused_categories(self, df_full, df_sub, uns):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 662 if not isinstance(X, AnnData):. 663 raise ValueError('`X` has to be an AnnData object.'). --> 664 self._init_as_view(X, oidx, vidx). 665 else:. 666 self._init_as_actual(. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _init_as_view(self, adata_ref, oidx, vidx). 691 self._varm = ArrayView(adata_ref.varm[vidx_normalized], view_args=(self, 'varm')). 692 # hackish solution here, no copy",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:2213,availability,sli,slicing,2213,"ew(self, index):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _getitem_view(self, index). 1303 def _getitem_view(self, index):. 1304 oidx, vidx = self._normalize_indices(index). -> 1305 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1306 . 1307 def _remove_unused_categories(self, df_full, df_sub, uns):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 662 if not isinstance(X, AnnData):. 663 raise ValueError('`X` has to be an AnnData object.'). --> 664 self._init_as_view(X, oidx, vidx). 665 else:. 666 self._init_as_actual(. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _init_as_view(self, adata_ref, oidx, vidx). 691 self._varm = ArrayView(adata_ref.varm[vidx_normalized], view_args=(self, 'varm')). 692 # hackish solution here, no copy should be necessary. --> 693 uns_new = deepcopy(self._adata_ref._uns). 694 # need to do the slicing before setting the updated self._n_obs, self._n_vars. 695 self._n_obs = self._adata_ref.n_obs # use the original n_obs here. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 178 y = x. 179 else:. --> 180 y = _reconstruct(x, memo, *rv). 181 . 182 # If is its own copy, don't memoize. ~/anaconda3/lib/python3.6/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy). 278 if state is not None:. 279 if deep:. --> 280 state = deepcopy(state, memo). 281 if hasattr(y, '__setstate__'):. 282 y.__setstate__(state). ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 148 copier = _deepcopy_dispatch.get(cls). 149 if copier:. --> 150 y = copier(x, memo). 151 else:. 152 try:. ~/anaconda3/lib/python3.6/copy.py in _deepcopy_dict(x, memo, deepcopy). 238 memo[id(x)] = y. 239 for key, value in x.items():. --> 240 y[deepcopy(key, memo)] = deepcopy(value, memo). 241 return y. 242 d[dict] = _deepcopy_dict. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, me",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:2588,availability,state,state,2588,"ata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 662 if not isinstance(X, AnnData):. 663 raise ValueError('`X` has to be an AnnData object.'). --> 664 self._init_as_view(X, oidx, vidx). 665 else:. 666 self._init_as_actual(. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _init_as_view(self, adata_ref, oidx, vidx). 691 self._varm = ArrayView(adata_ref.varm[vidx_normalized], view_args=(self, 'varm')). 692 # hackish solution here, no copy should be necessary. --> 693 uns_new = deepcopy(self._adata_ref._uns). 694 # need to do the slicing before setting the updated self._n_obs, self._n_vars. 695 self._n_obs = self._adata_ref.n_obs # use the original n_obs here. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 178 y = x. 179 else:. --> 180 y = _reconstruct(x, memo, *rv). 181 . 182 # If is its own copy, don't memoize. ~/anaconda3/lib/python3.6/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy). 278 if state is not None:. 279 if deep:. --> 280 state = deepcopy(state, memo). 281 if hasattr(y, '__setstate__'):. 282 y.__setstate__(state). ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 148 copier = _deepcopy_dispatch.get(cls). 149 if copier:. --> 150 y = copier(x, memo). 151 else:. 152 try:. ~/anaconda3/lib/python3.6/copy.py in _deepcopy_dict(x, memo, deepcopy). 238 memo[id(x)] = y. 239 for key, value in x.items():. --> 240 y[deepcopy(key, memo)] = deepcopy(value, memo). 241 return y. 242 d[dict] = _deepcopy_dict. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 148 copier = _deepcopy_dispatch.get(cls). 149 if copier:. --> 150 y = copier(x, memo). 151 else:. 152 try:. ~/anaconda3/lib/python3.6/copy.py in _deepcopy_tuple(x, memo, deepcopy). 218 . 219 def _deepcopy_tuple(x, memo, deepcopy=deepcopy):. --> 220 y = [deepcopy(a, memo) for a in x]. 221 # We're not going to put the tuple in the memo, but it's still important w",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:2633,availability,state,state,2633,"s, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 662 if not isinstance(X, AnnData):. 663 raise ValueError('`X` has to be an AnnData object.'). --> 664 self._init_as_view(X, oidx, vidx). 665 else:. 666 self._init_as_actual(. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _init_as_view(self, adata_ref, oidx, vidx). 691 self._varm = ArrayView(adata_ref.varm[vidx_normalized], view_args=(self, 'varm')). 692 # hackish solution here, no copy should be necessary. --> 693 uns_new = deepcopy(self._adata_ref._uns). 694 # need to do the slicing before setting the updated self._n_obs, self._n_vars. 695 self._n_obs = self._adata_ref.n_obs # use the original n_obs here. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 178 y = x. 179 else:. --> 180 y = _reconstruct(x, memo, *rv). 181 . 182 # If is its own copy, don't memoize. ~/anaconda3/lib/python3.6/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy). 278 if state is not None:. 279 if deep:. --> 280 state = deepcopy(state, memo). 281 if hasattr(y, '__setstate__'):. 282 y.__setstate__(state). ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 148 copier = _deepcopy_dispatch.get(cls). 149 if copier:. --> 150 y = copier(x, memo). 151 else:. 152 try:. ~/anaconda3/lib/python3.6/copy.py in _deepcopy_dict(x, memo, deepcopy). 238 memo[id(x)] = y. 239 for key, value in x.items():. --> 240 y[deepcopy(key, memo)] = deepcopy(value, memo). 241 return y. 242 d[dict] = _deepcopy_dict. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 148 copier = _deepcopy_dispatch.get(cls). 149 if copier:. --> 150 y = copier(x, memo). 151 else:. 152 try:. ~/anaconda3/lib/python3.6/copy.py in _deepcopy_tuple(x, memo, deepcopy). 218 . 219 def _deepcopy_tuple(x, memo, deepcopy=deepcopy):. --> 220 y = [deepcopy(a, memo) for a in x]. 221 # We're not going to put the tuple in the memo, but it's still important we. 222 # check for it, in case the tuple cont",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:2675,availability,state,state,2675,"filename, filemode, asview, oidx, vidx). 662 if not isinstance(X, AnnData):. 663 raise ValueError('`X` has to be an AnnData object.'). --> 664 self._init_as_view(X, oidx, vidx). 665 else:. 666 self._init_as_actual(. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _init_as_view(self, adata_ref, oidx, vidx). 691 self._varm = ArrayView(adata_ref.varm[vidx_normalized], view_args=(self, 'varm')). 692 # hackish solution here, no copy should be necessary. --> 693 uns_new = deepcopy(self._adata_ref._uns). 694 # need to do the slicing before setting the updated self._n_obs, self._n_vars. 695 self._n_obs = self._adata_ref.n_obs # use the original n_obs here. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 178 y = x. 179 else:. --> 180 y = _reconstruct(x, memo, *rv). 181 . 182 # If is its own copy, don't memoize. ~/anaconda3/lib/python3.6/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy). 278 if state is not None:. 279 if deep:. --> 280 state = deepcopy(state, memo). 281 if hasattr(y, '__setstate__'):. 282 y.__setstate__(state). ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 148 copier = _deepcopy_dispatch.get(cls). 149 if copier:. --> 150 y = copier(x, memo). 151 else:. 152 try:. ~/anaconda3/lib/python3.6/copy.py in _deepcopy_dict(x, memo, deepcopy). 238 memo[id(x)] = y. 239 for key, value in x.items():. --> 240 y[deepcopy(key, memo)] = deepcopy(value, memo). 241 return y. 242 d[dict] = _deepcopy_dict. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 148 copier = _deepcopy_dispatch.get(cls). 149 if copier:. --> 150 y = copier(x, memo). 151 else:. 152 try:. ~/anaconda3/lib/python3.6/copy.py in _deepcopy_tuple(x, memo, deepcopy). 218 . 219 def _deepcopy_tuple(x, memo, deepcopy=deepcopy):. --> 220 y = [deepcopy(a, memo) for a in x]. 221 # We're not going to put the tuple in the memo, but it's still important we. 222 # check for it, in case the tuple contains recursive mutable structures. ~/anaco",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:2692,availability,state,state,2692,"e, asview, oidx, vidx). 662 if not isinstance(X, AnnData):. 663 raise ValueError('`X` has to be an AnnData object.'). --> 664 self._init_as_view(X, oidx, vidx). 665 else:. 666 self._init_as_actual(. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _init_as_view(self, adata_ref, oidx, vidx). 691 self._varm = ArrayView(adata_ref.varm[vidx_normalized], view_args=(self, 'varm')). 692 # hackish solution here, no copy should be necessary. --> 693 uns_new = deepcopy(self._adata_ref._uns). 694 # need to do the slicing before setting the updated self._n_obs, self._n_vars. 695 self._n_obs = self._adata_ref.n_obs # use the original n_obs here. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 178 y = x. 179 else:. --> 180 y = _reconstruct(x, memo, *rv). 181 . 182 # If is its own copy, don't memoize. ~/anaconda3/lib/python3.6/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy). 278 if state is not None:. 279 if deep:. --> 280 state = deepcopy(state, memo). 281 if hasattr(y, '__setstate__'):. 282 y.__setstate__(state). ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 148 copier = _deepcopy_dispatch.get(cls). 149 if copier:. --> 150 y = copier(x, memo). 151 else:. 152 try:. ~/anaconda3/lib/python3.6/copy.py in _deepcopy_dict(x, memo, deepcopy). 238 memo[id(x)] = y. 239 for key, value in x.items():. --> 240 y[deepcopy(key, memo)] = deepcopy(value, memo). 241 return y. 242 d[dict] = _deepcopy_dict. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 148 copier = _deepcopy_dispatch.get(cls). 149 if copier:. --> 150 y = copier(x, memo). 151 else:. 152 try:. ~/anaconda3/lib/python3.6/copy.py in _deepcopy_tuple(x, memo, deepcopy). 218 . 219 def _deepcopy_tuple(x, memo, deepcopy=deepcopy):. --> 220 y = [deepcopy(a, memo) for a in x]. 221 # We're not going to put the tuple in the memo, but it's still important we. 222 # check for it, in case the tuple contains recursive mutable structures. ~/anaconda3/lib/python3.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:2761,availability,state,state,2761," ValueError('`X` has to be an AnnData object.'). --> 664 self._init_as_view(X, oidx, vidx). 665 else:. 666 self._init_as_actual(. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _init_as_view(self, adata_ref, oidx, vidx). 691 self._varm = ArrayView(adata_ref.varm[vidx_normalized], view_args=(self, 'varm')). 692 # hackish solution here, no copy should be necessary. --> 693 uns_new = deepcopy(self._adata_ref._uns). 694 # need to do the slicing before setting the updated self._n_obs, self._n_vars. 695 self._n_obs = self._adata_ref.n_obs # use the original n_obs here. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 178 y = x. 179 else:. --> 180 y = _reconstruct(x, memo, *rv). 181 . 182 # If is its own copy, don't memoize. ~/anaconda3/lib/python3.6/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy). 278 if state is not None:. 279 if deep:. --> 280 state = deepcopy(state, memo). 281 if hasattr(y, '__setstate__'):. 282 y.__setstate__(state). ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 148 copier = _deepcopy_dispatch.get(cls). 149 if copier:. --> 150 y = copier(x, memo). 151 else:. 152 try:. ~/anaconda3/lib/python3.6/copy.py in _deepcopy_dict(x, memo, deepcopy). 238 memo[id(x)] = y. 239 for key, value in x.items():. --> 240 y[deepcopy(key, memo)] = deepcopy(value, memo). 241 return y. 242 d[dict] = _deepcopy_dict. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 148 copier = _deepcopy_dispatch.get(cls). 149 if copier:. --> 150 y = copier(x, memo). 151 else:. 152 try:. ~/anaconda3/lib/python3.6/copy.py in _deepcopy_tuple(x, memo, deepcopy). 218 . 219 def _deepcopy_tuple(x, memo, deepcopy=deepcopy):. --> 220 y = [deepcopy(a, memo) for a in x]. 221 # We're not going to put the tuple in the memo, but it's still important we. 222 # check for it, in case the tuple contains recursive mutable structures. ~/anaconda3/lib/python3.6/copy.py in <listcomp>(.0). 218 . 219 def _deepcopy_tuple(x, memo, d",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:4228,availability,state,state,4228," copier = _deepcopy_dispatch.get(cls). 149 if copier:. --> 150 y = copier(x, memo). 151 else:. 152 try:. ~/anaconda3/lib/python3.6/copy.py in _deepcopy_tuple(x, memo, deepcopy). 218 . 219 def _deepcopy_tuple(x, memo, deepcopy=deepcopy):. --> 220 y = [deepcopy(a, memo) for a in x]. 221 # We're not going to put the tuple in the memo, but it's still important we. 222 # check for it, in case the tuple contains recursive mutable structures. ~/anaconda3/lib/python3.6/copy.py in <listcomp>(.0). 218 . 219 def _deepcopy_tuple(x, memo, deepcopy=deepcopy):. --> 220 y = [deepcopy(a, memo) for a in x]. 221 # We're not going to put the tuple in the memo, but it's still important we. 222 # check for it, in case the tuple contains recursive mutable structures. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 178 y = x. 179 else:. --> 180 y = _reconstruct(x, memo, *rv). 181 . 182 # If is its own copy, don't memoize. ~/anaconda3/lib/python3.6/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy). 278 if state is not None:. 279 if deep:. --> 280 state = deepcopy(state, memo). 281 if hasattr(y, '__setstate__'):. 282 y.__setstate__(state). ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 148 copier = _deepcopy_dispatch.get(cls). 149 if copier:. --> 150 y = copier(x, memo). 151 else:. 152 try:. ~/anaconda3/lib/python3.6/copy.py in _deepcopy_dict(x, memo, deepcopy). 238 memo[id(x)] = y. 239 for key, value in x.items():. --> 240 y[deepcopy(key, memo)] = deepcopy(value, memo). 241 return y. 242 d[dict] = _deepcopy_dict. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 178 y = x. 179 else:. --> 180 y = _reconstruct(x, memo, *rv). 181 . 182 # If is its own copy, don't memoize. ~/anaconda3/lib/python3.6/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy). 278 if state is not None:. 279 if deep:. --> 280 state = deepcopy(state, memo). 281 if hasattr(y, '__setstate__'):. 282 y.__setstate__(state). ~/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:4273,availability,state,state,4273," copier:. --> 150 y = copier(x, memo). 151 else:. 152 try:. ~/anaconda3/lib/python3.6/copy.py in _deepcopy_tuple(x, memo, deepcopy). 218 . 219 def _deepcopy_tuple(x, memo, deepcopy=deepcopy):. --> 220 y = [deepcopy(a, memo) for a in x]. 221 # We're not going to put the tuple in the memo, but it's still important we. 222 # check for it, in case the tuple contains recursive mutable structures. ~/anaconda3/lib/python3.6/copy.py in <listcomp>(.0). 218 . 219 def _deepcopy_tuple(x, memo, deepcopy=deepcopy):. --> 220 y = [deepcopy(a, memo) for a in x]. 221 # We're not going to put the tuple in the memo, but it's still important we. 222 # check for it, in case the tuple contains recursive mutable structures. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 178 y = x. 179 else:. --> 180 y = _reconstruct(x, memo, *rv). 181 . 182 # If is its own copy, don't memoize. ~/anaconda3/lib/python3.6/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy). 278 if state is not None:. 279 if deep:. --> 280 state = deepcopy(state, memo). 281 if hasattr(y, '__setstate__'):. 282 y.__setstate__(state). ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 148 copier = _deepcopy_dispatch.get(cls). 149 if copier:. --> 150 y = copier(x, memo). 151 else:. 152 try:. ~/anaconda3/lib/python3.6/copy.py in _deepcopy_dict(x, memo, deepcopy). 238 memo[id(x)] = y. 239 for key, value in x.items():. --> 240 y[deepcopy(key, memo)] = deepcopy(value, memo). 241 return y. 242 d[dict] = _deepcopy_dict. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 178 y = x. 179 else:. --> 180 y = _reconstruct(x, memo, *rv). 181 . 182 # If is its own copy, don't memoize. ~/anaconda3/lib/python3.6/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy). 278 if state is not None:. 279 if deep:. --> 280 state = deepcopy(state, memo). 281 if hasattr(y, '__setstate__'):. 282 y.__setstate__(state). ~/anaconda3/lib/python3.6/copy.py in deepcopy(x",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:4315,availability,state,state,4315," else:. 152 try:. ~/anaconda3/lib/python3.6/copy.py in _deepcopy_tuple(x, memo, deepcopy). 218 . 219 def _deepcopy_tuple(x, memo, deepcopy=deepcopy):. --> 220 y = [deepcopy(a, memo) for a in x]. 221 # We're not going to put the tuple in the memo, but it's still important we. 222 # check for it, in case the tuple contains recursive mutable structures. ~/anaconda3/lib/python3.6/copy.py in <listcomp>(.0). 218 . 219 def _deepcopy_tuple(x, memo, deepcopy=deepcopy):. --> 220 y = [deepcopy(a, memo) for a in x]. 221 # We're not going to put the tuple in the memo, but it's still important we. 222 # check for it, in case the tuple contains recursive mutable structures. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 178 y = x. 179 else:. --> 180 y = _reconstruct(x, memo, *rv). 181 . 182 # If is its own copy, don't memoize. ~/anaconda3/lib/python3.6/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy). 278 if state is not None:. 279 if deep:. --> 280 state = deepcopy(state, memo). 281 if hasattr(y, '__setstate__'):. 282 y.__setstate__(state). ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 148 copier = _deepcopy_dispatch.get(cls). 149 if copier:. --> 150 y = copier(x, memo). 151 else:. 152 try:. ~/anaconda3/lib/python3.6/copy.py in _deepcopy_dict(x, memo, deepcopy). 238 memo[id(x)] = y. 239 for key, value in x.items():. --> 240 y[deepcopy(key, memo)] = deepcopy(value, memo). 241 return y. 242 d[dict] = _deepcopy_dict. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 178 y = x. 179 else:. --> 180 y = _reconstruct(x, memo, *rv). 181 . 182 # If is its own copy, don't memoize. ~/anaconda3/lib/python3.6/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy). 278 if state is not None:. 279 if deep:. --> 280 state = deepcopy(state, memo). 281 if hasattr(y, '__setstate__'):. 282 y.__setstate__(state). ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 148 copier = _deepcopy_disp",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:4332,availability,state,state,4332," ~/anaconda3/lib/python3.6/copy.py in _deepcopy_tuple(x, memo, deepcopy). 218 . 219 def _deepcopy_tuple(x, memo, deepcopy=deepcopy):. --> 220 y = [deepcopy(a, memo) for a in x]. 221 # We're not going to put the tuple in the memo, but it's still important we. 222 # check for it, in case the tuple contains recursive mutable structures. ~/anaconda3/lib/python3.6/copy.py in <listcomp>(.0). 218 . 219 def _deepcopy_tuple(x, memo, deepcopy=deepcopy):. --> 220 y = [deepcopy(a, memo) for a in x]. 221 # We're not going to put the tuple in the memo, but it's still important we. 222 # check for it, in case the tuple contains recursive mutable structures. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 178 y = x. 179 else:. --> 180 y = _reconstruct(x, memo, *rv). 181 . 182 # If is its own copy, don't memoize. ~/anaconda3/lib/python3.6/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy). 278 if state is not None:. 279 if deep:. --> 280 state = deepcopy(state, memo). 281 if hasattr(y, '__setstate__'):. 282 y.__setstate__(state). ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 148 copier = _deepcopy_dispatch.get(cls). 149 if copier:. --> 150 y = copier(x, memo). 151 else:. 152 try:. ~/anaconda3/lib/python3.6/copy.py in _deepcopy_dict(x, memo, deepcopy). 238 memo[id(x)] = y. 239 for key, value in x.items():. --> 240 y[deepcopy(key, memo)] = deepcopy(value, memo). 241 return y. 242 d[dict] = _deepcopy_dict. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 178 y = x. 179 else:. --> 180 y = _reconstruct(x, memo, *rv). 181 . 182 # If is its own copy, don't memoize. ~/anaconda3/lib/python3.6/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy). 278 if state is not None:. 279 if deep:. --> 280 state = deepcopy(state, memo). 281 if hasattr(y, '__setstate__'):. 282 y.__setstate__(state). ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 148 copier = _deepcopy_dispatch.get(cls). 14",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:4401,availability,state,state,4401,"py). 218 . 219 def _deepcopy_tuple(x, memo, deepcopy=deepcopy):. --> 220 y = [deepcopy(a, memo) for a in x]. 221 # We're not going to put the tuple in the memo, but it's still important we. 222 # check for it, in case the tuple contains recursive mutable structures. ~/anaconda3/lib/python3.6/copy.py in <listcomp>(.0). 218 . 219 def _deepcopy_tuple(x, memo, deepcopy=deepcopy):. --> 220 y = [deepcopy(a, memo) for a in x]. 221 # We're not going to put the tuple in the memo, but it's still important we. 222 # check for it, in case the tuple contains recursive mutable structures. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 178 y = x. 179 else:. --> 180 y = _reconstruct(x, memo, *rv). 181 . 182 # If is its own copy, don't memoize. ~/anaconda3/lib/python3.6/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy). 278 if state is not None:. 279 if deep:. --> 280 state = deepcopy(state, memo). 281 if hasattr(y, '__setstate__'):. 282 y.__setstate__(state). ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 148 copier = _deepcopy_dispatch.get(cls). 149 if copier:. --> 150 y = copier(x, memo). 151 else:. 152 try:. ~/anaconda3/lib/python3.6/copy.py in _deepcopy_dict(x, memo, deepcopy). 238 memo[id(x)] = y. 239 for key, value in x.items():. --> 240 y[deepcopy(key, memo)] = deepcopy(value, memo). 241 return y. 242 d[dict] = _deepcopy_dict. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 178 y = x. 179 else:. --> 180 y = _reconstruct(x, memo, *rv). 181 . 182 # If is its own copy, don't memoize. ~/anaconda3/lib/python3.6/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy). 278 if state is not None:. 279 if deep:. --> 280 state = deepcopy(state, memo). 281 if hasattr(y, '__setstate__'):. 282 y.__setstate__(state). ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 148 copier = _deepcopy_dispatch.get(cls). 149 if copier:. --> 150 y = copier(x, memo). 151 else:. 152 try:. ~/ana",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:5048,availability,state,state,5048," y = x. 179 else:. --> 180 y = _reconstruct(x, memo, *rv). 181 . 182 # If is its own copy, don't memoize. ~/anaconda3/lib/python3.6/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy). 278 if state is not None:. 279 if deep:. --> 280 state = deepcopy(state, memo). 281 if hasattr(y, '__setstate__'):. 282 y.__setstate__(state). ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 148 copier = _deepcopy_dispatch.get(cls). 149 if copier:. --> 150 y = copier(x, memo). 151 else:. 152 try:. ~/anaconda3/lib/python3.6/copy.py in _deepcopy_dict(x, memo, deepcopy). 238 memo[id(x)] = y. 239 for key, value in x.items():. --> 240 y[deepcopy(key, memo)] = deepcopy(value, memo). 241 return y. 242 d[dict] = _deepcopy_dict. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 178 y = x. 179 else:. --> 180 y = _reconstruct(x, memo, *rv). 181 . 182 # If is its own copy, don't memoize. ~/anaconda3/lib/python3.6/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy). 278 if state is not None:. 279 if deep:. --> 280 state = deepcopy(state, memo). 281 if hasattr(y, '__setstate__'):. 282 y.__setstate__(state). ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 148 copier = _deepcopy_dispatch.get(cls). 149 if copier:. --> 150 y = copier(x, memo). 151 else:. 152 try:. ~/anaconda3/lib/python3.6/copy.py in _deepcopy_dict(x, memo, deepcopy). 238 memo[id(x)] = y. 239 for key, value in x.items():. --> 240 y[deepcopy(key, memo)] = deepcopy(value, memo). 241 return y. 242 d[dict] = _deepcopy_dict. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 178 y = x. 179 else:. --> 180 y = _reconstruct(x, memo, *rv). 181 . 182 # If is its own copy, don't memoize. ~/anaconda3/lib/python3.6/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy). 305 key = deepcopy(key, memo). 306 value = deepcopy(value, memo). --> 307 y[key] = value. 308 else:. 309 for key, value in dictiter:. ~/anaconda3",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:5093,availability,state,state,5093,", memo, *rv). 181 . 182 # If is its own copy, don't memoize. ~/anaconda3/lib/python3.6/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy). 278 if state is not None:. 279 if deep:. --> 280 state = deepcopy(state, memo). 281 if hasattr(y, '__setstate__'):. 282 y.__setstate__(state). ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 148 copier = _deepcopy_dispatch.get(cls). 149 if copier:. --> 150 y = copier(x, memo). 151 else:. 152 try:. ~/anaconda3/lib/python3.6/copy.py in _deepcopy_dict(x, memo, deepcopy). 238 memo[id(x)] = y. 239 for key, value in x.items():. --> 240 y[deepcopy(key, memo)] = deepcopy(value, memo). 241 return y. 242 d[dict] = _deepcopy_dict. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 178 y = x. 179 else:. --> 180 y = _reconstruct(x, memo, *rv). 181 . 182 # If is its own copy, don't memoize. ~/anaconda3/lib/python3.6/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy). 278 if state is not None:. 279 if deep:. --> 280 state = deepcopy(state, memo). 281 if hasattr(y, '__setstate__'):. 282 y.__setstate__(state). ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 148 copier = _deepcopy_dispatch.get(cls). 149 if copier:. --> 150 y = copier(x, memo). 151 else:. 152 try:. ~/anaconda3/lib/python3.6/copy.py in _deepcopy_dict(x, memo, deepcopy). 238 memo[id(x)] = y. 239 for key, value in x.items():. --> 240 y[deepcopy(key, memo)] = deepcopy(value, memo). 241 return y. 242 d[dict] = _deepcopy_dict. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 178 y = x. 179 else:. --> 180 y = _reconstruct(x, memo, *rv). 181 . 182 # If is its own copy, don't memoize. ~/anaconda3/lib/python3.6/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy). 305 key = deepcopy(key, memo). 306 value = deepcopy(value, memo). --> 307 y[key] = value. 308 else:. 309 for key, value in dictiter:. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:5135,availability,state,state,5135,"py, don't memoize. ~/anaconda3/lib/python3.6/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy). 278 if state is not None:. 279 if deep:. --> 280 state = deepcopy(state, memo). 281 if hasattr(y, '__setstate__'):. 282 y.__setstate__(state). ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 148 copier = _deepcopy_dispatch.get(cls). 149 if copier:. --> 150 y = copier(x, memo). 151 else:. 152 try:. ~/anaconda3/lib/python3.6/copy.py in _deepcopy_dict(x, memo, deepcopy). 238 memo[id(x)] = y. 239 for key, value in x.items():. --> 240 y[deepcopy(key, memo)] = deepcopy(value, memo). 241 return y. 242 d[dict] = _deepcopy_dict. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 178 y = x. 179 else:. --> 180 y = _reconstruct(x, memo, *rv). 181 . 182 # If is its own copy, don't memoize. ~/anaconda3/lib/python3.6/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy). 278 if state is not None:. 279 if deep:. --> 280 state = deepcopy(state, memo). 281 if hasattr(y, '__setstate__'):. 282 y.__setstate__(state). ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 148 copier = _deepcopy_dispatch.get(cls). 149 if copier:. --> 150 y = copier(x, memo). 151 else:. 152 try:. ~/anaconda3/lib/python3.6/copy.py in _deepcopy_dict(x, memo, deepcopy). 238 memo[id(x)] = y. 239 for key, value in x.items():. --> 240 y[deepcopy(key, memo)] = deepcopy(value, memo). 241 return y. 242 d[dict] = _deepcopy_dict. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 178 y = x. 179 else:. --> 180 y = _reconstruct(x, memo, *rv). 181 . 182 # If is its own copy, don't memoize. ~/anaconda3/lib/python3.6/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy). 305 key = deepcopy(key, memo). 306 value = deepcopy(value, memo). --> 307 y[key] = value. 308 else:. 309 for key, value in dictiter:. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __setitem__(self, idx, value). 437 else",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:5152,availability,state,state,5152,". ~/anaconda3/lib/python3.6/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy). 278 if state is not None:. 279 if deep:. --> 280 state = deepcopy(state, memo). 281 if hasattr(y, '__setstate__'):. 282 y.__setstate__(state). ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 148 copier = _deepcopy_dispatch.get(cls). 149 if copier:. --> 150 y = copier(x, memo). 151 else:. 152 try:. ~/anaconda3/lib/python3.6/copy.py in _deepcopy_dict(x, memo, deepcopy). 238 memo[id(x)] = y. 239 for key, value in x.items():. --> 240 y[deepcopy(key, memo)] = deepcopy(value, memo). 241 return y. 242 d[dict] = _deepcopy_dict. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 178 y = x. 179 else:. --> 180 y = _reconstruct(x, memo, *rv). 181 . 182 # If is its own copy, don't memoize. ~/anaconda3/lib/python3.6/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy). 278 if state is not None:. 279 if deep:. --> 280 state = deepcopy(state, memo). 281 if hasattr(y, '__setstate__'):. 282 y.__setstate__(state). ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 148 copier = _deepcopy_dispatch.get(cls). 149 if copier:. --> 150 y = copier(x, memo). 151 else:. 152 try:. ~/anaconda3/lib/python3.6/copy.py in _deepcopy_dict(x, memo, deepcopy). 238 memo[id(x)] = y. 239 for key, value in x.items():. --> 240 y[deepcopy(key, memo)] = deepcopy(value, memo). 241 return y. 242 d[dict] = _deepcopy_dict. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 178 y = x. 179 else:. --> 180 y = _reconstruct(x, memo, *rv). 181 . 182 # If is its own copy, don't memoize. ~/anaconda3/lib/python3.6/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy). 305 key = deepcopy(key, memo). 306 value = deepcopy(value, memo). --> 307 y[key] = value. 308 else:. 309 for key, value in dictiter:. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __setitem__(self, idx, value). 437 else:. 438 adata_view",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:5221,availability,state,state,5221,"gs, state, listiter, dictiter, deepcopy). 278 if state is not None:. 279 if deep:. --> 280 state = deepcopy(state, memo). 281 if hasattr(y, '__setstate__'):. 282 y.__setstate__(state). ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 148 copier = _deepcopy_dispatch.get(cls). 149 if copier:. --> 150 y = copier(x, memo). 151 else:. 152 try:. ~/anaconda3/lib/python3.6/copy.py in _deepcopy_dict(x, memo, deepcopy). 238 memo[id(x)] = y. 239 for key, value in x.items():. --> 240 y[deepcopy(key, memo)] = deepcopy(value, memo). 241 return y. 242 d[dict] = _deepcopy_dict. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 178 y = x. 179 else:. --> 180 y = _reconstruct(x, memo, *rv). 181 . 182 # If is its own copy, don't memoize. ~/anaconda3/lib/python3.6/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy). 278 if state is not None:. 279 if deep:. --> 280 state = deepcopy(state, memo). 281 if hasattr(y, '__setstate__'):. 282 y.__setstate__(state). ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 148 copier = _deepcopy_dispatch.get(cls). 149 if copier:. --> 150 y = copier(x, memo). 151 else:. 152 try:. ~/anaconda3/lib/python3.6/copy.py in _deepcopy_dict(x, memo, deepcopy). 238 memo[id(x)] = y. 239 for key, value in x.items():. --> 240 y[deepcopy(key, memo)] = deepcopy(value, memo). 241 return y. 242 d[dict] = _deepcopy_dict. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 178 y = x. 179 else:. --> 180 y = _reconstruct(x, memo, *rv). 181 . 182 # If is its own copy, don't memoize. ~/anaconda3/lib/python3.6/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy). 305 key = deepcopy(key, memo). 306 value = deepcopy(value, memo). --> 307 y[key] = value. 308 else:. 309 for key, value in dictiter:. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __setitem__(self, idx, value). 437 else:. 438 adata_view, attr_name = self._view_args. --> 439 _init_actual_AnnData(adata_vie",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:5868,availability,state,state,5868," y = x. 179 else:. --> 180 y = _reconstruct(x, memo, *rv). 181 . 182 # If is its own copy, don't memoize. ~/anaconda3/lib/python3.6/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy). 278 if state is not None:. 279 if deep:. --> 280 state = deepcopy(state, memo). 281 if hasattr(y, '__setstate__'):. 282 y.__setstate__(state). ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 148 copier = _deepcopy_dispatch.get(cls). 149 if copier:. --> 150 y = copier(x, memo). 151 else:. 152 try:. ~/anaconda3/lib/python3.6/copy.py in _deepcopy_dict(x, memo, deepcopy). 238 memo[id(x)] = y. 239 for key, value in x.items():. --> 240 y[deepcopy(key, memo)] = deepcopy(value, memo). 241 return y. 242 d[dict] = _deepcopy_dict. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 178 y = x. 179 else:. --> 180 y = _reconstruct(x, memo, *rv). 181 . 182 # If is its own copy, don't memoize. ~/anaconda3/lib/python3.6/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy). 305 key = deepcopy(key, memo). 306 value = deepcopy(value, memo). --> 307 y[key] = value. 308 else:. 309 for key, value in dictiter:. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __setitem__(self, idx, value). 437 else:. 438 adata_view, attr_name = self._view_args. --> 439 _init_actual_AnnData(adata_view). 440 getattr(adata_view, attr_name)[idx] = value. 441 . ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _init_actual_AnnData(adata_view). 355 . 356 def _init_actual_AnnData(adata_view):. --> 357 if adata_view.isbacked:. 358 raise ValueError(. 359 'You cannot modify elements of an AnnData view, '. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in isbacked(self). 1195 def isbacked(self):. 1196 """"""``True`` if object is backed on disk, ``False`` otherwise."""""". -> 1197 return self.filename is not None. 1198 . 1199 @property. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in filename(self). 1211 want to copy the p",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:355,deployability,modul,module,355,"@falexwolf do you want a object? the fix was working until today, now even after removing all `_color` from `.uns` I still have no success... <details>. <summary>Traceback</summary>. ```pytb. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-12-a0821f8568d8> in <module>(). 22 axs[i].set_title(c). 23 # plot_boxplot_nonzero(tiss[tiss.obs['cell_ontology_class']==cell,:],geneofinterest,'age',axs[i],show = False). ---> 24 plot_boxplot_cell_fraction(tiss[tiss.obs['auto_cell_ontology_class']==c],geneofinterest,'age',c,axs[i],show = False). 25 print(c + ' is done!'). 26 i = i+1. <ipython-input-10-40fc020a5ff4> in plot_boxplot_cell_fraction(adata, gene, label, title, ax, show). 1 def plot_boxplot_cell_fraction(adata, gene, label, title, ax, show=True):. ----> 2 gene_vals = np.asarray(adata[:, gene].X).flatten(). 3 . 4 labels = ['3m','24m']. 5 # labels = list(set(adata.obs[label])). ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). 1299 def __getitem__(self, index):. 1300 """"""Returns a sliced view of the object."""""". -> 1301 return self._getitem_view(index). 1302 . 1303 def _getitem_view(self, index):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _getitem_view(self, index). 1303 def _getitem_view(self, index):. 1304 oidx, vidx = self._normalize_indices(index). -> 1305 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1306 . 1307 def _remove_unused_categories(self, df_full, df_sub, uns):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 662 if not isinstance(X, AnnData):. 663 raise ValueError('`X` has to be an AnnData object.'). --> 664 self._init_as_view(X, oidx, vidx). 665 else:. 666 self._init_as_actual(. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _init_as_view(self, adata_ref, oidx, vidx). 691",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:2240,deployability,updat,updated,2240,"a3/lib/python3.6/site-packages/anndata/base.py in _getitem_view(self, index). 1303 def _getitem_view(self, index):. 1304 oidx, vidx = self._normalize_indices(index). -> 1305 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1306 . 1307 def _remove_unused_categories(self, df_full, df_sub, uns):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 662 if not isinstance(X, AnnData):. 663 raise ValueError('`X` has to be an AnnData object.'). --> 664 self._init_as_view(X, oidx, vidx). 665 else:. 666 self._init_as_actual(. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _init_as_view(self, adata_ref, oidx, vidx). 691 self._varm = ArrayView(adata_ref.varm[vidx_normalized], view_args=(self, 'varm')). 692 # hackish solution here, no copy should be necessary. --> 693 uns_new = deepcopy(self._adata_ref._uns). 694 # need to do the slicing before setting the updated self._n_obs, self._n_vars. 695 self._n_obs = self._adata_ref.n_obs # use the original n_obs here. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 178 y = x. 179 else:. --> 180 y = _reconstruct(x, memo, *rv). 181 . 182 # If is its own copy, don't memoize. ~/anaconda3/lib/python3.6/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy). 278 if state is not None:. 279 if deep:. --> 280 state = deepcopy(state, memo). 281 if hasattr(y, '__setstate__'):. 282 y.__setstate__(state). ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 148 copier = _deepcopy_dispatch.get(cls). 149 if copier:. --> 150 y = copier(x, memo). 151 else:. 152 try:. ~/anaconda3/lib/python3.6/copy.py in _deepcopy_dict(x, memo, deepcopy). 238 memo[id(x)] = y. 239 for key, value in x.items():. --> 240 y[deepcopy(key, memo)] = deepcopy(value, memo). 241 return y. 242 d[dict] = _deepcopy_dict. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 148 copier = _de",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:3632,deployability,contain,contains,3632,"te is not None:. 279 if deep:. --> 280 state = deepcopy(state, memo). 281 if hasattr(y, '__setstate__'):. 282 y.__setstate__(state). ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 148 copier = _deepcopy_dispatch.get(cls). 149 if copier:. --> 150 y = copier(x, memo). 151 else:. 152 try:. ~/anaconda3/lib/python3.6/copy.py in _deepcopy_dict(x, memo, deepcopy). 238 memo[id(x)] = y. 239 for key, value in x.items():. --> 240 y[deepcopy(key, memo)] = deepcopy(value, memo). 241 return y. 242 d[dict] = _deepcopy_dict. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 148 copier = _deepcopy_dispatch.get(cls). 149 if copier:. --> 150 y = copier(x, memo). 151 else:. 152 try:. ~/anaconda3/lib/python3.6/copy.py in _deepcopy_tuple(x, memo, deepcopy). 218 . 219 def _deepcopy_tuple(x, memo, deepcopy=deepcopy):. --> 220 y = [deepcopy(a, memo) for a in x]. 221 # We're not going to put the tuple in the memo, but it's still important we. 222 # check for it, in case the tuple contains recursive mutable structures. ~/anaconda3/lib/python3.6/copy.py in <listcomp>(.0). 218 . 219 def _deepcopy_tuple(x, memo, deepcopy=deepcopy):. --> 220 y = [deepcopy(a, memo) for a in x]. 221 # We're not going to put the tuple in the memo, but it's still important we. 222 # check for it, in case the tuple contains recursive mutable structures. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 178 y = x. 179 else:. --> 180 y = _reconstruct(x, memo, *rv). 181 . 182 # If is its own copy, don't memoize. ~/anaconda3/lib/python3.6/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy). 278 if state is not None:. 279 if deep:. --> 280 state = deepcopy(state, memo). 281 if hasattr(y, '__setstate__'):. 282 y.__setstate__(state). ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 148 copier = _deepcopy_dispatch.get(cls). 149 if copier:. --> 150 y = copier(x, memo). 151 else:. 152 try:. ~/anaconda3/lib/python3.6/copy.py in _deepcopy_dict(x, me",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:3947,deployability,contain,contains,3947,"lib/python3.6/copy.py in _deepcopy_dict(x, memo, deepcopy). 238 memo[id(x)] = y. 239 for key, value in x.items():. --> 240 y[deepcopy(key, memo)] = deepcopy(value, memo). 241 return y. 242 d[dict] = _deepcopy_dict. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 148 copier = _deepcopy_dispatch.get(cls). 149 if copier:. --> 150 y = copier(x, memo). 151 else:. 152 try:. ~/anaconda3/lib/python3.6/copy.py in _deepcopy_tuple(x, memo, deepcopy). 218 . 219 def _deepcopy_tuple(x, memo, deepcopy=deepcopy):. --> 220 y = [deepcopy(a, memo) for a in x]. 221 # We're not going to put the tuple in the memo, but it's still important we. 222 # check for it, in case the tuple contains recursive mutable structures. ~/anaconda3/lib/python3.6/copy.py in <listcomp>(.0). 218 . 219 def _deepcopy_tuple(x, memo, deepcopy=deepcopy):. --> 220 y = [deepcopy(a, memo) for a in x]. 221 # We're not going to put the tuple in the memo, but it's still important we. 222 # check for it, in case the tuple contains recursive mutable structures. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 178 y = x. 179 else:. --> 180 y = _reconstruct(x, memo, *rv). 181 . 182 # If is its own copy, don't memoize. ~/anaconda3/lib/python3.6/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy). 278 if state is not None:. 279 if deep:. --> 280 state = deepcopy(state, memo). 281 if hasattr(y, '__setstate__'):. 282 y.__setstate__(state). ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 148 copier = _deepcopy_dispatch.get(cls). 149 if copier:. --> 150 y = copier(x, memo). 151 else:. 152 try:. ~/anaconda3/lib/python3.6/copy.py in _deepcopy_dict(x, memo, deepcopy). 238 memo[id(x)] = y. 239 for key, value in x.items():. --> 240 y[deepcopy(key, memo)] = deepcopy(value, memo). 241 return y. 242 d[dict] = _deepcopy_dict. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 178 y = x. 179 else:. --> 180 y = _reconstruct(x, memo, *rv). 181 . 182 # If is its",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:2588,integrability,state,state,2588,"ata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 662 if not isinstance(X, AnnData):. 663 raise ValueError('`X` has to be an AnnData object.'). --> 664 self._init_as_view(X, oidx, vidx). 665 else:. 666 self._init_as_actual(. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _init_as_view(self, adata_ref, oidx, vidx). 691 self._varm = ArrayView(adata_ref.varm[vidx_normalized], view_args=(self, 'varm')). 692 # hackish solution here, no copy should be necessary. --> 693 uns_new = deepcopy(self._adata_ref._uns). 694 # need to do the slicing before setting the updated self._n_obs, self._n_vars. 695 self._n_obs = self._adata_ref.n_obs # use the original n_obs here. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 178 y = x. 179 else:. --> 180 y = _reconstruct(x, memo, *rv). 181 . 182 # If is its own copy, don't memoize. ~/anaconda3/lib/python3.6/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy). 278 if state is not None:. 279 if deep:. --> 280 state = deepcopy(state, memo). 281 if hasattr(y, '__setstate__'):. 282 y.__setstate__(state). ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 148 copier = _deepcopy_dispatch.get(cls). 149 if copier:. --> 150 y = copier(x, memo). 151 else:. 152 try:. ~/anaconda3/lib/python3.6/copy.py in _deepcopy_dict(x, memo, deepcopy). 238 memo[id(x)] = y. 239 for key, value in x.items():. --> 240 y[deepcopy(key, memo)] = deepcopy(value, memo). 241 return y. 242 d[dict] = _deepcopy_dict. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 148 copier = _deepcopy_dispatch.get(cls). 149 if copier:. --> 150 y = copier(x, memo). 151 else:. 152 try:. ~/anaconda3/lib/python3.6/copy.py in _deepcopy_tuple(x, memo, deepcopy). 218 . 219 def _deepcopy_tuple(x, memo, deepcopy=deepcopy):. --> 220 y = [deepcopy(a, memo) for a in x]. 221 # We're not going to put the tuple in the memo, but it's still important w",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:2633,integrability,state,state,2633,"s, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 662 if not isinstance(X, AnnData):. 663 raise ValueError('`X` has to be an AnnData object.'). --> 664 self._init_as_view(X, oidx, vidx). 665 else:. 666 self._init_as_actual(. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _init_as_view(self, adata_ref, oidx, vidx). 691 self._varm = ArrayView(adata_ref.varm[vidx_normalized], view_args=(self, 'varm')). 692 # hackish solution here, no copy should be necessary. --> 693 uns_new = deepcopy(self._adata_ref._uns). 694 # need to do the slicing before setting the updated self._n_obs, self._n_vars. 695 self._n_obs = self._adata_ref.n_obs # use the original n_obs here. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 178 y = x. 179 else:. --> 180 y = _reconstruct(x, memo, *rv). 181 . 182 # If is its own copy, don't memoize. ~/anaconda3/lib/python3.6/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy). 278 if state is not None:. 279 if deep:. --> 280 state = deepcopy(state, memo). 281 if hasattr(y, '__setstate__'):. 282 y.__setstate__(state). ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 148 copier = _deepcopy_dispatch.get(cls). 149 if copier:. --> 150 y = copier(x, memo). 151 else:. 152 try:. ~/anaconda3/lib/python3.6/copy.py in _deepcopy_dict(x, memo, deepcopy). 238 memo[id(x)] = y. 239 for key, value in x.items():. --> 240 y[deepcopy(key, memo)] = deepcopy(value, memo). 241 return y. 242 d[dict] = _deepcopy_dict. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 148 copier = _deepcopy_dispatch.get(cls). 149 if copier:. --> 150 y = copier(x, memo). 151 else:. 152 try:. ~/anaconda3/lib/python3.6/copy.py in _deepcopy_tuple(x, memo, deepcopy). 218 . 219 def _deepcopy_tuple(x, memo, deepcopy=deepcopy):. --> 220 y = [deepcopy(a, memo) for a in x]. 221 # We're not going to put the tuple in the memo, but it's still important we. 222 # check for it, in case the tuple cont",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:2675,integrability,state,state,2675,"filename, filemode, asview, oidx, vidx). 662 if not isinstance(X, AnnData):. 663 raise ValueError('`X` has to be an AnnData object.'). --> 664 self._init_as_view(X, oidx, vidx). 665 else:. 666 self._init_as_actual(. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _init_as_view(self, adata_ref, oidx, vidx). 691 self._varm = ArrayView(adata_ref.varm[vidx_normalized], view_args=(self, 'varm')). 692 # hackish solution here, no copy should be necessary. --> 693 uns_new = deepcopy(self._adata_ref._uns). 694 # need to do the slicing before setting the updated self._n_obs, self._n_vars. 695 self._n_obs = self._adata_ref.n_obs # use the original n_obs here. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 178 y = x. 179 else:. --> 180 y = _reconstruct(x, memo, *rv). 181 . 182 # If is its own copy, don't memoize. ~/anaconda3/lib/python3.6/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy). 278 if state is not None:. 279 if deep:. --> 280 state = deepcopy(state, memo). 281 if hasattr(y, '__setstate__'):. 282 y.__setstate__(state). ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 148 copier = _deepcopy_dispatch.get(cls). 149 if copier:. --> 150 y = copier(x, memo). 151 else:. 152 try:. ~/anaconda3/lib/python3.6/copy.py in _deepcopy_dict(x, memo, deepcopy). 238 memo[id(x)] = y. 239 for key, value in x.items():. --> 240 y[deepcopy(key, memo)] = deepcopy(value, memo). 241 return y. 242 d[dict] = _deepcopy_dict. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 148 copier = _deepcopy_dispatch.get(cls). 149 if copier:. --> 150 y = copier(x, memo). 151 else:. 152 try:. ~/anaconda3/lib/python3.6/copy.py in _deepcopy_tuple(x, memo, deepcopy). 218 . 219 def _deepcopy_tuple(x, memo, deepcopy=deepcopy):. --> 220 y = [deepcopy(a, memo) for a in x]. 221 # We're not going to put the tuple in the memo, but it's still important we. 222 # check for it, in case the tuple contains recursive mutable structures. ~/anaco",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:2692,integrability,state,state,2692,"e, asview, oidx, vidx). 662 if not isinstance(X, AnnData):. 663 raise ValueError('`X` has to be an AnnData object.'). --> 664 self._init_as_view(X, oidx, vidx). 665 else:. 666 self._init_as_actual(. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _init_as_view(self, adata_ref, oidx, vidx). 691 self._varm = ArrayView(adata_ref.varm[vidx_normalized], view_args=(self, 'varm')). 692 # hackish solution here, no copy should be necessary. --> 693 uns_new = deepcopy(self._adata_ref._uns). 694 # need to do the slicing before setting the updated self._n_obs, self._n_vars. 695 self._n_obs = self._adata_ref.n_obs # use the original n_obs here. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 178 y = x. 179 else:. --> 180 y = _reconstruct(x, memo, *rv). 181 . 182 # If is its own copy, don't memoize. ~/anaconda3/lib/python3.6/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy). 278 if state is not None:. 279 if deep:. --> 280 state = deepcopy(state, memo). 281 if hasattr(y, '__setstate__'):. 282 y.__setstate__(state). ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 148 copier = _deepcopy_dispatch.get(cls). 149 if copier:. --> 150 y = copier(x, memo). 151 else:. 152 try:. ~/anaconda3/lib/python3.6/copy.py in _deepcopy_dict(x, memo, deepcopy). 238 memo[id(x)] = y. 239 for key, value in x.items():. --> 240 y[deepcopy(key, memo)] = deepcopy(value, memo). 241 return y. 242 d[dict] = _deepcopy_dict. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 148 copier = _deepcopy_dispatch.get(cls). 149 if copier:. --> 150 y = copier(x, memo). 151 else:. 152 try:. ~/anaconda3/lib/python3.6/copy.py in _deepcopy_tuple(x, memo, deepcopy). 218 . 219 def _deepcopy_tuple(x, memo, deepcopy=deepcopy):. --> 220 y = [deepcopy(a, memo) for a in x]. 221 # We're not going to put the tuple in the memo, but it's still important we. 222 # check for it, in case the tuple contains recursive mutable structures. ~/anaconda3/lib/python3.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:2761,integrability,state,state,2761," ValueError('`X` has to be an AnnData object.'). --> 664 self._init_as_view(X, oidx, vidx). 665 else:. 666 self._init_as_actual(. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _init_as_view(self, adata_ref, oidx, vidx). 691 self._varm = ArrayView(adata_ref.varm[vidx_normalized], view_args=(self, 'varm')). 692 # hackish solution here, no copy should be necessary. --> 693 uns_new = deepcopy(self._adata_ref._uns). 694 # need to do the slicing before setting the updated self._n_obs, self._n_vars. 695 self._n_obs = self._adata_ref.n_obs # use the original n_obs here. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 178 y = x. 179 else:. --> 180 y = _reconstruct(x, memo, *rv). 181 . 182 # If is its own copy, don't memoize. ~/anaconda3/lib/python3.6/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy). 278 if state is not None:. 279 if deep:. --> 280 state = deepcopy(state, memo). 281 if hasattr(y, '__setstate__'):. 282 y.__setstate__(state). ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 148 copier = _deepcopy_dispatch.get(cls). 149 if copier:. --> 150 y = copier(x, memo). 151 else:. 152 try:. ~/anaconda3/lib/python3.6/copy.py in _deepcopy_dict(x, memo, deepcopy). 238 memo[id(x)] = y. 239 for key, value in x.items():. --> 240 y[deepcopy(key, memo)] = deepcopy(value, memo). 241 return y. 242 d[dict] = _deepcopy_dict. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 148 copier = _deepcopy_dispatch.get(cls). 149 if copier:. --> 150 y = copier(x, memo). 151 else:. 152 try:. ~/anaconda3/lib/python3.6/copy.py in _deepcopy_tuple(x, memo, deepcopy). 218 . 219 def _deepcopy_tuple(x, memo, deepcopy=deepcopy):. --> 220 y = [deepcopy(a, memo) for a in x]. 221 # We're not going to put the tuple in the memo, but it's still important we. 222 # check for it, in case the tuple contains recursive mutable structures. ~/anaconda3/lib/python3.6/copy.py in <listcomp>(.0). 218 . 219 def _deepcopy_tuple(x, memo, d",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:4228,integrability,state,state,4228," copier = _deepcopy_dispatch.get(cls). 149 if copier:. --> 150 y = copier(x, memo). 151 else:. 152 try:. ~/anaconda3/lib/python3.6/copy.py in _deepcopy_tuple(x, memo, deepcopy). 218 . 219 def _deepcopy_tuple(x, memo, deepcopy=deepcopy):. --> 220 y = [deepcopy(a, memo) for a in x]. 221 # We're not going to put the tuple in the memo, but it's still important we. 222 # check for it, in case the tuple contains recursive mutable structures. ~/anaconda3/lib/python3.6/copy.py in <listcomp>(.0). 218 . 219 def _deepcopy_tuple(x, memo, deepcopy=deepcopy):. --> 220 y = [deepcopy(a, memo) for a in x]. 221 # We're not going to put the tuple in the memo, but it's still important we. 222 # check for it, in case the tuple contains recursive mutable structures. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 178 y = x. 179 else:. --> 180 y = _reconstruct(x, memo, *rv). 181 . 182 # If is its own copy, don't memoize. ~/anaconda3/lib/python3.6/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy). 278 if state is not None:. 279 if deep:. --> 280 state = deepcopy(state, memo). 281 if hasattr(y, '__setstate__'):. 282 y.__setstate__(state). ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 148 copier = _deepcopy_dispatch.get(cls). 149 if copier:. --> 150 y = copier(x, memo). 151 else:. 152 try:. ~/anaconda3/lib/python3.6/copy.py in _deepcopy_dict(x, memo, deepcopy). 238 memo[id(x)] = y. 239 for key, value in x.items():. --> 240 y[deepcopy(key, memo)] = deepcopy(value, memo). 241 return y. 242 d[dict] = _deepcopy_dict. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 178 y = x. 179 else:. --> 180 y = _reconstruct(x, memo, *rv). 181 . 182 # If is its own copy, don't memoize. ~/anaconda3/lib/python3.6/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy). 278 if state is not None:. 279 if deep:. --> 280 state = deepcopy(state, memo). 281 if hasattr(y, '__setstate__'):. 282 y.__setstate__(state). ~/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:4273,integrability,state,state,4273," copier:. --> 150 y = copier(x, memo). 151 else:. 152 try:. ~/anaconda3/lib/python3.6/copy.py in _deepcopy_tuple(x, memo, deepcopy). 218 . 219 def _deepcopy_tuple(x, memo, deepcopy=deepcopy):. --> 220 y = [deepcopy(a, memo) for a in x]. 221 # We're not going to put the tuple in the memo, but it's still important we. 222 # check for it, in case the tuple contains recursive mutable structures. ~/anaconda3/lib/python3.6/copy.py in <listcomp>(.0). 218 . 219 def _deepcopy_tuple(x, memo, deepcopy=deepcopy):. --> 220 y = [deepcopy(a, memo) for a in x]. 221 # We're not going to put the tuple in the memo, but it's still important we. 222 # check for it, in case the tuple contains recursive mutable structures. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 178 y = x. 179 else:. --> 180 y = _reconstruct(x, memo, *rv). 181 . 182 # If is its own copy, don't memoize. ~/anaconda3/lib/python3.6/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy). 278 if state is not None:. 279 if deep:. --> 280 state = deepcopy(state, memo). 281 if hasattr(y, '__setstate__'):. 282 y.__setstate__(state). ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 148 copier = _deepcopy_dispatch.get(cls). 149 if copier:. --> 150 y = copier(x, memo). 151 else:. 152 try:. ~/anaconda3/lib/python3.6/copy.py in _deepcopy_dict(x, memo, deepcopy). 238 memo[id(x)] = y. 239 for key, value in x.items():. --> 240 y[deepcopy(key, memo)] = deepcopy(value, memo). 241 return y. 242 d[dict] = _deepcopy_dict. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 178 y = x. 179 else:. --> 180 y = _reconstruct(x, memo, *rv). 181 . 182 # If is its own copy, don't memoize. ~/anaconda3/lib/python3.6/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy). 278 if state is not None:. 279 if deep:. --> 280 state = deepcopy(state, memo). 281 if hasattr(y, '__setstate__'):. 282 y.__setstate__(state). ~/anaconda3/lib/python3.6/copy.py in deepcopy(x",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:4315,integrability,state,state,4315," else:. 152 try:. ~/anaconda3/lib/python3.6/copy.py in _deepcopy_tuple(x, memo, deepcopy). 218 . 219 def _deepcopy_tuple(x, memo, deepcopy=deepcopy):. --> 220 y = [deepcopy(a, memo) for a in x]. 221 # We're not going to put the tuple in the memo, but it's still important we. 222 # check for it, in case the tuple contains recursive mutable structures. ~/anaconda3/lib/python3.6/copy.py in <listcomp>(.0). 218 . 219 def _deepcopy_tuple(x, memo, deepcopy=deepcopy):. --> 220 y = [deepcopy(a, memo) for a in x]. 221 # We're not going to put the tuple in the memo, but it's still important we. 222 # check for it, in case the tuple contains recursive mutable structures. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 178 y = x. 179 else:. --> 180 y = _reconstruct(x, memo, *rv). 181 . 182 # If is its own copy, don't memoize. ~/anaconda3/lib/python3.6/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy). 278 if state is not None:. 279 if deep:. --> 280 state = deepcopy(state, memo). 281 if hasattr(y, '__setstate__'):. 282 y.__setstate__(state). ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 148 copier = _deepcopy_dispatch.get(cls). 149 if copier:. --> 150 y = copier(x, memo). 151 else:. 152 try:. ~/anaconda3/lib/python3.6/copy.py in _deepcopy_dict(x, memo, deepcopy). 238 memo[id(x)] = y. 239 for key, value in x.items():. --> 240 y[deepcopy(key, memo)] = deepcopy(value, memo). 241 return y. 242 d[dict] = _deepcopy_dict. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 178 y = x. 179 else:. --> 180 y = _reconstruct(x, memo, *rv). 181 . 182 # If is its own copy, don't memoize. ~/anaconda3/lib/python3.6/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy). 278 if state is not None:. 279 if deep:. --> 280 state = deepcopy(state, memo). 281 if hasattr(y, '__setstate__'):. 282 y.__setstate__(state). ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 148 copier = _deepcopy_disp",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:4332,integrability,state,state,4332," ~/anaconda3/lib/python3.6/copy.py in _deepcopy_tuple(x, memo, deepcopy). 218 . 219 def _deepcopy_tuple(x, memo, deepcopy=deepcopy):. --> 220 y = [deepcopy(a, memo) for a in x]. 221 # We're not going to put the tuple in the memo, but it's still important we. 222 # check for it, in case the tuple contains recursive mutable structures. ~/anaconda3/lib/python3.6/copy.py in <listcomp>(.0). 218 . 219 def _deepcopy_tuple(x, memo, deepcopy=deepcopy):. --> 220 y = [deepcopy(a, memo) for a in x]. 221 # We're not going to put the tuple in the memo, but it's still important we. 222 # check for it, in case the tuple contains recursive mutable structures. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 178 y = x. 179 else:. --> 180 y = _reconstruct(x, memo, *rv). 181 . 182 # If is its own copy, don't memoize. ~/anaconda3/lib/python3.6/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy). 278 if state is not None:. 279 if deep:. --> 280 state = deepcopy(state, memo). 281 if hasattr(y, '__setstate__'):. 282 y.__setstate__(state). ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 148 copier = _deepcopy_dispatch.get(cls). 149 if copier:. --> 150 y = copier(x, memo). 151 else:. 152 try:. ~/anaconda3/lib/python3.6/copy.py in _deepcopy_dict(x, memo, deepcopy). 238 memo[id(x)] = y. 239 for key, value in x.items():. --> 240 y[deepcopy(key, memo)] = deepcopy(value, memo). 241 return y. 242 d[dict] = _deepcopy_dict. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 178 y = x. 179 else:. --> 180 y = _reconstruct(x, memo, *rv). 181 . 182 # If is its own copy, don't memoize. ~/anaconda3/lib/python3.6/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy). 278 if state is not None:. 279 if deep:. --> 280 state = deepcopy(state, memo). 281 if hasattr(y, '__setstate__'):. 282 y.__setstate__(state). ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 148 copier = _deepcopy_dispatch.get(cls). 14",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:4401,integrability,state,state,4401,"py). 218 . 219 def _deepcopy_tuple(x, memo, deepcopy=deepcopy):. --> 220 y = [deepcopy(a, memo) for a in x]. 221 # We're not going to put the tuple in the memo, but it's still important we. 222 # check for it, in case the tuple contains recursive mutable structures. ~/anaconda3/lib/python3.6/copy.py in <listcomp>(.0). 218 . 219 def _deepcopy_tuple(x, memo, deepcopy=deepcopy):. --> 220 y = [deepcopy(a, memo) for a in x]. 221 # We're not going to put the tuple in the memo, but it's still important we. 222 # check for it, in case the tuple contains recursive mutable structures. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 178 y = x. 179 else:. --> 180 y = _reconstruct(x, memo, *rv). 181 . 182 # If is its own copy, don't memoize. ~/anaconda3/lib/python3.6/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy). 278 if state is not None:. 279 if deep:. --> 280 state = deepcopy(state, memo). 281 if hasattr(y, '__setstate__'):. 282 y.__setstate__(state). ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 148 copier = _deepcopy_dispatch.get(cls). 149 if copier:. --> 150 y = copier(x, memo). 151 else:. 152 try:. ~/anaconda3/lib/python3.6/copy.py in _deepcopy_dict(x, memo, deepcopy). 238 memo[id(x)] = y. 239 for key, value in x.items():. --> 240 y[deepcopy(key, memo)] = deepcopy(value, memo). 241 return y. 242 d[dict] = _deepcopy_dict. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 178 y = x. 179 else:. --> 180 y = _reconstruct(x, memo, *rv). 181 . 182 # If is its own copy, don't memoize. ~/anaconda3/lib/python3.6/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy). 278 if state is not None:. 279 if deep:. --> 280 state = deepcopy(state, memo). 281 if hasattr(y, '__setstate__'):. 282 y.__setstate__(state). ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 148 copier = _deepcopy_dispatch.get(cls). 149 if copier:. --> 150 y = copier(x, memo). 151 else:. 152 try:. ~/ana",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:5048,integrability,state,state,5048," y = x. 179 else:. --> 180 y = _reconstruct(x, memo, *rv). 181 . 182 # If is its own copy, don't memoize. ~/anaconda3/lib/python3.6/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy). 278 if state is not None:. 279 if deep:. --> 280 state = deepcopy(state, memo). 281 if hasattr(y, '__setstate__'):. 282 y.__setstate__(state). ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 148 copier = _deepcopy_dispatch.get(cls). 149 if copier:. --> 150 y = copier(x, memo). 151 else:. 152 try:. ~/anaconda3/lib/python3.6/copy.py in _deepcopy_dict(x, memo, deepcopy). 238 memo[id(x)] = y. 239 for key, value in x.items():. --> 240 y[deepcopy(key, memo)] = deepcopy(value, memo). 241 return y. 242 d[dict] = _deepcopy_dict. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 178 y = x. 179 else:. --> 180 y = _reconstruct(x, memo, *rv). 181 . 182 # If is its own copy, don't memoize. ~/anaconda3/lib/python3.6/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy). 278 if state is not None:. 279 if deep:. --> 280 state = deepcopy(state, memo). 281 if hasattr(y, '__setstate__'):. 282 y.__setstate__(state). ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 148 copier = _deepcopy_dispatch.get(cls). 149 if copier:. --> 150 y = copier(x, memo). 151 else:. 152 try:. ~/anaconda3/lib/python3.6/copy.py in _deepcopy_dict(x, memo, deepcopy). 238 memo[id(x)] = y. 239 for key, value in x.items():. --> 240 y[deepcopy(key, memo)] = deepcopy(value, memo). 241 return y. 242 d[dict] = _deepcopy_dict. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 178 y = x. 179 else:. --> 180 y = _reconstruct(x, memo, *rv). 181 . 182 # If is its own copy, don't memoize. ~/anaconda3/lib/python3.6/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy). 305 key = deepcopy(key, memo). 306 value = deepcopy(value, memo). --> 307 y[key] = value. 308 else:. 309 for key, value in dictiter:. ~/anaconda3",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:5093,integrability,state,state,5093,", memo, *rv). 181 . 182 # If is its own copy, don't memoize. ~/anaconda3/lib/python3.6/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy). 278 if state is not None:. 279 if deep:. --> 280 state = deepcopy(state, memo). 281 if hasattr(y, '__setstate__'):. 282 y.__setstate__(state). ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 148 copier = _deepcopy_dispatch.get(cls). 149 if copier:. --> 150 y = copier(x, memo). 151 else:. 152 try:. ~/anaconda3/lib/python3.6/copy.py in _deepcopy_dict(x, memo, deepcopy). 238 memo[id(x)] = y. 239 for key, value in x.items():. --> 240 y[deepcopy(key, memo)] = deepcopy(value, memo). 241 return y. 242 d[dict] = _deepcopy_dict. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 178 y = x. 179 else:. --> 180 y = _reconstruct(x, memo, *rv). 181 . 182 # If is its own copy, don't memoize. ~/anaconda3/lib/python3.6/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy). 278 if state is not None:. 279 if deep:. --> 280 state = deepcopy(state, memo). 281 if hasattr(y, '__setstate__'):. 282 y.__setstate__(state). ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 148 copier = _deepcopy_dispatch.get(cls). 149 if copier:. --> 150 y = copier(x, memo). 151 else:. 152 try:. ~/anaconda3/lib/python3.6/copy.py in _deepcopy_dict(x, memo, deepcopy). 238 memo[id(x)] = y. 239 for key, value in x.items():. --> 240 y[deepcopy(key, memo)] = deepcopy(value, memo). 241 return y. 242 d[dict] = _deepcopy_dict. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 178 y = x. 179 else:. --> 180 y = _reconstruct(x, memo, *rv). 181 . 182 # If is its own copy, don't memoize. ~/anaconda3/lib/python3.6/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy). 305 key = deepcopy(key, memo). 306 value = deepcopy(value, memo). --> 307 y[key] = value. 308 else:. 309 for key, value in dictiter:. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:5135,integrability,state,state,5135,"py, don't memoize. ~/anaconda3/lib/python3.6/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy). 278 if state is not None:. 279 if deep:. --> 280 state = deepcopy(state, memo). 281 if hasattr(y, '__setstate__'):. 282 y.__setstate__(state). ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 148 copier = _deepcopy_dispatch.get(cls). 149 if copier:. --> 150 y = copier(x, memo). 151 else:. 152 try:. ~/anaconda3/lib/python3.6/copy.py in _deepcopy_dict(x, memo, deepcopy). 238 memo[id(x)] = y. 239 for key, value in x.items():. --> 240 y[deepcopy(key, memo)] = deepcopy(value, memo). 241 return y. 242 d[dict] = _deepcopy_dict. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 178 y = x. 179 else:. --> 180 y = _reconstruct(x, memo, *rv). 181 . 182 # If is its own copy, don't memoize. ~/anaconda3/lib/python3.6/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy). 278 if state is not None:. 279 if deep:. --> 280 state = deepcopy(state, memo). 281 if hasattr(y, '__setstate__'):. 282 y.__setstate__(state). ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 148 copier = _deepcopy_dispatch.get(cls). 149 if copier:. --> 150 y = copier(x, memo). 151 else:. 152 try:. ~/anaconda3/lib/python3.6/copy.py in _deepcopy_dict(x, memo, deepcopy). 238 memo[id(x)] = y. 239 for key, value in x.items():. --> 240 y[deepcopy(key, memo)] = deepcopy(value, memo). 241 return y. 242 d[dict] = _deepcopy_dict. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 178 y = x. 179 else:. --> 180 y = _reconstruct(x, memo, *rv). 181 . 182 # If is its own copy, don't memoize. ~/anaconda3/lib/python3.6/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy). 305 key = deepcopy(key, memo). 306 value = deepcopy(value, memo). --> 307 y[key] = value. 308 else:. 309 for key, value in dictiter:. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __setitem__(self, idx, value). 437 else",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:5152,integrability,state,state,5152,". ~/anaconda3/lib/python3.6/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy). 278 if state is not None:. 279 if deep:. --> 280 state = deepcopy(state, memo). 281 if hasattr(y, '__setstate__'):. 282 y.__setstate__(state). ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 148 copier = _deepcopy_dispatch.get(cls). 149 if copier:. --> 150 y = copier(x, memo). 151 else:. 152 try:. ~/anaconda3/lib/python3.6/copy.py in _deepcopy_dict(x, memo, deepcopy). 238 memo[id(x)] = y. 239 for key, value in x.items():. --> 240 y[deepcopy(key, memo)] = deepcopy(value, memo). 241 return y. 242 d[dict] = _deepcopy_dict. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 178 y = x. 179 else:. --> 180 y = _reconstruct(x, memo, *rv). 181 . 182 # If is its own copy, don't memoize. ~/anaconda3/lib/python3.6/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy). 278 if state is not None:. 279 if deep:. --> 280 state = deepcopy(state, memo). 281 if hasattr(y, '__setstate__'):. 282 y.__setstate__(state). ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 148 copier = _deepcopy_dispatch.get(cls). 149 if copier:. --> 150 y = copier(x, memo). 151 else:. 152 try:. ~/anaconda3/lib/python3.6/copy.py in _deepcopy_dict(x, memo, deepcopy). 238 memo[id(x)] = y. 239 for key, value in x.items():. --> 240 y[deepcopy(key, memo)] = deepcopy(value, memo). 241 return y. 242 d[dict] = _deepcopy_dict. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 178 y = x. 179 else:. --> 180 y = _reconstruct(x, memo, *rv). 181 . 182 # If is its own copy, don't memoize. ~/anaconda3/lib/python3.6/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy). 305 key = deepcopy(key, memo). 306 value = deepcopy(value, memo). --> 307 y[key] = value. 308 else:. 309 for key, value in dictiter:. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __setitem__(self, idx, value). 437 else:. 438 adata_view",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:5221,integrability,state,state,5221,"gs, state, listiter, dictiter, deepcopy). 278 if state is not None:. 279 if deep:. --> 280 state = deepcopy(state, memo). 281 if hasattr(y, '__setstate__'):. 282 y.__setstate__(state). ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 148 copier = _deepcopy_dispatch.get(cls). 149 if copier:. --> 150 y = copier(x, memo). 151 else:. 152 try:. ~/anaconda3/lib/python3.6/copy.py in _deepcopy_dict(x, memo, deepcopy). 238 memo[id(x)] = y. 239 for key, value in x.items():. --> 240 y[deepcopy(key, memo)] = deepcopy(value, memo). 241 return y. 242 d[dict] = _deepcopy_dict. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 178 y = x. 179 else:. --> 180 y = _reconstruct(x, memo, *rv). 181 . 182 # If is its own copy, don't memoize. ~/anaconda3/lib/python3.6/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy). 278 if state is not None:. 279 if deep:. --> 280 state = deepcopy(state, memo). 281 if hasattr(y, '__setstate__'):. 282 y.__setstate__(state). ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 148 copier = _deepcopy_dispatch.get(cls). 149 if copier:. --> 150 y = copier(x, memo). 151 else:. 152 try:. ~/anaconda3/lib/python3.6/copy.py in _deepcopy_dict(x, memo, deepcopy). 238 memo[id(x)] = y. 239 for key, value in x.items():. --> 240 y[deepcopy(key, memo)] = deepcopy(value, memo). 241 return y. 242 d[dict] = _deepcopy_dict. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 178 y = x. 179 else:. --> 180 y = _reconstruct(x, memo, *rv). 181 . 182 # If is its own copy, don't memoize. ~/anaconda3/lib/python3.6/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy). 305 key = deepcopy(key, memo). 306 value = deepcopy(value, memo). --> 307 y[key] = value. 308 else:. 309 for key, value in dictiter:. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __setitem__(self, idx, value). 437 else:. 438 adata_view, attr_name = self._view_args. --> 439 _init_actual_AnnData(adata_vie",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:5868,integrability,state,state,5868," y = x. 179 else:. --> 180 y = _reconstruct(x, memo, *rv). 181 . 182 # If is its own copy, don't memoize. ~/anaconda3/lib/python3.6/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy). 278 if state is not None:. 279 if deep:. --> 280 state = deepcopy(state, memo). 281 if hasattr(y, '__setstate__'):. 282 y.__setstate__(state). ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 148 copier = _deepcopy_dispatch.get(cls). 149 if copier:. --> 150 y = copier(x, memo). 151 else:. 152 try:. ~/anaconda3/lib/python3.6/copy.py in _deepcopy_dict(x, memo, deepcopy). 238 memo[id(x)] = y. 239 for key, value in x.items():. --> 240 y[deepcopy(key, memo)] = deepcopy(value, memo). 241 return y. 242 d[dict] = _deepcopy_dict. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 178 y = x. 179 else:. --> 180 y = _reconstruct(x, memo, *rv). 181 . 182 # If is its own copy, don't memoize. ~/anaconda3/lib/python3.6/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy). 305 key = deepcopy(key, memo). 306 value = deepcopy(value, memo). --> 307 y[key] = value. 308 else:. 309 for key, value in dictiter:. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __setitem__(self, idx, value). 437 else:. 438 adata_view, attr_name = self._view_args. --> 439 _init_actual_AnnData(adata_view). 440 getattr(adata_view, attr_name)[idx] = value. 441 . ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _init_actual_AnnData(adata_view). 355 . 356 def _init_actual_AnnData(adata_view):. --> 357 if adata_view.isbacked:. 358 raise ValueError(. 359 'You cannot modify elements of an AnnData view, '. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in isbacked(self). 1195 def isbacked(self):. 1196 """"""``True`` if object is backed on disk, ``False`` otherwise."""""". -> 1197 return self.filename is not None. 1198 . 1199 @property. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in filename(self). 1211 want to copy the p",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:355,modifiability,modul,module,355,"@falexwolf do you want a object? the fix was working until today, now even after removing all `_color` from `.uns` I still have no success... <details>. <summary>Traceback</summary>. ```pytb. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-12-a0821f8568d8> in <module>(). 22 axs[i].set_title(c). 23 # plot_boxplot_nonzero(tiss[tiss.obs['cell_ontology_class']==cell,:],geneofinterest,'age',axs[i],show = False). ---> 24 plot_boxplot_cell_fraction(tiss[tiss.obs['auto_cell_ontology_class']==c],geneofinterest,'age',c,axs[i],show = False). 25 print(c + ' is done!'). 26 i = i+1. <ipython-input-10-40fc020a5ff4> in plot_boxplot_cell_fraction(adata, gene, label, title, ax, show). 1 def plot_boxplot_cell_fraction(adata, gene, label, title, ax, show=True):. ----> 2 gene_vals = np.asarray(adata[:, gene].X).flatten(). 3 . 4 labels = ['3m','24m']. 5 # labels = list(set(adata.obs[label])). ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). 1299 def __getitem__(self, index):. 1300 """"""Returns a sliced view of the object."""""". -> 1301 return self._getitem_view(index). 1302 . 1303 def _getitem_view(self, index):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _getitem_view(self, index). 1303 def _getitem_view(self, index):. 1304 oidx, vidx = self._normalize_indices(index). -> 1305 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1306 . 1307 def _remove_unused_categories(self, df_full, df_sub, uns):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 662 if not isinstance(X, AnnData):. 663 raise ValueError('`X` has to be an AnnData object.'). --> 664 self._init_as_view(X, oidx, vidx). 665 else:. 666 self._init_as_actual(. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _init_as_view(self, adata_ref, oidx, vidx). 691",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:1009,modifiability,pac,packages,1009," you want a object? the fix was working until today, now even after removing all `_color` from `.uns` I still have no success... <details>. <summary>Traceback</summary>. ```pytb. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-12-a0821f8568d8> in <module>(). 22 axs[i].set_title(c). 23 # plot_boxplot_nonzero(tiss[tiss.obs['cell_ontology_class']==cell,:],geneofinterest,'age',axs[i],show = False). ---> 24 plot_boxplot_cell_fraction(tiss[tiss.obs['auto_cell_ontology_class']==c],geneofinterest,'age',c,axs[i],show = False). 25 print(c + ' is done!'). 26 i = i+1. <ipython-input-10-40fc020a5ff4> in plot_boxplot_cell_fraction(adata, gene, label, title, ax, show). 1 def plot_boxplot_cell_fraction(adata, gene, label, title, ax, show=True):. ----> 2 gene_vals = np.asarray(adata[:, gene].X).flatten(). 3 . 4 labels = ['3m','24m']. 5 # labels = list(set(adata.obs[label])). ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). 1299 def __getitem__(self, index):. 1300 """"""Returns a sliced view of the object."""""". -> 1301 return self._getitem_view(index). 1302 . 1303 def _getitem_view(self, index):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _getitem_view(self, index). 1303 def _getitem_view(self, index):. 1304 oidx, vidx = self._normalize_indices(index). -> 1305 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1306 . 1307 def _remove_unused_categories(self, df_full, df_sub, uns):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 662 if not isinstance(X, AnnData):. 663 raise ValueError('`X` has to be an AnnData object.'). --> 664 self._init_as_view(X, oidx, vidx). 665 else:. 666 self._init_as_actual(. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _init_as_view(self, adata_ref, oidx, vidx). 691 self._varm =",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:1266,modifiability,pac,packages,1266,"ttributeError Traceback (most recent call last). <ipython-input-12-a0821f8568d8> in <module>(). 22 axs[i].set_title(c). 23 # plot_boxplot_nonzero(tiss[tiss.obs['cell_ontology_class']==cell,:],geneofinterest,'age',axs[i],show = False). ---> 24 plot_boxplot_cell_fraction(tiss[tiss.obs['auto_cell_ontology_class']==c],geneofinterest,'age',c,axs[i],show = False). 25 print(c + ' is done!'). 26 i = i+1. <ipython-input-10-40fc020a5ff4> in plot_boxplot_cell_fraction(adata, gene, label, title, ax, show). 1 def plot_boxplot_cell_fraction(adata, gene, label, title, ax, show=True):. ----> 2 gene_vals = np.asarray(adata[:, gene].X).flatten(). 3 . 4 labels = ['3m','24m']. 5 # labels = list(set(adata.obs[label])). ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). 1299 def __getitem__(self, index):. 1300 """"""Returns a sliced view of the object."""""". -> 1301 return self._getitem_view(index). 1302 . 1303 def _getitem_view(self, index):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _getitem_view(self, index). 1303 def _getitem_view(self, index):. 1304 oidx, vidx = self._normalize_indices(index). -> 1305 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1306 . 1307 def _remove_unused_categories(self, df_full, df_sub, uns):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 662 if not isinstance(X, AnnData):. 663 raise ValueError('`X` has to be an AnnData object.'). --> 664 self._init_as_view(X, oidx, vidx). 665 else:. 666 self._init_as_actual(. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _init_as_view(self, adata_ref, oidx, vidx). 691 self._varm = ArrayView(adata_ref.varm[vidx_normalized], view_args=(self, 'varm')). 692 # hackish solution here, no copy should be necessary. --> 693 uns_new = deepcopy(self._adata_ref._uns). 694 # need to do the slicing before setting the updated self._n_obs, self._n_v",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:1578,modifiability,pac,packages,1578,"=c],geneofinterest,'age',c,axs[i],show = False). 25 print(c + ' is done!'). 26 i = i+1. <ipython-input-10-40fc020a5ff4> in plot_boxplot_cell_fraction(adata, gene, label, title, ax, show). 1 def plot_boxplot_cell_fraction(adata, gene, label, title, ax, show=True):. ----> 2 gene_vals = np.asarray(adata[:, gene].X).flatten(). 3 . 4 labels = ['3m','24m']. 5 # labels = list(set(adata.obs[label])). ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). 1299 def __getitem__(self, index):. 1300 """"""Returns a sliced view of the object."""""". -> 1301 return self._getitem_view(index). 1302 . 1303 def _getitem_view(self, index):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _getitem_view(self, index). 1303 def _getitem_view(self, index):. 1304 oidx, vidx = self._normalize_indices(index). -> 1305 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1306 . 1307 def _remove_unused_categories(self, df_full, df_sub, uns):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 662 if not isinstance(X, AnnData):. 663 raise ValueError('`X` has to be an AnnData object.'). --> 664 self._init_as_view(X, oidx, vidx). 665 else:. 666 self._init_as_actual(. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _init_as_view(self, adata_ref, oidx, vidx). 691 self._varm = ArrayView(adata_ref.varm[vidx_normalized], view_args=(self, 'varm')). 692 # hackish solution here, no copy should be necessary. --> 693 uns_new = deepcopy(self._adata_ref._uns). 694 # need to do the slicing before setting the updated self._n_obs, self._n_vars. 695 self._n_obs = self._adata_ref.n_obs # use the original n_obs here. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 178 y = x. 179 else:. --> 180 y = _reconstruct(x, memo, *rv). 181 . 182 # If is its own copy, don't memoize. ~/anaconda3/lib/python3.6/copy.py in _reconstruct(x, memo, func, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:1651,modifiability,layer,layers,1651,"'). 26 i = i+1. <ipython-input-10-40fc020a5ff4> in plot_boxplot_cell_fraction(adata, gene, label, title, ax, show). 1 def plot_boxplot_cell_fraction(adata, gene, label, title, ax, show=True):. ----> 2 gene_vals = np.asarray(adata[:, gene].X).flatten(). 3 . 4 labels = ['3m','24m']. 5 # labels = list(set(adata.obs[label])). ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). 1299 def __getitem__(self, index):. 1300 """"""Returns a sliced view of the object."""""". -> 1301 return self._getitem_view(index). 1302 . 1303 def _getitem_view(self, index):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _getitem_view(self, index). 1303 def _getitem_view(self, index):. 1304 oidx, vidx = self._normalize_indices(index). -> 1305 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1306 . 1307 def _remove_unused_categories(self, df_full, df_sub, uns):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 662 if not isinstance(X, AnnData):. 663 raise ValueError('`X` has to be an AnnData object.'). --> 664 self._init_as_view(X, oidx, vidx). 665 else:. 666 self._init_as_actual(. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _init_as_view(self, adata_ref, oidx, vidx). 691 self._varm = ArrayView(adata_ref.varm[vidx_normalized], view_args=(self, 'varm')). 692 # hackish solution here, no copy should be necessary. --> 693 uns_new = deepcopy(self._adata_ref._uns). 694 # need to do the slicing before setting the updated self._n_obs, self._n_vars. 695 self._n_obs = self._adata_ref.n_obs # use the original n_obs here. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 178 y = x. 179 else:. --> 180 y = _reconstruct(x, memo, *rv). 181 . 182 # If is its own copy, don't memoize. ~/anaconda3/lib/python3.6/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy). 278 if state is not None:. 2",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:1925,modifiability,pac,packages,1925,"24m']. 5 # labels = list(set(adata.obs[label])). ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). 1299 def __getitem__(self, index):. 1300 """"""Returns a sliced view of the object."""""". -> 1301 return self._getitem_view(index). 1302 . 1303 def _getitem_view(self, index):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _getitem_view(self, index). 1303 def _getitem_view(self, index):. 1304 oidx, vidx = self._normalize_indices(index). -> 1305 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1306 . 1307 def _remove_unused_categories(self, df_full, df_sub, uns):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 662 if not isinstance(X, AnnData):. 663 raise ValueError('`X` has to be an AnnData object.'). --> 664 self._init_as_view(X, oidx, vidx). 665 else:. 666 self._init_as_actual(. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _init_as_view(self, adata_ref, oidx, vidx). 691 self._varm = ArrayView(adata_ref.varm[vidx_normalized], view_args=(self, 'varm')). 692 # hackish solution here, no copy should be necessary. --> 693 uns_new = deepcopy(self._adata_ref._uns). 694 # need to do the slicing before setting the updated self._n_obs, self._n_vars. 695 self._n_obs = self._adata_ref.n_obs # use the original n_obs here. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 178 y = x. 179 else:. --> 180 y = _reconstruct(x, memo, *rv). 181 . 182 # If is its own copy, don't memoize. ~/anaconda3/lib/python3.6/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy). 278 if state is not None:. 279 if deep:. --> 280 state = deepcopy(state, memo). 281 if hasattr(y, '__setstate__'):. 282 y.__setstate__(state). ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 148 copier = _deepcopy_dispatch.get(cls). 149 if copier:. --> 150 y = copier(x, memo). 151 else:. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:6071,modifiability,pac,packages,6071,"r, deepcopy). 278 if state is not None:. 279 if deep:. --> 280 state = deepcopy(state, memo). 281 if hasattr(y, '__setstate__'):. 282 y.__setstate__(state). ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 148 copier = _deepcopy_dispatch.get(cls). 149 if copier:. --> 150 y = copier(x, memo). 151 else:. 152 try:. ~/anaconda3/lib/python3.6/copy.py in _deepcopy_dict(x, memo, deepcopy). 238 memo[id(x)] = y. 239 for key, value in x.items():. --> 240 y[deepcopy(key, memo)] = deepcopy(value, memo). 241 return y. 242 d[dict] = _deepcopy_dict. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 178 y = x. 179 else:. --> 180 y = _reconstruct(x, memo, *rv). 181 . 182 # If is its own copy, don't memoize. ~/anaconda3/lib/python3.6/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy). 305 key = deepcopy(key, memo). 306 value = deepcopy(value, memo). --> 307 y[key] = value. 308 else:. 309 for key, value in dictiter:. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __setitem__(self, idx, value). 437 else:. 438 adata_view, attr_name = self._view_args. --> 439 _init_actual_AnnData(adata_view). 440 getattr(adata_view, attr_name)[idx] = value. 441 . ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _init_actual_AnnData(adata_view). 355 . 356 def _init_actual_AnnData(adata_view):. --> 357 if adata_view.isbacked:. 358 raise ValueError(. 359 'You cannot modify elements of an AnnData view, '. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in isbacked(self). 1195 def isbacked(self):. 1196 """"""``True`` if object is backed on disk, ``False`` otherwise."""""". -> 1197 return self.filename is not None. 1198 . 1199 @property. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in filename(self). 1211 want to copy the previous file, use ``copy(filename='new_filename')``. 1212 """""". -> 1213 return self.file.filename. 1214 . 1215 @filename.setter. AttributeError: 'AnnData' object has no attribute 'file'. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:6314,modifiability,pac,packages,6314,"r, deepcopy). 278 if state is not None:. 279 if deep:. --> 280 state = deepcopy(state, memo). 281 if hasattr(y, '__setstate__'):. 282 y.__setstate__(state). ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 148 copier = _deepcopy_dispatch.get(cls). 149 if copier:. --> 150 y = copier(x, memo). 151 else:. 152 try:. ~/anaconda3/lib/python3.6/copy.py in _deepcopy_dict(x, memo, deepcopy). 238 memo[id(x)] = y. 239 for key, value in x.items():. --> 240 y[deepcopy(key, memo)] = deepcopy(value, memo). 241 return y. 242 d[dict] = _deepcopy_dict. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 178 y = x. 179 else:. --> 180 y = _reconstruct(x, memo, *rv). 181 . 182 # If is its own copy, don't memoize. ~/anaconda3/lib/python3.6/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy). 305 key = deepcopy(key, memo). 306 value = deepcopy(value, memo). --> 307 y[key] = value. 308 else:. 309 for key, value in dictiter:. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __setitem__(self, idx, value). 437 else:. 438 adata_view, attr_name = self._view_args. --> 439 _init_actual_AnnData(adata_view). 440 getattr(adata_view, attr_name)[idx] = value. 441 . ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _init_actual_AnnData(adata_view). 355 . 356 def _init_actual_AnnData(adata_view):. --> 357 if adata_view.isbacked:. 358 raise ValueError(. 359 'You cannot modify elements of an AnnData view, '. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in isbacked(self). 1195 def isbacked(self):. 1196 """"""``True`` if object is backed on disk, ``False`` otherwise."""""". -> 1197 return self.filename is not None. 1198 . 1199 @property. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in filename(self). 1211 want to copy the previous file, use ``copy(filename='new_filename')``. 1212 """""". -> 1213 return self.file.filename. 1214 . 1215 @filename.setter. AttributeError: 'AnnData' object has no attribute 'file'. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:6567,modifiability,pac,packages,6567,"r, deepcopy). 278 if state is not None:. 279 if deep:. --> 280 state = deepcopy(state, memo). 281 if hasattr(y, '__setstate__'):. 282 y.__setstate__(state). ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 148 copier = _deepcopy_dispatch.get(cls). 149 if copier:. --> 150 y = copier(x, memo). 151 else:. 152 try:. ~/anaconda3/lib/python3.6/copy.py in _deepcopy_dict(x, memo, deepcopy). 238 memo[id(x)] = y. 239 for key, value in x.items():. --> 240 y[deepcopy(key, memo)] = deepcopy(value, memo). 241 return y. 242 d[dict] = _deepcopy_dict. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 178 y = x. 179 else:. --> 180 y = _reconstruct(x, memo, *rv). 181 . 182 # If is its own copy, don't memoize. ~/anaconda3/lib/python3.6/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy). 305 key = deepcopy(key, memo). 306 value = deepcopy(value, memo). --> 307 y[key] = value. 308 else:. 309 for key, value in dictiter:. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __setitem__(self, idx, value). 437 else:. 438 adata_view, attr_name = self._view_args. --> 439 _init_actual_AnnData(adata_view). 440 getattr(adata_view, attr_name)[idx] = value. 441 . ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _init_actual_AnnData(adata_view). 355 . 356 def _init_actual_AnnData(adata_view):. --> 357 if adata_view.isbacked:. 358 raise ValueError(. 359 'You cannot modify elements of an AnnData view, '. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in isbacked(self). 1195 def isbacked(self):. 1196 """"""``True`` if object is backed on disk, ``False`` otherwise."""""". -> 1197 return self.filename is not None. 1198 . 1199 @property. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in filename(self). 1211 want to copy the previous file, use ``copy(filename='new_filename')``. 1212 """""". -> 1213 return self.file.filename. 1214 . 1215 @filename.setter. AttributeError: 'AnnData' object has no attribute 'file'. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:6804,modifiability,pac,packages,6804,"r, deepcopy). 278 if state is not None:. 279 if deep:. --> 280 state = deepcopy(state, memo). 281 if hasattr(y, '__setstate__'):. 282 y.__setstate__(state). ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 148 copier = _deepcopy_dispatch.get(cls). 149 if copier:. --> 150 y = copier(x, memo). 151 else:. 152 try:. ~/anaconda3/lib/python3.6/copy.py in _deepcopy_dict(x, memo, deepcopy). 238 memo[id(x)] = y. 239 for key, value in x.items():. --> 240 y[deepcopy(key, memo)] = deepcopy(value, memo). 241 return y. 242 d[dict] = _deepcopy_dict. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 178 y = x. 179 else:. --> 180 y = _reconstruct(x, memo, *rv). 181 . 182 # If is its own copy, don't memoize. ~/anaconda3/lib/python3.6/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy). 305 key = deepcopy(key, memo). 306 value = deepcopy(value, memo). --> 307 y[key] = value. 308 else:. 309 for key, value in dictiter:. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __setitem__(self, idx, value). 437 else:. 438 adata_view, attr_name = self._view_args. --> 439 _init_actual_AnnData(adata_view). 440 getattr(adata_view, attr_name)[idx] = value. 441 . ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _init_actual_AnnData(adata_view). 355 . 356 def _init_actual_AnnData(adata_view):. --> 357 if adata_view.isbacked:. 358 raise ValueError(. 359 'You cannot modify elements of an AnnData view, '. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in isbacked(self). 1195 def isbacked(self):. 1196 """"""``True`` if object is backed on disk, ``False`` otherwise."""""". -> 1197 return self.filename is not None. 1198 . 1199 @property. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in filename(self). 1211 want to copy the previous file, use ``copy(filename='new_filename')``. 1212 """""". -> 1213 return self.file.filename. 1214 . 1215 @filename.setter. AttributeError: 'AnnData' object has no attribute 'file'. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:6677,performance,disk,disk,6677,"r, deepcopy). 278 if state is not None:. 279 if deep:. --> 280 state = deepcopy(state, memo). 281 if hasattr(y, '__setstate__'):. 282 y.__setstate__(state). ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 148 copier = _deepcopy_dispatch.get(cls). 149 if copier:. --> 150 y = copier(x, memo). 151 else:. 152 try:. ~/anaconda3/lib/python3.6/copy.py in _deepcopy_dict(x, memo, deepcopy). 238 memo[id(x)] = y. 239 for key, value in x.items():. --> 240 y[deepcopy(key, memo)] = deepcopy(value, memo). 241 return y. 242 d[dict] = _deepcopy_dict. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 178 y = x. 179 else:. --> 180 y = _reconstruct(x, memo, *rv). 181 . 182 # If is its own copy, don't memoize. ~/anaconda3/lib/python3.6/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy). 305 key = deepcopy(key, memo). 306 value = deepcopy(value, memo). --> 307 y[key] = value. 308 else:. 309 for key, value in dictiter:. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __setitem__(self, idx, value). 437 else:. 438 adata_view, attr_name = self._view_args. --> 439 _init_actual_AnnData(adata_view). 440 getattr(adata_view, attr_name)[idx] = value. 441 . ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _init_actual_AnnData(adata_view). 355 . 356 def _init_actual_AnnData(adata_view):. --> 357 if adata_view.isbacked:. 358 raise ValueError(. 359 'You cannot modify elements of an AnnData view, '. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in isbacked(self). 1195 def isbacked(self):. 1196 """"""``True`` if object is backed on disk, ``False`` otherwise."""""". -> 1197 return self.filename is not None. 1198 . 1199 @property. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in filename(self). 1211 want to copy the previous file, use ``copy(filename='new_filename')``. 1212 """""". -> 1213 return self.file.filename. 1214 . 1215 @filename.setter. AttributeError: 'AnnData' object has no attribute 'file'. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:1117,reliability,sli,sliced,1117,"ll have no success... <details>. <summary>Traceback</summary>. ```pytb. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-12-a0821f8568d8> in <module>(). 22 axs[i].set_title(c). 23 # plot_boxplot_nonzero(tiss[tiss.obs['cell_ontology_class']==cell,:],geneofinterest,'age',axs[i],show = False). ---> 24 plot_boxplot_cell_fraction(tiss[tiss.obs['auto_cell_ontology_class']==c],geneofinterest,'age',c,axs[i],show = False). 25 print(c + ' is done!'). 26 i = i+1. <ipython-input-10-40fc020a5ff4> in plot_boxplot_cell_fraction(adata, gene, label, title, ax, show). 1 def plot_boxplot_cell_fraction(adata, gene, label, title, ax, show=True):. ----> 2 gene_vals = np.asarray(adata[:, gene].X).flatten(). 3 . 4 labels = ['3m','24m']. 5 # labels = list(set(adata.obs[label])). ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). 1299 def __getitem__(self, index):. 1300 """"""Returns a sliced view of the object."""""". -> 1301 return self._getitem_view(index). 1302 . 1303 def _getitem_view(self, index):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _getitem_view(self, index). 1303 def _getitem_view(self, index):. 1304 oidx, vidx = self._normalize_indices(index). -> 1305 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1306 . 1307 def _remove_unused_categories(self, df_full, df_sub, uns):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 662 if not isinstance(X, AnnData):. 663 raise ValueError('`X` has to be an AnnData object.'). --> 664 self._init_as_view(X, oidx, vidx). 665 else:. 666 self._init_as_actual(. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _init_as_view(self, adata_ref, oidx, vidx). 691 self._varm = ArrayView(adata_ref.varm[vidx_normalized], view_args=(self, 'varm')). 692 # hackish solution here, no copy",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:2213,reliability,sli,slicing,2213,"ew(self, index):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _getitem_view(self, index). 1303 def _getitem_view(self, index):. 1304 oidx, vidx = self._normalize_indices(index). -> 1305 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1306 . 1307 def _remove_unused_categories(self, df_full, df_sub, uns):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 662 if not isinstance(X, AnnData):. 663 raise ValueError('`X` has to be an AnnData object.'). --> 664 self._init_as_view(X, oidx, vidx). 665 else:. 666 self._init_as_actual(. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _init_as_view(self, adata_ref, oidx, vidx). 691 self._varm = ArrayView(adata_ref.varm[vidx_normalized], view_args=(self, 'varm')). 692 # hackish solution here, no copy should be necessary. --> 693 uns_new = deepcopy(self._adata_ref._uns). 694 # need to do the slicing before setting the updated self._n_obs, self._n_vars. 695 self._n_obs = self._adata_ref.n_obs # use the original n_obs here. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 178 y = x. 179 else:. --> 180 y = _reconstruct(x, memo, *rv). 181 . 182 # If is its own copy, don't memoize. ~/anaconda3/lib/python3.6/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy). 278 if state is not None:. 279 if deep:. --> 280 state = deepcopy(state, memo). 281 if hasattr(y, '__setstate__'):. 282 y.__setstate__(state). ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 148 copier = _deepcopy_dispatch.get(cls). 149 if copier:. --> 150 y = copier(x, memo). 151 else:. 152 try:. ~/anaconda3/lib/python3.6/copy.py in _deepcopy_dict(x, memo, deepcopy). 238 memo[id(x)] = y. 239 for key, value in x.items():. --> 240 y[deepcopy(key, memo)] = deepcopy(value, memo). 241 return y. 242 d[dict] = _deepcopy_dict. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, me",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:328,safety,input,input-,328,"@falexwolf do you want a object? the fix was working until today, now even after removing all `_color` from `.uns` I still have no success... <details>. <summary>Traceback</summary>. ```pytb. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-12-a0821f8568d8> in <module>(). 22 axs[i].set_title(c). 23 # plot_boxplot_nonzero(tiss[tiss.obs['cell_ontology_class']==cell,:],geneofinterest,'age',axs[i],show = False). ---> 24 plot_boxplot_cell_fraction(tiss[tiss.obs['auto_cell_ontology_class']==c],geneofinterest,'age',c,axs[i],show = False). 25 print(c + ' is done!'). 26 i = i+1. <ipython-input-10-40fc020a5ff4> in plot_boxplot_cell_fraction(adata, gene, label, title, ax, show). 1 def plot_boxplot_cell_fraction(adata, gene, label, title, ax, show=True):. ----> 2 gene_vals = np.asarray(adata[:, gene].X).flatten(). 3 . 4 labels = ['3m','24m']. 5 # labels = list(set(adata.obs[label])). ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). 1299 def __getitem__(self, index):. 1300 """"""Returns a sliced view of the object."""""". -> 1301 return self._getitem_view(index). 1302 . 1303 def _getitem_view(self, index):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _getitem_view(self, index). 1303 def _getitem_view(self, index):. 1304 oidx, vidx = self._normalize_indices(index). -> 1305 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1306 . 1307 def _remove_unused_categories(self, df_full, df_sub, uns):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 662 if not isinstance(X, AnnData):. 663 raise ValueError('`X` has to be an AnnData object.'). --> 664 self._init_as_view(X, oidx, vidx). 665 else:. 666 self._init_as_actual(. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _init_as_view(self, adata_ref, oidx, vidx). 691",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:355,safety,modul,module,355,"@falexwolf do you want a object? the fix was working until today, now even after removing all `_color` from `.uns` I still have no success... <details>. <summary>Traceback</summary>. ```pytb. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-12-a0821f8568d8> in <module>(). 22 axs[i].set_title(c). 23 # plot_boxplot_nonzero(tiss[tiss.obs['cell_ontology_class']==cell,:],geneofinterest,'age',axs[i],show = False). ---> 24 plot_boxplot_cell_fraction(tiss[tiss.obs['auto_cell_ontology_class']==c],geneofinterest,'age',c,axs[i],show = False). 25 print(c + ' is done!'). 26 i = i+1. <ipython-input-10-40fc020a5ff4> in plot_boxplot_cell_fraction(adata, gene, label, title, ax, show). 1 def plot_boxplot_cell_fraction(adata, gene, label, title, ax, show=True):. ----> 2 gene_vals = np.asarray(adata[:, gene].X).flatten(). 3 . 4 labels = ['3m','24m']. 5 # labels = list(set(adata.obs[label])). ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). 1299 def __getitem__(self, index):. 1300 """"""Returns a sliced view of the object."""""". -> 1301 return self._getitem_view(index). 1302 . 1303 def _getitem_view(self, index):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _getitem_view(self, index). 1303 def _getitem_view(self, index):. 1304 oidx, vidx = self._normalize_indices(index). -> 1305 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1306 . 1307 def _remove_unused_categories(self, df_full, df_sub, uns):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 662 if not isinstance(X, AnnData):. 663 raise ValueError('`X` has to be an AnnData object.'). --> 664 self._init_as_view(X, oidx, vidx). 665 else:. 666 self._init_as_actual(. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _init_as_view(self, adata_ref, oidx, vidx). 691",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:679,safety,input,input-,679,"@falexwolf do you want a object? the fix was working until today, now even after removing all `_color` from `.uns` I still have no success... <details>. <summary>Traceback</summary>. ```pytb. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-12-a0821f8568d8> in <module>(). 22 axs[i].set_title(c). 23 # plot_boxplot_nonzero(tiss[tiss.obs['cell_ontology_class']==cell,:],geneofinterest,'age',axs[i],show = False). ---> 24 plot_boxplot_cell_fraction(tiss[tiss.obs['auto_cell_ontology_class']==c],geneofinterest,'age',c,axs[i],show = False). 25 print(c + ' is done!'). 26 i = i+1. <ipython-input-10-40fc020a5ff4> in plot_boxplot_cell_fraction(adata, gene, label, title, ax, show). 1 def plot_boxplot_cell_fraction(adata, gene, label, title, ax, show=True):. ----> 2 gene_vals = np.asarray(adata[:, gene].X).flatten(). 3 . 4 labels = ['3m','24m']. 5 # labels = list(set(adata.obs[label])). ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). 1299 def __getitem__(self, index):. 1300 """"""Returns a sliced view of the object."""""". -> 1301 return self._getitem_view(index). 1302 . 1303 def _getitem_view(self, index):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _getitem_view(self, index). 1303 def _getitem_view(self, index):. 1304 oidx, vidx = self._normalize_indices(index). -> 1305 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1306 . 1307 def _remove_unused_categories(self, df_full, df_sub, uns):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 662 if not isinstance(X, AnnData):. 663 raise ValueError('`X` has to be an AnnData object.'). --> 664 self._init_as_view(X, oidx, vidx). 665 else:. 666 self._init_as_actual(. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _init_as_view(self, adata_ref, oidx, vidx). 691",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:2240,safety,updat,updated,2240,"a3/lib/python3.6/site-packages/anndata/base.py in _getitem_view(self, index). 1303 def _getitem_view(self, index):. 1304 oidx, vidx = self._normalize_indices(index). -> 1305 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1306 . 1307 def _remove_unused_categories(self, df_full, df_sub, uns):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 662 if not isinstance(X, AnnData):. 663 raise ValueError('`X` has to be an AnnData object.'). --> 664 self._init_as_view(X, oidx, vidx). 665 else:. 666 self._init_as_actual(. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _init_as_view(self, adata_ref, oidx, vidx). 691 self._varm = ArrayView(adata_ref.varm[vidx_normalized], view_args=(self, 'varm')). 692 # hackish solution here, no copy should be necessary. --> 693 uns_new = deepcopy(self._adata_ref._uns). 694 # need to do the slicing before setting the updated self._n_obs, self._n_vars. 695 self._n_obs = self._adata_ref.n_obs # use the original n_obs here. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 178 y = x. 179 else:. --> 180 y = _reconstruct(x, memo, *rv). 181 . 182 # If is its own copy, don't memoize. ~/anaconda3/lib/python3.6/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy). 278 if state is not None:. 279 if deep:. --> 280 state = deepcopy(state, memo). 281 if hasattr(y, '__setstate__'):. 282 y.__setstate__(state). ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 148 copier = _deepcopy_dispatch.get(cls). 149 if copier:. --> 150 y = copier(x, memo). 151 else:. 152 try:. ~/anaconda3/lib/python3.6/copy.py in _deepcopy_dict(x, memo, deepcopy). 238 memo[id(x)] = y. 239 for key, value in x.items():. --> 240 y[deepcopy(key, memo)] = deepcopy(value, memo). 241 return y. 242 d[dict] = _deepcopy_dict. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 148 copier = _de",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:2090,security,hack,hackish,2090,"x):. 1300 """"""Returns a sliced view of the object."""""". -> 1301 return self._getitem_view(index). 1302 . 1303 def _getitem_view(self, index):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _getitem_view(self, index). 1303 def _getitem_view(self, index):. 1304 oidx, vidx = self._normalize_indices(index). -> 1305 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1306 . 1307 def _remove_unused_categories(self, df_full, df_sub, uns):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 662 if not isinstance(X, AnnData):. 663 raise ValueError('`X` has to be an AnnData object.'). --> 664 self._init_as_view(X, oidx, vidx). 665 else:. 666 self._init_as_actual(. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _init_as_view(self, adata_ref, oidx, vidx). 691 self._varm = ArrayView(adata_ref.varm[vidx_normalized], view_args=(self, 'varm')). 692 # hackish solution here, no copy should be necessary. --> 693 uns_new = deepcopy(self._adata_ref._uns). 694 # need to do the slicing before setting the updated self._n_obs, self._n_vars. 695 self._n_obs = self._adata_ref.n_obs # use the original n_obs here. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 178 y = x. 179 else:. --> 180 y = _reconstruct(x, memo, *rv). 181 . 182 # If is its own copy, don't memoize. ~/anaconda3/lib/python3.6/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy). 278 if state is not None:. 279 if deep:. --> 280 state = deepcopy(state, memo). 281 if hasattr(y, '__setstate__'):. 282 y.__setstate__(state). ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 148 copier = _deepcopy_dispatch.get(cls). 149 if copier:. --> 150 y = copier(x, memo). 151 else:. 152 try:. ~/anaconda3/lib/python3.6/copy.py in _deepcopy_dict(x, memo, deepcopy). 238 memo[id(x)] = y. 239 for key, value in x.items():. --> 240 y[deepcopy(key, memo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:2240,security,updat,updated,2240,"a3/lib/python3.6/site-packages/anndata/base.py in _getitem_view(self, index). 1303 def _getitem_view(self, index):. 1304 oidx, vidx = self._normalize_indices(index). -> 1305 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1306 . 1307 def _remove_unused_categories(self, df_full, df_sub, uns):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 662 if not isinstance(X, AnnData):. 663 raise ValueError('`X` has to be an AnnData object.'). --> 664 self._init_as_view(X, oidx, vidx). 665 else:. 666 self._init_as_actual(. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _init_as_view(self, adata_ref, oidx, vidx). 691 self._varm = ArrayView(adata_ref.varm[vidx_normalized], view_args=(self, 'varm')). 692 # hackish solution here, no copy should be necessary. --> 693 uns_new = deepcopy(self._adata_ref._uns). 694 # need to do the slicing before setting the updated self._n_obs, self._n_vars. 695 self._n_obs = self._adata_ref.n_obs # use the original n_obs here. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 178 y = x. 179 else:. --> 180 y = _reconstruct(x, memo, *rv). 181 . 182 # If is its own copy, don't memoize. ~/anaconda3/lib/python3.6/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy). 278 if state is not None:. 279 if deep:. --> 280 state = deepcopy(state, memo). 281 if hasattr(y, '__setstate__'):. 282 y.__setstate__(state). ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 148 copier = _deepcopy_dispatch.get(cls). 149 if copier:. --> 150 y = copier(x, memo). 151 else:. 152 try:. ~/anaconda3/lib/python3.6/copy.py in _deepcopy_dict(x, memo, deepcopy). 238 memo[id(x)] = y. 239 for key, value in x.items():. --> 240 y[deepcopy(key, memo)] = deepcopy(value, memo). 241 return y. 242 d[dict] = _deepcopy_dict. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 148 copier = _de",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:6497,security,modif,modify,6497,"r, deepcopy). 278 if state is not None:. 279 if deep:. --> 280 state = deepcopy(state, memo). 281 if hasattr(y, '__setstate__'):. 282 y.__setstate__(state). ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 148 copier = _deepcopy_dispatch.get(cls). 149 if copier:. --> 150 y = copier(x, memo). 151 else:. 152 try:. ~/anaconda3/lib/python3.6/copy.py in _deepcopy_dict(x, memo, deepcopy). 238 memo[id(x)] = y. 239 for key, value in x.items():. --> 240 y[deepcopy(key, memo)] = deepcopy(value, memo). 241 return y. 242 d[dict] = _deepcopy_dict. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil). 178 y = x. 179 else:. --> 180 y = _reconstruct(x, memo, *rv). 181 . 182 # If is its own copy, don't memoize. ~/anaconda3/lib/python3.6/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy). 305 key = deepcopy(key, memo). 306 value = deepcopy(value, memo). --> 307 y[key] = value. 308 else:. 309 for key, value in dictiter:. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __setitem__(self, idx, value). 437 else:. 438 adata_view, attr_name = self._view_args. --> 439 _init_actual_AnnData(adata_view). 440 getattr(adata_view, attr_name)[idx] = value. 441 . ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _init_actual_AnnData(adata_view). 355 . 356 def _init_actual_AnnData(adata_view):. --> 357 if adata_view.isbacked:. 358 raise ValueError(. 359 'You cannot modify elements of an AnnData view, '. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in isbacked(self). 1195 def isbacked(self):. 1196 """"""``True`` if object is backed on disk, ``False`` otherwise."""""". -> 1197 return self.filename is not None. 1198 . 1199 @property. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in filename(self). 1211 want to copy the previous file, use ``copy(filename='new_filename')``. 1212 """""". -> 1213 return self.file.filename. 1214 . 1215 @filename.setter. AttributeError: 'AnnData' object has no attribute 'file'. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:162,testability,Trace,Traceback,162,"@falexwolf do you want a object? the fix was working until today, now even after removing all `_color` from `.uns` I still have no success... <details>. <summary>Traceback</summary>. ```pytb. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-12-a0821f8568d8> in <module>(). 22 axs[i].set_title(c). 23 # plot_boxplot_nonzero(tiss[tiss.obs['cell_ontology_class']==cell,:],geneofinterest,'age',axs[i],show = False). ---> 24 plot_boxplot_cell_fraction(tiss[tiss.obs['auto_cell_ontology_class']==c],geneofinterest,'age',c,axs[i],show = False). 25 print(c + ' is done!'). 26 i = i+1. <ipython-input-10-40fc020a5ff4> in plot_boxplot_cell_fraction(adata, gene, label, title, ax, show). 1 def plot_boxplot_cell_fraction(adata, gene, label, title, ax, show=True):. ----> 2 gene_vals = np.asarray(adata[:, gene].X).flatten(). 3 . 4 labels = ['3m','24m']. 5 # labels = list(set(adata.obs[label])). ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). 1299 def __getitem__(self, index):. 1300 """"""Returns a sliced view of the object."""""". -> 1301 return self._getitem_view(index). 1302 . 1303 def _getitem_view(self, index):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _getitem_view(self, index). 1303 def _getitem_view(self, index):. 1304 oidx, vidx = self._normalize_indices(index). -> 1305 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1306 . 1307 def _remove_unused_categories(self, df_full, df_sub, uns):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 662 if not isinstance(X, AnnData):. 663 raise ValueError('`X` has to be an AnnData object.'). --> 664 self._init_as_view(X, oidx, vidx). 665 else:. 666 self._init_as_actual(. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _init_as_view(self, adata_ref, oidx, vidx). 691",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:284,testability,Trace,Traceback,284,"@falexwolf do you want a object? the fix was working until today, now even after removing all `_color` from `.uns` I still have no success... <details>. <summary>Traceback</summary>. ```pytb. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-12-a0821f8568d8> in <module>(). 22 axs[i].set_title(c). 23 # plot_boxplot_nonzero(tiss[tiss.obs['cell_ontology_class']==cell,:],geneofinterest,'age',axs[i],show = False). ---> 24 plot_boxplot_cell_fraction(tiss[tiss.obs['auto_cell_ontology_class']==c],geneofinterest,'age',c,axs[i],show = False). 25 print(c + ' is done!'). 26 i = i+1. <ipython-input-10-40fc020a5ff4> in plot_boxplot_cell_fraction(adata, gene, label, title, ax, show). 1 def plot_boxplot_cell_fraction(adata, gene, label, title, ax, show=True):. ----> 2 gene_vals = np.asarray(adata[:, gene].X).flatten(). 3 . 4 labels = ['3m','24m']. 5 # labels = list(set(adata.obs[label])). ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). 1299 def __getitem__(self, index):. 1300 """"""Returns a sliced view of the object."""""". -> 1301 return self._getitem_view(index). 1302 . 1303 def _getitem_view(self, index):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _getitem_view(self, index). 1303 def _getitem_view(self, index):. 1304 oidx, vidx = self._normalize_indices(index). -> 1305 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1306 . 1307 def _remove_unused_categories(self, df_full, df_sub, uns):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 662 if not isinstance(X, AnnData):. 663 raise ValueError('`X` has to be an AnnData object.'). --> 664 self._init_as_view(X, oidx, vidx). 665 else:. 666 self._init_as_actual(. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _init_as_view(self, adata_ref, oidx, vidx). 691",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:328,usability,input,input-,328,"@falexwolf do you want a object? the fix was working until today, now even after removing all `_color` from `.uns` I still have no success... <details>. <summary>Traceback</summary>. ```pytb. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-12-a0821f8568d8> in <module>(). 22 axs[i].set_title(c). 23 # plot_boxplot_nonzero(tiss[tiss.obs['cell_ontology_class']==cell,:],geneofinterest,'age',axs[i],show = False). ---> 24 plot_boxplot_cell_fraction(tiss[tiss.obs['auto_cell_ontology_class']==c],geneofinterest,'age',c,axs[i],show = False). 25 print(c + ' is done!'). 26 i = i+1. <ipython-input-10-40fc020a5ff4> in plot_boxplot_cell_fraction(adata, gene, label, title, ax, show). 1 def plot_boxplot_cell_fraction(adata, gene, label, title, ax, show=True):. ----> 2 gene_vals = np.asarray(adata[:, gene].X).flatten(). 3 . 4 labels = ['3m','24m']. 5 # labels = list(set(adata.obs[label])). ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). 1299 def __getitem__(self, index):. 1300 """"""Returns a sliced view of the object."""""". -> 1301 return self._getitem_view(index). 1302 . 1303 def _getitem_view(self, index):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _getitem_view(self, index). 1303 def _getitem_view(self, index):. 1304 oidx, vidx = self._normalize_indices(index). -> 1305 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1306 . 1307 def _remove_unused_categories(self, df_full, df_sub, uns):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 662 if not isinstance(X, AnnData):. 663 raise ValueError('`X` has to be an AnnData object.'). --> 664 self._init_as_view(X, oidx, vidx). 665 else:. 666 self._init_as_actual(. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _init_as_view(self, adata_ref, oidx, vidx). 691",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:679,usability,input,input-,679,"@falexwolf do you want a object? the fix was working until today, now even after removing all `_color` from `.uns` I still have no success... <details>. <summary>Traceback</summary>. ```pytb. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-12-a0821f8568d8> in <module>(). 22 axs[i].set_title(c). 23 # plot_boxplot_nonzero(tiss[tiss.obs['cell_ontology_class']==cell,:],geneofinterest,'age',axs[i],show = False). ---> 24 plot_boxplot_cell_fraction(tiss[tiss.obs['auto_cell_ontology_class']==c],geneofinterest,'age',c,axs[i],show = False). 25 print(c + ' is done!'). 26 i = i+1. <ipython-input-10-40fc020a5ff4> in plot_boxplot_cell_fraction(adata, gene, label, title, ax, show). 1 def plot_boxplot_cell_fraction(adata, gene, label, title, ax, show=True):. ----> 2 gene_vals = np.asarray(adata[:, gene].X).flatten(). 3 . 4 labels = ['3m','24m']. 5 # labels = list(set(adata.obs[label])). ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index). 1299 def __getitem__(self, index):. 1300 """"""Returns a sliced view of the object."""""". -> 1301 return self._getitem_view(index). 1302 . 1303 def _getitem_view(self, index):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _getitem_view(self, index). 1303 def _getitem_view(self, index):. 1304 oidx, vidx = self._normalize_indices(index). -> 1305 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1306 . 1307 def _remove_unused_categories(self, df_full, df_sub, uns):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx). 662 if not isinstance(X, AnnData):. 663 raise ValueError('`X` has to be an AnnData object.'). --> 664 self._init_as_view(X, oidx, vidx). 665 else:. 666 self._init_as_actual(. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _init_as_view(self, adata_ref, oidx, vidx). 691",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:114,deployability,version,version,114,"Yes, please upload an object if this persists and I'll fix it. But please also make sure you are using the latest version of `anndata`. I fixed some stuff around `deepcopy` after you filed this bug.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:114,integrability,version,version,114,"Yes, please upload an object if this persists and I'll fix it. But please also make sure you are using the latest version of `anndata`. I fixed some stuff around `deepcopy` after you filed this bug.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:114,modifiability,version,version,114,"Yes, please upload an object if this persists and I'll fix it. But please also make sure you are using the latest version of `anndata`. I fixed some stuff around `deepcopy` after you filed this bug.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:86,energy efficiency,current,currently,86,"hi @falexwolf, it still isn't working. The problem is with categorical variables, I'm currently doing this before subsetting:. ```. cat_columns = adata.obs.select_dtypes(['category']).columns. adata.obs[cat_columns] = adata.obs[cat_columns].astype(str). del cat_columns. ```. but it's really annoying, specially when using scvelo. Can you look into it? Also something problematic is that the `adata.uns['variable_color']` doesn't delete after you delete `adata.obs['variable']` so when you run into the subsetting problem my fix doesn't work if this is the situation and I've to manually delete the columns one by one... perhaps make this part of `sanitize_anndata`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:114,integrability,sub,subsetting,114,"hi @falexwolf, it still isn't working. The problem is with categorical variables, I'm currently doing this before subsetting:. ```. cat_columns = adata.obs.select_dtypes(['category']).columns. adata.obs[cat_columns] = adata.obs[cat_columns].astype(str). del cat_columns. ```. but it's really annoying, specially when using scvelo. Can you look into it? Also something problematic is that the `adata.uns['variable_color']` doesn't delete after you delete `adata.obs['variable']` so when you run into the subsetting problem my fix doesn't work if this is the situation and I've to manually delete the columns one by one... perhaps make this part of `sanitize_anndata`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:503,integrability,sub,subsetting,503,"hi @falexwolf, it still isn't working. The problem is with categorical variables, I'm currently doing this before subsetting:. ```. cat_columns = adata.obs.select_dtypes(['category']).columns. adata.obs[cat_columns] = adata.obs[cat_columns].astype(str). del cat_columns. ```. but it's really annoying, specially when using scvelo. Can you look into it? Also something problematic is that the `adata.uns['variable_color']` doesn't delete after you delete `adata.obs['variable']` so when you run into the subsetting problem my fix doesn't work if this is the situation and I've to manually delete the columns one by one... perhaps make this part of `sanitize_anndata`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:71,modifiability,variab,variables,71,"hi @falexwolf, it still isn't working. The problem is with categorical variables, I'm currently doing this before subsetting:. ```. cat_columns = adata.obs.select_dtypes(['category']).columns. adata.obs[cat_columns] = adata.obs[cat_columns].astype(str). del cat_columns. ```. but it's really annoying, specially when using scvelo. Can you look into it? Also something problematic is that the `adata.uns['variable_color']` doesn't delete after you delete `adata.obs['variable']` so when you run into the subsetting problem my fix doesn't work if this is the situation and I've to manually delete the columns one by one... perhaps make this part of `sanitize_anndata`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:466,modifiability,variab,variable,466,"hi @falexwolf, it still isn't working. The problem is with categorical variables, I'm currently doing this before subsetting:. ```. cat_columns = adata.obs.select_dtypes(['category']).columns. adata.obs[cat_columns] = adata.obs[cat_columns].astype(str). del cat_columns. ```. but it's really annoying, specially when using scvelo. Can you look into it? Also something problematic is that the `adata.uns['variable_color']` doesn't delete after you delete `adata.obs['variable']` so when you run into the subsetting problem my fix doesn't work if this is the situation and I've to manually delete the columns one by one... perhaps make this part of `sanitize_anndata`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:422,reliability,doe,doesn,422,"hi @falexwolf, it still isn't working. The problem is with categorical variables, I'm currently doing this before subsetting:. ```. cat_columns = adata.obs.select_dtypes(['category']).columns. adata.obs[cat_columns] = adata.obs[cat_columns].astype(str). del cat_columns. ```. but it's really annoying, specially when using scvelo. Can you look into it? Also something problematic is that the `adata.uns['variable_color']` doesn't delete after you delete `adata.obs['variable']` so when you run into the subsetting problem my fix doesn't work if this is the situation and I've to manually delete the columns one by one... perhaps make this part of `sanitize_anndata`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:529,reliability,doe,doesn,529,"hi @falexwolf, it still isn't working. The problem is with categorical variables, I'm currently doing this before subsetting:. ```. cat_columns = adata.obs.select_dtypes(['category']).columns. adata.obs[cat_columns] = adata.obs[cat_columns].astype(str). del cat_columns. ```. but it's really annoying, specially when using scvelo. Can you look into it? Also something problematic is that the `adata.uns['variable_color']` doesn't delete after you delete `adata.obs['variable']` so when you run into the subsetting problem my fix doesn't work if this is the situation and I've to manually delete the columns one by one... perhaps make this part of `sanitize_anndata`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:53,energy efficiency,current,currently,53,"@aopisco are you sure its the same problem? Theres currently a problem with categorical changes in pandas 0.24, and this issue here has been existing for longer.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:72,availability,error,error,72,"@flying-sheep yes, it's exactly the same problem, with the exactly same error message that only happens when I (or the function) wanna subset an existing adata object",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:78,integrability,messag,message,78,"@flying-sheep yes, it's exactly the same problem, with the exactly same error message that only happens when I (or the function) wanna subset an existing adata object",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:135,integrability,sub,subset,135,"@flying-sheep yes, it's exactly the same problem, with the exactly same error message that only happens when I (or the function) wanna subset an existing adata object",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:78,interoperability,messag,message,78,"@flying-sheep yes, it's exactly the same problem, with the exactly same error message that only happens when I (or the function) wanna subset an existing adata object",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:72,performance,error,error,72,"@flying-sheep yes, it's exactly the same problem, with the exactly same error message that only happens when I (or the function) wanna subset an existing adata object",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:72,safety,error,error,72,"@flying-sheep yes, it's exactly the same problem, with the exactly same error message that only happens when I (or the function) wanna subset an existing adata object",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:72,usability,error,error,72,"@flying-sheep yes, it's exactly the same problem, with the exactly same error message that only happens when I (or the function) wanna subset an existing adata object",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:40,integrability,coupl,couple,40,Can we get a small AnnData object and a couple of lines of code that allows reproducing the problem? I still wouldn't know how to fix this as I've never experienced it... Sorry about the trouble that you're experiencing!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:40,modifiability,coupl,couple,40,Can we get a small AnnData object and a couple of lines of code that allows reproducing the problem? I still wouldn't know how to fix this as I've never experienced it... Sorry about the trouble that you're experiencing!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:40,testability,coupl,couple,40,Can we get a small AnnData object and a couple of lines of code that allows reproducing the problem? I still wouldn't know how to fix this as I've never experienced it... Sorry about the trouble that you're experiencing!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:153,usability,experien,experienced,153,Can we get a small AnnData object and a couple of lines of code that allows reproducing the problem? I still wouldn't know how to fix this as I've never experienced it... Sorry about the trouble that you're experiencing!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:207,usability,experien,experiencing,207,Can we get a small AnnData object and a couple of lines of code that allows reproducing the problem? I still wouldn't know how to fix this as I've never experienced it... Sorry about the trouble that you're experiencing!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:270,availability,error,errors,270,"Hi @aopisco ! @falexwolf I ran into the same problem but got everything to work by deleting all the unnecessary items in adata.uns. ```py. keep = ['neighbors', ]. keys = list(adata.uns.keys()). for key in keys:. if key not in keep:. del adata.uns[key]. ```. I don't get errors anymore but I fear that this might cause other problems I'm currently unaware of.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:337,energy efficiency,current,currently,337,"Hi @aopisco ! @falexwolf I ran into the same problem but got everything to work by deleting all the unnecessary items in adata.uns. ```py. keep = ['neighbors', ]. keys = list(adata.uns.keys()). for key in keys:. if key not in keep:. del adata.uns[key]. ```. I don't get errors anymore but I fear that this might cause other problems I'm currently unaware of.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:270,performance,error,errors,270,"Hi @aopisco ! @falexwolf I ran into the same problem but got everything to work by deleting all the unnecessary items in adata.uns. ```py. keep = ['neighbors', ]. keys = list(adata.uns.keys()). for key in keys:. if key not in keep:. del adata.uns[key]. ```. I don't get errors anymore but I fear that this might cause other problems I'm currently unaware of.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:270,safety,error,errors,270,"Hi @aopisco ! @falexwolf I ran into the same problem but got everything to work by deleting all the unnecessary items in adata.uns. ```py. keep = ['neighbors', ]. keys = list(adata.uns.keys()). for key in keys:. if key not in keep:. del adata.uns[key]. ```. I don't get errors anymore but I fear that this might cause other problems I'm currently unaware of.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:270,usability,error,errors,270,"Hi @aopisco ! @falexwolf I ran into the same problem but got everything to work by deleting all the unnecessary items in adata.uns. ```py. keep = ['neighbors', ]. keys = list(adata.uns.keys()). for key in keys:. if key not in keep:. del adata.uns[key]. ```. I don't get errors anymore but I fear that this might cause other problems I'm currently unaware of.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:25,performance,time,time,25,I do get this issue from time to time and @HYsxe 's solution works for me (thanks!). Any idea why this is? Thanks!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/issues/363:33,performance,time,time,33,I do get this issue from time to time and @HYsxe 's solution works for me (thanks!). Any idea why this is? Thanks!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363
https://github.com/scverse/scanpy/pull/364:699,availability,cluster,cluster,699,"Hi,. I have checked your code and it looks good to me, very neat, actually. Very important from my point of view is the neighbourhood size estimation - 15 neighbours is too few even for small data sets. . If you do not have a heuristic, yet, I suggest to use a rough estimate for the neighbourhood size similarly to the R implementation as the quarter of the mean batch size: . `floor(mean(batch_size)/4)` . I should come up with a better estimate, though. I can help you with the docs, if you like! . Lastly, I'd find it important that you can apply kBET group-wise. For example, you may add a `groupby` parameter and then kBET automatically computes a rejection rate (or acceptance rate) for each cluster. I hope that's not too much overhead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/364
https://github.com/scverse/scanpy/pull/364:629,deployability,automat,automatically,629,"Hi,. I have checked your code and it looks good to me, very neat, actually. Very important from my point of view is the neighbourhood size estimation - 15 neighbours is too few even for small data sets. . If you do not have a heuristic, yet, I suggest to use a rough estimate for the neighbourhood size similarly to the R implementation as the quarter of the mean batch size: . `floor(mean(batch_size)/4)` . I should come up with a better estimate, though. I can help you with the docs, if you like! . Lastly, I'd find it important that you can apply kBET group-wise. For example, you may add a `groupby` parameter and then kBET automatically computes a rejection rate (or acceptance rate) for each cluster. I hope that's not too much overhead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/364
https://github.com/scverse/scanpy/pull/364:699,deployability,cluster,cluster,699,"Hi,. I have checked your code and it looks good to me, very neat, actually. Very important from my point of view is the neighbourhood size estimation - 15 neighbours is too few even for small data sets. . If you do not have a heuristic, yet, I suggest to use a rough estimate for the neighbourhood size similarly to the R implementation as the quarter of the mean batch size: . `floor(mean(batch_size)/4)` . I should come up with a better estimate, though. I can help you with the docs, if you like! . Lastly, I'd find it important that you can apply kBET group-wise. For example, you may add a `groupby` parameter and then kBET automatically computes a rejection rate (or acceptance rate) for each cluster. I hope that's not too much overhead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/364
https://github.com/scverse/scanpy/pull/364:139,energy efficiency,estimat,estimation,139,"Hi,. I have checked your code and it looks good to me, very neat, actually. Very important from my point of view is the neighbourhood size estimation - 15 neighbours is too few even for small data sets. . If you do not have a heuristic, yet, I suggest to use a rough estimate for the neighbourhood size similarly to the R implementation as the quarter of the mean batch size: . `floor(mean(batch_size)/4)` . I should come up with a better estimate, though. I can help you with the docs, if you like! . Lastly, I'd find it important that you can apply kBET group-wise. For example, you may add a `groupby` parameter and then kBET automatically computes a rejection rate (or acceptance rate) for each cluster. I hope that's not too much overhead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/364
https://github.com/scverse/scanpy/pull/364:267,energy efficiency,estimat,estimate,267,"Hi,. I have checked your code and it looks good to me, very neat, actually. Very important from my point of view is the neighbourhood size estimation - 15 neighbours is too few even for small data sets. . If you do not have a heuristic, yet, I suggest to use a rough estimate for the neighbourhood size similarly to the R implementation as the quarter of the mean batch size: . `floor(mean(batch_size)/4)` . I should come up with a better estimate, though. I can help you with the docs, if you like! . Lastly, I'd find it important that you can apply kBET group-wise. For example, you may add a `groupby` parameter and then kBET automatically computes a rejection rate (or acceptance rate) for each cluster. I hope that's not too much overhead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/364
https://github.com/scverse/scanpy/pull/364:439,energy efficiency,estimat,estimate,439,"Hi,. I have checked your code and it looks good to me, very neat, actually. Very important from my point of view is the neighbourhood size estimation - 15 neighbours is too few even for small data sets. . If you do not have a heuristic, yet, I suggest to use a rough estimate for the neighbourhood size similarly to the R implementation as the quarter of the mean batch size: . `floor(mean(batch_size)/4)` . I should come up with a better estimate, though. I can help you with the docs, if you like! . Lastly, I'd find it important that you can apply kBET group-wise. For example, you may add a `groupby` parameter and then kBET automatically computes a rejection rate (or acceptance rate) for each cluster. I hope that's not too much overhead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/364
https://github.com/scverse/scanpy/pull/364:364,integrability,batch,batch,364,"Hi,. I have checked your code and it looks good to me, very neat, actually. Very important from my point of view is the neighbourhood size estimation - 15 neighbours is too few even for small data sets. . If you do not have a heuristic, yet, I suggest to use a rough estimate for the neighbourhood size similarly to the R implementation as the quarter of the mean batch size: . `floor(mean(batch_size)/4)` . I should come up with a better estimate, though. I can help you with the docs, if you like! . Lastly, I'd find it important that you can apply kBET group-wise. For example, you may add a `groupby` parameter and then kBET automatically computes a rejection rate (or acceptance rate) for each cluster. I hope that's not too much overhead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/364
https://github.com/scverse/scanpy/pull/364:605,modifiability,paramet,parameter,605,"Hi,. I have checked your code and it looks good to me, very neat, actually. Very important from my point of view is the neighbourhood size estimation - 15 neighbours is too few even for small data sets. . If you do not have a heuristic, yet, I suggest to use a rough estimate for the neighbourhood size similarly to the R implementation as the quarter of the mean batch size: . `floor(mean(batch_size)/4)` . I should come up with a better estimate, though. I can help you with the docs, if you like! . Lastly, I'd find it important that you can apply kBET group-wise. For example, you may add a `groupby` parameter and then kBET automatically computes a rejection rate (or acceptance rate) for each cluster. I hope that's not too much overhead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/364
https://github.com/scverse/scanpy/pull/364:364,performance,batch,batch,364,"Hi,. I have checked your code and it looks good to me, very neat, actually. Very important from my point of view is the neighbourhood size estimation - 15 neighbours is too few even for small data sets. . If you do not have a heuristic, yet, I suggest to use a rough estimate for the neighbourhood size similarly to the R implementation as the quarter of the mean batch size: . `floor(mean(batch_size)/4)` . I should come up with a better estimate, though. I can help you with the docs, if you like! . Lastly, I'd find it important that you can apply kBET group-wise. For example, you may add a `groupby` parameter and then kBET automatically computes a rejection rate (or acceptance rate) for each cluster. I hope that's not too much overhead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/364
https://github.com/scverse/scanpy/pull/364:735,performance,overhead,overhead,735,"Hi,. I have checked your code and it looks good to me, very neat, actually. Very important from my point of view is the neighbourhood size estimation - 15 neighbours is too few even for small data sets. . If you do not have a heuristic, yet, I suggest to use a rough estimate for the neighbourhood size similarly to the R implementation as the quarter of the mean batch size: . `floor(mean(batch_size)/4)` . I should come up with a better estimate, though. I can help you with the docs, if you like! . Lastly, I'd find it important that you can apply kBET group-wise. For example, you may add a `groupby` parameter and then kBET automatically computes a rejection rate (or acceptance rate) for each cluster. I hope that's not too much overhead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/364
https://github.com/scverse/scanpy/pull/364:629,testability,automat,automatically,629,"Hi,. I have checked your code and it looks good to me, very neat, actually. Very important from my point of view is the neighbourhood size estimation - 15 neighbours is too few even for small data sets. . If you do not have a heuristic, yet, I suggest to use a rough estimate for the neighbourhood size similarly to the R implementation as the quarter of the mean batch size: . `floor(mean(batch_size)/4)` . I should come up with a better estimate, though. I can help you with the docs, if you like! . Lastly, I'd find it important that you can apply kBET group-wise. For example, you may add a `groupby` parameter and then kBET automatically computes a rejection rate (or acceptance rate) for each cluster. I hope that's not too much overhead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/364
https://github.com/scverse/scanpy/pull/364:463,usability,help,help,463,"Hi,. I have checked your code and it looks good to me, very neat, actually. Very important from my point of view is the neighbourhood size estimation - 15 neighbours is too few even for small data sets. . If you do not have a heuristic, yet, I suggest to use a rough estimate for the neighbourhood size similarly to the R implementation as the quarter of the mean batch size: . `floor(mean(batch_size)/4)` . I should come up with a better estimate, though. I can help you with the docs, if you like! . Lastly, I'd find it important that you can apply kBET group-wise. For example, you may add a `groupby` parameter and then kBET automatically computes a rejection rate (or acceptance rate) for each cluster. I hope that's not too much overhead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/364
https://github.com/scverse/scanpy/pull/364:368,availability,cluster,cluster,368,"How are things going? I think all of this looks fine except for two things:. * we should *not* have a function `pp.kbet_neighbors` as it won't be used by anything else, `pp.neighbors` exists as it's a detrimental preprocessing step for 10 other tools; `tl.kbet` should make the call of `kbet_neighbors` internally and that should be a private function. * the heading *cluster scores* is very confusing; I'd put into a *metrics* section and the notion that has been established for these kind of things (mostly by the Seurat developers, I think) is ""alignment metric""; I can easily make the change if we're ready to go... PS: Why are there no contributions by you, @mbuttner? After all, it's your method and paper. :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/364
https://github.com/scverse/scanpy/pull/364:368,deployability,cluster,cluster,368,"How are things going? I think all of this looks fine except for two things:. * we should *not* have a function `pp.kbet_neighbors` as it won't be used by anything else, `pp.neighbors` exists as it's a detrimental preprocessing step for 10 other tools; `tl.kbet` should make the call of `kbet_neighbors` internally and that should be a private function. * the heading *cluster scores* is very confusing; I'd put into a *metrics* section and the notion that has been established for these kind of things (mostly by the Seurat developers, I think) is ""alignment metric""; I can easily make the change if we're ready to go... PS: Why are there no contributions by you, @mbuttner? After all, it's your method and paper. :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/364
https://github.com/scverse/scanpy/pull/364:53,safety,except,except,53,"How are things going? I think all of this looks fine except for two things:. * we should *not* have a function `pp.kbet_neighbors` as it won't be used by anything else, `pp.neighbors` exists as it's a detrimental preprocessing step for 10 other tools; `tl.kbet` should make the call of `kbet_neighbors` internally and that should be a private function. * the heading *cluster scores* is very confusing; I'd put into a *metrics* section and the notion that has been established for these kind of things (mostly by the Seurat developers, I think) is ""alignment metric""; I can easily make the change if we're ready to go... PS: Why are there no contributions by you, @mbuttner? After all, it's your method and paper. :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/364
https://github.com/scverse/scanpy/pull/364:245,usability,tool,tools,245,"How are things going? I think all of this looks fine except for two things:. * we should *not* have a function `pp.kbet_neighbors` as it won't be used by anything else, `pp.neighbors` exists as it's a detrimental preprocessing step for 10 other tools; `tl.kbet` should make the call of `kbet_neighbors` internally and that should be a private function. * the heading *cluster scores* is very confusing; I'd put into a *metrics* section and the notion that has been established for these kind of things (mostly by the Seurat developers, I think) is ""alignment metric""; I can easily make the change if we're ready to go... PS: Why are there no contributions by you, @mbuttner? After all, it's your method and paper. :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/364
https://github.com/scverse/scanpy/pull/364:103,modifiability,pac,package,103,"Hi @flying-sheep,. does the base functionality already work, or would you still advise me to use the R package if I want to use kBET?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/364
https://github.com/scverse/scanpy/pull/364:19,reliability,doe,does,19,"Hi @flying-sheep,. does the base functionality already work, or would you still advise me to use the R package if I want to use kBET?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/364
https://github.com/scverse/scanpy/pull/364:183,integrability,batch,batch,183,"Id advise to use the R package. The score is still off and I didnt figure out why. Sorry. /edit: it works now. Id like to have a good toy example for the tests that actually has a batch effect, then Id merge this",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/364
https://github.com/scverse/scanpy/pull/364:24,modifiability,pac,package,24,"Id advise to use the R package. The score is still off and I didnt figure out why. Sorry. /edit: it works now. Id like to have a good toy example for the tests that actually has a batch effect, then Id merge this",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/364
https://github.com/scverse/scanpy/pull/364:183,performance,batch,batch,183,"Id advise to use the R package. The score is still off and I didnt figure out why. Sorry. /edit: it works now. Id like to have a good toy example for the tests that actually has a batch effect, then Id merge this",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/364
https://github.com/scverse/scanpy/pull/364:157,safety,test,tests,157,"Id advise to use the R package. The score is still off and I didnt figure out why. Sorry. /edit: it works now. Id like to have a good toy example for the tests that actually has a batch effect, then Id merge this",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/364
https://github.com/scverse/scanpy/pull/364:157,testability,test,tests,157,"Id advise to use the R package. The score is still off and I didnt figure out why. Sorry. /edit: it works now. Id like to have a good toy example for the tests that actually has a batch effect, then Id merge this",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/364
https://github.com/scverse/scanpy/pull/364:12,integrability,batch,batch,12,"Some(most?) batch correction methods produce outputs in a lower dimensional space (like CCA or scVI), which are more likely to be stored in `.obsm`. Therefore, if you guys decide to call `kbet_neighbors` and `pp.neighbors` internally, do not forget to provide options like `use_rep` in `sc.tl.kbet()` to make things like `sc.pp.neighbors(..., use_rep='X_CCA')` possible. Another idea is to keep multiple kNN graphs in `.uns['neighbors']` e.g. `.uns['neighbors']['default']` and `.uns['neighbors']['kbet']`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/364
https://github.com/scverse/scanpy/pull/364:12,performance,batch,batch,12,"Some(most?) batch correction methods produce outputs in a lower dimensional space (like CCA or scVI), which are more likely to be stored in `.obsm`. Therefore, if you guys decide to call `kbet_neighbors` and `pp.neighbors` internally, do not forget to provide options like `use_rep` in `sc.tl.kbet()` to make things like `sc.pp.neighbors(..., use_rep='X_CCA')` possible. Another idea is to keep multiple kNN graphs in `.uns['neighbors']` e.g. `.uns['neighbors']['default']` and `.uns['neighbors']['kbet']`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/364
https://github.com/scverse/scanpy/pull/364:26,usability,statu,status,26,@flying-sheep What is the status of this PR?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/364
https://github.com/scverse/scanpy/pull/364:183,usability,close,close,183,https://scib-metrics.readthedocs.io/en/stable/generated/scib_metrics.kbet.html. @adamgayoso scib-metrics will feature a kbet implementation. Think that this PR is pretty dead so I'll close it in favor of an ecosystem implementation. Feel free to reopen if we should discuss this again.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/364
https://github.com/scverse/scanpy/pull/364:248,availability,slo,slow,248,"I didn't realize there was a PR for this, there are some things here that we don't have, like adjusting the pvalues and the neighbor search. The former is fast to add (though I didn't see it in the R implementation?) and the latter could be really slow to have in practice",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/364
https://github.com/scverse/scanpy/pull/364:248,reliability,slo,slow,248,"I didn't realize there was a PR for this, there are some things here that we don't have, like adjusting the pvalues and the neighbor search. The former is fast to add (though I didn't see it in the R implementation?) and the latter could be really slow to have in practice",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/364
https://github.com/scverse/scanpy/pull/364:264,reliability,pra,practice,264,"I didn't realize there was a PR for this, there are some things here that we don't have, like adjusting the pvalues and the neighbor search. The former is fast to add (though I didn't see it in the R implementation?) and the latter could be really slow to have in practice",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/364
https://github.com/scverse/scanpy/pull/364:184,availability,slo,slow,184,"Yes, we did an attempt of implementing kBET in Python, but we did not follow up with sufficient testing whether the implementation reproduces the R package. The neighborhood search is slow indeed. For better comparability one should fix one neighborhood per dataset. That's also the reasons why we dropped it in the scIB package. There was no pvalue adjustment in the original test, indeed. I created a statistic of 100 kBET results instead to compute a mean rejection rate and a range. There is a pvalue computation for the rejection rate, but it's almost always significant, and you rarely achieve rejection rates were the significance level is relevant. Moreover, it is a test statistic over a test statistic, so I am not quite sure how pvalues should be adjusted. . > I didn't realize there was a PR for this, there are some things here that we don't have, like adjusting the pvalues and the neighbor search. The former is fast to add (though I didn't see it in the R implementation?) and the latter could be really slow to have in practice.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/364
https://github.com/scverse/scanpy/pull/364:1020,availability,slo,slow,1020,"Yes, we did an attempt of implementing kBET in Python, but we did not follow up with sufficient testing whether the implementation reproduces the R package. The neighborhood search is slow indeed. For better comparability one should fix one neighborhood per dataset. That's also the reasons why we dropped it in the scIB package. There was no pvalue adjustment in the original test, indeed. I created a statistic of 100 kBET results instead to compute a mean rejection rate and a range. There is a pvalue computation for the rejection rate, but it's almost always significant, and you rarely achieve rejection rates were the significance level is relevant. Moreover, it is a test statistic over a test statistic, so I am not quite sure how pvalues should be adjusted. . > I didn't realize there was a PR for this, there are some things here that we don't have, like adjusting the pvalues and the neighbor search. The former is fast to add (though I didn't see it in the R implementation?) and the latter could be really slow to have in practice.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/364
https://github.com/scverse/scanpy/pull/364:148,modifiability,pac,package,148,"Yes, we did an attempt of implementing kBET in Python, but we did not follow up with sufficient testing whether the implementation reproduces the R package. The neighborhood search is slow indeed. For better comparability one should fix one neighborhood per dataset. That's also the reasons why we dropped it in the scIB package. There was no pvalue adjustment in the original test, indeed. I created a statistic of 100 kBET results instead to compute a mean rejection rate and a range. There is a pvalue computation for the rejection rate, but it's almost always significant, and you rarely achieve rejection rates were the significance level is relevant. Moreover, it is a test statistic over a test statistic, so I am not quite sure how pvalues should be adjusted. . > I didn't realize there was a PR for this, there are some things here that we don't have, like adjusting the pvalues and the neighbor search. The former is fast to add (though I didn't see it in the R implementation?) and the latter could be really slow to have in practice.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/364
https://github.com/scverse/scanpy/pull/364:321,modifiability,pac,package,321,"Yes, we did an attempt of implementing kBET in Python, but we did not follow up with sufficient testing whether the implementation reproduces the R package. The neighborhood search is slow indeed. For better comparability one should fix one neighborhood per dataset. That's also the reasons why we dropped it in the scIB package. There was no pvalue adjustment in the original test, indeed. I created a statistic of 100 kBET results instead to compute a mean rejection rate and a range. There is a pvalue computation for the rejection rate, but it's almost always significant, and you rarely achieve rejection rates were the significance level is relevant. Moreover, it is a test statistic over a test statistic, so I am not quite sure how pvalues should be adjusted. . > I didn't realize there was a PR for this, there are some things here that we don't have, like adjusting the pvalues and the neighbor search. The former is fast to add (though I didn't see it in the R implementation?) and the latter could be really slow to have in practice.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/364
https://github.com/scverse/scanpy/pull/364:184,reliability,slo,slow,184,"Yes, we did an attempt of implementing kBET in Python, but we did not follow up with sufficient testing whether the implementation reproduces the R package. The neighborhood search is slow indeed. For better comparability one should fix one neighborhood per dataset. That's also the reasons why we dropped it in the scIB package. There was no pvalue adjustment in the original test, indeed. I created a statistic of 100 kBET results instead to compute a mean rejection rate and a range. There is a pvalue computation for the rejection rate, but it's almost always significant, and you rarely achieve rejection rates were the significance level is relevant. Moreover, it is a test statistic over a test statistic, so I am not quite sure how pvalues should be adjusted. . > I didn't realize there was a PR for this, there are some things here that we don't have, like adjusting the pvalues and the neighbor search. The former is fast to add (though I didn't see it in the R implementation?) and the latter could be really slow to have in practice.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/364
https://github.com/scverse/scanpy/pull/364:1020,reliability,slo,slow,1020,"Yes, we did an attempt of implementing kBET in Python, but we did not follow up with sufficient testing whether the implementation reproduces the R package. The neighborhood search is slow indeed. For better comparability one should fix one neighborhood per dataset. That's also the reasons why we dropped it in the scIB package. There was no pvalue adjustment in the original test, indeed. I created a statistic of 100 kBET results instead to compute a mean rejection rate and a range. There is a pvalue computation for the rejection rate, but it's almost always significant, and you rarely achieve rejection rates were the significance level is relevant. Moreover, it is a test statistic over a test statistic, so I am not quite sure how pvalues should be adjusted. . > I didn't realize there was a PR for this, there are some things here that we don't have, like adjusting the pvalues and the neighbor search. The former is fast to add (though I didn't see it in the R implementation?) and the latter could be really slow to have in practice.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/364
https://github.com/scverse/scanpy/pull/364:1036,reliability,pra,practice,1036,"Yes, we did an attempt of implementing kBET in Python, but we did not follow up with sufficient testing whether the implementation reproduces the R package. The neighborhood search is slow indeed. For better comparability one should fix one neighborhood per dataset. That's also the reasons why we dropped it in the scIB package. There was no pvalue adjustment in the original test, indeed. I created a statistic of 100 kBET results instead to compute a mean rejection rate and a range. There is a pvalue computation for the rejection rate, but it's almost always significant, and you rarely achieve rejection rates were the significance level is relevant. Moreover, it is a test statistic over a test statistic, so I am not quite sure how pvalues should be adjusted. . > I didn't realize there was a PR for this, there are some things here that we don't have, like adjusting the pvalues and the neighbor search. The former is fast to add (though I didn't see it in the R implementation?) and the latter could be really slow to have in practice.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/364
https://github.com/scverse/scanpy/pull/364:96,safety,test,testing,96,"Yes, we did an attempt of implementing kBET in Python, but we did not follow up with sufficient testing whether the implementation reproduces the R package. The neighborhood search is slow indeed. For better comparability one should fix one neighborhood per dataset. That's also the reasons why we dropped it in the scIB package. There was no pvalue adjustment in the original test, indeed. I created a statistic of 100 kBET results instead to compute a mean rejection rate and a range. There is a pvalue computation for the rejection rate, but it's almost always significant, and you rarely achieve rejection rates were the significance level is relevant. Moreover, it is a test statistic over a test statistic, so I am not quite sure how pvalues should be adjusted. . > I didn't realize there was a PR for this, there are some things here that we don't have, like adjusting the pvalues and the neighbor search. The former is fast to add (though I didn't see it in the R implementation?) and the latter could be really slow to have in practice.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/364
https://github.com/scverse/scanpy/pull/364:377,safety,test,test,377,"Yes, we did an attempt of implementing kBET in Python, but we did not follow up with sufficient testing whether the implementation reproduces the R package. The neighborhood search is slow indeed. For better comparability one should fix one neighborhood per dataset. That's also the reasons why we dropped it in the scIB package. There was no pvalue adjustment in the original test, indeed. I created a statistic of 100 kBET results instead to compute a mean rejection rate and a range. There is a pvalue computation for the rejection rate, but it's almost always significant, and you rarely achieve rejection rates were the significance level is relevant. Moreover, it is a test statistic over a test statistic, so I am not quite sure how pvalues should be adjusted. . > I didn't realize there was a PR for this, there are some things here that we don't have, like adjusting the pvalues and the neighbor search. The former is fast to add (though I didn't see it in the R implementation?) and the latter could be really slow to have in practice.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/364
https://github.com/scverse/scanpy/pull/364:675,safety,test,test,675,"Yes, we did an attempt of implementing kBET in Python, but we did not follow up with sufficient testing whether the implementation reproduces the R package. The neighborhood search is slow indeed. For better comparability one should fix one neighborhood per dataset. That's also the reasons why we dropped it in the scIB package. There was no pvalue adjustment in the original test, indeed. I created a statistic of 100 kBET results instead to compute a mean rejection rate and a range. There is a pvalue computation for the rejection rate, but it's almost always significant, and you rarely achieve rejection rates were the significance level is relevant. Moreover, it is a test statistic over a test statistic, so I am not quite sure how pvalues should be adjusted. . > I didn't realize there was a PR for this, there are some things here that we don't have, like adjusting the pvalues and the neighbor search. The former is fast to add (though I didn't see it in the R implementation?) and the latter could be really slow to have in practice.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/364
https://github.com/scverse/scanpy/pull/364:697,safety,test,test,697,"Yes, we did an attempt of implementing kBET in Python, but we did not follow up with sufficient testing whether the implementation reproduces the R package. The neighborhood search is slow indeed. For better comparability one should fix one neighborhood per dataset. That's also the reasons why we dropped it in the scIB package. There was no pvalue adjustment in the original test, indeed. I created a statistic of 100 kBET results instead to compute a mean rejection rate and a range. There is a pvalue computation for the rejection rate, but it's almost always significant, and you rarely achieve rejection rates were the significance level is relevant. Moreover, it is a test statistic over a test statistic, so I am not quite sure how pvalues should be adjusted. . > I didn't realize there was a PR for this, there are some things here that we don't have, like adjusting the pvalues and the neighbor search. The former is fast to add (though I didn't see it in the R implementation?) and the latter could be really slow to have in practice.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/364
https://github.com/scverse/scanpy/pull/364:564,security,sign,significant,564,"Yes, we did an attempt of implementing kBET in Python, but we did not follow up with sufficient testing whether the implementation reproduces the R package. The neighborhood search is slow indeed. For better comparability one should fix one neighborhood per dataset. That's also the reasons why we dropped it in the scIB package. There was no pvalue adjustment in the original test, indeed. I created a statistic of 100 kBET results instead to compute a mean rejection rate and a range. There is a pvalue computation for the rejection rate, but it's almost always significant, and you rarely achieve rejection rates were the significance level is relevant. Moreover, it is a test statistic over a test statistic, so I am not quite sure how pvalues should be adjusted. . > I didn't realize there was a PR for this, there are some things here that we don't have, like adjusting the pvalues and the neighbor search. The former is fast to add (though I didn't see it in the R implementation?) and the latter could be really slow to have in practice.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/364
https://github.com/scverse/scanpy/pull/364:625,security,sign,significance,625,"Yes, we did an attempt of implementing kBET in Python, but we did not follow up with sufficient testing whether the implementation reproduces the R package. The neighborhood search is slow indeed. For better comparability one should fix one neighborhood per dataset. That's also the reasons why we dropped it in the scIB package. There was no pvalue adjustment in the original test, indeed. I created a statistic of 100 kBET results instead to compute a mean rejection rate and a range. There is a pvalue computation for the rejection rate, but it's almost always significant, and you rarely achieve rejection rates were the significance level is relevant. Moreover, it is a test statistic over a test statistic, so I am not quite sure how pvalues should be adjusted. . > I didn't realize there was a PR for this, there are some things here that we don't have, like adjusting the pvalues and the neighbor search. The former is fast to add (though I didn't see it in the R implementation?) and the latter could be really slow to have in practice.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/364
https://github.com/scverse/scanpy/pull/364:96,testability,test,testing,96,"Yes, we did an attempt of implementing kBET in Python, but we did not follow up with sufficient testing whether the implementation reproduces the R package. The neighborhood search is slow indeed. For better comparability one should fix one neighborhood per dataset. That's also the reasons why we dropped it in the scIB package. There was no pvalue adjustment in the original test, indeed. I created a statistic of 100 kBET results instead to compute a mean rejection rate and a range. There is a pvalue computation for the rejection rate, but it's almost always significant, and you rarely achieve rejection rates were the significance level is relevant. Moreover, it is a test statistic over a test statistic, so I am not quite sure how pvalues should be adjusted. . > I didn't realize there was a PR for this, there are some things here that we don't have, like adjusting the pvalues and the neighbor search. The former is fast to add (though I didn't see it in the R implementation?) and the latter could be really slow to have in practice.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/364
https://github.com/scverse/scanpy/pull/364:377,testability,test,test,377,"Yes, we did an attempt of implementing kBET in Python, but we did not follow up with sufficient testing whether the implementation reproduces the R package. The neighborhood search is slow indeed. For better comparability one should fix one neighborhood per dataset. That's also the reasons why we dropped it in the scIB package. There was no pvalue adjustment in the original test, indeed. I created a statistic of 100 kBET results instead to compute a mean rejection rate and a range. There is a pvalue computation for the rejection rate, but it's almost always significant, and you rarely achieve rejection rates were the significance level is relevant. Moreover, it is a test statistic over a test statistic, so I am not quite sure how pvalues should be adjusted. . > I didn't realize there was a PR for this, there are some things here that we don't have, like adjusting the pvalues and the neighbor search. The former is fast to add (though I didn't see it in the R implementation?) and the latter could be really slow to have in practice.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/364
https://github.com/scverse/scanpy/pull/364:675,testability,test,test,675,"Yes, we did an attempt of implementing kBET in Python, but we did not follow up with sufficient testing whether the implementation reproduces the R package. The neighborhood search is slow indeed. For better comparability one should fix one neighborhood per dataset. That's also the reasons why we dropped it in the scIB package. There was no pvalue adjustment in the original test, indeed. I created a statistic of 100 kBET results instead to compute a mean rejection rate and a range. There is a pvalue computation for the rejection rate, but it's almost always significant, and you rarely achieve rejection rates were the significance level is relevant. Moreover, it is a test statistic over a test statistic, so I am not quite sure how pvalues should be adjusted. . > I didn't realize there was a PR for this, there are some things here that we don't have, like adjusting the pvalues and the neighbor search. The former is fast to add (though I didn't see it in the R implementation?) and the latter could be really slow to have in practice.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/364
https://github.com/scverse/scanpy/pull/364:697,testability,test,test,697,"Yes, we did an attempt of implementing kBET in Python, but we did not follow up with sufficient testing whether the implementation reproduces the R package. The neighborhood search is slow indeed. For better comparability one should fix one neighborhood per dataset. That's also the reasons why we dropped it in the scIB package. There was no pvalue adjustment in the original test, indeed. I created a statistic of 100 kBET results instead to compute a mean rejection rate and a range. There is a pvalue computation for the rejection rate, but it's almost always significant, and you rarely achieve rejection rates were the significance level is relevant. Moreover, it is a test statistic over a test statistic, so I am not quite sure how pvalues should be adjusted. . > I didn't realize there was a PR for this, there are some things here that we don't have, like adjusting the pvalues and the neighbor search. The former is fast to add (though I didn't see it in the R implementation?) and the latter could be really slow to have in practice.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/364
https://github.com/scverse/scanpy/pull/364:238,availability,slo,slow,238,Ok I think I understand. Now that I see the python code I understand that one only needs to run neighbors once for the max K and then run kbet a few times to maximize the acceptance rate. This would actually not be hard to add and not be slow. It takes about 5 seconds to run kbet for a fixed K on 900k cells,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/364
https://github.com/scverse/scanpy/pull/364:149,performance,time,times,149,Ok I think I understand. Now that I see the python code I understand that one only needs to run neighbors once for the max K and then run kbet a few times to maximize the acceptance rate. This would actually not be hard to add and not be slow. It takes about 5 seconds to run kbet for a fixed K on 900k cells,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/364
https://github.com/scverse/scanpy/pull/364:238,reliability,slo,slow,238,Ok I think I understand. Now that I see the python code I understand that one only needs to run neighbors once for the max K and then run kbet a few times to maximize the acceptance rate. This would actually not be hard to add and not be slow. It takes about 5 seconds to run kbet for a fixed K on 900k cells,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/364
https://github.com/scverse/scanpy/pull/364:13,testability,understand,understand,13,Ok I think I understand. Now that I see the python code I understand that one only needs to run neighbors once for the max K and then run kbet a few times to maximize the acceptance rate. This would actually not be hard to add and not be slow. It takes about 5 seconds to run kbet for a fixed K on 900k cells,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/364
https://github.com/scverse/scanpy/pull/364:58,testability,understand,understand,58,Ok I think I understand. Now that I see the python code I understand that one only needs to run neighbors once for the max K and then run kbet a few times to maximize the acceptance rate. This would actually not be hard to add and not be slow. It takes about 5 seconds to run kbet for a fixed K on 900k cells,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/364
https://github.com/scverse/scanpy/pull/364:425,deployability,depend,dependence,425,"Yes, I totally agree that creating a fast implementation is probably not straightforward. The major bottleneck IMHO is computing this many neighbors to maximize the rejection rate, especially with 900k cells. In the original paper, we tried to find a range of neighborhood sizes K that return a maximal rejection rate, which is roughly between K = 50 to 0.5 * N where N denotes the number of cells, but there might be also a dependence on the number of batches, which we did not fully explore.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/364
https://github.com/scverse/scanpy/pull/364:425,integrability,depend,dependence,425,"Yes, I totally agree that creating a fast implementation is probably not straightforward. The major bottleneck IMHO is computing this many neighbors to maximize the rejection rate, especially with 900k cells. In the original paper, we tried to find a range of neighborhood sizes K that return a maximal rejection rate, which is roughly between K = 50 to 0.5 * N where N denotes the number of cells, but there might be also a dependence on the number of batches, which we did not fully explore.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/364
https://github.com/scverse/scanpy/pull/364:453,integrability,batch,batches,453,"Yes, I totally agree that creating a fast implementation is probably not straightforward. The major bottleneck IMHO is computing this many neighbors to maximize the rejection rate, especially with 900k cells. In the original paper, we tried to find a range of neighborhood sizes K that return a maximal rejection rate, which is roughly between K = 50 to 0.5 * N where N denotes the number of cells, but there might be also a dependence on the number of batches, which we did not fully explore.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/364
https://github.com/scverse/scanpy/pull/364:425,modifiability,depend,dependence,425,"Yes, I totally agree that creating a fast implementation is probably not straightforward. The major bottleneck IMHO is computing this many neighbors to maximize the rejection rate, especially with 900k cells. In the original paper, we tried to find a range of neighborhood sizes K that return a maximal rejection rate, which is roughly between K = 50 to 0.5 * N where N denotes the number of cells, but there might be also a dependence on the number of batches, which we did not fully explore.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/364
https://github.com/scverse/scanpy/pull/364:100,performance,bottleneck,bottleneck,100,"Yes, I totally agree that creating a fast implementation is probably not straightforward. The major bottleneck IMHO is computing this many neighbors to maximize the rejection rate, especially with 900k cells. In the original paper, we tried to find a range of neighborhood sizes K that return a maximal rejection rate, which is roughly between K = 50 to 0.5 * N where N denotes the number of cells, but there might be also a dependence on the number of batches, which we did not fully explore.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/364
https://github.com/scverse/scanpy/pull/364:453,performance,batch,batches,453,"Yes, I totally agree that creating a fast implementation is probably not straightforward. The major bottleneck IMHO is computing this many neighbors to maximize the rejection rate, especially with 900k cells. In the original paper, we tried to find a range of neighborhood sizes K that return a maximal rejection rate, which is roughly between K = 50 to 0.5 * N where N denotes the number of cells, but there might be also a dependence on the number of batches, which we did not fully explore.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/364
https://github.com/scverse/scanpy/pull/364:425,safety,depend,dependence,425,"Yes, I totally agree that creating a fast implementation is probably not straightforward. The major bottleneck IMHO is computing this many neighbors to maximize the rejection rate, especially with 900k cells. In the original paper, we tried to find a range of neighborhood sizes K that return a maximal rejection rate, which is roughly between K = 50 to 0.5 * N where N denotes the number of cells, but there might be also a dependence on the number of batches, which we did not fully explore.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/364
https://github.com/scverse/scanpy/pull/364:425,testability,depend,dependence,425,"Yes, I totally agree that creating a fast implementation is probably not straightforward. The major bottleneck IMHO is computing this many neighbors to maximize the rejection rate, especially with 900k cells. In the original paper, we tried to find a range of neighborhood sizes K that return a maximal rejection rate, which is roughly between K = 50 to 0.5 * N where N denotes the number of cells, but there might be also a dependence on the number of batches, which we did not fully explore.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/364
https://github.com/scverse/scanpy/issues/365:60,availability,cluster,clustermap,60,"@Puumanamana I was having a similar issue when using `sc.pl.clustermap`, I've solved it by pulling the latest version from github",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/365
https://github.com/scverse/scanpy/issues/365:60,deployability,cluster,clustermap,60,"@Puumanamana I was having a similar issue when using `sc.pl.clustermap`, I've solved it by pulling the latest version from github",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/365
https://github.com/scverse/scanpy/issues/365:110,deployability,version,version,110,"@Puumanamana I was having a similar issue when using `sc.pl.clustermap`, I've solved it by pulling the latest version from github",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/365
https://github.com/scverse/scanpy/issues/365:110,integrability,version,version,110,"@Puumanamana I was having a similar issue when using `sc.pl.clustermap`, I've solved it by pulling the latest version from github",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/365
https://github.com/scverse/scanpy/issues/365:110,modifiability,version,version,110,"@Puumanamana I was having a similar issue when using `sc.pl.clustermap`, I've solved it by pulling the latest version from github",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/365
https://github.com/scverse/scanpy/issues/365:380,modifiability,pac,packages,380,"> They are annotated with a list of length the number of cells. Here is what I have for adata.obs['celltype'].cat.categories:. Index(['Group1', 'Group2', 'Group3'], dtype='object'). This is perfectly the canonical way it should look like - I thought that instead of 'Group1', you'd have a list. Could you print `groups_order_save` before line 376 in `~/.py3Env/lib/python3.5/site-packages/scanpy/tools/rank_genes_groups.py`? I don't know how it can happen that it stores strange things, but there seems to be a bug there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/365
https://github.com/scverse/scanpy/issues/365:396,usability,tool,tools,396,"> They are annotated with a list of length the number of cells. Here is what I have for adata.obs['celltype'].cat.categories:. Index(['Group1', 'Group2', 'Group3'], dtype='object'). This is perfectly the canonical way it should look like - I thought that instead of 'Group1', you'd have a list. Could you print `groups_order_save` before line 376 in `~/.py3Env/lib/python3.5/site-packages/scanpy/tools/rank_genes_groups.py`? I don't know how it can happen that it stores strange things, but there seems to be a bug there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/365
https://github.com/scverse/scanpy/issues/365:27,interoperability,format,format,27,"I get the same in the list format:. `groups_order_save`. `> ['Group1', 'Group2', 'Group3']`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/365
https://github.com/scverse/scanpy/issues/365:124,availability,error,error,124,This is all looks fine and should work perfectly. I'd need an example with some data and the lines of code that produce the error.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/365
https://github.com/scverse/scanpy/issues/365:124,performance,error,error,124,This is all looks fine and should work perfectly. I'd need an example with some data and the lines of code that produce the error.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/365
https://github.com/scverse/scanpy/issues/365:124,safety,error,error,124,This is all looks fine and should work perfectly. I'd need an example with some data and the lines of code that produce the error.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/365
https://github.com/scverse/scanpy/issues/365:124,usability,error,error,124,This is all looks fine and should work perfectly. I'd need an example with some data and the lines of code that produce the error.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/365
https://github.com/scverse/scanpy/issues/365:103,availability,error,error,103,"I can do that. I have a pickled object that I can share with you (how?). Here is how you reproduce the error:. ```. import scanpy.api as sc. import pickle. # Load the object. with open(""example.pkl"",""rb"") as handle:. adata = pickle.load(handle). # Run Scanpy. sc.tl.rank_genes_groups(adata,groupby=""celltype""). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/365
https://github.com/scverse/scanpy/issues/365:130,deployability,api,api,130,"I can do that. I have a pickled object that I can share with you (how?). Here is how you reproduce the error:. ```. import scanpy.api as sc. import pickle. # Load the object. with open(""example.pkl"",""rb"") as handle:. adata = pickle.load(handle). # Run Scanpy. sc.tl.rank_genes_groups(adata,groupby=""celltype""). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/365
https://github.com/scverse/scanpy/issues/365:158,energy efficiency,Load,Load,158,"I can do that. I have a pickled object that I can share with you (how?). Here is how you reproduce the error:. ```. import scanpy.api as sc. import pickle. # Load the object. with open(""example.pkl"",""rb"") as handle:. adata = pickle.load(handle). # Run Scanpy. sc.tl.rank_genes_groups(adata,groupby=""celltype""). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/365
https://github.com/scverse/scanpy/issues/365:232,energy efficiency,load,load,232,"I can do that. I have a pickled object that I can share with you (how?). Here is how you reproduce the error:. ```. import scanpy.api as sc. import pickle. # Load the object. with open(""example.pkl"",""rb"") as handle:. adata = pickle.load(handle). # Run Scanpy. sc.tl.rank_genes_groups(adata,groupby=""celltype""). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/365
https://github.com/scverse/scanpy/issues/365:130,integrability,api,api,130,"I can do that. I have a pickled object that I can share with you (how?). Here is how you reproduce the error:. ```. import scanpy.api as sc. import pickle. # Load the object. with open(""example.pkl"",""rb"") as handle:. adata = pickle.load(handle). # Run Scanpy. sc.tl.rank_genes_groups(adata,groupby=""celltype""). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/365
https://github.com/scverse/scanpy/issues/365:50,interoperability,share,share,50,"I can do that. I have a pickled object that I can share with you (how?). Here is how you reproduce the error:. ```. import scanpy.api as sc. import pickle. # Load the object. with open(""example.pkl"",""rb"") as handle:. adata = pickle.load(handle). # Run Scanpy. sc.tl.rank_genes_groups(adata,groupby=""celltype""). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/365
https://github.com/scverse/scanpy/issues/365:130,interoperability,api,api,130,"I can do that. I have a pickled object that I can share with you (how?). Here is how you reproduce the error:. ```. import scanpy.api as sc. import pickle. # Load the object. with open(""example.pkl"",""rb"") as handle:. adata = pickle.load(handle). # Run Scanpy. sc.tl.rank_genes_groups(adata,groupby=""celltype""). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/365
https://github.com/scverse/scanpy/issues/365:103,performance,error,error,103,"I can do that. I have a pickled object that I can share with you (how?). Here is how you reproduce the error:. ```. import scanpy.api as sc. import pickle. # Load the object. with open(""example.pkl"",""rb"") as handle:. adata = pickle.load(handle). # Run Scanpy. sc.tl.rank_genes_groups(adata,groupby=""celltype""). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/365
https://github.com/scverse/scanpy/issues/365:158,performance,Load,Load,158,"I can do that. I have a pickled object that I can share with you (how?). Here is how you reproduce the error:. ```. import scanpy.api as sc. import pickle. # Load the object. with open(""example.pkl"",""rb"") as handle:. adata = pickle.load(handle). # Run Scanpy. sc.tl.rank_genes_groups(adata,groupby=""celltype""). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/365
https://github.com/scverse/scanpy/issues/365:232,performance,load,load,232,"I can do that. I have a pickled object that I can share with you (how?). Here is how you reproduce the error:. ```. import scanpy.api as sc. import pickle. # Load the object. with open(""example.pkl"",""rb"") as handle:. adata = pickle.load(handle). # Run Scanpy. sc.tl.rank_genes_groups(adata,groupby=""celltype""). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/365
https://github.com/scverse/scanpy/issues/365:103,safety,error,error,103,"I can do that. I have a pickled object that I can share with you (how?). Here is how you reproduce the error:. ```. import scanpy.api as sc. import pickle. # Load the object. with open(""example.pkl"",""rb"") as handle:. adata = pickle.load(handle). # Run Scanpy. sc.tl.rank_genes_groups(adata,groupby=""celltype""). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/365
https://github.com/scverse/scanpy/issues/365:103,usability,error,error,103,"I can do that. I have a pickled object that I can share with you (how?). Here is how you reproduce the error:. ```. import scanpy.api as sc. import pickle. # Load the object. with open(""example.pkl"",""rb"") as handle:. adata = pickle.load(handle). # Run Scanpy. sc.tl.rank_genes_groups(adata,groupby=""celltype""). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/365
https://github.com/scverse/scanpy/issues/365:0,energy efficiency,Cool,Cool,0,"Cool! By email is fine: alex.surname@helmholtz-muenchen.de, by surname is wolf. Thank you! :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/365
https://github.com/scverse/scanpy/issues/365:27,testability,simpl,simply,27,"PS: You can of course also simply upload here on GitHub in a comment, as you want. . PPS: The canonical way of saving AnnData's is via `.write('myfile.h5ad')`. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/365
https://github.com/scverse/scanpy/issues/365:27,usability,simpl,simply,27,"PS: You can of course also simply upload here on GitHub in a comment, as you want. . PPS: The canonical way of saving AnnData's is via `.write('myfile.h5ad')`. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/365
https://github.com/scverse/scanpy/issues/365:5,deployability,observ,observations,5,"Your observations index contains tuples, which is, kind of weird:. ![image](https://user-images.githubusercontent.com/16916678/48802882-7125d600-ecde-11e8-84ce-cb0d5eea1f71.png). Let me look further...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/365
https://github.com/scverse/scanpy/issues/365:24,deployability,contain,contains,24,"Your observations index contains tuples, which is, kind of weird:. ![image](https://user-images.githubusercontent.com/16916678/48802882-7125d600-ecde-11e8-84ce-cb0d5eea1f71.png). Let me look further...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/365
https://github.com/scverse/scanpy/issues/365:5,testability,observ,observations,5,"Your observations index contains tuples, which is, kind of weird:. ![image](https://user-images.githubusercontent.com/16916678/48802882-7125d600-ecde-11e8-84ce-cb0d5eea1f71.png). Let me look further...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/365
https://github.com/scverse/scanpy/issues/365:84,usability,user,user-images,84,"Your observations index contains tuples, which is, kind of weird:. ![image](https://user-images.githubusercontent.com/16916678/48802882-7125d600-ecde-11e8-84ce-cb0d5eea1f71.png). Let me look further...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/365
https://github.com/scverse/scanpy/issues/365:213,safety,input,input,213,"That was it, it works! It's weird because everything worked beside rank_genes_group (recipe_seurat, Louvain, ...). The problem was the way I created my AnnData object. When filling the obs_name and var_names, the input were data frames, not lists. Thanks a lot for your help.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/365
https://github.com/scverse/scanpy/issues/365:213,usability,input,input,213,"That was it, it works! It's weird because everything worked beside rank_genes_group (recipe_seurat, Louvain, ...). The problem was the way I created my AnnData object. When filling the obs_name and var_names, the input were data frames, not lists. Thanks a lot for your help.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/365
https://github.com/scverse/scanpy/issues/365:270,usability,help,help,270,"That was it, it works! It's weird because everything worked beside rank_genes_group (recipe_seurat, Louvain, ...). The problem was the way I created my AnnData object. When filling the obs_name and var_names, the input were data frames, not lists. Thanks a lot for your help.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/365
https://github.com/scverse/scanpy/issues/365:8,availability,error,error,8,But the error comes from your variable names being tuples. The following fixes it. ```. adata.var_names = [i[0] for i in adata.var_names]. ```. Let me think where it would be best to output a warning. I'm not quite sure where else it would lead to collisions. Do you need tuples in the index?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/365
https://github.com/scverse/scanpy/issues/365:30,modifiability,variab,variable,30,But the error comes from your variable names being tuples. The following fixes it. ```. adata.var_names = [i[0] for i in adata.var_names]. ```. Let me think where it would be best to output a warning. I'm not quite sure where else it would lead to collisions. Do you need tuples in the index?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/365
https://github.com/scverse/scanpy/issues/365:8,performance,error,error,8,But the error comes from your variable names being tuples. The following fixes it. ```. adata.var_names = [i[0] for i in adata.var_names]. ```. Let me think where it would be best to output a warning. I'm not quite sure where else it would lead to collisions. Do you need tuples in the index?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/365
https://github.com/scverse/scanpy/issues/365:8,safety,error,error,8,But the error comes from your variable names being tuples. The following fixes it. ```. adata.var_names = [i[0] for i in adata.var_names]. ```. Let me think where it would be best to output a warning. I'm not quite sure where else it would lead to collisions. Do you need tuples in the index?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/365
https://github.com/scverse/scanpy/issues/365:8,usability,error,error,8,But the error comes from your variable names being tuples. The following fixes it. ```. adata.var_names = [i[0] for i in adata.var_names]. ```. Let me think where it would be best to output a warning. I'm not quite sure where else it would lead to collisions. Do you need tuples in the index?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/365
https://github.com/scverse/scanpy/issues/365:82,safety,test,test,82,do you have any references on t-test_overestim_var ? I cannot find papers on this test method. what's the difference from t-test ? @falexwolf,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/365
https://github.com/scverse/scanpy/issues/365:124,safety,test,test,124,do you have any references on t-test_overestim_var ? I cannot find papers on this test method. what's the difference from t-test ? @falexwolf,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/365
https://github.com/scverse/scanpy/issues/365:82,testability,test,test,82,do you have any references on t-test_overestim_var ? I cannot find papers on this test method. what's the difference from t-test ? @falexwolf,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/365
https://github.com/scverse/scanpy/issues/365:124,testability,test,test,124,do you have any references on t-test_overestim_var ? I cannot find papers on this test method. what's the difference from t-test ? @falexwolf,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/365
https://github.com/scverse/scanpy/issues/365:327,safety,test,test,327,"No, there aren't any references. It's most easy to understand from this: https://github.com/theislab/scanpy/blob/662f66a4c2bc9a254990792f570cc971a444c575/scanpy/tools/_rank_genes_groups.py#L191. We had quite some material before (@tcallies, where did it go?), but we're now moving away from it and will set a different default test in the future.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/365
https://github.com/scverse/scanpy/issues/365:51,testability,understand,understand,51,"No, there aren't any references. It's most easy to understand from this: https://github.com/theislab/scanpy/blob/662f66a4c2bc9a254990792f570cc971a444c575/scanpy/tools/_rank_genes_groups.py#L191. We had quite some material before (@tcallies, where did it go?), but we're now moving away from it and will set a different default test in the future.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/365
https://github.com/scverse/scanpy/issues/365:327,testability,test,test,327,"No, there aren't any references. It's most easy to understand from this: https://github.com/theislab/scanpy/blob/662f66a4c2bc9a254990792f570cc971a444c575/scanpy/tools/_rank_genes_groups.py#L191. We had quite some material before (@tcallies, where did it go?), but we're now moving away from it and will set a different default test in the future.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/365
https://github.com/scverse/scanpy/issues/365:161,usability,tool,tools,161,"No, there aren't any references. It's most easy to understand from this: https://github.com/theislab/scanpy/blob/662f66a4c2bc9a254990792f570cc971a444c575/scanpy/tools/_rank_genes_groups.py#L191. We had quite some material before (@tcallies, where did it go?), but we're now moving away from it and will set a different default test in the future.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/365
https://github.com/scverse/scanpy/issues/365:227,integrability,pub,publications,227,"> do you have any references on t-test_overestim_var ? I cannot find papers on this test method. what's the difference from t-test ? @falexwolf. I am also interested in formal descriptions about t-test_overestim_var or related publications, thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/365
https://github.com/scverse/scanpy/issues/365:84,safety,test,test,84,"> do you have any references on t-test_overestim_var ? I cannot find papers on this test method. what's the difference from t-test ? @falexwolf. I am also interested in formal descriptions about t-test_overestim_var or related publications, thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/365
https://github.com/scverse/scanpy/issues/365:126,safety,test,test,126,"> do you have any references on t-test_overestim_var ? I cannot find papers on this test method. what's the difference from t-test ? @falexwolf. I am also interested in formal descriptions about t-test_overestim_var or related publications, thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/365
https://github.com/scverse/scanpy/issues/365:84,testability,test,test,84,"> do you have any references on t-test_overestim_var ? I cannot find papers on this test method. what's the difference from t-test ? @falexwolf. I am also interested in formal descriptions about t-test_overestim_var or related publications, thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/365
https://github.com/scverse/scanpy/issues/365:126,testability,test,test,126,"> do you have any references on t-test_overestim_var ? I cannot find papers on this test method. what's the difference from t-test ? @falexwolf. I am also interested in formal descriptions about t-test_overestim_var or related publications, thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/365
https://github.com/scverse/scanpy/issues/366:66,deployability,api,api,66,Anything that helps here: https://scanpy.readthedocs.io/en/latest/api/index.html#reading?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/366
https://github.com/scverse/scanpy/issues/366:66,integrability,api,api,66,Anything that helps here: https://scanpy.readthedocs.io/en/latest/api/index.html#reading?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/366
https://github.com/scverse/scanpy/issues/366:66,interoperability,api,api,66,Anything that helps here: https://scanpy.readthedocs.io/en/latest/api/index.html#reading?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/366
https://github.com/scverse/scanpy/issues/366:14,usability,help,helps,14,Anything that helps here: https://scanpy.readthedocs.io/en/latest/api/index.html#reading?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/366
https://github.com/scverse/scanpy/issues/366:63,usability,user,user-images,63,Thanks it worked. ![screen shot 2018-11-20 at 5 51 30](https://user-images.githubusercontent.com/39877296/48734284-5aed2b00-ec88-11e8-8e2b-0660526a62c2.png).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/366
https://github.com/scverse/scanpy/issues/368:581,integrability,sub,subscribed,581,"You can increase the distance between plots by setting `wspace=0.3`. (default is 0.1). Otherwise you can do: `ncols=1` or `legend_loc='on. data`. On Mon, Nov 19, 2018 at 5:12 PM Florian R. Hlzlwimmer <. notifications@github.com> wrote:. > I've got some problems with overlapping legends:. >. > sc.pl.umap(adata_pp, color=[""treatment"", ""mixture_assignment""]). >. > [image: grafik]. > <https://user-images.githubusercontent.com/1200058/48719273-60f9f200-ec1d-11e8-8cab-08434ad02411.png>. >. > Is there something I can do about this? >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/368>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1S9nFS4TDVTF0H3nNSFCtXjAutOUks5uwth2gaJpZM4YpbLy>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/368
https://github.com/scverse/scanpy/issues/368:778,security,auth,auth,778,"You can increase the distance between plots by setting `wspace=0.3`. (default is 0.1). Otherwise you can do: `ncols=1` or `legend_loc='on. data`. On Mon, Nov 19, 2018 at 5:12 PM Florian R. Hlzlwimmer <. notifications@github.com> wrote:. > I've got some problems with overlapping legends:. >. > sc.pl.umap(adata_pp, color=[""treatment"", ""mixture_assignment""]). >. > [image: grafik]. > <https://user-images.githubusercontent.com/1200058/48719273-60f9f200-ec1d-11e8-8cab-08434ad02411.png>. >. > Is there something I can do about this? >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/368>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1S9nFS4TDVTF0H3nNSFCtXjAutOUks5uwth2gaJpZM4YpbLy>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/368
https://github.com/scverse/scanpy/issues/368:393,usability,user,user-images,393,"You can increase the distance between plots by setting `wspace=0.3`. (default is 0.1). Otherwise you can do: `ncols=1` or `legend_loc='on. data`. On Mon, Nov 19, 2018 at 5:12 PM Florian R. Hlzlwimmer <. notifications@github.com> wrote:. > I've got some problems with overlapping legends:. >. > sc.pl.umap(adata_pp, color=[""treatment"", ""mixture_assignment""]). >. > [image: grafik]. > <https://user-images.githubusercontent.com/1200058/48719273-60f9f200-ec1d-11e8-8cab-08434ad02411.png>. >. > Is there something I can do about this? >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/368>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1S9nFS4TDVTF0H3nNSFCtXjAutOUks5uwth2gaJpZM4YpbLy>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/368
https://github.com/scverse/scanpy/issues/368:26,usability,help,help,26,"@fidelram Thanks for your help, `wspace=0.3` perfectly solves this problem :+1:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/368
https://github.com/scverse/scanpy/pull/369:75,usability,document,documentation,75,"Wow, this looks great! One remark for future PRs: Were migrating to a new documentation style using type annotations (see e.g. https://github.com/theislab/scanpy/blob/master/scanpy/tools/louvain.py). could you use that in future PRs, please? (no need to do it in this one)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369
https://github.com/scverse/scanpy/pull/369:182,usability,tool,tools,182,"Wow, this looks great! One remark for future PRs: Were migrating to a new documentation style using type annotations (see e.g. https://github.com/theislab/scanpy/blob/master/scanpy/tools/louvain.py). could you use that in future PRs, please? (no need to do it in this one)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369
https://github.com/scverse/scanpy/pull/369:58,usability,document,documentation,58,@flying-sheep I will be happy to start using the proposed documentation style for future PRs!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369
https://github.com/scverse/scanpy/pull/369:47,deployability,updat,updating,47,"Together with the suggested changes, I am also updating my usual notebook containing examples of all the plots (~~not yet updated:~~ https://gist.github.com/fidelram/2289b7a8d6da055fb058ac9a79ed485c). However, don't you think that this could be part of a the scanpy tutorials section?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369
https://github.com/scverse/scanpy/pull/369:74,deployability,contain,containing,74,"Together with the suggested changes, I am also updating my usual notebook containing examples of all the plots (~~not yet updated:~~ https://gist.github.com/fidelram/2289b7a8d6da055fb058ac9a79ed485c). However, don't you think that this could be part of a the scanpy tutorials section?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369
https://github.com/scverse/scanpy/pull/369:122,deployability,updat,updated,122,"Together with the suggested changes, I am also updating my usual notebook containing examples of all the plots (~~not yet updated:~~ https://gist.github.com/fidelram/2289b7a8d6da055fb058ac9a79ed485c). However, don't you think that this could be part of a the scanpy tutorials section?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369
https://github.com/scverse/scanpy/pull/369:47,safety,updat,updating,47,"Together with the suggested changes, I am also updating my usual notebook containing examples of all the plots (~~not yet updated:~~ https://gist.github.com/fidelram/2289b7a8d6da055fb058ac9a79ed485c). However, don't you think that this could be part of a the scanpy tutorials section?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369
https://github.com/scverse/scanpy/pull/369:122,safety,updat,updated,122,"Together with the suggested changes, I am also updating my usual notebook containing examples of all the plots (~~not yet updated:~~ https://gist.github.com/fidelram/2289b7a8d6da055fb058ac9a79ed485c). However, don't you think that this could be part of a the scanpy tutorials section?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369
https://github.com/scverse/scanpy/pull/369:47,security,updat,updating,47,"Together with the suggested changes, I am also updating my usual notebook containing examples of all the plots (~~not yet updated:~~ https://gist.github.com/fidelram/2289b7a8d6da055fb058ac9a79ed485c). However, don't you think that this could be part of a the scanpy tutorials section?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369
https://github.com/scverse/scanpy/pull/369:122,security,updat,updated,122,"Together with the suggested changes, I am also updating my usual notebook containing examples of all the plots (~~not yet updated:~~ https://gist.github.com/fidelram/2289b7a8d6da055fb058ac9a79ed485c). However, don't you think that this could be part of a the scanpy tutorials section?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369
https://github.com/scverse/scanpy/pull/369:108,deployability,integr,integrated,108,"@fidelram, I really like your plotting gallery! Would be cool to have that as part of the tutorials or even integrated in the main documentation (enhance each plotting function with an example image?)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369
https://github.com/scverse/scanpy/pull/369:57,energy efficiency,cool,cool,57,"@fidelram, I really like your plotting gallery! Would be cool to have that as part of the tutorials or even integrated in the main documentation (enhance each plotting function with an example image?)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369
https://github.com/scverse/scanpy/pull/369:108,integrability,integr,integrated,108,"@fidelram, I really like your plotting gallery! Would be cool to have that as part of the tutorials or even integrated in the main documentation (enhance each plotting function with an example image?)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369
https://github.com/scverse/scanpy/pull/369:108,interoperability,integr,integrated,108,"@fidelram, I really like your plotting gallery! Would be cool to have that as part of the tutorials or even integrated in the main documentation (enhance each plotting function with an example image?)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369
https://github.com/scverse/scanpy/pull/369:108,modifiability,integr,integrated,108,"@fidelram, I really like your plotting gallery! Would be cool to have that as part of the tutorials or even integrated in the main documentation (enhance each plotting function with an example image?)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369
https://github.com/scverse/scanpy/pull/369:108,reliability,integr,integrated,108,"@fidelram, I really like your plotting gallery! Would be cool to have that as part of the tutorials or even integrated in the main documentation (enhance each plotting function with an example image?)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369
https://github.com/scverse/scanpy/pull/369:108,security,integr,integrated,108,"@fidelram, I really like your plotting gallery! Would be cool to have that as part of the tutorials or even integrated in the main documentation (enhance each plotting function with an example image?)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369
https://github.com/scverse/scanpy/pull/369:108,testability,integr,integrated,108,"@fidelram, I really like your plotting gallery! Would be cool to have that as part of the tutorials or even integrated in the main documentation (enhance each plotting function with an example image?)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369
https://github.com/scverse/scanpy/pull/369:131,usability,document,documentation,131,"@fidelram, I really like your plotting gallery! Would be cool to have that as part of the tutorials or even integrated in the main documentation (enhance each plotting function with an example image?)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369
https://github.com/scverse/scanpy/pull/369:94,energy efficiency,current,currently,94,"OK, my changes in 426f028708cdd203b7d97d48eb558e695090da82 didnt make the tests break! Do we currently not use the plotting test results?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369
https://github.com/scverse/scanpy/pull/369:75,safety,test,tests,75,"OK, my changes in 426f028708cdd203b7d97d48eb558e695090da82 didnt make the tests break! Do we currently not use the plotting test results?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369
https://github.com/scverse/scanpy/pull/369:125,safety,test,test,125,"OK, my changes in 426f028708cdd203b7d97d48eb558e695090da82 didnt make the tests break! Do we currently not use the plotting test results?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369
https://github.com/scverse/scanpy/pull/369:75,testability,test,tests,75,"OK, my changes in 426f028708cdd203b7d97d48eb558e695090da82 didnt make the tests break! Do we currently not use the plotting test results?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369
https://github.com/scverse/scanpy/pull/369:125,testability,test,test,125,"OK, my changes in 426f028708cdd203b7d97d48eb558e695090da82 didnt make the tests break! Do we currently not use the plotting test results?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369
https://github.com/scverse/scanpy/pull/369:300,deployability,build,build,300,"> However, don't you think that this could be part of a the scanpy tutorials section? Of course, this should become a part of the scanpy tutorials section! That would be awesome! I already asked you for that some time ago. :wink: I'd also start adding calls producing images to the doc pages. Let me build the core infrastructure for having the tutorials run on readthedocs and adding notebooks containing only code to the scanpy main repo. If you want, to shortcut, you can make a PR to scanpy_usage and upload your notebook there. Or, equally well, we simply link to your notebook from the tutorials page.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369
https://github.com/scverse/scanpy/pull/369:315,deployability,infrastructur,infrastructure,315,"> However, don't you think that this could be part of a the scanpy tutorials section? Of course, this should become a part of the scanpy tutorials section! That would be awesome! I already asked you for that some time ago. :wink: I'd also start adding calls producing images to the doc pages. Let me build the core infrastructure for having the tutorials run on readthedocs and adding notebooks containing only code to the scanpy main repo. If you want, to shortcut, you can make a PR to scanpy_usage and upload your notebook there. Or, equally well, we simply link to your notebook from the tutorials page.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369
https://github.com/scverse/scanpy/pull/369:395,deployability,contain,containing,395,"> However, don't you think that this could be part of a the scanpy tutorials section? Of course, this should become a part of the scanpy tutorials section! That would be awesome! I already asked you for that some time ago. :wink: I'd also start adding calls producing images to the doc pages. Let me build the core infrastructure for having the tutorials run on readthedocs and adding notebooks containing only code to the scanpy main repo. If you want, to shortcut, you can make a PR to scanpy_usage and upload your notebook there. Or, equally well, we simply link to your notebook from the tutorials page.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369
https://github.com/scverse/scanpy/pull/369:310,energy efficiency,core,core,310,"> However, don't you think that this could be part of a the scanpy tutorials section? Of course, this should become a part of the scanpy tutorials section! That would be awesome! I already asked you for that some time ago. :wink: I'd also start adding calls producing images to the doc pages. Let me build the core infrastructure for having the tutorials run on readthedocs and adding notebooks containing only code to the scanpy main repo. If you want, to shortcut, you can make a PR to scanpy_usage and upload your notebook there. Or, equally well, we simply link to your notebook from the tutorials page.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369
https://github.com/scverse/scanpy/pull/369:213,performance,time,time,213,"> However, don't you think that this could be part of a the scanpy tutorials section? Of course, this should become a part of the scanpy tutorials section! That would be awesome! I already asked you for that some time ago. :wink: I'd also start adding calls producing images to the doc pages. Let me build the core infrastructure for having the tutorials run on readthedocs and adding notebooks containing only code to the scanpy main repo. If you want, to shortcut, you can make a PR to scanpy_usage and upload your notebook there. Or, equally well, we simply link to your notebook from the tutorials page.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369
https://github.com/scverse/scanpy/pull/369:554,testability,simpl,simply,554,"> However, don't you think that this could be part of a the scanpy tutorials section? Of course, this should become a part of the scanpy tutorials section! That would be awesome! I already asked you for that some time ago. :wink: I'd also start adding calls producing images to the doc pages. Let me build the core infrastructure for having the tutorials run on readthedocs and adding notebooks containing only code to the scanpy main repo. If you want, to shortcut, you can make a PR to scanpy_usage and upload your notebook there. Or, equally well, we simply link to your notebook from the tutorials page.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369
https://github.com/scverse/scanpy/pull/369:457,usability,shortcut,shortcut,457,"> However, don't you think that this could be part of a the scanpy tutorials section? Of course, this should become a part of the scanpy tutorials section! That would be awesome! I already asked you for that some time ago. :wink: I'd also start adding calls producing images to the doc pages. Let me build the core infrastructure for having the tutorials run on readthedocs and adding notebooks containing only code to the scanpy main repo. If you want, to shortcut, you can make a PR to scanpy_usage and upload your notebook there. Or, equally well, we simply link to your notebook from the tutorials page.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369
https://github.com/scverse/scanpy/pull/369:554,usability,simpl,simply,554,"> However, don't you think that this could be part of a the scanpy tutorials section? Of course, this should become a part of the scanpy tutorials section! That would be awesome! I already asked you for that some time ago. :wink: I'd also start adding calls producing images to the doc pages. Let me build the core infrastructure for having the tutorials run on readthedocs and adding notebooks containing only code to the scanpy main repo. If you want, to shortcut, you can make a PR to scanpy_usage and upload your notebook there. Or, equally well, we simply link to your notebook from the tutorials page.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369
https://github.com/scverse/scanpy/pull/369:0,energy efficiency,Green,Green,0,Green light on my side to merge.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369
https://github.com/scverse/scanpy/pull/369:69,deployability,updat,update,69,"See my last comment. After fixing the colormaps in this PR, I didnt update the images, but the tests still pass. Whats up with that?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369
https://github.com/scverse/scanpy/pull/369:69,safety,updat,update,69,"See my last comment. After fixing the colormaps in this PR, I didnt update the images, but the tests still pass. Whats up with that?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369
https://github.com/scverse/scanpy/pull/369:96,safety,test,tests,96,"See my last comment. After fixing the colormaps in this PR, I didnt update the images, but the tests still pass. Whats up with that?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369
https://github.com/scverse/scanpy/pull/369:69,security,updat,update,69,"See my last comment. After fixing the colormaps in this PR, I didnt update the images, but the tests still pass. Whats up with that?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369
https://github.com/scverse/scanpy/pull/369:96,testability,test,tests,96,"See my last comment. After fixing the colormaps in this PR, I didnt update the images, but the tests still pass. Whats up with that?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369
https://github.com/scverse/scanpy/pull/369:103,safety,test,tests,103,"@flying-sheep I think that your changes should produce images that are almost equal to the ones on the tests as your changes simply introduce a different way to get the colormap. Btw, what is the advantage of using `ListedColormap` and `BoundaryNorm` instead of `LinearSegmentedColormap` ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369
https://github.com/scverse/scanpy/pull/369:103,testability,test,tests,103,"@flying-sheep I think that your changes should produce images that are almost equal to the ones on the tests as your changes simply introduce a different way to get the colormap. Btw, what is the advantage of using `ListedColormap` and `BoundaryNorm` instead of `LinearSegmentedColormap` ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369
https://github.com/scverse/scanpy/pull/369:125,testability,simpl,simply,125,"@flying-sheep I think that your changes should produce images that are almost equal to the ones on the tests as your changes simply introduce a different way to get the colormap. Btw, what is the advantage of using `ListedColormap` and `BoundaryNorm` instead of `LinearSegmentedColormap` ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369
https://github.com/scverse/scanpy/pull/369:125,usability,simpl,simply,125,"@flying-sheep I think that your changes should produce images that are almost equal to the ones on the tests as your changes simply introduce a different way to get the colormap. Btw, what is the advantage of using `ListedColormap` and `BoundaryNorm` instead of `LinearSegmentedColormap` ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369
https://github.com/scverse/scanpy/pull/369:623,deployability,continu,continuous,623,"The important part is the `BoundaryNorm`. We got a really weird selection of colors without it, since the default is to treat the colormap as a linear space. `max(vec)` gets the last color, `min(vec)` the first one, and everything else some color between. Using the `BoundaryNorm` I defined, numbers in `[0, len(colors)-1]` get the color at the respective index, and everything smaller or bigger would get the first or last color (not ideal, but better than what we had). I dont think it really makes a difference, but ListedColormap is a colormap for discrete uses like ours, LinearSegmentedColormap is for interpolating continuous values onto the map. Before | After. --- | ---. ![before](https://user-images.githubusercontent.com/291575/48907731-3ba8f600-ee60-11e8-9b87-8e095f6ed764.png) | ![after](https://user-images.githubusercontent.com/291575/49027776-e25f0080-f198-11e8-825e-1e98659cbc3a.png). In the before pic, we map `[0,1,2,3]` onto `[0;19]`, which results in `[0, 5.75, 10.5, 14.25, 19]`, and `[to_hex(tab20.colors[math.ceil(i)]) for i in [0, 5.75, 10.5, 14.25, 19]]` gives us [`#1f77b4`, `#98df8a`, `#8c564b`, `#c7c7c7`, `#9edae5`].",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369
https://github.com/scverse/scanpy/pull/369:700,usability,user,user-images,700,"The important part is the `BoundaryNorm`. We got a really weird selection of colors without it, since the default is to treat the colormap as a linear space. `max(vec)` gets the last color, `min(vec)` the first one, and everything else some color between. Using the `BoundaryNorm` I defined, numbers in `[0, len(colors)-1]` get the color at the respective index, and everything smaller or bigger would get the first or last color (not ideal, but better than what we had). I dont think it really makes a difference, but ListedColormap is a colormap for discrete uses like ours, LinearSegmentedColormap is for interpolating continuous values onto the map. Before | After. --- | ---. ![before](https://user-images.githubusercontent.com/291575/48907731-3ba8f600-ee60-11e8-9b87-8e095f6ed764.png) | ![after](https://user-images.githubusercontent.com/291575/49027776-e25f0080-f198-11e8-825e-1e98659cbc3a.png). In the before pic, we map `[0,1,2,3]` onto `[0;19]`, which results in `[0, 5.75, 10.5, 14.25, 19]`, and `[to_hex(tab20.colors[math.ceil(i)]) for i in [0, 5.75, 10.5, 14.25, 19]]` gives us [`#1f77b4`, `#98df8a`, `#8c564b`, `#c7c7c7`, `#9edae5`].",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369
https://github.com/scverse/scanpy/pull/369:811,usability,user,user-images,811,"The important part is the `BoundaryNorm`. We got a really weird selection of colors without it, since the default is to treat the colormap as a linear space. `max(vec)` gets the last color, `min(vec)` the first one, and everything else some color between. Using the `BoundaryNorm` I defined, numbers in `[0, len(colors)-1]` get the color at the respective index, and everything smaller or bigger would get the first or last color (not ideal, but better than what we had). I dont think it really makes a difference, but ListedColormap is a colormap for discrete uses like ours, LinearSegmentedColormap is for interpolating continuous values onto the map. Before | After. --- | ---. ![before](https://user-images.githubusercontent.com/291575/48907731-3ba8f600-ee60-11e8-9b87-8e095f6ed764.png) | ![after](https://user-images.githubusercontent.com/291575/49027776-e25f0080-f198-11e8-825e-1e98659cbc3a.png). In the before pic, we map `[0,1,2,3]` onto `[0;19]`, which results in `[0, 5.75, 10.5, 14.25, 19]`, and `[to_hex(tab20.colors[math.ceil(i)]) for i in [0, 5.75, 10.5, 14.25, 19]]` gives us [`#1f77b4`, `#98df8a`, `#8c564b`, `#c7c7c7`, `#9edae5`].",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369
https://github.com/scverse/scanpy/pull/369:35,deployability,Releas,Release,35,Thank you again! I'm merging this. Release prior to this is 1.3.4.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369
https://github.com/scverse/scanpy/pull/369:324,integrability,event,event-,324,"Great! Thanks. On Fri, Nov 30, 2018 at 3:23 AM Alex Wolf <notifications@github.com> wrote:. > Merged #369 <https://github.com/theislab/scanpy/pull/369> into master. >. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/369#event-1996905539>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1doJSSvm59saaB0TBQ_ix50EbwNfks5u0JaEgaJpZM4YrJn3>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369
https://github.com/scverse/scanpy/pull/369:414,security,auth,auth,414,"Great! Thanks. On Fri, Nov 30, 2018 at 3:23 AM Alex Wolf <notifications@github.com> wrote:. > Merged #369 <https://github.com/theislab/scanpy/pull/369> into master. >. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/369#event-1996905539>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1doJSSvm59saaB0TBQ_ix50EbwNfks5u0JaEgaJpZM4YrJn3>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369
https://github.com/scverse/scanpy/issues/370:21,deployability,version,version,21,I should add. scanpy version = '1.3.2'. Python 3.6.6.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/370
https://github.com/scverse/scanpy/issues/370:21,integrability,version,version,21,I should add. scanpy version = '1.3.2'. Python 3.6.6.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/370
https://github.com/scverse/scanpy/issues/370:21,modifiability,version,version,21,I should add. scanpy version = '1.3.2'. Python 3.6.6.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/370
https://github.com/scverse/scanpy/issues/370:2,deployability,upgrad,upgraded,2,I upgraded to 1.3.3 and the bug persists. . PS: I accidentally closed the issue for some reason.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/370
https://github.com/scverse/scanpy/issues/370:2,modifiability,upgrad,upgraded,2,I upgraded to 1.3.3 and the bug persists. . PS: I accidentally closed the issue for some reason.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/370
https://github.com/scverse/scanpy/issues/370:50,safety,accid,accidentally,50,I upgraded to 1.3.3 and the bug persists. . PS: I accidentally closed the issue for some reason.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/370
https://github.com/scverse/scanpy/issues/370:63,usability,close,closed,63,I upgraded to 1.3.3 and the bug persists. . PS: I accidentally closed the issue for some reason.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/370
https://github.com/scverse/scanpy/issues/370:152,deployability,upgrad,upgraded,152,"I can reproduce the problem. Very strange. I will submit a PR to fix it. On Wed, Nov 21, 2018 at 5:03 AM Andreas <notifications@github.com> wrote:. > I upgraded to 1.3.3 and the bug persists. >. > PS: I accidentally closed the issue for some reason. >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/370#issuecomment-440522061>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1b3a4RVAX6v4o3oY_e3a1sh1Rnq2ks5uxNCCgaJpZM4YrmLi>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/370
https://github.com/scverse/scanpy/issues/370:50,integrability,sub,submit,50,"I can reproduce the problem. Very strange. I will submit a PR to fix it. On Wed, Nov 21, 2018 at 5:03 AM Andreas <notifications@github.com> wrote:. > I upgraded to 1.3.3 and the bug persists. >. > PS: I accidentally closed the issue for some reason. >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/370#issuecomment-440522061>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1b3a4RVAX6v4o3oY_e3a1sh1Rnq2ks5uxNCCgaJpZM4YrmLi>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/370
https://github.com/scverse/scanpy/issues/370:299,integrability,sub,subscribed,299,"I can reproduce the problem. Very strange. I will submit a PR to fix it. On Wed, Nov 21, 2018 at 5:03 AM Andreas <notifications@github.com> wrote:. > I upgraded to 1.3.3 and the bug persists. >. > PS: I accidentally closed the issue for some reason. >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/370#issuecomment-440522061>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1b3a4RVAX6v4o3oY_e3a1sh1Rnq2ks5uxNCCgaJpZM4YrmLi>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/370
https://github.com/scverse/scanpy/issues/370:152,modifiability,upgrad,upgraded,152,"I can reproduce the problem. Very strange. I will submit a PR to fix it. On Wed, Nov 21, 2018 at 5:03 AM Andreas <notifications@github.com> wrote:. > I upgraded to 1.3.3 and the bug persists. >. > PS: I accidentally closed the issue for some reason. >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/370#issuecomment-440522061>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1b3a4RVAX6v4o3oY_e3a1sh1Rnq2ks5uxNCCgaJpZM4YrmLi>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/370
https://github.com/scverse/scanpy/issues/370:203,safety,accid,accidentally,203,"I can reproduce the problem. Very strange. I will submit a PR to fix it. On Wed, Nov 21, 2018 at 5:03 AM Andreas <notifications@github.com> wrote:. > I upgraded to 1.3.3 and the bug persists. >. > PS: I accidentally closed the issue for some reason. >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/370#issuecomment-440522061>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1b3a4RVAX6v4o3oY_e3a1sh1Rnq2ks5uxNCCgaJpZM4YrmLi>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/370
https://github.com/scverse/scanpy/issues/370:522,security,auth,auth,522,"I can reproduce the problem. Very strange. I will submit a PR to fix it. On Wed, Nov 21, 2018 at 5:03 AM Andreas <notifications@github.com> wrote:. > I upgraded to 1.3.3 and the bug persists. >. > PS: I accidentally closed the issue for some reason. >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/370#issuecomment-440522061>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1b3a4RVAX6v4o3oY_e3a1sh1Rnq2ks5uxNCCgaJpZM4YrmLi>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/370
https://github.com/scverse/scanpy/issues/370:216,usability,close,closed,216,"I can reproduce the problem. Very strange. I will submit a PR to fix it. On Wed, Nov 21, 2018 at 5:03 AM Andreas <notifications@github.com> wrote:. > I upgraded to 1.3.3 and the bug persists. >. > PS: I accidentally closed the issue for some reason. >. > . > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/370#issuecomment-440522061>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1b3a4RVAX6v4o3oY_e3a1sh1Rnq2ks5uxNCCgaJpZM4YrmLi>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/370
https://github.com/scverse/scanpy/pull/371:225,integrability,filter,filtering,225,Also one problem is that i don't understand where to put `materialize_as_ndarray` as it used [here](https://github.com/theislab/scanpy/blob/44c038ad7b6488407958ab020858923b25368d97/scanpy/preprocessing/simple.py#L598) and in filtering.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/371
https://github.com/scverse/scanpy/pull/371:33,testability,understand,understand,33,Also one problem is that i don't understand where to put `materialize_as_ndarray` as it used [here](https://github.com/theislab/scanpy/blob/44c038ad7b6488407958ab020858923b25368d97/scanpy/preprocessing/simple.py#L598) and in filtering.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/371
https://github.com/scverse/scanpy/pull/371:202,testability,simpl,simple,202,Also one problem is that i don't understand where to put `materialize_as_ndarray` as it used [here](https://github.com/theislab/scanpy/blob/44c038ad7b6488407958ab020858923b25368d97/scanpy/preprocessing/simple.py#L598) and in filtering.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/371
https://github.com/scverse/scanpy/pull/371:202,usability,simpl,simple,202,Also one problem is that i don't understand where to put `materialize_as_ndarray` as it used [here](https://github.com/theislab/scanpy/blob/44c038ad7b6488407958ab020858923b25368d97/scanpy/preprocessing/simple.py#L598) and in filtering.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/371
https://github.com/scverse/scanpy/pull/371:131,deployability,stage,stage,131,"If you're not able to figure out `matieralize_as_ndarray`, that's fine. That's for the distributed part, which can come at a later stage. Important is that the old `normalize_per_cell` function continues to work as expected.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/371
https://github.com/scverse/scanpy/pull/371:194,deployability,continu,continues,194,"If you're not able to figure out `matieralize_as_ndarray`, that's fine. That's for the distributed part, which can come at a later stage. Important is that the old `normalize_per_cell` function continues to work as expected.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/371
https://github.com/scverse/scanpy/pull/371:87,interoperability,distribut,distributed,87,"If you're not able to figure out `matieralize_as_ndarray`, that's fine. That's for the distributed part, which can come at a later stage. Important is that the old `normalize_per_cell` function continues to work as expected.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/371
https://github.com/scverse/scanpy/pull/371:167,deployability,version,version,167,"Hi, @falexwolf . I think so. However, there is a question of what to do with current normalize_per_cell. Should i move it to deprecated? Also, it supports zarr and my version doesn't.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/371
https://github.com/scverse/scanpy/pull/371:77,energy efficiency,current,current,77,"Hi, @falexwolf . I think so. However, there is a question of what to do with current normalize_per_cell. Should i move it to deprecated? Also, it supports zarr and my version doesn't.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/371
https://github.com/scverse/scanpy/pull/371:167,integrability,version,version,167,"Hi, @falexwolf . I think so. However, there is a question of what to do with current normalize_per_cell. Should i move it to deprecated? Also, it supports zarr and my version doesn't.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/371
https://github.com/scverse/scanpy/pull/371:167,modifiability,version,version,167,"Hi, @falexwolf . I think so. However, there is a question of what to do with current normalize_per_cell. Should i move it to deprecated? Also, it supports zarr and my version doesn't.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/371
https://github.com/scverse/scanpy/pull/371:175,reliability,doe,doesn,175,"Hi, @falexwolf . I think so. However, there is a question of what to do with current normalize_per_cell. Should i move it to deprecated? Also, it supports zarr and my version doesn't.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/371
https://github.com/scverse/scanpy/pull/371:146,usability,support,supports,146,"Hi, @falexwolf . I think so. However, there is a question of what to do with current normalize_per_cell. Should i move it to deprecated? Also, it supports zarr and my version doesn't.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/371
https://github.com/scverse/scanpy/pull/371:31,deployability,continu,continue,31,"@tomwhite: Are you planning to continue to develop and maintain the distributed backend of Scanpy? If yes, should we announce it at some point? I already added you to the authors' list but there is still no entry here: https://scanpy.readthedocs.io/en/latest/#on-master-january-1-2019 There should definitely be. There should also be an announcement on twitter. @Koncopd: If Tom's answer is ""yes"", we definitely need support of zarr/dask in the new functions. Sorry, that I wasn't explicit about it...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/371
https://github.com/scverse/scanpy/pull/371:68,interoperability,distribut,distributed,68,"@tomwhite: Are you planning to continue to develop and maintain the distributed backend of Scanpy? If yes, should we announce it at some point? I already added you to the authors' list but there is still no entry here: https://scanpy.readthedocs.io/en/latest/#on-master-january-1-2019 There should definitely be. There should also be an announcement on twitter. @Koncopd: If Tom's answer is ""yes"", we definitely need support of zarr/dask in the new functions. Sorry, that I wasn't explicit about it...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/371
https://github.com/scverse/scanpy/pull/371:55,modifiability,maintain,maintain,55,"@tomwhite: Are you planning to continue to develop and maintain the distributed backend of Scanpy? If yes, should we announce it at some point? I already added you to the authors' list but there is still no entry here: https://scanpy.readthedocs.io/en/latest/#on-master-january-1-2019 There should definitely be. There should also be an announcement on twitter. @Koncopd: If Tom's answer is ""yes"", we definitely need support of zarr/dask in the new functions. Sorry, that I wasn't explicit about it...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/371
https://github.com/scverse/scanpy/pull/371:55,safety,maintain,maintain,55,"@tomwhite: Are you planning to continue to develop and maintain the distributed backend of Scanpy? If yes, should we announce it at some point? I already added you to the authors' list but there is still no entry here: https://scanpy.readthedocs.io/en/latest/#on-master-january-1-2019 There should definitely be. There should also be an announcement on twitter. @Koncopd: If Tom's answer is ""yes"", we definitely need support of zarr/dask in the new functions. Sorry, that I wasn't explicit about it...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/371
https://github.com/scverse/scanpy/pull/371:171,security,auth,authors,171,"@tomwhite: Are you planning to continue to develop and maintain the distributed backend of Scanpy? If yes, should we announce it at some point? I already added you to the authors' list but there is still no entry here: https://scanpy.readthedocs.io/en/latest/#on-master-january-1-2019 There should definitely be. There should also be an announcement on twitter. @Koncopd: If Tom's answer is ""yes"", we definitely need support of zarr/dask in the new functions. Sorry, that I wasn't explicit about it...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/371
https://github.com/scverse/scanpy/pull/371:19,testability,plan,planning,19,"@tomwhite: Are you planning to continue to develop and maintain the distributed backend of Scanpy? If yes, should we announce it at some point? I already added you to the authors' list but there is still no entry here: https://scanpy.readthedocs.io/en/latest/#on-master-january-1-2019 There should definitely be. There should also be an announcement on twitter. @Koncopd: If Tom's answer is ""yes"", we definitely need support of zarr/dask in the new functions. Sorry, that I wasn't explicit about it...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/371
https://github.com/scverse/scanpy/pull/371:417,usability,support,support,417,"@tomwhite: Are you planning to continue to develop and maintain the distributed backend of Scanpy? If yes, should we announce it at some point? I already added you to the authors' list but there is still no entry here: https://scanpy.readthedocs.io/en/latest/#on-master-january-1-2019 There should definitely be. There should also be an announcement on twitter. @Koncopd: If Tom's answer is ""yes"", we definitely need support of zarr/dask in the new functions. Sorry, that I wasn't explicit about it...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/371
https://github.com/scverse/scanpy/pull/371:34,deployability,releas,release,34,"@tomwhite OK, I added this to the release notes (https://github.com/theislab/scanpy/commit/cee23dc13cf2b77d8e23ee0f91eb55fac0e35ed8, sorry confounded with some style change); it would be nice to have a link to your performance benchmarks... Let me know when we should announce it on twitter. I'm also happy to retweet...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/371
https://github.com/scverse/scanpy/pull/371:215,performance,perform,performance,215,"@tomwhite OK, I added this to the release notes (https://github.com/theislab/scanpy/commit/cee23dc13cf2b77d8e23ee0f91eb55fac0e35ed8, sorry confounded with some style change); it would be nice to have a link to your performance benchmarks... Let me know when we should announce it on twitter. I'm also happy to retweet...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/371
https://github.com/scverse/scanpy/pull/371:215,usability,perform,performance,215,"@tomwhite OK, I added this to the release notes (https://github.com/theislab/scanpy/commit/cee23dc13cf2b77d8e23ee0f91eb55fac0e35ed8, sorry confounded with some style change); it would be nice to have a link to your performance benchmarks... Let me know when we should announce it on twitter. I'm also happy to retweet...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/371
https://github.com/scverse/scanpy/pull/371:19,usability,support,support,19,"Ok, i will add the support for them.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/371
https://github.com/scverse/scanpy/issues/373:68,availability,state,stated,68,"Hi! I think we have a different focus here, and not all of what you stated as fact is correct, so Ill do my best to clear this up:. 1. There is an advantage for type hints in common Scanpy usage. IPython should use Jedi to create autocompletions since this summer, but they forgot to reenable it. I sent them an issue to do so, ipython/ipython#11503 and a fix in ipython/ipython#11506. Jedi supports type hints, so with `c.Completer.use_jedi = True` now or by default in a month, people will profit from them. Furthermore, people are using scanpy in applications and scripts, not just in notebooks. When you use an IDE (or install the jedi extension in EMACS) you should profit from it. 2. The Jupyter shift-tab help being hard to read in the presence of type hints is what I consider a bug. I reported it in ipython/ipython#11504 and fixed it in ipython/ipython#11505. 3. The numpy is on it (see [here](https://github.com/numpy/numpy-stubs)) and will probably integrate it once there needs to be no Python 2 compat. e.g. scikit-learn waits for numpy: scikit-learn/scikit-learn#11170. I see your concern about entry hurdles, but I dont agree. Its super easy. `Union` is or, `Optional` is or `None`. If theres questions, they can be answered. (or people click on the links in the docs and read like one sentence of explanation). 4. If you want we can change how all that is rendered. `Union[a, b]` could be done as ``` :class:`a` or :class:`b` ``` But its really not hard. Honestly I think the `Callable[]` is much better than the textual description that was there before: Until it was there, people (including me when i was writing that annotation) had to dive into the code to figure out what function signature is *really* expected there. Now they have to be able to parse what that `Callable[[a,b], c]` there means. If they have never encountered it before, they can click on it, read one sentence of explanation and know that `a` and `b` are parameters and `c` the return type. Done i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:624,deployability,instal,install,624,"Hi! I think we have a different focus here, and not all of what you stated as fact is correct, so Ill do my best to clear this up:. 1. There is an advantage for type hints in common Scanpy usage. IPython should use Jedi to create autocompletions since this summer, but they forgot to reenable it. I sent them an issue to do so, ipython/ipython#11503 and a fix in ipython/ipython#11506. Jedi supports type hints, so with `c.Completer.use_jedi = True` now or by default in a month, people will profit from them. Furthermore, people are using scanpy in applications and scripts, not just in notebooks. When you use an IDE (or install the jedi extension in EMACS) you should profit from it. 2. The Jupyter shift-tab help being hard to read in the presence of type hints is what I consider a bug. I reported it in ipython/ipython#11504 and fixed it in ipython/ipython#11505. 3. The numpy is on it (see [here](https://github.com/numpy/numpy-stubs)) and will probably integrate it once there needs to be no Python 2 compat. e.g. scikit-learn waits for numpy: scikit-learn/scikit-learn#11170. I see your concern about entry hurdles, but I dont agree. Its super easy. `Union` is or, `Optional` is or `None`. If theres questions, they can be answered. (or people click on the links in the docs and read like one sentence of explanation). 4. If you want we can change how all that is rendered. `Union[a, b]` could be done as ``` :class:`a` or :class:`b` ``` But its really not hard. Honestly I think the `Callable[]` is much better than the textual description that was there before: Until it was there, people (including me when i was writing that annotation) had to dive into the code to figure out what function signature is *really* expected there. Now they have to be able to parse what that `Callable[[a,b], c]` there means. If they have never encountered it before, they can click on it, read one sentence of explanation and know that `a` and `b` are parameters and `c` the return type. Done i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:962,deployability,integr,integrate,962,"Hi! I think we have a different focus here, and not all of what you stated as fact is correct, so Ill do my best to clear this up:. 1. There is an advantage for type hints in common Scanpy usage. IPython should use Jedi to create autocompletions since this summer, but they forgot to reenable it. I sent them an issue to do so, ipython/ipython#11503 and a fix in ipython/ipython#11506. Jedi supports type hints, so with `c.Completer.use_jedi = True` now or by default in a month, people will profit from them. Furthermore, people are using scanpy in applications and scripts, not just in notebooks. When you use an IDE (or install the jedi extension in EMACS) you should profit from it. 2. The Jupyter shift-tab help being hard to read in the presence of type hints is what I consider a bug. I reported it in ipython/ipython#11504 and fixed it in ipython/ipython#11505. 3. The numpy is on it (see [here](https://github.com/numpy/numpy-stubs)) and will probably integrate it once there needs to be no Python 2 compat. e.g. scikit-learn waits for numpy: scikit-learn/scikit-learn#11170. I see your concern about entry hurdles, but I dont agree. Its super easy. `Union` is or, `Optional` is or `None`. If theres questions, they can be answered. (or people click on the links in the docs and read like one sentence of explanation). 4. If you want we can change how all that is rendered. `Union[a, b]` could be done as ``` :class:`a` or :class:`b` ``` But its really not hard. Honestly I think the `Callable[]` is much better than the textual description that was there before: Until it was there, people (including me when i was writing that annotation) had to dive into the code to figure out what function signature is *really* expected there. Now they have to be able to parse what that `Callable[[a,b], c]` there means. If they have never encountered it before, they can click on it, read one sentence of explanation and know that `a` and `b` are parameters and `c` the return type. Done i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:68,integrability,state,stated,68,"Hi! I think we have a different focus here, and not all of what you stated as fact is correct, so Ill do my best to clear this up:. 1. There is an advantage for type hints in common Scanpy usage. IPython should use Jedi to create autocompletions since this summer, but they forgot to reenable it. I sent them an issue to do so, ipython/ipython#11503 and a fix in ipython/ipython#11506. Jedi supports type hints, so with `c.Completer.use_jedi = True` now or by default in a month, people will profit from them. Furthermore, people are using scanpy in applications and scripts, not just in notebooks. When you use an IDE (or install the jedi extension in EMACS) you should profit from it. 2. The Jupyter shift-tab help being hard to read in the presence of type hints is what I consider a bug. I reported it in ipython/ipython#11504 and fixed it in ipython/ipython#11505. 3. The numpy is on it (see [here](https://github.com/numpy/numpy-stubs)) and will probably integrate it once there needs to be no Python 2 compat. e.g. scikit-learn waits for numpy: scikit-learn/scikit-learn#11170. I see your concern about entry hurdles, but I dont agree. Its super easy. `Union` is or, `Optional` is or `None`. If theres questions, they can be answered. (or people click on the links in the docs and read like one sentence of explanation). 4. If you want we can change how all that is rendered. `Union[a, b]` could be done as ``` :class:`a` or :class:`b` ``` But its really not hard. Honestly I think the `Callable[]` is much better than the textual description that was there before: Until it was there, people (including me when i was writing that annotation) had to dive into the code to figure out what function signature is *really* expected there. Now they have to be able to parse what that `Callable[[a,b], c]` there means. If they have never encountered it before, they can click on it, read one sentence of explanation and know that `a` and `b` are parameters and `c` the return type. Done i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:962,integrability,integr,integrate,962,"Hi! I think we have a different focus here, and not all of what you stated as fact is correct, so Ill do my best to clear this up:. 1. There is an advantage for type hints in common Scanpy usage. IPython should use Jedi to create autocompletions since this summer, but they forgot to reenable it. I sent them an issue to do so, ipython/ipython#11503 and a fix in ipython/ipython#11506. Jedi supports type hints, so with `c.Completer.use_jedi = True` now or by default in a month, people will profit from them. Furthermore, people are using scanpy in applications and scripts, not just in notebooks. When you use an IDE (or install the jedi extension in EMACS) you should profit from it. 2. The Jupyter shift-tab help being hard to read in the presence of type hints is what I consider a bug. I reported it in ipython/ipython#11504 and fixed it in ipython/ipython#11505. 3. The numpy is on it (see [here](https://github.com/numpy/numpy-stubs)) and will probably integrate it once there needs to be no Python 2 compat. e.g. scikit-learn waits for numpy: scikit-learn/scikit-learn#11170. I see your concern about entry hurdles, but I dont agree. Its super easy. `Union` is or, `Optional` is or `None`. If theres questions, they can be answered. (or people click on the links in the docs and read like one sentence of explanation). 4. If you want we can change how all that is rendered. `Union[a, b]` could be done as ``` :class:`a` or :class:`b` ``` But its really not hard. Honestly I think the `Callable[]` is much better than the textual description that was there before: Until it was there, people (including me when i was writing that annotation) had to dive into the code to figure out what function signature is *really* expected there. Now they have to be able to parse what that `Callable[[a,b], c]` there means. If they have never encountered it before, they can click on it, read one sentence of explanation and know that `a` and `b` are parameters and `c` the return type. Done i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:936,interoperability,stub,stubs,936,"Hi! I think we have a different focus here, and not all of what you stated as fact is correct, so Ill do my best to clear this up:. 1. There is an advantage for type hints in common Scanpy usage. IPython should use Jedi to create autocompletions since this summer, but they forgot to reenable it. I sent them an issue to do so, ipython/ipython#11503 and a fix in ipython/ipython#11506. Jedi supports type hints, so with `c.Completer.use_jedi = True` now or by default in a month, people will profit from them. Furthermore, people are using scanpy in applications and scripts, not just in notebooks. When you use an IDE (or install the jedi extension in EMACS) you should profit from it. 2. The Jupyter shift-tab help being hard to read in the presence of type hints is what I consider a bug. I reported it in ipython/ipython#11504 and fixed it in ipython/ipython#11505. 3. The numpy is on it (see [here](https://github.com/numpy/numpy-stubs)) and will probably integrate it once there needs to be no Python 2 compat. e.g. scikit-learn waits for numpy: scikit-learn/scikit-learn#11170. I see your concern about entry hurdles, but I dont agree. Its super easy. `Union` is or, `Optional` is or `None`. If theres questions, they can be answered. (or people click on the links in the docs and read like one sentence of explanation). 4. If you want we can change how all that is rendered. `Union[a, b]` could be done as ``` :class:`a` or :class:`b` ``` But its really not hard. Honestly I think the `Callable[]` is much better than the textual description that was there before: Until it was there, people (including me when i was writing that annotation) had to dive into the code to figure out what function signature is *really* expected there. Now they have to be able to parse what that `Callable[[a,b], c]` there means. If they have never encountered it before, they can click on it, read one sentence of explanation and know that `a` and `b` are parameters and `c` the return type. Done i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:962,interoperability,integr,integrate,962,"Hi! I think we have a different focus here, and not all of what you stated as fact is correct, so Ill do my best to clear this up:. 1. There is an advantage for type hints in common Scanpy usage. IPython should use Jedi to create autocompletions since this summer, but they forgot to reenable it. I sent them an issue to do so, ipython/ipython#11503 and a fix in ipython/ipython#11506. Jedi supports type hints, so with `c.Completer.use_jedi = True` now or by default in a month, people will profit from them. Furthermore, people are using scanpy in applications and scripts, not just in notebooks. When you use an IDE (or install the jedi extension in EMACS) you should profit from it. 2. The Jupyter shift-tab help being hard to read in the presence of type hints is what I consider a bug. I reported it in ipython/ipython#11504 and fixed it in ipython/ipython#11505. 3. The numpy is on it (see [here](https://github.com/numpy/numpy-stubs)) and will probably integrate it once there needs to be no Python 2 compat. e.g. scikit-learn waits for numpy: scikit-learn/scikit-learn#11170. I see your concern about entry hurdles, but I dont agree. Its super easy. `Union` is or, `Optional` is or `None`. If theres questions, they can be answered. (or people click on the links in the docs and read like one sentence of explanation). 4. If you want we can change how all that is rendered. `Union[a, b]` could be done as ``` :class:`a` or :class:`b` ``` But its really not hard. Honestly I think the `Callable[]` is much better than the textual description that was there before: Until it was there, people (including me when i was writing that annotation) had to dive into the code to figure out what function signature is *really* expected there. Now they have to be able to parse what that `Callable[[a,b], c]` there means. If they have never encountered it before, they can click on it, read one sentence of explanation and know that `a` and `b` are parameters and `c` the return type. Done i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:2455,interoperability,specif,specify,2455," type hints is what I consider a bug. I reported it in ipython/ipython#11504 and fixed it in ipython/ipython#11505. 3. The numpy is on it (see [here](https://github.com/numpy/numpy-stubs)) and will probably integrate it once there needs to be no Python 2 compat. e.g. scikit-learn waits for numpy: scikit-learn/scikit-learn#11170. I see your concern about entry hurdles, but I dont agree. Its super easy. `Union` is or, `Optional` is or `None`. If theres questions, they can be answered. (or people click on the links in the docs and read like one sentence of explanation). 4. If you want we can change how all that is rendered. `Union[a, b]` could be done as ``` :class:`a` or :class:`b` ``` But its really not hard. Honestly I think the `Callable[]` is much better than the textual description that was there before: Until it was there, people (including me when i was writing that annotation) had to dive into the code to figure out what function signature is *really* expected there. Now they have to be able to parse what that `Callable[[a,b], c]` there means. If they have never encountered it before, they can click on it, read one sentence of explanation and know that `a` and `b` are parameters and `c` the return type. Done in 5 seconds. Its a big improvement to no longer have fuzzy explanations. E.g. what does int, optional mean in function x? Can you pass `None` there or does it just mean theres a default value? No idea until you look into the code. `Optional[int]` always means you can pass None or an int here. > The mix of auto-generated types in the docs and the manual annotations also looks inhomogeneous. I like it the fact that we can do that. It allows us to specify things we cant express in type hints (like only this set of strings is accepted) In places where it looks inhomogeneous, we should think about how to make it look more homogeneous. Any ideas? I think that should cover it. Im awaiting your comment about `Union[a, b]`  `a or b` / `a | b`!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:641,modifiability,extens,extension,641,"Hi! I think we have a different focus here, and not all of what you stated as fact is correct, so Ill do my best to clear this up:. 1. There is an advantage for type hints in common Scanpy usage. IPython should use Jedi to create autocompletions since this summer, but they forgot to reenable it. I sent them an issue to do so, ipython/ipython#11503 and a fix in ipython/ipython#11506. Jedi supports type hints, so with `c.Completer.use_jedi = True` now or by default in a month, people will profit from them. Furthermore, people are using scanpy in applications and scripts, not just in notebooks. When you use an IDE (or install the jedi extension in EMACS) you should profit from it. 2. The Jupyter shift-tab help being hard to read in the presence of type hints is what I consider a bug. I reported it in ipython/ipython#11504 and fixed it in ipython/ipython#11505. 3. The numpy is on it (see [here](https://github.com/numpy/numpy-stubs)) and will probably integrate it once there needs to be no Python 2 compat. e.g. scikit-learn waits for numpy: scikit-learn/scikit-learn#11170. I see your concern about entry hurdles, but I dont agree. Its super easy. `Union` is or, `Optional` is or `None`. If theres questions, they can be answered. (or people click on the links in the docs and read like one sentence of explanation). 4. If you want we can change how all that is rendered. `Union[a, b]` could be done as ``` :class:`a` or :class:`b` ``` But its really not hard. Honestly I think the `Callable[]` is much better than the textual description that was there before: Until it was there, people (including me when i was writing that annotation) had to dive into the code to figure out what function signature is *really* expected there. Now they have to be able to parse what that `Callable[[a,b], c]` there means. If they have never encountered it before, they can click on it, read one sentence of explanation and know that `a` and `b` are parameters and `c` the return type. Done i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:962,modifiability,integr,integrate,962,"Hi! I think we have a different focus here, and not all of what you stated as fact is correct, so Ill do my best to clear this up:. 1. There is an advantage for type hints in common Scanpy usage. IPython should use Jedi to create autocompletions since this summer, but they forgot to reenable it. I sent them an issue to do so, ipython/ipython#11503 and a fix in ipython/ipython#11506. Jedi supports type hints, so with `c.Completer.use_jedi = True` now or by default in a month, people will profit from them. Furthermore, people are using scanpy in applications and scripts, not just in notebooks. When you use an IDE (or install the jedi extension in EMACS) you should profit from it. 2. The Jupyter shift-tab help being hard to read in the presence of type hints is what I consider a bug. I reported it in ipython/ipython#11504 and fixed it in ipython/ipython#11505. 3. The numpy is on it (see [here](https://github.com/numpy/numpy-stubs)) and will probably integrate it once there needs to be no Python 2 compat. e.g. scikit-learn waits for numpy: scikit-learn/scikit-learn#11170. I see your concern about entry hurdles, but I dont agree. Its super easy. `Union` is or, `Optional` is or `None`. If theres questions, they can be answered. (or people click on the links in the docs and read like one sentence of explanation). 4. If you want we can change how all that is rendered. `Union[a, b]` could be done as ``` :class:`a` or :class:`b` ``` But its really not hard. Honestly I think the `Callable[]` is much better than the textual description that was there before: Until it was there, people (including me when i was writing that annotation) had to dive into the code to figure out what function signature is *really* expected there. Now they have to be able to parse what that `Callable[[a,b], c]` there means. If they have never encountered it before, they can click on it, read one sentence of explanation and know that `a` and `b` are parameters and `c` the return type. Done i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1097,modifiability,concern,concern,1097,"l do my best to clear this up:. 1. There is an advantage for type hints in common Scanpy usage. IPython should use Jedi to create autocompletions since this summer, but they forgot to reenable it. I sent them an issue to do so, ipython/ipython#11503 and a fix in ipython/ipython#11506. Jedi supports type hints, so with `c.Completer.use_jedi = True` now or by default in a month, people will profit from them. Furthermore, people are using scanpy in applications and scripts, not just in notebooks. When you use an IDE (or install the jedi extension in EMACS) you should profit from it. 2. The Jupyter shift-tab help being hard to read in the presence of type hints is what I consider a bug. I reported it in ipython/ipython#11504 and fixed it in ipython/ipython#11505. 3. The numpy is on it (see [here](https://github.com/numpy/numpy-stubs)) and will probably integrate it once there needs to be no Python 2 compat. e.g. scikit-learn waits for numpy: scikit-learn/scikit-learn#11170. I see your concern about entry hurdles, but I dont agree. Its super easy. `Union` is or, `Optional` is or `None`. If theres questions, they can be answered. (or people click on the links in the docs and read like one sentence of explanation). 4. If you want we can change how all that is rendered. `Union[a, b]` could be done as ``` :class:`a` or :class:`b` ``` But its really not hard. Honestly I think the `Callable[]` is much better than the textual description that was there before: Until it was there, people (including me when i was writing that annotation) had to dive into the code to figure out what function signature is *really* expected there. Now they have to be able to parse what that `Callable[[a,b], c]` there means. If they have never encountered it before, they can click on it, read one sentence of explanation and know that `a` and `b` are parameters and `c` the return type. Done in 5 seconds. Its a big improvement to no longer have fuzzy explanations. E.g. what does int, option",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1958,modifiability,paramet,parameters,1958," type hints is what I consider a bug. I reported it in ipython/ipython#11504 and fixed it in ipython/ipython#11505. 3. The numpy is on it (see [here](https://github.com/numpy/numpy-stubs)) and will probably integrate it once there needs to be no Python 2 compat. e.g. scikit-learn waits for numpy: scikit-learn/scikit-learn#11170. I see your concern about entry hurdles, but I dont agree. Its super easy. `Union` is or, `Optional` is or `None`. If theres questions, they can be answered. (or people click on the links in the docs and read like one sentence of explanation). 4. If you want we can change how all that is rendered. `Union[a, b]` could be done as ``` :class:`a` or :class:`b` ``` But its really not hard. Honestly I think the `Callable[]` is much better than the textual description that was there before: Until it was there, people (including me when i was writing that annotation) had to dive into the code to figure out what function signature is *really* expected there. Now they have to be able to parse what that `Callable[[a,b], c]` there means. If they have never encountered it before, they can click on it, read one sentence of explanation and know that `a` and `b` are parameters and `c` the return type. Done in 5 seconds. Its a big improvement to no longer have fuzzy explanations. E.g. what does int, optional mean in function x? Can you pass `None` there or does it just mean theres a default value? No idea until you look into the code. `Optional[int]` always means you can pass None or an int here. > The mix of auto-generated types in the docs and the manual annotations also looks inhomogeneous. I like it the fact that we can do that. It allows us to specify things we cant express in type hints (like only this set of strings is accepted) In places where it looks inhomogeneous, we should think about how to make it look more homogeneous. Any ideas? I think that should cover it. Im awaiting your comment about `Union[a, b]`  `a or b` / `a | b`!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:962,reliability,integr,integrate,962,"Hi! I think we have a different focus here, and not all of what you stated as fact is correct, so Ill do my best to clear this up:. 1. There is an advantage for type hints in common Scanpy usage. IPython should use Jedi to create autocompletions since this summer, but they forgot to reenable it. I sent them an issue to do so, ipython/ipython#11503 and a fix in ipython/ipython#11506. Jedi supports type hints, so with `c.Completer.use_jedi = True` now or by default in a month, people will profit from them. Furthermore, people are using scanpy in applications and scripts, not just in notebooks. When you use an IDE (or install the jedi extension in EMACS) you should profit from it. 2. The Jupyter shift-tab help being hard to read in the presence of type hints is what I consider a bug. I reported it in ipython/ipython#11504 and fixed it in ipython/ipython#11505. 3. The numpy is on it (see [here](https://github.com/numpy/numpy-stubs)) and will probably integrate it once there needs to be no Python 2 compat. e.g. scikit-learn waits for numpy: scikit-learn/scikit-learn#11170. I see your concern about entry hurdles, but I dont agree. Its super easy. `Union` is or, `Optional` is or `None`. If theres questions, they can be answered. (or people click on the links in the docs and read like one sentence of explanation). 4. If you want we can change how all that is rendered. `Union[a, b]` could be done as ``` :class:`a` or :class:`b` ``` But its really not hard. Honestly I think the `Callable[]` is much better than the textual description that was there before: Until it was there, people (including me when i was writing that annotation) had to dive into the code to figure out what function signature is *really* expected there. Now they have to be able to parse what that `Callable[[a,b], c]` there means. If they have never encountered it before, they can click on it, read one sentence of explanation and know that `a` and `b` are parameters and `c` the return type. Done i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:2084,reliability,doe,does,2084," type hints is what I consider a bug. I reported it in ipython/ipython#11504 and fixed it in ipython/ipython#11505. 3. The numpy is on it (see [here](https://github.com/numpy/numpy-stubs)) and will probably integrate it once there needs to be no Python 2 compat. e.g. scikit-learn waits for numpy: scikit-learn/scikit-learn#11170. I see your concern about entry hurdles, but I dont agree. Its super easy. `Union` is or, `Optional` is or `None`. If theres questions, they can be answered. (or people click on the links in the docs and read like one sentence of explanation). 4. If you want we can change how all that is rendered. `Union[a, b]` could be done as ``` :class:`a` or :class:`b` ``` But its really not hard. Honestly I think the `Callable[]` is much better than the textual description that was there before: Until it was there, people (including me when i was writing that annotation) had to dive into the code to figure out what function signature is *really* expected there. Now they have to be able to parse what that `Callable[[a,b], c]` there means. If they have never encountered it before, they can click on it, read one sentence of explanation and know that `a` and `b` are parameters and `c` the return type. Done in 5 seconds. Its a big improvement to no longer have fuzzy explanations. E.g. what does int, optional mean in function x? Can you pass `None` there or does it just mean theres a default value? No idea until you look into the code. `Optional[int]` always means you can pass None or an int here. > The mix of auto-generated types in the docs and the manual annotations also looks inhomogeneous. I like it the fact that we can do that. It allows us to specify things we cant express in type hints (like only this set of strings is accepted) In places where it looks inhomogeneous, we should think about how to make it look more homogeneous. Any ideas? I think that should cover it. Im awaiting your comment about `Union[a, b]`  `a or b` / `a | b`!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:2154,reliability,doe,does,2154," type hints is what I consider a bug. I reported it in ipython/ipython#11504 and fixed it in ipython/ipython#11505. 3. The numpy is on it (see [here](https://github.com/numpy/numpy-stubs)) and will probably integrate it once there needs to be no Python 2 compat. e.g. scikit-learn waits for numpy: scikit-learn/scikit-learn#11170. I see your concern about entry hurdles, but I dont agree. Its super easy. `Union` is or, `Optional` is or `None`. If theres questions, they can be answered. (or people click on the links in the docs and read like one sentence of explanation). 4. If you want we can change how all that is rendered. `Union[a, b]` could be done as ``` :class:`a` or :class:`b` ``` But its really not hard. Honestly I think the `Callable[]` is much better than the textual description that was there before: Until it was there, people (including me when i was writing that annotation) had to dive into the code to figure out what function signature is *really* expected there. Now they have to be able to parse what that `Callable[[a,b], c]` there means. If they have never encountered it before, they can click on it, read one sentence of explanation and know that `a` and `b` are parameters and `c` the return type. Done in 5 seconds. Its a big improvement to no longer have fuzzy explanations. E.g. what does int, optional mean in function x? Can you pass `None` there or does it just mean theres a default value? No idea until you look into the code. `Optional[int]` always means you can pass None or an int here. > The mix of auto-generated types in the docs and the manual annotations also looks inhomogeneous. I like it the fact that we can do that. It allows us to specify things we cant express in type hints (like only this set of strings is accepted) In places where it looks inhomogeneous, we should think about how to make it look more homogeneous. Any ideas? I think that should cover it. Im awaiting your comment about `Union[a, b]`  `a or b` / `a | b`!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:424,safety,Compl,Completer,424,"Hi! I think we have a different focus here, and not all of what you stated as fact is correct, so Ill do my best to clear this up:. 1. There is an advantage for type hints in common Scanpy usage. IPython should use Jedi to create autocompletions since this summer, but they forgot to reenable it. I sent them an issue to do so, ipython/ipython#11503 and a fix in ipython/ipython#11506. Jedi supports type hints, so with `c.Completer.use_jedi = True` now or by default in a month, people will profit from them. Furthermore, people are using scanpy in applications and scripts, not just in notebooks. When you use an IDE (or install the jedi extension in EMACS) you should profit from it. 2. The Jupyter shift-tab help being hard to read in the presence of type hints is what I consider a bug. I reported it in ipython/ipython#11504 and fixed it in ipython/ipython#11505. 3. The numpy is on it (see [here](https://github.com/numpy/numpy-stubs)) and will probably integrate it once there needs to be no Python 2 compat. e.g. scikit-learn waits for numpy: scikit-learn/scikit-learn#11170. I see your concern about entry hurdles, but I dont agree. Its super easy. `Union` is or, `Optional` is or `None`. If theres questions, they can be answered. (or people click on the links in the docs and read like one sentence of explanation). 4. If you want we can change how all that is rendered. `Union[a, b]` could be done as ``` :class:`a` or :class:`b` ``` But its really not hard. Honestly I think the `Callable[]` is much better than the textual description that was there before: Until it was there, people (including me when i was writing that annotation) had to dive into the code to figure out what function signature is *really* expected there. Now they have to be able to parse what that `Callable[[a,b], c]` there means. If they have never encountered it before, they can click on it, read one sentence of explanation and know that `a` and `b` are parameters and `c` the return type. Done i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:424,security,Compl,Completer,424,"Hi! I think we have a different focus here, and not all of what you stated as fact is correct, so Ill do my best to clear this up:. 1. There is an advantage for type hints in common Scanpy usage. IPython should use Jedi to create autocompletions since this summer, but they forgot to reenable it. I sent them an issue to do so, ipython/ipython#11503 and a fix in ipython/ipython#11506. Jedi supports type hints, so with `c.Completer.use_jedi = True` now or by default in a month, people will profit from them. Furthermore, people are using scanpy in applications and scripts, not just in notebooks. When you use an IDE (or install the jedi extension in EMACS) you should profit from it. 2. The Jupyter shift-tab help being hard to read in the presence of type hints is what I consider a bug. I reported it in ipython/ipython#11504 and fixed it in ipython/ipython#11505. 3. The numpy is on it (see [here](https://github.com/numpy/numpy-stubs)) and will probably integrate it once there needs to be no Python 2 compat. e.g. scikit-learn waits for numpy: scikit-learn/scikit-learn#11170. I see your concern about entry hurdles, but I dont agree. Its super easy. `Union` is or, `Optional` is or `None`. If theres questions, they can be answered. (or people click on the links in the docs and read like one sentence of explanation). 4. If you want we can change how all that is rendered. `Union[a, b]` could be done as ``` :class:`a` or :class:`b` ``` But its really not hard. Honestly I think the `Callable[]` is much better than the textual description that was there before: Until it was there, people (including me when i was writing that annotation) had to dive into the code to figure out what function signature is *really* expected there. Now they have to be able to parse what that `Callable[[a,b], c]` there means. If they have never encountered it before, they can click on it, read one sentence of explanation and know that `a` and `b` are parameters and `c` the return type. Done i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:962,security,integr,integrate,962,"Hi! I think we have a different focus here, and not all of what you stated as fact is correct, so Ill do my best to clear this up:. 1. There is an advantage for type hints in common Scanpy usage. IPython should use Jedi to create autocompletions since this summer, but they forgot to reenable it. I sent them an issue to do so, ipython/ipython#11503 and a fix in ipython/ipython#11506. Jedi supports type hints, so with `c.Completer.use_jedi = True` now or by default in a month, people will profit from them. Furthermore, people are using scanpy in applications and scripts, not just in notebooks. When you use an IDE (or install the jedi extension in EMACS) you should profit from it. 2. The Jupyter shift-tab help being hard to read in the presence of type hints is what I consider a bug. I reported it in ipython/ipython#11504 and fixed it in ipython/ipython#11505. 3. The numpy is on it (see [here](https://github.com/numpy/numpy-stubs)) and will probably integrate it once there needs to be no Python 2 compat. e.g. scikit-learn waits for numpy: scikit-learn/scikit-learn#11170. I see your concern about entry hurdles, but I dont agree. Its super easy. `Union` is or, `Optional` is or `None`. If theres questions, they can be answered. (or people click on the links in the docs and read like one sentence of explanation). 4. If you want we can change how all that is rendered. `Union[a, b]` could be done as ``` :class:`a` or :class:`b` ``` But its really not hard. Honestly I think the `Callable[]` is much better than the textual description that was there before: Until it was there, people (including me when i was writing that annotation) had to dive into the code to figure out what function signature is *really* expected there. Now they have to be able to parse what that `Callable[[a,b], c]` there means. If they have never encountered it before, they can click on it, read one sentence of explanation and know that `a` and `b` are parameters and `c` the return type. Done i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1715,security,sign,signature,1715,"ing hard to read in the presence of type hints is what I consider a bug. I reported it in ipython/ipython#11504 and fixed it in ipython/ipython#11505. 3. The numpy is on it (see [here](https://github.com/numpy/numpy-stubs)) and will probably integrate it once there needs to be no Python 2 compat. e.g. scikit-learn waits for numpy: scikit-learn/scikit-learn#11170. I see your concern about entry hurdles, but I dont agree. Its super easy. `Union` is or, `Optional` is or `None`. If theres questions, they can be answered. (or people click on the links in the docs and read like one sentence of explanation). 4. If you want we can change how all that is rendered. `Union[a, b]` could be done as ``` :class:`a` or :class:`b` ``` But its really not hard. Honestly I think the `Callable[]` is much better than the textual description that was there before: Until it was there, people (including me when i was writing that annotation) had to dive into the code to figure out what function signature is *really* expected there. Now they have to be able to parse what that `Callable[[a,b], c]` there means. If they have never encountered it before, they can click on it, read one sentence of explanation and know that `a` and `b` are parameters and `c` the return type. Done in 5 seconds. Its a big improvement to no longer have fuzzy explanations. E.g. what does int, optional mean in function x? Can you pass `None` there or does it just mean theres a default value? No idea until you look into the code. `Optional[int]` always means you can pass None or an int here. > The mix of auto-generated types in the docs and the manual annotations also looks inhomogeneous. I like it the fact that we can do that. It allows us to specify things we cant express in type hints (like only this set of strings is accepted) In places where it looks inhomogeneous, we should think about how to make it look more homogeneous. Any ideas? I think that should cover it. Im awaiting your comment about ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:936,testability,stub,stubs,936,"Hi! I think we have a different focus here, and not all of what you stated as fact is correct, so Ill do my best to clear this up:. 1. There is an advantage for type hints in common Scanpy usage. IPython should use Jedi to create autocompletions since this summer, but they forgot to reenable it. I sent them an issue to do so, ipython/ipython#11503 and a fix in ipython/ipython#11506. Jedi supports type hints, so with `c.Completer.use_jedi = True` now or by default in a month, people will profit from them. Furthermore, people are using scanpy in applications and scripts, not just in notebooks. When you use an IDE (or install the jedi extension in EMACS) you should profit from it. 2. The Jupyter shift-tab help being hard to read in the presence of type hints is what I consider a bug. I reported it in ipython/ipython#11504 and fixed it in ipython/ipython#11505. 3. The numpy is on it (see [here](https://github.com/numpy/numpy-stubs)) and will probably integrate it once there needs to be no Python 2 compat. e.g. scikit-learn waits for numpy: scikit-learn/scikit-learn#11170. I see your concern about entry hurdles, but I dont agree. Its super easy. `Union` is or, `Optional` is or `None`. If theres questions, they can be answered. (or people click on the links in the docs and read like one sentence of explanation). 4. If you want we can change how all that is rendered. `Union[a, b]` could be done as ``` :class:`a` or :class:`b` ``` But its really not hard. Honestly I think the `Callable[]` is much better than the textual description that was there before: Until it was there, people (including me when i was writing that annotation) had to dive into the code to figure out what function signature is *really* expected there. Now they have to be able to parse what that `Callable[[a,b], c]` there means. If they have never encountered it before, they can click on it, read one sentence of explanation and know that `a` and `b` are parameters and `c` the return type. Done i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:962,testability,integr,integrate,962,"Hi! I think we have a different focus here, and not all of what you stated as fact is correct, so Ill do my best to clear this up:. 1. There is an advantage for type hints in common Scanpy usage. IPython should use Jedi to create autocompletions since this summer, but they forgot to reenable it. I sent them an issue to do so, ipython/ipython#11503 and a fix in ipython/ipython#11506. Jedi supports type hints, so with `c.Completer.use_jedi = True` now or by default in a month, people will profit from them. Furthermore, people are using scanpy in applications and scripts, not just in notebooks. When you use an IDE (or install the jedi extension in EMACS) you should profit from it. 2. The Jupyter shift-tab help being hard to read in the presence of type hints is what I consider a bug. I reported it in ipython/ipython#11504 and fixed it in ipython/ipython#11505. 3. The numpy is on it (see [here](https://github.com/numpy/numpy-stubs)) and will probably integrate it once there needs to be no Python 2 compat. e.g. scikit-learn waits for numpy: scikit-learn/scikit-learn#11170. I see your concern about entry hurdles, but I dont agree. Its super easy. `Union` is or, `Optional` is or `None`. If theres questions, they can be answered. (or people click on the links in the docs and read like one sentence of explanation). 4. If you want we can change how all that is rendered. `Union[a, b]` could be done as ``` :class:`a` or :class:`b` ``` But its really not hard. Honestly I think the `Callable[]` is much better than the textual description that was there before: Until it was there, people (including me when i was writing that annotation) had to dive into the code to figure out what function signature is *really* expected there. Now they have to be able to parse what that `Callable[[a,b], c]` there means. If they have never encountered it before, they can click on it, read one sentence of explanation and know that `a` and `b` are parameters and `c` the return type. Done i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1097,testability,concern,concern,1097,"l do my best to clear this up:. 1. There is an advantage for type hints in common Scanpy usage. IPython should use Jedi to create autocompletions since this summer, but they forgot to reenable it. I sent them an issue to do so, ipython/ipython#11503 and a fix in ipython/ipython#11506. Jedi supports type hints, so with `c.Completer.use_jedi = True` now or by default in a month, people will profit from them. Furthermore, people are using scanpy in applications and scripts, not just in notebooks. When you use an IDE (or install the jedi extension in EMACS) you should profit from it. 2. The Jupyter shift-tab help being hard to read in the presence of type hints is what I consider a bug. I reported it in ipython/ipython#11504 and fixed it in ipython/ipython#11505. 3. The numpy is on it (see [here](https://github.com/numpy/numpy-stubs)) and will probably integrate it once there needs to be no Python 2 compat. e.g. scikit-learn waits for numpy: scikit-learn/scikit-learn#11170. I see your concern about entry hurdles, but I dont agree. Its super easy. `Union` is or, `Optional` is or `None`. If theres questions, they can be answered. (or people click on the links in the docs and read like one sentence of explanation). 4. If you want we can change how all that is rendered. `Union[a, b]` could be done as ``` :class:`a` or :class:`b` ``` But its really not hard. Honestly I think the `Callable[]` is much better than the textual description that was there before: Until it was there, people (including me when i was writing that annotation) had to dive into the code to figure out what function signature is *really* expected there. Now they have to be able to parse what that `Callable[[a,b], c]` there means. If they have never encountered it before, they can click on it, read one sentence of explanation and know that `a` and `b` are parameters and `c` the return type. Done in 5 seconds. Its a big improvement to no longer have fuzzy explanations. E.g. what does int, option",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:2054,testability,fuzzy,fuzzy,2054," type hints is what I consider a bug. I reported it in ipython/ipython#11504 and fixed it in ipython/ipython#11505. 3. The numpy is on it (see [here](https://github.com/numpy/numpy-stubs)) and will probably integrate it once there needs to be no Python 2 compat. e.g. scikit-learn waits for numpy: scikit-learn/scikit-learn#11170. I see your concern about entry hurdles, but I dont agree. Its super easy. `Union` is or, `Optional` is or `None`. If theres questions, they can be answered. (or people click on the links in the docs and read like one sentence of explanation). 4. If you want we can change how all that is rendered. `Union[a, b]` could be done as ``` :class:`a` or :class:`b` ``` But its really not hard. Honestly I think the `Callable[]` is much better than the textual description that was there before: Until it was there, people (including me when i was writing that annotation) had to dive into the code to figure out what function signature is *really* expected there. Now they have to be able to parse what that `Callable[[a,b], c]` there means. If they have never encountered it before, they can click on it, read one sentence of explanation and know that `a` and `b` are parameters and `c` the return type. Done in 5 seconds. Its a big improvement to no longer have fuzzy explanations. E.g. what does int, optional mean in function x? Can you pass `None` there or does it just mean theres a default value? No idea until you look into the code. `Optional[int]` always means you can pass None or an int here. > The mix of auto-generated types in the docs and the manual annotations also looks inhomogeneous. I like it the fact that we can do that. It allows us to specify things we cant express in type hints (like only this set of strings is accepted) In places where it looks inhomogeneous, we should think about how to make it look more homogeneous. Any ideas? I think that should cover it. Im awaiting your comment about `Union[a, b]`  `a or b` / `a | b`!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:117,usability,clear,clear,117,"Hi! I think we have a different focus here, and not all of what you stated as fact is correct, so Ill do my best to clear this up:. 1. There is an advantage for type hints in common Scanpy usage. IPython should use Jedi to create autocompletions since this summer, but they forgot to reenable it. I sent them an issue to do so, ipython/ipython#11503 and a fix in ipython/ipython#11506. Jedi supports type hints, so with `c.Completer.use_jedi = True` now or by default in a month, people will profit from them. Furthermore, people are using scanpy in applications and scripts, not just in notebooks. When you use an IDE (or install the jedi extension in EMACS) you should profit from it. 2. The Jupyter shift-tab help being hard to read in the presence of type hints is what I consider a bug. I reported it in ipython/ipython#11504 and fixed it in ipython/ipython#11505. 3. The numpy is on it (see [here](https://github.com/numpy/numpy-stubs)) and will probably integrate it once there needs to be no Python 2 compat. e.g. scikit-learn waits for numpy: scikit-learn/scikit-learn#11170. I see your concern about entry hurdles, but I dont agree. Its super easy. `Union` is or, `Optional` is or `None`. If theres questions, they can be answered. (or people click on the links in the docs and read like one sentence of explanation). 4. If you want we can change how all that is rendered. `Union[a, b]` could be done as ``` :class:`a` or :class:`b` ``` But its really not hard. Honestly I think the `Callable[]` is much better than the textual description that was there before: Until it was there, people (including me when i was writing that annotation) had to dive into the code to figure out what function signature is *really* expected there. Now they have to be able to parse what that `Callable[[a,b], c]` there means. If they have never encountered it before, they can click on it, read one sentence of explanation and know that `a` and `b` are parameters and `c` the return type. Done i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:167,usability,hint,hints,167,"Hi! I think we have a different focus here, and not all of what you stated as fact is correct, so Ill do my best to clear this up:. 1. There is an advantage for type hints in common Scanpy usage. IPython should use Jedi to create autocompletions since this summer, but they forgot to reenable it. I sent them an issue to do so, ipython/ipython#11503 and a fix in ipython/ipython#11506. Jedi supports type hints, so with `c.Completer.use_jedi = True` now or by default in a month, people will profit from them. Furthermore, people are using scanpy in applications and scripts, not just in notebooks. When you use an IDE (or install the jedi extension in EMACS) you should profit from it. 2. The Jupyter shift-tab help being hard to read in the presence of type hints is what I consider a bug. I reported it in ipython/ipython#11504 and fixed it in ipython/ipython#11505. 3. The numpy is on it (see [here](https://github.com/numpy/numpy-stubs)) and will probably integrate it once there needs to be no Python 2 compat. e.g. scikit-learn waits for numpy: scikit-learn/scikit-learn#11170. I see your concern about entry hurdles, but I dont agree. Its super easy. `Union` is or, `Optional` is or `None`. If theres questions, they can be answered. (or people click on the links in the docs and read like one sentence of explanation). 4. If you want we can change how all that is rendered. `Union[a, b]` could be done as ``` :class:`a` or :class:`b` ``` But its really not hard. Honestly I think the `Callable[]` is much better than the textual description that was there before: Until it was there, people (including me when i was writing that annotation) had to dive into the code to figure out what function signature is *really* expected there. Now they have to be able to parse what that `Callable[[a,b], c]` there means. If they have never encountered it before, they can click on it, read one sentence of explanation and know that `a` and `b` are parameters and `c` the return type. Done i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:392,usability,support,supports,392,"Hi! I think we have a different focus here, and not all of what you stated as fact is correct, so Ill do my best to clear this up:. 1. There is an advantage for type hints in common Scanpy usage. IPython should use Jedi to create autocompletions since this summer, but they forgot to reenable it. I sent them an issue to do so, ipython/ipython#11503 and a fix in ipython/ipython#11506. Jedi supports type hints, so with `c.Completer.use_jedi = True` now or by default in a month, people will profit from them. Furthermore, people are using scanpy in applications and scripts, not just in notebooks. When you use an IDE (or install the jedi extension in EMACS) you should profit from it. 2. The Jupyter shift-tab help being hard to read in the presence of type hints is what I consider a bug. I reported it in ipython/ipython#11504 and fixed it in ipython/ipython#11505. 3. The numpy is on it (see [here](https://github.com/numpy/numpy-stubs)) and will probably integrate it once there needs to be no Python 2 compat. e.g. scikit-learn waits for numpy: scikit-learn/scikit-learn#11170. I see your concern about entry hurdles, but I dont agree. Its super easy. `Union` is or, `Optional` is or `None`. If theres questions, they can be answered. (or people click on the links in the docs and read like one sentence of explanation). 4. If you want we can change how all that is rendered. `Union[a, b]` could be done as ``` :class:`a` or :class:`b` ``` But its really not hard. Honestly I think the `Callable[]` is much better than the textual description that was there before: Until it was there, people (including me when i was writing that annotation) had to dive into the code to figure out what function signature is *really* expected there. Now they have to be able to parse what that `Callable[[a,b], c]` there means. If they have never encountered it before, they can click on it, read one sentence of explanation and know that `a` and `b` are parameters and `c` the return type. Done i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:406,usability,hint,hints,406,"Hi! I think we have a different focus here, and not all of what you stated as fact is correct, so Ill do my best to clear this up:. 1. There is an advantage for type hints in common Scanpy usage. IPython should use Jedi to create autocompletions since this summer, but they forgot to reenable it. I sent them an issue to do so, ipython/ipython#11503 and a fix in ipython/ipython#11506. Jedi supports type hints, so with `c.Completer.use_jedi = True` now or by default in a month, people will profit from them. Furthermore, people are using scanpy in applications and scripts, not just in notebooks. When you use an IDE (or install the jedi extension in EMACS) you should profit from it. 2. The Jupyter shift-tab help being hard to read in the presence of type hints is what I consider a bug. I reported it in ipython/ipython#11504 and fixed it in ipython/ipython#11505. 3. The numpy is on it (see [here](https://github.com/numpy/numpy-stubs)) and will probably integrate it once there needs to be no Python 2 compat. e.g. scikit-learn waits for numpy: scikit-learn/scikit-learn#11170. I see your concern about entry hurdles, but I dont agree. Its super easy. `Union` is or, `Optional` is or `None`. If theres questions, they can be answered. (or people click on the links in the docs and read like one sentence of explanation). 4. If you want we can change how all that is rendered. `Union[a, b]` could be done as ``` :class:`a` or :class:`b` ``` But its really not hard. Honestly I think the `Callable[]` is much better than the textual description that was there before: Until it was there, people (including me when i was writing that annotation) had to dive into the code to figure out what function signature is *really* expected there. Now they have to be able to parse what that `Callable[[a,b], c]` there means. If they have never encountered it before, they can click on it, read one sentence of explanation and know that `a` and `b` are parameters and `c` the return type. Done i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:713,usability,help,help,713,"Hi! I think we have a different focus here, and not all of what you stated as fact is correct, so Ill do my best to clear this up:. 1. There is an advantage for type hints in common Scanpy usage. IPython should use Jedi to create autocompletions since this summer, but they forgot to reenable it. I sent them an issue to do so, ipython/ipython#11503 and a fix in ipython/ipython#11506. Jedi supports type hints, so with `c.Completer.use_jedi = True` now or by default in a month, people will profit from them. Furthermore, people are using scanpy in applications and scripts, not just in notebooks. When you use an IDE (or install the jedi extension in EMACS) you should profit from it. 2. The Jupyter shift-tab help being hard to read in the presence of type hints is what I consider a bug. I reported it in ipython/ipython#11504 and fixed it in ipython/ipython#11505. 3. The numpy is on it (see [here](https://github.com/numpy/numpy-stubs)) and will probably integrate it once there needs to be no Python 2 compat. e.g. scikit-learn waits for numpy: scikit-learn/scikit-learn#11170. I see your concern about entry hurdles, but I dont agree. Its super easy. `Union` is or, `Optional` is or `None`. If theres questions, they can be answered. (or people click on the links in the docs and read like one sentence of explanation). 4. If you want we can change how all that is rendered. `Union[a, b]` could be done as ``` :class:`a` or :class:`b` ``` But its really not hard. Honestly I think the `Callable[]` is much better than the textual description that was there before: Until it was there, people (including me when i was writing that annotation) had to dive into the code to figure out what function signature is *really* expected there. Now they have to be able to parse what that `Callable[[a,b], c]` there means. If they have never encountered it before, they can click on it, read one sentence of explanation and know that `a` and `b` are parameters and `c` the return type. Done i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:761,usability,hint,hints,761,"Hi! I think we have a different focus here, and not all of what you stated as fact is correct, so Ill do my best to clear this up:. 1. There is an advantage for type hints in common Scanpy usage. IPython should use Jedi to create autocompletions since this summer, but they forgot to reenable it. I sent them an issue to do so, ipython/ipython#11503 and a fix in ipython/ipython#11506. Jedi supports type hints, so with `c.Completer.use_jedi = True` now or by default in a month, people will profit from them. Furthermore, people are using scanpy in applications and scripts, not just in notebooks. When you use an IDE (or install the jedi extension in EMACS) you should profit from it. 2. The Jupyter shift-tab help being hard to read in the presence of type hints is what I consider a bug. I reported it in ipython/ipython#11504 and fixed it in ipython/ipython#11505. 3. The numpy is on it (see [here](https://github.com/numpy/numpy-stubs)) and will probably integrate it once there needs to be no Python 2 compat. e.g. scikit-learn waits for numpy: scikit-learn/scikit-learn#11170. I see your concern about entry hurdles, but I dont agree. Its super easy. `Union` is or, `Optional` is or `None`. If theres questions, they can be answered. (or people click on the links in the docs and read like one sentence of explanation). 4. If you want we can change how all that is rendered. `Union[a, b]` could be done as ``` :class:`a` or :class:`b` ``` But its really not hard. Honestly I think the `Callable[]` is much better than the textual description that was there before: Until it was there, people (including me when i was writing that annotation) had to dive into the code to figure out what function signature is *really* expected there. Now they have to be able to parse what that `Callable[[a,b], c]` there means. If they have never encountered it before, they can click on it, read one sentence of explanation and know that `a` and `b` are parameters and `c` the return type. Done i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1030,usability,learn,learn,1030,"ocus here, and not all of what you stated as fact is correct, so Ill do my best to clear this up:. 1. There is an advantage for type hints in common Scanpy usage. IPython should use Jedi to create autocompletions since this summer, but they forgot to reenable it. I sent them an issue to do so, ipython/ipython#11503 and a fix in ipython/ipython#11506. Jedi supports type hints, so with `c.Completer.use_jedi = True` now or by default in a month, people will profit from them. Furthermore, people are using scanpy in applications and scripts, not just in notebooks. When you use an IDE (or install the jedi extension in EMACS) you should profit from it. 2. The Jupyter shift-tab help being hard to read in the presence of type hints is what I consider a bug. I reported it in ipython/ipython#11504 and fixed it in ipython/ipython#11505. 3. The numpy is on it (see [here](https://github.com/numpy/numpy-stubs)) and will probably integrate it once there needs to be no Python 2 compat. e.g. scikit-learn waits for numpy: scikit-learn/scikit-learn#11170. I see your concern about entry hurdles, but I dont agree. Its super easy. `Union` is or, `Optional` is or `None`. If theres questions, they can be answered. (or people click on the links in the docs and read like one sentence of explanation). 4. If you want we can change how all that is rendered. `Union[a, b]` could be done as ``` :class:`a` or :class:`b` ``` But its really not hard. Honestly I think the `Callable[]` is much better than the textual description that was there before: Until it was there, people (including me when i was writing that annotation) had to dive into the code to figure out what function signature is *really* expected there. Now they have to be able to parse what that `Callable[[a,b], c]` there means. If they have never encountered it before, they can click on it, read one sentence of explanation and know that `a` and `b` are parameters and `c` the return type. Done in 5 seconds. Its a big improveme",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1060,usability,learn,learn,1060," you stated as fact is correct, so Ill do my best to clear this up:. 1. There is an advantage for type hints in common Scanpy usage. IPython should use Jedi to create autocompletions since this summer, but they forgot to reenable it. I sent them an issue to do so, ipython/ipython#11503 and a fix in ipython/ipython#11506. Jedi supports type hints, so with `c.Completer.use_jedi = True` now or by default in a month, people will profit from them. Furthermore, people are using scanpy in applications and scripts, not just in notebooks. When you use an IDE (or install the jedi extension in EMACS) you should profit from it. 2. The Jupyter shift-tab help being hard to read in the presence of type hints is what I consider a bug. I reported it in ipython/ipython#11504 and fixed it in ipython/ipython#11505. 3. The numpy is on it (see [here](https://github.com/numpy/numpy-stubs)) and will probably integrate it once there needs to be no Python 2 compat. e.g. scikit-learn waits for numpy: scikit-learn/scikit-learn#11170. I see your concern about entry hurdles, but I dont agree. Its super easy. `Union` is or, `Optional` is or `None`. If theres questions, they can be answered. (or people click on the links in the docs and read like one sentence of explanation). 4. If you want we can change how all that is rendered. `Union[a, b]` could be done as ``` :class:`a` or :class:`b` ``` But its really not hard. Honestly I think the `Callable[]` is much better than the textual description that was there before: Until it was there, people (including me when i was writing that annotation) had to dive into the code to figure out what function signature is *really* expected there. Now they have to be able to parse what that `Callable[[a,b], c]` there means. If they have never encountered it before, they can click on it, read one sentence of explanation and know that `a` and `b` are parameters and `c` the return type. Done in 5 seconds. Its a big improvement to no longer have fuzzy exp",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1073,usability,learn,learn,1073,"s fact is correct, so Ill do my best to clear this up:. 1. There is an advantage for type hints in common Scanpy usage. IPython should use Jedi to create autocompletions since this summer, but they forgot to reenable it. I sent them an issue to do so, ipython/ipython#11503 and a fix in ipython/ipython#11506. Jedi supports type hints, so with `c.Completer.use_jedi = True` now or by default in a month, people will profit from them. Furthermore, people are using scanpy in applications and scripts, not just in notebooks. When you use an IDE (or install the jedi extension in EMACS) you should profit from it. 2. The Jupyter shift-tab help being hard to read in the presence of type hints is what I consider a bug. I reported it in ipython/ipython#11504 and fixed it in ipython/ipython#11505. 3. The numpy is on it (see [here](https://github.com/numpy/numpy-stubs)) and will probably integrate it once there needs to be no Python 2 compat. e.g. scikit-learn waits for numpy: scikit-learn/scikit-learn#11170. I see your concern about entry hurdles, but I dont agree. Its super easy. `Union` is or, `Optional` is or `None`. If theres questions, they can be answered. (or people click on the links in the docs and read like one sentence of explanation). 4. If you want we can change how all that is rendered. `Union[a, b]` could be done as ``` :class:`a` or :class:`b` ``` But its really not hard. Honestly I think the `Callable[]` is much better than the textual description that was there before: Until it was there, people (including me when i was writing that annotation) had to dive into the code to figure out what function signature is *really* expected there. Now they have to be able to parse what that `Callable[[a,b], c]` there means. If they have never encountered it before, they can click on it, read one sentence of explanation and know that `a` and `b` are parameters and `c` the return type. Done in 5 seconds. Its a big improvement to no longer have fuzzy explanations. E.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:2495,usability,hint,hints,2495," type hints is what I consider a bug. I reported it in ipython/ipython#11504 and fixed it in ipython/ipython#11505. 3. The numpy is on it (see [here](https://github.com/numpy/numpy-stubs)) and will probably integrate it once there needs to be no Python 2 compat. e.g. scikit-learn waits for numpy: scikit-learn/scikit-learn#11170. I see your concern about entry hurdles, but I dont agree. Its super easy. `Union` is or, `Optional` is or `None`. If theres questions, they can be answered. (or people click on the links in the docs and read like one sentence of explanation). 4. If you want we can change how all that is rendered. `Union[a, b]` could be done as ``` :class:`a` or :class:`b` ``` But its really not hard. Honestly I think the `Callable[]` is much better than the textual description that was there before: Until it was there, people (including me when i was writing that annotation) had to dive into the code to figure out what function signature is *really* expected there. Now they have to be able to parse what that `Callable[[a,b], c]` there means. If they have never encountered it before, they can click on it, read one sentence of explanation and know that `a` and `b` are parameters and `c` the return type. Done in 5 seconds. Its a big improvement to no longer have fuzzy explanations. E.g. what does int, optional mean in function x? Can you pass `None` there or does it just mean theres a default value? No idea until you look into the code. `Optional[int]` always means you can pass None or an int here. > The mix of auto-generated types in the docs and the manual annotations also looks inhomogeneous. I like it the fact that we can do that. It allows us to specify things we cant express in type hints (like only this set of strings is accepted) In places where it looks inhomogeneous, we should think about how to make it look more homogeneous. Any ideas? I think that should cover it. Im awaiting your comment about `Union[a, b]`  `a or b` / `a | b`!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1053,deployability,stack,stack,1053,"Hey, this has been something that's been confusing me a bit when annotating my arguments. Since python is pretty polymorphic (until its not), I find it hard to capture the traits an object should have using types I'm familiar with. Some examples:. * If you need to provide a list of genes, this could be a finite (ordered?) iterable whose elements are coercible to the same type as `obs_names`. . * An integer. Could be a numpy integer, could be a python integer. What's are the correct typings for these? Do I do a Union of everything I can think of that matches this? Is there a way to say: ""should behave right if I call `np.array` on it"" (limiting possible arguments types to pd.Series, list, tuple, np.array, dask array, and probably some others)? I guess I'd like to so some information on best practices and common idioms in the contribution guide. I haven't seen too many scientific python packages use type annotations, so I'm not sure how set conventions are. If anyone has seen some good writing on type annotations for the scientific python stack, I'd love to take a look.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:113,modifiability,polymorph,polymorphic,113,"Hey, this has been something that's been confusing me a bit when annotating my arguments. Since python is pretty polymorphic (until its not), I find it hard to capture the traits an object should have using types I'm familiar with. Some examples:. * If you need to provide a list of genes, this could be a finite (ordered?) iterable whose elements are coercible to the same type as `obs_names`. . * An integer. Could be a numpy integer, could be a python integer. What's are the correct typings for these? Do I do a Union of everything I can think of that matches this? Is there a way to say: ""should behave right if I call `np.array` on it"" (limiting possible arguments types to pd.Series, list, tuple, np.array, dask array, and probably some others)? I guess I'd like to so some information on best practices and common idioms in the contribution guide. I haven't seen too many scientific python packages use type annotations, so I'm not sure how set conventions are. If anyone has seen some good writing on type annotations for the scientific python stack, I'd love to take a look.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:898,modifiability,pac,packages,898,"Hey, this has been something that's been confusing me a bit when annotating my arguments. Since python is pretty polymorphic (until its not), I find it hard to capture the traits an object should have using types I'm familiar with. Some examples:. * If you need to provide a list of genes, this could be a finite (ordered?) iterable whose elements are coercible to the same type as `obs_names`. . * An integer. Could be a numpy integer, could be a python integer. What's are the correct typings for these? Do I do a Union of everything I can think of that matches this? Is there a way to say: ""should behave right if I call `np.array` on it"" (limiting possible arguments types to pd.Series, list, tuple, np.array, dask array, and probably some others)? I guess I'd like to so some information on best practices and common idioms in the contribution guide. I haven't seen too many scientific python packages use type annotations, so I'm not sure how set conventions are. If anyone has seen some good writing on type annotations for the scientific python stack, I'd love to take a look.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:801,reliability,pra,practices,801,"Hey, this has been something that's been confusing me a bit when annotating my arguments. Since python is pretty polymorphic (until its not), I find it hard to capture the traits an object should have using types I'm familiar with. Some examples:. * If you need to provide a list of genes, this could be a finite (ordered?) iterable whose elements are coercible to the same type as `obs_names`. . * An integer. Could be a numpy integer, could be a python integer. What's are the correct typings for these? Do I do a Union of everything I can think of that matches this? Is there a way to say: ""should behave right if I call `np.array` on it"" (limiting possible arguments types to pd.Series, list, tuple, np.array, dask array, and probably some others)? I guess I'd like to so some information on best practices and common idioms in the contribution guide. I haven't seen too many scientific python packages use type annotations, so I'm not sure how set conventions are. If anyone has seen some good writing on type annotations for the scientific python stack, I'd love to take a look.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:849,usability,guid,guide,849,"Hey, this has been something that's been confusing me a bit when annotating my arguments. Since python is pretty polymorphic (until its not), I find it hard to capture the traits an object should have using types I'm familiar with. Some examples:. * If you need to provide a list of genes, this could be a finite (ordered?) iterable whose elements are coercible to the same type as `obs_names`. . * An integer. Could be a numpy integer, could be a python integer. What's are the correct typings for these? Do I do a Union of everything I can think of that matches this? Is there a way to say: ""should behave right if I call `np.array` on it"" (limiting possible arguments types to pd.Series, list, tuple, np.array, dask array, and probably some others)? I guess I'd like to so some information on best practices and common idioms in the contribution guide. I haven't seen too many scientific python packages use type annotations, so I'm not sure how set conventions are. If anyone has seen some good writing on type annotations for the scientific python stack, I'd love to take a look.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:514,deployability,depend,depending,514,"Hi! Theres a series of abstract base classes (other languages call them interfaces) that can be used if you know specifically what you want (e.g. `typing.Sequence[T]` if it only needs to be iterable and indexable, but not necessarily a `list`). About your examples, generally I always have to dig into the code to figure such things out. Annoying, but it means that people after me can just use the type annotations instead of wasting their time doing the same. - Currently we only accept `str` for obs_names, so depending on the actual usage in the function, `Iterable[str]` or `Sequence[str]` would be it I guess. - Usually single integers being passed around should be python `int`s, because they can be any size. But if you need to accept `int32` and `int64`, you can do `Union[int, np.integer]`. > Is there a way to say: ""should behave right if I call np.array on it"". In truth, `array` eats just about anything (with weird results), and for us this is probably a good idea:. ```py. Number = Union[float, int, np.integer, np.floating]. Num1DArrayLike = Sequence[Number]. Num2DArrayLike = Sequence[Num1DArrayLike]. Num3DArrayLike = Sequence[Num2DArrayLike]. NumNDArrayLike = Union[Num1DArrayLike, Num2DArrayLike, Num3DArrayLike]. ```. But if we want to be exact about `array_like`s, wed need this ABC:. ```py. class ArrayLike(ABC):. """"""An array,. any object exposing the array interface,. an object whose __array__ method returns an array,. or any (nested) sequence. """""". @classmethod. def __subclasshook__(cls, C):. if issubclass(C, np.ndarray):. return True. if any('__array_interface__' in B.__dict__ for B in C.__mro__):. return True. if any('__array__' in B.__dict__ for B in C.__mro__):. return True. return Sequence.__subclasshook__(cls, C). ```. ----. Two thoughts here:. 1. Its fine if you dont know exactly. Just use your best guess. The worst case is that someone wastes a second of runtime converting their argument to the type you thought was needed, while the original type woul",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:465,energy efficiency,Current,Currently,465,"Hi! Theres a series of abstract base classes (other languages call them interfaces) that can be used if you know specifically what you want (e.g. `typing.Sequence[T]` if it only needs to be iterable and indexable, but not necessarily a `list`). About your examples, generally I always have to dig into the code to figure such things out. Annoying, but it means that people after me can just use the type annotations instead of wasting their time doing the same. - Currently we only accept `str` for obs_names, so depending on the actual usage in the function, `Iterable[str]` or `Sequence[str]` would be it I guess. - Usually single integers being passed around should be python `int`s, because they can be any size. But if you need to accept `int32` and `int64`, you can do `Union[int, np.integer]`. > Is there a way to say: ""should behave right if I call np.array on it"". In truth, `array` eats just about anything (with weird results), and for us this is probably a good idea:. ```py. Number = Union[float, int, np.integer, np.floating]. Num1DArrayLike = Sequence[Number]. Num2DArrayLike = Sequence[Num1DArrayLike]. Num3DArrayLike = Sequence[Num2DArrayLike]. NumNDArrayLike = Union[Num1DArrayLike, Num2DArrayLike, Num3DArrayLike]. ```. But if we want to be exact about `array_like`s, wed need this ABC:. ```py. class ArrayLike(ABC):. """"""An array,. any object exposing the array interface,. an object whose __array__ method returns an array,. or any (nested) sequence. """""". @classmethod. def __subclasshook__(cls, C):. if issubclass(C, np.ndarray):. return True. if any('__array_interface__' in B.__dict__ for B in C.__mro__):. return True. if any('__array__' in B.__dict__ for B in C.__mro__):. return True. return Sequence.__subclasshook__(cls, C). ```. ----. Two thoughts here:. 1. Its fine if you dont know exactly. Just use your best guess. The worst case is that someone wastes a second of runtime converting their argument to the type you thought was needed, while the original type woul",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:24,integrability,abstract,abstract,24,"Hi! Theres a series of abstract base classes (other languages call them interfaces) that can be used if you know specifically what you want (e.g. `typing.Sequence[T]` if it only needs to be iterable and indexable, but not necessarily a `list`). About your examples, generally I always have to dig into the code to figure such things out. Annoying, but it means that people after me can just use the type annotations instead of wasting their time doing the same. - Currently we only accept `str` for obs_names, so depending on the actual usage in the function, `Iterable[str]` or `Sequence[str]` would be it I guess. - Usually single integers being passed around should be python `int`s, because they can be any size. But if you need to accept `int32` and `int64`, you can do `Union[int, np.integer]`. > Is there a way to say: ""should behave right if I call np.array on it"". In truth, `array` eats just about anything (with weird results), and for us this is probably a good idea:. ```py. Number = Union[float, int, np.integer, np.floating]. Num1DArrayLike = Sequence[Number]. Num2DArrayLike = Sequence[Num1DArrayLike]. Num3DArrayLike = Sequence[Num2DArrayLike]. NumNDArrayLike = Union[Num1DArrayLike, Num2DArrayLike, Num3DArrayLike]. ```. But if we want to be exact about `array_like`s, wed need this ABC:. ```py. class ArrayLike(ABC):. """"""An array,. any object exposing the array interface,. an object whose __array__ method returns an array,. or any (nested) sequence. """""". @classmethod. def __subclasshook__(cls, C):. if issubclass(C, np.ndarray):. return True. if any('__array_interface__' in B.__dict__ for B in C.__mro__):. return True. if any('__array__' in B.__dict__ for B in C.__mro__):. return True. return Sequence.__subclasshook__(cls, C). ```. ----. Two thoughts here:. 1. Its fine if you dont know exactly. Just use your best guess. The worst case is that someone wastes a second of runtime converting their argument to the type you thought was needed, while the original type woul",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:73,integrability,interfac,interfaces,73,"Hi! Theres a series of abstract base classes (other languages call them interfaces) that can be used if you know specifically what you want (e.g. `typing.Sequence[T]` if it only needs to be iterable and indexable, but not necessarily a `list`). About your examples, generally I always have to dig into the code to figure such things out. Annoying, but it means that people after me can just use the type annotations instead of wasting their time doing the same. - Currently we only accept `str` for obs_names, so depending on the actual usage in the function, `Iterable[str]` or `Sequence[str]` would be it I guess. - Usually single integers being passed around should be python `int`s, because they can be any size. But if you need to accept `int32` and `int64`, you can do `Union[int, np.integer]`. > Is there a way to say: ""should behave right if I call np.array on it"". In truth, `array` eats just about anything (with weird results), and for us this is probably a good idea:. ```py. Number = Union[float, int, np.integer, np.floating]. Num1DArrayLike = Sequence[Number]. Num2DArrayLike = Sequence[Num1DArrayLike]. Num3DArrayLike = Sequence[Num2DArrayLike]. NumNDArrayLike = Union[Num1DArrayLike, Num2DArrayLike, Num3DArrayLike]. ```. But if we want to be exact about `array_like`s, wed need this ABC:. ```py. class ArrayLike(ABC):. """"""An array,. any object exposing the array interface,. an object whose __array__ method returns an array,. or any (nested) sequence. """""". @classmethod. def __subclasshook__(cls, C):. if issubclass(C, np.ndarray):. return True. if any('__array_interface__' in B.__dict__ for B in C.__mro__):. return True. if any('__array__' in B.__dict__ for B in C.__mro__):. return True. return Sequence.__subclasshook__(cls, C). ```. ----. Two thoughts here:. 1. Its fine if you dont know exactly. Just use your best guess. The worst case is that someone wastes a second of runtime converting their argument to the type you thought was needed, while the original type woul",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:514,integrability,depend,depending,514,"Hi! Theres a series of abstract base classes (other languages call them interfaces) that can be used if you know specifically what you want (e.g. `typing.Sequence[T]` if it only needs to be iterable and indexable, but not necessarily a `list`). About your examples, generally I always have to dig into the code to figure such things out. Annoying, but it means that people after me can just use the type annotations instead of wasting their time doing the same. - Currently we only accept `str` for obs_names, so depending on the actual usage in the function, `Iterable[str]` or `Sequence[str]` would be it I guess. - Usually single integers being passed around should be python `int`s, because they can be any size. But if you need to accept `int32` and `int64`, you can do `Union[int, np.integer]`. > Is there a way to say: ""should behave right if I call np.array on it"". In truth, `array` eats just about anything (with weird results), and for us this is probably a good idea:. ```py. Number = Union[float, int, np.integer, np.floating]. Num1DArrayLike = Sequence[Number]. Num2DArrayLike = Sequence[Num1DArrayLike]. Num3DArrayLike = Sequence[Num2DArrayLike]. NumNDArrayLike = Union[Num1DArrayLike, Num2DArrayLike, Num3DArrayLike]. ```. But if we want to be exact about `array_like`s, wed need this ABC:. ```py. class ArrayLike(ABC):. """"""An array,. any object exposing the array interface,. an object whose __array__ method returns an array,. or any (nested) sequence. """""". @classmethod. def __subclasshook__(cls, C):. if issubclass(C, np.ndarray):. return True. if any('__array_interface__' in B.__dict__ for B in C.__mro__):. return True. if any('__array__' in B.__dict__ for B in C.__mro__):. return True. return Sequence.__subclasshook__(cls, C). ```. ----. Two thoughts here:. 1. Its fine if you dont know exactly. Just use your best guess. The worst case is that someone wastes a second of runtime converting their argument to the type you thought was needed, while the original type woul",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1383,integrability,interfac,interface,1383," specifically what you want (e.g. `typing.Sequence[T]` if it only needs to be iterable and indexable, but not necessarily a `list`). About your examples, generally I always have to dig into the code to figure such things out. Annoying, but it means that people after me can just use the type annotations instead of wasting their time doing the same. - Currently we only accept `str` for obs_names, so depending on the actual usage in the function, `Iterable[str]` or `Sequence[str]` would be it I guess. - Usually single integers being passed around should be python `int`s, because they can be any size. But if you need to accept `int32` and `int64`, you can do `Union[int, np.integer]`. > Is there a way to say: ""should behave right if I call np.array on it"". In truth, `array` eats just about anything (with weird results), and for us this is probably a good idea:. ```py. Number = Union[float, int, np.integer, np.floating]. Num1DArrayLike = Sequence[Number]. Num2DArrayLike = Sequence[Num1DArrayLike]. Num3DArrayLike = Sequence[Num2DArrayLike]. NumNDArrayLike = Union[Num1DArrayLike, Num2DArrayLike, Num3DArrayLike]. ```. But if we want to be exact about `array_like`s, wed need this ABC:. ```py. class ArrayLike(ABC):. """"""An array,. any object exposing the array interface,. an object whose __array__ method returns an array,. or any (nested) sequence. """""". @classmethod. def __subclasshook__(cls, C):. if issubclass(C, np.ndarray):. return True. if any('__array_interface__' in B.__dict__ for B in C.__mro__):. return True. if any('__array__' in B.__dict__ for B in C.__mro__):. return True. return Sequence.__subclasshook__(cls, C). ```. ----. Two thoughts here:. 1. Its fine if you dont know exactly. Just use your best guess. The worst case is that someone wastes a second of runtime converting their argument to the type you thought was needed, while the original type would have been OK. 2. Its good if someone thinks about all that because that means things dont break unexpectedly!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:73,interoperability,interfac,interfaces,73,"Hi! Theres a series of abstract base classes (other languages call them interfaces) that can be used if you know specifically what you want (e.g. `typing.Sequence[T]` if it only needs to be iterable and indexable, but not necessarily a `list`). About your examples, generally I always have to dig into the code to figure such things out. Annoying, but it means that people after me can just use the type annotations instead of wasting their time doing the same. - Currently we only accept `str` for obs_names, so depending on the actual usage in the function, `Iterable[str]` or `Sequence[str]` would be it I guess. - Usually single integers being passed around should be python `int`s, because they can be any size. But if you need to accept `int32` and `int64`, you can do `Union[int, np.integer]`. > Is there a way to say: ""should behave right if I call np.array on it"". In truth, `array` eats just about anything (with weird results), and for us this is probably a good idea:. ```py. Number = Union[float, int, np.integer, np.floating]. Num1DArrayLike = Sequence[Number]. Num2DArrayLike = Sequence[Num1DArrayLike]. Num3DArrayLike = Sequence[Num2DArrayLike]. NumNDArrayLike = Union[Num1DArrayLike, Num2DArrayLike, Num3DArrayLike]. ```. But if we want to be exact about `array_like`s, wed need this ABC:. ```py. class ArrayLike(ABC):. """"""An array,. any object exposing the array interface,. an object whose __array__ method returns an array,. or any (nested) sequence. """""". @classmethod. def __subclasshook__(cls, C):. if issubclass(C, np.ndarray):. return True. if any('__array_interface__' in B.__dict__ for B in C.__mro__):. return True. if any('__array__' in B.__dict__ for B in C.__mro__):. return True. return Sequence.__subclasshook__(cls, C). ```. ----. Two thoughts here:. 1. Its fine if you dont know exactly. Just use your best guess. The worst case is that someone wastes a second of runtime converting their argument to the type you thought was needed, while the original type woul",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:114,interoperability,specif,specifically,114,"Hi! Theres a series of abstract base classes (other languages call them interfaces) that can be used if you know specifically what you want (e.g. `typing.Sequence[T]` if it only needs to be iterable and indexable, but not necessarily a `list`). About your examples, generally I always have to dig into the code to figure such things out. Annoying, but it means that people after me can just use the type annotations instead of wasting their time doing the same. - Currently we only accept `str` for obs_names, so depending on the actual usage in the function, `Iterable[str]` or `Sequence[str]` would be it I guess. - Usually single integers being passed around should be python `int`s, because they can be any size. But if you need to accept `int32` and `int64`, you can do `Union[int, np.integer]`. > Is there a way to say: ""should behave right if I call np.array on it"". In truth, `array` eats just about anything (with weird results), and for us this is probably a good idea:. ```py. Number = Union[float, int, np.integer, np.floating]. Num1DArrayLike = Sequence[Number]. Num2DArrayLike = Sequence[Num1DArrayLike]. Num3DArrayLike = Sequence[Num2DArrayLike]. NumNDArrayLike = Union[Num1DArrayLike, Num2DArrayLike, Num3DArrayLike]. ```. But if we want to be exact about `array_like`s, wed need this ABC:. ```py. class ArrayLike(ABC):. """"""An array,. any object exposing the array interface,. an object whose __array__ method returns an array,. or any (nested) sequence. """""". @classmethod. def __subclasshook__(cls, C):. if issubclass(C, np.ndarray):. return True. if any('__array_interface__' in B.__dict__ for B in C.__mro__):. return True. if any('__array__' in B.__dict__ for B in C.__mro__):. return True. return Sequence.__subclasshook__(cls, C). ```. ----. Two thoughts here:. 1. Its fine if you dont know exactly. Just use your best guess. The worst case is that someone wastes a second of runtime converting their argument to the type you thought was needed, while the original type woul",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1383,interoperability,interfac,interface,1383," specifically what you want (e.g. `typing.Sequence[T]` if it only needs to be iterable and indexable, but not necessarily a `list`). About your examples, generally I always have to dig into the code to figure such things out. Annoying, but it means that people after me can just use the type annotations instead of wasting their time doing the same. - Currently we only accept `str` for obs_names, so depending on the actual usage in the function, `Iterable[str]` or `Sequence[str]` would be it I guess. - Usually single integers being passed around should be python `int`s, because they can be any size. But if you need to accept `int32` and `int64`, you can do `Union[int, np.integer]`. > Is there a way to say: ""should behave right if I call np.array on it"". In truth, `array` eats just about anything (with weird results), and for us this is probably a good idea:. ```py. Number = Union[float, int, np.integer, np.floating]. Num1DArrayLike = Sequence[Number]. Num2DArrayLike = Sequence[Num1DArrayLike]. Num3DArrayLike = Sequence[Num2DArrayLike]. NumNDArrayLike = Union[Num1DArrayLike, Num2DArrayLike, Num3DArrayLike]. ```. But if we want to be exact about `array_like`s, wed need this ABC:. ```py. class ArrayLike(ABC):. """"""An array,. any object exposing the array interface,. an object whose __array__ method returns an array,. or any (nested) sequence. """""". @classmethod. def __subclasshook__(cls, C):. if issubclass(C, np.ndarray):. return True. if any('__array_interface__' in B.__dict__ for B in C.__mro__):. return True. if any('__array__' in B.__dict__ for B in C.__mro__):. return True. return Sequence.__subclasshook__(cls, C). ```. ----. Two thoughts here:. 1. Its fine if you dont know exactly. Just use your best guess. The worst case is that someone wastes a second of runtime converting their argument to the type you thought was needed, while the original type would have been OK. 2. Its good if someone thinks about all that because that means things dont break unexpectedly!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:24,modifiability,abstract,abstract,24,"Hi! Theres a series of abstract base classes (other languages call them interfaces) that can be used if you know specifically what you want (e.g. `typing.Sequence[T]` if it only needs to be iterable and indexable, but not necessarily a `list`). About your examples, generally I always have to dig into the code to figure such things out. Annoying, but it means that people after me can just use the type annotations instead of wasting their time doing the same. - Currently we only accept `str` for obs_names, so depending on the actual usage in the function, `Iterable[str]` or `Sequence[str]` would be it I guess. - Usually single integers being passed around should be python `int`s, because they can be any size. But if you need to accept `int32` and `int64`, you can do `Union[int, np.integer]`. > Is there a way to say: ""should behave right if I call np.array on it"". In truth, `array` eats just about anything (with weird results), and for us this is probably a good idea:. ```py. Number = Union[float, int, np.integer, np.floating]. Num1DArrayLike = Sequence[Number]. Num2DArrayLike = Sequence[Num1DArrayLike]. Num3DArrayLike = Sequence[Num2DArrayLike]. NumNDArrayLike = Union[Num1DArrayLike, Num2DArrayLike, Num3DArrayLike]. ```. But if we want to be exact about `array_like`s, wed need this ABC:. ```py. class ArrayLike(ABC):. """"""An array,. any object exposing the array interface,. an object whose __array__ method returns an array,. or any (nested) sequence. """""". @classmethod. def __subclasshook__(cls, C):. if issubclass(C, np.ndarray):. return True. if any('__array_interface__' in B.__dict__ for B in C.__mro__):. return True. if any('__array__' in B.__dict__ for B in C.__mro__):. return True. return Sequence.__subclasshook__(cls, C). ```. ----. Two thoughts here:. 1. Its fine if you dont know exactly. Just use your best guess. The worst case is that someone wastes a second of runtime converting their argument to the type you thought was needed, while the original type woul",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:73,modifiability,interfac,interfaces,73,"Hi! Theres a series of abstract base classes (other languages call them interfaces) that can be used if you know specifically what you want (e.g. `typing.Sequence[T]` if it only needs to be iterable and indexable, but not necessarily a `list`). About your examples, generally I always have to dig into the code to figure such things out. Annoying, but it means that people after me can just use the type annotations instead of wasting their time doing the same. - Currently we only accept `str` for obs_names, so depending on the actual usage in the function, `Iterable[str]` or `Sequence[str]` would be it I guess. - Usually single integers being passed around should be python `int`s, because they can be any size. But if you need to accept `int32` and `int64`, you can do `Union[int, np.integer]`. > Is there a way to say: ""should behave right if I call np.array on it"". In truth, `array` eats just about anything (with weird results), and for us this is probably a good idea:. ```py. Number = Union[float, int, np.integer, np.floating]. Num1DArrayLike = Sequence[Number]. Num2DArrayLike = Sequence[Num1DArrayLike]. Num3DArrayLike = Sequence[Num2DArrayLike]. NumNDArrayLike = Union[Num1DArrayLike, Num2DArrayLike, Num3DArrayLike]. ```. But if we want to be exact about `array_like`s, wed need this ABC:. ```py. class ArrayLike(ABC):. """"""An array,. any object exposing the array interface,. an object whose __array__ method returns an array,. or any (nested) sequence. """""". @classmethod. def __subclasshook__(cls, C):. if issubclass(C, np.ndarray):. return True. if any('__array_interface__' in B.__dict__ for B in C.__mro__):. return True. if any('__array__' in B.__dict__ for B in C.__mro__):. return True. return Sequence.__subclasshook__(cls, C). ```. ----. Two thoughts here:. 1. Its fine if you dont know exactly. Just use your best guess. The worst case is that someone wastes a second of runtime converting their argument to the type you thought was needed, while the original type woul",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:514,modifiability,depend,depending,514,"Hi! Theres a series of abstract base classes (other languages call them interfaces) that can be used if you know specifically what you want (e.g. `typing.Sequence[T]` if it only needs to be iterable and indexable, but not necessarily a `list`). About your examples, generally I always have to dig into the code to figure such things out. Annoying, but it means that people after me can just use the type annotations instead of wasting their time doing the same. - Currently we only accept `str` for obs_names, so depending on the actual usage in the function, `Iterable[str]` or `Sequence[str]` would be it I guess. - Usually single integers being passed around should be python `int`s, because they can be any size. But if you need to accept `int32` and `int64`, you can do `Union[int, np.integer]`. > Is there a way to say: ""should behave right if I call np.array on it"". In truth, `array` eats just about anything (with weird results), and for us this is probably a good idea:. ```py. Number = Union[float, int, np.integer, np.floating]. Num1DArrayLike = Sequence[Number]. Num2DArrayLike = Sequence[Num1DArrayLike]. Num3DArrayLike = Sequence[Num2DArrayLike]. NumNDArrayLike = Union[Num1DArrayLike, Num2DArrayLike, Num3DArrayLike]. ```. But if we want to be exact about `array_like`s, wed need this ABC:. ```py. class ArrayLike(ABC):. """"""An array,. any object exposing the array interface,. an object whose __array__ method returns an array,. or any (nested) sequence. """""". @classmethod. def __subclasshook__(cls, C):. if issubclass(C, np.ndarray):. return True. if any('__array_interface__' in B.__dict__ for B in C.__mro__):. return True. if any('__array__' in B.__dict__ for B in C.__mro__):. return True. return Sequence.__subclasshook__(cls, C). ```. ----. Two thoughts here:. 1. Its fine if you dont know exactly. Just use your best guess. The worst case is that someone wastes a second of runtime converting their argument to the type you thought was needed, while the original type woul",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1383,modifiability,interfac,interface,1383," specifically what you want (e.g. `typing.Sequence[T]` if it only needs to be iterable and indexable, but not necessarily a `list`). About your examples, generally I always have to dig into the code to figure such things out. Annoying, but it means that people after me can just use the type annotations instead of wasting their time doing the same. - Currently we only accept `str` for obs_names, so depending on the actual usage in the function, `Iterable[str]` or `Sequence[str]` would be it I guess. - Usually single integers being passed around should be python `int`s, because they can be any size. But if you need to accept `int32` and `int64`, you can do `Union[int, np.integer]`. > Is there a way to say: ""should behave right if I call np.array on it"". In truth, `array` eats just about anything (with weird results), and for us this is probably a good idea:. ```py. Number = Union[float, int, np.integer, np.floating]. Num1DArrayLike = Sequence[Number]. Num2DArrayLike = Sequence[Num1DArrayLike]. Num3DArrayLike = Sequence[Num2DArrayLike]. NumNDArrayLike = Union[Num1DArrayLike, Num2DArrayLike, Num3DArrayLike]. ```. But if we want to be exact about `array_like`s, wed need this ABC:. ```py. class ArrayLike(ABC):. """"""An array,. any object exposing the array interface,. an object whose __array__ method returns an array,. or any (nested) sequence. """""". @classmethod. def __subclasshook__(cls, C):. if issubclass(C, np.ndarray):. return True. if any('__array_interface__' in B.__dict__ for B in C.__mro__):. return True. if any('__array__' in B.__dict__ for B in C.__mro__):. return True. return Sequence.__subclasshook__(cls, C). ```. ----. Two thoughts here:. 1. Its fine if you dont know exactly. Just use your best guess. The worst case is that someone wastes a second of runtime converting their argument to the type you thought was needed, while the original type would have been OK. 2. Its good if someone thinks about all that because that means things dont break unexpectedly!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:442,performance,time,time,442,"Hi! Theres a series of abstract base classes (other languages call them interfaces) that can be used if you know specifically what you want (e.g. `typing.Sequence[T]` if it only needs to be iterable and indexable, but not necessarily a `list`). About your examples, generally I always have to dig into the code to figure such things out. Annoying, but it means that people after me can just use the type annotations instead of wasting their time doing the same. - Currently we only accept `str` for obs_names, so depending on the actual usage in the function, `Iterable[str]` or `Sequence[str]` would be it I guess. - Usually single integers being passed around should be python `int`s, because they can be any size. But if you need to accept `int32` and `int64`, you can do `Union[int, np.integer]`. > Is there a way to say: ""should behave right if I call np.array on it"". In truth, `array` eats just about anything (with weird results), and for us this is probably a good idea:. ```py. Number = Union[float, int, np.integer, np.floating]. Num1DArrayLike = Sequence[Number]. Num2DArrayLike = Sequence[Num1DArrayLike]. Num3DArrayLike = Sequence[Num2DArrayLike]. NumNDArrayLike = Union[Num1DArrayLike, Num2DArrayLike, Num3DArrayLike]. ```. But if we want to be exact about `array_like`s, wed need this ABC:. ```py. class ArrayLike(ABC):. """"""An array,. any object exposing the array interface,. an object whose __array__ method returns an array,. or any (nested) sequence. """""". @classmethod. def __subclasshook__(cls, C):. if issubclass(C, np.ndarray):. return True. if any('__array_interface__' in B.__dict__ for B in C.__mro__):. return True. if any('__array__' in B.__dict__ for B in C.__mro__):. return True. return Sequence.__subclasshook__(cls, C). ```. ----. Two thoughts here:. 1. Its fine if you dont know exactly. Just use your best guess. The worst case is that someone wastes a second of runtime converting their argument to the type you thought was needed, while the original type woul",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:514,safety,depend,depending,514,"Hi! Theres a series of abstract base classes (other languages call them interfaces) that can be used if you know specifically what you want (e.g. `typing.Sequence[T]` if it only needs to be iterable and indexable, but not necessarily a `list`). About your examples, generally I always have to dig into the code to figure such things out. Annoying, but it means that people after me can just use the type annotations instead of wasting their time doing the same. - Currently we only accept `str` for obs_names, so depending on the actual usage in the function, `Iterable[str]` or `Sequence[str]` would be it I guess. - Usually single integers being passed around should be python `int`s, because they can be any size. But if you need to accept `int32` and `int64`, you can do `Union[int, np.integer]`. > Is there a way to say: ""should behave right if I call np.array on it"". In truth, `array` eats just about anything (with weird results), and for us this is probably a good idea:. ```py. Number = Union[float, int, np.integer, np.floating]. Num1DArrayLike = Sequence[Number]. Num2DArrayLike = Sequence[Num1DArrayLike]. Num3DArrayLike = Sequence[Num2DArrayLike]. NumNDArrayLike = Union[Num1DArrayLike, Num2DArrayLike, Num3DArrayLike]. ```. But if we want to be exact about `array_like`s, wed need this ABC:. ```py. class ArrayLike(ABC):. """"""An array,. any object exposing the array interface,. an object whose __array__ method returns an array,. or any (nested) sequence. """""". @classmethod. def __subclasshook__(cls, C):. if issubclass(C, np.ndarray):. return True. if any('__array_interface__' in B.__dict__ for B in C.__mro__):. return True. if any('__array__' in B.__dict__ for B in C.__mro__):. return True. return Sequence.__subclasshook__(cls, C). ```. ----. Two thoughts here:. 1. Its fine if you dont know exactly. Just use your best guess. The worst case is that someone wastes a second of runtime converting their argument to the type you thought was needed, while the original type woul",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1364,security,expos,exposing,1364," specifically what you want (e.g. `typing.Sequence[T]` if it only needs to be iterable and indexable, but not necessarily a `list`). About your examples, generally I always have to dig into the code to figure such things out. Annoying, but it means that people after me can just use the type annotations instead of wasting their time doing the same. - Currently we only accept `str` for obs_names, so depending on the actual usage in the function, `Iterable[str]` or `Sequence[str]` would be it I guess. - Usually single integers being passed around should be python `int`s, because they can be any size. But if you need to accept `int32` and `int64`, you can do `Union[int, np.integer]`. > Is there a way to say: ""should behave right if I call np.array on it"". In truth, `array` eats just about anything (with weird results), and for us this is probably a good idea:. ```py. Number = Union[float, int, np.integer, np.floating]. Num1DArrayLike = Sequence[Number]. Num2DArrayLike = Sequence[Num1DArrayLike]. Num3DArrayLike = Sequence[Num2DArrayLike]. NumNDArrayLike = Union[Num1DArrayLike, Num2DArrayLike, Num3DArrayLike]. ```. But if we want to be exact about `array_like`s, wed need this ABC:. ```py. class ArrayLike(ABC):. """"""An array,. any object exposing the array interface,. an object whose __array__ method returns an array,. or any (nested) sequence. """""". @classmethod. def __subclasshook__(cls, C):. if issubclass(C, np.ndarray):. return True. if any('__array_interface__' in B.__dict__ for B in C.__mro__):. return True. if any('__array__' in B.__dict__ for B in C.__mro__):. return True. return Sequence.__subclasshook__(cls, C). ```. ----. Two thoughts here:. 1. Its fine if you dont know exactly. Just use your best guess. The worst case is that someone wastes a second of runtime converting their argument to the type you thought was needed, while the original type would have been OK. 2. Its good if someone thinks about all that because that means things dont break unexpectedly!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:514,testability,depend,depending,514,"Hi! Theres a series of abstract base classes (other languages call them interfaces) that can be used if you know specifically what you want (e.g. `typing.Sequence[T]` if it only needs to be iterable and indexable, but not necessarily a `list`). About your examples, generally I always have to dig into the code to figure such things out. Annoying, but it means that people after me can just use the type annotations instead of wasting their time doing the same. - Currently we only accept `str` for obs_names, so depending on the actual usage in the function, `Iterable[str]` or `Sequence[str]` would be it I guess. - Usually single integers being passed around should be python `int`s, because they can be any size. But if you need to accept `int32` and `int64`, you can do `Union[int, np.integer]`. > Is there a way to say: ""should behave right if I call np.array on it"". In truth, `array` eats just about anything (with weird results), and for us this is probably a good idea:. ```py. Number = Union[float, int, np.integer, np.floating]. Num1DArrayLike = Sequence[Number]. Num2DArrayLike = Sequence[Num1DArrayLike]. Num3DArrayLike = Sequence[Num2DArrayLike]. NumNDArrayLike = Union[Num1DArrayLike, Num2DArrayLike, Num3DArrayLike]. ```. But if we want to be exact about `array_like`s, wed need this ABC:. ```py. class ArrayLike(ABC):. """"""An array,. any object exposing the array interface,. an object whose __array__ method returns an array,. or any (nested) sequence. """""". @classmethod. def __subclasshook__(cls, C):. if issubclass(C, np.ndarray):. return True. if any('__array_interface__' in B.__dict__ for B in C.__mro__):. return True. if any('__array__' in B.__dict__ for B in C.__mro__):. return True. return Sequence.__subclasshook__(cls, C). ```. ----. Two thoughts here:. 1. Its fine if you dont know exactly. Just use your best guess. The worst case is that someone wastes a second of runtime converting their argument to the type you thought was needed, while the original type woul",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:210,availability,error,errors,210,@flying-sheep Regarding your first thought... it may cause issues when interfacing with other functions that do not have type annotations on the arguments. And users may then find it difficult to interpret the errors.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:71,integrability,interfac,interfacing,71,@flying-sheep Regarding your first thought... it may cause issues when interfacing with other functions that do not have type annotations on the arguments. And users may then find it difficult to interpret the errors.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:71,interoperability,interfac,interfacing,71,@flying-sheep Regarding your first thought... it may cause issues when interfacing with other functions that do not have type annotations on the arguments. And users may then find it difficult to interpret the errors.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:71,modifiability,interfac,interfacing,71,@flying-sheep Regarding your first thought... it may cause issues when interfacing with other functions that do not have type annotations on the arguments. And users may then find it difficult to interpret the errors.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:210,performance,error,errors,210,@flying-sheep Regarding your first thought... it may cause issues when interfacing with other functions that do not have type annotations on the arguments. And users may then find it difficult to interpret the errors.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:210,safety,error,errors,210,@flying-sheep Regarding your first thought... it may cause issues when interfacing with other functions that do not have type annotations on the arguments. And users may then find it difficult to interpret the errors.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:160,usability,user,users,160,@flying-sheep Regarding your first thought... it may cause issues when interfacing with other functions that do not have type annotations on the arguments. And users may then find it difficult to interpret the errors.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:210,usability,error,errors,210,@flying-sheep Regarding your first thought... it may cause issues when interfacing with other functions that do not have type annotations on the arguments. And users may then find it difficult to interpret the errors.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:99,safety,test,tests,99,no? why would it? type annotations are only used for people and IDEs (unless you use mypy in your tests to check if everything is sound),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:99,testability,test,tests,99,no? why would it? type annotations are only used for people and IDEs (unless you use mypy in your tests to check if everything is sound),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:172,safety,input,input,172,"Assuming I understand typing correctly... I'm imagining this in the same way that anndata's copy function always cast to `np.float32` regardless of `adata.X` type. If your input expects `adata.X` in `np.float32`, and a previous function (e.g., ComBat) has changed the data to `np.foat64`, then you may not know why running ComBat and then copying your dataset no longer works.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:11,testability,understand,understand,11,"Assuming I understand typing correctly... I'm imagining this in the same way that anndata's copy function always cast to `np.float32` regardless of `adata.X` type. If your input expects `adata.X` in `np.float32`, and a previous function (e.g., ComBat) has changed the data to `np.foat64`, then you may not know why running ComBat and then copying your dataset no longer works.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:172,usability,input,input,172,"Assuming I understand typing correctly... I'm imagining this in the same way that anndata's copy function always cast to `np.float32` regardless of `adata.X` type. If your input expects `adata.X` in `np.float32`, and a previous function (e.g., ComBat) has changed the data to `np.foat64`, then you may not know why running ComBat and then copying your dataset no longer works.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:90,availability,error,errors,90,"How? As said, theyre just for people and IDEs. Scanpy doesnt use them. It doesnt throw errors in case something doesnt fit. We could use https://pypi.org/project/typecheck-decorator/ to throw errors when something is passed that doesnt fit the annotations. However, doing so has a performance hit and requires flawless annotations (because if the annotations were wrong, that *would* start suddenly throwing errors). Im just adding type annotations to improve user friendliness by being more clear what functions accept, and because it makes writing documentation easier.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:196,availability,error,errors,196,"How? As said, theyre just for people and IDEs. Scanpy doesnt use them. It doesnt throw errors in case something doesnt fit. We could use https://pypi.org/project/typecheck-decorator/ to throw errors when something is passed that doesnt fit the annotations. However, doing so has a performance hit and requires flawless annotations (because if the annotations were wrong, that *would* start suddenly throwing errors). Im just adding type annotations to improve user friendliness by being more clear what functions accept, and because it makes writing documentation easier.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:413,availability,error,errors,413,"How? As said, theyre just for people and IDEs. Scanpy doesnt use them. It doesnt throw errors in case something doesnt fit. We could use https://pypi.org/project/typecheck-decorator/ to throw errors when something is passed that doesnt fit the annotations. However, doing so has a performance hit and requires flawless annotations (because if the annotations were wrong, that *would* start suddenly throwing errors). Im just adding type annotations to improve user friendliness by being more clear what functions accept, and because it makes writing documentation easier.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:176,modifiability,deco,decorator,176,"How? As said, theyre just for people and IDEs. Scanpy doesnt use them. It doesnt throw errors in case something doesnt fit. We could use https://pypi.org/project/typecheck-decorator/ to throw errors when something is passed that doesnt fit the annotations. However, doing so has a performance hit and requires flawless annotations (because if the annotations were wrong, that *would* start suddenly throwing errors). Im just adding type annotations to improve user friendliness by being more clear what functions accept, and because it makes writing documentation easier.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:90,performance,error,errors,90,"How? As said, theyre just for people and IDEs. Scanpy doesnt use them. It doesnt throw errors in case something doesnt fit. We could use https://pypi.org/project/typecheck-decorator/ to throw errors when something is passed that doesnt fit the annotations. However, doing so has a performance hit and requires flawless annotations (because if the annotations were wrong, that *would* start suddenly throwing errors). Im just adding type annotations to improve user friendliness by being more clear what functions accept, and because it makes writing documentation easier.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:196,performance,error,errors,196,"How? As said, theyre just for people and IDEs. Scanpy doesnt use them. It doesnt throw errors in case something doesnt fit. We could use https://pypi.org/project/typecheck-decorator/ to throw errors when something is passed that doesnt fit the annotations. However, doing so has a performance hit and requires flawless annotations (because if the annotations were wrong, that *would* start suddenly throwing errors). Im just adding type annotations to improve user friendliness by being more clear what functions accept, and because it makes writing documentation easier.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:286,performance,perform,performance,286,"How? As said, theyre just for people and IDEs. Scanpy doesnt use them. It doesnt throw errors in case something doesnt fit. We could use https://pypi.org/project/typecheck-decorator/ to throw errors when something is passed that doesnt fit the annotations. However, doing so has a performance hit and requires flawless annotations (because if the annotations were wrong, that *would* start suddenly throwing errors). Im just adding type annotations to improve user friendliness by being more clear what functions accept, and because it makes writing documentation easier.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:413,performance,error,errors,413,"How? As said, theyre just for people and IDEs. Scanpy doesnt use them. It doesnt throw errors in case something doesnt fit. We could use https://pypi.org/project/typecheck-decorator/ to throw errors when something is passed that doesnt fit the annotations. However, doing so has a performance hit and requires flawless annotations (because if the annotations were wrong, that *would* start suddenly throwing errors). Im just adding type annotations to improve user friendliness by being more clear what functions accept, and because it makes writing documentation easier.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:55,reliability,doe,doesn,55,"How? As said, theyre just for people and IDEs. Scanpy doesnt use them. It doesnt throw errors in case something doesnt fit. We could use https://pypi.org/project/typecheck-decorator/ to throw errors when something is passed that doesnt fit the annotations. However, doing so has a performance hit and requires flawless annotations (because if the annotations were wrong, that *would* start suddenly throwing errors). Im just adding type annotations to improve user friendliness by being more clear what functions accept, and because it makes writing documentation easier.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:76,reliability,doe,doesn,76,"How? As said, theyre just for people and IDEs. Scanpy doesnt use them. It doesnt throw errors in case something doesnt fit. We could use https://pypi.org/project/typecheck-decorator/ to throw errors when something is passed that doesnt fit the annotations. However, doing so has a performance hit and requires flawless annotations (because if the annotations were wrong, that *would* start suddenly throwing errors). Im just adding type annotations to improve user friendliness by being more clear what functions accept, and because it makes writing documentation easier.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:115,reliability,doe,doesn,115,"How? As said, theyre just for people and IDEs. Scanpy doesnt use them. It doesnt throw errors in case something doesnt fit. We could use https://pypi.org/project/typecheck-decorator/ to throw errors when something is passed that doesnt fit the annotations. However, doing so has a performance hit and requires flawless annotations (because if the annotations were wrong, that *would* start suddenly throwing errors). Im just adding type annotations to improve user friendliness by being more clear what functions accept, and because it makes writing documentation easier.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:233,reliability,doe,doesn,233,"How? As said, theyre just for people and IDEs. Scanpy doesnt use them. It doesnt throw errors in case something doesnt fit. We could use https://pypi.org/project/typecheck-decorator/ to throw errors when something is passed that doesnt fit the annotations. However, doing so has a performance hit and requires flawless annotations (because if the annotations were wrong, that *would* start suddenly throwing errors). Im just adding type annotations to improve user friendliness by being more clear what functions accept, and because it makes writing documentation easier.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:90,safety,error,errors,90,"How? As said, theyre just for people and IDEs. Scanpy doesnt use them. It doesnt throw errors in case something doesnt fit. We could use https://pypi.org/project/typecheck-decorator/ to throw errors when something is passed that doesnt fit the annotations. However, doing so has a performance hit and requires flawless annotations (because if the annotations were wrong, that *would* start suddenly throwing errors). Im just adding type annotations to improve user friendliness by being more clear what functions accept, and because it makes writing documentation easier.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:196,safety,error,errors,196,"How? As said, theyre just for people and IDEs. Scanpy doesnt use them. It doesnt throw errors in case something doesnt fit. We could use https://pypi.org/project/typecheck-decorator/ to throw errors when something is passed that doesnt fit the annotations. However, doing so has a performance hit and requires flawless annotations (because if the annotations were wrong, that *would* start suddenly throwing errors). Im just adding type annotations to improve user friendliness by being more clear what functions accept, and because it makes writing documentation easier.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:413,safety,error,errors,413,"How? As said, theyre just for people and IDEs. Scanpy doesnt use them. It doesnt throw errors in case something doesnt fit. We could use https://pypi.org/project/typecheck-decorator/ to throw errors when something is passed that doesnt fit the annotations. However, doing so has a performance hit and requires flawless annotations (because if the annotations were wrong, that *would* start suddenly throwing errors). Im just adding type annotations to improve user friendliness by being more clear what functions accept, and because it makes writing documentation easier.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:90,usability,error,errors,90,"How? As said, theyre just for people and IDEs. Scanpy doesnt use them. It doesnt throw errors in case something doesnt fit. We could use https://pypi.org/project/typecheck-decorator/ to throw errors when something is passed that doesnt fit the annotations. However, doing so has a performance hit and requires flawless annotations (because if the annotations were wrong, that *would* start suddenly throwing errors). Im just adding type annotations to improve user friendliness by being more clear what functions accept, and because it makes writing documentation easier.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:196,usability,error,errors,196,"How? As said, theyre just for people and IDEs. Scanpy doesnt use them. It doesnt throw errors in case something doesnt fit. We could use https://pypi.org/project/typecheck-decorator/ to throw errors when something is passed that doesnt fit the annotations. However, doing so has a performance hit and requires flawless annotations (because if the annotations were wrong, that *would* start suddenly throwing errors). Im just adding type annotations to improve user friendliness by being more clear what functions accept, and because it makes writing documentation easier.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:286,usability,perform,performance,286,"How? As said, theyre just for people and IDEs. Scanpy doesnt use them. It doesnt throw errors in case something doesnt fit. We could use https://pypi.org/project/typecheck-decorator/ to throw errors when something is passed that doesnt fit the annotations. However, doing so has a performance hit and requires flawless annotations (because if the annotations were wrong, that *would* start suddenly throwing errors). Im just adding type annotations to improve user friendliness by being more clear what functions accept, and because it makes writing documentation easier.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:413,usability,error,errors,413,"How? As said, theyre just for people and IDEs. Scanpy doesnt use them. It doesnt throw errors in case something doesnt fit. We could use https://pypi.org/project/typecheck-decorator/ to throw errors when something is passed that doesnt fit the annotations. However, doing so has a performance hit and requires flawless annotations (because if the annotations were wrong, that *would* start suddenly throwing errors). Im just adding type annotations to improve user friendliness by being more clear what functions accept, and because it makes writing documentation easier.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:466,usability,user,user,466,"How? As said, theyre just for people and IDEs. Scanpy doesnt use them. It doesnt throw errors in case something doesnt fit. We could use https://pypi.org/project/typecheck-decorator/ to throw errors when something is passed that doesnt fit the annotations. However, doing so has a performance hit and requires flawless annotations (because if the annotations were wrong, that *would* start suddenly throwing errors). Im just adding type annotations to improve user friendliness by being more clear what functions accept, and because it makes writing documentation easier.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:498,usability,clear,clear,498,"How? As said, theyre just for people and IDEs. Scanpy doesnt use them. It doesnt throw errors in case something doesnt fit. We could use https://pypi.org/project/typecheck-decorator/ to throw errors when something is passed that doesnt fit the annotations. However, doing so has a performance hit and requires flawless annotations (because if the annotations were wrong, that *would* start suddenly throwing errors). Im just adding type annotations to improve user friendliness by being more clear what functions accept, and because it makes writing documentation easier.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:556,usability,document,documentation,556,"How? As said, theyre just for people and IDEs. Scanpy doesnt use them. It doesnt throw errors in case something doesnt fit. We could use https://pypi.org/project/typecheck-decorator/ to throw errors when something is passed that doesnt fit the annotations. However, doing so has a performance hit and requires flawless annotations (because if the annotations were wrong, that *would* start suddenly throwing errors). Im just adding type annotations to improve user friendliness by being more clear what functions accept, and because it makes writing documentation easier.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:119,availability,error,errors,119,"In that case, I don't fully understand this typing and will just continue reading quietly ;). I assumed it would throw errors as for example in C++.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:65,deployability,continu,continue,65,"In that case, I don't fully understand this typing and will just continue reading quietly ;). I assumed it would throw errors as for example in C++.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:119,performance,error,errors,119,"In that case, I don't fully understand this typing and will just continue reading quietly ;). I assumed it would throw errors as for example in C++.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:119,safety,error,errors,119,"In that case, I don't fully understand this typing and will just continue reading quietly ;). I assumed it would throw errors as for example in C++.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:28,testability,understand,understand,28,"In that case, I don't fully understand this typing and will just continue reading quietly ;). I assumed it would throw errors as for example in C++.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:119,usability,error,errors,119,"In that case, I don't fully understand this typing and will just continue reading quietly ;). I assumed it would throw errors as for example in C++.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:638,availability,error,error,638,"Theres a few uses:. 1. Humans. Once you understand the syntax ([very easy](https://docs.python.org/3/library/typing.html), i just get `Generator` wrong all the time) it improves your understanding what a function really accepts and returns. 2. IDEs. Theyll get better when inferring the types of variables and will show you more actual problems in the code and less false positives. 3. Testing. Some projects use mypy to check if all code in your repo typechecks properly, which can be integrated into a test suite. 4. Runtime type checking. Has a performance hit (as said) but given proper type hints, it makes your code safer and the error messages better (Function blah excepted a parameter foo of type Bar, but you passed a foo of type Baz). im not planning to do 3 and 4 (yet, and probably never)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:488,deployability,integr,integrated,488,"Theres a few uses:. 1. Humans. Once you understand the syntax ([very easy](https://docs.python.org/3/library/typing.html), i just get `Generator` wrong all the time) it improves your understanding what a function really accepts and returns. 2. IDEs. Theyll get better when inferring the types of variables and will show you more actual problems in the code and less false positives. 3. Testing. Some projects use mypy to check if all code in your repo typechecks properly, which can be integrated into a test suite. 4. Runtime type checking. Has a performance hit (as said) but given proper type hints, it makes your code safer and the error messages better (Function blah excepted a parameter foo of type Bar, but you passed a foo of type Baz). im not planning to do 3 and 4 (yet, and probably never)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:488,integrability,integr,integrated,488,"Theres a few uses:. 1. Humans. Once you understand the syntax ([very easy](https://docs.python.org/3/library/typing.html), i just get `Generator` wrong all the time) it improves your understanding what a function really accepts and returns. 2. IDEs. Theyll get better when inferring the types of variables and will show you more actual problems in the code and less false positives. 3. Testing. Some projects use mypy to check if all code in your repo typechecks properly, which can be integrated into a test suite. 4. Runtime type checking. Has a performance hit (as said) but given proper type hints, it makes your code safer and the error messages better (Function blah excepted a parameter foo of type Bar, but you passed a foo of type Baz). im not planning to do 3 and 4 (yet, and probably never)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:644,integrability,messag,messages,644,"Theres a few uses:. 1. Humans. Once you understand the syntax ([very easy](https://docs.python.org/3/library/typing.html), i just get `Generator` wrong all the time) it improves your understanding what a function really accepts and returns. 2. IDEs. Theyll get better when inferring the types of variables and will show you more actual problems in the code and less false positives. 3. Testing. Some projects use mypy to check if all code in your repo typechecks properly, which can be integrated into a test suite. 4. Runtime type checking. Has a performance hit (as said) but given proper type hints, it makes your code safer and the error messages better (Function blah excepted a parameter foo of type Bar, but you passed a foo of type Baz). im not planning to do 3 and 4 (yet, and probably never)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:488,interoperability,integr,integrated,488,"Theres a few uses:. 1. Humans. Once you understand the syntax ([very easy](https://docs.python.org/3/library/typing.html), i just get `Generator` wrong all the time) it improves your understanding what a function really accepts and returns. 2. IDEs. Theyll get better when inferring the types of variables and will show you more actual problems in the code and less false positives. 3. Testing. Some projects use mypy to check if all code in your repo typechecks properly, which can be integrated into a test suite. 4. Runtime type checking. Has a performance hit (as said) but given proper type hints, it makes your code safer and the error messages better (Function blah excepted a parameter foo of type Bar, but you passed a foo of type Baz). im not planning to do 3 and 4 (yet, and probably never)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:644,interoperability,messag,messages,644,"Theres a few uses:. 1. Humans. Once you understand the syntax ([very easy](https://docs.python.org/3/library/typing.html), i just get `Generator` wrong all the time) it improves your understanding what a function really accepts and returns. 2. IDEs. Theyll get better when inferring the types of variables and will show you more actual problems in the code and less false positives. 3. Testing. Some projects use mypy to check if all code in your repo typechecks properly, which can be integrated into a test suite. 4. Runtime type checking. Has a performance hit (as said) but given proper type hints, it makes your code safer and the error messages better (Function blah excepted a parameter foo of type Bar, but you passed a foo of type Baz). im not planning to do 3 and 4 (yet, and probably never)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:298,modifiability,variab,variables,298,"Theres a few uses:. 1. Humans. Once you understand the syntax ([very easy](https://docs.python.org/3/library/typing.html), i just get `Generator` wrong all the time) it improves your understanding what a function really accepts and returns. 2. IDEs. Theyll get better when inferring the types of variables and will show you more actual problems in the code and less false positives. 3. Testing. Some projects use mypy to check if all code in your repo typechecks properly, which can be integrated into a test suite. 4. Runtime type checking. Has a performance hit (as said) but given proper type hints, it makes your code safer and the error messages better (Function blah excepted a parameter foo of type Bar, but you passed a foo of type Baz). im not planning to do 3 and 4 (yet, and probably never)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:488,modifiability,integr,integrated,488,"Theres a few uses:. 1. Humans. Once you understand the syntax ([very easy](https://docs.python.org/3/library/typing.html), i just get `Generator` wrong all the time) it improves your understanding what a function really accepts and returns. 2. IDEs. Theyll get better when inferring the types of variables and will show you more actual problems in the code and less false positives. 3. Testing. Some projects use mypy to check if all code in your repo typechecks properly, which can be integrated into a test suite. 4. Runtime type checking. Has a performance hit (as said) but given proper type hints, it makes your code safer and the error messages better (Function blah excepted a parameter foo of type Bar, but you passed a foo of type Baz). im not planning to do 3 and 4 (yet, and probably never)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:687,modifiability,paramet,parameter,687,"Theres a few uses:. 1. Humans. Once you understand the syntax ([very easy](https://docs.python.org/3/library/typing.html), i just get `Generator` wrong all the time) it improves your understanding what a function really accepts and returns. 2. IDEs. Theyll get better when inferring the types of variables and will show you more actual problems in the code and less false positives. 3. Testing. Some projects use mypy to check if all code in your repo typechecks properly, which can be integrated into a test suite. 4. Runtime type checking. Has a performance hit (as said) but given proper type hints, it makes your code safer and the error messages better (Function blah excepted a parameter foo of type Bar, but you passed a foo of type Baz). im not planning to do 3 and 4 (yet, and probably never)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:161,performance,time,time,161,"Theres a few uses:. 1. Humans. Once you understand the syntax ([very easy](https://docs.python.org/3/library/typing.html), i just get `Generator` wrong all the time) it improves your understanding what a function really accepts and returns. 2. IDEs. Theyll get better when inferring the types of variables and will show you more actual problems in the code and less false positives. 3. Testing. Some projects use mypy to check if all code in your repo typechecks properly, which can be integrated into a test suite. 4. Runtime type checking. Has a performance hit (as said) but given proper type hints, it makes your code safer and the error messages better (Function blah excepted a parameter foo of type Bar, but you passed a foo of type Baz). im not planning to do 3 and 4 (yet, and probably never)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:550,performance,perform,performance,550,"Theres a few uses:. 1. Humans. Once you understand the syntax ([very easy](https://docs.python.org/3/library/typing.html), i just get `Generator` wrong all the time) it improves your understanding what a function really accepts and returns. 2. IDEs. Theyll get better when inferring the types of variables and will show you more actual problems in the code and less false positives. 3. Testing. Some projects use mypy to check if all code in your repo typechecks properly, which can be integrated into a test suite. 4. Runtime type checking. Has a performance hit (as said) but given proper type hints, it makes your code safer and the error messages better (Function blah excepted a parameter foo of type Bar, but you passed a foo of type Baz). im not planning to do 3 and 4 (yet, and probably never)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:638,performance,error,error,638,"Theres a few uses:. 1. Humans. Once you understand the syntax ([very easy](https://docs.python.org/3/library/typing.html), i just get `Generator` wrong all the time) it improves your understanding what a function really accepts and returns. 2. IDEs. Theyll get better when inferring the types of variables and will show you more actual problems in the code and less false positives. 3. Testing. Some projects use mypy to check if all code in your repo typechecks properly, which can be integrated into a test suite. 4. Runtime type checking. Has a performance hit (as said) but given proper type hints, it makes your code safer and the error messages better (Function blah excepted a parameter foo of type Bar, but you passed a foo of type Baz). im not planning to do 3 and 4 (yet, and probably never)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:488,reliability,integr,integrated,488,"Theres a few uses:. 1. Humans. Once you understand the syntax ([very easy](https://docs.python.org/3/library/typing.html), i just get `Generator` wrong all the time) it improves your understanding what a function really accepts and returns. 2. IDEs. Theyll get better when inferring the types of variables and will show you more actual problems in the code and less false positives. 3. Testing. Some projects use mypy to check if all code in your repo typechecks properly, which can be integrated into a test suite. 4. Runtime type checking. Has a performance hit (as said) but given proper type hints, it makes your code safer and the error messages better (Function blah excepted a parameter foo of type Bar, but you passed a foo of type Baz). im not planning to do 3 and 4 (yet, and probably never)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:388,safety,Test,Testing,388,"Theres a few uses:. 1. Humans. Once you understand the syntax ([very easy](https://docs.python.org/3/library/typing.html), i just get `Generator` wrong all the time) it improves your understanding what a function really accepts and returns. 2. IDEs. Theyll get better when inferring the types of variables and will show you more actual problems in the code and less false positives. 3. Testing. Some projects use mypy to check if all code in your repo typechecks properly, which can be integrated into a test suite. 4. Runtime type checking. Has a performance hit (as said) but given proper type hints, it makes your code safer and the error messages better (Function blah excepted a parameter foo of type Bar, but you passed a foo of type Baz). im not planning to do 3 and 4 (yet, and probably never)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:506,safety,test,test,506,"Theres a few uses:. 1. Humans. Once you understand the syntax ([very easy](https://docs.python.org/3/library/typing.html), i just get `Generator` wrong all the time) it improves your understanding what a function really accepts and returns. 2. IDEs. Theyll get better when inferring the types of variables and will show you more actual problems in the code and less false positives. 3. Testing. Some projects use mypy to check if all code in your repo typechecks properly, which can be integrated into a test suite. 4. Runtime type checking. Has a performance hit (as said) but given proper type hints, it makes your code safer and the error messages better (Function blah excepted a parameter foo of type Bar, but you passed a foo of type Baz). im not planning to do 3 and 4 (yet, and probably never)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:624,safety,safe,safer,624,"Theres a few uses:. 1. Humans. Once you understand the syntax ([very easy](https://docs.python.org/3/library/typing.html), i just get `Generator` wrong all the time) it improves your understanding what a function really accepts and returns. 2. IDEs. Theyll get better when inferring the types of variables and will show you more actual problems in the code and less false positives. 3. Testing. Some projects use mypy to check if all code in your repo typechecks properly, which can be integrated into a test suite. 4. Runtime type checking. Has a performance hit (as said) but given proper type hints, it makes your code safer and the error messages better (Function blah excepted a parameter foo of type Bar, but you passed a foo of type Baz). im not planning to do 3 and 4 (yet, and probably never)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:638,safety,error,error,638,"Theres a few uses:. 1. Humans. Once you understand the syntax ([very easy](https://docs.python.org/3/library/typing.html), i just get `Generator` wrong all the time) it improves your understanding what a function really accepts and returns. 2. IDEs. Theyll get better when inferring the types of variables and will show you more actual problems in the code and less false positives. 3. Testing. Some projects use mypy to check if all code in your repo typechecks properly, which can be integrated into a test suite. 4. Runtime type checking. Has a performance hit (as said) but given proper type hints, it makes your code safer and the error messages better (Function blah excepted a parameter foo of type Bar, but you passed a foo of type Baz). im not planning to do 3 and 4 (yet, and probably never)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:676,safety,except,excepted,676,"Theres a few uses:. 1. Humans. Once you understand the syntax ([very easy](https://docs.python.org/3/library/typing.html), i just get `Generator` wrong all the time) it improves your understanding what a function really accepts and returns. 2. IDEs. Theyll get better when inferring the types of variables and will show you more actual problems in the code and less false positives. 3. Testing. Some projects use mypy to check if all code in your repo typechecks properly, which can be integrated into a test suite. 4. Runtime type checking. Has a performance hit (as said) but given proper type hints, it makes your code safer and the error messages better (Function blah excepted a parameter foo of type Bar, but you passed a foo of type Baz). im not planning to do 3 and 4 (yet, and probably never)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:488,security,integr,integrated,488,"Theres a few uses:. 1. Humans. Once you understand the syntax ([very easy](https://docs.python.org/3/library/typing.html), i just get `Generator` wrong all the time) it improves your understanding what a function really accepts and returns. 2. IDEs. Theyll get better when inferring the types of variables and will show you more actual problems in the code and less false positives. 3. Testing. Some projects use mypy to check if all code in your repo typechecks properly, which can be integrated into a test suite. 4. Runtime type checking. Has a performance hit (as said) but given proper type hints, it makes your code safer and the error messages better (Function blah excepted a parameter foo of type Bar, but you passed a foo of type Baz). im not planning to do 3 and 4 (yet, and probably never)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:41,testability,understand,understand,41,"Theres a few uses:. 1. Humans. Once you understand the syntax ([very easy](https://docs.python.org/3/library/typing.html), i just get `Generator` wrong all the time) it improves your understanding what a function really accepts and returns. 2. IDEs. Theyll get better when inferring the types of variables and will show you more actual problems in the code and less false positives. 3. Testing. Some projects use mypy to check if all code in your repo typechecks properly, which can be integrated into a test suite. 4. Runtime type checking. Has a performance hit (as said) but given proper type hints, it makes your code safer and the error messages better (Function blah excepted a parameter foo of type Bar, but you passed a foo of type Baz). im not planning to do 3 and 4 (yet, and probably never)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:184,testability,understand,understanding,184,"Theres a few uses:. 1. Humans. Once you understand the syntax ([very easy](https://docs.python.org/3/library/typing.html), i just get `Generator` wrong all the time) it improves your understanding what a function really accepts and returns. 2. IDEs. Theyll get better when inferring the types of variables and will show you more actual problems in the code and less false positives. 3. Testing. Some projects use mypy to check if all code in your repo typechecks properly, which can be integrated into a test suite. 4. Runtime type checking. Has a performance hit (as said) but given proper type hints, it makes your code safer and the error messages better (Function blah excepted a parameter foo of type Bar, but you passed a foo of type Baz). im not planning to do 3 and 4 (yet, and probably never)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:388,testability,Test,Testing,388,"Theres a few uses:. 1. Humans. Once you understand the syntax ([very easy](https://docs.python.org/3/library/typing.html), i just get `Generator` wrong all the time) it improves your understanding what a function really accepts and returns. 2. IDEs. Theyll get better when inferring the types of variables and will show you more actual problems in the code and less false positives. 3. Testing. Some projects use mypy to check if all code in your repo typechecks properly, which can be integrated into a test suite. 4. Runtime type checking. Has a performance hit (as said) but given proper type hints, it makes your code safer and the error messages better (Function blah excepted a parameter foo of type Bar, but you passed a foo of type Baz). im not planning to do 3 and 4 (yet, and probably never)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:488,testability,integr,integrated,488,"Theres a few uses:. 1. Humans. Once you understand the syntax ([very easy](https://docs.python.org/3/library/typing.html), i just get `Generator` wrong all the time) it improves your understanding what a function really accepts and returns. 2. IDEs. Theyll get better when inferring the types of variables and will show you more actual problems in the code and less false positives. 3. Testing. Some projects use mypy to check if all code in your repo typechecks properly, which can be integrated into a test suite. 4. Runtime type checking. Has a performance hit (as said) but given proper type hints, it makes your code safer and the error messages better (Function blah excepted a parameter foo of type Bar, but you passed a foo of type Baz). im not planning to do 3 and 4 (yet, and probably never)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:506,testability,test,test,506,"Theres a few uses:. 1. Humans. Once you understand the syntax ([very easy](https://docs.python.org/3/library/typing.html), i just get `Generator` wrong all the time) it improves your understanding what a function really accepts and returns. 2. IDEs. Theyll get better when inferring the types of variables and will show you more actual problems in the code and less false positives. 3. Testing. Some projects use mypy to check if all code in your repo typechecks properly, which can be integrated into a test suite. 4. Runtime type checking. Has a performance hit (as said) but given proper type hints, it makes your code safer and the error messages better (Function blah excepted a parameter foo of type Bar, but you passed a foo of type Baz). im not planning to do 3 and 4 (yet, and probably never)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:758,testability,plan,planning,758,"Theres a few uses:. 1. Humans. Once you understand the syntax ([very easy](https://docs.python.org/3/library/typing.html), i just get `Generator` wrong all the time) it improves your understanding what a function really accepts and returns. 2. IDEs. Theyll get better when inferring the types of variables and will show you more actual problems in the code and less false positives. 3. Testing. Some projects use mypy to check if all code in your repo typechecks properly, which can be integrated into a test suite. 4. Runtime type checking. Has a performance hit (as said) but given proper type hints, it makes your code safer and the error messages better (Function blah excepted a parameter foo of type Bar, but you passed a foo of type Baz). im not planning to do 3 and 4 (yet, and probably never)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:550,usability,perform,performance,550,"Theres a few uses:. 1. Humans. Once you understand the syntax ([very easy](https://docs.python.org/3/library/typing.html), i just get `Generator` wrong all the time) it improves your understanding what a function really accepts and returns. 2. IDEs. Theyll get better when inferring the types of variables and will show you more actual problems in the code and less false positives. 3. Testing. Some projects use mypy to check if all code in your repo typechecks properly, which can be integrated into a test suite. 4. Runtime type checking. Has a performance hit (as said) but given proper type hints, it makes your code safer and the error messages better (Function blah excepted a parameter foo of type Bar, but you passed a foo of type Baz). im not planning to do 3 and 4 (yet, and probably never)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:598,usability,hint,hints,598,"Theres a few uses:. 1. Humans. Once you understand the syntax ([very easy](https://docs.python.org/3/library/typing.html), i just get `Generator` wrong all the time) it improves your understanding what a function really accepts and returns. 2. IDEs. Theyll get better when inferring the types of variables and will show you more actual problems in the code and less false positives. 3. Testing. Some projects use mypy to check if all code in your repo typechecks properly, which can be integrated into a test suite. 4. Runtime type checking. Has a performance hit (as said) but given proper type hints, it makes your code safer and the error messages better (Function blah excepted a parameter foo of type Bar, but you passed a foo of type Baz). im not planning to do 3 and 4 (yet, and probably never)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:638,usability,error,error,638,"Theres a few uses:. 1. Humans. Once you understand the syntax ([very easy](https://docs.python.org/3/library/typing.html), i just get `Generator` wrong all the time) it improves your understanding what a function really accepts and returns. 2. IDEs. Theyll get better when inferring the types of variables and will show you more actual problems in the code and less false positives. 3. Testing. Some projects use mypy to check if all code in your repo typechecks properly, which can be integrated into a test suite. 4. Runtime type checking. Has a performance hit (as said) but given proper type hints, it makes your code safer and the error messages better (Function blah excepted a parameter foo of type Bar, but you passed a foo of type Baz). im not planning to do 3 and 4 (yet, and probably never)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:315,availability,error,errors,315,"@flying-sheep Thanks for the thorough response! This is a topic I have a lot of thoughts on, though I'm not so sure how coherently I can communicate all of them. On your first thought:. The worst case scenario I see here me typing something so poorly a newbie trying to follow the documentation gets horrible numba errors they can't figure out. This could happen if I hadn't thought about `Set` being a subtype of `Collection`. It can be difficult to know the `abc`s in `typing` are supposed to mean without spending a while using them. For example, this is not what I was expecting:. ```python. >>> issubclass(np.ndarray, typing.Collection). True. >>> issubclass(np.ndarray, typing.Sequence). False. ```. On defining `abc`s for this package, I think it would have to be done in a way where it was easy to discover exactly what is meant by the annotated types. Personally, I read most of my documentation through the ipython repl, where it can be difficult to figure out where a type referenced in a doc string is defined. Otherwise, I'd be interested in seeing how other people are doing it, but like @falexwolf, most of the packages I use don't have type annotations. Again, I think these would be less of an issue if quality writing on type annotation usage, particularly for scientific python, was available. As a Julia user, I found [this blog post](https://white.ucc.asn.au/2018/10/03/Dispatch,-Traits-and-Metaprogramming-Over-Reflection.html) very helpful not just for understanding how to implement trait types in Julia, but also when they're useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1302,availability,avail,available,1302,"@flying-sheep Thanks for the thorough response! This is a topic I have a lot of thoughts on, though I'm not so sure how coherently I can communicate all of them. On your first thought:. The worst case scenario I see here me typing something so poorly a newbie trying to follow the documentation gets horrible numba errors they can't figure out. This could happen if I hadn't thought about `Set` being a subtype of `Collection`. It can be difficult to know the `abc`s in `typing` are supposed to mean without spending a while using them. For example, this is not what I was expecting:. ```python. >>> issubclass(np.ndarray, typing.Collection). True. >>> issubclass(np.ndarray, typing.Sequence). False. ```. On defining `abc`s for this package, I think it would have to be done in a way where it was easy to discover exactly what is meant by the annotated types. Personally, I read most of my documentation through the ipython repl, where it can be difficult to figure out where a type referenced in a doc string is defined. Otherwise, I'd be interested in seeing how other people are doing it, but like @falexwolf, most of the packages I use don't have type annotations. Again, I think these would be less of an issue if quality writing on type annotation usage, particularly for scientific python, was available. As a Julia user, I found [this blog post](https://white.ucc.asn.au/2018/10/03/Dispatch,-Traits-and-Metaprogramming-Over-Reflection.html) very helpful not just for understanding how to implement trait types in Julia, but also when they're useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:58,integrability,topic,topic,58,"@flying-sheep Thanks for the thorough response! This is a topic I have a lot of thoughts on, though I'm not so sure how coherently I can communicate all of them. On your first thought:. The worst case scenario I see here me typing something so poorly a newbie trying to follow the documentation gets horrible numba errors they can't figure out. This could happen if I hadn't thought about `Set` being a subtype of `Collection`. It can be difficult to know the `abc`s in `typing` are supposed to mean without spending a while using them. For example, this is not what I was expecting:. ```python. >>> issubclass(np.ndarray, typing.Collection). True. >>> issubclass(np.ndarray, typing.Sequence). False. ```. On defining `abc`s for this package, I think it would have to be done in a way where it was easy to discover exactly what is meant by the annotated types. Personally, I read most of my documentation through the ipython repl, where it can be difficult to figure out where a type referenced in a doc string is defined. Otherwise, I'd be interested in seeing how other people are doing it, but like @falexwolf, most of the packages I use don't have type annotations. Again, I think these would be less of an issue if quality writing on type annotation usage, particularly for scientific python, was available. As a Julia user, I found [this blog post](https://white.ucc.asn.au/2018/10/03/Dispatch,-Traits-and-Metaprogramming-Over-Reflection.html) very helpful not just for understanding how to implement trait types in Julia, but also when they're useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:403,integrability,sub,subtype,403,"@flying-sheep Thanks for the thorough response! This is a topic I have a lot of thoughts on, though I'm not so sure how coherently I can communicate all of them. On your first thought:. The worst case scenario I see here me typing something so poorly a newbie trying to follow the documentation gets horrible numba errors they can't figure out. This could happen if I hadn't thought about `Set` being a subtype of `Collection`. It can be difficult to know the `abc`s in `typing` are supposed to mean without spending a while using them. For example, this is not what I was expecting:. ```python. >>> issubclass(np.ndarray, typing.Collection). True. >>> issubclass(np.ndarray, typing.Sequence). False. ```. On defining `abc`s for this package, I think it would have to be done in a way where it was easy to discover exactly what is meant by the annotated types. Personally, I read most of my documentation through the ipython repl, where it can be difficult to figure out where a type referenced in a doc string is defined. Otherwise, I'd be interested in seeing how other people are doing it, but like @falexwolf, most of the packages I use don't have type annotations. Again, I think these would be less of an issue if quality writing on type annotation usage, particularly for scientific python, was available. As a Julia user, I found [this blog post](https://white.ucc.asn.au/2018/10/03/Dispatch,-Traits-and-Metaprogramming-Over-Reflection.html) very helpful not just for understanding how to implement trait types in Julia, but also when they're useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:806,integrability,discover,discover,806,"@flying-sheep Thanks for the thorough response! This is a topic I have a lot of thoughts on, though I'm not so sure how coherently I can communicate all of them. On your first thought:. The worst case scenario I see here me typing something so poorly a newbie trying to follow the documentation gets horrible numba errors they can't figure out. This could happen if I hadn't thought about `Set` being a subtype of `Collection`. It can be difficult to know the `abc`s in `typing` are supposed to mean without spending a while using them. For example, this is not what I was expecting:. ```python. >>> issubclass(np.ndarray, typing.Collection). True. >>> issubclass(np.ndarray, typing.Sequence). False. ```. On defining `abc`s for this package, I think it would have to be done in a way where it was easy to discover exactly what is meant by the annotated types. Personally, I read most of my documentation through the ipython repl, where it can be difficult to figure out where a type referenced in a doc string is defined. Otherwise, I'd be interested in seeing how other people are doing it, but like @falexwolf, most of the packages I use don't have type annotations. Again, I think these would be less of an issue if quality writing on type annotation usage, particularly for scientific python, was available. As a Julia user, I found [this blog post](https://white.ucc.asn.au/2018/10/03/Dispatch,-Traits-and-Metaprogramming-Over-Reflection.html) very helpful not just for understanding how to implement trait types in Julia, but also when they're useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:806,interoperability,discover,discover,806,"@flying-sheep Thanks for the thorough response! This is a topic I have a lot of thoughts on, though I'm not so sure how coherently I can communicate all of them. On your first thought:. The worst case scenario I see here me typing something so poorly a newbie trying to follow the documentation gets horrible numba errors they can't figure out. This could happen if I hadn't thought about `Set` being a subtype of `Collection`. It can be difficult to know the `abc`s in `typing` are supposed to mean without spending a while using them. For example, this is not what I was expecting:. ```python. >>> issubclass(np.ndarray, typing.Collection). True. >>> issubclass(np.ndarray, typing.Sequence). False. ```. On defining `abc`s for this package, I think it would have to be done in a way where it was easy to discover exactly what is meant by the annotated types. Personally, I read most of my documentation through the ipython repl, where it can be difficult to figure out where a type referenced in a doc string is defined. Otherwise, I'd be interested in seeing how other people are doing it, but like @falexwolf, most of the packages I use don't have type annotations. Again, I think these would be less of an issue if quality writing on type annotation usage, particularly for scientific python, was available. As a Julia user, I found [this blog post](https://white.ucc.asn.au/2018/10/03/Dispatch,-Traits-and-Metaprogramming-Over-Reflection.html) very helpful not just for understanding how to implement trait types in Julia, but also when they're useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:201,modifiability,scenario,scenario,201,"@flying-sheep Thanks for the thorough response! This is a topic I have a lot of thoughts on, though I'm not so sure how coherently I can communicate all of them. On your first thought:. The worst case scenario I see here me typing something so poorly a newbie trying to follow the documentation gets horrible numba errors they can't figure out. This could happen if I hadn't thought about `Set` being a subtype of `Collection`. It can be difficult to know the `abc`s in `typing` are supposed to mean without spending a while using them. For example, this is not what I was expecting:. ```python. >>> issubclass(np.ndarray, typing.Collection). True. >>> issubclass(np.ndarray, typing.Sequence). False. ```. On defining `abc`s for this package, I think it would have to be done in a way where it was easy to discover exactly what is meant by the annotated types. Personally, I read most of my documentation through the ipython repl, where it can be difficult to figure out where a type referenced in a doc string is defined. Otherwise, I'd be interested in seeing how other people are doing it, but like @falexwolf, most of the packages I use don't have type annotations. Again, I think these would be less of an issue if quality writing on type annotation usage, particularly for scientific python, was available. As a Julia user, I found [this blog post](https://white.ucc.asn.au/2018/10/03/Dispatch,-Traits-and-Metaprogramming-Over-Reflection.html) very helpful not just for understanding how to implement trait types in Julia, but also when they're useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:734,modifiability,pac,package,734,"@flying-sheep Thanks for the thorough response! This is a topic I have a lot of thoughts on, though I'm not so sure how coherently I can communicate all of them. On your first thought:. The worst case scenario I see here me typing something so poorly a newbie trying to follow the documentation gets horrible numba errors they can't figure out. This could happen if I hadn't thought about `Set` being a subtype of `Collection`. It can be difficult to know the `abc`s in `typing` are supposed to mean without spending a while using them. For example, this is not what I was expecting:. ```python. >>> issubclass(np.ndarray, typing.Collection). True. >>> issubclass(np.ndarray, typing.Sequence). False. ```. On defining `abc`s for this package, I think it would have to be done in a way where it was easy to discover exactly what is meant by the annotated types. Personally, I read most of my documentation through the ipython repl, where it can be difficult to figure out where a type referenced in a doc string is defined. Otherwise, I'd be interested in seeing how other people are doing it, but like @falexwolf, most of the packages I use don't have type annotations. Again, I think these would be less of an issue if quality writing on type annotation usage, particularly for scientific python, was available. As a Julia user, I found [this blog post](https://white.ucc.asn.au/2018/10/03/Dispatch,-Traits-and-Metaprogramming-Over-Reflection.html) very helpful not just for understanding how to implement trait types in Julia, but also when they're useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1126,modifiability,pac,packages,1126,"@flying-sheep Thanks for the thorough response! This is a topic I have a lot of thoughts on, though I'm not so sure how coherently I can communicate all of them. On your first thought:. The worst case scenario I see here me typing something so poorly a newbie trying to follow the documentation gets horrible numba errors they can't figure out. This could happen if I hadn't thought about `Set` being a subtype of `Collection`. It can be difficult to know the `abc`s in `typing` are supposed to mean without spending a while using them. For example, this is not what I was expecting:. ```python. >>> issubclass(np.ndarray, typing.Collection). True. >>> issubclass(np.ndarray, typing.Sequence). False. ```. On defining `abc`s for this package, I think it would have to be done in a way where it was easy to discover exactly what is meant by the annotated types. Personally, I read most of my documentation through the ipython repl, where it can be difficult to figure out where a type referenced in a doc string is defined. Otherwise, I'd be interested in seeing how other people are doing it, but like @falexwolf, most of the packages I use don't have type annotations. Again, I think these would be less of an issue if quality writing on type annotation usage, particularly for scientific python, was available. As a Julia user, I found [this blog post](https://white.ucc.asn.au/2018/10/03/Dispatch,-Traits-and-Metaprogramming-Over-Reflection.html) very helpful not just for understanding how to implement trait types in Julia, but also when they're useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:315,performance,error,errors,315,"@flying-sheep Thanks for the thorough response! This is a topic I have a lot of thoughts on, though I'm not so sure how coherently I can communicate all of them. On your first thought:. The worst case scenario I see here me typing something so poorly a newbie trying to follow the documentation gets horrible numba errors they can't figure out. This could happen if I hadn't thought about `Set` being a subtype of `Collection`. It can be difficult to know the `abc`s in `typing` are supposed to mean without spending a while using them. For example, this is not what I was expecting:. ```python. >>> issubclass(np.ndarray, typing.Collection). True. >>> issubclass(np.ndarray, typing.Sequence). False. ```. On defining `abc`s for this package, I think it would have to be done in a way where it was easy to discover exactly what is meant by the annotated types. Personally, I read most of my documentation through the ipython repl, where it can be difficult to figure out where a type referenced in a doc string is defined. Otherwise, I'd be interested in seeing how other people are doing it, but like @falexwolf, most of the packages I use don't have type annotations. Again, I think these would be less of an issue if quality writing on type annotation usage, particularly for scientific python, was available. As a Julia user, I found [this blog post](https://white.ucc.asn.au/2018/10/03/Dispatch,-Traits-and-Metaprogramming-Over-Reflection.html) very helpful not just for understanding how to implement trait types in Julia, but also when they're useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1302,reliability,availab,available,1302,"@flying-sheep Thanks for the thorough response! This is a topic I have a lot of thoughts on, though I'm not so sure how coherently I can communicate all of them. On your first thought:. The worst case scenario I see here me typing something so poorly a newbie trying to follow the documentation gets horrible numba errors they can't figure out. This could happen if I hadn't thought about `Set` being a subtype of `Collection`. It can be difficult to know the `abc`s in `typing` are supposed to mean without spending a while using them. For example, this is not what I was expecting:. ```python. >>> issubclass(np.ndarray, typing.Collection). True. >>> issubclass(np.ndarray, typing.Sequence). False. ```. On defining `abc`s for this package, I think it would have to be done in a way where it was easy to discover exactly what is meant by the annotated types. Personally, I read most of my documentation through the ipython repl, where it can be difficult to figure out where a type referenced in a doc string is defined. Otherwise, I'd be interested in seeing how other people are doing it, but like @falexwolf, most of the packages I use don't have type annotations. Again, I think these would be less of an issue if quality writing on type annotation usage, particularly for scientific python, was available. As a Julia user, I found [this blog post](https://white.ucc.asn.au/2018/10/03/Dispatch,-Traits-and-Metaprogramming-Over-Reflection.html) very helpful not just for understanding how to implement trait types in Julia, but also when they're useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:315,safety,error,errors,315,"@flying-sheep Thanks for the thorough response! This is a topic I have a lot of thoughts on, though I'm not so sure how coherently I can communicate all of them. On your first thought:. The worst case scenario I see here me typing something so poorly a newbie trying to follow the documentation gets horrible numba errors they can't figure out. This could happen if I hadn't thought about `Set` being a subtype of `Collection`. It can be difficult to know the `abc`s in `typing` are supposed to mean without spending a while using them. For example, this is not what I was expecting:. ```python. >>> issubclass(np.ndarray, typing.Collection). True. >>> issubclass(np.ndarray, typing.Sequence). False. ```. On defining `abc`s for this package, I think it would have to be done in a way where it was easy to discover exactly what is meant by the annotated types. Personally, I read most of my documentation through the ipython repl, where it can be difficult to figure out where a type referenced in a doc string is defined. Otherwise, I'd be interested in seeing how other people are doing it, but like @falexwolf, most of the packages I use don't have type annotations. Again, I think these would be less of an issue if quality writing on type annotation usage, particularly for scientific python, was available. As a Julia user, I found [this blog post](https://white.ucc.asn.au/2018/10/03/Dispatch,-Traits-and-Metaprogramming-Over-Reflection.html) very helpful not just for understanding how to implement trait types in Julia, but also when they're useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1302,safety,avail,available,1302,"@flying-sheep Thanks for the thorough response! This is a topic I have a lot of thoughts on, though I'm not so sure how coherently I can communicate all of them. On your first thought:. The worst case scenario I see here me typing something so poorly a newbie trying to follow the documentation gets horrible numba errors they can't figure out. This could happen if I hadn't thought about `Set` being a subtype of `Collection`. It can be difficult to know the `abc`s in `typing` are supposed to mean without spending a while using them. For example, this is not what I was expecting:. ```python. >>> issubclass(np.ndarray, typing.Collection). True. >>> issubclass(np.ndarray, typing.Sequence). False. ```. On defining `abc`s for this package, I think it would have to be done in a way where it was easy to discover exactly what is meant by the annotated types. Personally, I read most of my documentation through the ipython repl, where it can be difficult to figure out where a type referenced in a doc string is defined. Otherwise, I'd be interested in seeing how other people are doing it, but like @falexwolf, most of the packages I use don't have type annotations. Again, I think these would be less of an issue if quality writing on type annotation usage, particularly for scientific python, was available. As a Julia user, I found [this blog post](https://white.ucc.asn.au/2018/10/03/Dispatch,-Traits-and-Metaprogramming-Over-Reflection.html) very helpful not just for understanding how to implement trait types in Julia, but also when they're useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1302,security,availab,available,1302,"@flying-sheep Thanks for the thorough response! This is a topic I have a lot of thoughts on, though I'm not so sure how coherently I can communicate all of them. On your first thought:. The worst case scenario I see here me typing something so poorly a newbie trying to follow the documentation gets horrible numba errors they can't figure out. This could happen if I hadn't thought about `Set` being a subtype of `Collection`. It can be difficult to know the `abc`s in `typing` are supposed to mean without spending a while using them. For example, this is not what I was expecting:. ```python. >>> issubclass(np.ndarray, typing.Collection). True. >>> issubclass(np.ndarray, typing.Sequence). False. ```. On defining `abc`s for this package, I think it would have to be done in a way where it was easy to discover exactly what is meant by the annotated types. Personally, I read most of my documentation through the ipython repl, where it can be difficult to figure out where a type referenced in a doc string is defined. Otherwise, I'd be interested in seeing how other people are doing it, but like @falexwolf, most of the packages I use don't have type annotations. Again, I think these would be less of an issue if quality writing on type annotation usage, particularly for scientific python, was available. As a Julia user, I found [this blog post](https://white.ucc.asn.au/2018/10/03/Dispatch,-Traits-and-Metaprogramming-Over-Reflection.html) very helpful not just for understanding how to implement trait types in Julia, but also when they're useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1476,testability,understand,understanding,1476,"@flying-sheep Thanks for the thorough response! This is a topic I have a lot of thoughts on, though I'm not so sure how coherently I can communicate all of them. On your first thought:. The worst case scenario I see here me typing something so poorly a newbie trying to follow the documentation gets horrible numba errors they can't figure out. This could happen if I hadn't thought about `Set` being a subtype of `Collection`. It can be difficult to know the `abc`s in `typing` are supposed to mean without spending a while using them. For example, this is not what I was expecting:. ```python. >>> issubclass(np.ndarray, typing.Collection). True. >>> issubclass(np.ndarray, typing.Sequence). False. ```. On defining `abc`s for this package, I think it would have to be done in a way where it was easy to discover exactly what is meant by the annotated types. Personally, I read most of my documentation through the ipython repl, where it can be difficult to figure out where a type referenced in a doc string is defined. Otherwise, I'd be interested in seeing how other people are doing it, but like @falexwolf, most of the packages I use don't have type annotations. Again, I think these would be less of an issue if quality writing on type annotation usage, particularly for scientific python, was available. As a Julia user, I found [this blog post](https://white.ucc.asn.au/2018/10/03/Dispatch,-Traits-and-Metaprogramming-Over-Reflection.html) very helpful not just for understanding how to implement trait types in Julia, but also when they're useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:281,usability,document,documentation,281,"@flying-sheep Thanks for the thorough response! This is a topic I have a lot of thoughts on, though I'm not so sure how coherently I can communicate all of them. On your first thought:. The worst case scenario I see here me typing something so poorly a newbie trying to follow the documentation gets horrible numba errors they can't figure out. This could happen if I hadn't thought about `Set` being a subtype of `Collection`. It can be difficult to know the `abc`s in `typing` are supposed to mean without spending a while using them. For example, this is not what I was expecting:. ```python. >>> issubclass(np.ndarray, typing.Collection). True. >>> issubclass(np.ndarray, typing.Sequence). False. ```. On defining `abc`s for this package, I think it would have to be done in a way where it was easy to discover exactly what is meant by the annotated types. Personally, I read most of my documentation through the ipython repl, where it can be difficult to figure out where a type referenced in a doc string is defined. Otherwise, I'd be interested in seeing how other people are doing it, but like @falexwolf, most of the packages I use don't have type annotations. Again, I think these would be less of an issue if quality writing on type annotation usage, particularly for scientific python, was available. As a Julia user, I found [this blog post](https://white.ucc.asn.au/2018/10/03/Dispatch,-Traits-and-Metaprogramming-Over-Reflection.html) very helpful not just for understanding how to implement trait types in Julia, but also when they're useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:315,usability,error,errors,315,"@flying-sheep Thanks for the thorough response! This is a topic I have a lot of thoughts on, though I'm not so sure how coherently I can communicate all of them. On your first thought:. The worst case scenario I see here me typing something so poorly a newbie trying to follow the documentation gets horrible numba errors they can't figure out. This could happen if I hadn't thought about `Set` being a subtype of `Collection`. It can be difficult to know the `abc`s in `typing` are supposed to mean without spending a while using them. For example, this is not what I was expecting:. ```python. >>> issubclass(np.ndarray, typing.Collection). True. >>> issubclass(np.ndarray, typing.Sequence). False. ```. On defining `abc`s for this package, I think it would have to be done in a way where it was easy to discover exactly what is meant by the annotated types. Personally, I read most of my documentation through the ipython repl, where it can be difficult to figure out where a type referenced in a doc string is defined. Otherwise, I'd be interested in seeing how other people are doing it, but like @falexwolf, most of the packages I use don't have type annotations. Again, I think these would be less of an issue if quality writing on type annotation usage, particularly for scientific python, was available. As a Julia user, I found [this blog post](https://white.ucc.asn.au/2018/10/03/Dispatch,-Traits-and-Metaprogramming-Over-Reflection.html) very helpful not just for understanding how to implement trait types in Julia, but also when they're useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:806,usability,discov,discover,806,"@flying-sheep Thanks for the thorough response! This is a topic I have a lot of thoughts on, though I'm not so sure how coherently I can communicate all of them. On your first thought:. The worst case scenario I see here me typing something so poorly a newbie trying to follow the documentation gets horrible numba errors they can't figure out. This could happen if I hadn't thought about `Set` being a subtype of `Collection`. It can be difficult to know the `abc`s in `typing` are supposed to mean without spending a while using them. For example, this is not what I was expecting:. ```python. >>> issubclass(np.ndarray, typing.Collection). True. >>> issubclass(np.ndarray, typing.Sequence). False. ```. On defining `abc`s for this package, I think it would have to be done in a way where it was easy to discover exactly what is meant by the annotated types. Personally, I read most of my documentation through the ipython repl, where it can be difficult to figure out where a type referenced in a doc string is defined. Otherwise, I'd be interested in seeing how other people are doing it, but like @falexwolf, most of the packages I use don't have type annotations. Again, I think these would be less of an issue if quality writing on type annotation usage, particularly for scientific python, was available. As a Julia user, I found [this blog post](https://white.ucc.asn.au/2018/10/03/Dispatch,-Traits-and-Metaprogramming-Over-Reflection.html) very helpful not just for understanding how to implement trait types in Julia, but also when they're useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:861,usability,Person,Personally,861,"@flying-sheep Thanks for the thorough response! This is a topic I have a lot of thoughts on, though I'm not so sure how coherently I can communicate all of them. On your first thought:. The worst case scenario I see here me typing something so poorly a newbie trying to follow the documentation gets horrible numba errors they can't figure out. This could happen if I hadn't thought about `Set` being a subtype of `Collection`. It can be difficult to know the `abc`s in `typing` are supposed to mean without spending a while using them. For example, this is not what I was expecting:. ```python. >>> issubclass(np.ndarray, typing.Collection). True. >>> issubclass(np.ndarray, typing.Sequence). False. ```. On defining `abc`s for this package, I think it would have to be done in a way where it was easy to discover exactly what is meant by the annotated types. Personally, I read most of my documentation through the ipython repl, where it can be difficult to figure out where a type referenced in a doc string is defined. Otherwise, I'd be interested in seeing how other people are doing it, but like @falexwolf, most of the packages I use don't have type annotations. Again, I think these would be less of an issue if quality writing on type annotation usage, particularly for scientific python, was available. As a Julia user, I found [this blog post](https://white.ucc.asn.au/2018/10/03/Dispatch,-Traits-and-Metaprogramming-Over-Reflection.html) very helpful not just for understanding how to implement trait types in Julia, but also when they're useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:891,usability,document,documentation,891,"@flying-sheep Thanks for the thorough response! This is a topic I have a lot of thoughts on, though I'm not so sure how coherently I can communicate all of them. On your first thought:. The worst case scenario I see here me typing something so poorly a newbie trying to follow the documentation gets horrible numba errors they can't figure out. This could happen if I hadn't thought about `Set` being a subtype of `Collection`. It can be difficult to know the `abc`s in `typing` are supposed to mean without spending a while using them. For example, this is not what I was expecting:. ```python. >>> issubclass(np.ndarray, typing.Collection). True. >>> issubclass(np.ndarray, typing.Sequence). False. ```. On defining `abc`s for this package, I think it would have to be done in a way where it was easy to discover exactly what is meant by the annotated types. Personally, I read most of my documentation through the ipython repl, where it can be difficult to figure out where a type referenced in a doc string is defined. Otherwise, I'd be interested in seeing how other people are doing it, but like @falexwolf, most of the packages I use don't have type annotations. Again, I think these would be less of an issue if quality writing on type annotation usage, particularly for scientific python, was available. As a Julia user, I found [this blog post](https://white.ucc.asn.au/2018/10/03/Dispatch,-Traits-and-Metaprogramming-Over-Reflection.html) very helpful not just for understanding how to implement trait types in Julia, but also when they're useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1324,usability,user,user,1324,"@flying-sheep Thanks for the thorough response! This is a topic I have a lot of thoughts on, though I'm not so sure how coherently I can communicate all of them. On your first thought:. The worst case scenario I see here me typing something so poorly a newbie trying to follow the documentation gets horrible numba errors they can't figure out. This could happen if I hadn't thought about `Set` being a subtype of `Collection`. It can be difficult to know the `abc`s in `typing` are supposed to mean without spending a while using them. For example, this is not what I was expecting:. ```python. >>> issubclass(np.ndarray, typing.Collection). True. >>> issubclass(np.ndarray, typing.Sequence). False. ```. On defining `abc`s for this package, I think it would have to be done in a way where it was easy to discover exactly what is meant by the annotated types. Personally, I read most of my documentation through the ipython repl, where it can be difficult to figure out where a type referenced in a doc string is defined. Otherwise, I'd be interested in seeing how other people are doing it, but like @falexwolf, most of the packages I use don't have type annotations. Again, I think these would be less of an issue if quality writing on type annotation usage, particularly for scientific python, was available. As a Julia user, I found [this blog post](https://white.ucc.asn.au/2018/10/03/Dispatch,-Traits-and-Metaprogramming-Over-Reflection.html) very helpful not just for understanding how to implement trait types in Julia, but also when they're useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1455,usability,help,helpful,1455,"@flying-sheep Thanks for the thorough response! This is a topic I have a lot of thoughts on, though I'm not so sure how coherently I can communicate all of them. On your first thought:. The worst case scenario I see here me typing something so poorly a newbie trying to follow the documentation gets horrible numba errors they can't figure out. This could happen if I hadn't thought about `Set` being a subtype of `Collection`. It can be difficult to know the `abc`s in `typing` are supposed to mean without spending a while using them. For example, this is not what I was expecting:. ```python. >>> issubclass(np.ndarray, typing.Collection). True. >>> issubclass(np.ndarray, typing.Sequence). False. ```. On defining `abc`s for this package, I think it would have to be done in a way where it was easy to discover exactly what is meant by the annotated types. Personally, I read most of my documentation through the ipython repl, where it can be difficult to figure out where a type referenced in a doc string is defined. Otherwise, I'd be interested in seeing how other people are doing it, but like @falexwolf, most of the packages I use don't have type annotations. Again, I think these would be less of an issue if quality writing on type annotation usage, particularly for scientific python, was available. As a Julia user, I found [this blog post](https://white.ucc.asn.au/2018/10/03/Dispatch,-Traits-and-Metaprogramming-Over-Reflection.html) very helpful not just for understanding how to implement trait types in Julia, but also when they're useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:164,energy efficiency,sustainab,sustainable,164,"@flying-sheep As always, thank you for your thorough thoughts on the topic! And as always, my ""hacking-numerics"" perspective likely is not a path that is long term sustainable. With what I wrote at the very beginning of this thread, I simply wanted to express that I thought that we shouldn't transition quickly and immediately; for the cosmetic reasons and for the reason of staying away from creating entry hurdles. I still don't think that scanpy needs to precede major packages like numpy and many others in adapting type annotations. But, in essence, I trust you and if you want to push this further I'm fine if scanpy becomes somewhat a field of experimentation for how to deal with type annotations in scientific and numerics-centered software. . @ivirshup Thank you very much for your remarks, too! I agree with your concerns and examples, but wouldn't have been able to summarize them as neatly. *Conclusion:* @flying-sheep if you feel you have bandwidth for improving the cosmetics (thanks for what you did already, also the PR to ipython) that lead to more homogeneous docstrings (I'd say: `Union[a, b]`  `a, b`), of course, please go ahead. If people make PRs with old-school docstrings and without type annotations, I'd still not trouble them, for now. When we have converged on new docstrings and canonical type annotations so that at least people who really know what they're doing (@ivirshup) don't feel things are ambiguous anymore (say in a year), we can start to rigorously ask for them. PS: Thanks for the hints about Jedi etc. @flying-sheep. But likely, I'll keep playing around and reading documentation of packages using shift-tab in jupyter and develop using emacs relatively plain (there were times when I worked with quite some extensions, but these days, I'm back to almost plain for performance reasons - I know that's probably not smart, but anyways)...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:512,energy efficiency,adapt,adapting,512,"@flying-sheep As always, thank you for your thorough thoughts on the topic! And as always, my ""hacking-numerics"" perspective likely is not a path that is long term sustainable. With what I wrote at the very beginning of this thread, I simply wanted to express that I thought that we shouldn't transition quickly and immediately; for the cosmetic reasons and for the reason of staying away from creating entry hurdles. I still don't think that scanpy needs to precede major packages like numpy and many others in adapting type annotations. But, in essence, I trust you and if you want to push this further I'm fine if scanpy becomes somewhat a field of experimentation for how to deal with type annotations in scientific and numerics-centered software. . @ivirshup Thank you very much for your remarks, too! I agree with your concerns and examples, but wouldn't have been able to summarize them as neatly. *Conclusion:* @flying-sheep if you feel you have bandwidth for improving the cosmetics (thanks for what you did already, also the PR to ipython) that lead to more homogeneous docstrings (I'd say: `Union[a, b]`  `a, b`), of course, please go ahead. If people make PRs with old-school docstrings and without type annotations, I'd still not trouble them, for now. When we have converged on new docstrings and canonical type annotations so that at least people who really know what they're doing (@ivirshup) don't feel things are ambiguous anymore (say in a year), we can start to rigorously ask for them. PS: Thanks for the hints about Jedi etc. @flying-sheep. But likely, I'll keep playing around and reading documentation of packages using shift-tab in jupyter and develop using emacs relatively plain (there were times when I worked with quite some extensions, but these days, I'm back to almost plain for performance reasons - I know that's probably not smart, but anyways)...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:69,integrability,topic,topic,69,"@flying-sheep As always, thank you for your thorough thoughts on the topic! And as always, my ""hacking-numerics"" perspective likely is not a path that is long term sustainable. With what I wrote at the very beginning of this thread, I simply wanted to express that I thought that we shouldn't transition quickly and immediately; for the cosmetic reasons and for the reason of staying away from creating entry hurdles. I still don't think that scanpy needs to precede major packages like numpy and many others in adapting type annotations. But, in essence, I trust you and if you want to push this further I'm fine if scanpy becomes somewhat a field of experimentation for how to deal with type annotations in scientific and numerics-centered software. . @ivirshup Thank you very much for your remarks, too! I agree with your concerns and examples, but wouldn't have been able to summarize them as neatly. *Conclusion:* @flying-sheep if you feel you have bandwidth for improving the cosmetics (thanks for what you did already, also the PR to ipython) that lead to more homogeneous docstrings (I'd say: `Union[a, b]`  `a, b`), of course, please go ahead. If people make PRs with old-school docstrings and without type annotations, I'd still not trouble them, for now. When we have converged on new docstrings and canonical type annotations so that at least people who really know what they're doing (@ivirshup) don't feel things are ambiguous anymore (say in a year), we can start to rigorously ask for them. PS: Thanks for the hints about Jedi etc. @flying-sheep. But likely, I'll keep playing around and reading documentation of packages using shift-tab in jupyter and develop using emacs relatively plain (there were times when I worked with quite some extensions, but these days, I'm back to almost plain for performance reasons - I know that's probably not smart, but anyways)...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:512,integrability,adapt,adapting,512,"@flying-sheep As always, thank you for your thorough thoughts on the topic! And as always, my ""hacking-numerics"" perspective likely is not a path that is long term sustainable. With what I wrote at the very beginning of this thread, I simply wanted to express that I thought that we shouldn't transition quickly and immediately; for the cosmetic reasons and for the reason of staying away from creating entry hurdles. I still don't think that scanpy needs to precede major packages like numpy and many others in adapting type annotations. But, in essence, I trust you and if you want to push this further I'm fine if scanpy becomes somewhat a field of experimentation for how to deal with type annotations in scientific and numerics-centered software. . @ivirshup Thank you very much for your remarks, too! I agree with your concerns and examples, but wouldn't have been able to summarize them as neatly. *Conclusion:* @flying-sheep if you feel you have bandwidth for improving the cosmetics (thanks for what you did already, also the PR to ipython) that lead to more homogeneous docstrings (I'd say: `Union[a, b]`  `a, b`), of course, please go ahead. If people make PRs with old-school docstrings and without type annotations, I'd still not trouble them, for now. When we have converged on new docstrings and canonical type annotations so that at least people who really know what they're doing (@ivirshup) don't feel things are ambiguous anymore (say in a year), we can start to rigorously ask for them. PS: Thanks for the hints about Jedi etc. @flying-sheep. But likely, I'll keep playing around and reading documentation of packages using shift-tab in jupyter and develop using emacs relatively plain (there were times when I worked with quite some extensions, but these days, I'm back to almost plain for performance reasons - I know that's probably not smart, but anyways)...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:512,interoperability,adapt,adapting,512,"@flying-sheep As always, thank you for your thorough thoughts on the topic! And as always, my ""hacking-numerics"" perspective likely is not a path that is long term sustainable. With what I wrote at the very beginning of this thread, I simply wanted to express that I thought that we shouldn't transition quickly and immediately; for the cosmetic reasons and for the reason of staying away from creating entry hurdles. I still don't think that scanpy needs to precede major packages like numpy and many others in adapting type annotations. But, in essence, I trust you and if you want to push this further I'm fine if scanpy becomes somewhat a field of experimentation for how to deal with type annotations in scientific and numerics-centered software. . @ivirshup Thank you very much for your remarks, too! I agree with your concerns and examples, but wouldn't have been able to summarize them as neatly. *Conclusion:* @flying-sheep if you feel you have bandwidth for improving the cosmetics (thanks for what you did already, also the PR to ipython) that lead to more homogeneous docstrings (I'd say: `Union[a, b]`  `a, b`), of course, please go ahead. If people make PRs with old-school docstrings and without type annotations, I'd still not trouble them, for now. When we have converged on new docstrings and canonical type annotations so that at least people who really know what they're doing (@ivirshup) don't feel things are ambiguous anymore (say in a year), we can start to rigorously ask for them. PS: Thanks for the hints about Jedi etc. @flying-sheep. But likely, I'll keep playing around and reading documentation of packages using shift-tab in jupyter and develop using emacs relatively plain (there were times when I worked with quite some extensions, but these days, I'm back to almost plain for performance reasons - I know that's probably not smart, but anyways)...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:473,modifiability,pac,packages,473,"@flying-sheep As always, thank you for your thorough thoughts on the topic! And as always, my ""hacking-numerics"" perspective likely is not a path that is long term sustainable. With what I wrote at the very beginning of this thread, I simply wanted to express that I thought that we shouldn't transition quickly and immediately; for the cosmetic reasons and for the reason of staying away from creating entry hurdles. I still don't think that scanpy needs to precede major packages like numpy and many others in adapting type annotations. But, in essence, I trust you and if you want to push this further I'm fine if scanpy becomes somewhat a field of experimentation for how to deal with type annotations in scientific and numerics-centered software. . @ivirshup Thank you very much for your remarks, too! I agree with your concerns and examples, but wouldn't have been able to summarize them as neatly. *Conclusion:* @flying-sheep if you feel you have bandwidth for improving the cosmetics (thanks for what you did already, also the PR to ipython) that lead to more homogeneous docstrings (I'd say: `Union[a, b]`  `a, b`), of course, please go ahead. If people make PRs with old-school docstrings and without type annotations, I'd still not trouble them, for now. When we have converged on new docstrings and canonical type annotations so that at least people who really know what they're doing (@ivirshup) don't feel things are ambiguous anymore (say in a year), we can start to rigorously ask for them. PS: Thanks for the hints about Jedi etc. @flying-sheep. But likely, I'll keep playing around and reading documentation of packages using shift-tab in jupyter and develop using emacs relatively plain (there were times when I worked with quite some extensions, but these days, I'm back to almost plain for performance reasons - I know that's probably not smart, but anyways)...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:512,modifiability,adapt,adapting,512,"@flying-sheep As always, thank you for your thorough thoughts on the topic! And as always, my ""hacking-numerics"" perspective likely is not a path that is long term sustainable. With what I wrote at the very beginning of this thread, I simply wanted to express that I thought that we shouldn't transition quickly and immediately; for the cosmetic reasons and for the reason of staying away from creating entry hurdles. I still don't think that scanpy needs to precede major packages like numpy and many others in adapting type annotations. But, in essence, I trust you and if you want to push this further I'm fine if scanpy becomes somewhat a field of experimentation for how to deal with type annotations in scientific and numerics-centered software. . @ivirshup Thank you very much for your remarks, too! I agree with your concerns and examples, but wouldn't have been able to summarize them as neatly. *Conclusion:* @flying-sheep if you feel you have bandwidth for improving the cosmetics (thanks for what you did already, also the PR to ipython) that lead to more homogeneous docstrings (I'd say: `Union[a, b]`  `a, b`), of course, please go ahead. If people make PRs with old-school docstrings and without type annotations, I'd still not trouble them, for now. When we have converged on new docstrings and canonical type annotations so that at least people who really know what they're doing (@ivirshup) don't feel things are ambiguous anymore (say in a year), we can start to rigorously ask for them. PS: Thanks for the hints about Jedi etc. @flying-sheep. But likely, I'll keep playing around and reading documentation of packages using shift-tab in jupyter and develop using emacs relatively plain (there were times when I worked with quite some extensions, but these days, I'm back to almost plain for performance reasons - I know that's probably not smart, but anyways)...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:825,modifiability,concern,concerns,825,"@flying-sheep As always, thank you for your thorough thoughts on the topic! And as always, my ""hacking-numerics"" perspective likely is not a path that is long term sustainable. With what I wrote at the very beginning of this thread, I simply wanted to express that I thought that we shouldn't transition quickly and immediately; for the cosmetic reasons and for the reason of staying away from creating entry hurdles. I still don't think that scanpy needs to precede major packages like numpy and many others in adapting type annotations. But, in essence, I trust you and if you want to push this further I'm fine if scanpy becomes somewhat a field of experimentation for how to deal with type annotations in scientific and numerics-centered software. . @ivirshup Thank you very much for your remarks, too! I agree with your concerns and examples, but wouldn't have been able to summarize them as neatly. *Conclusion:* @flying-sheep if you feel you have bandwidth for improving the cosmetics (thanks for what you did already, also the PR to ipython) that lead to more homogeneous docstrings (I'd say: `Union[a, b]`  `a, b`), of course, please go ahead. If people make PRs with old-school docstrings and without type annotations, I'd still not trouble them, for now. When we have converged on new docstrings and canonical type annotations so that at least people who really know what they're doing (@ivirshup) don't feel things are ambiguous anymore (say in a year), we can start to rigorously ask for them. PS: Thanks for the hints about Jedi etc. @flying-sheep. But likely, I'll keep playing around and reading documentation of packages using shift-tab in jupyter and develop using emacs relatively plain (there were times when I worked with quite some extensions, but these days, I'm back to almost plain for performance reasons - I know that's probably not smart, but anyways)...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1630,modifiability,pac,packages,1630,"@flying-sheep As always, thank you for your thorough thoughts on the topic! And as always, my ""hacking-numerics"" perspective likely is not a path that is long term sustainable. With what I wrote at the very beginning of this thread, I simply wanted to express that I thought that we shouldn't transition quickly and immediately; for the cosmetic reasons and for the reason of staying away from creating entry hurdles. I still don't think that scanpy needs to precede major packages like numpy and many others in adapting type annotations. But, in essence, I trust you and if you want to push this further I'm fine if scanpy becomes somewhat a field of experimentation for how to deal with type annotations in scientific and numerics-centered software. . @ivirshup Thank you very much for your remarks, too! I agree with your concerns and examples, but wouldn't have been able to summarize them as neatly. *Conclusion:* @flying-sheep if you feel you have bandwidth for improving the cosmetics (thanks for what you did already, also the PR to ipython) that lead to more homogeneous docstrings (I'd say: `Union[a, b]`  `a, b`), of course, please go ahead. If people make PRs with old-school docstrings and without type annotations, I'd still not trouble them, for now. When we have converged on new docstrings and canonical type annotations so that at least people who really know what they're doing (@ivirshup) don't feel things are ambiguous anymore (say in a year), we can start to rigorously ask for them. PS: Thanks for the hints about Jedi etc. @flying-sheep. But likely, I'll keep playing around and reading documentation of packages using shift-tab in jupyter and develop using emacs relatively plain (there were times when I worked with quite some extensions, but these days, I'm back to almost plain for performance reasons - I know that's probably not smart, but anyways)...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1755,modifiability,extens,extensions,1755,"@flying-sheep As always, thank you for your thorough thoughts on the topic! And as always, my ""hacking-numerics"" perspective likely is not a path that is long term sustainable. With what I wrote at the very beginning of this thread, I simply wanted to express that I thought that we shouldn't transition quickly and immediately; for the cosmetic reasons and for the reason of staying away from creating entry hurdles. I still don't think that scanpy needs to precede major packages like numpy and many others in adapting type annotations. But, in essence, I trust you and if you want to push this further I'm fine if scanpy becomes somewhat a field of experimentation for how to deal with type annotations in scientific and numerics-centered software. . @ivirshup Thank you very much for your remarks, too! I agree with your concerns and examples, but wouldn't have been able to summarize them as neatly. *Conclusion:* @flying-sheep if you feel you have bandwidth for improving the cosmetics (thanks for what you did already, also the PR to ipython) that lead to more homogeneous docstrings (I'd say: `Union[a, b]`  `a, b`), of course, please go ahead. If people make PRs with old-school docstrings and without type annotations, I'd still not trouble them, for now. When we have converged on new docstrings and canonical type annotations so that at least people who really know what they're doing (@ivirshup) don't feel things are ambiguous anymore (say in a year), we can start to rigorously ask for them. PS: Thanks for the hints about Jedi etc. @flying-sheep. But likely, I'll keep playing around and reading documentation of packages using shift-tab in jupyter and develop using emacs relatively plain (there were times when I worked with quite some extensions, but these days, I'm back to almost plain for performance reasons - I know that's probably not smart, but anyways)...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1719,performance,time,times,1719,"@flying-sheep As always, thank you for your thorough thoughts on the topic! And as always, my ""hacking-numerics"" perspective likely is not a path that is long term sustainable. With what I wrote at the very beginning of this thread, I simply wanted to express that I thought that we shouldn't transition quickly and immediately; for the cosmetic reasons and for the reason of staying away from creating entry hurdles. I still don't think that scanpy needs to precede major packages like numpy and many others in adapting type annotations. But, in essence, I trust you and if you want to push this further I'm fine if scanpy becomes somewhat a field of experimentation for how to deal with type annotations in scientific and numerics-centered software. . @ivirshup Thank you very much for your remarks, too! I agree with your concerns and examples, but wouldn't have been able to summarize them as neatly. *Conclusion:* @flying-sheep if you feel you have bandwidth for improving the cosmetics (thanks for what you did already, also the PR to ipython) that lead to more homogeneous docstrings (I'd say: `Union[a, b]`  `a, b`), of course, please go ahead. If people make PRs with old-school docstrings and without type annotations, I'd still not trouble them, for now. When we have converged on new docstrings and canonical type annotations so that at least people who really know what they're doing (@ivirshup) don't feel things are ambiguous anymore (say in a year), we can start to rigorously ask for them. PS: Thanks for the hints about Jedi etc. @flying-sheep. But likely, I'll keep playing around and reading documentation of packages using shift-tab in jupyter and develop using emacs relatively plain (there were times when I worked with quite some extensions, but these days, I'm back to almost plain for performance reasons - I know that's probably not smart, but anyways)...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1812,performance,perform,performance,1812,"@flying-sheep As always, thank you for your thorough thoughts on the topic! And as always, my ""hacking-numerics"" perspective likely is not a path that is long term sustainable. With what I wrote at the very beginning of this thread, I simply wanted to express that I thought that we shouldn't transition quickly and immediately; for the cosmetic reasons and for the reason of staying away from creating entry hurdles. I still don't think that scanpy needs to precede major packages like numpy and many others in adapting type annotations. But, in essence, I trust you and if you want to push this further I'm fine if scanpy becomes somewhat a field of experimentation for how to deal with type annotations in scientific and numerics-centered software. . @ivirshup Thank you very much for your remarks, too! I agree with your concerns and examples, but wouldn't have been able to summarize them as neatly. *Conclusion:* @flying-sheep if you feel you have bandwidth for improving the cosmetics (thanks for what you did already, also the PR to ipython) that lead to more homogeneous docstrings (I'd say: `Union[a, b]`  `a, b`), of course, please go ahead. If people make PRs with old-school docstrings and without type annotations, I'd still not trouble them, for now. When we have converged on new docstrings and canonical type annotations so that at least people who really know what they're doing (@ivirshup) don't feel things are ambiguous anymore (say in a year), we can start to rigorously ask for them. PS: Thanks for the hints about Jedi etc. @flying-sheep. But likely, I'll keep playing around and reading documentation of packages using shift-tab in jupyter and develop using emacs relatively plain (there were times when I worked with quite some extensions, but these days, I'm back to almost plain for performance reasons - I know that's probably not smart, but anyways)...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:95,security,hack,hacking-numerics,95,"@flying-sheep As always, thank you for your thorough thoughts on the topic! And as always, my ""hacking-numerics"" perspective likely is not a path that is long term sustainable. With what I wrote at the very beginning of this thread, I simply wanted to express that I thought that we shouldn't transition quickly and immediately; for the cosmetic reasons and for the reason of staying away from creating entry hurdles. I still don't think that scanpy needs to precede major packages like numpy and many others in adapting type annotations. But, in essence, I trust you and if you want to push this further I'm fine if scanpy becomes somewhat a field of experimentation for how to deal with type annotations in scientific and numerics-centered software. . @ivirshup Thank you very much for your remarks, too! I agree with your concerns and examples, but wouldn't have been able to summarize them as neatly. *Conclusion:* @flying-sheep if you feel you have bandwidth for improving the cosmetics (thanks for what you did already, also the PR to ipython) that lead to more homogeneous docstrings (I'd say: `Union[a, b]`  `a, b`), of course, please go ahead. If people make PRs with old-school docstrings and without type annotations, I'd still not trouble them, for now. When we have converged on new docstrings and canonical type annotations so that at least people who really know what they're doing (@ivirshup) don't feel things are ambiguous anymore (say in a year), we can start to rigorously ask for them. PS: Thanks for the hints about Jedi etc. @flying-sheep. But likely, I'll keep playing around and reading documentation of packages using shift-tab in jupyter and develop using emacs relatively plain (there were times when I worked with quite some extensions, but these days, I'm back to almost plain for performance reasons - I know that's probably not smart, but anyways)...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:558,security,trust,trust,558,"@flying-sheep As always, thank you for your thorough thoughts on the topic! And as always, my ""hacking-numerics"" perspective likely is not a path that is long term sustainable. With what I wrote at the very beginning of this thread, I simply wanted to express that I thought that we shouldn't transition quickly and immediately; for the cosmetic reasons and for the reason of staying away from creating entry hurdles. I still don't think that scanpy needs to precede major packages like numpy and many others in adapting type annotations. But, in essence, I trust you and if you want to push this further I'm fine if scanpy becomes somewhat a field of experimentation for how to deal with type annotations in scientific and numerics-centered software. . @ivirshup Thank you very much for your remarks, too! I agree with your concerns and examples, but wouldn't have been able to summarize them as neatly. *Conclusion:* @flying-sheep if you feel you have bandwidth for improving the cosmetics (thanks for what you did already, also the PR to ipython) that lead to more homogeneous docstrings (I'd say: `Union[a, b]`  `a, b`), of course, please go ahead. If people make PRs with old-school docstrings and without type annotations, I'd still not trouble them, for now. When we have converged on new docstrings and canonical type annotations so that at least people who really know what they're doing (@ivirshup) don't feel things are ambiguous anymore (say in a year), we can start to rigorously ask for them. PS: Thanks for the hints about Jedi etc. @flying-sheep. But likely, I'll keep playing around and reading documentation of packages using shift-tab in jupyter and develop using emacs relatively plain (there were times when I worked with quite some extensions, but these days, I'm back to almost plain for performance reasons - I know that's probably not smart, but anyways)...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:235,testability,simpl,simply,235,"@flying-sheep As always, thank you for your thorough thoughts on the topic! And as always, my ""hacking-numerics"" perspective likely is not a path that is long term sustainable. With what I wrote at the very beginning of this thread, I simply wanted to express that I thought that we shouldn't transition quickly and immediately; for the cosmetic reasons and for the reason of staying away from creating entry hurdles. I still don't think that scanpy needs to precede major packages like numpy and many others in adapting type annotations. But, in essence, I trust you and if you want to push this further I'm fine if scanpy becomes somewhat a field of experimentation for how to deal with type annotations in scientific and numerics-centered software. . @ivirshup Thank you very much for your remarks, too! I agree with your concerns and examples, but wouldn't have been able to summarize them as neatly. *Conclusion:* @flying-sheep if you feel you have bandwidth for improving the cosmetics (thanks for what you did already, also the PR to ipython) that lead to more homogeneous docstrings (I'd say: `Union[a, b]`  `a, b`), of course, please go ahead. If people make PRs with old-school docstrings and without type annotations, I'd still not trouble them, for now. When we have converged on new docstrings and canonical type annotations so that at least people who really know what they're doing (@ivirshup) don't feel things are ambiguous anymore (say in a year), we can start to rigorously ask for them. PS: Thanks for the hints about Jedi etc. @flying-sheep. But likely, I'll keep playing around and reading documentation of packages using shift-tab in jupyter and develop using emacs relatively plain (there were times when I worked with quite some extensions, but these days, I'm back to almost plain for performance reasons - I know that's probably not smart, but anyways)...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:825,testability,concern,concerns,825,"@flying-sheep As always, thank you for your thorough thoughts on the topic! And as always, my ""hacking-numerics"" perspective likely is not a path that is long term sustainable. With what I wrote at the very beginning of this thread, I simply wanted to express that I thought that we shouldn't transition quickly and immediately; for the cosmetic reasons and for the reason of staying away from creating entry hurdles. I still don't think that scanpy needs to precede major packages like numpy and many others in adapting type annotations. But, in essence, I trust you and if you want to push this further I'm fine if scanpy becomes somewhat a field of experimentation for how to deal with type annotations in scientific and numerics-centered software. . @ivirshup Thank you very much for your remarks, too! I agree with your concerns and examples, but wouldn't have been able to summarize them as neatly. *Conclusion:* @flying-sheep if you feel you have bandwidth for improving the cosmetics (thanks for what you did already, also the PR to ipython) that lead to more homogeneous docstrings (I'd say: `Union[a, b]`  `a, b`), of course, please go ahead. If people make PRs with old-school docstrings and without type annotations, I'd still not trouble them, for now. When we have converged on new docstrings and canonical type annotations so that at least people who really know what they're doing (@ivirshup) don't feel things are ambiguous anymore (say in a year), we can start to rigorously ask for them. PS: Thanks for the hints about Jedi etc. @flying-sheep. But likely, I'll keep playing around and reading documentation of packages using shift-tab in jupyter and develop using emacs relatively plain (there were times when I worked with quite some extensions, but these days, I'm back to almost plain for performance reasons - I know that's probably not smart, but anyways)...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:235,usability,simpl,simply,235,"@flying-sheep As always, thank you for your thorough thoughts on the topic! And as always, my ""hacking-numerics"" perspective likely is not a path that is long term sustainable. With what I wrote at the very beginning of this thread, I simply wanted to express that I thought that we shouldn't transition quickly and immediately; for the cosmetic reasons and for the reason of staying away from creating entry hurdles. I still don't think that scanpy needs to precede major packages like numpy and many others in adapting type annotations. But, in essence, I trust you and if you want to push this further I'm fine if scanpy becomes somewhat a field of experimentation for how to deal with type annotations in scientific and numerics-centered software. . @ivirshup Thank you very much for your remarks, too! I agree with your concerns and examples, but wouldn't have been able to summarize them as neatly. *Conclusion:* @flying-sheep if you feel you have bandwidth for improving the cosmetics (thanks for what you did already, also the PR to ipython) that lead to more homogeneous docstrings (I'd say: `Union[a, b]`  `a, b`), of course, please go ahead. If people make PRs with old-school docstrings and without type annotations, I'd still not trouble them, for now. When we have converged on new docstrings and canonical type annotations so that at least people who really know what they're doing (@ivirshup) don't feel things are ambiguous anymore (say in a year), we can start to rigorously ask for them. PS: Thanks for the hints about Jedi etc. @flying-sheep. But likely, I'll keep playing around and reading documentation of packages using shift-tab in jupyter and develop using emacs relatively plain (there were times when I worked with quite some extensions, but these days, I'm back to almost plain for performance reasons - I know that's probably not smart, but anyways)...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1527,usability,hint,hints,1527,"@flying-sheep As always, thank you for your thorough thoughts on the topic! And as always, my ""hacking-numerics"" perspective likely is not a path that is long term sustainable. With what I wrote at the very beginning of this thread, I simply wanted to express that I thought that we shouldn't transition quickly and immediately; for the cosmetic reasons and for the reason of staying away from creating entry hurdles. I still don't think that scanpy needs to precede major packages like numpy and many others in adapting type annotations. But, in essence, I trust you and if you want to push this further I'm fine if scanpy becomes somewhat a field of experimentation for how to deal with type annotations in scientific and numerics-centered software. . @ivirshup Thank you very much for your remarks, too! I agree with your concerns and examples, but wouldn't have been able to summarize them as neatly. *Conclusion:* @flying-sheep if you feel you have bandwidth for improving the cosmetics (thanks for what you did already, also the PR to ipython) that lead to more homogeneous docstrings (I'd say: `Union[a, b]`  `a, b`), of course, please go ahead. If people make PRs with old-school docstrings and without type annotations, I'd still not trouble them, for now. When we have converged on new docstrings and canonical type annotations so that at least people who really know what they're doing (@ivirshup) don't feel things are ambiguous anymore (say in a year), we can start to rigorously ask for them. PS: Thanks for the hints about Jedi etc. @flying-sheep. But likely, I'll keep playing around and reading documentation of packages using shift-tab in jupyter and develop using emacs relatively plain (there were times when I worked with quite some extensions, but these days, I'm back to almost plain for performance reasons - I know that's probably not smart, but anyways)...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1613,usability,document,documentation,1613,"@flying-sheep As always, thank you for your thorough thoughts on the topic! And as always, my ""hacking-numerics"" perspective likely is not a path that is long term sustainable. With what I wrote at the very beginning of this thread, I simply wanted to express that I thought that we shouldn't transition quickly and immediately; for the cosmetic reasons and for the reason of staying away from creating entry hurdles. I still don't think that scanpy needs to precede major packages like numpy and many others in adapting type annotations. But, in essence, I trust you and if you want to push this further I'm fine if scanpy becomes somewhat a field of experimentation for how to deal with type annotations in scientific and numerics-centered software. . @ivirshup Thank you very much for your remarks, too! I agree with your concerns and examples, but wouldn't have been able to summarize them as neatly. *Conclusion:* @flying-sheep if you feel you have bandwidth for improving the cosmetics (thanks for what you did already, also the PR to ipython) that lead to more homogeneous docstrings (I'd say: `Union[a, b]`  `a, b`), of course, please go ahead. If people make PRs with old-school docstrings and without type annotations, I'd still not trouble them, for now. When we have converged on new docstrings and canonical type annotations so that at least people who really know what they're doing (@ivirshup) don't feel things are ambiguous anymore (say in a year), we can start to rigorously ask for them. PS: Thanks for the hints about Jedi etc. @flying-sheep. But likely, I'll keep playing around and reading documentation of packages using shift-tab in jupyter and develop using emacs relatively plain (there were times when I worked with quite some extensions, but these days, I'm back to almost plain for performance reasons - I know that's probably not smart, but anyways)...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1812,usability,perform,performance,1812,"@flying-sheep As always, thank you for your thorough thoughts on the topic! And as always, my ""hacking-numerics"" perspective likely is not a path that is long term sustainable. With what I wrote at the very beginning of this thread, I simply wanted to express that I thought that we shouldn't transition quickly and immediately; for the cosmetic reasons and for the reason of staying away from creating entry hurdles. I still don't think that scanpy needs to precede major packages like numpy and many others in adapting type annotations. But, in essence, I trust you and if you want to push this further I'm fine if scanpy becomes somewhat a field of experimentation for how to deal with type annotations in scientific and numerics-centered software. . @ivirshup Thank you very much for your remarks, too! I agree with your concerns and examples, but wouldn't have been able to summarize them as neatly. *Conclusion:* @flying-sheep if you feel you have bandwidth for improving the cosmetics (thanks for what you did already, also the PR to ipython) that lead to more homogeneous docstrings (I'd say: `Union[a, b]`  `a, b`), of course, please go ahead. If people make PRs with old-school docstrings and without type annotations, I'd still not trouble them, for now. When we have converged on new docstrings and canonical type annotations so that at least people who really know what they're doing (@ivirshup) don't feel things are ambiguous anymore (say in a year), we can start to rigorously ask for them. PS: Thanks for the hints about Jedi etc. @flying-sheep. But likely, I'll keep playing around and reading documentation of packages using shift-tab in jupyter and develop using emacs relatively plain (there were times when I worked with quite some extensions, but these days, I'm back to almost plain for performance reasons - I know that's probably not smart, but anyways)...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:142,availability,error,errors,142,"@ivirshup. > The worst case scenario I see here me typing something so poorly a newbie trying to follow the documentation gets horrible numba errors they can't figure out. Well, thats an improvement over the current situation of the freeform text type annotations make me guess what I can pass and I get horrible numba errors, right? > `issubclass(np.ndarray, typing.Sequence) == False`. That looks like a bug. The docs to `Sequence` say: Concrete subclasses must override `__new__` or `__init__`, `__getitem__`, and `__len__`, and. ```py. >>> np.ndarray.__new__ . <function ndarray.__new__(*args, **kwargs)>. >>> np.ndarray.__getitem__ . <slot wrapper '__getitem__' of 'numpy.ndarray' objects>. >>> np.ndarray.__len__ . <slot wrapper '__len__' of 'numpy.ndarray' objects>. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:321,availability,error,errors,321,"@ivirshup. > The worst case scenario I see here me typing something so poorly a newbie trying to follow the documentation gets horrible numba errors they can't figure out. Well, thats an improvement over the current situation of the freeform text type annotations make me guess what I can pass and I get horrible numba errors, right? > `issubclass(np.ndarray, typing.Sequence) == False`. That looks like a bug. The docs to `Sequence` say: Concrete subclasses must override `__new__` or `__init__`, `__getitem__`, and `__len__`, and. ```py. >>> np.ndarray.__new__ . <function ndarray.__new__(*args, **kwargs)>. >>> np.ndarray.__getitem__ . <slot wrapper '__getitem__' of 'numpy.ndarray' objects>. >>> np.ndarray.__len__ . <slot wrapper '__len__' of 'numpy.ndarray' objects>. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:645,availability,slo,slot,645,"@ivirshup. > The worst case scenario I see here me typing something so poorly a newbie trying to follow the documentation gets horrible numba errors they can't figure out. Well, thats an improvement over the current situation of the freeform text type annotations make me guess what I can pass and I get horrible numba errors, right? > `issubclass(np.ndarray, typing.Sequence) == False`. That looks like a bug. The docs to `Sequence` say: Concrete subclasses must override `__new__` or `__init__`, `__getitem__`, and `__len__`, and. ```py. >>> np.ndarray.__new__ . <function ndarray.__new__(*args, **kwargs)>. >>> np.ndarray.__getitem__ . <slot wrapper '__getitem__' of 'numpy.ndarray' objects>. >>> np.ndarray.__len__ . <slot wrapper '__len__' of 'numpy.ndarray' objects>. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:727,availability,slo,slot,727,"@ivirshup. > The worst case scenario I see here me typing something so poorly a newbie trying to follow the documentation gets horrible numba errors they can't figure out. Well, thats an improvement over the current situation of the freeform text type annotations make me guess what I can pass and I get horrible numba errors, right? > `issubclass(np.ndarray, typing.Sequence) == False`. That looks like a bug. The docs to `Sequence` say: Concrete subclasses must override `__new__` or `__init__`, `__getitem__`, and `__len__`, and. ```py. >>> np.ndarray.__new__ . <function ndarray.__new__(*args, **kwargs)>. >>> np.ndarray.__getitem__ . <slot wrapper '__getitem__' of 'numpy.ndarray' objects>. >>> np.ndarray.__len__ . <slot wrapper '__len__' of 'numpy.ndarray' objects>. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:209,energy efficiency,current,current,209,"@ivirshup. > The worst case scenario I see here me typing something so poorly a newbie trying to follow the documentation gets horrible numba errors they can't figure out. Well, thats an improvement over the current situation of the freeform text type annotations make me guess what I can pass and I get horrible numba errors, right? > `issubclass(np.ndarray, typing.Sequence) == False`. That looks like a bug. The docs to `Sequence` say: Concrete subclasses must override `__new__` or `__init__`, `__getitem__`, and `__len__`, and. ```py. >>> np.ndarray.__new__ . <function ndarray.__new__(*args, **kwargs)>. >>> np.ndarray.__getitem__ . <slot wrapper '__getitem__' of 'numpy.ndarray' objects>. >>> np.ndarray.__len__ . <slot wrapper '__len__' of 'numpy.ndarray' objects>. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:452,integrability,sub,subclasses,452,"@ivirshup. > The worst case scenario I see here me typing something so poorly a newbie trying to follow the documentation gets horrible numba errors they can't figure out. Well, thats an improvement over the current situation of the freeform text type annotations make me guess what I can pass and I get horrible numba errors, right? > `issubclass(np.ndarray, typing.Sequence) == False`. That looks like a bug. The docs to `Sequence` say: Concrete subclasses must override `__new__` or `__init__`, `__getitem__`, and `__len__`, and. ```py. >>> np.ndarray.__new__ . <function ndarray.__new__(*args, **kwargs)>. >>> np.ndarray.__getitem__ . <slot wrapper '__getitem__' of 'numpy.ndarray' objects>. >>> np.ndarray.__len__ . <slot wrapper '__len__' of 'numpy.ndarray' objects>. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:650,integrability,wrap,wrapper,650,"@ivirshup. > The worst case scenario I see here me typing something so poorly a newbie trying to follow the documentation gets horrible numba errors they can't figure out. Well, thats an improvement over the current situation of the freeform text type annotations make me guess what I can pass and I get horrible numba errors, right? > `issubclass(np.ndarray, typing.Sequence) == False`. That looks like a bug. The docs to `Sequence` say: Concrete subclasses must override `__new__` or `__init__`, `__getitem__`, and `__len__`, and. ```py. >>> np.ndarray.__new__ . <function ndarray.__new__(*args, **kwargs)>. >>> np.ndarray.__getitem__ . <slot wrapper '__getitem__' of 'numpy.ndarray' objects>. >>> np.ndarray.__len__ . <slot wrapper '__len__' of 'numpy.ndarray' objects>. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:732,integrability,wrap,wrapper,732,"@ivirshup. > The worst case scenario I see here me typing something so poorly a newbie trying to follow the documentation gets horrible numba errors they can't figure out. Well, thats an improvement over the current situation of the freeform text type annotations make me guess what I can pass and I get horrible numba errors, right? > `issubclass(np.ndarray, typing.Sequence) == False`. That looks like a bug. The docs to `Sequence` say: Concrete subclasses must override `__new__` or `__init__`, `__getitem__`, and `__len__`, and. ```py. >>> np.ndarray.__new__ . <function ndarray.__new__(*args, **kwargs)>. >>> np.ndarray.__getitem__ . <slot wrapper '__getitem__' of 'numpy.ndarray' objects>. >>> np.ndarray.__len__ . <slot wrapper '__len__' of 'numpy.ndarray' objects>. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:650,interoperability,wrapper,wrapper,650,"@ivirshup. > The worst case scenario I see here me typing something so poorly a newbie trying to follow the documentation gets horrible numba errors they can't figure out. Well, thats an improvement over the current situation of the freeform text type annotations make me guess what I can pass and I get horrible numba errors, right? > `issubclass(np.ndarray, typing.Sequence) == False`. That looks like a bug. The docs to `Sequence` say: Concrete subclasses must override `__new__` or `__init__`, `__getitem__`, and `__len__`, and. ```py. >>> np.ndarray.__new__ . <function ndarray.__new__(*args, **kwargs)>. >>> np.ndarray.__getitem__ . <slot wrapper '__getitem__' of 'numpy.ndarray' objects>. >>> np.ndarray.__len__ . <slot wrapper '__len__' of 'numpy.ndarray' objects>. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:732,interoperability,wrapper,wrapper,732,"@ivirshup. > The worst case scenario I see here me typing something so poorly a newbie trying to follow the documentation gets horrible numba errors they can't figure out. Well, thats an improvement over the current situation of the freeform text type annotations make me guess what I can pass and I get horrible numba errors, right? > `issubclass(np.ndarray, typing.Sequence) == False`. That looks like a bug. The docs to `Sequence` say: Concrete subclasses must override `__new__` or `__init__`, `__getitem__`, and `__len__`, and. ```py. >>> np.ndarray.__new__ . <function ndarray.__new__(*args, **kwargs)>. >>> np.ndarray.__getitem__ . <slot wrapper '__getitem__' of 'numpy.ndarray' objects>. >>> np.ndarray.__len__ . <slot wrapper '__len__' of 'numpy.ndarray' objects>. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:28,modifiability,scenario,scenario,28,"@ivirshup. > The worst case scenario I see here me typing something so poorly a newbie trying to follow the documentation gets horrible numba errors they can't figure out. Well, thats an improvement over the current situation of the freeform text type annotations make me guess what I can pass and I get horrible numba errors, right? > `issubclass(np.ndarray, typing.Sequence) == False`. That looks like a bug. The docs to `Sequence` say: Concrete subclasses must override `__new__` or `__init__`, `__getitem__`, and `__len__`, and. ```py. >>> np.ndarray.__new__ . <function ndarray.__new__(*args, **kwargs)>. >>> np.ndarray.__getitem__ . <slot wrapper '__getitem__' of 'numpy.ndarray' objects>. >>> np.ndarray.__len__ . <slot wrapper '__len__' of 'numpy.ndarray' objects>. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:142,performance,error,errors,142,"@ivirshup. > The worst case scenario I see here me typing something so poorly a newbie trying to follow the documentation gets horrible numba errors they can't figure out. Well, thats an improvement over the current situation of the freeform text type annotations make me guess what I can pass and I get horrible numba errors, right? > `issubclass(np.ndarray, typing.Sequence) == False`. That looks like a bug. The docs to `Sequence` say: Concrete subclasses must override `__new__` or `__init__`, `__getitem__`, and `__len__`, and. ```py. >>> np.ndarray.__new__ . <function ndarray.__new__(*args, **kwargs)>. >>> np.ndarray.__getitem__ . <slot wrapper '__getitem__' of 'numpy.ndarray' objects>. >>> np.ndarray.__len__ . <slot wrapper '__len__' of 'numpy.ndarray' objects>. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:321,performance,error,errors,321,"@ivirshup. > The worst case scenario I see here me typing something so poorly a newbie trying to follow the documentation gets horrible numba errors they can't figure out. Well, thats an improvement over the current situation of the freeform text type annotations make me guess what I can pass and I get horrible numba errors, right? > `issubclass(np.ndarray, typing.Sequence) == False`. That looks like a bug. The docs to `Sequence` say: Concrete subclasses must override `__new__` or `__init__`, `__getitem__`, and `__len__`, and. ```py. >>> np.ndarray.__new__ . <function ndarray.__new__(*args, **kwargs)>. >>> np.ndarray.__getitem__ . <slot wrapper '__getitem__' of 'numpy.ndarray' objects>. >>> np.ndarray.__len__ . <slot wrapper '__len__' of 'numpy.ndarray' objects>. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:645,reliability,slo,slot,645,"@ivirshup. > The worst case scenario I see here me typing something so poorly a newbie trying to follow the documentation gets horrible numba errors they can't figure out. Well, thats an improvement over the current situation of the freeform text type annotations make me guess what I can pass and I get horrible numba errors, right? > `issubclass(np.ndarray, typing.Sequence) == False`. That looks like a bug. The docs to `Sequence` say: Concrete subclasses must override `__new__` or `__init__`, `__getitem__`, and `__len__`, and. ```py. >>> np.ndarray.__new__ . <function ndarray.__new__(*args, **kwargs)>. >>> np.ndarray.__getitem__ . <slot wrapper '__getitem__' of 'numpy.ndarray' objects>. >>> np.ndarray.__len__ . <slot wrapper '__len__' of 'numpy.ndarray' objects>. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:727,reliability,slo,slot,727,"@ivirshup. > The worst case scenario I see here me typing something so poorly a newbie trying to follow the documentation gets horrible numba errors they can't figure out. Well, thats an improvement over the current situation of the freeform text type annotations make me guess what I can pass and I get horrible numba errors, right? > `issubclass(np.ndarray, typing.Sequence) == False`. That looks like a bug. The docs to `Sequence` say: Concrete subclasses must override `__new__` or `__init__`, `__getitem__`, and `__len__`, and. ```py. >>> np.ndarray.__new__ . <function ndarray.__new__(*args, **kwargs)>. >>> np.ndarray.__getitem__ . <slot wrapper '__getitem__' of 'numpy.ndarray' objects>. >>> np.ndarray.__len__ . <slot wrapper '__len__' of 'numpy.ndarray' objects>. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:142,safety,error,errors,142,"@ivirshup. > The worst case scenario I see here me typing something so poorly a newbie trying to follow the documentation gets horrible numba errors they can't figure out. Well, thats an improvement over the current situation of the freeform text type annotations make me guess what I can pass and I get horrible numba errors, right? > `issubclass(np.ndarray, typing.Sequence) == False`. That looks like a bug. The docs to `Sequence` say: Concrete subclasses must override `__new__` or `__init__`, `__getitem__`, and `__len__`, and. ```py. >>> np.ndarray.__new__ . <function ndarray.__new__(*args, **kwargs)>. >>> np.ndarray.__getitem__ . <slot wrapper '__getitem__' of 'numpy.ndarray' objects>. >>> np.ndarray.__len__ . <slot wrapper '__len__' of 'numpy.ndarray' objects>. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:321,safety,error,errors,321,"@ivirshup. > The worst case scenario I see here me typing something so poorly a newbie trying to follow the documentation gets horrible numba errors they can't figure out. Well, thats an improvement over the current situation of the freeform text type annotations make me guess what I can pass and I get horrible numba errors, right? > `issubclass(np.ndarray, typing.Sequence) == False`. That looks like a bug. The docs to `Sequence` say: Concrete subclasses must override `__new__` or `__init__`, `__getitem__`, and `__len__`, and. ```py. >>> np.ndarray.__new__ . <function ndarray.__new__(*args, **kwargs)>. >>> np.ndarray.__getitem__ . <slot wrapper '__getitem__' of 'numpy.ndarray' objects>. >>> np.ndarray.__len__ . <slot wrapper '__len__' of 'numpy.ndarray' objects>. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:108,usability,document,documentation,108,"@ivirshup. > The worst case scenario I see here me typing something so poorly a newbie trying to follow the documentation gets horrible numba errors they can't figure out. Well, thats an improvement over the current situation of the freeform text type annotations make me guess what I can pass and I get horrible numba errors, right? > `issubclass(np.ndarray, typing.Sequence) == False`. That looks like a bug. The docs to `Sequence` say: Concrete subclasses must override `__new__` or `__init__`, `__getitem__`, and `__len__`, and. ```py. >>> np.ndarray.__new__ . <function ndarray.__new__(*args, **kwargs)>. >>> np.ndarray.__getitem__ . <slot wrapper '__getitem__' of 'numpy.ndarray' objects>. >>> np.ndarray.__len__ . <slot wrapper '__len__' of 'numpy.ndarray' objects>. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:142,usability,error,errors,142,"@ivirshup. > The worst case scenario I see here me typing something so poorly a newbie trying to follow the documentation gets horrible numba errors they can't figure out. Well, thats an improvement over the current situation of the freeform text type annotations make me guess what I can pass and I get horrible numba errors, right? > `issubclass(np.ndarray, typing.Sequence) == False`. That looks like a bug. The docs to `Sequence` say: Concrete subclasses must override `__new__` or `__init__`, `__getitem__`, and `__len__`, and. ```py. >>> np.ndarray.__new__ . <function ndarray.__new__(*args, **kwargs)>. >>> np.ndarray.__getitem__ . <slot wrapper '__getitem__' of 'numpy.ndarray' objects>. >>> np.ndarray.__len__ . <slot wrapper '__len__' of 'numpy.ndarray' objects>. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:321,usability,error,errors,321,"@ivirshup. > The worst case scenario I see here me typing something so poorly a newbie trying to follow the documentation gets horrible numba errors they can't figure out. Well, thats an improvement over the current situation of the freeform text type annotations make me guess what I can pass and I get horrible numba errors, right? > `issubclass(np.ndarray, typing.Sequence) == False`. That looks like a bug. The docs to `Sequence` say: Concrete subclasses must override `__new__` or `__init__`, `__getitem__`, and `__len__`, and. ```py. >>> np.ndarray.__new__ . <function ndarray.__new__(*args, **kwargs)>. >>> np.ndarray.__getitem__ . <slot wrapper '__getitem__' of 'numpy.ndarray' objects>. >>> np.ndarray.__len__ . <slot wrapper '__len__' of 'numpy.ndarray' objects>. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:431,deployability,patch,patching,431,"@falexwolf . > we shouldn't transition quickly and immediately; for the cosmetic reasons and for the reason of staying away from creating entry hurdles. Got it! so no move fast and break things but instead to identify problems and fix them before they occur. I think the most painful issues here are. 1. the signature rendering in ipython. Fixed in ipython/ipython#11505, We might incorporate a fix right now ourselves by monkey-patching `inspect.Signature.__str__` if we want. 2. losing contributions because of an entry hurdle. Hard to measure if this happens. If we lose someone, they wont announce it. So maybe friendly [PR/issue templates](https://help.github.com/articles/about-issue-and-pull-request-templates/) or [contributing guidelines](https://help.github.com/articles/setting-guidelines-for-repository-contributors/) might help prevent that! ---. > if you feel you have bandwidth for improving the cosmetics (thanks for what you did already, also the PR to ipython) that lead to more homogeneous docstrings (I'd say: `Union[a, b]  a, b`), of course, please go ahead. Will do, but a comma is ambiguous, as it could mean union, intersection, or (in Python) tuple. I think `Union[a, b, c]`  `a, b, or c` would be clearer. I think we should leave everything else as is: `Option[...]`, is clear enough, and `Callable` is better than introducing our own syntax (Some other languages know things like `(a, b) -> c` as type for functions, but Python doesnt). > When we have converged on new docstrings and canonical type annotations so that at least people who really know what they're doing (@ivirshup) don't feel things are ambiguous anymore (say in a year), we can start to rigorously ask for them. good call! I might just edit them in-PR as I did to fix the colormaps in @fidelrams last PR.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:540,energy efficiency,measur,measure,540,"@falexwolf . > we shouldn't transition quickly and immediately; for the cosmetic reasons and for the reason of staying away from creating entry hurdles. Got it! so no move fast and break things but instead to identify problems and fix them before they occur. I think the most painful issues here are. 1. the signature rendering in ipython. Fixed in ipython/ipython#11505, We might incorporate a fix right now ourselves by monkey-patching `inspect.Signature.__str__` if we want. 2. losing contributions because of an entry hurdle. Hard to measure if this happens. If we lose someone, they wont announce it. So maybe friendly [PR/issue templates](https://help.github.com/articles/about-issue-and-pull-request-templates/) or [contributing guidelines](https://help.github.com/articles/setting-guidelines-for-repository-contributors/) might help prevent that! ---. > if you feel you have bandwidth for improving the cosmetics (thanks for what you did already, also the PR to ipython) that lead to more homogeneous docstrings (I'd say: `Union[a, b]  a, b`), of course, please go ahead. Will do, but a comma is ambiguous, as it could mean union, intersection, or (in Python) tuple. I think `Union[a, b, c]`  `a, b, or c` would be clearer. I think we should leave everything else as is: `Option[...]`, is clear enough, and `Callable` is better than introducing our own syntax (Some other languages know things like `(a, b) -> c` as type for functions, but Python doesnt). > When we have converged on new docstrings and canonical type annotations so that at least people who really know what they're doing (@ivirshup) don't feel things are ambiguous anymore (say in a year), we can start to rigorously ask for them. good call! I might just edit them in-PR as I did to fix the colormaps in @fidelrams last PR.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:807,integrability,repositor,repository-contributors,807,"@falexwolf . > we shouldn't transition quickly and immediately; for the cosmetic reasons and for the reason of staying away from creating entry hurdles. Got it! so no move fast and break things but instead to identify problems and fix them before they occur. I think the most painful issues here are. 1. the signature rendering in ipython. Fixed in ipython/ipython#11505, We might incorporate a fix right now ourselves by monkey-patching `inspect.Signature.__str__` if we want. 2. losing contributions because of an entry hurdle. Hard to measure if this happens. If we lose someone, they wont announce it. So maybe friendly [PR/issue templates](https://help.github.com/articles/about-issue-and-pull-request-templates/) or [contributing guidelines](https://help.github.com/articles/setting-guidelines-for-repository-contributors/) might help prevent that! ---. > if you feel you have bandwidth for improving the cosmetics (thanks for what you did already, also the PR to ipython) that lead to more homogeneous docstrings (I'd say: `Union[a, b]  a, b`), of course, please go ahead. Will do, but a comma is ambiguous, as it could mean union, intersection, or (in Python) tuple. I think `Union[a, b, c]`  `a, b, or c` would be clearer. I think we should leave everything else as is: `Option[...]`, is clear enough, and `Callable` is better than introducing our own syntax (Some other languages know things like `(a, b) -> c` as type for functions, but Python doesnt). > When we have converged on new docstrings and canonical type annotations so that at least people who really know what they're doing (@ivirshup) don't feel things are ambiguous anymore (say in a year), we can start to rigorously ask for them. good call! I might just edit them in-PR as I did to fix the colormaps in @fidelrams last PR.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:807,interoperability,repositor,repository-contributors,807,"@falexwolf . > we shouldn't transition quickly and immediately; for the cosmetic reasons and for the reason of staying away from creating entry hurdles. Got it! so no move fast and break things but instead to identify problems and fix them before they occur. I think the most painful issues here are. 1. the signature rendering in ipython. Fixed in ipython/ipython#11505, We might incorporate a fix right now ourselves by monkey-patching `inspect.Signature.__str__` if we want. 2. losing contributions because of an entry hurdle. Hard to measure if this happens. If we lose someone, they wont announce it. So maybe friendly [PR/issue templates](https://help.github.com/articles/about-issue-and-pull-request-templates/) or [contributing guidelines](https://help.github.com/articles/setting-guidelines-for-repository-contributors/) might help prevent that! ---. > if you feel you have bandwidth for improving the cosmetics (thanks for what you did already, also the PR to ipython) that lead to more homogeneous docstrings (I'd say: `Union[a, b]  a, b`), of course, please go ahead. Will do, but a comma is ambiguous, as it could mean union, intersection, or (in Python) tuple. I think `Union[a, b, c]`  `a, b, or c` would be clearer. I think we should leave everything else as is: `Option[...]`, is clear enough, and `Callable` is better than introducing our own syntax (Some other languages know things like `(a, b) -> c` as type for functions, but Python doesnt). > When we have converged on new docstrings and canonical type annotations so that at least people who really know what they're doing (@ivirshup) don't feel things are ambiguous anymore (say in a year), we can start to rigorously ask for them. good call! I might just edit them in-PR as I did to fix the colormaps in @fidelrams last PR.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1460,reliability,doe,doesn,1460,"@falexwolf . > we shouldn't transition quickly and immediately; for the cosmetic reasons and for the reason of staying away from creating entry hurdles. Got it! so no move fast and break things but instead to identify problems and fix them before they occur. I think the most painful issues here are. 1. the signature rendering in ipython. Fixed in ipython/ipython#11505, We might incorporate a fix right now ourselves by monkey-patching `inspect.Signature.__str__` if we want. 2. losing contributions because of an entry hurdle. Hard to measure if this happens. If we lose someone, they wont announce it. So maybe friendly [PR/issue templates](https://help.github.com/articles/about-issue-and-pull-request-templates/) or [contributing guidelines](https://help.github.com/articles/setting-guidelines-for-repository-contributors/) might help prevent that! ---. > if you feel you have bandwidth for improving the cosmetics (thanks for what you did already, also the PR to ipython) that lead to more homogeneous docstrings (I'd say: `Union[a, b]  a, b`), of course, please go ahead. Will do, but a comma is ambiguous, as it could mean union, intersection, or (in Python) tuple. I think `Union[a, b, c]`  `a, b, or c` would be clearer. I think we should leave everything else as is: `Option[...]`, is clear enough, and `Callable` is better than introducing our own syntax (Some other languages know things like `(a, b) -> c` as type for functions, but Python doesnt). > When we have converged on new docstrings and canonical type annotations so that at least people who really know what they're doing (@ivirshup) don't feel things are ambiguous anymore (say in a year), we can start to rigorously ask for them. good call! I might just edit them in-PR as I did to fix the colormaps in @fidelrams last PR.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:431,safety,patch,patching,431,"@falexwolf . > we shouldn't transition quickly and immediately; for the cosmetic reasons and for the reason of staying away from creating entry hurdles. Got it! so no move fast and break things but instead to identify problems and fix them before they occur. I think the most painful issues here are. 1. the signature rendering in ipython. Fixed in ipython/ipython#11505, We might incorporate a fix right now ourselves by monkey-patching `inspect.Signature.__str__` if we want. 2. losing contributions because of an entry hurdle. Hard to measure if this happens. If we lose someone, they wont announce it. So maybe friendly [PR/issue templates](https://help.github.com/articles/about-issue-and-pull-request-templates/) or [contributing guidelines](https://help.github.com/articles/setting-guidelines-for-repository-contributors/) might help prevent that! ---. > if you feel you have bandwidth for improving the cosmetics (thanks for what you did already, also the PR to ipython) that lead to more homogeneous docstrings (I'd say: `Union[a, b]  a, b`), of course, please go ahead. Will do, but a comma is ambiguous, as it could mean union, intersection, or (in Python) tuple. I think `Union[a, b, c]`  `a, b, or c` would be clearer. I think we should leave everything else as is: `Option[...]`, is clear enough, and `Callable` is better than introducing our own syntax (Some other languages know things like `(a, b) -> c` as type for functions, but Python doesnt). > When we have converged on new docstrings and canonical type annotations so that at least people who really know what they're doing (@ivirshup) don't feel things are ambiguous anymore (say in a year), we can start to rigorously ask for them. good call! I might just edit them in-PR as I did to fix the colormaps in @fidelrams last PR.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:844,safety,prevent,prevent,844,"@falexwolf . > we shouldn't transition quickly and immediately; for the cosmetic reasons and for the reason of staying away from creating entry hurdles. Got it! so no move fast and break things but instead to identify problems and fix them before they occur. I think the most painful issues here are. 1. the signature rendering in ipython. Fixed in ipython/ipython#11505, We might incorporate a fix right now ourselves by monkey-patching `inspect.Signature.__str__` if we want. 2. losing contributions because of an entry hurdle. Hard to measure if this happens. If we lose someone, they wont announce it. So maybe friendly [PR/issue templates](https://help.github.com/articles/about-issue-and-pull-request-templates/) or [contributing guidelines](https://help.github.com/articles/setting-guidelines-for-repository-contributors/) might help prevent that! ---. > if you feel you have bandwidth for improving the cosmetics (thanks for what you did already, also the PR to ipython) that lead to more homogeneous docstrings (I'd say: `Union[a, b]  a, b`), of course, please go ahead. Will do, but a comma is ambiguous, as it could mean union, intersection, or (in Python) tuple. I think `Union[a, b, c]`  `a, b, or c` would be clearer. I think we should leave everything else as is: `Option[...]`, is clear enough, and `Callable` is better than introducing our own syntax (Some other languages know things like `(a, b) -> c` as type for functions, but Python doesnt). > When we have converged on new docstrings and canonical type annotations so that at least people who really know what they're doing (@ivirshup) don't feel things are ambiguous anymore (say in a year), we can start to rigorously ask for them. good call! I might just edit them in-PR as I did to fix the colormaps in @fidelrams last PR.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:211,security,ident,identify,211,"@falexwolf . > we shouldn't transition quickly and immediately; for the cosmetic reasons and for the reason of staying away from creating entry hurdles. Got it! so no move fast and break things but instead to identify problems and fix them before they occur. I think the most painful issues here are. 1. the signature rendering in ipython. Fixed in ipython/ipython#11505, We might incorporate a fix right now ourselves by monkey-patching `inspect.Signature.__str__` if we want. 2. losing contributions because of an entry hurdle. Hard to measure if this happens. If we lose someone, they wont announce it. So maybe friendly [PR/issue templates](https://help.github.com/articles/about-issue-and-pull-request-templates/) or [contributing guidelines](https://help.github.com/articles/setting-guidelines-for-repository-contributors/) might help prevent that! ---. > if you feel you have bandwidth for improving the cosmetics (thanks for what you did already, also the PR to ipython) that lead to more homogeneous docstrings (I'd say: `Union[a, b]  a, b`), of course, please go ahead. Will do, but a comma is ambiguous, as it could mean union, intersection, or (in Python) tuple. I think `Union[a, b, c]`  `a, b, or c` would be clearer. I think we should leave everything else as is: `Option[...]`, is clear enough, and `Callable` is better than introducing our own syntax (Some other languages know things like `(a, b) -> c` as type for functions, but Python doesnt). > When we have converged on new docstrings and canonical type annotations so that at least people who really know what they're doing (@ivirshup) don't feel things are ambiguous anymore (say in a year), we can start to rigorously ask for them. good call! I might just edit them in-PR as I did to fix the colormaps in @fidelrams last PR.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:310,security,sign,signature,310,"@falexwolf . > we shouldn't transition quickly and immediately; for the cosmetic reasons and for the reason of staying away from creating entry hurdles. Got it! so no move fast and break things but instead to identify problems and fix them before they occur. I think the most painful issues here are. 1. the signature rendering in ipython. Fixed in ipython/ipython#11505, We might incorporate a fix right now ourselves by monkey-patching `inspect.Signature.__str__` if we want. 2. losing contributions because of an entry hurdle. Hard to measure if this happens. If we lose someone, they wont announce it. So maybe friendly [PR/issue templates](https://help.github.com/articles/about-issue-and-pull-request-templates/) or [contributing guidelines](https://help.github.com/articles/setting-guidelines-for-repository-contributors/) might help prevent that! ---. > if you feel you have bandwidth for improving the cosmetics (thanks for what you did already, also the PR to ipython) that lead to more homogeneous docstrings (I'd say: `Union[a, b]  a, b`), of course, please go ahead. Will do, but a comma is ambiguous, as it could mean union, intersection, or (in Python) tuple. I think `Union[a, b, c]`  `a, b, or c` would be clearer. I think we should leave everything else as is: `Option[...]`, is clear enough, and `Callable` is better than introducing our own syntax (Some other languages know things like `(a, b) -> c` as type for functions, but Python doesnt). > When we have converged on new docstrings and canonical type annotations so that at least people who really know what they're doing (@ivirshup) don't feel things are ambiguous anymore (say in a year), we can start to rigorously ask for them. good call! I might just edit them in-PR as I did to fix the colormaps in @fidelrams last PR.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:431,security,patch,patching,431,"@falexwolf . > we shouldn't transition quickly and immediately; for the cosmetic reasons and for the reason of staying away from creating entry hurdles. Got it! so no move fast and break things but instead to identify problems and fix them before they occur. I think the most painful issues here are. 1. the signature rendering in ipython. Fixed in ipython/ipython#11505, We might incorporate a fix right now ourselves by monkey-patching `inspect.Signature.__str__` if we want. 2. losing contributions because of an entry hurdle. Hard to measure if this happens. If we lose someone, they wont announce it. So maybe friendly [PR/issue templates](https://help.github.com/articles/about-issue-and-pull-request-templates/) or [contributing guidelines](https://help.github.com/articles/setting-guidelines-for-repository-contributors/) might help prevent that! ---. > if you feel you have bandwidth for improving the cosmetics (thanks for what you did already, also the PR to ipython) that lead to more homogeneous docstrings (I'd say: `Union[a, b]  a, b`), of course, please go ahead. Will do, but a comma is ambiguous, as it could mean union, intersection, or (in Python) tuple. I think `Union[a, b, c]`  `a, b, or c` would be clearer. I think we should leave everything else as is: `Option[...]`, is clear enough, and `Callable` is better than introducing our own syntax (Some other languages know things like `(a, b) -> c` as type for functions, but Python doesnt). > When we have converged on new docstrings and canonical type annotations so that at least people who really know what they're doing (@ivirshup) don't feel things are ambiguous anymore (say in a year), we can start to rigorously ask for them. good call! I might just edit them in-PR as I did to fix the colormaps in @fidelrams last PR.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:449,security,Sign,Signature,449,"@falexwolf . > we shouldn't transition quickly and immediately; for the cosmetic reasons and for the reason of staying away from creating entry hurdles. Got it! so no move fast and break things but instead to identify problems and fix them before they occur. I think the most painful issues here are. 1. the signature rendering in ipython. Fixed in ipython/ipython#11505, We might incorporate a fix right now ourselves by monkey-patching `inspect.Signature.__str__` if we want. 2. losing contributions because of an entry hurdle. Hard to measure if this happens. If we lose someone, they wont announce it. So maybe friendly [PR/issue templates](https://help.github.com/articles/about-issue-and-pull-request-templates/) or [contributing guidelines](https://help.github.com/articles/setting-guidelines-for-repository-contributors/) might help prevent that! ---. > if you feel you have bandwidth for improving the cosmetics (thanks for what you did already, also the PR to ipython) that lead to more homogeneous docstrings (I'd say: `Union[a, b]  a, b`), of course, please go ahead. Will do, but a comma is ambiguous, as it could mean union, intersection, or (in Python) tuple. I think `Union[a, b, c]`  `a, b, or c` would be clearer. I think we should leave everything else as is: `Option[...]`, is clear enough, and `Callable` is better than introducing our own syntax (Some other languages know things like `(a, b) -> c` as type for functions, but Python doesnt). > When we have converged on new docstrings and canonical type annotations so that at least people who really know what they're doing (@ivirshup) don't feel things are ambiguous anymore (say in a year), we can start to rigorously ask for them. good call! I might just edit them in-PR as I did to fix the colormaps in @fidelrams last PR.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:844,security,preven,prevent,844,"@falexwolf . > we shouldn't transition quickly and immediately; for the cosmetic reasons and for the reason of staying away from creating entry hurdles. Got it! so no move fast and break things but instead to identify problems and fix them before they occur. I think the most painful issues here are. 1. the signature rendering in ipython. Fixed in ipython/ipython#11505, We might incorporate a fix right now ourselves by monkey-patching `inspect.Signature.__str__` if we want. 2. losing contributions because of an entry hurdle. Hard to measure if this happens. If we lose someone, they wont announce it. So maybe friendly [PR/issue templates](https://help.github.com/articles/about-issue-and-pull-request-templates/) or [contributing guidelines](https://help.github.com/articles/setting-guidelines-for-repository-contributors/) might help prevent that! ---. > if you feel you have bandwidth for improving the cosmetics (thanks for what you did already, also the PR to ipython) that lead to more homogeneous docstrings (I'd say: `Union[a, b]  a, b`), of course, please go ahead. Will do, but a comma is ambiguous, as it could mean union, intersection, or (in Python) tuple. I think `Union[a, b, c]`  `a, b, or c` would be clearer. I think we should leave everything else as is: `Option[...]`, is clear enough, and `Callable` is better than introducing our own syntax (Some other languages know things like `(a, b) -> c` as type for functions, but Python doesnt). > When we have converged on new docstrings and canonical type annotations so that at least people who really know what they're doing (@ivirshup) don't feel things are ambiguous anymore (say in a year), we can start to rigorously ask for them. good call! I might just edit them in-PR as I did to fix the colormaps in @fidelrams last PR.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:656,usability,help,help,656,"@falexwolf . > we shouldn't transition quickly and immediately; for the cosmetic reasons and for the reason of staying away from creating entry hurdles. Got it! so no move fast and break things but instead to identify problems and fix them before they occur. I think the most painful issues here are. 1. the signature rendering in ipython. Fixed in ipython/ipython#11505, We might incorporate a fix right now ourselves by monkey-patching `inspect.Signature.__str__` if we want. 2. losing contributions because of an entry hurdle. Hard to measure if this happens. If we lose someone, they wont announce it. So maybe friendly [PR/issue templates](https://help.github.com/articles/about-issue-and-pull-request-templates/) or [contributing guidelines](https://help.github.com/articles/setting-guidelines-for-repository-contributors/) might help prevent that! ---. > if you feel you have bandwidth for improving the cosmetics (thanks for what you did already, also the PR to ipython) that lead to more homogeneous docstrings (I'd say: `Union[a, b]  a, b`), of course, please go ahead. Will do, but a comma is ambiguous, as it could mean union, intersection, or (in Python) tuple. I think `Union[a, b, c]`  `a, b, or c` would be clearer. I think we should leave everything else as is: `Option[...]`, is clear enough, and `Callable` is better than introducing our own syntax (Some other languages know things like `(a, b) -> c` as type for functions, but Python doesnt). > When we have converged on new docstrings and canonical type annotations so that at least people who really know what they're doing (@ivirshup) don't feel things are ambiguous anymore (say in a year), we can start to rigorously ask for them. good call! I might just edit them in-PR as I did to fix the colormaps in @fidelrams last PR.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:739,usability,guid,guidelines,739,"@falexwolf . > we shouldn't transition quickly and immediately; for the cosmetic reasons and for the reason of staying away from creating entry hurdles. Got it! so no move fast and break things but instead to identify problems and fix them before they occur. I think the most painful issues here are. 1. the signature rendering in ipython. Fixed in ipython/ipython#11505, We might incorporate a fix right now ourselves by monkey-patching `inspect.Signature.__str__` if we want. 2. losing contributions because of an entry hurdle. Hard to measure if this happens. If we lose someone, they wont announce it. So maybe friendly [PR/issue templates](https://help.github.com/articles/about-issue-and-pull-request-templates/) or [contributing guidelines](https://help.github.com/articles/setting-guidelines-for-repository-contributors/) might help prevent that! ---. > if you feel you have bandwidth for improving the cosmetics (thanks for what you did already, also the PR to ipython) that lead to more homogeneous docstrings (I'd say: `Union[a, b]  a, b`), of course, please go ahead. Will do, but a comma is ambiguous, as it could mean union, intersection, or (in Python) tuple. I think `Union[a, b, c]`  `a, b, or c` would be clearer. I think we should leave everything else as is: `Option[...]`, is clear enough, and `Callable` is better than introducing our own syntax (Some other languages know things like `(a, b) -> c` as type for functions, but Python doesnt). > When we have converged on new docstrings and canonical type annotations so that at least people who really know what they're doing (@ivirshup) don't feel things are ambiguous anymore (say in a year), we can start to rigorously ask for them. good call! I might just edit them in-PR as I did to fix the colormaps in @fidelrams last PR.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:759,usability,help,help,759,"@falexwolf . > we shouldn't transition quickly and immediately; for the cosmetic reasons and for the reason of staying away from creating entry hurdles. Got it! so no move fast and break things but instead to identify problems and fix them before they occur. I think the most painful issues here are. 1. the signature rendering in ipython. Fixed in ipython/ipython#11505, We might incorporate a fix right now ourselves by monkey-patching `inspect.Signature.__str__` if we want. 2. losing contributions because of an entry hurdle. Hard to measure if this happens. If we lose someone, they wont announce it. So maybe friendly [PR/issue templates](https://help.github.com/articles/about-issue-and-pull-request-templates/) or [contributing guidelines](https://help.github.com/articles/setting-guidelines-for-repository-contributors/) might help prevent that! ---. > if you feel you have bandwidth for improving the cosmetics (thanks for what you did already, also the PR to ipython) that lead to more homogeneous docstrings (I'd say: `Union[a, b]  a, b`), of course, please go ahead. Will do, but a comma is ambiguous, as it could mean union, intersection, or (in Python) tuple. I think `Union[a, b, c]`  `a, b, or c` would be clearer. I think we should leave everything else as is: `Option[...]`, is clear enough, and `Callable` is better than introducing our own syntax (Some other languages know things like `(a, b) -> c` as type for functions, but Python doesnt). > When we have converged on new docstrings and canonical type annotations so that at least people who really know what they're doing (@ivirshup) don't feel things are ambiguous anymore (say in a year), we can start to rigorously ask for them. good call! I might just edit them in-PR as I did to fix the colormaps in @fidelrams last PR.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:792,usability,guid,guidelines-for-repository-contributors,792,"@falexwolf . > we shouldn't transition quickly and immediately; for the cosmetic reasons and for the reason of staying away from creating entry hurdles. Got it! so no move fast and break things but instead to identify problems and fix them before they occur. I think the most painful issues here are. 1. the signature rendering in ipython. Fixed in ipython/ipython#11505, We might incorporate a fix right now ourselves by monkey-patching `inspect.Signature.__str__` if we want. 2. losing contributions because of an entry hurdle. Hard to measure if this happens. If we lose someone, they wont announce it. So maybe friendly [PR/issue templates](https://help.github.com/articles/about-issue-and-pull-request-templates/) or [contributing guidelines](https://help.github.com/articles/setting-guidelines-for-repository-contributors/) might help prevent that! ---. > if you feel you have bandwidth for improving the cosmetics (thanks for what you did already, also the PR to ipython) that lead to more homogeneous docstrings (I'd say: `Union[a, b]  a, b`), of course, please go ahead. Will do, but a comma is ambiguous, as it could mean union, intersection, or (in Python) tuple. I think `Union[a, b, c]`  `a, b, or c` would be clearer. I think we should leave everything else as is: `Option[...]`, is clear enough, and `Callable` is better than introducing our own syntax (Some other languages know things like `(a, b) -> c` as type for functions, but Python doesnt). > When we have converged on new docstrings and canonical type annotations so that at least people who really know what they're doing (@ivirshup) don't feel things are ambiguous anymore (say in a year), we can start to rigorously ask for them. good call! I might just edit them in-PR as I did to fix the colormaps in @fidelrams last PR.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:839,usability,help,help,839,"@falexwolf . > we shouldn't transition quickly and immediately; for the cosmetic reasons and for the reason of staying away from creating entry hurdles. Got it! so no move fast and break things but instead to identify problems and fix them before they occur. I think the most painful issues here are. 1. the signature rendering in ipython. Fixed in ipython/ipython#11505, We might incorporate a fix right now ourselves by monkey-patching `inspect.Signature.__str__` if we want. 2. losing contributions because of an entry hurdle. Hard to measure if this happens. If we lose someone, they wont announce it. So maybe friendly [PR/issue templates](https://help.github.com/articles/about-issue-and-pull-request-templates/) or [contributing guidelines](https://help.github.com/articles/setting-guidelines-for-repository-contributors/) might help prevent that! ---. > if you feel you have bandwidth for improving the cosmetics (thanks for what you did already, also the PR to ipython) that lead to more homogeneous docstrings (I'd say: `Union[a, b]  a, b`), of course, please go ahead. Will do, but a comma is ambiguous, as it could mean union, intersection, or (in Python) tuple. I think `Union[a, b, c]`  `a, b, or c` would be clearer. I think we should leave everything else as is: `Option[...]`, is clear enough, and `Callable` is better than introducing our own syntax (Some other languages know things like `(a, b) -> c` as type for functions, but Python doesnt). > When we have converged on new docstrings and canonical type annotations so that at least people who really know what they're doing (@ivirshup) don't feel things are ambiguous anymore (say in a year), we can start to rigorously ask for them. good call! I might just edit them in-PR as I did to fix the colormaps in @fidelrams last PR.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1228,usability,clear,clearer,1228,"@falexwolf . > we shouldn't transition quickly and immediately; for the cosmetic reasons and for the reason of staying away from creating entry hurdles. Got it! so no move fast and break things but instead to identify problems and fix them before they occur. I think the most painful issues here are. 1. the signature rendering in ipython. Fixed in ipython/ipython#11505, We might incorporate a fix right now ourselves by monkey-patching `inspect.Signature.__str__` if we want. 2. losing contributions because of an entry hurdle. Hard to measure if this happens. If we lose someone, they wont announce it. So maybe friendly [PR/issue templates](https://help.github.com/articles/about-issue-and-pull-request-templates/) or [contributing guidelines](https://help.github.com/articles/setting-guidelines-for-repository-contributors/) might help prevent that! ---. > if you feel you have bandwidth for improving the cosmetics (thanks for what you did already, also the PR to ipython) that lead to more homogeneous docstrings (I'd say: `Union[a, b]  a, b`), of course, please go ahead. Will do, but a comma is ambiguous, as it could mean union, intersection, or (in Python) tuple. I think `Union[a, b, c]`  `a, b, or c` would be clearer. I think we should leave everything else as is: `Option[...]`, is clear enough, and `Callable` is better than introducing our own syntax (Some other languages know things like `(a, b) -> c` as type for functions, but Python doesnt). > When we have converged on new docstrings and canonical type annotations so that at least people who really know what they're doing (@ivirshup) don't feel things are ambiguous anymore (say in a year), we can start to rigorously ask for them. good call! I might just edit them in-PR as I did to fix the colormaps in @fidelrams last PR.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1302,usability,clear,clear,1302,"@falexwolf . > we shouldn't transition quickly and immediately; for the cosmetic reasons and for the reason of staying away from creating entry hurdles. Got it! so no move fast and break things but instead to identify problems and fix them before they occur. I think the most painful issues here are. 1. the signature rendering in ipython. Fixed in ipython/ipython#11505, We might incorporate a fix right now ourselves by monkey-patching `inspect.Signature.__str__` if we want. 2. losing contributions because of an entry hurdle. Hard to measure if this happens. If we lose someone, they wont announce it. So maybe friendly [PR/issue templates](https://help.github.com/articles/about-issue-and-pull-request-templates/) or [contributing guidelines](https://help.github.com/articles/setting-guidelines-for-repository-contributors/) might help prevent that! ---. > if you feel you have bandwidth for improving the cosmetics (thanks for what you did already, also the PR to ipython) that lead to more homogeneous docstrings (I'd say: `Union[a, b]  a, b`), of course, please go ahead. Will do, but a comma is ambiguous, as it could mean union, intersection, or (in Python) tuple. I think `Union[a, b, c]`  `a, b, or c` would be clearer. I think we should leave everything else as is: `Option[...]`, is clear enough, and `Callable` is better than introducing our own syntax (Some other languages know things like `(a, b) -> c` as type for functions, but Python doesnt). > When we have converged on new docstrings and canonical type annotations so that at least people who really know what they're doing (@ivirshup) don't feel things are ambiguous anymore (say in a year), we can start to rigorously ask for them. good call! I might just edit them in-PR as I did to fix the colormaps in @fidelrams last PR.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:601,availability,consist,consistency,601,"Great! . One last thing: In docstrings, why would you interpret a comma separated list as intersection or a tuple? This is not code but for humans. I'm even having a hard time to imagine the case that gives rise to an intersection. Also, a tuple in a docstring should always be verbose with `(,)`, so that no confusion is possible; we'll enforce that in the docs. Right now, the convention across all the major packages is to simply print out a comma separated list of types if you are allowed to pass different types to a parameter. This produces the least amount of visual distraction and maintains consistency for how it's done in Scanpy in the manual docstrings and everywhere else. If there is a case where an intersection is relevant, I'd treat that separately. Finally: `a, b, or c` is pretty elegant, too... but if there are no good answers to my two remarks I'd go for `a, b, c`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:411,modifiability,pac,packages,411,"Great! . One last thing: In docstrings, why would you interpret a comma separated list as intersection or a tuple? This is not code but for humans. I'm even having a hard time to imagine the case that gives rise to an intersection. Also, a tuple in a docstring should always be verbose with `(,)`, so that no confusion is possible; we'll enforce that in the docs. Right now, the convention across all the major packages is to simply print out a comma separated list of types if you are allowed to pass different types to a parameter. This produces the least amount of visual distraction and maintains consistency for how it's done in Scanpy in the manual docstrings and everywhere else. If there is a case where an intersection is relevant, I'd treat that separately. Finally: `a, b, or c` is pretty elegant, too... but if there are no good answers to my two remarks I'd go for `a, b, c`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:523,modifiability,paramet,parameter,523,"Great! . One last thing: In docstrings, why would you interpret a comma separated list as intersection or a tuple? This is not code but for humans. I'm even having a hard time to imagine the case that gives rise to an intersection. Also, a tuple in a docstring should always be verbose with `(,)`, so that no confusion is possible; we'll enforce that in the docs. Right now, the convention across all the major packages is to simply print out a comma separated list of types if you are allowed to pass different types to a parameter. This produces the least amount of visual distraction and maintains consistency for how it's done in Scanpy in the manual docstrings and everywhere else. If there is a case where an intersection is relevant, I'd treat that separately. Finally: `a, b, or c` is pretty elegant, too... but if there are no good answers to my two remarks I'd go for `a, b, c`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:591,modifiability,maintain,maintains,591,"Great! . One last thing: In docstrings, why would you interpret a comma separated list as intersection or a tuple? This is not code but for humans. I'm even having a hard time to imagine the case that gives rise to an intersection. Also, a tuple in a docstring should always be verbose with `(,)`, so that no confusion is possible; we'll enforce that in the docs. Right now, the convention across all the major packages is to simply print out a comma separated list of types if you are allowed to pass different types to a parameter. This produces the least amount of visual distraction and maintains consistency for how it's done in Scanpy in the manual docstrings and everywhere else. If there is a case where an intersection is relevant, I'd treat that separately. Finally: `a, b, or c` is pretty elegant, too... but if there are no good answers to my two remarks I'd go for `a, b, c`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:171,performance,time,time,171,"Great! . One last thing: In docstrings, why would you interpret a comma separated list as intersection or a tuple? This is not code but for humans. I'm even having a hard time to imagine the case that gives rise to an intersection. Also, a tuple in a docstring should always be verbose with `(,)`, so that no confusion is possible; we'll enforce that in the docs. Right now, the convention across all the major packages is to simply print out a comma separated list of types if you are allowed to pass different types to a parameter. This produces the least amount of visual distraction and maintains consistency for how it's done in Scanpy in the manual docstrings and everywhere else. If there is a case where an intersection is relevant, I'd treat that separately. Finally: `a, b, or c` is pretty elegant, too... but if there are no good answers to my two remarks I'd go for `a, b, c`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:591,safety,maintain,maintains,591,"Great! . One last thing: In docstrings, why would you interpret a comma separated list as intersection or a tuple? This is not code but for humans. I'm even having a hard time to imagine the case that gives rise to an intersection. Also, a tuple in a docstring should always be verbose with `(,)`, so that no confusion is possible; we'll enforce that in the docs. Right now, the convention across all the major packages is to simply print out a comma separated list of types if you are allowed to pass different types to a parameter. This produces the least amount of visual distraction and maintains consistency for how it's done in Scanpy in the manual docstrings and everywhere else. If there is a case where an intersection is relevant, I'd treat that separately. Finally: `a, b, or c` is pretty elegant, too... but if there are no good answers to my two remarks I'd go for `a, b, c`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:426,testability,simpl,simply,426,"Great! . One last thing: In docstrings, why would you interpret a comma separated list as intersection or a tuple? This is not code but for humans. I'm even having a hard time to imagine the case that gives rise to an intersection. Also, a tuple in a docstring should always be verbose with `(,)`, so that no confusion is possible; we'll enforce that in the docs. Right now, the convention across all the major packages is to simply print out a comma separated list of types if you are allowed to pass different types to a parameter. This produces the least amount of visual distraction and maintains consistency for how it's done in Scanpy in the manual docstrings and everywhere else. If there is a case where an intersection is relevant, I'd treat that separately. Finally: `a, b, or c` is pretty elegant, too... but if there are no good answers to my two remarks I'd go for `a, b, c`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:426,usability,simpl,simply,426,"Great! . One last thing: In docstrings, why would you interpret a comma separated list as intersection or a tuple? This is not code but for humans. I'm even having a hard time to imagine the case that gives rise to an intersection. Also, a tuple in a docstring should always be verbose with `(,)`, so that no confusion is possible; we'll enforce that in the docs. Right now, the convention across all the major packages is to simply print out a comma separated list of types if you are allowed to pass different types to a parameter. This produces the least amount of visual distraction and maintains consistency for how it's done in Scanpy in the manual docstrings and everywhere else. If there is a case where an intersection is relevant, I'd treat that separately. Finally: `a, b, or c` is pretty elegant, too... but if there are no good answers to my two remarks I'd go for `a, b, c`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:568,usability,visual,visual,568,"Great! . One last thing: In docstrings, why would you interpret a comma separated list as intersection or a tuple? This is not code but for humans. I'm even having a hard time to imagine the case that gives rise to an intersection. Also, a tuple in a docstring should always be verbose with `(,)`, so that no confusion is possible; we'll enforce that in the docs. Right now, the convention across all the major packages is to simply print out a comma separated list of types if you are allowed to pass different types to a parameter. This produces the least amount of visual distraction and maintains consistency for how it's done in Scanpy in the manual docstrings and everywhere else. If there is a case where an intersection is relevant, I'd treat that separately. Finally: `a, b, or c` is pretty elegant, too... but if there are no good answers to my two remarks I'd go for `a, b, c`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:601,usability,consist,consistency,601,"Great! . One last thing: In docstrings, why would you interpret a comma separated list as intersection or a tuple? This is not code but for humans. I'm even having a hard time to imagine the case that gives rise to an intersection. Also, a tuple in a docstring should always be verbose with `(,)`, so that no confusion is possible; we'll enforce that in the docs. Right now, the convention across all the major packages is to simply print out a comma separated list of types if you are allowed to pass different types to a parameter. This produces the least amount of visual distraction and maintains consistency for how it's done in Scanpy in the manual docstrings and everywhere else. If there is a case where an intersection is relevant, I'd treat that separately. Finally: `a, b, or c` is pretty elegant, too... but if there are no good answers to my two remarks I'd go for `a, b, c`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:18,deployability,api,api-wrap,18,"I created [legacy-api-wrap](https://github.com/flying-sheep/legacy-api-wrap). Only caveat: Scanpy is still 3.5 compatible, and Im using f string syntax in it and its dependency [get_version](https://github.com/flying-sheep/get_version), so its python 3.6 only (which could be circumvented via [future-fstrings](https://github.com/asottile/future-fstrings) or so)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:67,deployability,api,api-wrap,67,"I created [legacy-api-wrap](https://github.com/flying-sheep/legacy-api-wrap). Only caveat: Scanpy is still 3.5 compatible, and Im using f string syntax in it and its dependency [get_version](https://github.com/flying-sheep/get_version), so its python 3.6 only (which could be circumvented via [future-fstrings](https://github.com/asottile/future-fstrings) or so)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:167,deployability,depend,dependency,167,"I created [legacy-api-wrap](https://github.com/flying-sheep/legacy-api-wrap). Only caveat: Scanpy is still 3.5 compatible, and Im using f string syntax in it and its dependency [get_version](https://github.com/flying-sheep/get_version), so its python 3.6 only (which could be circumvented via [future-fstrings](https://github.com/asottile/future-fstrings) or so)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:18,integrability,api,api-wrap,18,"I created [legacy-api-wrap](https://github.com/flying-sheep/legacy-api-wrap). Only caveat: Scanpy is still 3.5 compatible, and Im using f string syntax in it and its dependency [get_version](https://github.com/flying-sheep/get_version), so its python 3.6 only (which could be circumvented via [future-fstrings](https://github.com/asottile/future-fstrings) or so)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:67,integrability,api,api-wrap,67,"I created [legacy-api-wrap](https://github.com/flying-sheep/legacy-api-wrap). Only caveat: Scanpy is still 3.5 compatible, and Im using f string syntax in it and its dependency [get_version](https://github.com/flying-sheep/get_version), so its python 3.6 only (which could be circumvented via [future-fstrings](https://github.com/asottile/future-fstrings) or so)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:167,integrability,depend,dependency,167,"I created [legacy-api-wrap](https://github.com/flying-sheep/legacy-api-wrap). Only caveat: Scanpy is still 3.5 compatible, and Im using f string syntax in it and its dependency [get_version](https://github.com/flying-sheep/get_version), so its python 3.6 only (which could be circumvented via [future-fstrings](https://github.com/asottile/future-fstrings) or so)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:18,interoperability,api,api-wrap,18,"I created [legacy-api-wrap](https://github.com/flying-sheep/legacy-api-wrap). Only caveat: Scanpy is still 3.5 compatible, and Im using f string syntax in it and its dependency [get_version](https://github.com/flying-sheep/get_version), so its python 3.6 only (which could be circumvented via [future-fstrings](https://github.com/asottile/future-fstrings) or so)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:67,interoperability,api,api-wrap,67,"I created [legacy-api-wrap](https://github.com/flying-sheep/legacy-api-wrap). Only caveat: Scanpy is still 3.5 compatible, and Im using f string syntax in it and its dependency [get_version](https://github.com/flying-sheep/get_version), so its python 3.6 only (which could be circumvented via [future-fstrings](https://github.com/asottile/future-fstrings) or so)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:111,interoperability,compatib,compatible,111,"I created [legacy-api-wrap](https://github.com/flying-sheep/legacy-api-wrap). Only caveat: Scanpy is still 3.5 compatible, and Im using f string syntax in it and its dependency [get_version](https://github.com/flying-sheep/get_version), so its python 3.6 only (which could be circumvented via [future-fstrings](https://github.com/asottile/future-fstrings) or so)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:167,modifiability,depend,dependency,167,"I created [legacy-api-wrap](https://github.com/flying-sheep/legacy-api-wrap). Only caveat: Scanpy is still 3.5 compatible, and Im using f string syntax in it and its dependency [get_version](https://github.com/flying-sheep/get_version), so its python 3.6 only (which could be circumvented via [future-fstrings](https://github.com/asottile/future-fstrings) or so)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:167,safety,depend,dependency,167,"I created [legacy-api-wrap](https://github.com/flying-sheep/legacy-api-wrap). Only caveat: Scanpy is still 3.5 compatible, and Im using f string syntax in it and its dependency [get_version](https://github.com/flying-sheep/get_version), so its python 3.6 only (which could be circumvented via [future-fstrings](https://github.com/asottile/future-fstrings) or so)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:167,testability,depend,dependency,167,"I created [legacy-api-wrap](https://github.com/flying-sheep/legacy-api-wrap). Only caveat: Scanpy is still 3.5 compatible, and Im using f string syntax in it and its dependency [get_version](https://github.com/flying-sheep/get_version), so its python 3.6 only (which could be circumvented via [future-fstrings](https://github.com/asottile/future-fstrings) or so)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:158,deployability,compos,composite,158,"> In docstrings, why would you interpret a comma separated list as intersection or a tuple? In natural language a, b, c usually means a, b, and c (i.e. a composite or a logical intersection). And an intersection type is one that has all the attributes of all the types, like in `class x(a, b, c): ...` (where commas are also used). In Python plain `a, b, c` constructs a tuple (a composite type): `tup = a, b, c`. It took me a long time to find a numpy function that uses commas for anything other than the , optional, but of course [youre right](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.matrix_rank.html). They do it like that. Why dont people think before establishing conventions. A good example of that functions docs is also how braindead the optional is: for `tol`, it means or None, for `hermetian` it means has a default (probably, no way to know for sure). Goddamn. > Ah, we already have a contributing sheet. oh, is this visible? or does it need to be uppercase for that? CONTRIBUTING.md? I dont see it when creating an issue, but maybe because Im an organization member?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:173,deployability,log,logical,173,"> In docstrings, why would you interpret a comma separated list as intersection or a tuple? In natural language a, b, c usually means a, b, and c (i.e. a composite or a logical intersection). And an intersection type is one that has all the attributes of all the types, like in `class x(a, b, c): ...` (where commas are also used). In Python plain `a, b, c` constructs a tuple (a composite type): `tup = a, b, c`. It took me a long time to find a numpy function that uses commas for anything other than the , optional, but of course [youre right](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.matrix_rank.html). They do it like that. Why dont people think before establishing conventions. A good example of that functions docs is also how braindead the optional is: for `tol`, it means or None, for `hermetian` it means has a default (probably, no way to know for sure). Goddamn. > Ah, we already have a contributing sheet. oh, is this visible? or does it need to be uppercase for that? CONTRIBUTING.md? I dont see it when creating an issue, but maybe because Im an organization member?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:384,deployability,compos,composite,384,"> In docstrings, why would you interpret a comma separated list as intersection or a tuple? In natural language a, b, c usually means a, b, and c (i.e. a composite or a logical intersection). And an intersection type is one that has all the attributes of all the types, like in `class x(a, b, c): ...` (where commas are also used). In Python plain `a, b, c` constructs a tuple (a composite type): `tup = a, b, c`. It took me a long time to find a numpy function that uses commas for anything other than the , optional, but of course [youre right](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.matrix_rank.html). They do it like that. Why dont people think before establishing conventions. A good example of that functions docs is also how braindead the optional is: for `tol`, it means or None, for `hermetian` it means has a default (probably, no way to know for sure). Goddamn. > Ah, we already have a contributing sheet. oh, is this visible? or does it need to be uppercase for that? CONTRIBUTING.md? I dont see it when creating an issue, but maybe because Im an organization member?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:158,modifiability,compos,composite,158,"> In docstrings, why would you interpret a comma separated list as intersection or a tuple? In natural language a, b, c usually means a, b, and c (i.e. a composite or a logical intersection). And an intersection type is one that has all the attributes of all the types, like in `class x(a, b, c): ...` (where commas are also used). In Python plain `a, b, c` constructs a tuple (a composite type): `tup = a, b, c`. It took me a long time to find a numpy function that uses commas for anything other than the , optional, but of course [youre right](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.matrix_rank.html). They do it like that. Why dont people think before establishing conventions. A good example of that functions docs is also how braindead the optional is: for `tol`, it means or None, for `hermetian` it means has a default (probably, no way to know for sure). Goddamn. > Ah, we already have a contributing sheet. oh, is this visible? or does it need to be uppercase for that? CONTRIBUTING.md? I dont see it when creating an issue, but maybe because Im an organization member?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:384,modifiability,compos,composite,384,"> In docstrings, why would you interpret a comma separated list as intersection or a tuple? In natural language a, b, c usually means a, b, and c (i.e. a composite or a logical intersection). And an intersection type is one that has all the attributes of all the types, like in `class x(a, b, c): ...` (where commas are also used). In Python plain `a, b, c` constructs a tuple (a composite type): `tup = a, b, c`. It took me a long time to find a numpy function that uses commas for anything other than the , optional, but of course [youre right](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.matrix_rank.html). They do it like that. Why dont people think before establishing conventions. A good example of that functions docs is also how braindead the optional is: for `tol`, it means or None, for `hermetian` it means has a default (probably, no way to know for sure). Goddamn. > Ah, we already have a contributing sheet. oh, is this visible? or does it need to be uppercase for that? CONTRIBUTING.md? I dont see it when creating an issue, but maybe because Im an organization member?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:436,performance,time,time,436,"> In docstrings, why would you interpret a comma separated list as intersection or a tuple? In natural language a, b, c usually means a, b, and c (i.e. a composite or a logical intersection). And an intersection type is one that has all the attributes of all the types, like in `class x(a, b, c): ...` (where commas are also used). In Python plain `a, b, c` constructs a tuple (a composite type): `tup = a, b, c`. It took me a long time to find a numpy function that uses commas for anything other than the , optional, but of course [youre right](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.matrix_rank.html). They do it like that. Why dont people think before establishing conventions. A good example of that functions docs is also how braindead the optional is: for `tol`, it means or None, for `hermetian` it means has a default (probably, no way to know for sure). Goddamn. > Ah, we already have a contributing sheet. oh, is this visible? or does it need to be uppercase for that? CONTRIBUTING.md? I dont see it when creating an issue, but maybe because Im an organization member?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:984,reliability,doe,does,984,"> In docstrings, why would you interpret a comma separated list as intersection or a tuple? In natural language a, b, c usually means a, b, and c (i.e. a composite or a logical intersection). And an intersection type is one that has all the attributes of all the types, like in `class x(a, b, c): ...` (where commas are also used). In Python plain `a, b, c` constructs a tuple (a composite type): `tup = a, b, c`. It took me a long time to find a numpy function that uses commas for anything other than the , optional, but of course [youre right](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.matrix_rank.html). They do it like that. Why dont people think before establishing conventions. A good example of that functions docs is also how braindead the optional is: for `tol`, it means or None, for `hermetian` it means has a default (probably, no way to know for sure). Goddamn. > Ah, we already have a contributing sheet. oh, is this visible? or does it need to be uppercase for that? CONTRIBUTING.md? I dont see it when creating an issue, but maybe because Im an organization member?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:173,safety,log,logical,173,"> In docstrings, why would you interpret a comma separated list as intersection or a tuple? In natural language a, b, c usually means a, b, and c (i.e. a composite or a logical intersection). And an intersection type is one that has all the attributes of all the types, like in `class x(a, b, c): ...` (where commas are also used). In Python plain `a, b, c` constructs a tuple (a composite type): `tup = a, b, c`. It took me a long time to find a numpy function that uses commas for anything other than the , optional, but of course [youre right](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.matrix_rank.html). They do it like that. Why dont people think before establishing conventions. A good example of that functions docs is also how braindead the optional is: for `tol`, it means or None, for `hermetian` it means has a default (probably, no way to know for sure). Goddamn. > Ah, we already have a contributing sheet. oh, is this visible? or does it need to be uppercase for that? CONTRIBUTING.md? I dont see it when creating an issue, but maybe because Im an organization member?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:173,security,log,logical,173,"> In docstrings, why would you interpret a comma separated list as intersection or a tuple? In natural language a, b, c usually means a, b, and c (i.e. a composite or a logical intersection). And an intersection type is one that has all the attributes of all the types, like in `class x(a, b, c): ...` (where commas are also used). In Python plain `a, b, c` constructs a tuple (a composite type): `tup = a, b, c`. It took me a long time to find a numpy function that uses commas for anything other than the , optional, but of course [youre right](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.matrix_rank.html). They do it like that. Why dont people think before establishing conventions. A good example of that functions docs is also how braindead the optional is: for `tol`, it means or None, for `hermetian` it means has a default (probably, no way to know for sure). Goddamn. > Ah, we already have a contributing sheet. oh, is this visible? or does it need to be uppercase for that? CONTRIBUTING.md? I dont see it when creating an issue, but maybe because Im an organization member?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:173,testability,log,logical,173,"> In docstrings, why would you interpret a comma separated list as intersection or a tuple? In natural language a, b, c usually means a, b, and c (i.e. a composite or a logical intersection). And an intersection type is one that has all the attributes of all the types, like in `class x(a, b, c): ...` (where commas are also used). In Python plain `a, b, c` constructs a tuple (a composite type): `tup = a, b, c`. It took me a long time to find a numpy function that uses commas for anything other than the , optional, but of course [youre right](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.matrix_rank.html). They do it like that. Why dont people think before establishing conventions. A good example of that functions docs is also how braindead the optional is: for `tol`, it means or None, for `hermetian` it means has a default (probably, no way to know for sure). Goddamn. > Ah, we already have a contributing sheet. oh, is this visible? or does it need to be uppercase for that? CONTRIBUTING.md? I dont see it when creating an issue, but maybe because Im an organization member?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:609,deployability,log,logical,609,"> In natural language... ... I know all that. :wink: But numpy, pandas, scikit learn, tensorflow, seaborn all have the comma-separated list as a convention and I'd really like to stick to that convention. > A good example... . No, the `optional` keyword always means that a parameter has a default. Very often, people forget to append ""or None"" (`, None`) to the list of possible types. Btw: that's maybe a nice way of thinking about it for you: you use a ""tuple of possible types"" to denote that any of these types can be passed in the function. As mentioned before, there is no point in using set-theoretic/logical notions like union or intersection as the topic is so simple that it doesn't need it (no need for an intersection, it's not even clear what that would mean; if you're stringent about it, it's also not clear for union). So, let's simply take the comma-separated list.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:659,integrability,topic,topic,659,"> In natural language... ... I know all that. :wink: But numpy, pandas, scikit learn, tensorflow, seaborn all have the comma-separated list as a convention and I'd really like to stick to that convention. > A good example... . No, the `optional` keyword always means that a parameter has a default. Very often, people forget to append ""or None"" (`, None`) to the list of possible types. Btw: that's maybe a nice way of thinking about it for you: you use a ""tuple of possible types"" to denote that any of these types can be passed in the function. As mentioned before, there is no point in using set-theoretic/logical notions like union or intersection as the topic is so simple that it doesn't need it (no need for an intersection, it's not even clear what that would mean; if you're stringent about it, it's also not clear for union). So, let's simply take the comma-separated list.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:274,modifiability,paramet,parameter,274,"> In natural language... ... I know all that. :wink: But numpy, pandas, scikit learn, tensorflow, seaborn all have the comma-separated list as a convention and I'd really like to stick to that convention. > A good example... . No, the `optional` keyword always means that a parameter has a default. Very often, people forget to append ""or None"" (`, None`) to the list of possible types. Btw: that's maybe a nice way of thinking about it for you: you use a ""tuple of possible types"" to denote that any of these types can be passed in the function. As mentioned before, there is no point in using set-theoretic/logical notions like union or intersection as the topic is so simple that it doesn't need it (no need for an intersection, it's not even clear what that would mean; if you're stringent about it, it's also not clear for union). So, let's simply take the comma-separated list.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:686,reliability,doe,doesn,686,"> In natural language... ... I know all that. :wink: But numpy, pandas, scikit learn, tensorflow, seaborn all have the comma-separated list as a convention and I'd really like to stick to that convention. > A good example... . No, the `optional` keyword always means that a parameter has a default. Very often, people forget to append ""or None"" (`, None`) to the list of possible types. Btw: that's maybe a nice way of thinking about it for you: you use a ""tuple of possible types"" to denote that any of these types can be passed in the function. As mentioned before, there is no point in using set-theoretic/logical notions like union or intersection as the topic is so simple that it doesn't need it (no need for an intersection, it's not even clear what that would mean; if you're stringent about it, it's also not clear for union). So, let's simply take the comma-separated list.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:609,safety,log,logical,609,"> In natural language... ... I know all that. :wink: But numpy, pandas, scikit learn, tensorflow, seaborn all have the comma-separated list as a convention and I'd really like to stick to that convention. > A good example... . No, the `optional` keyword always means that a parameter has a default. Very often, people forget to append ""or None"" (`, None`) to the list of possible types. Btw: that's maybe a nice way of thinking about it for you: you use a ""tuple of possible types"" to denote that any of these types can be passed in the function. As mentioned before, there is no point in using set-theoretic/logical notions like union or intersection as the topic is so simple that it doesn't need it (no need for an intersection, it's not even clear what that would mean; if you're stringent about it, it's also not clear for union). So, let's simply take the comma-separated list.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:609,security,log,logical,609,"> In natural language... ... I know all that. :wink: But numpy, pandas, scikit learn, tensorflow, seaborn all have the comma-separated list as a convention and I'd really like to stick to that convention. > A good example... . No, the `optional` keyword always means that a parameter has a default. Very often, people forget to append ""or None"" (`, None`) to the list of possible types. Btw: that's maybe a nice way of thinking about it for you: you use a ""tuple of possible types"" to denote that any of these types can be passed in the function. As mentioned before, there is no point in using set-theoretic/logical notions like union or intersection as the topic is so simple that it doesn't need it (no need for an intersection, it's not even clear what that would mean; if you're stringent about it, it's also not clear for union). So, let's simply take the comma-separated list.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:609,testability,log,logical,609,"> In natural language... ... I know all that. :wink: But numpy, pandas, scikit learn, tensorflow, seaborn all have the comma-separated list as a convention and I'd really like to stick to that convention. > A good example... . No, the `optional` keyword always means that a parameter has a default. Very often, people forget to append ""or None"" (`, None`) to the list of possible types. Btw: that's maybe a nice way of thinking about it for you: you use a ""tuple of possible types"" to denote that any of these types can be passed in the function. As mentioned before, there is no point in using set-theoretic/logical notions like union or intersection as the topic is so simple that it doesn't need it (no need for an intersection, it's not even clear what that would mean; if you're stringent about it, it's also not clear for union). So, let's simply take the comma-separated list.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:671,testability,simpl,simple,671,"> In natural language... ... I know all that. :wink: But numpy, pandas, scikit learn, tensorflow, seaborn all have the comma-separated list as a convention and I'd really like to stick to that convention. > A good example... . No, the `optional` keyword always means that a parameter has a default. Very often, people forget to append ""or None"" (`, None`) to the list of possible types. Btw: that's maybe a nice way of thinking about it for you: you use a ""tuple of possible types"" to denote that any of these types can be passed in the function. As mentioned before, there is no point in using set-theoretic/logical notions like union or intersection as the topic is so simple that it doesn't need it (no need for an intersection, it's not even clear what that would mean; if you're stringent about it, it's also not clear for union). So, let's simply take the comma-separated list.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:846,testability,simpl,simply,846,"> In natural language... ... I know all that. :wink: But numpy, pandas, scikit learn, tensorflow, seaborn all have the comma-separated list as a convention and I'd really like to stick to that convention. > A good example... . No, the `optional` keyword always means that a parameter has a default. Very often, people forget to append ""or None"" (`, None`) to the list of possible types. Btw: that's maybe a nice way of thinking about it for you: you use a ""tuple of possible types"" to denote that any of these types can be passed in the function. As mentioned before, there is no point in using set-theoretic/logical notions like union or intersection as the topic is so simple that it doesn't need it (no need for an intersection, it's not even clear what that would mean; if you're stringent about it, it's also not clear for union). So, let's simply take the comma-separated list.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:79,usability,learn,learn,79,"> In natural language... ... I know all that. :wink: But numpy, pandas, scikit learn, tensorflow, seaborn all have the comma-separated list as a convention and I'd really like to stick to that convention. > A good example... . No, the `optional` keyword always means that a parameter has a default. Very often, people forget to append ""or None"" (`, None`) to the list of possible types. Btw: that's maybe a nice way of thinking about it for you: you use a ""tuple of possible types"" to denote that any of these types can be passed in the function. As mentioned before, there is no point in using set-theoretic/logical notions like union or intersection as the topic is so simple that it doesn't need it (no need for an intersection, it's not even clear what that would mean; if you're stringent about it, it's also not clear for union). So, let's simply take the comma-separated list.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:671,usability,simpl,simple,671,"> In natural language... ... I know all that. :wink: But numpy, pandas, scikit learn, tensorflow, seaborn all have the comma-separated list as a convention and I'd really like to stick to that convention. > A good example... . No, the `optional` keyword always means that a parameter has a default. Very often, people forget to append ""or None"" (`, None`) to the list of possible types. Btw: that's maybe a nice way of thinking about it for you: you use a ""tuple of possible types"" to denote that any of these types can be passed in the function. As mentioned before, there is no point in using set-theoretic/logical notions like union or intersection as the topic is so simple that it doesn't need it (no need for an intersection, it's not even clear what that would mean; if you're stringent about it, it's also not clear for union). So, let's simply take the comma-separated list.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:746,usability,clear,clear,746,"> In natural language... ... I know all that. :wink: But numpy, pandas, scikit learn, tensorflow, seaborn all have the comma-separated list as a convention and I'd really like to stick to that convention. > A good example... . No, the `optional` keyword always means that a parameter has a default. Very often, people forget to append ""or None"" (`, None`) to the list of possible types. Btw: that's maybe a nice way of thinking about it for you: you use a ""tuple of possible types"" to denote that any of these types can be passed in the function. As mentioned before, there is no point in using set-theoretic/logical notions like union or intersection as the topic is so simple that it doesn't need it (no need for an intersection, it's not even clear what that would mean; if you're stringent about it, it's also not clear for union). So, let's simply take the comma-separated list.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:818,usability,clear,clear,818,"> In natural language... ... I know all that. :wink: But numpy, pandas, scikit learn, tensorflow, seaborn all have the comma-separated list as a convention and I'd really like to stick to that convention. > A good example... . No, the `optional` keyword always means that a parameter has a default. Very often, people forget to append ""or None"" (`, None`) to the list of possible types. Btw: that's maybe a nice way of thinking about it for you: you use a ""tuple of possible types"" to denote that any of these types can be passed in the function. As mentioned before, there is no point in using set-theoretic/logical notions like union or intersection as the topic is so simple that it doesn't need it (no need for an intersection, it's not even clear what that would mean; if you're stringent about it, it's also not clear for union). So, let's simply take the comma-separated list.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:846,usability,simpl,simply,846,"> In natural language... ... I know all that. :wink: But numpy, pandas, scikit learn, tensorflow, seaborn all have the comma-separated list as a convention and I'd really like to stick to that convention. > A good example... . No, the `optional` keyword always means that a parameter has a default. Very often, people forget to append ""or None"" (`, None`) to the list of possible types. Btw: that's maybe a nice way of thinking about it for you: you use a ""tuple of possible types"" to denote that any of these types can be passed in the function. As mentioned before, there is no point in using set-theoretic/logical notions like union or intersection as the topic is so simple that it doesn't need it (no need for an intersection, it's not even clear what that would mean; if you're stringent about it, it's also not clear for union). So, let's simply take the comma-separated list.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:381,deployability,automat,automated,381,"> I know all that. :wink:. And I know that you know! I just like to be comprehensive when presenting my arguments! > But numpy, pandas, scikit learn, tensorflow, seaborn all have the comma-separated list as a convention and I'd really like to stick to that convention. OK. Id prefer a, b, or c, but Ill concede. It would also be no problem to change it later since all will be automated :+1: . > No, the optional keyword always means that a parameter has a default. Very often, people forget to append ""or None"" (, None) to the list of possible types. Well, when I open scanpy in PyCharm and someone forgot that in a type annotation, it highlights that fact to me. Pretty nice. > As mentioned before, there is no point in using set-theoretic/logical notions like union or intersection as the topic is so simple that it doesn't need it (no need for an intersection, it's not even clear what that would mean; if you're stringent about it, it's also not clear for union). Oh, then you didnt hear of type theory. Its a branch of logic: Type systems are formal systems, and in most of them the terms I used are well defined. The kinds of composite types I mentioned are:. - `Union` of types / Sum Type / [Tagged Union](https://en.wikipedia.org/wiki/Tagged_union): Variables with one of those have one of several fixed types. - Subtype / [Intersection Type](https://en.wikipedia.org/wiki/Type_system#Intersection_types): Variables have all the properties of the supertypes. - `Tuple` / [Product Type](https://en.wikipedia.org/wiki/Product_type): Variables contain multiple entries that each have one corresponding type.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:746,deployability,log,logical,746,"> I know all that. :wink:. And I know that you know! I just like to be comprehensive when presenting my arguments! > But numpy, pandas, scikit learn, tensorflow, seaborn all have the comma-separated list as a convention and I'd really like to stick to that convention. OK. Id prefer a, b, or c, but Ill concede. It would also be no problem to change it later since all will be automated :+1: . > No, the optional keyword always means that a parameter has a default. Very often, people forget to append ""or None"" (, None) to the list of possible types. Well, when I open scanpy in PyCharm and someone forgot that in a type annotation, it highlights that fact to me. Pretty nice. > As mentioned before, there is no point in using set-theoretic/logical notions like union or intersection as the topic is so simple that it doesn't need it (no need for an intersection, it's not even clear what that would mean; if you're stringent about it, it's also not clear for union). Oh, then you didnt hear of type theory. Its a branch of logic: Type systems are formal systems, and in most of them the terms I used are well defined. The kinds of composite types I mentioned are:. - `Union` of types / Sum Type / [Tagged Union](https://en.wikipedia.org/wiki/Tagged_union): Variables with one of those have one of several fixed types. - Subtype / [Intersection Type](https://en.wikipedia.org/wiki/Type_system#Intersection_types): Variables have all the properties of the supertypes. - `Tuple` / [Product Type](https://en.wikipedia.org/wiki/Product_type): Variables contain multiple entries that each have one corresponding type.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1031,deployability,log,logic,1031,"> I know all that. :wink:. And I know that you know! I just like to be comprehensive when presenting my arguments! > But numpy, pandas, scikit learn, tensorflow, seaborn all have the comma-separated list as a convention and I'd really like to stick to that convention. OK. Id prefer a, b, or c, but Ill concede. It would also be no problem to change it later since all will be automated :+1: . > No, the optional keyword always means that a parameter has a default. Very often, people forget to append ""or None"" (, None) to the list of possible types. Well, when I open scanpy in PyCharm and someone forgot that in a type annotation, it highlights that fact to me. Pretty nice. > As mentioned before, there is no point in using set-theoretic/logical notions like union or intersection as the topic is so simple that it doesn't need it (no need for an intersection, it's not even clear what that would mean; if you're stringent about it, it's also not clear for union). Oh, then you didnt hear of type theory. Its a branch of logic: Type systems are formal systems, and in most of them the terms I used are well defined. The kinds of composite types I mentioned are:. - `Union` of types / Sum Type / [Tagged Union](https://en.wikipedia.org/wiki/Tagged_union): Variables with one of those have one of several fixed types. - Subtype / [Intersection Type](https://en.wikipedia.org/wiki/Type_system#Intersection_types): Variables have all the properties of the supertypes. - `Tuple` / [Product Type](https://en.wikipedia.org/wiki/Product_type): Variables contain multiple entries that each have one corresponding type.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1139,deployability,compos,composite,1139,"> I know all that. :wink:. And I know that you know! I just like to be comprehensive when presenting my arguments! > But numpy, pandas, scikit learn, tensorflow, seaborn all have the comma-separated list as a convention and I'd really like to stick to that convention. OK. Id prefer a, b, or c, but Ill concede. It would also be no problem to change it later since all will be automated :+1: . > No, the optional keyword always means that a parameter has a default. Very often, people forget to append ""or None"" (, None) to the list of possible types. Well, when I open scanpy in PyCharm and someone forgot that in a type annotation, it highlights that fact to me. Pretty nice. > As mentioned before, there is no point in using set-theoretic/logical notions like union or intersection as the topic is so simple that it doesn't need it (no need for an intersection, it's not even clear what that would mean; if you're stringent about it, it's also not clear for union). Oh, then you didnt hear of type theory. Its a branch of logic: Type systems are formal systems, and in most of them the terms I used are well defined. The kinds of composite types I mentioned are:. - `Union` of types / Sum Type / [Tagged Union](https://en.wikipedia.org/wiki/Tagged_union): Variables with one of those have one of several fixed types. - Subtype / [Intersection Type](https://en.wikipedia.org/wiki/Type_system#Intersection_types): Variables have all the properties of the supertypes. - `Tuple` / [Product Type](https://en.wikipedia.org/wiki/Product_type): Variables contain multiple entries that each have one corresponding type.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1556,deployability,contain,contain,1556,"> I know all that. :wink:. And I know that you know! I just like to be comprehensive when presenting my arguments! > But numpy, pandas, scikit learn, tensorflow, seaborn all have the comma-separated list as a convention and I'd really like to stick to that convention. OK. Id prefer a, b, or c, but Ill concede. It would also be no problem to change it later since all will be automated :+1: . > No, the optional keyword always means that a parameter has a default. Very often, people forget to append ""or None"" (, None) to the list of possible types. Well, when I open scanpy in PyCharm and someone forgot that in a type annotation, it highlights that fact to me. Pretty nice. > As mentioned before, there is no point in using set-theoretic/logical notions like union or intersection as the topic is so simple that it doesn't need it (no need for an intersection, it's not even clear what that would mean; if you're stringent about it, it's also not clear for union). Oh, then you didnt hear of type theory. Its a branch of logic: Type systems are formal systems, and in most of them the terms I used are well defined. The kinds of composite types I mentioned are:. - `Union` of types / Sum Type / [Tagged Union](https://en.wikipedia.org/wiki/Tagged_union): Variables with one of those have one of several fixed types. - Subtype / [Intersection Type](https://en.wikipedia.org/wiki/Type_system#Intersection_types): Variables have all the properties of the supertypes. - `Tuple` / [Product Type](https://en.wikipedia.org/wiki/Product_type): Variables contain multiple entries that each have one corresponding type.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:796,integrability,topic,topic,796,"> I know all that. :wink:. And I know that you know! I just like to be comprehensive when presenting my arguments! > But numpy, pandas, scikit learn, tensorflow, seaborn all have the comma-separated list as a convention and I'd really like to stick to that convention. OK. Id prefer a, b, or c, but Ill concede. It would also be no problem to change it later since all will be automated :+1: . > No, the optional keyword always means that a parameter has a default. Very often, people forget to append ""or None"" (, None) to the list of possible types. Well, when I open scanpy in PyCharm and someone forgot that in a type annotation, it highlights that fact to me. Pretty nice. > As mentioned before, there is no point in using set-theoretic/logical notions like union or intersection as the topic is so simple that it doesn't need it (no need for an intersection, it's not even clear what that would mean; if you're stringent about it, it's also not clear for union). Oh, then you didnt hear of type theory. Its a branch of logic: Type systems are formal systems, and in most of them the terms I used are well defined. The kinds of composite types I mentioned are:. - `Union` of types / Sum Type / [Tagged Union](https://en.wikipedia.org/wiki/Tagged_union): Variables with one of those have one of several fixed types. - Subtype / [Intersection Type](https://en.wikipedia.org/wiki/Type_system#Intersection_types): Variables have all the properties of the supertypes. - `Tuple` / [Product Type](https://en.wikipedia.org/wiki/Product_type): Variables contain multiple entries that each have one corresponding type.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1328,integrability,Sub,Subtype,1328,"> I know all that. :wink:. And I know that you know! I just like to be comprehensive when presenting my arguments! > But numpy, pandas, scikit learn, tensorflow, seaborn all have the comma-separated list as a convention and I'd really like to stick to that convention. OK. Id prefer a, b, or c, but Ill concede. It would also be no problem to change it later since all will be automated :+1: . > No, the optional keyword always means that a parameter has a default. Very often, people forget to append ""or None"" (, None) to the list of possible types. Well, when I open scanpy in PyCharm and someone forgot that in a type annotation, it highlights that fact to me. Pretty nice. > As mentioned before, there is no point in using set-theoretic/logical notions like union or intersection as the topic is so simple that it doesn't need it (no need for an intersection, it's not even clear what that would mean; if you're stringent about it, it's also not clear for union). Oh, then you didnt hear of type theory. Its a branch of logic: Type systems are formal systems, and in most of them the terms I used are well defined. The kinds of composite types I mentioned are:. - `Union` of types / Sum Type / [Tagged Union](https://en.wikipedia.org/wiki/Tagged_union): Variables with one of those have one of several fixed types. - Subtype / [Intersection Type](https://en.wikipedia.org/wiki/Type_system#Intersection_types): Variables have all the properties of the supertypes. - `Tuple` / [Product Type](https://en.wikipedia.org/wiki/Product_type): Variables contain multiple entries that each have one corresponding type.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:445,modifiability,paramet,parameter,445,"> I know all that. :wink:. And I know that you know! I just like to be comprehensive when presenting my arguments! > But numpy, pandas, scikit learn, tensorflow, seaborn all have the comma-separated list as a convention and I'd really like to stick to that convention. OK. Id prefer a, b, or c, but Ill concede. It would also be no problem to change it later since all will be automated :+1: . > No, the optional keyword always means that a parameter has a default. Very often, people forget to append ""or None"" (, None) to the list of possible types. Well, when I open scanpy in PyCharm and someone forgot that in a type annotation, it highlights that fact to me. Pretty nice. > As mentioned before, there is no point in using set-theoretic/logical notions like union or intersection as the topic is so simple that it doesn't need it (no need for an intersection, it's not even clear what that would mean; if you're stringent about it, it's also not clear for union). Oh, then you didnt hear of type theory. Its a branch of logic: Type systems are formal systems, and in most of them the terms I used are well defined. The kinds of composite types I mentioned are:. - `Union` of types / Sum Type / [Tagged Union](https://en.wikipedia.org/wiki/Tagged_union): Variables with one of those have one of several fixed types. - Subtype / [Intersection Type](https://en.wikipedia.org/wiki/Type_system#Intersection_types): Variables have all the properties of the supertypes. - `Tuple` / [Product Type](https://en.wikipedia.org/wiki/Product_type): Variables contain multiple entries that each have one corresponding type.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1139,modifiability,compos,composite,1139,"> I know all that. :wink:. And I know that you know! I just like to be comprehensive when presenting my arguments! > But numpy, pandas, scikit learn, tensorflow, seaborn all have the comma-separated list as a convention and I'd really like to stick to that convention. OK. Id prefer a, b, or c, but Ill concede. It would also be no problem to change it later since all will be automated :+1: . > No, the optional keyword always means that a parameter has a default. Very often, people forget to append ""or None"" (, None) to the list of possible types. Well, when I open scanpy in PyCharm and someone forgot that in a type annotation, it highlights that fact to me. Pretty nice. > As mentioned before, there is no point in using set-theoretic/logical notions like union or intersection as the topic is so simple that it doesn't need it (no need for an intersection, it's not even clear what that would mean; if you're stringent about it, it's also not clear for union). Oh, then you didnt hear of type theory. Its a branch of logic: Type systems are formal systems, and in most of them the terms I used are well defined. The kinds of composite types I mentioned are:. - `Union` of types / Sum Type / [Tagged Union](https://en.wikipedia.org/wiki/Tagged_union): Variables with one of those have one of several fixed types. - Subtype / [Intersection Type](https://en.wikipedia.org/wiki/Type_system#Intersection_types): Variables have all the properties of the supertypes. - `Tuple` / [Product Type](https://en.wikipedia.org/wiki/Product_type): Variables contain multiple entries that each have one corresponding type.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1265,modifiability,Variab,Variables,1265,"> I know all that. :wink:. And I know that you know! I just like to be comprehensive when presenting my arguments! > But numpy, pandas, scikit learn, tensorflow, seaborn all have the comma-separated list as a convention and I'd really like to stick to that convention. OK. Id prefer a, b, or c, but Ill concede. It would also be no problem to change it later since all will be automated :+1: . > No, the optional keyword always means that a parameter has a default. Very often, people forget to append ""or None"" (, None) to the list of possible types. Well, when I open scanpy in PyCharm and someone forgot that in a type annotation, it highlights that fact to me. Pretty nice. > As mentioned before, there is no point in using set-theoretic/logical notions like union or intersection as the topic is so simple that it doesn't need it (no need for an intersection, it's not even clear what that would mean; if you're stringent about it, it's also not clear for union). Oh, then you didnt hear of type theory. Its a branch of logic: Type systems are formal systems, and in most of them the terms I used are well defined. The kinds of composite types I mentioned are:. - `Union` of types / Sum Type / [Tagged Union](https://en.wikipedia.org/wiki/Tagged_union): Variables with one of those have one of several fixed types. - Subtype / [Intersection Type](https://en.wikipedia.org/wiki/Type_system#Intersection_types): Variables have all the properties of the supertypes. - `Tuple` / [Product Type](https://en.wikipedia.org/wiki/Product_type): Variables contain multiple entries that each have one corresponding type.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1421,modifiability,Variab,Variables,1421,"> I know all that. :wink:. And I know that you know! I just like to be comprehensive when presenting my arguments! > But numpy, pandas, scikit learn, tensorflow, seaborn all have the comma-separated list as a convention and I'd really like to stick to that convention. OK. Id prefer a, b, or c, but Ill concede. It would also be no problem to change it later since all will be automated :+1: . > No, the optional keyword always means that a parameter has a default. Very often, people forget to append ""or None"" (, None) to the list of possible types. Well, when I open scanpy in PyCharm and someone forgot that in a type annotation, it highlights that fact to me. Pretty nice. > As mentioned before, there is no point in using set-theoretic/logical notions like union or intersection as the topic is so simple that it doesn't need it (no need for an intersection, it's not even clear what that would mean; if you're stringent about it, it's also not clear for union). Oh, then you didnt hear of type theory. Its a branch of logic: Type systems are formal systems, and in most of them the terms I used are well defined. The kinds of composite types I mentioned are:. - `Union` of types / Sum Type / [Tagged Union](https://en.wikipedia.org/wiki/Tagged_union): Variables with one of those have one of several fixed types. - Subtype / [Intersection Type](https://en.wikipedia.org/wiki/Type_system#Intersection_types): Variables have all the properties of the supertypes. - `Tuple` / [Product Type](https://en.wikipedia.org/wiki/Product_type): Variables contain multiple entries that each have one corresponding type.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1546,modifiability,Variab,Variables,1546,"> I know all that. :wink:. And I know that you know! I just like to be comprehensive when presenting my arguments! > But numpy, pandas, scikit learn, tensorflow, seaborn all have the comma-separated list as a convention and I'd really like to stick to that convention. OK. Id prefer a, b, or c, but Ill concede. It would also be no problem to change it later since all will be automated :+1: . > No, the optional keyword always means that a parameter has a default. Very often, people forget to append ""or None"" (, None) to the list of possible types. Well, when I open scanpy in PyCharm and someone forgot that in a type annotation, it highlights that fact to me. Pretty nice. > As mentioned before, there is no point in using set-theoretic/logical notions like union or intersection as the topic is so simple that it doesn't need it (no need for an intersection, it's not even clear what that would mean; if you're stringent about it, it's also not clear for union). Oh, then you didnt hear of type theory. Its a branch of logic: Type systems are formal systems, and in most of them the terms I used are well defined. The kinds of composite types I mentioned are:. - `Union` of types / Sum Type / [Tagged Union](https://en.wikipedia.org/wiki/Tagged_union): Variables with one of those have one of several fixed types. - Subtype / [Intersection Type](https://en.wikipedia.org/wiki/Type_system#Intersection_types): Variables have all the properties of the supertypes. - `Tuple` / [Product Type](https://en.wikipedia.org/wiki/Product_type): Variables contain multiple entries that each have one corresponding type.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:823,reliability,doe,doesn,823,"> I know all that. :wink:. And I know that you know! I just like to be comprehensive when presenting my arguments! > But numpy, pandas, scikit learn, tensorflow, seaborn all have the comma-separated list as a convention and I'd really like to stick to that convention. OK. Id prefer a, b, or c, but Ill concede. It would also be no problem to change it later since all will be automated :+1: . > No, the optional keyword always means that a parameter has a default. Very often, people forget to append ""or None"" (, None) to the list of possible types. Well, when I open scanpy in PyCharm and someone forgot that in a type annotation, it highlights that fact to me. Pretty nice. > As mentioned before, there is no point in using set-theoretic/logical notions like union or intersection as the topic is so simple that it doesn't need it (no need for an intersection, it's not even clear what that would mean; if you're stringent about it, it's also not clear for union). Oh, then you didnt hear of type theory. Its a branch of logic: Type systems are formal systems, and in most of them the terms I used are well defined. The kinds of composite types I mentioned are:. - `Union` of types / Sum Type / [Tagged Union](https://en.wikipedia.org/wiki/Tagged_union): Variables with one of those have one of several fixed types. - Subtype / [Intersection Type](https://en.wikipedia.org/wiki/Type_system#Intersection_types): Variables have all the properties of the supertypes. - `Tuple` / [Product Type](https://en.wikipedia.org/wiki/Product_type): Variables contain multiple entries that each have one corresponding type.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:746,safety,log,logical,746,"> I know all that. :wink:. And I know that you know! I just like to be comprehensive when presenting my arguments! > But numpy, pandas, scikit learn, tensorflow, seaborn all have the comma-separated list as a convention and I'd really like to stick to that convention. OK. Id prefer a, b, or c, but Ill concede. It would also be no problem to change it later since all will be automated :+1: . > No, the optional keyword always means that a parameter has a default. Very often, people forget to append ""or None"" (, None) to the list of possible types. Well, when I open scanpy in PyCharm and someone forgot that in a type annotation, it highlights that fact to me. Pretty nice. > As mentioned before, there is no point in using set-theoretic/logical notions like union or intersection as the topic is so simple that it doesn't need it (no need for an intersection, it's not even clear what that would mean; if you're stringent about it, it's also not clear for union). Oh, then you didnt hear of type theory. Its a branch of logic: Type systems are formal systems, and in most of them the terms I used are well defined. The kinds of composite types I mentioned are:. - `Union` of types / Sum Type / [Tagged Union](https://en.wikipedia.org/wiki/Tagged_union): Variables with one of those have one of several fixed types. - Subtype / [Intersection Type](https://en.wikipedia.org/wiki/Type_system#Intersection_types): Variables have all the properties of the supertypes. - `Tuple` / [Product Type](https://en.wikipedia.org/wiki/Product_type): Variables contain multiple entries that each have one corresponding type.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1031,safety,log,logic,1031,"> I know all that. :wink:. And I know that you know! I just like to be comprehensive when presenting my arguments! > But numpy, pandas, scikit learn, tensorflow, seaborn all have the comma-separated list as a convention and I'd really like to stick to that convention. OK. Id prefer a, b, or c, but Ill concede. It would also be no problem to change it later since all will be automated :+1: . > No, the optional keyword always means that a parameter has a default. Very often, people forget to append ""or None"" (, None) to the list of possible types. Well, when I open scanpy in PyCharm and someone forgot that in a type annotation, it highlights that fact to me. Pretty nice. > As mentioned before, there is no point in using set-theoretic/logical notions like union or intersection as the topic is so simple that it doesn't need it (no need for an intersection, it's not even clear what that would mean; if you're stringent about it, it's also not clear for union). Oh, then you didnt hear of type theory. Its a branch of logic: Type systems are formal systems, and in most of them the terms I used are well defined. The kinds of composite types I mentioned are:. - `Union` of types / Sum Type / [Tagged Union](https://en.wikipedia.org/wiki/Tagged_union): Variables with one of those have one of several fixed types. - Subtype / [Intersection Type](https://en.wikipedia.org/wiki/Type_system#Intersection_types): Variables have all the properties of the supertypes. - `Tuple` / [Product Type](https://en.wikipedia.org/wiki/Product_type): Variables contain multiple entries that each have one corresponding type.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:746,security,log,logical,746,"> I know all that. :wink:. And I know that you know! I just like to be comprehensive when presenting my arguments! > But numpy, pandas, scikit learn, tensorflow, seaborn all have the comma-separated list as a convention and I'd really like to stick to that convention. OK. Id prefer a, b, or c, but Ill concede. It would also be no problem to change it later since all will be automated :+1: . > No, the optional keyword always means that a parameter has a default. Very often, people forget to append ""or None"" (, None) to the list of possible types. Well, when I open scanpy in PyCharm and someone forgot that in a type annotation, it highlights that fact to me. Pretty nice. > As mentioned before, there is no point in using set-theoretic/logical notions like union or intersection as the topic is so simple that it doesn't need it (no need for an intersection, it's not even clear what that would mean; if you're stringent about it, it's also not clear for union). Oh, then you didnt hear of type theory. Its a branch of logic: Type systems are formal systems, and in most of them the terms I used are well defined. The kinds of composite types I mentioned are:. - `Union` of types / Sum Type / [Tagged Union](https://en.wikipedia.org/wiki/Tagged_union): Variables with one of those have one of several fixed types. - Subtype / [Intersection Type](https://en.wikipedia.org/wiki/Type_system#Intersection_types): Variables have all the properties of the supertypes. - `Tuple` / [Product Type](https://en.wikipedia.org/wiki/Product_type): Variables contain multiple entries that each have one corresponding type.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1031,security,log,logic,1031,"> I know all that. :wink:. And I know that you know! I just like to be comprehensive when presenting my arguments! > But numpy, pandas, scikit learn, tensorflow, seaborn all have the comma-separated list as a convention and I'd really like to stick to that convention. OK. Id prefer a, b, or c, but Ill concede. It would also be no problem to change it later since all will be automated :+1: . > No, the optional keyword always means that a parameter has a default. Very often, people forget to append ""or None"" (, None) to the list of possible types. Well, when I open scanpy in PyCharm and someone forgot that in a type annotation, it highlights that fact to me. Pretty nice. > As mentioned before, there is no point in using set-theoretic/logical notions like union or intersection as the topic is so simple that it doesn't need it (no need for an intersection, it's not even clear what that would mean; if you're stringent about it, it's also not clear for union). Oh, then you didnt hear of type theory. Its a branch of logic: Type systems are formal systems, and in most of them the terms I used are well defined. The kinds of composite types I mentioned are:. - `Union` of types / Sum Type / [Tagged Union](https://en.wikipedia.org/wiki/Tagged_union): Variables with one of those have one of several fixed types. - Subtype / [Intersection Type](https://en.wikipedia.org/wiki/Type_system#Intersection_types): Variables have all the properties of the supertypes. - `Tuple` / [Product Type](https://en.wikipedia.org/wiki/Product_type): Variables contain multiple entries that each have one corresponding type.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:381,testability,automat,automated,381,"> I know all that. :wink:. And I know that you know! I just like to be comprehensive when presenting my arguments! > But numpy, pandas, scikit learn, tensorflow, seaborn all have the comma-separated list as a convention and I'd really like to stick to that convention. OK. Id prefer a, b, or c, but Ill concede. It would also be no problem to change it later since all will be automated :+1: . > No, the optional keyword always means that a parameter has a default. Very often, people forget to append ""or None"" (, None) to the list of possible types. Well, when I open scanpy in PyCharm and someone forgot that in a type annotation, it highlights that fact to me. Pretty nice. > As mentioned before, there is no point in using set-theoretic/logical notions like union or intersection as the topic is so simple that it doesn't need it (no need for an intersection, it's not even clear what that would mean; if you're stringent about it, it's also not clear for union). Oh, then you didnt hear of type theory. Its a branch of logic: Type systems are formal systems, and in most of them the terms I used are well defined. The kinds of composite types I mentioned are:. - `Union` of types / Sum Type / [Tagged Union](https://en.wikipedia.org/wiki/Tagged_union): Variables with one of those have one of several fixed types. - Subtype / [Intersection Type](https://en.wikipedia.org/wiki/Type_system#Intersection_types): Variables have all the properties of the supertypes. - `Tuple` / [Product Type](https://en.wikipedia.org/wiki/Product_type): Variables contain multiple entries that each have one corresponding type.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:746,testability,log,logical,746,"> I know all that. :wink:. And I know that you know! I just like to be comprehensive when presenting my arguments! > But numpy, pandas, scikit learn, tensorflow, seaborn all have the comma-separated list as a convention and I'd really like to stick to that convention. OK. Id prefer a, b, or c, but Ill concede. It would also be no problem to change it later since all will be automated :+1: . > No, the optional keyword always means that a parameter has a default. Very often, people forget to append ""or None"" (, None) to the list of possible types. Well, when I open scanpy in PyCharm and someone forgot that in a type annotation, it highlights that fact to me. Pretty nice. > As mentioned before, there is no point in using set-theoretic/logical notions like union or intersection as the topic is so simple that it doesn't need it (no need for an intersection, it's not even clear what that would mean; if you're stringent about it, it's also not clear for union). Oh, then you didnt hear of type theory. Its a branch of logic: Type systems are formal systems, and in most of them the terms I used are well defined. The kinds of composite types I mentioned are:. - `Union` of types / Sum Type / [Tagged Union](https://en.wikipedia.org/wiki/Tagged_union): Variables with one of those have one of several fixed types. - Subtype / [Intersection Type](https://en.wikipedia.org/wiki/Type_system#Intersection_types): Variables have all the properties of the supertypes. - `Tuple` / [Product Type](https://en.wikipedia.org/wiki/Product_type): Variables contain multiple entries that each have one corresponding type.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:808,testability,simpl,simple,808,"> I know all that. :wink:. And I know that you know! I just like to be comprehensive when presenting my arguments! > But numpy, pandas, scikit learn, tensorflow, seaborn all have the comma-separated list as a convention and I'd really like to stick to that convention. OK. Id prefer a, b, or c, but Ill concede. It would also be no problem to change it later since all will be automated :+1: . > No, the optional keyword always means that a parameter has a default. Very often, people forget to append ""or None"" (, None) to the list of possible types. Well, when I open scanpy in PyCharm and someone forgot that in a type annotation, it highlights that fact to me. Pretty nice. > As mentioned before, there is no point in using set-theoretic/logical notions like union or intersection as the topic is so simple that it doesn't need it (no need for an intersection, it's not even clear what that would mean; if you're stringent about it, it's also not clear for union). Oh, then you didnt hear of type theory. Its a branch of logic: Type systems are formal systems, and in most of them the terms I used are well defined. The kinds of composite types I mentioned are:. - `Union` of types / Sum Type / [Tagged Union](https://en.wikipedia.org/wiki/Tagged_union): Variables with one of those have one of several fixed types. - Subtype / [Intersection Type](https://en.wikipedia.org/wiki/Type_system#Intersection_types): Variables have all the properties of the supertypes. - `Tuple` / [Product Type](https://en.wikipedia.org/wiki/Product_type): Variables contain multiple entries that each have one corresponding type.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1031,testability,log,logic,1031,"> I know all that. :wink:. And I know that you know! I just like to be comprehensive when presenting my arguments! > But numpy, pandas, scikit learn, tensorflow, seaborn all have the comma-separated list as a convention and I'd really like to stick to that convention. OK. Id prefer a, b, or c, but Ill concede. It would also be no problem to change it later since all will be automated :+1: . > No, the optional keyword always means that a parameter has a default. Very often, people forget to append ""or None"" (, None) to the list of possible types. Well, when I open scanpy in PyCharm and someone forgot that in a type annotation, it highlights that fact to me. Pretty nice. > As mentioned before, there is no point in using set-theoretic/logical notions like union or intersection as the topic is so simple that it doesn't need it (no need for an intersection, it's not even clear what that would mean; if you're stringent about it, it's also not clear for union). Oh, then you didnt hear of type theory. Its a branch of logic: Type systems are formal systems, and in most of them the terms I used are well defined. The kinds of composite types I mentioned are:. - `Union` of types / Sum Type / [Tagged Union](https://en.wikipedia.org/wiki/Tagged_union): Variables with one of those have one of several fixed types. - Subtype / [Intersection Type](https://en.wikipedia.org/wiki/Type_system#Intersection_types): Variables have all the properties of the supertypes. - `Tuple` / [Product Type](https://en.wikipedia.org/wiki/Product_type): Variables contain multiple entries that each have one corresponding type.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:143,usability,learn,learn,143,"> I know all that. :wink:. And I know that you know! I just like to be comprehensive when presenting my arguments! > But numpy, pandas, scikit learn, tensorflow, seaborn all have the comma-separated list as a convention and I'd really like to stick to that convention. OK. Id prefer a, b, or c, but Ill concede. It would also be no problem to change it later since all will be automated :+1: . > No, the optional keyword always means that a parameter has a default. Very often, people forget to append ""or None"" (, None) to the list of possible types. Well, when I open scanpy in PyCharm and someone forgot that in a type annotation, it highlights that fact to me. Pretty nice. > As mentioned before, there is no point in using set-theoretic/logical notions like union or intersection as the topic is so simple that it doesn't need it (no need for an intersection, it's not even clear what that would mean; if you're stringent about it, it's also not clear for union). Oh, then you didnt hear of type theory. Its a branch of logic: Type systems are formal systems, and in most of them the terms I used are well defined. The kinds of composite types I mentioned are:. - `Union` of types / Sum Type / [Tagged Union](https://en.wikipedia.org/wiki/Tagged_union): Variables with one of those have one of several fixed types. - Subtype / [Intersection Type](https://en.wikipedia.org/wiki/Type_system#Intersection_types): Variables have all the properties of the supertypes. - `Tuple` / [Product Type](https://en.wikipedia.org/wiki/Product_type): Variables contain multiple entries that each have one corresponding type.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:277,usability,prefer,prefer,277,"> I know all that. :wink:. And I know that you know! I just like to be comprehensive when presenting my arguments! > But numpy, pandas, scikit learn, tensorflow, seaborn all have the comma-separated list as a convention and I'd really like to stick to that convention. OK. Id prefer a, b, or c, but Ill concede. It would also be no problem to change it later since all will be automated :+1: . > No, the optional keyword always means that a parameter has a default. Very often, people forget to append ""or None"" (, None) to the list of possible types. Well, when I open scanpy in PyCharm and someone forgot that in a type annotation, it highlights that fact to me. Pretty nice. > As mentioned before, there is no point in using set-theoretic/logical notions like union or intersection as the topic is so simple that it doesn't need it (no need for an intersection, it's not even clear what that would mean; if you're stringent about it, it's also not clear for union). Oh, then you didnt hear of type theory. Its a branch of logic: Type systems are formal systems, and in most of them the terms I used are well defined. The kinds of composite types I mentioned are:. - `Union` of types / Sum Type / [Tagged Union](https://en.wikipedia.org/wiki/Tagged_union): Variables with one of those have one of several fixed types. - Subtype / [Intersection Type](https://en.wikipedia.org/wiki/Type_system#Intersection_types): Variables have all the properties of the supertypes. - `Tuple` / [Product Type](https://en.wikipedia.org/wiki/Product_type): Variables contain multiple entries that each have one corresponding type.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:808,usability,simpl,simple,808,"> I know all that. :wink:. And I know that you know! I just like to be comprehensive when presenting my arguments! > But numpy, pandas, scikit learn, tensorflow, seaborn all have the comma-separated list as a convention and I'd really like to stick to that convention. OK. Id prefer a, b, or c, but Ill concede. It would also be no problem to change it later since all will be automated :+1: . > No, the optional keyword always means that a parameter has a default. Very often, people forget to append ""or None"" (, None) to the list of possible types. Well, when I open scanpy in PyCharm and someone forgot that in a type annotation, it highlights that fact to me. Pretty nice. > As mentioned before, there is no point in using set-theoretic/logical notions like union or intersection as the topic is so simple that it doesn't need it (no need for an intersection, it's not even clear what that would mean; if you're stringent about it, it's also not clear for union). Oh, then you didnt hear of type theory. Its a branch of logic: Type systems are formal systems, and in most of them the terms I used are well defined. The kinds of composite types I mentioned are:. - `Union` of types / Sum Type / [Tagged Union](https://en.wikipedia.org/wiki/Tagged_union): Variables with one of those have one of several fixed types. - Subtype / [Intersection Type](https://en.wikipedia.org/wiki/Type_system#Intersection_types): Variables have all the properties of the supertypes. - `Tuple` / [Product Type](https://en.wikipedia.org/wiki/Product_type): Variables contain multiple entries that each have one corresponding type.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:883,usability,clear,clear,883,"> I know all that. :wink:. And I know that you know! I just like to be comprehensive when presenting my arguments! > But numpy, pandas, scikit learn, tensorflow, seaborn all have the comma-separated list as a convention and I'd really like to stick to that convention. OK. Id prefer a, b, or c, but Ill concede. It would also be no problem to change it later since all will be automated :+1: . > No, the optional keyword always means that a parameter has a default. Very often, people forget to append ""or None"" (, None) to the list of possible types. Well, when I open scanpy in PyCharm and someone forgot that in a type annotation, it highlights that fact to me. Pretty nice. > As mentioned before, there is no point in using set-theoretic/logical notions like union or intersection as the topic is so simple that it doesn't need it (no need for an intersection, it's not even clear what that would mean; if you're stringent about it, it's also not clear for union). Oh, then you didnt hear of type theory. Its a branch of logic: Type systems are formal systems, and in most of them the terms I used are well defined. The kinds of composite types I mentioned are:. - `Union` of types / Sum Type / [Tagged Union](https://en.wikipedia.org/wiki/Tagged_union): Variables with one of those have one of several fixed types. - Subtype / [Intersection Type](https://en.wikipedia.org/wiki/Type_system#Intersection_types): Variables have all the properties of the supertypes. - `Tuple` / [Product Type](https://en.wikipedia.org/wiki/Product_type): Variables contain multiple entries that each have one corresponding type.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:955,usability,clear,clear,955,"> I know all that. :wink:. And I know that you know! I just like to be comprehensive when presenting my arguments! > But numpy, pandas, scikit learn, tensorflow, seaborn all have the comma-separated list as a convention and I'd really like to stick to that convention. OK. Id prefer a, b, or c, but Ill concede. It would also be no problem to change it later since all will be automated :+1: . > No, the optional keyword always means that a parameter has a default. Very often, people forget to append ""or None"" (, None) to the list of possible types. Well, when I open scanpy in PyCharm and someone forgot that in a type annotation, it highlights that fact to me. Pretty nice. > As mentioned before, there is no point in using set-theoretic/logical notions like union or intersection as the topic is so simple that it doesn't need it (no need for an intersection, it's not even clear what that would mean; if you're stringent about it, it's also not clear for union). Oh, then you didnt hear of type theory. Its a branch of logic: Type systems are formal systems, and in most of them the terms I used are well defined. The kinds of composite types I mentioned are:. - `Union` of types / Sum Type / [Tagged Union](https://en.wikipedia.org/wiki/Tagged_union): Variables with one of those have one of several fixed types. - Subtype / [Intersection Type](https://en.wikipedia.org/wiki/Type_system#Intersection_types): Variables have all the properties of the supertypes. - `Tuple` / [Product Type](https://en.wikipedia.org/wiki/Product_type): Variables contain multiple entries that each have one corresponding type.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:60,deployability,log,logic,60,"> Oh, then you didnt hear of type theory. Its a branch of logic:. Indeed, I have never heard of that. But I doubt that it would be considered a branch of logic. 1. `Union type` is a pretty bad descriptor for a variable that can take _one_ of a set of fixed types. A union usually denotes a composition of multiple sets giving rise to a new set that contains all elements from these sets. 2. `Subtype` is a great descriptor for a type that has properties of supertypes. 3. `Intersection type` is an insanely bad descriptor for a variable that denotes the intersection of _properties_ of supertypes; the concept of such a subtype might be something useful in some languages and some cases and it might deserve a special name as it's the converse behavior of subclassing. But I have no idea how such a type would be useful in Python and in all cases that I've encountered. The [example on Wikipedia](https://en.wikipedia.org/wiki/Type_system#Intersection_types) already constructs a highly artificial case, whose relevance is opaque to me even though Scanpy features it in many instances: functions that overload parameters and have different overloading-dependent return types (standard example is passing an array instead of an AnnData, which triggers the automatic return of the computed annotation). What do you think about 3, @flying-sheep?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:156,deployability,log,logic,156,"> Oh, then you didnt hear of type theory. Its a branch of logic:. Indeed, I have never heard of that. But I doubt that it would be considered a branch of logic. 1. `Union type` is a pretty bad descriptor for a variable that can take _one_ of a set of fixed types. A union usually denotes a composition of multiple sets giving rise to a new set that contains all elements from these sets. 2. `Subtype` is a great descriptor for a type that has properties of supertypes. 3. `Intersection type` is an insanely bad descriptor for a variable that denotes the intersection of _properties_ of supertypes; the concept of such a subtype might be something useful in some languages and some cases and it might deserve a special name as it's the converse behavior of subclassing. But I have no idea how such a type would be useful in Python and in all cases that I've encountered. The [example on Wikipedia](https://en.wikipedia.org/wiki/Type_system#Intersection_types) already constructs a highly artificial case, whose relevance is opaque to me even though Scanpy features it in many instances: functions that overload parameters and have different overloading-dependent return types (standard example is passing an array instead of an AnnData, which triggers the automatic return of the computed annotation). What do you think about 3, @flying-sheep?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:292,deployability,compos,composition,292,"> Oh, then you didnt hear of type theory. Its a branch of logic:. Indeed, I have never heard of that. But I doubt that it would be considered a branch of logic. 1. `Union type` is a pretty bad descriptor for a variable that can take _one_ of a set of fixed types. A union usually denotes a composition of multiple sets giving rise to a new set that contains all elements from these sets. 2. `Subtype` is a great descriptor for a type that has properties of supertypes. 3. `Intersection type` is an insanely bad descriptor for a variable that denotes the intersection of _properties_ of supertypes; the concept of such a subtype might be something useful in some languages and some cases and it might deserve a special name as it's the converse behavior of subclassing. But I have no idea how such a type would be useful in Python and in all cases that I've encountered. The [example on Wikipedia](https://en.wikipedia.org/wiki/Type_system#Intersection_types) already constructs a highly artificial case, whose relevance is opaque to me even though Scanpy features it in many instances: functions that overload parameters and have different overloading-dependent return types (standard example is passing an array instead of an AnnData, which triggers the automatic return of the computed annotation). What do you think about 3, @flying-sheep?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:351,deployability,contain,contains,351,"> Oh, then you didnt hear of type theory. Its a branch of logic:. Indeed, I have never heard of that. But I doubt that it would be considered a branch of logic. 1. `Union type` is a pretty bad descriptor for a variable that can take _one_ of a set of fixed types. A union usually denotes a composition of multiple sets giving rise to a new set that contains all elements from these sets. 2. `Subtype` is a great descriptor for a type that has properties of supertypes. 3. `Intersection type` is an insanely bad descriptor for a variable that denotes the intersection of _properties_ of supertypes; the concept of such a subtype might be something useful in some languages and some cases and it might deserve a special name as it's the converse behavior of subclassing. But I have no idea how such a type would be useful in Python and in all cases that I've encountered. The [example on Wikipedia](https://en.wikipedia.org/wiki/Type_system#Intersection_types) already constructs a highly artificial case, whose relevance is opaque to me even though Scanpy features it in many instances: functions that overload parameters and have different overloading-dependent return types (standard example is passing an array instead of an AnnData, which triggers the automatic return of the computed annotation). What do you think about 3, @flying-sheep?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1154,deployability,depend,dependent,1154,"> Oh, then you didnt hear of type theory. Its a branch of logic:. Indeed, I have never heard of that. But I doubt that it would be considered a branch of logic. 1. `Union type` is a pretty bad descriptor for a variable that can take _one_ of a set of fixed types. A union usually denotes a composition of multiple sets giving rise to a new set that contains all elements from these sets. 2. `Subtype` is a great descriptor for a type that has properties of supertypes. 3. `Intersection type` is an insanely bad descriptor for a variable that denotes the intersection of _properties_ of supertypes; the concept of such a subtype might be something useful in some languages and some cases and it might deserve a special name as it's the converse behavior of subclassing. But I have no idea how such a type would be useful in Python and in all cases that I've encountered. The [example on Wikipedia](https://en.wikipedia.org/wiki/Type_system#Intersection_types) already constructs a highly artificial case, whose relevance is opaque to me even though Scanpy features it in many instances: functions that overload parameters and have different overloading-dependent return types (standard example is passing an array instead of an AnnData, which triggers the automatic return of the computed annotation). What do you think about 3, @flying-sheep?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1257,deployability,automat,automatic,1257,"> Oh, then you didnt hear of type theory. Its a branch of logic:. Indeed, I have never heard of that. But I doubt that it would be considered a branch of logic. 1. `Union type` is a pretty bad descriptor for a variable that can take _one_ of a set of fixed types. A union usually denotes a composition of multiple sets giving rise to a new set that contains all elements from these sets. 2. `Subtype` is a great descriptor for a type that has properties of supertypes. 3. `Intersection type` is an insanely bad descriptor for a variable that denotes the intersection of _properties_ of supertypes; the concept of such a subtype might be something useful in some languages and some cases and it might deserve a special name as it's the converse behavior of subclassing. But I have no idea how such a type would be useful in Python and in all cases that I've encountered. The [example on Wikipedia](https://en.wikipedia.org/wiki/Type_system#Intersection_types) already constructs a highly artificial case, whose relevance is opaque to me even though Scanpy features it in many instances: functions that overload parameters and have different overloading-dependent return types (standard example is passing an array instead of an AnnData, which triggers the automatic return of the computed annotation). What do you think about 3, @flying-sheep?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:394,integrability,Sub,Subtype,394,"> Oh, then you didnt hear of type theory. Its a branch of logic:. Indeed, I have never heard of that. But I doubt that it would be considered a branch of logic. 1. `Union type` is a pretty bad descriptor for a variable that can take _one_ of a set of fixed types. A union usually denotes a composition of multiple sets giving rise to a new set that contains all elements from these sets. 2. `Subtype` is a great descriptor for a type that has properties of supertypes. 3. `Intersection type` is an insanely bad descriptor for a variable that denotes the intersection of _properties_ of supertypes; the concept of such a subtype might be something useful in some languages and some cases and it might deserve a special name as it's the converse behavior of subclassing. But I have no idea how such a type would be useful in Python and in all cases that I've encountered. The [example on Wikipedia](https://en.wikipedia.org/wiki/Type_system#Intersection_types) already constructs a highly artificial case, whose relevance is opaque to me even though Scanpy features it in many instances: functions that overload parameters and have different overloading-dependent return types (standard example is passing an array instead of an AnnData, which triggers the automatic return of the computed annotation). What do you think about 3, @flying-sheep?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:622,integrability,sub,subtype,622,"> Oh, then you didnt hear of type theory. Its a branch of logic:. Indeed, I have never heard of that. But I doubt that it would be considered a branch of logic. 1. `Union type` is a pretty bad descriptor for a variable that can take _one_ of a set of fixed types. A union usually denotes a composition of multiple sets giving rise to a new set that contains all elements from these sets. 2. `Subtype` is a great descriptor for a type that has properties of supertypes. 3. `Intersection type` is an insanely bad descriptor for a variable that denotes the intersection of _properties_ of supertypes; the concept of such a subtype might be something useful in some languages and some cases and it might deserve a special name as it's the converse behavior of subclassing. But I have no idea how such a type would be useful in Python and in all cases that I've encountered. The [example on Wikipedia](https://en.wikipedia.org/wiki/Type_system#Intersection_types) already constructs a highly artificial case, whose relevance is opaque to me even though Scanpy features it in many instances: functions that overload parameters and have different overloading-dependent return types (standard example is passing an array instead of an AnnData, which triggers the automatic return of the computed annotation). What do you think about 3, @flying-sheep?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:758,integrability,sub,subclassing,758,"> Oh, then you didnt hear of type theory. Its a branch of logic:. Indeed, I have never heard of that. But I doubt that it would be considered a branch of logic. 1. `Union type` is a pretty bad descriptor for a variable that can take _one_ of a set of fixed types. A union usually denotes a composition of multiple sets giving rise to a new set that contains all elements from these sets. 2. `Subtype` is a great descriptor for a type that has properties of supertypes. 3. `Intersection type` is an insanely bad descriptor for a variable that denotes the intersection of _properties_ of supertypes; the concept of such a subtype might be something useful in some languages and some cases and it might deserve a special name as it's the converse behavior of subclassing. But I have no idea how such a type would be useful in Python and in all cases that I've encountered. The [example on Wikipedia](https://en.wikipedia.org/wiki/Type_system#Intersection_types) already constructs a highly artificial case, whose relevance is opaque to me even though Scanpy features it in many instances: functions that overload parameters and have different overloading-dependent return types (standard example is passing an array instead of an AnnData, which triggers the automatic return of the computed annotation). What do you think about 3, @flying-sheep?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1154,integrability,depend,dependent,1154,"> Oh, then you didnt hear of type theory. Its a branch of logic:. Indeed, I have never heard of that. But I doubt that it would be considered a branch of logic. 1. `Union type` is a pretty bad descriptor for a variable that can take _one_ of a set of fixed types. A union usually denotes a composition of multiple sets giving rise to a new set that contains all elements from these sets. 2. `Subtype` is a great descriptor for a type that has properties of supertypes. 3. `Intersection type` is an insanely bad descriptor for a variable that denotes the intersection of _properties_ of supertypes; the concept of such a subtype might be something useful in some languages and some cases and it might deserve a special name as it's the converse behavior of subclassing. But I have no idea how such a type would be useful in Python and in all cases that I've encountered. The [example on Wikipedia](https://en.wikipedia.org/wiki/Type_system#Intersection_types) already constructs a highly artificial case, whose relevance is opaque to me even though Scanpy features it in many instances: functions that overload parameters and have different overloading-dependent return types (standard example is passing an array instead of an AnnData, which triggers the automatic return of the computed annotation). What do you think about 3, @flying-sheep?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:737,interoperability,convers,converse,737,"> Oh, then you didnt hear of type theory. Its a branch of logic:. Indeed, I have never heard of that. But I doubt that it would be considered a branch of logic. 1. `Union type` is a pretty bad descriptor for a variable that can take _one_ of a set of fixed types. A union usually denotes a composition of multiple sets giving rise to a new set that contains all elements from these sets. 2. `Subtype` is a great descriptor for a type that has properties of supertypes. 3. `Intersection type` is an insanely bad descriptor for a variable that denotes the intersection of _properties_ of supertypes; the concept of such a subtype might be something useful in some languages and some cases and it might deserve a special name as it's the converse behavior of subclassing. But I have no idea how such a type would be useful in Python and in all cases that I've encountered. The [example on Wikipedia](https://en.wikipedia.org/wiki/Type_system#Intersection_types) already constructs a highly artificial case, whose relevance is opaque to me even though Scanpy features it in many instances: functions that overload parameters and have different overloading-dependent return types (standard example is passing an array instead of an AnnData, which triggers the automatic return of the computed annotation). What do you think about 3, @flying-sheep?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1178,interoperability,standard,standard,1178,"> Oh, then you didnt hear of type theory. Its a branch of logic:. Indeed, I have never heard of that. But I doubt that it would be considered a branch of logic. 1. `Union type` is a pretty bad descriptor for a variable that can take _one_ of a set of fixed types. A union usually denotes a composition of multiple sets giving rise to a new set that contains all elements from these sets. 2. `Subtype` is a great descriptor for a type that has properties of supertypes. 3. `Intersection type` is an insanely bad descriptor for a variable that denotes the intersection of _properties_ of supertypes; the concept of such a subtype might be something useful in some languages and some cases and it might deserve a special name as it's the converse behavior of subclassing. But I have no idea how such a type would be useful in Python and in all cases that I've encountered. The [example on Wikipedia](https://en.wikipedia.org/wiki/Type_system#Intersection_types) already constructs a highly artificial case, whose relevance is opaque to me even though Scanpy features it in many instances: functions that overload parameters and have different overloading-dependent return types (standard example is passing an array instead of an AnnData, which triggers the automatic return of the computed annotation). What do you think about 3, @flying-sheep?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:212,modifiability,variab,variable,212,"> Oh, then you didnt hear of type theory. Its a branch of logic:. Indeed, I have never heard of that. But I doubt that it would be considered a branch of logic. 1. `Union type` is a pretty bad descriptor for a variable that can take _one_ of a set of fixed types. A union usually denotes a composition of multiple sets giving rise to a new set that contains all elements from these sets. 2. `Subtype` is a great descriptor for a type that has properties of supertypes. 3. `Intersection type` is an insanely bad descriptor for a variable that denotes the intersection of _properties_ of supertypes; the concept of such a subtype might be something useful in some languages and some cases and it might deserve a special name as it's the converse behavior of subclassing. But I have no idea how such a type would be useful in Python and in all cases that I've encountered. The [example on Wikipedia](https://en.wikipedia.org/wiki/Type_system#Intersection_types) already constructs a highly artificial case, whose relevance is opaque to me even though Scanpy features it in many instances: functions that overload parameters and have different overloading-dependent return types (standard example is passing an array instead of an AnnData, which triggers the automatic return of the computed annotation). What do you think about 3, @flying-sheep?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:292,modifiability,compos,composition,292,"> Oh, then you didnt hear of type theory. Its a branch of logic:. Indeed, I have never heard of that. But I doubt that it would be considered a branch of logic. 1. `Union type` is a pretty bad descriptor for a variable that can take _one_ of a set of fixed types. A union usually denotes a composition of multiple sets giving rise to a new set that contains all elements from these sets. 2. `Subtype` is a great descriptor for a type that has properties of supertypes. 3. `Intersection type` is an insanely bad descriptor for a variable that denotes the intersection of _properties_ of supertypes; the concept of such a subtype might be something useful in some languages and some cases and it might deserve a special name as it's the converse behavior of subclassing. But I have no idea how such a type would be useful in Python and in all cases that I've encountered. The [example on Wikipedia](https://en.wikipedia.org/wiki/Type_system#Intersection_types) already constructs a highly artificial case, whose relevance is opaque to me even though Scanpy features it in many instances: functions that overload parameters and have different overloading-dependent return types (standard example is passing an array instead of an AnnData, which triggers the automatic return of the computed annotation). What do you think about 3, @flying-sheep?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:530,modifiability,variab,variable,530,"> Oh, then you didnt hear of type theory. Its a branch of logic:. Indeed, I have never heard of that. But I doubt that it would be considered a branch of logic. 1. `Union type` is a pretty bad descriptor for a variable that can take _one_ of a set of fixed types. A union usually denotes a composition of multiple sets giving rise to a new set that contains all elements from these sets. 2. `Subtype` is a great descriptor for a type that has properties of supertypes. 3. `Intersection type` is an insanely bad descriptor for a variable that denotes the intersection of _properties_ of supertypes; the concept of such a subtype might be something useful in some languages and some cases and it might deserve a special name as it's the converse behavior of subclassing. But I have no idea how such a type would be useful in Python and in all cases that I've encountered. The [example on Wikipedia](https://en.wikipedia.org/wiki/Type_system#Intersection_types) already constructs a highly artificial case, whose relevance is opaque to me even though Scanpy features it in many instances: functions that overload parameters and have different overloading-dependent return types (standard example is passing an array instead of an AnnData, which triggers the automatic return of the computed annotation). What do you think about 3, @flying-sheep?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1112,modifiability,paramet,parameters,1112,"> Oh, then you didnt hear of type theory. Its a branch of logic:. Indeed, I have never heard of that. But I doubt that it would be considered a branch of logic. 1. `Union type` is a pretty bad descriptor for a variable that can take _one_ of a set of fixed types. A union usually denotes a composition of multiple sets giving rise to a new set that contains all elements from these sets. 2. `Subtype` is a great descriptor for a type that has properties of supertypes. 3. `Intersection type` is an insanely bad descriptor for a variable that denotes the intersection of _properties_ of supertypes; the concept of such a subtype might be something useful in some languages and some cases and it might deserve a special name as it's the converse behavior of subclassing. But I have no idea how such a type would be useful in Python and in all cases that I've encountered. The [example on Wikipedia](https://en.wikipedia.org/wiki/Type_system#Intersection_types) already constructs a highly artificial case, whose relevance is opaque to me even though Scanpy features it in many instances: functions that overload parameters and have different overloading-dependent return types (standard example is passing an array instead of an AnnData, which triggers the automatic return of the computed annotation). What do you think about 3, @flying-sheep?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1154,modifiability,depend,dependent,1154,"> Oh, then you didnt hear of type theory. Its a branch of logic:. Indeed, I have never heard of that. But I doubt that it would be considered a branch of logic. 1. `Union type` is a pretty bad descriptor for a variable that can take _one_ of a set of fixed types. A union usually denotes a composition of multiple sets giving rise to a new set that contains all elements from these sets. 2. `Subtype` is a great descriptor for a type that has properties of supertypes. 3. `Intersection type` is an insanely bad descriptor for a variable that denotes the intersection of _properties_ of supertypes; the concept of such a subtype might be something useful in some languages and some cases and it might deserve a special name as it's the converse behavior of subclassing. But I have no idea how such a type would be useful in Python and in all cases that I've encountered. The [example on Wikipedia](https://en.wikipedia.org/wiki/Type_system#Intersection_types) already constructs a highly artificial case, whose relevance is opaque to me even though Scanpy features it in many instances: functions that overload parameters and have different overloading-dependent return types (standard example is passing an array instead of an AnnData, which triggers the automatic return of the computed annotation). What do you think about 3, @flying-sheep?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:60,safety,log,logic,60,"> Oh, then you didnt hear of type theory. Its a branch of logic:. Indeed, I have never heard of that. But I doubt that it would be considered a branch of logic. 1. `Union type` is a pretty bad descriptor for a variable that can take _one_ of a set of fixed types. A union usually denotes a composition of multiple sets giving rise to a new set that contains all elements from these sets. 2. `Subtype` is a great descriptor for a type that has properties of supertypes. 3. `Intersection type` is an insanely bad descriptor for a variable that denotes the intersection of _properties_ of supertypes; the concept of such a subtype might be something useful in some languages and some cases and it might deserve a special name as it's the converse behavior of subclassing. But I have no idea how such a type would be useful in Python and in all cases that I've encountered. The [example on Wikipedia](https://en.wikipedia.org/wiki/Type_system#Intersection_types) already constructs a highly artificial case, whose relevance is opaque to me even though Scanpy features it in many instances: functions that overload parameters and have different overloading-dependent return types (standard example is passing an array instead of an AnnData, which triggers the automatic return of the computed annotation). What do you think about 3, @flying-sheep?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:156,safety,log,logic,156,"> Oh, then you didnt hear of type theory. Its a branch of logic:. Indeed, I have never heard of that. But I doubt that it would be considered a branch of logic. 1. `Union type` is a pretty bad descriptor for a variable that can take _one_ of a set of fixed types. A union usually denotes a composition of multiple sets giving rise to a new set that contains all elements from these sets. 2. `Subtype` is a great descriptor for a type that has properties of supertypes. 3. `Intersection type` is an insanely bad descriptor for a variable that denotes the intersection of _properties_ of supertypes; the concept of such a subtype might be something useful in some languages and some cases and it might deserve a special name as it's the converse behavior of subclassing. But I have no idea how such a type would be useful in Python and in all cases that I've encountered. The [example on Wikipedia](https://en.wikipedia.org/wiki/Type_system#Intersection_types) already constructs a highly artificial case, whose relevance is opaque to me even though Scanpy features it in many instances: functions that overload parameters and have different overloading-dependent return types (standard example is passing an array instead of an AnnData, which triggers the automatic return of the computed annotation). What do you think about 3, @flying-sheep?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1154,safety,depend,dependent,1154,"> Oh, then you didnt hear of type theory. Its a branch of logic:. Indeed, I have never heard of that. But I doubt that it would be considered a branch of logic. 1. `Union type` is a pretty bad descriptor for a variable that can take _one_ of a set of fixed types. A union usually denotes a composition of multiple sets giving rise to a new set that contains all elements from these sets. 2. `Subtype` is a great descriptor for a type that has properties of supertypes. 3. `Intersection type` is an insanely bad descriptor for a variable that denotes the intersection of _properties_ of supertypes; the concept of such a subtype might be something useful in some languages and some cases and it might deserve a special name as it's the converse behavior of subclassing. But I have no idea how such a type would be useful in Python and in all cases that I've encountered. The [example on Wikipedia](https://en.wikipedia.org/wiki/Type_system#Intersection_types) already constructs a highly artificial case, whose relevance is opaque to me even though Scanpy features it in many instances: functions that overload parameters and have different overloading-dependent return types (standard example is passing an array instead of an AnnData, which triggers the automatic return of the computed annotation). What do you think about 3, @flying-sheep?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:60,security,log,logic,60,"> Oh, then you didnt hear of type theory. Its a branch of logic:. Indeed, I have never heard of that. But I doubt that it would be considered a branch of logic. 1. `Union type` is a pretty bad descriptor for a variable that can take _one_ of a set of fixed types. A union usually denotes a composition of multiple sets giving rise to a new set that contains all elements from these sets. 2. `Subtype` is a great descriptor for a type that has properties of supertypes. 3. `Intersection type` is an insanely bad descriptor for a variable that denotes the intersection of _properties_ of supertypes; the concept of such a subtype might be something useful in some languages and some cases and it might deserve a special name as it's the converse behavior of subclassing. But I have no idea how such a type would be useful in Python and in all cases that I've encountered. The [example on Wikipedia](https://en.wikipedia.org/wiki/Type_system#Intersection_types) already constructs a highly artificial case, whose relevance is opaque to me even though Scanpy features it in many instances: functions that overload parameters and have different overloading-dependent return types (standard example is passing an array instead of an AnnData, which triggers the automatic return of the computed annotation). What do you think about 3, @flying-sheep?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:156,security,log,logic,156,"> Oh, then you didnt hear of type theory. Its a branch of logic:. Indeed, I have never heard of that. But I doubt that it would be considered a branch of logic. 1. `Union type` is a pretty bad descriptor for a variable that can take _one_ of a set of fixed types. A union usually denotes a composition of multiple sets giving rise to a new set that contains all elements from these sets. 2. `Subtype` is a great descriptor for a type that has properties of supertypes. 3. `Intersection type` is an insanely bad descriptor for a variable that denotes the intersection of _properties_ of supertypes; the concept of such a subtype might be something useful in some languages and some cases and it might deserve a special name as it's the converse behavior of subclassing. But I have no idea how such a type would be useful in Python and in all cases that I've encountered. The [example on Wikipedia](https://en.wikipedia.org/wiki/Type_system#Intersection_types) already constructs a highly artificial case, whose relevance is opaque to me even though Scanpy features it in many instances: functions that overload parameters and have different overloading-dependent return types (standard example is passing an array instead of an AnnData, which triggers the automatic return of the computed annotation). What do you think about 3, @flying-sheep?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:60,testability,log,logic,60,"> Oh, then you didnt hear of type theory. Its a branch of logic:. Indeed, I have never heard of that. But I doubt that it would be considered a branch of logic. 1. `Union type` is a pretty bad descriptor for a variable that can take _one_ of a set of fixed types. A union usually denotes a composition of multiple sets giving rise to a new set that contains all elements from these sets. 2. `Subtype` is a great descriptor for a type that has properties of supertypes. 3. `Intersection type` is an insanely bad descriptor for a variable that denotes the intersection of _properties_ of supertypes; the concept of such a subtype might be something useful in some languages and some cases and it might deserve a special name as it's the converse behavior of subclassing. But I have no idea how such a type would be useful in Python and in all cases that I've encountered. The [example on Wikipedia](https://en.wikipedia.org/wiki/Type_system#Intersection_types) already constructs a highly artificial case, whose relevance is opaque to me even though Scanpy features it in many instances: functions that overload parameters and have different overloading-dependent return types (standard example is passing an array instead of an AnnData, which triggers the automatic return of the computed annotation). What do you think about 3, @flying-sheep?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:156,testability,log,logic,156,"> Oh, then you didnt hear of type theory. Its a branch of logic:. Indeed, I have never heard of that. But I doubt that it would be considered a branch of logic. 1. `Union type` is a pretty bad descriptor for a variable that can take _one_ of a set of fixed types. A union usually denotes a composition of multiple sets giving rise to a new set that contains all elements from these sets. 2. `Subtype` is a great descriptor for a type that has properties of supertypes. 3. `Intersection type` is an insanely bad descriptor for a variable that denotes the intersection of _properties_ of supertypes; the concept of such a subtype might be something useful in some languages and some cases and it might deserve a special name as it's the converse behavior of subclassing. But I have no idea how such a type would be useful in Python and in all cases that I've encountered. The [example on Wikipedia](https://en.wikipedia.org/wiki/Type_system#Intersection_types) already constructs a highly artificial case, whose relevance is opaque to me even though Scanpy features it in many instances: functions that overload parameters and have different overloading-dependent return types (standard example is passing an array instead of an AnnData, which triggers the automatic return of the computed annotation). What do you think about 3, @flying-sheep?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1154,testability,depend,dependent,1154,"> Oh, then you didnt hear of type theory. Its a branch of logic:. Indeed, I have never heard of that. But I doubt that it would be considered a branch of logic. 1. `Union type` is a pretty bad descriptor for a variable that can take _one_ of a set of fixed types. A union usually denotes a composition of multiple sets giving rise to a new set that contains all elements from these sets. 2. `Subtype` is a great descriptor for a type that has properties of supertypes. 3. `Intersection type` is an insanely bad descriptor for a variable that denotes the intersection of _properties_ of supertypes; the concept of such a subtype might be something useful in some languages and some cases and it might deserve a special name as it's the converse behavior of subclassing. But I have no idea how such a type would be useful in Python and in all cases that I've encountered. The [example on Wikipedia](https://en.wikipedia.org/wiki/Type_system#Intersection_types) already constructs a highly artificial case, whose relevance is opaque to me even though Scanpy features it in many instances: functions that overload parameters and have different overloading-dependent return types (standard example is passing an array instead of an AnnData, which triggers the automatic return of the computed annotation). What do you think about 3, @flying-sheep?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1257,testability,automat,automatic,1257,"> Oh, then you didnt hear of type theory. Its a branch of logic:. Indeed, I have never heard of that. But I doubt that it would be considered a branch of logic. 1. `Union type` is a pretty bad descriptor for a variable that can take _one_ of a set of fixed types. A union usually denotes a composition of multiple sets giving rise to a new set that contains all elements from these sets. 2. `Subtype` is a great descriptor for a type that has properties of supertypes. 3. `Intersection type` is an insanely bad descriptor for a variable that denotes the intersection of _properties_ of supertypes; the concept of such a subtype might be something useful in some languages and some cases and it might deserve a special name as it's the converse behavior of subclassing. But I have no idea how such a type would be useful in Python and in all cases that I've encountered. The [example on Wikipedia](https://en.wikipedia.org/wiki/Type_system#Intersection_types) already constructs a highly artificial case, whose relevance is opaque to me even though Scanpy features it in many instances: functions that overload parameters and have different overloading-dependent return types (standard example is passing an array instead of an AnnData, which triggers the automatic return of the computed annotation). What do you think about 3, @flying-sheep?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:746,usability,behavi,behavior,746,"> Oh, then you didnt hear of type theory. Its a branch of logic:. Indeed, I have never heard of that. But I doubt that it would be considered a branch of logic. 1. `Union type` is a pretty bad descriptor for a variable that can take _one_ of a set of fixed types. A union usually denotes a composition of multiple sets giving rise to a new set that contains all elements from these sets. 2. `Subtype` is a great descriptor for a type that has properties of supertypes. 3. `Intersection type` is an insanely bad descriptor for a variable that denotes the intersection of _properties_ of supertypes; the concept of such a subtype might be something useful in some languages and some cases and it might deserve a special name as it's the converse behavior of subclassing. But I have no idea how such a type would be useful in Python and in all cases that I've encountered. The [example on Wikipedia](https://en.wikipedia.org/wiki/Type_system#Intersection_types) already constructs a highly artificial case, whose relevance is opaque to me even though Scanpy features it in many instances: functions that overload parameters and have different overloading-dependent return types (standard example is passing an array instead of an AnnData, which triggers the automatic return of the computed annotation). What do you think about 3, @flying-sheep?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:50,deployability,log,logic,50,"> I doubt that it would be considered a branch of logic. What do you define as logic here? I was talking about the logic theory that encompasses formal systems and so on. > [Union and intersection are bad names]. I agree, wikipedia enumerates more names, and explains where union comes from:. > tagged union, variant, variant record, choice type, discriminated union, disjoint union, or sum type. > . > Mathematically, tagged unions correspond to disjoint or discriminated unions, usually written using +. Given an element of a disjoint union A + B, it is possible to determine whether it came from A or B. If an element lies in both, there will be two effectively distinct copies of the value in A + B, one from A and one from B. . I think discriminated union/intersection of types would make sense here. leaving out the discriminated/tagged/disjoint here is the problem. in C theres actual *untagged* unions, which simply means that C reserves the memory for the largest of the intersected types and you need to keep track yourself of which the type of the value is. In python you can always do `isinstance`, so a more correct name for `Union[A, B]` would be `TaggedUnion[A, B]`. Id also like `OneOf[A, B]`, but that ship has sailed. And intersections are basically duck types or structural types (when anonymous) and traits/interfaces (when named). (i.e. `hasattr(obj, 'foo')` defines an intersection type of all types having that attribute. So it makes sense for python, its just defined more explicitly than by literally intersecting types.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:79,deployability,log,logic,79,"> I doubt that it would be considered a branch of logic. What do you define as logic here? I was talking about the logic theory that encompasses formal systems and so on. > [Union and intersection are bad names]. I agree, wikipedia enumerates more names, and explains where union comes from:. > tagged union, variant, variant record, choice type, discriminated union, disjoint union, or sum type. > . > Mathematically, tagged unions correspond to disjoint or discriminated unions, usually written using +. Given an element of a disjoint union A + B, it is possible to determine whether it came from A or B. If an element lies in both, there will be two effectively distinct copies of the value in A + B, one from A and one from B. . I think discriminated union/intersection of types would make sense here. leaving out the discriminated/tagged/disjoint here is the problem. in C theres actual *untagged* unions, which simply means that C reserves the memory for the largest of the intersected types and you need to keep track yourself of which the type of the value is. In python you can always do `isinstance`, so a more correct name for `Union[A, B]` would be `TaggedUnion[A, B]`. Id also like `OneOf[A, B]`, but that ship has sailed. And intersections are basically duck types or structural types (when anonymous) and traits/interfaces (when named). (i.e. `hasattr(obj, 'foo')` defines an intersection type of all types having that attribute. So it makes sense for python, its just defined more explicitly than by literally intersecting types.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:115,deployability,log,logic,115,"> I doubt that it would be considered a branch of logic. What do you define as logic here? I was talking about the logic theory that encompasses formal systems and so on. > [Union and intersection are bad names]. I agree, wikipedia enumerates more names, and explains where union comes from:. > tagged union, variant, variant record, choice type, discriminated union, disjoint union, or sum type. > . > Mathematically, tagged unions correspond to disjoint or discriminated unions, usually written using +. Given an element of a disjoint union A + B, it is possible to determine whether it came from A or B. If an element lies in both, there will be two effectively distinct copies of the value in A + B, one from A and one from B. . I think discriminated union/intersection of types would make sense here. leaving out the discriminated/tagged/disjoint here is the problem. in C theres actual *untagged* unions, which simply means that C reserves the memory for the largest of the intersected types and you need to keep track yourself of which the type of the value is. In python you can always do `isinstance`, so a more correct name for `Union[A, B]` would be `TaggedUnion[A, B]`. Id also like `OneOf[A, B]`, but that ship has sailed. And intersections are basically duck types or structural types (when anonymous) and traits/interfaces (when named). (i.e. `hasattr(obj, 'foo')` defines an intersection type of all types having that attribute. So it makes sense for python, its just defined more explicitly than by literally intersecting types.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1336,integrability,interfac,interfaces,1336,"> I doubt that it would be considered a branch of logic. What do you define as logic here? I was talking about the logic theory that encompasses formal systems and so on. > [Union and intersection are bad names]. I agree, wikipedia enumerates more names, and explains where union comes from:. > tagged union, variant, variant record, choice type, discriminated union, disjoint union, or sum type. > . > Mathematically, tagged unions correspond to disjoint or discriminated unions, usually written using +. Given an element of a disjoint union A + B, it is possible to determine whether it came from A or B. If an element lies in both, there will be two effectively distinct copies of the value in A + B, one from A and one from B. . I think discriminated union/intersection of types would make sense here. leaving out the discriminated/tagged/disjoint here is the problem. in C theres actual *untagged* unions, which simply means that C reserves the memory for the largest of the intersected types and you need to keep track yourself of which the type of the value is. In python you can always do `isinstance`, so a more correct name for `Union[A, B]` would be `TaggedUnion[A, B]`. Id also like `OneOf[A, B]`, but that ship has sailed. And intersections are basically duck types or structural types (when anonymous) and traits/interfaces (when named). (i.e. `hasattr(obj, 'foo')` defines an intersection type of all types having that attribute. So it makes sense for python, its just defined more explicitly than by literally intersecting types.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1336,interoperability,interfac,interfaces,1336,"> I doubt that it would be considered a branch of logic. What do you define as logic here? I was talking about the logic theory that encompasses formal systems and so on. > [Union and intersection are bad names]. I agree, wikipedia enumerates more names, and explains where union comes from:. > tagged union, variant, variant record, choice type, discriminated union, disjoint union, or sum type. > . > Mathematically, tagged unions correspond to disjoint or discriminated unions, usually written using +. Given an element of a disjoint union A + B, it is possible to determine whether it came from A or B. If an element lies in both, there will be two effectively distinct copies of the value in A + B, one from A and one from B. . I think discriminated union/intersection of types would make sense here. leaving out the discriminated/tagged/disjoint here is the problem. in C theres actual *untagged* unions, which simply means that C reserves the memory for the largest of the intersected types and you need to keep track yourself of which the type of the value is. In python you can always do `isinstance`, so a more correct name for `Union[A, B]` would be `TaggedUnion[A, B]`. Id also like `OneOf[A, B]`, but that ship has sailed. And intersections are basically duck types or structural types (when anonymous) and traits/interfaces (when named). (i.e. `hasattr(obj, 'foo')` defines an intersection type of all types having that attribute. So it makes sense for python, its just defined more explicitly than by literally intersecting types.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1336,modifiability,interfac,interfaces,1336,"> I doubt that it would be considered a branch of logic. What do you define as logic here? I was talking about the logic theory that encompasses formal systems and so on. > [Union and intersection are bad names]. I agree, wikipedia enumerates more names, and explains where union comes from:. > tagged union, variant, variant record, choice type, discriminated union, disjoint union, or sum type. > . > Mathematically, tagged unions correspond to disjoint or discriminated unions, usually written using +. Given an element of a disjoint union A + B, it is possible to determine whether it came from A or B. If an element lies in both, there will be two effectively distinct copies of the value in A + B, one from A and one from B. . I think discriminated union/intersection of types would make sense here. leaving out the discriminated/tagged/disjoint here is the problem. in C theres actual *untagged* unions, which simply means that C reserves the memory for the largest of the intersected types and you need to keep track yourself of which the type of the value is. In python you can always do `isinstance`, so a more correct name for `Union[A, B]` would be `TaggedUnion[A, B]`. Id also like `OneOf[A, B]`, but that ship has sailed. And intersections are basically duck types or structural types (when anonymous) and traits/interfaces (when named). (i.e. `hasattr(obj, 'foo')` defines an intersection type of all types having that attribute. So it makes sense for python, its just defined more explicitly than by literally intersecting types.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:958,performance,memor,memory,958,"> I doubt that it would be considered a branch of logic. What do you define as logic here? I was talking about the logic theory that encompasses formal systems and so on. > [Union and intersection are bad names]. I agree, wikipedia enumerates more names, and explains where union comes from:. > tagged union, variant, variant record, choice type, discriminated union, disjoint union, or sum type. > . > Mathematically, tagged unions correspond to disjoint or discriminated unions, usually written using +. Given an element of a disjoint union A + B, it is possible to determine whether it came from A or B. If an element lies in both, there will be two effectively distinct copies of the value in A + B, one from A and one from B. . I think discriminated union/intersection of types would make sense here. leaving out the discriminated/tagged/disjoint here is the problem. in C theres actual *untagged* unions, which simply means that C reserves the memory for the largest of the intersected types and you need to keep track yourself of which the type of the value is. In python you can always do `isinstance`, so a more correct name for `Union[A, B]` would be `TaggedUnion[A, B]`. Id also like `OneOf[A, B]`, but that ship has sailed. And intersections are basically duck types or structural types (when anonymous) and traits/interfaces (when named). (i.e. `hasattr(obj, 'foo')` defines an intersection type of all types having that attribute. So it makes sense for python, its just defined more explicitly than by literally intersecting types.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:50,safety,log,logic,50,"> I doubt that it would be considered a branch of logic. What do you define as logic here? I was talking about the logic theory that encompasses formal systems and so on. > [Union and intersection are bad names]. I agree, wikipedia enumerates more names, and explains where union comes from:. > tagged union, variant, variant record, choice type, discriminated union, disjoint union, or sum type. > . > Mathematically, tagged unions correspond to disjoint or discriminated unions, usually written using +. Given an element of a disjoint union A + B, it is possible to determine whether it came from A or B. If an element lies in both, there will be two effectively distinct copies of the value in A + B, one from A and one from B. . I think discriminated union/intersection of types would make sense here. leaving out the discriminated/tagged/disjoint here is the problem. in C theres actual *untagged* unions, which simply means that C reserves the memory for the largest of the intersected types and you need to keep track yourself of which the type of the value is. In python you can always do `isinstance`, so a more correct name for `Union[A, B]` would be `TaggedUnion[A, B]`. Id also like `OneOf[A, B]`, but that ship has sailed. And intersections are basically duck types or structural types (when anonymous) and traits/interfaces (when named). (i.e. `hasattr(obj, 'foo')` defines an intersection type of all types having that attribute. So it makes sense for python, its just defined more explicitly than by literally intersecting types.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:79,safety,log,logic,79,"> I doubt that it would be considered a branch of logic. What do you define as logic here? I was talking about the logic theory that encompasses formal systems and so on. > [Union and intersection are bad names]. I agree, wikipedia enumerates more names, and explains where union comes from:. > tagged union, variant, variant record, choice type, discriminated union, disjoint union, or sum type. > . > Mathematically, tagged unions correspond to disjoint or discriminated unions, usually written using +. Given an element of a disjoint union A + B, it is possible to determine whether it came from A or B. If an element lies in both, there will be two effectively distinct copies of the value in A + B, one from A and one from B. . I think discriminated union/intersection of types would make sense here. leaving out the discriminated/tagged/disjoint here is the problem. in C theres actual *untagged* unions, which simply means that C reserves the memory for the largest of the intersected types and you need to keep track yourself of which the type of the value is. In python you can always do `isinstance`, so a more correct name for `Union[A, B]` would be `TaggedUnion[A, B]`. Id also like `OneOf[A, B]`, but that ship has sailed. And intersections are basically duck types or structural types (when anonymous) and traits/interfaces (when named). (i.e. `hasattr(obj, 'foo')` defines an intersection type of all types having that attribute. So it makes sense for python, its just defined more explicitly than by literally intersecting types.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:115,safety,log,logic,115,"> I doubt that it would be considered a branch of logic. What do you define as logic here? I was talking about the logic theory that encompasses formal systems and so on. > [Union and intersection are bad names]. I agree, wikipedia enumerates more names, and explains where union comes from:. > tagged union, variant, variant record, choice type, discriminated union, disjoint union, or sum type. > . > Mathematically, tagged unions correspond to disjoint or discriminated unions, usually written using +. Given an element of a disjoint union A + B, it is possible to determine whether it came from A or B. If an element lies in both, there will be two effectively distinct copies of the value in A + B, one from A and one from B. . I think discriminated union/intersection of types would make sense here. leaving out the discriminated/tagged/disjoint here is the problem. in C theres actual *untagged* unions, which simply means that C reserves the memory for the largest of the intersected types and you need to keep track yourself of which the type of the value is. In python you can always do `isinstance`, so a more correct name for `Union[A, B]` would be `TaggedUnion[A, B]`. Id also like `OneOf[A, B]`, but that ship has sailed. And intersections are basically duck types or structural types (when anonymous) and traits/interfaces (when named). (i.e. `hasattr(obj, 'foo')` defines an intersection type of all types having that attribute. So it makes sense for python, its just defined more explicitly than by literally intersecting types.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:50,security,log,logic,50,"> I doubt that it would be considered a branch of logic. What do you define as logic here? I was talking about the logic theory that encompasses formal systems and so on. > [Union and intersection are bad names]. I agree, wikipedia enumerates more names, and explains where union comes from:. > tagged union, variant, variant record, choice type, discriminated union, disjoint union, or sum type. > . > Mathematically, tagged unions correspond to disjoint or discriminated unions, usually written using +. Given an element of a disjoint union A + B, it is possible to determine whether it came from A or B. If an element lies in both, there will be two effectively distinct copies of the value in A + B, one from A and one from B. . I think discriminated union/intersection of types would make sense here. leaving out the discriminated/tagged/disjoint here is the problem. in C theres actual *untagged* unions, which simply means that C reserves the memory for the largest of the intersected types and you need to keep track yourself of which the type of the value is. In python you can always do `isinstance`, so a more correct name for `Union[A, B]` would be `TaggedUnion[A, B]`. Id also like `OneOf[A, B]`, but that ship has sailed. And intersections are basically duck types or structural types (when anonymous) and traits/interfaces (when named). (i.e. `hasattr(obj, 'foo')` defines an intersection type of all types having that attribute. So it makes sense for python, its just defined more explicitly than by literally intersecting types.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:79,security,log,logic,79,"> I doubt that it would be considered a branch of logic. What do you define as logic here? I was talking about the logic theory that encompasses formal systems and so on. > [Union and intersection are bad names]. I agree, wikipedia enumerates more names, and explains where union comes from:. > tagged union, variant, variant record, choice type, discriminated union, disjoint union, or sum type. > . > Mathematically, tagged unions correspond to disjoint or discriminated unions, usually written using +. Given an element of a disjoint union A + B, it is possible to determine whether it came from A or B. If an element lies in both, there will be two effectively distinct copies of the value in A + B, one from A and one from B. . I think discriminated union/intersection of types would make sense here. leaving out the discriminated/tagged/disjoint here is the problem. in C theres actual *untagged* unions, which simply means that C reserves the memory for the largest of the intersected types and you need to keep track yourself of which the type of the value is. In python you can always do `isinstance`, so a more correct name for `Union[A, B]` would be `TaggedUnion[A, B]`. Id also like `OneOf[A, B]`, but that ship has sailed. And intersections are basically duck types or structural types (when anonymous) and traits/interfaces (when named). (i.e. `hasattr(obj, 'foo')` defines an intersection type of all types having that attribute. So it makes sense for python, its just defined more explicitly than by literally intersecting types.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:115,security,log,logic,115,"> I doubt that it would be considered a branch of logic. What do you define as logic here? I was talking about the logic theory that encompasses formal systems and so on. > [Union and intersection are bad names]. I agree, wikipedia enumerates more names, and explains where union comes from:. > tagged union, variant, variant record, choice type, discriminated union, disjoint union, or sum type. > . > Mathematically, tagged unions correspond to disjoint or discriminated unions, usually written using +. Given an element of a disjoint union A + B, it is possible to determine whether it came from A or B. If an element lies in both, there will be two effectively distinct copies of the value in A + B, one from A and one from B. . I think discriminated union/intersection of types would make sense here. leaving out the discriminated/tagged/disjoint here is the problem. in C theres actual *untagged* unions, which simply means that C reserves the memory for the largest of the intersected types and you need to keep track yourself of which the type of the value is. In python you can always do `isinstance`, so a more correct name for `Union[A, B]` would be `TaggedUnion[A, B]`. Id also like `OneOf[A, B]`, but that ship has sailed. And intersections are basically duck types or structural types (when anonymous) and traits/interfaces (when named). (i.e. `hasattr(obj, 'foo')` defines an intersection type of all types having that attribute. So it makes sense for python, its just defined more explicitly than by literally intersecting types.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:50,testability,log,logic,50,"> I doubt that it would be considered a branch of logic. What do you define as logic here? I was talking about the logic theory that encompasses formal systems and so on. > [Union and intersection are bad names]. I agree, wikipedia enumerates more names, and explains where union comes from:. > tagged union, variant, variant record, choice type, discriminated union, disjoint union, or sum type. > . > Mathematically, tagged unions correspond to disjoint or discriminated unions, usually written using +. Given an element of a disjoint union A + B, it is possible to determine whether it came from A or B. If an element lies in both, there will be two effectively distinct copies of the value in A + B, one from A and one from B. . I think discriminated union/intersection of types would make sense here. leaving out the discriminated/tagged/disjoint here is the problem. in C theres actual *untagged* unions, which simply means that C reserves the memory for the largest of the intersected types and you need to keep track yourself of which the type of the value is. In python you can always do `isinstance`, so a more correct name for `Union[A, B]` would be `TaggedUnion[A, B]`. Id also like `OneOf[A, B]`, but that ship has sailed. And intersections are basically duck types or structural types (when anonymous) and traits/interfaces (when named). (i.e. `hasattr(obj, 'foo')` defines an intersection type of all types having that attribute. So it makes sense for python, its just defined more explicitly than by literally intersecting types.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:79,testability,log,logic,79,"> I doubt that it would be considered a branch of logic. What do you define as logic here? I was talking about the logic theory that encompasses formal systems and so on. > [Union and intersection are bad names]. I agree, wikipedia enumerates more names, and explains where union comes from:. > tagged union, variant, variant record, choice type, discriminated union, disjoint union, or sum type. > . > Mathematically, tagged unions correspond to disjoint or discriminated unions, usually written using +. Given an element of a disjoint union A + B, it is possible to determine whether it came from A or B. If an element lies in both, there will be two effectively distinct copies of the value in A + B, one from A and one from B. . I think discriminated union/intersection of types would make sense here. leaving out the discriminated/tagged/disjoint here is the problem. in C theres actual *untagged* unions, which simply means that C reserves the memory for the largest of the intersected types and you need to keep track yourself of which the type of the value is. In python you can always do `isinstance`, so a more correct name for `Union[A, B]` would be `TaggedUnion[A, B]`. Id also like `OneOf[A, B]`, but that ship has sailed. And intersections are basically duck types or structural types (when anonymous) and traits/interfaces (when named). (i.e. `hasattr(obj, 'foo')` defines an intersection type of all types having that attribute. So it makes sense for python, its just defined more explicitly than by literally intersecting types.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:115,testability,log,logic,115,"> I doubt that it would be considered a branch of logic. What do you define as logic here? I was talking about the logic theory that encompasses formal systems and so on. > [Union and intersection are bad names]. I agree, wikipedia enumerates more names, and explains where union comes from:. > tagged union, variant, variant record, choice type, discriminated union, disjoint union, or sum type. > . > Mathematically, tagged unions correspond to disjoint or discriminated unions, usually written using +. Given an element of a disjoint union A + B, it is possible to determine whether it came from A or B. If an element lies in both, there will be two effectively distinct copies of the value in A + B, one from A and one from B. . I think discriminated union/intersection of types would make sense here. leaving out the discriminated/tagged/disjoint here is the problem. in C theres actual *untagged* unions, which simply means that C reserves the memory for the largest of the intersected types and you need to keep track yourself of which the type of the value is. In python you can always do `isinstance`, so a more correct name for `Union[A, B]` would be `TaggedUnion[A, B]`. Id also like `OneOf[A, B]`, but that ship has sailed. And intersections are basically duck types or structural types (when anonymous) and traits/interfaces (when named). (i.e. `hasattr(obj, 'foo')` defines an intersection type of all types having that attribute. So it makes sense for python, its just defined more explicitly than by literally intersecting types.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:925,testability,simpl,simply,925,"> I doubt that it would be considered a branch of logic. What do you define as logic here? I was talking about the logic theory that encompasses formal systems and so on. > [Union and intersection are bad names]. I agree, wikipedia enumerates more names, and explains where union comes from:. > tagged union, variant, variant record, choice type, discriminated union, disjoint union, or sum type. > . > Mathematically, tagged unions correspond to disjoint or discriminated unions, usually written using +. Given an element of a disjoint union A + B, it is possible to determine whether it came from A or B. If an element lies in both, there will be two effectively distinct copies of the value in A + B, one from A and one from B. . I think discriminated union/intersection of types would make sense here. leaving out the discriminated/tagged/disjoint here is the problem. in C theres actual *untagged* unions, which simply means that C reserves the memory for the largest of the intersected types and you need to keep track yourself of which the type of the value is. In python you can always do `isinstance`, so a more correct name for `Union[A, B]` would be `TaggedUnion[A, B]`. Id also like `OneOf[A, B]`, but that ship has sailed. And intersections are basically duck types or structural types (when anonymous) and traits/interfaces (when named). (i.e. `hasattr(obj, 'foo')` defines an intersection type of all types having that attribute. So it makes sense for python, its just defined more explicitly than by literally intersecting types.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:656,usability,effectiv,effectively,656,"> I doubt that it would be considered a branch of logic. What do you define as logic here? I was talking about the logic theory that encompasses formal systems and so on. > [Union and intersection are bad names]. I agree, wikipedia enumerates more names, and explains where union comes from:. > tagged union, variant, variant record, choice type, discriminated union, disjoint union, or sum type. > . > Mathematically, tagged unions correspond to disjoint or discriminated unions, usually written using +. Given an element of a disjoint union A + B, it is possible to determine whether it came from A or B. If an element lies in both, there will be two effectively distinct copies of the value in A + B, one from A and one from B. . I think discriminated union/intersection of types would make sense here. leaving out the discriminated/tagged/disjoint here is the problem. in C theres actual *untagged* unions, which simply means that C reserves the memory for the largest of the intersected types and you need to keep track yourself of which the type of the value is. In python you can always do `isinstance`, so a more correct name for `Union[A, B]` would be `TaggedUnion[A, B]`. Id also like `OneOf[A, B]`, but that ship has sailed. And intersections are basically duck types or structural types (when anonymous) and traits/interfaces (when named). (i.e. `hasattr(obj, 'foo')` defines an intersection type of all types having that attribute. So it makes sense for python, its just defined more explicitly than by literally intersecting types.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:925,usability,simpl,simply,925,"> I doubt that it would be considered a branch of logic. What do you define as logic here? I was talking about the logic theory that encompasses formal systems and so on. > [Union and intersection are bad names]. I agree, wikipedia enumerates more names, and explains where union comes from:. > tagged union, variant, variant record, choice type, discriminated union, disjoint union, or sum type. > . > Mathematically, tagged unions correspond to disjoint or discriminated unions, usually written using +. Given an element of a disjoint union A + B, it is possible to determine whether it came from A or B. If an element lies in both, there will be two effectively distinct copies of the value in A + B, one from A and one from B. . I think discriminated union/intersection of types would make sense here. leaving out the discriminated/tagged/disjoint here is the problem. in C theres actual *untagged* unions, which simply means that C reserves the memory for the largest of the intersected types and you need to keep track yourself of which the type of the value is. In python you can always do `isinstance`, so a more correct name for `Union[A, B]` would be `TaggedUnion[A, B]`. Id also like `OneOf[A, B]`, but that ship has sailed. And intersections are basically duck types or structural types (when anonymous) and traits/interfaces (when named). (i.e. `hasattr(obj, 'foo')` defines an intersection type of all types having that attribute. So it makes sense for python, its just defined more explicitly than by literally intersecting types.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:958,usability,memor,memory,958,"> I doubt that it would be considered a branch of logic. What do you define as logic here? I was talking about the logic theory that encompasses formal systems and so on. > [Union and intersection are bad names]. I agree, wikipedia enumerates more names, and explains where union comes from:. > tagged union, variant, variant record, choice type, discriminated union, disjoint union, or sum type. > . > Mathematically, tagged unions correspond to disjoint or discriminated unions, usually written using +. Given an element of a disjoint union A + B, it is possible to determine whether it came from A or B. If an element lies in both, there will be two effectively distinct copies of the value in A + B, one from A and one from B. . I think discriminated union/intersection of types would make sense here. leaving out the discriminated/tagged/disjoint here is the problem. in C theres actual *untagged* unions, which simply means that C reserves the memory for the largest of the intersected types and you need to keep track yourself of which the type of the value is. In python you can always do `isinstance`, so a more correct name for `Union[A, B]` would be `TaggedUnion[A, B]`. Id also like `OneOf[A, B]`, but that ship has sailed. And intersections are basically duck types or structural types (when anonymous) and traits/interfaces (when named). (i.e. `hasattr(obj, 'foo')` defines an intersection type of all types having that attribute. So it makes sense for python, its just defined more explicitly than by literally intersecting types.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:164,testability,context,context,164,"just passing by to say I love seeing this discussion, and particularly the type-algebra perspective @flying-sheep . I have to admit I am missing what the original context was, in case anyone wants to attempt a small summary. Something about describing types in docstrings (e.g. `a, b, or c`)? I am not the most important stakeholder by far though, so no worries either way. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:226,availability,slo,slowly,226,"sure! in short: alex said he didnt like the switch to type annotations at all, citing a few gripes. i went on to fix them at various places (fixes are now in) and argued against a few others. i convinced alex that we should (slowly and carefully) adapt type annotations. the only thing that was missing is a consensus on how to best pretty-print `typing.Union`, because alex was not a fan of the name and clumsiness. I preferred `a, b, or c`, he just `a, b, c`. i explained why `a, b, c` is a bad convention, but alex insisted to go with it because (sadly) everyone is doing it. from there on we went deeper into algebraic types and so on. without need really, as we already decided on what to do.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:248,energy efficiency,adapt,adapt,248,"sure! in short: alex said he didnt like the switch to type annotations at all, citing a few gripes. i went on to fix them at various places (fixes are now in) and argued against a few others. i convinced alex that we should (slowly and carefully) adapt type annotations. the only thing that was missing is a consensus on how to best pretty-print `typing.Union`, because alex was not a fan of the name and clumsiness. I preferred `a, b, or c`, he just `a, b, c`. i explained why `a, b, c` is a bad convention, but alex insisted to go with it because (sadly) everyone is doing it. from there on we went deeper into algebraic types and so on. without need really, as we already decided on what to do.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:248,integrability,adapt,adapt,248,"sure! in short: alex said he didnt like the switch to type annotations at all, citing a few gripes. i went on to fix them at various places (fixes are now in) and argued against a few others. i convinced alex that we should (slowly and carefully) adapt type annotations. the only thing that was missing is a consensus on how to best pretty-print `typing.Union`, because alex was not a fan of the name and clumsiness. I preferred `a, b, or c`, he just `a, b, c`. i explained why `a, b, c` is a bad convention, but alex insisted to go with it because (sadly) everyone is doing it. from there on we went deeper into algebraic types and so on. without need really, as we already decided on what to do.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:248,interoperability,adapt,adapt,248,"sure! in short: alex said he didnt like the switch to type annotations at all, citing a few gripes. i went on to fix them at various places (fixes are now in) and argued against a few others. i convinced alex that we should (slowly and carefully) adapt type annotations. the only thing that was missing is a consensus on how to best pretty-print `typing.Union`, because alex was not a fan of the name and clumsiness. I preferred `a, b, or c`, he just `a, b, c`. i explained why `a, b, c` is a bad convention, but alex insisted to go with it because (sadly) everyone is doing it. from there on we went deeper into algebraic types and so on. without need really, as we already decided on what to do.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:248,modifiability,adapt,adapt,248,"sure! in short: alex said he didnt like the switch to type annotations at all, citing a few gripes. i went on to fix them at various places (fixes are now in) and argued against a few others. i convinced alex that we should (slowly and carefully) adapt type annotations. the only thing that was missing is a consensus on how to best pretty-print `typing.Union`, because alex was not a fan of the name and clumsiness. I preferred `a, b, or c`, he just `a, b, c`. i explained why `a, b, c` is a bad convention, but alex insisted to go with it because (sadly) everyone is doing it. from there on we went deeper into algebraic types and so on. without need really, as we already decided on what to do.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:226,reliability,slo,slowly,226,"sure! in short: alex said he didnt like the switch to type annotations at all, citing a few gripes. i went on to fix them at various places (fixes are now in) and argued against a few others. i convinced alex that we should (slowly and carefully) adapt type annotations. the only thing that was missing is a consensus on how to best pretty-print `typing.Union`, because alex was not a fan of the name and clumsiness. I preferred `a, b, or c`, he just `a, b, c`. i explained why `a, b, c` is a bad convention, but alex insisted to go with it because (sadly) everyone is doing it. from there on we went deeper into algebraic types and so on. without need really, as we already decided on what to do.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:420,usability,prefer,preferred,420,"sure! in short: alex said he didnt like the switch to type annotations at all, citing a few gripes. i went on to fix them at various places (fixes are now in) and argued against a few others. i convinced alex that we should (slowly and carefully) adapt type annotations. the only thing that was missing is a consensus on how to best pretty-print `typing.Union`, because alex was not a fan of the name and clumsiness. I preferred `a, b, or c`, he just `a, b, c`. i explained why `a, b, c` is a bad convention, but alex insisted to go with it because (sadly) everyone is doing it. from there on we went deeper into algebraic types and so on. without need really, as we already decided on what to do.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:327,availability,state,statement,327,"> What do you define as logic here? [Logic](https://en.wikipedia.org/wiki/Logic) as the mother of all formal reasoning and its close relative set theory in mathematics. When you say type theory is a branch of logic then 90% of computer science is a branch of logic. In many contexts this might be a valid but not a very useful statement. > Id also like `OneOf[A, B]`. I love `OneOf[A, B]`. This also doesn't pretend to be logic or set stuff. > `hasattr(obj, 'foo')` defines an intersection type of all types having that attribute. This is what I meant when I said _intersection of properties of supertypes_. But I still don't know when you'd need such a type in a practical context, given that we just keep overloading functions like crazy and simply treat passed arguments dependent on their type. Any example when intersection types are actually useful? In a function we might see in Scanpy (this was the whole beginning of this discussion; I cannot imagine a case in which we need to label something _intersection type_ in the docs).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:24,deployability,log,logic,24,"> What do you define as logic here? [Logic](https://en.wikipedia.org/wiki/Logic) as the mother of all formal reasoning and its close relative set theory in mathematics. When you say type theory is a branch of logic then 90% of computer science is a branch of logic. In many contexts this might be a valid but not a very useful statement. > Id also like `OneOf[A, B]`. I love `OneOf[A, B]`. This also doesn't pretend to be logic or set stuff. > `hasattr(obj, 'foo')` defines an intersection type of all types having that attribute. This is what I meant when I said _intersection of properties of supertypes_. But I still don't know when you'd need such a type in a practical context, given that we just keep overloading functions like crazy and simply treat passed arguments dependent on their type. Any example when intersection types are actually useful? In a function we might see in Scanpy (this was the whole beginning of this discussion; I cannot imagine a case in which we need to label something _intersection type_ in the docs).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:37,deployability,Log,Logic,37,"> What do you define as logic here? [Logic](https://en.wikipedia.org/wiki/Logic) as the mother of all formal reasoning and its close relative set theory in mathematics. When you say type theory is a branch of logic then 90% of computer science is a branch of logic. In many contexts this might be a valid but not a very useful statement. > Id also like `OneOf[A, B]`. I love `OneOf[A, B]`. This also doesn't pretend to be logic or set stuff. > `hasattr(obj, 'foo')` defines an intersection type of all types having that attribute. This is what I meant when I said _intersection of properties of supertypes_. But I still don't know when you'd need such a type in a practical context, given that we just keep overloading functions like crazy and simply treat passed arguments dependent on their type. Any example when intersection types are actually useful? In a function we might see in Scanpy (this was the whole beginning of this discussion; I cannot imagine a case in which we need to label something _intersection type_ in the docs).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:74,deployability,Log,Logic,74,"> What do you define as logic here? [Logic](https://en.wikipedia.org/wiki/Logic) as the mother of all formal reasoning and its close relative set theory in mathematics. When you say type theory is a branch of logic then 90% of computer science is a branch of logic. In many contexts this might be a valid but not a very useful statement. > Id also like `OneOf[A, B]`. I love `OneOf[A, B]`. This also doesn't pretend to be logic or set stuff. > `hasattr(obj, 'foo')` defines an intersection type of all types having that attribute. This is what I meant when I said _intersection of properties of supertypes_. But I still don't know when you'd need such a type in a practical context, given that we just keep overloading functions like crazy and simply treat passed arguments dependent on their type. Any example when intersection types are actually useful? In a function we might see in Scanpy (this was the whole beginning of this discussion; I cannot imagine a case in which we need to label something _intersection type_ in the docs).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:209,deployability,log,logic,209,"> What do you define as logic here? [Logic](https://en.wikipedia.org/wiki/Logic) as the mother of all formal reasoning and its close relative set theory in mathematics. When you say type theory is a branch of logic then 90% of computer science is a branch of logic. In many contexts this might be a valid but not a very useful statement. > Id also like `OneOf[A, B]`. I love `OneOf[A, B]`. This also doesn't pretend to be logic or set stuff. > `hasattr(obj, 'foo')` defines an intersection type of all types having that attribute. This is what I meant when I said _intersection of properties of supertypes_. But I still don't know when you'd need such a type in a practical context, given that we just keep overloading functions like crazy and simply treat passed arguments dependent on their type. Any example when intersection types are actually useful? In a function we might see in Scanpy (this was the whole beginning of this discussion; I cannot imagine a case in which we need to label something _intersection type_ in the docs).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:259,deployability,log,logic,259,"> What do you define as logic here? [Logic](https://en.wikipedia.org/wiki/Logic) as the mother of all formal reasoning and its close relative set theory in mathematics. When you say type theory is a branch of logic then 90% of computer science is a branch of logic. In many contexts this might be a valid but not a very useful statement. > Id also like `OneOf[A, B]`. I love `OneOf[A, B]`. This also doesn't pretend to be logic or set stuff. > `hasattr(obj, 'foo')` defines an intersection type of all types having that attribute. This is what I meant when I said _intersection of properties of supertypes_. But I still don't know when you'd need such a type in a practical context, given that we just keep overloading functions like crazy and simply treat passed arguments dependent on their type. Any example when intersection types are actually useful? In a function we might see in Scanpy (this was the whole beginning of this discussion; I cannot imagine a case in which we need to label something _intersection type_ in the docs).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:423,deployability,log,logic,423,"> What do you define as logic here? [Logic](https://en.wikipedia.org/wiki/Logic) as the mother of all formal reasoning and its close relative set theory in mathematics. When you say type theory is a branch of logic then 90% of computer science is a branch of logic. In many contexts this might be a valid but not a very useful statement. > Id also like `OneOf[A, B]`. I love `OneOf[A, B]`. This also doesn't pretend to be logic or set stuff. > `hasattr(obj, 'foo')` defines an intersection type of all types having that attribute. This is what I meant when I said _intersection of properties of supertypes_. But I still don't know when you'd need such a type in a practical context, given that we just keep overloading functions like crazy and simply treat passed arguments dependent on their type. Any example when intersection types are actually useful? In a function we might see in Scanpy (this was the whole beginning of this discussion; I cannot imagine a case in which we need to label something _intersection type_ in the docs).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:775,deployability,depend,dependent,775,"> What do you define as logic here? [Logic](https://en.wikipedia.org/wiki/Logic) as the mother of all formal reasoning and its close relative set theory in mathematics. When you say type theory is a branch of logic then 90% of computer science is a branch of logic. In many contexts this might be a valid but not a very useful statement. > Id also like `OneOf[A, B]`. I love `OneOf[A, B]`. This also doesn't pretend to be logic or set stuff. > `hasattr(obj, 'foo')` defines an intersection type of all types having that attribute. This is what I meant when I said _intersection of properties of supertypes_. But I still don't know when you'd need such a type in a practical context, given that we just keep overloading functions like crazy and simply treat passed arguments dependent on their type. Any example when intersection types are actually useful? In a function we might see in Scanpy (this was the whole beginning of this discussion; I cannot imagine a case in which we need to label something _intersection type_ in the docs).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:327,integrability,state,statement,327,"> What do you define as logic here? [Logic](https://en.wikipedia.org/wiki/Logic) as the mother of all formal reasoning and its close relative set theory in mathematics. When you say type theory is a branch of logic then 90% of computer science is a branch of logic. In many contexts this might be a valid but not a very useful statement. > Id also like `OneOf[A, B]`. I love `OneOf[A, B]`. This also doesn't pretend to be logic or set stuff. > `hasattr(obj, 'foo')` defines an intersection type of all types having that attribute. This is what I meant when I said _intersection of properties of supertypes_. But I still don't know when you'd need such a type in a practical context, given that we just keep overloading functions like crazy and simply treat passed arguments dependent on their type. Any example when intersection types are actually useful? In a function we might see in Scanpy (this was the whole beginning of this discussion; I cannot imagine a case in which we need to label something _intersection type_ in the docs).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:775,integrability,depend,dependent,775,"> What do you define as logic here? [Logic](https://en.wikipedia.org/wiki/Logic) as the mother of all formal reasoning and its close relative set theory in mathematics. When you say type theory is a branch of logic then 90% of computer science is a branch of logic. In many contexts this might be a valid but not a very useful statement. > Id also like `OneOf[A, B]`. I love `OneOf[A, B]`. This also doesn't pretend to be logic or set stuff. > `hasattr(obj, 'foo')` defines an intersection type of all types having that attribute. This is what I meant when I said _intersection of properties of supertypes_. But I still don't know when you'd need such a type in a practical context, given that we just keep overloading functions like crazy and simply treat passed arguments dependent on their type. Any example when intersection types are actually useful? In a function we might see in Scanpy (this was the whole beginning of this discussion; I cannot imagine a case in which we need to label something _intersection type_ in the docs).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:775,modifiability,depend,dependent,775,"> What do you define as logic here? [Logic](https://en.wikipedia.org/wiki/Logic) as the mother of all formal reasoning and its close relative set theory in mathematics. When you say type theory is a branch of logic then 90% of computer science is a branch of logic. In many contexts this might be a valid but not a very useful statement. > Id also like `OneOf[A, B]`. I love `OneOf[A, B]`. This also doesn't pretend to be logic or set stuff. > `hasattr(obj, 'foo')` defines an intersection type of all types having that attribute. This is what I meant when I said _intersection of properties of supertypes_. But I still don't know when you'd need such a type in a practical context, given that we just keep overloading functions like crazy and simply treat passed arguments dependent on their type. Any example when intersection types are actually useful? In a function we might see in Scanpy (this was the whole beginning of this discussion; I cannot imagine a case in which we need to label something _intersection type_ in the docs).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:401,reliability,doe,doesn,401,"> What do you define as logic here? [Logic](https://en.wikipedia.org/wiki/Logic) as the mother of all formal reasoning and its close relative set theory in mathematics. When you say type theory is a branch of logic then 90% of computer science is a branch of logic. In many contexts this might be a valid but not a very useful statement. > Id also like `OneOf[A, B]`. I love `OneOf[A, B]`. This also doesn't pretend to be logic or set stuff. > `hasattr(obj, 'foo')` defines an intersection type of all types having that attribute. This is what I meant when I said _intersection of properties of supertypes_. But I still don't know when you'd need such a type in a practical context, given that we just keep overloading functions like crazy and simply treat passed arguments dependent on their type. Any example when intersection types are actually useful? In a function we might see in Scanpy (this was the whole beginning of this discussion; I cannot imagine a case in which we need to label something _intersection type_ in the docs).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:665,reliability,pra,practical,665,"> What do you define as logic here? [Logic](https://en.wikipedia.org/wiki/Logic) as the mother of all formal reasoning and its close relative set theory in mathematics. When you say type theory is a branch of logic then 90% of computer science is a branch of logic. In many contexts this might be a valid but not a very useful statement. > Id also like `OneOf[A, B]`. I love `OneOf[A, B]`. This also doesn't pretend to be logic or set stuff. > `hasattr(obj, 'foo')` defines an intersection type of all types having that attribute. This is what I meant when I said _intersection of properties of supertypes_. But I still don't know when you'd need such a type in a practical context, given that we just keep overloading functions like crazy and simply treat passed arguments dependent on their type. Any example when intersection types are actually useful? In a function we might see in Scanpy (this was the whole beginning of this discussion; I cannot imagine a case in which we need to label something _intersection type_ in the docs).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:24,safety,log,logic,24,"> What do you define as logic here? [Logic](https://en.wikipedia.org/wiki/Logic) as the mother of all formal reasoning and its close relative set theory in mathematics. When you say type theory is a branch of logic then 90% of computer science is a branch of logic. In many contexts this might be a valid but not a very useful statement. > Id also like `OneOf[A, B]`. I love `OneOf[A, B]`. This also doesn't pretend to be logic or set stuff. > `hasattr(obj, 'foo')` defines an intersection type of all types having that attribute. This is what I meant when I said _intersection of properties of supertypes_. But I still don't know when you'd need such a type in a practical context, given that we just keep overloading functions like crazy and simply treat passed arguments dependent on their type. Any example when intersection types are actually useful? In a function we might see in Scanpy (this was the whole beginning of this discussion; I cannot imagine a case in which we need to label something _intersection type_ in the docs).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:37,safety,Log,Logic,37,"> What do you define as logic here? [Logic](https://en.wikipedia.org/wiki/Logic) as the mother of all formal reasoning and its close relative set theory in mathematics. When you say type theory is a branch of logic then 90% of computer science is a branch of logic. In many contexts this might be a valid but not a very useful statement. > Id also like `OneOf[A, B]`. I love `OneOf[A, B]`. This also doesn't pretend to be logic or set stuff. > `hasattr(obj, 'foo')` defines an intersection type of all types having that attribute. This is what I meant when I said _intersection of properties of supertypes_. But I still don't know when you'd need such a type in a practical context, given that we just keep overloading functions like crazy and simply treat passed arguments dependent on their type. Any example when intersection types are actually useful? In a function we might see in Scanpy (this was the whole beginning of this discussion; I cannot imagine a case in which we need to label something _intersection type_ in the docs).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:74,safety,Log,Logic,74,"> What do you define as logic here? [Logic](https://en.wikipedia.org/wiki/Logic) as the mother of all formal reasoning and its close relative set theory in mathematics. When you say type theory is a branch of logic then 90% of computer science is a branch of logic. In many contexts this might be a valid but not a very useful statement. > Id also like `OneOf[A, B]`. I love `OneOf[A, B]`. This also doesn't pretend to be logic or set stuff. > `hasattr(obj, 'foo')` defines an intersection type of all types having that attribute. This is what I meant when I said _intersection of properties of supertypes_. But I still don't know when you'd need such a type in a practical context, given that we just keep overloading functions like crazy and simply treat passed arguments dependent on their type. Any example when intersection types are actually useful? In a function we might see in Scanpy (this was the whole beginning of this discussion; I cannot imagine a case in which we need to label something _intersection type_ in the docs).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:209,safety,log,logic,209,"> What do you define as logic here? [Logic](https://en.wikipedia.org/wiki/Logic) as the mother of all formal reasoning and its close relative set theory in mathematics. When you say type theory is a branch of logic then 90% of computer science is a branch of logic. In many contexts this might be a valid but not a very useful statement. > Id also like `OneOf[A, B]`. I love `OneOf[A, B]`. This also doesn't pretend to be logic or set stuff. > `hasattr(obj, 'foo')` defines an intersection type of all types having that attribute. This is what I meant when I said _intersection of properties of supertypes_. But I still don't know when you'd need such a type in a practical context, given that we just keep overloading functions like crazy and simply treat passed arguments dependent on their type. Any example when intersection types are actually useful? In a function we might see in Scanpy (this was the whole beginning of this discussion; I cannot imagine a case in which we need to label something _intersection type_ in the docs).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:259,safety,log,logic,259,"> What do you define as logic here? [Logic](https://en.wikipedia.org/wiki/Logic) as the mother of all formal reasoning and its close relative set theory in mathematics. When you say type theory is a branch of logic then 90% of computer science is a branch of logic. In many contexts this might be a valid but not a very useful statement. > Id also like `OneOf[A, B]`. I love `OneOf[A, B]`. This also doesn't pretend to be logic or set stuff. > `hasattr(obj, 'foo')` defines an intersection type of all types having that attribute. This is what I meant when I said _intersection of properties of supertypes_. But I still don't know when you'd need such a type in a practical context, given that we just keep overloading functions like crazy and simply treat passed arguments dependent on their type. Any example when intersection types are actually useful? In a function we might see in Scanpy (this was the whole beginning of this discussion; I cannot imagine a case in which we need to label something _intersection type_ in the docs).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:299,safety,valid,valid,299,"> What do you define as logic here? [Logic](https://en.wikipedia.org/wiki/Logic) as the mother of all formal reasoning and its close relative set theory in mathematics. When you say type theory is a branch of logic then 90% of computer science is a branch of logic. In many contexts this might be a valid but not a very useful statement. > Id also like `OneOf[A, B]`. I love `OneOf[A, B]`. This also doesn't pretend to be logic or set stuff. > `hasattr(obj, 'foo')` defines an intersection type of all types having that attribute. This is what I meant when I said _intersection of properties of supertypes_. But I still don't know when you'd need such a type in a practical context, given that we just keep overloading functions like crazy and simply treat passed arguments dependent on their type. Any example when intersection types are actually useful? In a function we might see in Scanpy (this was the whole beginning of this discussion; I cannot imagine a case in which we need to label something _intersection type_ in the docs).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:423,safety,log,logic,423,"> What do you define as logic here? [Logic](https://en.wikipedia.org/wiki/Logic) as the mother of all formal reasoning and its close relative set theory in mathematics. When you say type theory is a branch of logic then 90% of computer science is a branch of logic. In many contexts this might be a valid but not a very useful statement. > Id also like `OneOf[A, B]`. I love `OneOf[A, B]`. This also doesn't pretend to be logic or set stuff. > `hasattr(obj, 'foo')` defines an intersection type of all types having that attribute. This is what I meant when I said _intersection of properties of supertypes_. But I still don't know when you'd need such a type in a practical context, given that we just keep overloading functions like crazy and simply treat passed arguments dependent on their type. Any example when intersection types are actually useful? In a function we might see in Scanpy (this was the whole beginning of this discussion; I cannot imagine a case in which we need to label something _intersection type_ in the docs).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:775,safety,depend,dependent,775,"> What do you define as logic here? [Logic](https://en.wikipedia.org/wiki/Logic) as the mother of all formal reasoning and its close relative set theory in mathematics. When you say type theory is a branch of logic then 90% of computer science is a branch of logic. In many contexts this might be a valid but not a very useful statement. > Id also like `OneOf[A, B]`. I love `OneOf[A, B]`. This also doesn't pretend to be logic or set stuff. > `hasattr(obj, 'foo')` defines an intersection type of all types having that attribute. This is what I meant when I said _intersection of properties of supertypes_. But I still don't know when you'd need such a type in a practical context, given that we just keep overloading functions like crazy and simply treat passed arguments dependent on their type. Any example when intersection types are actually useful? In a function we might see in Scanpy (this was the whole beginning of this discussion; I cannot imagine a case in which we need to label something _intersection type_ in the docs).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:24,security,log,logic,24,"> What do you define as logic here? [Logic](https://en.wikipedia.org/wiki/Logic) as the mother of all formal reasoning and its close relative set theory in mathematics. When you say type theory is a branch of logic then 90% of computer science is a branch of logic. In many contexts this might be a valid but not a very useful statement. > Id also like `OneOf[A, B]`. I love `OneOf[A, B]`. This also doesn't pretend to be logic or set stuff. > `hasattr(obj, 'foo')` defines an intersection type of all types having that attribute. This is what I meant when I said _intersection of properties of supertypes_. But I still don't know when you'd need such a type in a practical context, given that we just keep overloading functions like crazy and simply treat passed arguments dependent on their type. Any example when intersection types are actually useful? In a function we might see in Scanpy (this was the whole beginning of this discussion; I cannot imagine a case in which we need to label something _intersection type_ in the docs).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:37,security,Log,Logic,37,"> What do you define as logic here? [Logic](https://en.wikipedia.org/wiki/Logic) as the mother of all formal reasoning and its close relative set theory in mathematics. When you say type theory is a branch of logic then 90% of computer science is a branch of logic. In many contexts this might be a valid but not a very useful statement. > Id also like `OneOf[A, B]`. I love `OneOf[A, B]`. This also doesn't pretend to be logic or set stuff. > `hasattr(obj, 'foo')` defines an intersection type of all types having that attribute. This is what I meant when I said _intersection of properties of supertypes_. But I still don't know when you'd need such a type in a practical context, given that we just keep overloading functions like crazy and simply treat passed arguments dependent on their type. Any example when intersection types are actually useful? In a function we might see in Scanpy (this was the whole beginning of this discussion; I cannot imagine a case in which we need to label something _intersection type_ in the docs).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:74,security,Log,Logic,74,"> What do you define as logic here? [Logic](https://en.wikipedia.org/wiki/Logic) as the mother of all formal reasoning and its close relative set theory in mathematics. When you say type theory is a branch of logic then 90% of computer science is a branch of logic. In many contexts this might be a valid but not a very useful statement. > Id also like `OneOf[A, B]`. I love `OneOf[A, B]`. This also doesn't pretend to be logic or set stuff. > `hasattr(obj, 'foo')` defines an intersection type of all types having that attribute. This is what I meant when I said _intersection of properties of supertypes_. But I still don't know when you'd need such a type in a practical context, given that we just keep overloading functions like crazy and simply treat passed arguments dependent on their type. Any example when intersection types are actually useful? In a function we might see in Scanpy (this was the whole beginning of this discussion; I cannot imagine a case in which we need to label something _intersection type_ in the docs).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:209,security,log,logic,209,"> What do you define as logic here? [Logic](https://en.wikipedia.org/wiki/Logic) as the mother of all formal reasoning and its close relative set theory in mathematics. When you say type theory is a branch of logic then 90% of computer science is a branch of logic. In many contexts this might be a valid but not a very useful statement. > Id also like `OneOf[A, B]`. I love `OneOf[A, B]`. This also doesn't pretend to be logic or set stuff. > `hasattr(obj, 'foo')` defines an intersection type of all types having that attribute. This is what I meant when I said _intersection of properties of supertypes_. But I still don't know when you'd need such a type in a practical context, given that we just keep overloading functions like crazy and simply treat passed arguments dependent on their type. Any example when intersection types are actually useful? In a function we might see in Scanpy (this was the whole beginning of this discussion; I cannot imagine a case in which we need to label something _intersection type_ in the docs).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:259,security,log,logic,259,"> What do you define as logic here? [Logic](https://en.wikipedia.org/wiki/Logic) as the mother of all formal reasoning and its close relative set theory in mathematics. When you say type theory is a branch of logic then 90% of computer science is a branch of logic. In many contexts this might be a valid but not a very useful statement. > Id also like `OneOf[A, B]`. I love `OneOf[A, B]`. This also doesn't pretend to be logic or set stuff. > `hasattr(obj, 'foo')` defines an intersection type of all types having that attribute. This is what I meant when I said _intersection of properties of supertypes_. But I still don't know when you'd need such a type in a practical context, given that we just keep overloading functions like crazy and simply treat passed arguments dependent on their type. Any example when intersection types are actually useful? In a function we might see in Scanpy (this was the whole beginning of this discussion; I cannot imagine a case in which we need to label something _intersection type_ in the docs).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:423,security,log,logic,423,"> What do you define as logic here? [Logic](https://en.wikipedia.org/wiki/Logic) as the mother of all formal reasoning and its close relative set theory in mathematics. When you say type theory is a branch of logic then 90% of computer science is a branch of logic. In many contexts this might be a valid but not a very useful statement. > Id also like `OneOf[A, B]`. I love `OneOf[A, B]`. This also doesn't pretend to be logic or set stuff. > `hasattr(obj, 'foo')` defines an intersection type of all types having that attribute. This is what I meant when I said _intersection of properties of supertypes_. But I still don't know when you'd need such a type in a practical context, given that we just keep overloading functions like crazy and simply treat passed arguments dependent on their type. Any example when intersection types are actually useful? In a function we might see in Scanpy (this was the whole beginning of this discussion; I cannot imagine a case in which we need to label something _intersection type_ in the docs).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:24,testability,log,logic,24,"> What do you define as logic here? [Logic](https://en.wikipedia.org/wiki/Logic) as the mother of all formal reasoning and its close relative set theory in mathematics. When you say type theory is a branch of logic then 90% of computer science is a branch of logic. In many contexts this might be a valid but not a very useful statement. > Id also like `OneOf[A, B]`. I love `OneOf[A, B]`. This also doesn't pretend to be logic or set stuff. > `hasattr(obj, 'foo')` defines an intersection type of all types having that attribute. This is what I meant when I said _intersection of properties of supertypes_. But I still don't know when you'd need such a type in a practical context, given that we just keep overloading functions like crazy and simply treat passed arguments dependent on their type. Any example when intersection types are actually useful? In a function we might see in Scanpy (this was the whole beginning of this discussion; I cannot imagine a case in which we need to label something _intersection type_ in the docs).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:37,testability,Log,Logic,37,"> What do you define as logic here? [Logic](https://en.wikipedia.org/wiki/Logic) as the mother of all formal reasoning and its close relative set theory in mathematics. When you say type theory is a branch of logic then 90% of computer science is a branch of logic. In many contexts this might be a valid but not a very useful statement. > Id also like `OneOf[A, B]`. I love `OneOf[A, B]`. This also doesn't pretend to be logic or set stuff. > `hasattr(obj, 'foo')` defines an intersection type of all types having that attribute. This is what I meant when I said _intersection of properties of supertypes_. But I still don't know when you'd need such a type in a practical context, given that we just keep overloading functions like crazy and simply treat passed arguments dependent on their type. Any example when intersection types are actually useful? In a function we might see in Scanpy (this was the whole beginning of this discussion; I cannot imagine a case in which we need to label something _intersection type_ in the docs).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:74,testability,Log,Logic,74,"> What do you define as logic here? [Logic](https://en.wikipedia.org/wiki/Logic) as the mother of all formal reasoning and its close relative set theory in mathematics. When you say type theory is a branch of logic then 90% of computer science is a branch of logic. In many contexts this might be a valid but not a very useful statement. > Id also like `OneOf[A, B]`. I love `OneOf[A, B]`. This also doesn't pretend to be logic or set stuff. > `hasattr(obj, 'foo')` defines an intersection type of all types having that attribute. This is what I meant when I said _intersection of properties of supertypes_. But I still don't know when you'd need such a type in a practical context, given that we just keep overloading functions like crazy and simply treat passed arguments dependent on their type. Any example when intersection types are actually useful? In a function we might see in Scanpy (this was the whole beginning of this discussion; I cannot imagine a case in which we need to label something _intersection type_ in the docs).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:209,testability,log,logic,209,"> What do you define as logic here? [Logic](https://en.wikipedia.org/wiki/Logic) as the mother of all formal reasoning and its close relative set theory in mathematics. When you say type theory is a branch of logic then 90% of computer science is a branch of logic. In many contexts this might be a valid but not a very useful statement. > Id also like `OneOf[A, B]`. I love `OneOf[A, B]`. This also doesn't pretend to be logic or set stuff. > `hasattr(obj, 'foo')` defines an intersection type of all types having that attribute. This is what I meant when I said _intersection of properties of supertypes_. But I still don't know when you'd need such a type in a practical context, given that we just keep overloading functions like crazy and simply treat passed arguments dependent on their type. Any example when intersection types are actually useful? In a function we might see in Scanpy (this was the whole beginning of this discussion; I cannot imagine a case in which we need to label something _intersection type_ in the docs).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:259,testability,log,logic,259,"> What do you define as logic here? [Logic](https://en.wikipedia.org/wiki/Logic) as the mother of all formal reasoning and its close relative set theory in mathematics. When you say type theory is a branch of logic then 90% of computer science is a branch of logic. In many contexts this might be a valid but not a very useful statement. > Id also like `OneOf[A, B]`. I love `OneOf[A, B]`. This also doesn't pretend to be logic or set stuff. > `hasattr(obj, 'foo')` defines an intersection type of all types having that attribute. This is what I meant when I said _intersection of properties of supertypes_. But I still don't know when you'd need such a type in a practical context, given that we just keep overloading functions like crazy and simply treat passed arguments dependent on their type. Any example when intersection types are actually useful? In a function we might see in Scanpy (this was the whole beginning of this discussion; I cannot imagine a case in which we need to label something _intersection type_ in the docs).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:274,testability,context,contexts,274,"> What do you define as logic here? [Logic](https://en.wikipedia.org/wiki/Logic) as the mother of all formal reasoning and its close relative set theory in mathematics. When you say type theory is a branch of logic then 90% of computer science is a branch of logic. In many contexts this might be a valid but not a very useful statement. > Id also like `OneOf[A, B]`. I love `OneOf[A, B]`. This also doesn't pretend to be logic or set stuff. > `hasattr(obj, 'foo')` defines an intersection type of all types having that attribute. This is what I meant when I said _intersection of properties of supertypes_. But I still don't know when you'd need such a type in a practical context, given that we just keep overloading functions like crazy and simply treat passed arguments dependent on their type. Any example when intersection types are actually useful? In a function we might see in Scanpy (this was the whole beginning of this discussion; I cannot imagine a case in which we need to label something _intersection type_ in the docs).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:423,testability,log,logic,423,"> What do you define as logic here? [Logic](https://en.wikipedia.org/wiki/Logic) as the mother of all formal reasoning and its close relative set theory in mathematics. When you say type theory is a branch of logic then 90% of computer science is a branch of logic. In many contexts this might be a valid but not a very useful statement. > Id also like `OneOf[A, B]`. I love `OneOf[A, B]`. This also doesn't pretend to be logic or set stuff. > `hasattr(obj, 'foo')` defines an intersection type of all types having that attribute. This is what I meant when I said _intersection of properties of supertypes_. But I still don't know when you'd need such a type in a practical context, given that we just keep overloading functions like crazy and simply treat passed arguments dependent on their type. Any example when intersection types are actually useful? In a function we might see in Scanpy (this was the whole beginning of this discussion; I cannot imagine a case in which we need to label something _intersection type_ in the docs).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:675,testability,context,context,675,"> What do you define as logic here? [Logic](https://en.wikipedia.org/wiki/Logic) as the mother of all formal reasoning and its close relative set theory in mathematics. When you say type theory is a branch of logic then 90% of computer science is a branch of logic. In many contexts this might be a valid but not a very useful statement. > Id also like `OneOf[A, B]`. I love `OneOf[A, B]`. This also doesn't pretend to be logic or set stuff. > `hasattr(obj, 'foo')` defines an intersection type of all types having that attribute. This is what I meant when I said _intersection of properties of supertypes_. But I still don't know when you'd need such a type in a practical context, given that we just keep overloading functions like crazy and simply treat passed arguments dependent on their type. Any example when intersection types are actually useful? In a function we might see in Scanpy (this was the whole beginning of this discussion; I cannot imagine a case in which we need to label something _intersection type_ in the docs).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:745,testability,simpl,simply,745,"> What do you define as logic here? [Logic](https://en.wikipedia.org/wiki/Logic) as the mother of all formal reasoning and its close relative set theory in mathematics. When you say type theory is a branch of logic then 90% of computer science is a branch of logic. In many contexts this might be a valid but not a very useful statement. > Id also like `OneOf[A, B]`. I love `OneOf[A, B]`. This also doesn't pretend to be logic or set stuff. > `hasattr(obj, 'foo')` defines an intersection type of all types having that attribute. This is what I meant when I said _intersection of properties of supertypes_. But I still don't know when you'd need such a type in a practical context, given that we just keep overloading functions like crazy and simply treat passed arguments dependent on their type. Any example when intersection types are actually useful? In a function we might see in Scanpy (this was the whole beginning of this discussion; I cannot imagine a case in which we need to label something _intersection type_ in the docs).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:775,testability,depend,dependent,775,"> What do you define as logic here? [Logic](https://en.wikipedia.org/wiki/Logic) as the mother of all formal reasoning and its close relative set theory in mathematics. When you say type theory is a branch of logic then 90% of computer science is a branch of logic. In many contexts this might be a valid but not a very useful statement. > Id also like `OneOf[A, B]`. I love `OneOf[A, B]`. This also doesn't pretend to be logic or set stuff. > `hasattr(obj, 'foo')` defines an intersection type of all types having that attribute. This is what I meant when I said _intersection of properties of supertypes_. But I still don't know when you'd need such a type in a practical context, given that we just keep overloading functions like crazy and simply treat passed arguments dependent on their type. Any example when intersection types are actually useful? In a function we might see in Scanpy (this was the whole beginning of this discussion; I cannot imagine a case in which we need to label something _intersection type_ in the docs).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:127,usability,close,close,127,"> What do you define as logic here? [Logic](https://en.wikipedia.org/wiki/Logic) as the mother of all formal reasoning and its close relative set theory in mathematics. When you say type theory is a branch of logic then 90% of computer science is a branch of logic. In many contexts this might be a valid but not a very useful statement. > Id also like `OneOf[A, B]`. I love `OneOf[A, B]`. This also doesn't pretend to be logic or set stuff. > `hasattr(obj, 'foo')` defines an intersection type of all types having that attribute. This is what I meant when I said _intersection of properties of supertypes_. But I still don't know when you'd need such a type in a practical context, given that we just keep overloading functions like crazy and simply treat passed arguments dependent on their type. Any example when intersection types are actually useful? In a function we might see in Scanpy (this was the whole beginning of this discussion; I cannot imagine a case in which we need to label something _intersection type_ in the docs).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:745,usability,simpl,simply,745,"> What do you define as logic here? [Logic](https://en.wikipedia.org/wiki/Logic) as the mother of all formal reasoning and its close relative set theory in mathematics. When you say type theory is a branch of logic then 90% of computer science is a branch of logic. In many contexts this might be a valid but not a very useful statement. > Id also like `OneOf[A, B]`. I love `OneOf[A, B]`. This also doesn't pretend to be logic or set stuff. > `hasattr(obj, 'foo')` defines an intersection type of all types having that attribute. This is what I meant when I said _intersection of properties of supertypes_. But I still don't know when you'd need such a type in a practical context, given that we just keep overloading functions like crazy and simply treat passed arguments dependent on their type. Any example when intersection types are actually useful? In a function we might see in Scanpy (this was the whole beginning of this discussion; I cannot imagine a case in which we need to label something _intersection type_ in the docs).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:159,availability,operat,operations,159,"Just catching up on this conversation after a conference, glad to see the interest! @falexwolf, I think I can add some context here. Within type theory, these operations with types are actually set operations on a type lattice. For example, a supertype is the union of its subtypes. This is a core feature of type systems. If you're curious about the details, I really like [this paper](doi.org/10.1145/512950.512973) defining abstract interpretation -- which type algebra is a case of. Here's an example I think is pretty cool. Julia's [`Distributions`](https://juliastats.github.io/Distributions.jl/) package uses intersection types in it's model of statistical distributions. `Distributions` defines types for a number of distributions as well as methods to work with them (i.e. `pdf`, `rand`). The following is roughly how intersections are used (more info [here](https://juliastats.github.io/Distributions.jl/stable/types.html)):. ```julia. # Types are capitalized. # `<:` is the subtype operator. # Distributions are parametric on their variate form and value support. Distribution{F<:VariateForm, S<:ValueSupport}. # Defining some abstract subtypes. DiscreteDistribution{F<:VariateForm} = Distribution{F<:VariateForm, Discrete} # Discrete is a subtype of ValueSupport. UnivariateDistributions{S<:ValueSupport} = Distribution{Univariate, S<:ValueSupport} # SingleVariate is a subtype of VariateForm. MultivariateDistribution{S<:ValueSupport} = Distribution{Multivariate, S<:ValueSupport} # SingleVariate is a subtype of VariateForm. # Defining an intersection type. DiscreteUnivariateDistribution = Distribution{Univariate, Discrete}. # `===` is the absolute identity operator. DiscreteUnivariateDistribution === typeintersect(DiscreteDistribution, UnivariateDistribution) # returns `true`. # Examples of `Distribution`s which are subtypes of `DiscreteUnivariateDistribution`, these return `true`. Hypergeometric <: DiscreteUnivariateDistribution. Bernoulli <: DiscreteUnivariateDistribution. `",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:198,availability,operat,operations,198,"Just catching up on this conversation after a conference, glad to see the interest! @falexwolf, I think I can add some context here. Within type theory, these operations with types are actually set operations on a type lattice. For example, a supertype is the union of its subtypes. This is a core feature of type systems. If you're curious about the details, I really like [this paper](doi.org/10.1145/512950.512973) defining abstract interpretation -- which type algebra is a case of. Here's an example I think is pretty cool. Julia's [`Distributions`](https://juliastats.github.io/Distributions.jl/) package uses intersection types in it's model of statistical distributions. `Distributions` defines types for a number of distributions as well as methods to work with them (i.e. `pdf`, `rand`). The following is roughly how intersections are used (more info [here](https://juliastats.github.io/Distributions.jl/stable/types.html)):. ```julia. # Types are capitalized. # `<:` is the subtype operator. # Distributions are parametric on their variate form and value support. Distribution{F<:VariateForm, S<:ValueSupport}. # Defining some abstract subtypes. DiscreteDistribution{F<:VariateForm} = Distribution{F<:VariateForm, Discrete} # Discrete is a subtype of ValueSupport. UnivariateDistributions{S<:ValueSupport} = Distribution{Univariate, S<:ValueSupport} # SingleVariate is a subtype of VariateForm. MultivariateDistribution{S<:ValueSupport} = Distribution{Multivariate, S<:ValueSupport} # SingleVariate is a subtype of VariateForm. # Defining an intersection type. DiscreteUnivariateDistribution = Distribution{Univariate, Discrete}. # `===` is the absolute identity operator. DiscreteUnivariateDistribution === typeintersect(DiscreteDistribution, UnivariateDistribution) # returns `true`. # Examples of `Distribution`s which are subtypes of `DiscreteUnivariateDistribution`, these return `true`. Hypergeometric <: DiscreteUnivariateDistribution. Bernoulli <: DiscreteUnivariateDistribution. `",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:993,availability,operat,operator,993,"Just catching up on this conversation after a conference, glad to see the interest! @falexwolf, I think I can add some context here. Within type theory, these operations with types are actually set operations on a type lattice. For example, a supertype is the union of its subtypes. This is a core feature of type systems. If you're curious about the details, I really like [this paper](doi.org/10.1145/512950.512973) defining abstract interpretation -- which type algebra is a case of. Here's an example I think is pretty cool. Julia's [`Distributions`](https://juliastats.github.io/Distributions.jl/) package uses intersection types in it's model of statistical distributions. `Distributions` defines types for a number of distributions as well as methods to work with them (i.e. `pdf`, `rand`). The following is roughly how intersections are used (more info [here](https://juliastats.github.io/Distributions.jl/stable/types.html)):. ```julia. # Types are capitalized. # `<:` is the subtype operator. # Distributions are parametric on their variate form and value support. Distribution{F<:VariateForm, S<:ValueSupport}. # Defining some abstract subtypes. DiscreteDistribution{F<:VariateForm} = Distribution{F<:VariateForm, Discrete} # Discrete is a subtype of ValueSupport. UnivariateDistributions{S<:ValueSupport} = Distribution{Univariate, S<:ValueSupport} # SingleVariate is a subtype of VariateForm. MultivariateDistribution{S<:ValueSupport} = Distribution{Multivariate, S<:ValueSupport} # SingleVariate is a subtype of VariateForm. # Defining an intersection type. DiscreteUnivariateDistribution = Distribution{Univariate, Discrete}. # `===` is the absolute identity operator. DiscreteUnivariateDistribution === typeintersect(DiscreteDistribution, UnivariateDistribution) # returns `true`. # Examples of `Distribution`s which are subtypes of `DiscreteUnivariateDistribution`, these return `true`. Hypergeometric <: DiscreteUnivariateDistribution. Bernoulli <: DiscreteUnivariateDistribution. `",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1674,availability,operat,operator,1674,"github.io/Distributions.jl/) package uses intersection types in it's model of statistical distributions. `Distributions` defines types for a number of distributions as well as methods to work with them (i.e. `pdf`, `rand`). The following is roughly how intersections are used (more info [here](https://juliastats.github.io/Distributions.jl/stable/types.html)):. ```julia. # Types are capitalized. # `<:` is the subtype operator. # Distributions are parametric on their variate form and value support. Distribution{F<:VariateForm, S<:ValueSupport}. # Defining some abstract subtypes. DiscreteDistribution{F<:VariateForm} = Distribution{F<:VariateForm, Discrete} # Discrete is a subtype of ValueSupport. UnivariateDistributions{S<:ValueSupport} = Distribution{Univariate, S<:ValueSupport} # SingleVariate is a subtype of VariateForm. MultivariateDistribution{S<:ValueSupport} = Distribution{Multivariate, S<:ValueSupport} # SingleVariate is a subtype of VariateForm. # Defining an intersection type. DiscreteUnivariateDistribution = Distribution{Univariate, Discrete}. # `===` is the absolute identity operator. DiscreteUnivariateDistribution === typeintersect(DiscreteDistribution, UnivariateDistribution) # returns `true`. # Examples of `Distribution`s which are subtypes of `DiscreteUnivariateDistribution`, these return `true`. Hypergeometric <: DiscreteUnivariateDistribution. Bernoulli <: DiscreteUnivariateDistribution. ```. Drawing out (crudely) part of the type lattice (edges denote subtype relationship, with supertypes towards the top):. ```. Distribution. / \. DiscreteDistribution UnivariateDistributions. \ /. DiscreteUnivariateDistribution # The intersection. / | \. Bernoulli Binomial Hypergeometric . ```. That said: Julia was designed to make reasoning about types straight forward, and this has limited application to Python. An example I could think of is needing key value lookup which is also ordered could be thought of as the the intersection of `Mapping` and `Sequence` types.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:293,energy efficiency,core,core,293,"Just catching up on this conversation after a conference, glad to see the interest! @falexwolf, I think I can add some context here. Within type theory, these operations with types are actually set operations on a type lattice. For example, a supertype is the union of its subtypes. This is a core feature of type systems. If you're curious about the details, I really like [this paper](doi.org/10.1145/512950.512973) defining abstract interpretation -- which type algebra is a case of. Here's an example I think is pretty cool. Julia's [`Distributions`](https://juliastats.github.io/Distributions.jl/) package uses intersection types in it's model of statistical distributions. `Distributions` defines types for a number of distributions as well as methods to work with them (i.e. `pdf`, `rand`). The following is roughly how intersections are used (more info [here](https://juliastats.github.io/Distributions.jl/stable/types.html)):. ```julia. # Types are capitalized. # `<:` is the subtype operator. # Distributions are parametric on their variate form and value support. Distribution{F<:VariateForm, S<:ValueSupport}. # Defining some abstract subtypes. DiscreteDistribution{F<:VariateForm} = Distribution{F<:VariateForm, Discrete} # Discrete is a subtype of ValueSupport. UnivariateDistributions{S<:ValueSupport} = Distribution{Univariate, S<:ValueSupport} # SingleVariate is a subtype of VariateForm. MultivariateDistribution{S<:ValueSupport} = Distribution{Multivariate, S<:ValueSupport} # SingleVariate is a subtype of VariateForm. # Defining an intersection type. DiscreteUnivariateDistribution = Distribution{Univariate, Discrete}. # `===` is the absolute identity operator. DiscreteUnivariateDistribution === typeintersect(DiscreteDistribution, UnivariateDistribution) # returns `true`. # Examples of `Distribution`s which are subtypes of `DiscreteUnivariateDistribution`, these return `true`. Hypergeometric <: DiscreteUnivariateDistribution. Bernoulli <: DiscreteUnivariateDistribution. `",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:523,energy efficiency,cool,cool,523,"Just catching up on this conversation after a conference, glad to see the interest! @falexwolf, I think I can add some context here. Within type theory, these operations with types are actually set operations on a type lattice. For example, a supertype is the union of its subtypes. This is a core feature of type systems. If you're curious about the details, I really like [this paper](doi.org/10.1145/512950.512973) defining abstract interpretation -- which type algebra is a case of. Here's an example I think is pretty cool. Julia's [`Distributions`](https://juliastats.github.io/Distributions.jl/) package uses intersection types in it's model of statistical distributions. `Distributions` defines types for a number of distributions as well as methods to work with them (i.e. `pdf`, `rand`). The following is roughly how intersections are used (more info [here](https://juliastats.github.io/Distributions.jl/stable/types.html)):. ```julia. # Types are capitalized. # `<:` is the subtype operator. # Distributions are parametric on their variate form and value support. Distribution{F<:VariateForm, S<:ValueSupport}. # Defining some abstract subtypes. DiscreteDistribution{F<:VariateForm} = Distribution{F<:VariateForm, Discrete} # Discrete is a subtype of ValueSupport. UnivariateDistributions{S<:ValueSupport} = Distribution{Univariate, S<:ValueSupport} # SingleVariate is a subtype of VariateForm. MultivariateDistribution{S<:ValueSupport} = Distribution{Multivariate, S<:ValueSupport} # SingleVariate is a subtype of VariateForm. # Defining an intersection type. DiscreteUnivariateDistribution = Distribution{Univariate, Discrete}. # `===` is the absolute identity operator. DiscreteUnivariateDistribution === typeintersect(DiscreteDistribution, UnivariateDistribution) # returns `true`. # Examples of `Distribution`s which are subtypes of `DiscreteUnivariateDistribution`, these return `true`. Hypergeometric <: DiscreteUnivariateDistribution. Bernoulli <: DiscreteUnivariateDistribution. `",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:643,energy efficiency,model,model,643,"Just catching up on this conversation after a conference, glad to see the interest! @falexwolf, I think I can add some context here. Within type theory, these operations with types are actually set operations on a type lattice. For example, a supertype is the union of its subtypes. This is a core feature of type systems. If you're curious about the details, I really like [this paper](doi.org/10.1145/512950.512973) defining abstract interpretation -- which type algebra is a case of. Here's an example I think is pretty cool. Julia's [`Distributions`](https://juliastats.github.io/Distributions.jl/) package uses intersection types in it's model of statistical distributions. `Distributions` defines types for a number of distributions as well as methods to work with them (i.e. `pdf`, `rand`). The following is roughly how intersections are used (more info [here](https://juliastats.github.io/Distributions.jl/stable/types.html)):. ```julia. # Types are capitalized. # `<:` is the subtype operator. # Distributions are parametric on their variate form and value support. Distribution{F<:VariateForm, S<:ValueSupport}. # Defining some abstract subtypes. DiscreteDistribution{F<:VariateForm} = Distribution{F<:VariateForm, Discrete} # Discrete is a subtype of ValueSupport. UnivariateDistributions{S<:ValueSupport} = Distribution{Univariate, S<:ValueSupport} # SingleVariate is a subtype of VariateForm. MultivariateDistribution{S<:ValueSupport} = Distribution{Multivariate, S<:ValueSupport} # SingleVariate is a subtype of VariateForm. # Defining an intersection type. DiscreteUnivariateDistribution = Distribution{Univariate, Discrete}. # `===` is the absolute identity operator. DiscreteUnivariateDistribution === typeintersect(DiscreteDistribution, UnivariateDistribution) # returns `true`. # Examples of `Distribution`s which are subtypes of `DiscreteUnivariateDistribution`, these return `true`. Hypergeometric <: DiscreteUnivariateDistribution. Bernoulli <: DiscreteUnivariateDistribution. `",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:2004,energy efficiency,Draw,Drawing,2004,"github.io/Distributions.jl/) package uses intersection types in it's model of statistical distributions. `Distributions` defines types for a number of distributions as well as methods to work with them (i.e. `pdf`, `rand`). The following is roughly how intersections are used (more info [here](https://juliastats.github.io/Distributions.jl/stable/types.html)):. ```julia. # Types are capitalized. # `<:` is the subtype operator. # Distributions are parametric on their variate form and value support. Distribution{F<:VariateForm, S<:ValueSupport}. # Defining some abstract subtypes. DiscreteDistribution{F<:VariateForm} = Distribution{F<:VariateForm, Discrete} # Discrete is a subtype of ValueSupport. UnivariateDistributions{S<:ValueSupport} = Distribution{Univariate, S<:ValueSupport} # SingleVariate is a subtype of VariateForm. MultivariateDistribution{S<:ValueSupport} = Distribution{Multivariate, S<:ValueSupport} # SingleVariate is a subtype of VariateForm. # Defining an intersection type. DiscreteUnivariateDistribution = Distribution{Univariate, Discrete}. # `===` is the absolute identity operator. DiscreteUnivariateDistribution === typeintersect(DiscreteDistribution, UnivariateDistribution) # returns `true`. # Examples of `Distribution`s which are subtypes of `DiscreteUnivariateDistribution`, these return `true`. Hypergeometric <: DiscreteUnivariateDistribution. Bernoulli <: DiscreteUnivariateDistribution. ```. Drawing out (crudely) part of the type lattice (edges denote subtype relationship, with supertypes towards the top):. ```. Distribution. / \. DiscreteDistribution UnivariateDistributions. \ /. DiscreteUnivariateDistribution # The intersection. / | \. Bernoulli Binomial Hypergeometric . ```. That said: Julia was designed to make reasoning about types straight forward, and this has limited application to Python. An example I could think of is needing key value lookup which is also ordered could be thought of as the the intersection of `Mapping` and `Sequence` types.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:273,integrability,sub,subtypes,273,"Just catching up on this conversation after a conference, glad to see the interest! @falexwolf, I think I can add some context here. Within type theory, these operations with types are actually set operations on a type lattice. For example, a supertype is the union of its subtypes. This is a core feature of type systems. If you're curious about the details, I really like [this paper](doi.org/10.1145/512950.512973) defining abstract interpretation -- which type algebra is a case of. Here's an example I think is pretty cool. Julia's [`Distributions`](https://juliastats.github.io/Distributions.jl/) package uses intersection types in it's model of statistical distributions. `Distributions` defines types for a number of distributions as well as methods to work with them (i.e. `pdf`, `rand`). The following is roughly how intersections are used (more info [here](https://juliastats.github.io/Distributions.jl/stable/types.html)):. ```julia. # Types are capitalized. # `<:` is the subtype operator. # Distributions are parametric on their variate form and value support. Distribution{F<:VariateForm, S<:ValueSupport}. # Defining some abstract subtypes. DiscreteDistribution{F<:VariateForm} = Distribution{F<:VariateForm, Discrete} # Discrete is a subtype of ValueSupport. UnivariateDistributions{S<:ValueSupport} = Distribution{Univariate, S<:ValueSupport} # SingleVariate is a subtype of VariateForm. MultivariateDistribution{S<:ValueSupport} = Distribution{Multivariate, S<:ValueSupport} # SingleVariate is a subtype of VariateForm. # Defining an intersection type. DiscreteUnivariateDistribution = Distribution{Univariate, Discrete}. # `===` is the absolute identity operator. DiscreteUnivariateDistribution === typeintersect(DiscreteDistribution, UnivariateDistribution) # returns `true`. # Examples of `Distribution`s which are subtypes of `DiscreteUnivariateDistribution`, these return `true`. Hypergeometric <: DiscreteUnivariateDistribution. Bernoulli <: DiscreteUnivariateDistribution. `",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:427,integrability,abstract,abstract,427,"Just catching up on this conversation after a conference, glad to see the interest! @falexwolf, I think I can add some context here. Within type theory, these operations with types are actually set operations on a type lattice. For example, a supertype is the union of its subtypes. This is a core feature of type systems. If you're curious about the details, I really like [this paper](doi.org/10.1145/512950.512973) defining abstract interpretation -- which type algebra is a case of. Here's an example I think is pretty cool. Julia's [`Distributions`](https://juliastats.github.io/Distributions.jl/) package uses intersection types in it's model of statistical distributions. `Distributions` defines types for a number of distributions as well as methods to work with them (i.e. `pdf`, `rand`). The following is roughly how intersections are used (more info [here](https://juliastats.github.io/Distributions.jl/stable/types.html)):. ```julia. # Types are capitalized. # `<:` is the subtype operator. # Distributions are parametric on their variate form and value support. Distribution{F<:VariateForm, S<:ValueSupport}. # Defining some abstract subtypes. DiscreteDistribution{F<:VariateForm} = Distribution{F<:VariateForm, Discrete} # Discrete is a subtype of ValueSupport. UnivariateDistributions{S<:ValueSupport} = Distribution{Univariate, S<:ValueSupport} # SingleVariate is a subtype of VariateForm. MultivariateDistribution{S<:ValueSupport} = Distribution{Multivariate, S<:ValueSupport} # SingleVariate is a subtype of VariateForm. # Defining an intersection type. DiscreteUnivariateDistribution = Distribution{Univariate, Discrete}. # `===` is the absolute identity operator. DiscreteUnivariateDistribution === typeintersect(DiscreteDistribution, UnivariateDistribution) # returns `true`. # Examples of `Distribution`s which are subtypes of `DiscreteUnivariateDistribution`, these return `true`. Hypergeometric <: DiscreteUnivariateDistribution. Bernoulli <: DiscreteUnivariateDistribution. `",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:985,integrability,sub,subtype,985,"Just catching up on this conversation after a conference, glad to see the interest! @falexwolf, I think I can add some context here. Within type theory, these operations with types are actually set operations on a type lattice. For example, a supertype is the union of its subtypes. This is a core feature of type systems. If you're curious about the details, I really like [this paper](doi.org/10.1145/512950.512973) defining abstract interpretation -- which type algebra is a case of. Here's an example I think is pretty cool. Julia's [`Distributions`](https://juliastats.github.io/Distributions.jl/) package uses intersection types in it's model of statistical distributions. `Distributions` defines types for a number of distributions as well as methods to work with them (i.e. `pdf`, `rand`). The following is roughly how intersections are used (more info [here](https://juliastats.github.io/Distributions.jl/stable/types.html)):. ```julia. # Types are capitalized. # `<:` is the subtype operator. # Distributions are parametric on their variate form and value support. Distribution{F<:VariateForm, S<:ValueSupport}. # Defining some abstract subtypes. DiscreteDistribution{F<:VariateForm} = Distribution{F<:VariateForm, Discrete} # Discrete is a subtype of ValueSupport. UnivariateDistributions{S<:ValueSupport} = Distribution{Univariate, S<:ValueSupport} # SingleVariate is a subtype of VariateForm. MultivariateDistribution{S<:ValueSupport} = Distribution{Multivariate, S<:ValueSupport} # SingleVariate is a subtype of VariateForm. # Defining an intersection type. DiscreteUnivariateDistribution = Distribution{Univariate, Discrete}. # `===` is the absolute identity operator. DiscreteUnivariateDistribution === typeintersect(DiscreteDistribution, UnivariateDistribution) # returns `true`. # Examples of `Distribution`s which are subtypes of `DiscreteUnivariateDistribution`, these return `true`. Hypergeometric <: DiscreteUnivariateDistribution. Bernoulli <: DiscreteUnivariateDistribution. `",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1138,integrability,abstract,abstract,1138,"pe theory, these operations with types are actually set operations on a type lattice. For example, a supertype is the union of its subtypes. This is a core feature of type systems. If you're curious about the details, I really like [this paper](doi.org/10.1145/512950.512973) defining abstract interpretation -- which type algebra is a case of. Here's an example I think is pretty cool. Julia's [`Distributions`](https://juliastats.github.io/Distributions.jl/) package uses intersection types in it's model of statistical distributions. `Distributions` defines types for a number of distributions as well as methods to work with them (i.e. `pdf`, `rand`). The following is roughly how intersections are used (more info [here](https://juliastats.github.io/Distributions.jl/stable/types.html)):. ```julia. # Types are capitalized. # `<:` is the subtype operator. # Distributions are parametric on their variate form and value support. Distribution{F<:VariateForm, S<:ValueSupport}. # Defining some abstract subtypes. DiscreteDistribution{F<:VariateForm} = Distribution{F<:VariateForm, Discrete} # Discrete is a subtype of ValueSupport. UnivariateDistributions{S<:ValueSupport} = Distribution{Univariate, S<:ValueSupport} # SingleVariate is a subtype of VariateForm. MultivariateDistribution{S<:ValueSupport} = Distribution{Multivariate, S<:ValueSupport} # SingleVariate is a subtype of VariateForm. # Defining an intersection type. DiscreteUnivariateDistribution = Distribution{Univariate, Discrete}. # `===` is the absolute identity operator. DiscreteUnivariateDistribution === typeintersect(DiscreteDistribution, UnivariateDistribution) # returns `true`. # Examples of `Distribution`s which are subtypes of `DiscreteUnivariateDistribution`, these return `true`. Hypergeometric <: DiscreteUnivariateDistribution. Bernoulli <: DiscreteUnivariateDistribution. ```. Drawing out (crudely) part of the type lattice (edges denote subtype relationship, with supertypes towards the top):. ```. Distribution. /",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1147,integrability,sub,subtypes,1147,", these operations with types are actually set operations on a type lattice. For example, a supertype is the union of its subtypes. This is a core feature of type systems. If you're curious about the details, I really like [this paper](doi.org/10.1145/512950.512973) defining abstract interpretation -- which type algebra is a case of. Here's an example I think is pretty cool. Julia's [`Distributions`](https://juliastats.github.io/Distributions.jl/) package uses intersection types in it's model of statistical distributions. `Distributions` defines types for a number of distributions as well as methods to work with them (i.e. `pdf`, `rand`). The following is roughly how intersections are used (more info [here](https://juliastats.github.io/Distributions.jl/stable/types.html)):. ```julia. # Types are capitalized. # `<:` is the subtype operator. # Distributions are parametric on their variate form and value support. Distribution{F<:VariateForm, S<:ValueSupport}. # Defining some abstract subtypes. DiscreteDistribution{F<:VariateForm} = Distribution{F<:VariateForm, Discrete} # Discrete is a subtype of ValueSupport. UnivariateDistributions{S<:ValueSupport} = Distribution{Univariate, S<:ValueSupport} # SingleVariate is a subtype of VariateForm. MultivariateDistribution{S<:ValueSupport} = Distribution{Multivariate, S<:ValueSupport} # SingleVariate is a subtype of VariateForm. # Defining an intersection type. DiscreteUnivariateDistribution = Distribution{Univariate, Discrete}. # `===` is the absolute identity operator. DiscreteUnivariateDistribution === typeintersect(DiscreteDistribution, UnivariateDistribution) # returns `true`. # Examples of `Distribution`s which are subtypes of `DiscreteUnivariateDistribution`, these return `true`. Hypergeometric <: DiscreteUnivariateDistribution. Bernoulli <: DiscreteUnivariateDistribution. ```. Drawing out (crudely) part of the type lattice (edges denote subtype relationship, with supertypes towards the top):. ```. Distribution. / \. Discr",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1251,integrability,sub,subtype,1251," the union of its subtypes. This is a core feature of type systems. If you're curious about the details, I really like [this paper](doi.org/10.1145/512950.512973) defining abstract interpretation -- which type algebra is a case of. Here's an example I think is pretty cool. Julia's [`Distributions`](https://juliastats.github.io/Distributions.jl/) package uses intersection types in it's model of statistical distributions. `Distributions` defines types for a number of distributions as well as methods to work with them (i.e. `pdf`, `rand`). The following is roughly how intersections are used (more info [here](https://juliastats.github.io/Distributions.jl/stable/types.html)):. ```julia. # Types are capitalized. # `<:` is the subtype operator. # Distributions are parametric on their variate form and value support. Distribution{F<:VariateForm, S<:ValueSupport}. # Defining some abstract subtypes. DiscreteDistribution{F<:VariateForm} = Distribution{F<:VariateForm, Discrete} # Discrete is a subtype of ValueSupport. UnivariateDistributions{S<:ValueSupport} = Distribution{Univariate, S<:ValueSupport} # SingleVariate is a subtype of VariateForm. MultivariateDistribution{S<:ValueSupport} = Distribution{Multivariate, S<:ValueSupport} # SingleVariate is a subtype of VariateForm. # Defining an intersection type. DiscreteUnivariateDistribution = Distribution{Univariate, Discrete}. # `===` is the absolute identity operator. DiscreteUnivariateDistribution === typeintersect(DiscreteDistribution, UnivariateDistribution) # returns `true`. # Examples of `Distribution`s which are subtypes of `DiscreteUnivariateDistribution`, these return `true`. Hypergeometric <: DiscreteUnivariateDistribution. Bernoulli <: DiscreteUnivariateDistribution. ```. Drawing out (crudely) part of the type lattice (edges denote subtype relationship, with supertypes towards the top):. ```. Distribution. / \. DiscreteDistribution UnivariateDistributions. \ /. DiscreteUnivariateDistribution # The intersection. / | \. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1382,integrability,sub,subtype,1382,"(doi.org/10.1145/512950.512973) defining abstract interpretation -- which type algebra is a case of. Here's an example I think is pretty cool. Julia's [`Distributions`](https://juliastats.github.io/Distributions.jl/) package uses intersection types in it's model of statistical distributions. `Distributions` defines types for a number of distributions as well as methods to work with them (i.e. `pdf`, `rand`). The following is roughly how intersections are used (more info [here](https://juliastats.github.io/Distributions.jl/stable/types.html)):. ```julia. # Types are capitalized. # `<:` is the subtype operator. # Distributions are parametric on their variate form and value support. Distribution{F<:VariateForm, S<:ValueSupport}. # Defining some abstract subtypes. DiscreteDistribution{F<:VariateForm} = Distribution{F<:VariateForm, Discrete} # Discrete is a subtype of ValueSupport. UnivariateDistributions{S<:ValueSupport} = Distribution{Univariate, S<:ValueSupport} # SingleVariate is a subtype of VariateForm. MultivariateDistribution{S<:ValueSupport} = Distribution{Multivariate, S<:ValueSupport} # SingleVariate is a subtype of VariateForm. # Defining an intersection type. DiscreteUnivariateDistribution = Distribution{Univariate, Discrete}. # `===` is the absolute identity operator. DiscreteUnivariateDistribution === typeintersect(DiscreteDistribution, UnivariateDistribution) # returns `true`. # Examples of `Distribution`s which are subtypes of `DiscreteUnivariateDistribution`, these return `true`. Hypergeometric <: DiscreteUnivariateDistribution. Bernoulli <: DiscreteUnivariateDistribution. ```. Drawing out (crudely) part of the type lattice (edges denote subtype relationship, with supertypes towards the top):. ```. Distribution. / \. DiscreteDistribution UnivariateDistributions. \ /. DiscreteUnivariateDistribution # The intersection. / | \. Bernoulli Binomial Hypergeometric . ```. That said: Julia was designed to make reasoning about types straight forward, and this has",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1515,integrability,sub,subtype,1515,"tty cool. Julia's [`Distributions`](https://juliastats.github.io/Distributions.jl/) package uses intersection types in it's model of statistical distributions. `Distributions` defines types for a number of distributions as well as methods to work with them (i.e. `pdf`, `rand`). The following is roughly how intersections are used (more info [here](https://juliastats.github.io/Distributions.jl/stable/types.html)):. ```julia. # Types are capitalized. # `<:` is the subtype operator. # Distributions are parametric on their variate form and value support. Distribution{F<:VariateForm, S<:ValueSupport}. # Defining some abstract subtypes. DiscreteDistribution{F<:VariateForm} = Distribution{F<:VariateForm, Discrete} # Discrete is a subtype of ValueSupport. UnivariateDistributions{S<:ValueSupport} = Distribution{Univariate, S<:ValueSupport} # SingleVariate is a subtype of VariateForm. MultivariateDistribution{S<:ValueSupport} = Distribution{Multivariate, S<:ValueSupport} # SingleVariate is a subtype of VariateForm. # Defining an intersection type. DiscreteUnivariateDistribution = Distribution{Univariate, Discrete}. # `===` is the absolute identity operator. DiscreteUnivariateDistribution === typeintersect(DiscreteDistribution, UnivariateDistribution) # returns `true`. # Examples of `Distribution`s which are subtypes of `DiscreteUnivariateDistribution`, these return `true`. Hypergeometric <: DiscreteUnivariateDistribution. Bernoulli <: DiscreteUnivariateDistribution. ```. Drawing out (crudely) part of the type lattice (edges denote subtype relationship, with supertypes towards the top):. ```. Distribution. / \. DiscreteDistribution UnivariateDistributions. \ /. DiscreteUnivariateDistribution # The intersection. / | \. Bernoulli Binomial Hypergeometric . ```. That said: Julia was designed to make reasoning about types straight forward, and this has limited application to Python. An example I could think of is needing key value lookup which is also ordered could be thought of as ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:1837,integrability,sub,subtypes,1837,"github.io/Distributions.jl/) package uses intersection types in it's model of statistical distributions. `Distributions` defines types for a number of distributions as well as methods to work with them (i.e. `pdf`, `rand`). The following is roughly how intersections are used (more info [here](https://juliastats.github.io/Distributions.jl/stable/types.html)):. ```julia. # Types are capitalized. # `<:` is the subtype operator. # Distributions are parametric on their variate form and value support. Distribution{F<:VariateForm, S<:ValueSupport}. # Defining some abstract subtypes. DiscreteDistribution{F<:VariateForm} = Distribution{F<:VariateForm, Discrete} # Discrete is a subtype of ValueSupport. UnivariateDistributions{S<:ValueSupport} = Distribution{Univariate, S<:ValueSupport} # SingleVariate is a subtype of VariateForm. MultivariateDistribution{S<:ValueSupport} = Distribution{Multivariate, S<:ValueSupport} # SingleVariate is a subtype of VariateForm. # Defining an intersection type. DiscreteUnivariateDistribution = Distribution{Univariate, Discrete}. # `===` is the absolute identity operator. DiscreteUnivariateDistribution === typeintersect(DiscreteDistribution, UnivariateDistribution) # returns `true`. # Examples of `Distribution`s which are subtypes of `DiscreteUnivariateDistribution`, these return `true`. Hypergeometric <: DiscreteUnivariateDistribution. Bernoulli <: DiscreteUnivariateDistribution. ```. Drawing out (crudely) part of the type lattice (edges denote subtype relationship, with supertypes towards the top):. ```. Distribution. / \. DiscreteDistribution UnivariateDistributions. \ /. DiscreteUnivariateDistribution # The intersection. / | \. Bernoulli Binomial Hypergeometric . ```. That said: Julia was designed to make reasoning about types straight forward, and this has limited application to Python. An example I could think of is needing key value lookup which is also ordered could be thought of as the the intersection of `Mapping` and `Sequence` types.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:2065,integrability,sub,subtype,2065,"github.io/Distributions.jl/) package uses intersection types in it's model of statistical distributions. `Distributions` defines types for a number of distributions as well as methods to work with them (i.e. `pdf`, `rand`). The following is roughly how intersections are used (more info [here](https://juliastats.github.io/Distributions.jl/stable/types.html)):. ```julia. # Types are capitalized. # `<:` is the subtype operator. # Distributions are parametric on their variate form and value support. Distribution{F<:VariateForm, S<:ValueSupport}. # Defining some abstract subtypes. DiscreteDistribution{F<:VariateForm} = Distribution{F<:VariateForm, Discrete} # Discrete is a subtype of ValueSupport. UnivariateDistributions{S<:ValueSupport} = Distribution{Univariate, S<:ValueSupport} # SingleVariate is a subtype of VariateForm. MultivariateDistribution{S<:ValueSupport} = Distribution{Multivariate, S<:ValueSupport} # SingleVariate is a subtype of VariateForm. # Defining an intersection type. DiscreteUnivariateDistribution = Distribution{Univariate, Discrete}. # `===` is the absolute identity operator. DiscreteUnivariateDistribution === typeintersect(DiscreteDistribution, UnivariateDistribution) # returns `true`. # Examples of `Distribution`s which are subtypes of `DiscreteUnivariateDistribution`, these return `true`. Hypergeometric <: DiscreteUnivariateDistribution. Bernoulli <: DiscreteUnivariateDistribution. ```. Drawing out (crudely) part of the type lattice (edges denote subtype relationship, with supertypes towards the top):. ```. Distribution. / \. DiscreteDistribution UnivariateDistributions. \ /. DiscreteUnivariateDistribution # The intersection. / | \. Bernoulli Binomial Hypergeometric . ```. That said: Julia was designed to make reasoning about types straight forward, and this has limited application to Python. An example I could think of is needing key value lookup which is also ordered could be thought of as the the intersection of `Mapping` and `Sequence` types.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:25,interoperability,convers,conversation,25,"Just catching up on this conversation after a conference, glad to see the interest! @falexwolf, I think I can add some context here. Within type theory, these operations with types are actually set operations on a type lattice. For example, a supertype is the union of its subtypes. This is a core feature of type systems. If you're curious about the details, I really like [this paper](doi.org/10.1145/512950.512973) defining abstract interpretation -- which type algebra is a case of. Here's an example I think is pretty cool. Julia's [`Distributions`](https://juliastats.github.io/Distributions.jl/) package uses intersection types in it's model of statistical distributions. `Distributions` defines types for a number of distributions as well as methods to work with them (i.e. `pdf`, `rand`). The following is roughly how intersections are used (more info [here](https://juliastats.github.io/Distributions.jl/stable/types.html)):. ```julia. # Types are capitalized. # `<:` is the subtype operator. # Distributions are parametric on their variate form and value support. Distribution{F<:VariateForm, S<:ValueSupport}. # Defining some abstract subtypes. DiscreteDistribution{F<:VariateForm} = Distribution{F<:VariateForm, Discrete} # Discrete is a subtype of ValueSupport. UnivariateDistributions{S<:ValueSupport} = Distribution{Univariate, S<:ValueSupport} # SingleVariate is a subtype of VariateForm. MultivariateDistribution{S<:ValueSupport} = Distribution{Multivariate, S<:ValueSupport} # SingleVariate is a subtype of VariateForm. # Defining an intersection type. DiscreteUnivariateDistribution = Distribution{Univariate, Discrete}. # `===` is the absolute identity operator. DiscreteUnivariateDistribution === typeintersect(DiscreteDistribution, UnivariateDistribution) # returns `true`. # Examples of `Distribution`s which are subtypes of `DiscreteUnivariateDistribution`, these return `true`. Hypergeometric <: DiscreteUnivariateDistribution. Bernoulli <: DiscreteUnivariateDistribution. `",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
https://github.com/scverse/scanpy/issues/373:539,interoperability,Distribut,Distributions,539,"Just catching up on this conversation after a conference, glad to see the interest! @falexwolf, I think I can add some context here. Within type theory, these operations with types are actually set operations on a type lattice. For example, a supertype is the union of its subtypes. This is a core feature of type systems. If you're curious about the details, I really like [this paper](doi.org/10.1145/512950.512973) defining abstract interpretation -- which type algebra is a case of. Here's an example I think is pretty cool. Julia's [`Distributions`](https://juliastats.github.io/Distributions.jl/) package uses intersection types in it's model of statistical distributions. `Distributions` defines types for a number of distributions as well as methods to work with them (i.e. `pdf`, `rand`). The following is roughly how intersections are used (more info [here](https://juliastats.github.io/Distributions.jl/stable/types.html)):. ```julia. # Types are capitalized. # `<:` is the subtype operator. # Distributions are parametric on their variate form and value support. Distribution{F<:VariateForm, S<:ValueSupport}. # Defining some abstract subtypes. DiscreteDistribution{F<:VariateForm} = Distribution{F<:VariateForm, Discrete} # Discrete is a subtype of ValueSupport. UnivariateDistributions{S<:ValueSupport} = Distribution{Univariate, S<:ValueSupport} # SingleVariate is a subtype of VariateForm. MultivariateDistribution{S<:ValueSupport} = Distribution{Multivariate, S<:ValueSupport} # SingleVariate is a subtype of VariateForm. # Defining an intersection type. DiscreteUnivariateDistribution = Distribution{Univariate, Discrete}. # `===` is the absolute identity operator. DiscreteUnivariateDistribution === typeintersect(DiscreteDistribution, UnivariateDistribution) # returns `true`. # Examples of `Distribution`s which are subtypes of `DiscreteUnivariateDistribution`, these return `true`. Hypergeometric <: DiscreteUnivariateDistribution. Bernoulli <: DiscreteUnivariateDistribution. `",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373
