id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/google/deepvariant/issues/104:580,security,compl,complete,580,"@ksw9 can you post the full sequence of commands? I tried inside the docker, and it seems to be fine:. ```. [pichuan@pichuan-centos-test ~]$ sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.1. Unable to find image 'gcr.io/deepvariant-docker/deepvariant:0.7.1' locally. Trying to pull repository gcr.io/deepvariant-docker/deepvariant ... . 0.7.1: Pulling from gcr.io/deepvariant-docker/deepvariant. 18d680d61657: Pull complete . 0addb6fece63: Pull complete . 78e58219b215: Pull complete . eb6959a66df2: Pull complete . 54de1d38bbd7: Pull complete . d17c3563217d: Pull complete . ba1bdbdefce9: Pull complete . 94eba53c4ad9: Pull complete . 413f494b0501: Pull complete . 4d89363e7fb4: Pull complete . e9213d1ccf36: Pull complete . fb6121657d6b: Pull complete . Digest: sha256:705523e7bc241e0d3e1e57d4d338c83e289d51a8231c09b0d2dee03c015cee0f. Status: Downloaded newer image for gcr.io/deepvariant-docker/deepvariant:0.7.1. root@e2fb03e85f9e:/# python -c ""import numpy"". root@e2fb03e85f9e:/# python -c ""import numpy as np"". root@e2fb03e85f9e:/# . ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104
https://github.com/google/deepvariant/issues/104:610,security,compl,complete,610,"@ksw9 can you post the full sequence of commands? I tried inside the docker, and it seems to be fine:. ```. [pichuan@pichuan-centos-test ~]$ sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.1. Unable to find image 'gcr.io/deepvariant-docker/deepvariant:0.7.1' locally. Trying to pull repository gcr.io/deepvariant-docker/deepvariant ... . 0.7.1: Pulling from gcr.io/deepvariant-docker/deepvariant. 18d680d61657: Pull complete . 0addb6fece63: Pull complete . 78e58219b215: Pull complete . eb6959a66df2: Pull complete . 54de1d38bbd7: Pull complete . d17c3563217d: Pull complete . ba1bdbdefce9: Pull complete . 94eba53c4ad9: Pull complete . 413f494b0501: Pull complete . 4d89363e7fb4: Pull complete . e9213d1ccf36: Pull complete . fb6121657d6b: Pull complete . Digest: sha256:705523e7bc241e0d3e1e57d4d338c83e289d51a8231c09b0d2dee03c015cee0f. Status: Downloaded newer image for gcr.io/deepvariant-docker/deepvariant:0.7.1. root@e2fb03e85f9e:/# python -c ""import numpy"". root@e2fb03e85f9e:/# python -c ""import numpy as np"". root@e2fb03e85f9e:/# . ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104
https://github.com/google/deepvariant/issues/104:640,security,compl,complete,640,"@ksw9 can you post the full sequence of commands? I tried inside the docker, and it seems to be fine:. ```. [pichuan@pichuan-centos-test ~]$ sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.1. Unable to find image 'gcr.io/deepvariant-docker/deepvariant:0.7.1' locally. Trying to pull repository gcr.io/deepvariant-docker/deepvariant ... . 0.7.1: Pulling from gcr.io/deepvariant-docker/deepvariant. 18d680d61657: Pull complete . 0addb6fece63: Pull complete . 78e58219b215: Pull complete . eb6959a66df2: Pull complete . 54de1d38bbd7: Pull complete . d17c3563217d: Pull complete . ba1bdbdefce9: Pull complete . 94eba53c4ad9: Pull complete . 413f494b0501: Pull complete . 4d89363e7fb4: Pull complete . e9213d1ccf36: Pull complete . fb6121657d6b: Pull complete . Digest: sha256:705523e7bc241e0d3e1e57d4d338c83e289d51a8231c09b0d2dee03c015cee0f. Status: Downloaded newer image for gcr.io/deepvariant-docker/deepvariant:0.7.1. root@e2fb03e85f9e:/# python -c ""import numpy"". root@e2fb03e85f9e:/# python -c ""import numpy as np"". root@e2fb03e85f9e:/# . ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104
https://github.com/google/deepvariant/issues/104:670,security,compl,complete,670,"@ksw9 can you post the full sequence of commands? I tried inside the docker, and it seems to be fine:. ```. [pichuan@pichuan-centos-test ~]$ sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.1. Unable to find image 'gcr.io/deepvariant-docker/deepvariant:0.7.1' locally. Trying to pull repository gcr.io/deepvariant-docker/deepvariant ... . 0.7.1: Pulling from gcr.io/deepvariant-docker/deepvariant. 18d680d61657: Pull complete . 0addb6fece63: Pull complete . 78e58219b215: Pull complete . eb6959a66df2: Pull complete . 54de1d38bbd7: Pull complete . d17c3563217d: Pull complete . ba1bdbdefce9: Pull complete . 94eba53c4ad9: Pull complete . 413f494b0501: Pull complete . 4d89363e7fb4: Pull complete . e9213d1ccf36: Pull complete . fb6121657d6b: Pull complete . Digest: sha256:705523e7bc241e0d3e1e57d4d338c83e289d51a8231c09b0d2dee03c015cee0f. Status: Downloaded newer image for gcr.io/deepvariant-docker/deepvariant:0.7.1. root@e2fb03e85f9e:/# python -c ""import numpy"". root@e2fb03e85f9e:/# python -c ""import numpy as np"". root@e2fb03e85f9e:/# . ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104
https://github.com/google/deepvariant/issues/104:700,security,compl,complete,700,"@ksw9 can you post the full sequence of commands? I tried inside the docker, and it seems to be fine:. ```. [pichuan@pichuan-centos-test ~]$ sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.1. Unable to find image 'gcr.io/deepvariant-docker/deepvariant:0.7.1' locally. Trying to pull repository gcr.io/deepvariant-docker/deepvariant ... . 0.7.1: Pulling from gcr.io/deepvariant-docker/deepvariant. 18d680d61657: Pull complete . 0addb6fece63: Pull complete . 78e58219b215: Pull complete . eb6959a66df2: Pull complete . 54de1d38bbd7: Pull complete . d17c3563217d: Pull complete . ba1bdbdefce9: Pull complete . 94eba53c4ad9: Pull complete . 413f494b0501: Pull complete . 4d89363e7fb4: Pull complete . e9213d1ccf36: Pull complete . fb6121657d6b: Pull complete . Digest: sha256:705523e7bc241e0d3e1e57d4d338c83e289d51a8231c09b0d2dee03c015cee0f. Status: Downloaded newer image for gcr.io/deepvariant-docker/deepvariant:0.7.1. root@e2fb03e85f9e:/# python -c ""import numpy"". root@e2fb03e85f9e:/# python -c ""import numpy as np"". root@e2fb03e85f9e:/# . ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104
https://github.com/google/deepvariant/issues/104:730,security,compl,complete,730,"@ksw9 can you post the full sequence of commands? I tried inside the docker, and it seems to be fine:. ```. [pichuan@pichuan-centos-test ~]$ sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.1. Unable to find image 'gcr.io/deepvariant-docker/deepvariant:0.7.1' locally. Trying to pull repository gcr.io/deepvariant-docker/deepvariant ... . 0.7.1: Pulling from gcr.io/deepvariant-docker/deepvariant. 18d680d61657: Pull complete . 0addb6fece63: Pull complete . 78e58219b215: Pull complete . eb6959a66df2: Pull complete . 54de1d38bbd7: Pull complete . d17c3563217d: Pull complete . ba1bdbdefce9: Pull complete . 94eba53c4ad9: Pull complete . 413f494b0501: Pull complete . 4d89363e7fb4: Pull complete . e9213d1ccf36: Pull complete . fb6121657d6b: Pull complete . Digest: sha256:705523e7bc241e0d3e1e57d4d338c83e289d51a8231c09b0d2dee03c015cee0f. Status: Downloaded newer image for gcr.io/deepvariant-docker/deepvariant:0.7.1. root@e2fb03e85f9e:/# python -c ""import numpy"". root@e2fb03e85f9e:/# python -c ""import numpy as np"". root@e2fb03e85f9e:/# . ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104
https://github.com/google/deepvariant/issues/104:760,security,compl,complete,760,"@ksw9 can you post the full sequence of commands? I tried inside the docker, and it seems to be fine:. ```. [pichuan@pichuan-centos-test ~]$ sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.1. Unable to find image 'gcr.io/deepvariant-docker/deepvariant:0.7.1' locally. Trying to pull repository gcr.io/deepvariant-docker/deepvariant ... . 0.7.1: Pulling from gcr.io/deepvariant-docker/deepvariant. 18d680d61657: Pull complete . 0addb6fece63: Pull complete . 78e58219b215: Pull complete . eb6959a66df2: Pull complete . 54de1d38bbd7: Pull complete . d17c3563217d: Pull complete . ba1bdbdefce9: Pull complete . 94eba53c4ad9: Pull complete . 413f494b0501: Pull complete . 4d89363e7fb4: Pull complete . e9213d1ccf36: Pull complete . fb6121657d6b: Pull complete . Digest: sha256:705523e7bc241e0d3e1e57d4d338c83e289d51a8231c09b0d2dee03c015cee0f. Status: Downloaded newer image for gcr.io/deepvariant-docker/deepvariant:0.7.1. root@e2fb03e85f9e:/# python -c ""import numpy"". root@e2fb03e85f9e:/# python -c ""import numpy as np"". root@e2fb03e85f9e:/# . ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104
https://github.com/google/deepvariant/issues/104:132,testability,test,test,132,"@ksw9 can you post the full sequence of commands? I tried inside the docker, and it seems to be fine:. ```. [pichuan@pichuan-centos-test ~]$ sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.1. Unable to find image 'gcr.io/deepvariant-docker/deepvariant:0.7.1' locally. Trying to pull repository gcr.io/deepvariant-docker/deepvariant ... . 0.7.1: Pulling from gcr.io/deepvariant-docker/deepvariant. 18d680d61657: Pull complete . 0addb6fece63: Pull complete . 78e58219b215: Pull complete . eb6959a66df2: Pull complete . 54de1d38bbd7: Pull complete . d17c3563217d: Pull complete . ba1bdbdefce9: Pull complete . 94eba53c4ad9: Pull complete . 413f494b0501: Pull complete . 4d89363e7fb4: Pull complete . e9213d1ccf36: Pull complete . fb6121657d6b: Pull complete . Digest: sha256:705523e7bc241e0d3e1e57d4d338c83e289d51a8231c09b0d2dee03c015cee0f. Status: Downloaded newer image for gcr.io/deepvariant-docker/deepvariant:0.7.1. root@e2fb03e85f9e:/# python -c ""import numpy"". root@e2fb03e85f9e:/# python -c ""import numpy as np"". root@e2fb03e85f9e:/# . ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104
https://github.com/google/deepvariant/issues/104:40,usability,command,commands,40,"@ksw9 can you post the full sequence of commands? I tried inside the docker, and it seems to be fine:. ```. [pichuan@pichuan-centos-test ~]$ sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.1. Unable to find image 'gcr.io/deepvariant-docker/deepvariant:0.7.1' locally. Trying to pull repository gcr.io/deepvariant-docker/deepvariant ... . 0.7.1: Pulling from gcr.io/deepvariant-docker/deepvariant. 18d680d61657: Pull complete . 0addb6fece63: Pull complete . 78e58219b215: Pull complete . eb6959a66df2: Pull complete . 54de1d38bbd7: Pull complete . d17c3563217d: Pull complete . ba1bdbdefce9: Pull complete . 94eba53c4ad9: Pull complete . 413f494b0501: Pull complete . 4d89363e7fb4: Pull complete . e9213d1ccf36: Pull complete . fb6121657d6b: Pull complete . Digest: sha256:705523e7bc241e0d3e1e57d4d338c83e289d51a8231c09b0d2dee03c015cee0f. Status: Downloaded newer image for gcr.io/deepvariant-docker/deepvariant:0.7.1. root@e2fb03e85f9e:/# python -c ""import numpy"". root@e2fb03e85f9e:/# python -c ""import numpy as np"". root@e2fb03e85f9e:/# . ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104
https://github.com/google/deepvariant/issues/104:852,usability,Statu,Status,852,"@ksw9 can you post the full sequence of commands? I tried inside the docker, and it seems to be fine:. ```. [pichuan@pichuan-centos-test ~]$ sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.1. Unable to find image 'gcr.io/deepvariant-docker/deepvariant:0.7.1' locally. Trying to pull repository gcr.io/deepvariant-docker/deepvariant ... . 0.7.1: Pulling from gcr.io/deepvariant-docker/deepvariant. 18d680d61657: Pull complete . 0addb6fece63: Pull complete . 78e58219b215: Pull complete . eb6959a66df2: Pull complete . 54de1d38bbd7: Pull complete . d17c3563217d: Pull complete . ba1bdbdefce9: Pull complete . 94eba53c4ad9: Pull complete . 413f494b0501: Pull complete . 4d89363e7fb4: Pull complete . e9213d1ccf36: Pull complete . fb6121657d6b: Pull complete . Digest: sha256:705523e7bc241e0d3e1e57d4d338c83e289d51a8231c09b0d2dee03c015cee0f. Status: Downloaded newer image for gcr.io/deepvariant-docker/deepvariant:0.7.1. root@e2fb03e85f9e:/# python -c ""import numpy"". root@e2fb03e85f9e:/# python -c ""import numpy as np"". root@e2fb03e85f9e:/# . ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104
https://github.com/google/deepvariant/issues/104:170,availability,error,error,170,"Hi, . I am now testing the quickstart with the test data (https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-quick-start.md) inside of docker and I get the error ValueError: Not found: Could not open quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. I followed the instructions and . ```. OUTPUT_DIR=quickstart-testdata/. REF=quickstart-testdata/ucsc.hg19.chr20.unittest.fasta. BAM=quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ## test deepvariant. docker run \. -v /home/${USER}:/home/${USER} \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". ```. The BAM file is there, when I try ls $BAM, I get: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. Any suggestions would be great, thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104
https://github.com/google/deepvariant/issues/104:170,performance,error,error,170,"Hi, . I am now testing the quickstart with the test data (https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-quick-start.md) inside of docker and I get the error ValueError: Not found: Could not open quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. I followed the instructions and . ```. OUTPUT_DIR=quickstart-testdata/. REF=quickstart-testdata/ucsc.hg19.chr20.unittest.fasta. BAM=quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ## test deepvariant. docker run \. -v /home/${USER}:/home/${USER} \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". ```. The BAM file is there, when I try ls $BAM, I get: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. Any suggestions would be great, thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104
https://github.com/google/deepvariant/issues/104:15,safety,test,testing,15,"Hi, . I am now testing the quickstart with the test data (https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-quick-start.md) inside of docker and I get the error ValueError: Not found: Could not open quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. I followed the instructions and . ```. OUTPUT_DIR=quickstart-testdata/. REF=quickstart-testdata/ucsc.hg19.chr20.unittest.fasta. BAM=quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ## test deepvariant. docker run \. -v /home/${USER}:/home/${USER} \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". ```. The BAM file is there, when I try ls $BAM, I get: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. Any suggestions would be great, thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104
https://github.com/google/deepvariant/issues/104:47,safety,test,test,47,"Hi, . I am now testing the quickstart with the test data (https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-quick-start.md) inside of docker and I get the error ValueError: Not found: Could not open quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. I followed the instructions and . ```. OUTPUT_DIR=quickstart-testdata/. REF=quickstart-testdata/ucsc.hg19.chr20.unittest.fasta. BAM=quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ## test deepvariant. docker run \. -v /home/${USER}:/home/${USER} \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". ```. The BAM file is there, when I try ls $BAM, I get: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. Any suggestions would be great, thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104
https://github.com/google/deepvariant/issues/104:170,safety,error,error,170,"Hi, . I am now testing the quickstart with the test data (https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-quick-start.md) inside of docker and I get the error ValueError: Not found: Could not open quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. I followed the instructions and . ```. OUTPUT_DIR=quickstart-testdata/. REF=quickstart-testdata/ucsc.hg19.chr20.unittest.fasta. BAM=quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ## test deepvariant. docker run \. -v /home/${USER}:/home/${USER} \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". ```. The BAM file is there, when I try ls $BAM, I get: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. Any suggestions would be great, thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104
https://github.com/google/deepvariant/issues/104:225,safety,test,testdata,225,"Hi, . I am now testing the quickstart with the test data (https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-quick-start.md) inside of docker and I get the error ValueError: Not found: Could not open quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. I followed the instructions and . ```. OUTPUT_DIR=quickstart-testdata/. REF=quickstart-testdata/ucsc.hg19.chr20.unittest.fasta. BAM=quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ## test deepvariant. docker run \. -v /home/${USER}:/home/${USER} \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". ```. The BAM file is there, when I try ls $BAM, I get: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. Any suggestions would be great, thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104
https://github.com/google/deepvariant/issues/104:327,safety,test,testdata,327,"Hi, . I am now testing the quickstart with the test data (https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-quick-start.md) inside of docker and I get the error ValueError: Not found: Could not open quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. I followed the instructions and . ```. OUTPUT_DIR=quickstart-testdata/. REF=quickstart-testdata/ucsc.hg19.chr20.unittest.fasta. BAM=quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ## test deepvariant. docker run \. -v /home/${USER}:/home/${USER} \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". ```. The BAM file is there, when I try ls $BAM, I get: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. Any suggestions would be great, thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104
https://github.com/google/deepvariant/issues/104:353,safety,test,testdata,353,"Hi, . I am now testing the quickstart with the test data (https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-quick-start.md) inside of docker and I get the error ValueError: Not found: Could not open quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. I followed the instructions and . ```. OUTPUT_DIR=quickstart-testdata/. REF=quickstart-testdata/ucsc.hg19.chr20.unittest.fasta. BAM=quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ## test deepvariant. docker run \. -v /home/${USER}:/home/${USER} \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". ```. The BAM file is there, when I try ls $BAM, I get: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. Any suggestions would be great, thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104
https://github.com/google/deepvariant/issues/104:409,safety,test,testdata,409,"Hi, . I am now testing the quickstart with the test data (https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-quick-start.md) inside of docker and I get the error ValueError: Not found: Could not open quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. I followed the instructions and . ```. OUTPUT_DIR=quickstart-testdata/. REF=quickstart-testdata/ucsc.hg19.chr20.unittest.fasta. BAM=quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ## test deepvariant. docker run \. -v /home/${USER}:/home/${USER} \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". ```. The BAM file is there, when I try ls $BAM, I get: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. Any suggestions would be great, thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104
https://github.com/google/deepvariant/issues/104:453,safety,test,test,453,"Hi, . I am now testing the quickstart with the test data (https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-quick-start.md) inside of docker and I get the error ValueError: Not found: Could not open quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. I followed the instructions and . ```. OUTPUT_DIR=quickstart-testdata/. REF=quickstart-testdata/ucsc.hg19.chr20.unittest.fasta. BAM=quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ## test deepvariant. docker run \. -v /home/${USER}:/home/${USER} \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". ```. The BAM file is there, when I try ls $BAM, I get: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. Any suggestions would be great, thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104
https://github.com/google/deepvariant/issues/104:829,safety,test,testdata,829,"Hi, . I am now testing the quickstart with the test data (https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-quick-start.md) inside of docker and I get the error ValueError: Not found: Could not open quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. I followed the instructions and . ```. OUTPUT_DIR=quickstart-testdata/. REF=quickstart-testdata/ucsc.hg19.chr20.unittest.fasta. BAM=quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ## test deepvariant. docker run \. -v /home/${USER}:/home/${USER} \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". ```. The BAM file is there, when I try ls $BAM, I get: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. Any suggestions would be great, thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104
https://github.com/google/deepvariant/issues/104:15,testability,test,testing,15,"Hi, . I am now testing the quickstart with the test data (https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-quick-start.md) inside of docker and I get the error ValueError: Not found: Could not open quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. I followed the instructions and . ```. OUTPUT_DIR=quickstart-testdata/. REF=quickstart-testdata/ucsc.hg19.chr20.unittest.fasta. BAM=quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ## test deepvariant. docker run \. -v /home/${USER}:/home/${USER} \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". ```. The BAM file is there, when I try ls $BAM, I get: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. Any suggestions would be great, thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104
https://github.com/google/deepvariant/issues/104:47,testability,test,test,47,"Hi, . I am now testing the quickstart with the test data (https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-quick-start.md) inside of docker and I get the error ValueError: Not found: Could not open quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. I followed the instructions and . ```. OUTPUT_DIR=quickstart-testdata/. REF=quickstart-testdata/ucsc.hg19.chr20.unittest.fasta. BAM=quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ## test deepvariant. docker run \. -v /home/${USER}:/home/${USER} \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". ```. The BAM file is there, when I try ls $BAM, I get: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. Any suggestions would be great, thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104
https://github.com/google/deepvariant/issues/104:225,testability,test,testdata,225,"Hi, . I am now testing the quickstart with the test data (https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-quick-start.md) inside of docker and I get the error ValueError: Not found: Could not open quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. I followed the instructions and . ```. OUTPUT_DIR=quickstart-testdata/. REF=quickstart-testdata/ucsc.hg19.chr20.unittest.fasta. BAM=quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ## test deepvariant. docker run \. -v /home/${USER}:/home/${USER} \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". ```. The BAM file is there, when I try ls $BAM, I get: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. Any suggestions would be great, thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104
https://github.com/google/deepvariant/issues/104:327,testability,test,testdata,327,"Hi, . I am now testing the quickstart with the test data (https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-quick-start.md) inside of docker and I get the error ValueError: Not found: Could not open quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. I followed the instructions and . ```. OUTPUT_DIR=quickstart-testdata/. REF=quickstart-testdata/ucsc.hg19.chr20.unittest.fasta. BAM=quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ## test deepvariant. docker run \. -v /home/${USER}:/home/${USER} \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". ```. The BAM file is there, when I try ls $BAM, I get: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. Any suggestions would be great, thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104
https://github.com/google/deepvariant/issues/104:353,testability,test,testdata,353,"Hi, . I am now testing the quickstart with the test data (https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-quick-start.md) inside of docker and I get the error ValueError: Not found: Could not open quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. I followed the instructions and . ```. OUTPUT_DIR=quickstart-testdata/. REF=quickstart-testdata/ucsc.hg19.chr20.unittest.fasta. BAM=quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ## test deepvariant. docker run \. -v /home/${USER}:/home/${USER} \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". ```. The BAM file is there, when I try ls $BAM, I get: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. Any suggestions would be great, thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104
https://github.com/google/deepvariant/issues/104:378,testability,unit,unittest,378,"Hi, . I am now testing the quickstart with the test data (https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-quick-start.md) inside of docker and I get the error ValueError: Not found: Could not open quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. I followed the instructions and . ```. OUTPUT_DIR=quickstart-testdata/. REF=quickstart-testdata/ucsc.hg19.chr20.unittest.fasta. BAM=quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ## test deepvariant. docker run \. -v /home/${USER}:/home/${USER} \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". ```. The BAM file is there, when I try ls $BAM, I get: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. Any suggestions would be great, thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104
https://github.com/google/deepvariant/issues/104:409,testability,test,testdata,409,"Hi, . I am now testing the quickstart with the test data (https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-quick-start.md) inside of docker and I get the error ValueError: Not found: Could not open quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. I followed the instructions and . ```. OUTPUT_DIR=quickstart-testdata/. REF=quickstart-testdata/ucsc.hg19.chr20.unittest.fasta. BAM=quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ## test deepvariant. docker run \. -v /home/${USER}:/home/${USER} \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". ```. The BAM file is there, when I try ls $BAM, I get: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. Any suggestions would be great, thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104
https://github.com/google/deepvariant/issues/104:453,testability,test,test,453,"Hi, . I am now testing the quickstart with the test data (https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-quick-start.md) inside of docker and I get the error ValueError: Not found: Could not open quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. I followed the instructions and . ```. OUTPUT_DIR=quickstart-testdata/. REF=quickstart-testdata/ucsc.hg19.chr20.unittest.fasta. BAM=quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ## test deepvariant. docker run \. -v /home/${USER}:/home/${USER} \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". ```. The BAM file is there, when I try ls $BAM, I get: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. Any suggestions would be great, thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104
https://github.com/google/deepvariant/issues/104:829,testability,test,testdata,829,"Hi, . I am now testing the quickstart with the test data (https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-quick-start.md) inside of docker and I get the error ValueError: Not found: Could not open quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. I followed the instructions and . ```. OUTPUT_DIR=quickstart-testdata/. REF=quickstart-testdata/ucsc.hg19.chr20.unittest.fasta. BAM=quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ## test deepvariant. docker run \. -v /home/${USER}:/home/${USER} \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". ```. The BAM file is there, when I try ls $BAM, I get: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. Any suggestions would be great, thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104
https://github.com/google/deepvariant/issues/104:170,usability,error,error,170,"Hi, . I am now testing the quickstart with the test data (https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-quick-start.md) inside of docker and I get the error ValueError: Not found: Could not open quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. I followed the instructions and . ```. OUTPUT_DIR=quickstart-testdata/. REF=quickstart-testdata/ucsc.hg19.chr20.unittest.fasta. BAM=quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ## test deepvariant. docker run \. -v /home/${USER}:/home/${USER} \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". ```. The BAM file is there, when I try ls $BAM, I get: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. Any suggestions would be great, thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104
https://github.com/google/deepvariant/issues/104:496,usability,USER,USER,496,"Hi, . I am now testing the quickstart with the test data (https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-quick-start.md) inside of docker and I get the error ValueError: Not found: Could not open quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. I followed the instructions and . ```. OUTPUT_DIR=quickstart-testdata/. REF=quickstart-testdata/ucsc.hg19.chr20.unittest.fasta. BAM=quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ## test deepvariant. docker run \. -v /home/${USER}:/home/${USER} \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". ```. The BAM file is there, when I try ls $BAM, I get: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. Any suggestions would be great, thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104
https://github.com/google/deepvariant/issues/104:510,usability,USER,USER,510,"Hi, . I am now testing the quickstart with the test data (https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-quick-start.md) inside of docker and I get the error ValueError: Not found: Could not open quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. I followed the instructions and . ```. OUTPUT_DIR=quickstart-testdata/. REF=quickstart-testdata/ucsc.hg19.chr20.unittest.fasta. BAM=quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ## test deepvariant. docker run \. -v /home/${USER}:/home/${USER} \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""${REF}"" \. --reads ""${BAM}"" \. --regions ""chr20:10,000,000-10,010,000"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". ```. The BAM file is there, when I try ls $BAM, I get: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. Any suggestions would be great, thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104
https://github.com/google/deepvariant/issues/104:56,deployability,updat,updated,56,"@ksw9 . Hi, the current quickstart needs a small fix (I updated it internally; should come out in the next release):. `/home/${USER}` should be `${HOME}` to be more general. And, note that in order for docker to access your file system, you do need the `-v` path. So you probably want something like:. ```. OUTPUT_DIR=${HOME}/quickstart-output. mkdir -p ""${OUTPUT_DIR}"". REF=${HOME}/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta. BAM=${HOME}/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ```. and . `-v ${HOME}:${HOME} `. in the docker command",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104
https://github.com/google/deepvariant/issues/104:107,deployability,releas,release,107,"@ksw9 . Hi, the current quickstart needs a small fix (I updated it internally; should come out in the next release):. `/home/${USER}` should be `${HOME}` to be more general. And, note that in order for docker to access your file system, you do need the `-v` path. So you probably want something like:. ```. OUTPUT_DIR=${HOME}/quickstart-output. mkdir -p ""${OUTPUT_DIR}"". REF=${HOME}/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta. BAM=${HOME}/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ```. and . `-v ${HOME}:${HOME} `. in the docker command",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104
https://github.com/google/deepvariant/issues/104:16,energy efficiency,current,current,16,"@ksw9 . Hi, the current quickstart needs a small fix (I updated it internally; should come out in the next release):. `/home/${USER}` should be `${HOME}` to be more general. And, note that in order for docker to access your file system, you do need the `-v` path. So you probably want something like:. ```. OUTPUT_DIR=${HOME}/quickstart-output. mkdir -p ""${OUTPUT_DIR}"". REF=${HOME}/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta. BAM=${HOME}/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ```. and . `-v ${HOME}:${HOME} `. in the docker command",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104
https://github.com/google/deepvariant/issues/104:56,safety,updat,updated,56,"@ksw9 . Hi, the current quickstart needs a small fix (I updated it internally; should come out in the next release):. `/home/${USER}` should be `${HOME}` to be more general. And, note that in order for docker to access your file system, you do need the `-v` path. So you probably want something like:. ```. OUTPUT_DIR=${HOME}/quickstart-output. mkdir -p ""${OUTPUT_DIR}"". REF=${HOME}/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta. BAM=${HOME}/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ```. and . `-v ${HOME}:${HOME} `. in the docker command",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104
https://github.com/google/deepvariant/issues/104:394,safety,test,testdata,394,"@ksw9 . Hi, the current quickstart needs a small fix (I updated it internally; should come out in the next release):. `/home/${USER}` should be `${HOME}` to be more general. And, note that in order for docker to access your file system, you do need the `-v` path. So you probably want something like:. ```. OUTPUT_DIR=${HOME}/quickstart-output. mkdir -p ""${OUTPUT_DIR}"". REF=${HOME}/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta. BAM=${HOME}/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ```. and . `-v ${HOME}:${HOME} `. in the docker command",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104
https://github.com/google/deepvariant/issues/104:458,safety,test,testdata,458,"@ksw9 . Hi, the current quickstart needs a small fix (I updated it internally; should come out in the next release):. `/home/${USER}` should be `${HOME}` to be more general. And, note that in order for docker to access your file system, you do need the `-v` path. So you probably want something like:. ```. OUTPUT_DIR=${HOME}/quickstart-output. mkdir -p ""${OUTPUT_DIR}"". REF=${HOME}/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta. BAM=${HOME}/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ```. and . `-v ${HOME}:${HOME} `. in the docker command",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104
https://github.com/google/deepvariant/issues/104:56,security,updat,updated,56,"@ksw9 . Hi, the current quickstart needs a small fix (I updated it internally; should come out in the next release):. `/home/${USER}` should be `${HOME}` to be more general. And, note that in order for docker to access your file system, you do need the `-v` path. So you probably want something like:. ```. OUTPUT_DIR=${HOME}/quickstart-output. mkdir -p ""${OUTPUT_DIR}"". REF=${HOME}/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta. BAM=${HOME}/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ```. and . `-v ${HOME}:${HOME} `. in the docker command",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104
https://github.com/google/deepvariant/issues/104:212,security,access,access,212,"@ksw9 . Hi, the current quickstart needs a small fix (I updated it internally; should come out in the next release):. `/home/${USER}` should be `${HOME}` to be more general. And, note that in order for docker to access your file system, you do need the `-v` path. So you probably want something like:. ```. OUTPUT_DIR=${HOME}/quickstart-output. mkdir -p ""${OUTPUT_DIR}"". REF=${HOME}/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta. BAM=${HOME}/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ```. and . `-v ${HOME}:${HOME} `. in the docker command",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104
https://github.com/google/deepvariant/issues/104:394,testability,test,testdata,394,"@ksw9 . Hi, the current quickstart needs a small fix (I updated it internally; should come out in the next release):. `/home/${USER}` should be `${HOME}` to be more general. And, note that in order for docker to access your file system, you do need the `-v` path. So you probably want something like:. ```. OUTPUT_DIR=${HOME}/quickstart-output. mkdir -p ""${OUTPUT_DIR}"". REF=${HOME}/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta. BAM=${HOME}/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ```. and . `-v ${HOME}:${HOME} `. in the docker command",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104
https://github.com/google/deepvariant/issues/104:419,testability,unit,unittest,419,"@ksw9 . Hi, the current quickstart needs a small fix (I updated it internally; should come out in the next release):. `/home/${USER}` should be `${HOME}` to be more general. And, note that in order for docker to access your file system, you do need the `-v` path. So you probably want something like:. ```. OUTPUT_DIR=${HOME}/quickstart-output. mkdir -p ""${OUTPUT_DIR}"". REF=${HOME}/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta. BAM=${HOME}/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ```. and . `-v ${HOME}:${HOME} `. in the docker command",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104
https://github.com/google/deepvariant/issues/104:458,testability,test,testdata,458,"@ksw9 . Hi, the current quickstart needs a small fix (I updated it internally; should come out in the next release):. `/home/${USER}` should be `${HOME}` to be more general. And, note that in order for docker to access your file system, you do need the `-v` path. So you probably want something like:. ```. OUTPUT_DIR=${HOME}/quickstart-output. mkdir -p ""${OUTPUT_DIR}"". REF=${HOME}/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta. BAM=${HOME}/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ```. and . `-v ${HOME}:${HOME} `. in the docker command",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104
https://github.com/google/deepvariant/issues/104:127,usability,USER,USER,127,"@ksw9 . Hi, the current quickstart needs a small fix (I updated it internally; should come out in the next release):. `/home/${USER}` should be `${HOME}` to be more general. And, note that in order for docker to access your file system, you do need the `-v` path. So you probably want something like:. ```. OUTPUT_DIR=${HOME}/quickstart-output. mkdir -p ""${OUTPUT_DIR}"". REF=${HOME}/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta. BAM=${HOME}/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ```. and . `-v ${HOME}:${HOME} `. in the docker command",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104
https://github.com/google/deepvariant/issues/104:547,usability,command,command,547,"@ksw9 . Hi, the current quickstart needs a small fix (I updated it internally; should come out in the next release):. `/home/${USER}` should be `${HOME}` to be more general. And, note that in order for docker to access your file system, you do need the `-v` path. So you probably want something like:. ```. OUTPUT_DIR=${HOME}/quickstart-output. mkdir -p ""${OUTPUT_DIR}"". REF=${HOME}/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta. BAM=${HOME}/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. ```. and . `-v ${HOME}:${HOME} `. in the docker command",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104
https://github.com/google/deepvariant/issues/104:38,usability,help,help,38,"Fantastic, this works. Thanks for the help! Best,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104
https://github.com/google/deepvariant/issues/104:9,reliability,doe,does,9,"Hi, This does not work for me. I still get . ValueError: Not found: Could not open quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. Any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104
https://github.com/google/deepvariant/issues/104:94,safety,test,testdata,94,"Hi, This does not work for me. I still get . ValueError: Not found: Could not open quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. Any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104
https://github.com/google/deepvariant/issues/104:94,testability,test,testdata,94,"Hi, This does not work for me. I still get . ValueError: Not found: Could not open quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. Any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104
https://github.com/google/deepvariant/issues/104:37,availability,error,errors,37,Do you mean you're still getting the errors after that? Can you post your commands?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104
https://github.com/google/deepvariant/issues/104:37,performance,error,errors,37,Do you mean you're still getting the errors after that? Can you post your commands?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104
https://github.com/google/deepvariant/issues/104:37,safety,error,errors,37,Do you mean you're still getting the errors after that? Can you post your commands?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104
https://github.com/google/deepvariant/issues/104:37,usability,error,errors,37,Do you mean you're still getting the errors after that? Can you post your commands?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104
https://github.com/google/deepvariant/issues/104:74,usability,command,commands,74,Do you mean you're still getting the errors after that? Can you post your commands?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104
https://github.com/google/deepvariant/issues/104:155,usability,User,Users,155,"Nvm, it worked now. The path always has to start with ${HOME} in all files provided it seems, no matter if the path is correct in some other manner: e.g. /Users/>user_name<. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/104
https://github.com/google/deepvariant/issues/105:156,deployability,log,log,156,"Hi,. we have seen regions that run for longer. Up to 14 or 22 hours still sound like longer than what I personally have seen before. . If you look into the log files of the `make_examples` shard (under `$HOME/case-study/output/logs/1/`), you can see log like this:. ```. I0815 16:59:22.392369 139972404860672 make_examples.py:825] Found 0 candidates in 1:2392001-2393000 [1000 bp] [0.08s elapsed]. I0815 16:59:22.490614 139972404860672 make_examples.py:825] Found 1 candidates in 1:2456001-2457000 [1000 bp] [0.10s elapsed]. I0815 16:59:22.616637 139972404860672 make_examples.py:825] Found 4 candidates in 1:2520001-2521000 [1000 bp] [0.12s elapsed]. ```. If you find where it stopped, you can usually use the pattern of regions to find out which one it got stuck on. Let me know if that helps. I'll also discuss with the team to see if we can print out more incremental logs in cases like this to help understand what the code is doing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:227,deployability,log,logs,227,"Hi,. we have seen regions that run for longer. Up to 14 or 22 hours still sound like longer than what I personally have seen before. . If you look into the log files of the `make_examples` shard (under `$HOME/case-study/output/logs/1/`), you can see log like this:. ```. I0815 16:59:22.392369 139972404860672 make_examples.py:825] Found 0 candidates in 1:2392001-2393000 [1000 bp] [0.08s elapsed]. I0815 16:59:22.490614 139972404860672 make_examples.py:825] Found 1 candidates in 1:2456001-2457000 [1000 bp] [0.10s elapsed]. I0815 16:59:22.616637 139972404860672 make_examples.py:825] Found 4 candidates in 1:2520001-2521000 [1000 bp] [0.12s elapsed]. ```. If you find where it stopped, you can usually use the pattern of regions to find out which one it got stuck on. Let me know if that helps. I'll also discuss with the team to see if we can print out more incremental logs in cases like this to help understand what the code is doing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:250,deployability,log,log,250,"Hi,. we have seen regions that run for longer. Up to 14 or 22 hours still sound like longer than what I personally have seen before. . If you look into the log files of the `make_examples` shard (under `$HOME/case-study/output/logs/1/`), you can see log like this:. ```. I0815 16:59:22.392369 139972404860672 make_examples.py:825] Found 0 candidates in 1:2392001-2393000 [1000 bp] [0.08s elapsed]. I0815 16:59:22.490614 139972404860672 make_examples.py:825] Found 1 candidates in 1:2456001-2457000 [1000 bp] [0.10s elapsed]. I0815 16:59:22.616637 139972404860672 make_examples.py:825] Found 4 candidates in 1:2520001-2521000 [1000 bp] [0.12s elapsed]. ```. If you find where it stopped, you can usually use the pattern of regions to find out which one it got stuck on. Let me know if that helps. I'll also discuss with the team to see if we can print out more incremental logs in cases like this to help understand what the code is doing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:872,deployability,log,logs,872,"Hi,. we have seen regions that run for longer. Up to 14 or 22 hours still sound like longer than what I personally have seen before. . If you look into the log files of the `make_examples` shard (under `$HOME/case-study/output/logs/1/`), you can see log like this:. ```. I0815 16:59:22.392369 139972404860672 make_examples.py:825] Found 0 candidates in 1:2392001-2393000 [1000 bp] [0.08s elapsed]. I0815 16:59:22.490614 139972404860672 make_examples.py:825] Found 1 candidates in 1:2456001-2457000 [1000 bp] [0.10s elapsed]. I0815 16:59:22.616637 139972404860672 make_examples.py:825] Found 4 candidates in 1:2520001-2521000 [1000 bp] [0.12s elapsed]. ```. If you find where it stopped, you can usually use the pattern of regions to find out which one it got stuck on. Let me know if that helps. I'll also discuss with the team to see if we can print out more incremental logs in cases like this to help understand what the code is doing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:156,safety,log,log,156,"Hi,. we have seen regions that run for longer. Up to 14 or 22 hours still sound like longer than what I personally have seen before. . If you look into the log files of the `make_examples` shard (under `$HOME/case-study/output/logs/1/`), you can see log like this:. ```. I0815 16:59:22.392369 139972404860672 make_examples.py:825] Found 0 candidates in 1:2392001-2393000 [1000 bp] [0.08s elapsed]. I0815 16:59:22.490614 139972404860672 make_examples.py:825] Found 1 candidates in 1:2456001-2457000 [1000 bp] [0.10s elapsed]. I0815 16:59:22.616637 139972404860672 make_examples.py:825] Found 4 candidates in 1:2520001-2521000 [1000 bp] [0.12s elapsed]. ```. If you find where it stopped, you can usually use the pattern of regions to find out which one it got stuck on. Let me know if that helps. I'll also discuss with the team to see if we can print out more incremental logs in cases like this to help understand what the code is doing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:227,safety,log,logs,227,"Hi,. we have seen regions that run for longer. Up to 14 or 22 hours still sound like longer than what I personally have seen before. . If you look into the log files of the `make_examples` shard (under `$HOME/case-study/output/logs/1/`), you can see log like this:. ```. I0815 16:59:22.392369 139972404860672 make_examples.py:825] Found 0 candidates in 1:2392001-2393000 [1000 bp] [0.08s elapsed]. I0815 16:59:22.490614 139972404860672 make_examples.py:825] Found 1 candidates in 1:2456001-2457000 [1000 bp] [0.10s elapsed]. I0815 16:59:22.616637 139972404860672 make_examples.py:825] Found 4 candidates in 1:2520001-2521000 [1000 bp] [0.12s elapsed]. ```. If you find where it stopped, you can usually use the pattern of regions to find out which one it got stuck on. Let me know if that helps. I'll also discuss with the team to see if we can print out more incremental logs in cases like this to help understand what the code is doing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:250,safety,log,log,250,"Hi,. we have seen regions that run for longer. Up to 14 or 22 hours still sound like longer than what I personally have seen before. . If you look into the log files of the `make_examples` shard (under `$HOME/case-study/output/logs/1/`), you can see log like this:. ```. I0815 16:59:22.392369 139972404860672 make_examples.py:825] Found 0 candidates in 1:2392001-2393000 [1000 bp] [0.08s elapsed]. I0815 16:59:22.490614 139972404860672 make_examples.py:825] Found 1 candidates in 1:2456001-2457000 [1000 bp] [0.10s elapsed]. I0815 16:59:22.616637 139972404860672 make_examples.py:825] Found 4 candidates in 1:2520001-2521000 [1000 bp] [0.12s elapsed]. ```. If you find where it stopped, you can usually use the pattern of regions to find out which one it got stuck on. Let me know if that helps. I'll also discuss with the team to see if we can print out more incremental logs in cases like this to help understand what the code is doing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:872,safety,log,logs,872,"Hi,. we have seen regions that run for longer. Up to 14 or 22 hours still sound like longer than what I personally have seen before. . If you look into the log files of the `make_examples` shard (under `$HOME/case-study/output/logs/1/`), you can see log like this:. ```. I0815 16:59:22.392369 139972404860672 make_examples.py:825] Found 0 candidates in 1:2392001-2393000 [1000 bp] [0.08s elapsed]. I0815 16:59:22.490614 139972404860672 make_examples.py:825] Found 1 candidates in 1:2456001-2457000 [1000 bp] [0.10s elapsed]. I0815 16:59:22.616637 139972404860672 make_examples.py:825] Found 4 candidates in 1:2520001-2521000 [1000 bp] [0.12s elapsed]. ```. If you find where it stopped, you can usually use the pattern of regions to find out which one it got stuck on. Let me know if that helps. I'll also discuss with the team to see if we can print out more incremental logs in cases like this to help understand what the code is doing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:156,security,log,log,156,"Hi,. we have seen regions that run for longer. Up to 14 or 22 hours still sound like longer than what I personally have seen before. . If you look into the log files of the `make_examples` shard (under `$HOME/case-study/output/logs/1/`), you can see log like this:. ```. I0815 16:59:22.392369 139972404860672 make_examples.py:825] Found 0 candidates in 1:2392001-2393000 [1000 bp] [0.08s elapsed]. I0815 16:59:22.490614 139972404860672 make_examples.py:825] Found 1 candidates in 1:2456001-2457000 [1000 bp] [0.10s elapsed]. I0815 16:59:22.616637 139972404860672 make_examples.py:825] Found 4 candidates in 1:2520001-2521000 [1000 bp] [0.12s elapsed]. ```. If you find where it stopped, you can usually use the pattern of regions to find out which one it got stuck on. Let me know if that helps. I'll also discuss with the team to see if we can print out more incremental logs in cases like this to help understand what the code is doing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:227,security,log,logs,227,"Hi,. we have seen regions that run for longer. Up to 14 or 22 hours still sound like longer than what I personally have seen before. . If you look into the log files of the `make_examples` shard (under `$HOME/case-study/output/logs/1/`), you can see log like this:. ```. I0815 16:59:22.392369 139972404860672 make_examples.py:825] Found 0 candidates in 1:2392001-2393000 [1000 bp] [0.08s elapsed]. I0815 16:59:22.490614 139972404860672 make_examples.py:825] Found 1 candidates in 1:2456001-2457000 [1000 bp] [0.10s elapsed]. I0815 16:59:22.616637 139972404860672 make_examples.py:825] Found 4 candidates in 1:2520001-2521000 [1000 bp] [0.12s elapsed]. ```. If you find where it stopped, you can usually use the pattern of regions to find out which one it got stuck on. Let me know if that helps. I'll also discuss with the team to see if we can print out more incremental logs in cases like this to help understand what the code is doing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:250,security,log,log,250,"Hi,. we have seen regions that run for longer. Up to 14 or 22 hours still sound like longer than what I personally have seen before. . If you look into the log files of the `make_examples` shard (under `$HOME/case-study/output/logs/1/`), you can see log like this:. ```. I0815 16:59:22.392369 139972404860672 make_examples.py:825] Found 0 candidates in 1:2392001-2393000 [1000 bp] [0.08s elapsed]. I0815 16:59:22.490614 139972404860672 make_examples.py:825] Found 1 candidates in 1:2456001-2457000 [1000 bp] [0.10s elapsed]. I0815 16:59:22.616637 139972404860672 make_examples.py:825] Found 4 candidates in 1:2520001-2521000 [1000 bp] [0.12s elapsed]. ```. If you find where it stopped, you can usually use the pattern of regions to find out which one it got stuck on. Let me know if that helps. I'll also discuss with the team to see if we can print out more incremental logs in cases like this to help understand what the code is doing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:823,security,team,team,823,"Hi,. we have seen regions that run for longer. Up to 14 or 22 hours still sound like longer than what I personally have seen before. . If you look into the log files of the `make_examples` shard (under `$HOME/case-study/output/logs/1/`), you can see log like this:. ```. I0815 16:59:22.392369 139972404860672 make_examples.py:825] Found 0 candidates in 1:2392001-2393000 [1000 bp] [0.08s elapsed]. I0815 16:59:22.490614 139972404860672 make_examples.py:825] Found 1 candidates in 1:2456001-2457000 [1000 bp] [0.10s elapsed]. I0815 16:59:22.616637 139972404860672 make_examples.py:825] Found 4 candidates in 1:2520001-2521000 [1000 bp] [0.12s elapsed]. ```. If you find where it stopped, you can usually use the pattern of regions to find out which one it got stuck on. Let me know if that helps. I'll also discuss with the team to see if we can print out more incremental logs in cases like this to help understand what the code is doing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:872,security,log,logs,872,"Hi,. we have seen regions that run for longer. Up to 14 or 22 hours still sound like longer than what I personally have seen before. . If you look into the log files of the `make_examples` shard (under `$HOME/case-study/output/logs/1/`), you can see log like this:. ```. I0815 16:59:22.392369 139972404860672 make_examples.py:825] Found 0 candidates in 1:2392001-2393000 [1000 bp] [0.08s elapsed]. I0815 16:59:22.490614 139972404860672 make_examples.py:825] Found 1 candidates in 1:2456001-2457000 [1000 bp] [0.10s elapsed]. I0815 16:59:22.616637 139972404860672 make_examples.py:825] Found 4 candidates in 1:2520001-2521000 [1000 bp] [0.12s elapsed]. ```. If you find where it stopped, you can usually use the pattern of regions to find out which one it got stuck on. Let me know if that helps. I'll also discuss with the team to see if we can print out more incremental logs in cases like this to help understand what the code is doing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:156,testability,log,log,156,"Hi,. we have seen regions that run for longer. Up to 14 or 22 hours still sound like longer than what I personally have seen before. . If you look into the log files of the `make_examples` shard (under `$HOME/case-study/output/logs/1/`), you can see log like this:. ```. I0815 16:59:22.392369 139972404860672 make_examples.py:825] Found 0 candidates in 1:2392001-2393000 [1000 bp] [0.08s elapsed]. I0815 16:59:22.490614 139972404860672 make_examples.py:825] Found 1 candidates in 1:2456001-2457000 [1000 bp] [0.10s elapsed]. I0815 16:59:22.616637 139972404860672 make_examples.py:825] Found 4 candidates in 1:2520001-2521000 [1000 bp] [0.12s elapsed]. ```. If you find where it stopped, you can usually use the pattern of regions to find out which one it got stuck on. Let me know if that helps. I'll also discuss with the team to see if we can print out more incremental logs in cases like this to help understand what the code is doing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:227,testability,log,logs,227,"Hi,. we have seen regions that run for longer. Up to 14 or 22 hours still sound like longer than what I personally have seen before. . If you look into the log files of the `make_examples` shard (under `$HOME/case-study/output/logs/1/`), you can see log like this:. ```. I0815 16:59:22.392369 139972404860672 make_examples.py:825] Found 0 candidates in 1:2392001-2393000 [1000 bp] [0.08s elapsed]. I0815 16:59:22.490614 139972404860672 make_examples.py:825] Found 1 candidates in 1:2456001-2457000 [1000 bp] [0.10s elapsed]. I0815 16:59:22.616637 139972404860672 make_examples.py:825] Found 4 candidates in 1:2520001-2521000 [1000 bp] [0.12s elapsed]. ```. If you find where it stopped, you can usually use the pattern of regions to find out which one it got stuck on. Let me know if that helps. I'll also discuss with the team to see if we can print out more incremental logs in cases like this to help understand what the code is doing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:250,testability,log,log,250,"Hi,. we have seen regions that run for longer. Up to 14 or 22 hours still sound like longer than what I personally have seen before. . If you look into the log files of the `make_examples` shard (under `$HOME/case-study/output/logs/1/`), you can see log like this:. ```. I0815 16:59:22.392369 139972404860672 make_examples.py:825] Found 0 candidates in 1:2392001-2393000 [1000 bp] [0.08s elapsed]. I0815 16:59:22.490614 139972404860672 make_examples.py:825] Found 1 candidates in 1:2456001-2457000 [1000 bp] [0.10s elapsed]. I0815 16:59:22.616637 139972404860672 make_examples.py:825] Found 4 candidates in 1:2520001-2521000 [1000 bp] [0.12s elapsed]. ```. If you find where it stopped, you can usually use the pattern of regions to find out which one it got stuck on. Let me know if that helps. I'll also discuss with the team to see if we can print out more incremental logs in cases like this to help understand what the code is doing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:872,testability,log,logs,872,"Hi,. we have seen regions that run for longer. Up to 14 or 22 hours still sound like longer than what I personally have seen before. . If you look into the log files of the `make_examples` shard (under `$HOME/case-study/output/logs/1/`), you can see log like this:. ```. I0815 16:59:22.392369 139972404860672 make_examples.py:825] Found 0 candidates in 1:2392001-2393000 [1000 bp] [0.08s elapsed]. I0815 16:59:22.490614 139972404860672 make_examples.py:825] Found 1 candidates in 1:2456001-2457000 [1000 bp] [0.10s elapsed]. I0815 16:59:22.616637 139972404860672 make_examples.py:825] Found 4 candidates in 1:2520001-2521000 [1000 bp] [0.12s elapsed]. ```. If you find where it stopped, you can usually use the pattern of regions to find out which one it got stuck on. Let me know if that helps. I'll also discuss with the team to see if we can print out more incremental logs in cases like this to help understand what the code is doing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:904,testability,understand,understand,904,"Hi,. we have seen regions that run for longer. Up to 14 or 22 hours still sound like longer than what I personally have seen before. . If you look into the log files of the `make_examples` shard (under `$HOME/case-study/output/logs/1/`), you can see log like this:. ```. I0815 16:59:22.392369 139972404860672 make_examples.py:825] Found 0 candidates in 1:2392001-2393000 [1000 bp] [0.08s elapsed]. I0815 16:59:22.490614 139972404860672 make_examples.py:825] Found 1 candidates in 1:2456001-2457000 [1000 bp] [0.10s elapsed]. I0815 16:59:22.616637 139972404860672 make_examples.py:825] Found 4 candidates in 1:2520001-2521000 [1000 bp] [0.12s elapsed]. ```. If you find where it stopped, you can usually use the pattern of regions to find out which one it got stuck on. Let me know if that helps. I'll also discuss with the team to see if we can print out more incremental logs in cases like this to help understand what the code is doing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:104,usability,person,personally,104,"Hi,. we have seen regions that run for longer. Up to 14 or 22 hours still sound like longer than what I personally have seen before. . If you look into the log files of the `make_examples` shard (under `$HOME/case-study/output/logs/1/`), you can see log like this:. ```. I0815 16:59:22.392369 139972404860672 make_examples.py:825] Found 0 candidates in 1:2392001-2393000 [1000 bp] [0.08s elapsed]. I0815 16:59:22.490614 139972404860672 make_examples.py:825] Found 1 candidates in 1:2456001-2457000 [1000 bp] [0.10s elapsed]. I0815 16:59:22.616637 139972404860672 make_examples.py:825] Found 4 candidates in 1:2520001-2521000 [1000 bp] [0.12s elapsed]. ```. If you find where it stopped, you can usually use the pattern of regions to find out which one it got stuck on. Let me know if that helps. I'll also discuss with the team to see if we can print out more incremental logs in cases like this to help understand what the code is doing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:678,usability,stop,stopped,678,"Hi,. we have seen regions that run for longer. Up to 14 or 22 hours still sound like longer than what I personally have seen before. . If you look into the log files of the `make_examples` shard (under `$HOME/case-study/output/logs/1/`), you can see log like this:. ```. I0815 16:59:22.392369 139972404860672 make_examples.py:825] Found 0 candidates in 1:2392001-2393000 [1000 bp] [0.08s elapsed]. I0815 16:59:22.490614 139972404860672 make_examples.py:825] Found 1 candidates in 1:2456001-2457000 [1000 bp] [0.10s elapsed]. I0815 16:59:22.616637 139972404860672 make_examples.py:825] Found 4 candidates in 1:2520001-2521000 [1000 bp] [0.12s elapsed]. ```. If you find where it stopped, you can usually use the pattern of regions to find out which one it got stuck on. Let me know if that helps. I'll also discuss with the team to see if we can print out more incremental logs in cases like this to help understand what the code is doing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:789,usability,help,helps,789,"Hi,. we have seen regions that run for longer. Up to 14 or 22 hours still sound like longer than what I personally have seen before. . If you look into the log files of the `make_examples` shard (under `$HOME/case-study/output/logs/1/`), you can see log like this:. ```. I0815 16:59:22.392369 139972404860672 make_examples.py:825] Found 0 candidates in 1:2392001-2393000 [1000 bp] [0.08s elapsed]. I0815 16:59:22.490614 139972404860672 make_examples.py:825] Found 1 candidates in 1:2456001-2457000 [1000 bp] [0.10s elapsed]. I0815 16:59:22.616637 139972404860672 make_examples.py:825] Found 4 candidates in 1:2520001-2521000 [1000 bp] [0.12s elapsed]. ```. If you find where it stopped, you can usually use the pattern of regions to find out which one it got stuck on. Let me know if that helps. I'll also discuss with the team to see if we can print out more incremental logs in cases like this to help understand what the code is doing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:899,usability,help,help,899,"Hi,. we have seen regions that run for longer. Up to 14 or 22 hours still sound like longer than what I personally have seen before. . If you look into the log files of the `make_examples` shard (under `$HOME/case-study/output/logs/1/`), you can see log like this:. ```. I0815 16:59:22.392369 139972404860672 make_examples.py:825] Found 0 candidates in 1:2392001-2393000 [1000 bp] [0.08s elapsed]. I0815 16:59:22.490614 139972404860672 make_examples.py:825] Found 1 candidates in 1:2456001-2457000 [1000 bp] [0.10s elapsed]. I0815 16:59:22.616637 139972404860672 make_examples.py:825] Found 4 candidates in 1:2520001-2521000 [1000 bp] [0.12s elapsed]. ```. If you find where it stopped, you can usually use the pattern of regions to find out which one it got stuck on. Let me know if that helps. I'll also discuss with the team to see if we can print out more incremental logs in cases like this to help understand what the code is doing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:85,deployability,fail,failed,85,"Thank you - this ended up being a problem on our end with the way we were restarting failed jobs; our system was failing to recognize that this job had failed and was not properly restarting this as it should be. I am now running this on a whole genome and all the shards complete in about 1hr, but call_variants has been running for 24hrs. Looking over the case study here https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-case-study.md I am expecting it to take 3-4hrs. In the case study it looks like it was run on 1 CPU. When I noticed this argument:. <pre>. --num_readers: Number of parallel readers to create for examples. (default: '8'). (an integer). </pre>. I restarted it with 8 CPU (it looks like it actually uses 6). But, does this argument actually control threading? As my next move I was planning to run call_variants on each shard from make_examples. Thanks for any tips/advice you may have.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:113,deployability,fail,failing,113,"Thank you - this ended up being a problem on our end with the way we were restarting failed jobs; our system was failing to recognize that this job had failed and was not properly restarting this as it should be. I am now running this on a whole genome and all the shards complete in about 1hr, but call_variants has been running for 24hrs. Looking over the case study here https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-case-study.md I am expecting it to take 3-4hrs. In the case study it looks like it was run on 1 CPU. When I noticed this argument:. <pre>. --num_readers: Number of parallel readers to create for examples. (default: '8'). (an integer). </pre>. I restarted it with 8 CPU (it looks like it actually uses 6). But, does this argument actually control threading? As my next move I was planning to run call_variants on each shard from make_examples. Thanks for any tips/advice you may have.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:152,deployability,fail,failed,152,"Thank you - this ended up being a problem on our end with the way we were restarting failed jobs; our system was failing to recognize that this job had failed and was not properly restarting this as it should be. I am now running this on a whole genome and all the shards complete in about 1hr, but call_variants has been running for 24hrs. Looking over the case study here https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-case-study.md I am expecting it to take 3-4hrs. In the case study it looks like it was run on 1 CPU. When I noticed this argument:. <pre>. --num_readers: Number of parallel readers to create for examples. (default: '8'). (an integer). </pre>. I restarted it with 8 CPU (it looks like it actually uses 6). But, does this argument actually control threading? As my next move I was planning to run call_variants on each shard from make_examples. Thanks for any tips/advice you may have.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:535,energy efficiency,CPU,CPU,535,"Thank you - this ended up being a problem on our end with the way we were restarting failed jobs; our system was failing to recognize that this job had failed and was not properly restarting this as it should be. I am now running this on a whole genome and all the shards complete in about 1hr, but call_variants has been running for 24hrs. Looking over the case study here https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-case-study.md I am expecting it to take 3-4hrs. In the case study it looks like it was run on 1 CPU. When I noticed this argument:. <pre>. --num_readers: Number of parallel readers to create for examples. (default: '8'). (an integer). </pre>. I restarted it with 8 CPU (it looks like it actually uses 6). But, does this argument actually control threading? As my next move I was planning to run call_variants on each shard from make_examples. Thanks for any tips/advice you may have.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:704,energy efficiency,CPU,CPU,704,"Thank you - this ended up being a problem on our end with the way we were restarting failed jobs; our system was failing to recognize that this job had failed and was not properly restarting this as it should be. I am now running this on a whole genome and all the shards complete in about 1hr, but call_variants has been running for 24hrs. Looking over the case study here https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-case-study.md I am expecting it to take 3-4hrs. In the case study it looks like it was run on 1 CPU. When I noticed this argument:. <pre>. --num_readers: Number of parallel readers to create for examples. (default: '8'). (an integer). </pre>. I restarted it with 8 CPU (it looks like it actually uses 6). But, does this argument actually control threading? As my next move I was planning to run call_variants on each shard from make_examples. Thanks for any tips/advice you may have.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:535,performance,CPU,CPU,535,"Thank you - this ended up being a problem on our end with the way we were restarting failed jobs; our system was failing to recognize that this job had failed and was not properly restarting this as it should be. I am now running this on a whole genome and all the shards complete in about 1hr, but call_variants has been running for 24hrs. Looking over the case study here https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-case-study.md I am expecting it to take 3-4hrs. In the case study it looks like it was run on 1 CPU. When I noticed this argument:. <pre>. --num_readers: Number of parallel readers to create for examples. (default: '8'). (an integer). </pre>. I restarted it with 8 CPU (it looks like it actually uses 6). But, does this argument actually control threading? As my next move I was planning to run call_variants on each shard from make_examples. Thanks for any tips/advice you may have.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:603,performance,parallel,parallel,603,"Thank you - this ended up being a problem on our end with the way we were restarting failed jobs; our system was failing to recognize that this job had failed and was not properly restarting this as it should be. I am now running this on a whole genome and all the shards complete in about 1hr, but call_variants has been running for 24hrs. Looking over the case study here https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-case-study.md I am expecting it to take 3-4hrs. In the case study it looks like it was run on 1 CPU. When I noticed this argument:. <pre>. --num_readers: Number of parallel readers to create for examples. (default: '8'). (an integer). </pre>. I restarted it with 8 CPU (it looks like it actually uses 6). But, does this argument actually control threading? As my next move I was planning to run call_variants on each shard from make_examples. Thanks for any tips/advice you may have.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:704,performance,CPU,CPU,704,"Thank you - this ended up being a problem on our end with the way we were restarting failed jobs; our system was failing to recognize that this job had failed and was not properly restarting this as it should be. I am now running this on a whole genome and all the shards complete in about 1hr, but call_variants has been running for 24hrs. Looking over the case study here https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-case-study.md I am expecting it to take 3-4hrs. In the case study it looks like it was run on 1 CPU. When I noticed this argument:. <pre>. --num_readers: Number of parallel readers to create for examples. (default: '8'). (an integer). </pre>. I restarted it with 8 CPU (it looks like it actually uses 6). But, does this argument actually control threading? As my next move I was planning to run call_variants on each shard from make_examples. Thanks for any tips/advice you may have.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:85,reliability,fail,failed,85,"Thank you - this ended up being a problem on our end with the way we were restarting failed jobs; our system was failing to recognize that this job had failed and was not properly restarting this as it should be. I am now running this on a whole genome and all the shards complete in about 1hr, but call_variants has been running for 24hrs. Looking over the case study here https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-case-study.md I am expecting it to take 3-4hrs. In the case study it looks like it was run on 1 CPU. When I noticed this argument:. <pre>. --num_readers: Number of parallel readers to create for examples. (default: '8'). (an integer). </pre>. I restarted it with 8 CPU (it looks like it actually uses 6). But, does this argument actually control threading? As my next move I was planning to run call_variants on each shard from make_examples. Thanks for any tips/advice you may have.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:113,reliability,fail,failing,113,"Thank you - this ended up being a problem on our end with the way we were restarting failed jobs; our system was failing to recognize that this job had failed and was not properly restarting this as it should be. I am now running this on a whole genome and all the shards complete in about 1hr, but call_variants has been running for 24hrs. Looking over the case study here https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-case-study.md I am expecting it to take 3-4hrs. In the case study it looks like it was run on 1 CPU. When I noticed this argument:. <pre>. --num_readers: Number of parallel readers to create for examples. (default: '8'). (an integer). </pre>. I restarted it with 8 CPU (it looks like it actually uses 6). But, does this argument actually control threading? As my next move I was planning to run call_variants on each shard from make_examples. Thanks for any tips/advice you may have.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:152,reliability,fail,failed,152,"Thank you - this ended up being a problem on our end with the way we were restarting failed jobs; our system was failing to recognize that this job had failed and was not properly restarting this as it should be. I am now running this on a whole genome and all the shards complete in about 1hr, but call_variants has been running for 24hrs. Looking over the case study here https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-case-study.md I am expecting it to take 3-4hrs. In the case study it looks like it was run on 1 CPU. When I noticed this argument:. <pre>. --num_readers: Number of parallel readers to create for examples. (default: '8'). (an integer). </pre>. I restarted it with 8 CPU (it looks like it actually uses 6). But, does this argument actually control threading? As my next move I was planning to run call_variants on each shard from make_examples. Thanks for any tips/advice you may have.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:749,reliability,doe,does,749,"Thank you - this ended up being a problem on our end with the way we were restarting failed jobs; our system was failing to recognize that this job had failed and was not properly restarting this as it should be. I am now running this on a whole genome and all the shards complete in about 1hr, but call_variants has been running for 24hrs. Looking over the case study here https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-case-study.md I am expecting it to take 3-4hrs. In the case study it looks like it was run on 1 CPU. When I noticed this argument:. <pre>. --num_readers: Number of parallel readers to create for examples. (default: '8'). (an integer). </pre>. I restarted it with 8 CPU (it looks like it actually uses 6). But, does this argument actually control threading? As my next move I was planning to run call_variants on each shard from make_examples. Thanks for any tips/advice you may have.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:272,safety,compl,complete,272,"Thank you - this ended up being a problem on our end with the way we were restarting failed jobs; our system was failing to recognize that this job had failed and was not properly restarting this as it should be. I am now running this on a whole genome and all the shards complete in about 1hr, but call_variants has been running for 24hrs. Looking over the case study here https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-case-study.md I am expecting it to take 3-4hrs. In the case study it looks like it was run on 1 CPU. When I noticed this argument:. <pre>. --num_readers: Number of parallel readers to create for examples. (default: '8'). (an integer). </pre>. I restarted it with 8 CPU (it looks like it actually uses 6). But, does this argument actually control threading? As my next move I was planning to run call_variants on each shard from make_examples. Thanks for any tips/advice you may have.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:272,security,compl,complete,272,"Thank you - this ended up being a problem on our end with the way we were restarting failed jobs; our system was failing to recognize that this job had failed and was not properly restarting this as it should be. I am now running this on a whole genome and all the shards complete in about 1hr, but call_variants has been running for 24hrs. Looking over the case study here https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-case-study.md I am expecting it to take 3-4hrs. In the case study it looks like it was run on 1 CPU. When I noticed this argument:. <pre>. --num_readers: Number of parallel readers to create for examples. (default: '8'). (an integer). </pre>. I restarted it with 8 CPU (it looks like it actually uses 6). But, does this argument actually control threading? As my next move I was planning to run call_variants on each shard from make_examples. Thanks for any tips/advice you may have.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:777,security,control,control,777,"Thank you - this ended up being a problem on our end with the way we were restarting failed jobs; our system was failing to recognize that this job had failed and was not properly restarting this as it should be. I am now running this on a whole genome and all the shards complete in about 1hr, but call_variants has been running for 24hrs. Looking over the case study here https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-case-study.md I am expecting it to take 3-4hrs. In the case study it looks like it was run on 1 CPU. When I noticed this argument:. <pre>. --num_readers: Number of parallel readers to create for examples. (default: '8'). (an integer). </pre>. I restarted it with 8 CPU (it looks like it actually uses 6). But, does this argument actually control threading? As my next move I was planning to run call_variants on each shard from make_examples. Thanks for any tips/advice you may have.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:777,testability,control,control,777,"Thank you - this ended up being a problem on our end with the way we were restarting failed jobs; our system was failing to recognize that this job had failed and was not properly restarting this as it should be. I am now running this on a whole genome and all the shards complete in about 1hr, but call_variants has been running for 24hrs. Looking over the case study here https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-case-study.md I am expecting it to take 3-4hrs. In the case study it looks like it was run on 1 CPU. When I noticed this argument:. <pre>. --num_readers: Number of parallel readers to create for examples. (default: '8'). (an integer). </pre>. I restarted it with 8 CPU (it looks like it actually uses 6). But, does this argument actually control threading? As my next move I was planning to run call_variants on each shard from make_examples. Thanks for any tips/advice you may have.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:818,testability,plan,planning,818,"Thank you - this ended up being a problem on our end with the way we were restarting failed jobs; our system was failing to recognize that this job had failed and was not properly restarting this as it should be. I am now running this on a whole genome and all the shards complete in about 1hr, but call_variants has been running for 24hrs. Looking over the case study here https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-case-study.md I am expecting it to take 3-4hrs. In the case study it looks like it was run on 1 CPU. When I noticed this argument:. <pre>. --num_readers: Number of parallel readers to create for examples. (default: '8'). (an integer). </pre>. I restarted it with 8 CPU (it looks like it actually uses 6). But, does this argument actually control threading? As my next move I was planning to run call_variants on each shard from make_examples. Thanks for any tips/advice you may have.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:897,usability,tip,tips,897,"Thank you - this ended up being a problem on our end with the way we were restarting failed jobs; our system was failing to recognize that this job had failed and was not properly restarting this as it should be. I am now running this on a whole genome and all the shards complete in about 1hr, but call_variants has been running for 24hrs. Looking over the case study here https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-case-study.md I am expecting it to take 3-4hrs. In the case study it looks like it was run on 1 CPU. When I noticed this argument:. <pre>. --num_readers: Number of parallel readers to create for examples. (default: '8'). (an integer). </pre>. I restarted it with 8 CPU (it looks like it actually uses 6). But, does this argument actually control threading? As my next move I was planning to run call_variants on each shard from make_examples. Thanks for any tips/advice you may have.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:390,availability,Monitor,MonitoredSession,390,"The short answer is yes. That's because that parameter gets fed to `prepare_inputs()`, which generates a `DeepVariantInput` object that generates a `tf.data.Dataset`, which gets fed in your case into an [`Estimator`](https://github.com/google/deepvariant/blob/r0.7/deepvariant/modeling.py#L415-L419) it constructs for you, that will then will be processed via `predict()` which sets up a [`MonitoredSession`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/estimator/estimator.py#L595) which by default each Session is thread-safe. I believe the rest get used to monitor the sessions, but I would need to double-check on that. Turtles all the way down :). Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:586,availability,monitor,monitor,586,"The short answer is yes. That's because that parameter gets fed to `prepare_inputs()`, which generates a `DeepVariantInput` object that generates a `tf.data.Dataset`, which gets fed in your case into an [`Estimator`](https://github.com/google/deepvariant/blob/r0.7/deepvariant/modeling.py#L415-L419) it constructs for you, that will then will be processed via `predict()` which sets up a [`MonitoredSession`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/estimator/estimator.py#L595) which by default each Session is thread-safe. I believe the rest get used to monitor the sessions, but I would need to double-check on that. Turtles all the way down :). Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:670,availability,down,down,670,"The short answer is yes. That's because that parameter gets fed to `prepare_inputs()`, which generates a `DeepVariantInput` object that generates a `tf.data.Dataset`, which gets fed in your case into an [`Estimator`](https://github.com/google/deepvariant/blob/r0.7/deepvariant/modeling.py#L415-L419) it constructs for you, that will then will be processed via `predict()` which sets up a [`MonitoredSession`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/estimator/estimator.py#L595) which by default each Session is thread-safe. I believe the rest get used to monitor the sessions, but I would need to double-check on that. Turtles all the way down :). Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:390,deployability,Monitor,MonitoredSession,390,"The short answer is yes. That's because that parameter gets fed to `prepare_inputs()`, which generates a `DeepVariantInput` object that generates a `tf.data.Dataset`, which gets fed in your case into an [`Estimator`](https://github.com/google/deepvariant/blob/r0.7/deepvariant/modeling.py#L415-L419) it constructs for you, that will then will be processed via `predict()` which sets up a [`MonitoredSession`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/estimator/estimator.py#L595) which by default each Session is thread-safe. I believe the rest get used to monitor the sessions, but I would need to double-check on that. Turtles all the way down :). Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:586,deployability,monitor,monitor,586,"The short answer is yes. That's because that parameter gets fed to `prepare_inputs()`, which generates a `DeepVariantInput` object that generates a `tf.data.Dataset`, which gets fed in your case into an [`Estimator`](https://github.com/google/deepvariant/blob/r0.7/deepvariant/modeling.py#L415-L419) it constructs for you, that will then will be processed via `predict()` which sets up a [`MonitoredSession`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/estimator/estimator.py#L595) which by default each Session is thread-safe. I believe the rest get used to monitor the sessions, but I would need to double-check on that. Turtles all the way down :). Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:205,energy efficiency,Estimat,Estimator,205,"The short answer is yes. That's because that parameter gets fed to `prepare_inputs()`, which generates a `DeepVariantInput` object that generates a `tf.data.Dataset`, which gets fed in your case into an [`Estimator`](https://github.com/google/deepvariant/blob/r0.7/deepvariant/modeling.py#L415-L419) it constructs for you, that will then will be processed via `predict()` which sets up a [`MonitoredSession`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/estimator/estimator.py#L595) which by default each Session is thread-safe. I believe the rest get used to monitor the sessions, but I would need to double-check on that. Turtles all the way down :). Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:277,energy efficiency,model,modeling,277,"The short answer is yes. That's because that parameter gets fed to `prepare_inputs()`, which generates a `DeepVariantInput` object that generates a `tf.data.Dataset`, which gets fed in your case into an [`Estimator`](https://github.com/google/deepvariant/blob/r0.7/deepvariant/modeling.py#L415-L419) it constructs for you, that will then will be processed via `predict()` which sets up a [`MonitoredSession`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/estimator/estimator.py#L595) which by default each Session is thread-safe. I believe the rest get used to monitor the sessions, but I would need to double-check on that. Turtles all the way down :). Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:361,energy efficiency,predict,predict,361,"The short answer is yes. That's because that parameter gets fed to `prepare_inputs()`, which generates a `DeepVariantInput` object that generates a `tf.data.Dataset`, which gets fed in your case into an [`Estimator`](https://github.com/google/deepvariant/blob/r0.7/deepvariant/modeling.py#L415-L419) it constructs for you, that will then will be processed via `predict()` which sets up a [`MonitoredSession`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/estimator/estimator.py#L595) which by default each Session is thread-safe. I believe the rest get used to monitor the sessions, but I would need to double-check on that. Turtles all the way down :). Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:390,energy efficiency,Monitor,MonitoredSession,390,"The short answer is yes. That's because that parameter gets fed to `prepare_inputs()`, which generates a `DeepVariantInput` object that generates a `tf.data.Dataset`, which gets fed in your case into an [`Estimator`](https://github.com/google/deepvariant/blob/r0.7/deepvariant/modeling.py#L415-L419) it constructs for you, that will then will be processed via `predict()` which sets up a [`MonitoredSession`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/estimator/estimator.py#L595) which by default each Session is thread-safe. I believe the rest get used to monitor the sessions, but I would need to double-check on that. Turtles all the way down :). Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:480,energy efficiency,estimat,estimator,480,"The short answer is yes. That's because that parameter gets fed to `prepare_inputs()`, which generates a `DeepVariantInput` object that generates a `tf.data.Dataset`, which gets fed in your case into an [`Estimator`](https://github.com/google/deepvariant/blob/r0.7/deepvariant/modeling.py#L415-L419) it constructs for you, that will then will be processed via `predict()` which sets up a [`MonitoredSession`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/estimator/estimator.py#L595) which by default each Session is thread-safe. I believe the rest get used to monitor the sessions, but I would need to double-check on that. Turtles all the way down :). Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:490,energy efficiency,estimat,estimator,490,"The short answer is yes. That's because that parameter gets fed to `prepare_inputs()`, which generates a `DeepVariantInput` object that generates a `tf.data.Dataset`, which gets fed in your case into an [`Estimator`](https://github.com/google/deepvariant/blob/r0.7/deepvariant/modeling.py#L415-L419) it constructs for you, that will then will be processed via `predict()` which sets up a [`MonitoredSession`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/estimator/estimator.py#L595) which by default each Session is thread-safe. I believe the rest get used to monitor the sessions, but I would need to double-check on that. Turtles all the way down :). Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:586,energy efficiency,monitor,monitor,586,"The short answer is yes. That's because that parameter gets fed to `prepare_inputs()`, which generates a `DeepVariantInput` object that generates a `tf.data.Dataset`, which gets fed in your case into an [`Estimator`](https://github.com/google/deepvariant/blob/r0.7/deepvariant/modeling.py#L415-L419) it constructs for you, that will then will be processed via `predict()` which sets up a [`MonitoredSession`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/estimator/estimator.py#L595) which by default each Session is thread-safe. I believe the rest get used to monitor the sessions, but I would need to double-check on that. Turtles all the way down :). Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:45,modifiability,paramet,parameter,45,"The short answer is yes. That's because that parameter gets fed to `prepare_inputs()`, which generates a `DeepVariantInput` object that generates a `tf.data.Dataset`, which gets fed in your case into an [`Estimator`](https://github.com/google/deepvariant/blob/r0.7/deepvariant/modeling.py#L415-L419) it constructs for you, that will then will be processed via `predict()` which sets up a [`MonitoredSession`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/estimator/estimator.py#L595) which by default each Session is thread-safe. I believe the rest get used to monitor the sessions, but I would need to double-check on that. Turtles all the way down :). Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:390,reliability,Monitor,MonitoredSession,390,"The short answer is yes. That's because that parameter gets fed to `prepare_inputs()`, which generates a `DeepVariantInput` object that generates a `tf.data.Dataset`, which gets fed in your case into an [`Estimator`](https://github.com/google/deepvariant/blob/r0.7/deepvariant/modeling.py#L415-L419) it constructs for you, that will then will be processed via `predict()` which sets up a [`MonitoredSession`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/estimator/estimator.py#L595) which by default each Session is thread-safe. I believe the rest get used to monitor the sessions, but I would need to double-check on that. Turtles all the way down :). Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:586,reliability,monitor,monitor,586,"The short answer is yes. That's because that parameter gets fed to `prepare_inputs()`, which generates a `DeepVariantInput` object that generates a `tf.data.Dataset`, which gets fed in your case into an [`Estimator`](https://github.com/google/deepvariant/blob/r0.7/deepvariant/modeling.py#L415-L419) it constructs for you, that will then will be processed via `predict()` which sets up a [`MonitoredSession`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/estimator/estimator.py#L595) which by default each Session is thread-safe. I believe the rest get used to monitor the sessions, but I would need to double-check on that. Turtles all the way down :). Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:361,safety,predict,predict,361,"The short answer is yes. That's because that parameter gets fed to `prepare_inputs()`, which generates a `DeepVariantInput` object that generates a `tf.data.Dataset`, which gets fed in your case into an [`Estimator`](https://github.com/google/deepvariant/blob/r0.7/deepvariant/modeling.py#L415-L419) it constructs for you, that will then will be processed via `predict()` which sets up a [`MonitoredSession`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/estimator/estimator.py#L595) which by default each Session is thread-safe. I believe the rest get used to monitor the sessions, but I would need to double-check on that. Turtles all the way down :). Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:390,safety,Monitor,MonitoredSession,390,"The short answer is yes. That's because that parameter gets fed to `prepare_inputs()`, which generates a `DeepVariantInput` object that generates a `tf.data.Dataset`, which gets fed in your case into an [`Estimator`](https://github.com/google/deepvariant/blob/r0.7/deepvariant/modeling.py#L415-L419) it constructs for you, that will then will be processed via `predict()` which sets up a [`MonitoredSession`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/estimator/estimator.py#L595) which by default each Session is thread-safe. I believe the rest get used to monitor the sessions, but I would need to double-check on that. Turtles all the way down :). Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:549,safety,safe,safe,549,"The short answer is yes. That's because that parameter gets fed to `prepare_inputs()`, which generates a `DeepVariantInput` object that generates a `tf.data.Dataset`, which gets fed in your case into an [`Estimator`](https://github.com/google/deepvariant/blob/r0.7/deepvariant/modeling.py#L415-L419) it constructs for you, that will then will be processed via `predict()` which sets up a [`MonitoredSession`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/estimator/estimator.py#L595) which by default each Session is thread-safe. I believe the rest get used to monitor the sessions, but I would need to double-check on that. Turtles all the way down :). Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:586,safety,monitor,monitor,586,"The short answer is yes. That's because that parameter gets fed to `prepare_inputs()`, which generates a `DeepVariantInput` object that generates a `tf.data.Dataset`, which gets fed in your case into an [`Estimator`](https://github.com/google/deepvariant/blob/r0.7/deepvariant/modeling.py#L415-L419) it constructs for you, that will then will be processed via `predict()` which sets up a [`MonitoredSession`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/estimator/estimator.py#L595) which by default each Session is thread-safe. I believe the rest get used to monitor the sessions, but I would need to double-check on that. Turtles all the way down :). Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:277,security,model,modeling,277,"The short answer is yes. That's because that parameter gets fed to `prepare_inputs()`, which generates a `DeepVariantInput` object that generates a `tf.data.Dataset`, which gets fed in your case into an [`Estimator`](https://github.com/google/deepvariant/blob/r0.7/deepvariant/modeling.py#L415-L419) it constructs for you, that will then will be processed via `predict()` which sets up a [`MonitoredSession`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/estimator/estimator.py#L595) which by default each Session is thread-safe. I believe the rest get used to monitor the sessions, but I would need to double-check on that. Turtles all the way down :). Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:531,security,Session,Session,531,"The short answer is yes. That's because that parameter gets fed to `prepare_inputs()`, which generates a `DeepVariantInput` object that generates a `tf.data.Dataset`, which gets fed in your case into an [`Estimator`](https://github.com/google/deepvariant/blob/r0.7/deepvariant/modeling.py#L415-L419) it constructs for you, that will then will be processed via `predict()` which sets up a [`MonitoredSession`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/estimator/estimator.py#L595) which by default each Session is thread-safe. I believe the rest get used to monitor the sessions, but I would need to double-check on that. Turtles all the way down :). Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:598,security,session,sessions,598,"The short answer is yes. That's because that parameter gets fed to `prepare_inputs()`, which generates a `DeepVariantInput` object that generates a `tf.data.Dataset`, which gets fed in your case into an [`Estimator`](https://github.com/google/deepvariant/blob/r0.7/deepvariant/modeling.py#L415-L419) it constructs for you, that will then will be processed via `predict()` which sets up a [`MonitoredSession`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/estimator/estimator.py#L595) which by default each Session is thread-safe. I believe the rest get used to monitor the sessions, but I would need to double-check on that. Turtles all the way down :). Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:390,testability,Monitor,MonitoredSession,390,"The short answer is yes. That's because that parameter gets fed to `prepare_inputs()`, which generates a `DeepVariantInput` object that generates a `tf.data.Dataset`, which gets fed in your case into an [`Estimator`](https://github.com/google/deepvariant/blob/r0.7/deepvariant/modeling.py#L415-L419) it constructs for you, that will then will be processed via `predict()` which sets up a [`MonitoredSession`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/estimator/estimator.py#L595) which by default each Session is thread-safe. I believe the rest get used to monitor the sessions, but I would need to double-check on that. Turtles all the way down :). Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:586,testability,monitor,monitor,586,"The short answer is yes. That's because that parameter gets fed to `prepare_inputs()`, which generates a `DeepVariantInput` object that generates a `tf.data.Dataset`, which gets fed in your case into an [`Estimator`](https://github.com/google/deepvariant/blob/r0.7/deepvariant/modeling.py#L415-L419) it constructs for you, that will then will be processed via `predict()` which sets up a [`MonitoredSession`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/estimator/estimator.py#L595) which by default each Session is thread-safe. I believe the rest get used to monitor the sessions, but I would need to double-check on that. Turtles all the way down :). Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:687,usability,help,helps,687,"The short answer is yes. That's because that parameter gets fed to `prepare_inputs()`, which generates a `DeepVariantInput` object that generates a `tf.data.Dataset`, which gets fed in your case into an [`Estimator`](https://github.com/google/deepvariant/blob/r0.7/deepvariant/modeling.py#L415-L419) it constructs for you, that will then will be processed via `predict()` which sets up a [`MonitoredSession`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/estimator/estimator.py#L595) which by default each Session is thread-safe. I believe the rest get used to monitor the sessions, but I would need to double-check on that. Turtles all the way down :). Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:137,availability,mask,mask,137,"@mclaugsf Thanks for the update! . 1) In terms of failed jobs -- we also noticed that our current recommendation of case studies tend to mask the issues if a run failed in the middle, because we currently pipe all output to log files. We're making some changes so that if anything fails in the middle, it'll be more clear to the users later. I'm still fixing a few more things, hopefully it'll come out in the next release. For now it's a good idea to just check the log files to make sure previous runs were successfully done before proceeding. 2) For call_variants, can you check your `call_variants.log` file and see what the lines look like? In my run for the WGS casestudy, it converges to something like:. ```. I0815 18:49:08.438520 140611550078720 call_variants.py:359] Processed 113665 examples in 223 batches [0.222 sec per 100]. I0815 18:49:09.491303 140611550078720 call_variants.py:359] Processed 114177 examples in 224 batches [0.222 sec per 100]. I0815 18:49:10.535501 140611550078720 call_variants.py:359] Processed 114689 examples in 225 batches [0.221 sec per 100]. ```. In our case study, we recommend just running one `call_variants` per machine. `call_variants` itself does utilize multiple CPUs now, so if you use top or htop to check your run, you should see that it uses more than one CPU. In my previous experience, running multiple `call_variants` on the same machine tends to make the run slower. If you're running call_variants separately on each shard, and if you can do each of them on different machines, that's probably most ideal. But if you plan to try running multiple `call_variants` on the same machine, you might want to watch out the speed because it will likely not be linearly faster. (If you find otherwise, let me know. I haven't tried it myself for a while now).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:1415,availability,slo,slower,1415,"@mclaugsf Thanks for the update! . 1) In terms of failed jobs -- we also noticed that our current recommendation of case studies tend to mask the issues if a run failed in the middle, because we currently pipe all output to log files. We're making some changes so that if anything fails in the middle, it'll be more clear to the users later. I'm still fixing a few more things, hopefully it'll come out in the next release. For now it's a good idea to just check the log files to make sure previous runs were successfully done before proceeding. 2) For call_variants, can you check your `call_variants.log` file and see what the lines look like? In my run for the WGS casestudy, it converges to something like:. ```. I0815 18:49:08.438520 140611550078720 call_variants.py:359] Processed 113665 examples in 223 batches [0.222 sec per 100]. I0815 18:49:09.491303 140611550078720 call_variants.py:359] Processed 114177 examples in 224 batches [0.222 sec per 100]. I0815 18:49:10.535501 140611550078720 call_variants.py:359] Processed 114689 examples in 225 batches [0.221 sec per 100]. ```. In our case study, we recommend just running one `call_variants` per machine. `call_variants` itself does utilize multiple CPUs now, so if you use top or htop to check your run, you should see that it uses more than one CPU. In my previous experience, running multiple `call_variants` on the same machine tends to make the run slower. If you're running call_variants separately on each shard, and if you can do each of them on different machines, that's probably most ideal. But if you plan to try running multiple `call_variants` on the same machine, you might want to watch out the speed because it will likely not be linearly faster. (If you find otherwise, let me know. I haven't tried it myself for a while now).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:25,deployability,updat,update,25,"@mclaugsf Thanks for the update! . 1) In terms of failed jobs -- we also noticed that our current recommendation of case studies tend to mask the issues if a run failed in the middle, because we currently pipe all output to log files. We're making some changes so that if anything fails in the middle, it'll be more clear to the users later. I'm still fixing a few more things, hopefully it'll come out in the next release. For now it's a good idea to just check the log files to make sure previous runs were successfully done before proceeding. 2) For call_variants, can you check your `call_variants.log` file and see what the lines look like? In my run for the WGS casestudy, it converges to something like:. ```. I0815 18:49:08.438520 140611550078720 call_variants.py:359] Processed 113665 examples in 223 batches [0.222 sec per 100]. I0815 18:49:09.491303 140611550078720 call_variants.py:359] Processed 114177 examples in 224 batches [0.222 sec per 100]. I0815 18:49:10.535501 140611550078720 call_variants.py:359] Processed 114689 examples in 225 batches [0.221 sec per 100]. ```. In our case study, we recommend just running one `call_variants` per machine. `call_variants` itself does utilize multiple CPUs now, so if you use top or htop to check your run, you should see that it uses more than one CPU. In my previous experience, running multiple `call_variants` on the same machine tends to make the run slower. If you're running call_variants separately on each shard, and if you can do each of them on different machines, that's probably most ideal. But if you plan to try running multiple `call_variants` on the same machine, you might want to watch out the speed because it will likely not be linearly faster. (If you find otherwise, let me know. I haven't tried it myself for a while now).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:50,deployability,fail,failed,50,"@mclaugsf Thanks for the update! . 1) In terms of failed jobs -- we also noticed that our current recommendation of case studies tend to mask the issues if a run failed in the middle, because we currently pipe all output to log files. We're making some changes so that if anything fails in the middle, it'll be more clear to the users later. I'm still fixing a few more things, hopefully it'll come out in the next release. For now it's a good idea to just check the log files to make sure previous runs were successfully done before proceeding. 2) For call_variants, can you check your `call_variants.log` file and see what the lines look like? In my run for the WGS casestudy, it converges to something like:. ```. I0815 18:49:08.438520 140611550078720 call_variants.py:359] Processed 113665 examples in 223 batches [0.222 sec per 100]. I0815 18:49:09.491303 140611550078720 call_variants.py:359] Processed 114177 examples in 224 batches [0.222 sec per 100]. I0815 18:49:10.535501 140611550078720 call_variants.py:359] Processed 114689 examples in 225 batches [0.221 sec per 100]. ```. In our case study, we recommend just running one `call_variants` per machine. `call_variants` itself does utilize multiple CPUs now, so if you use top or htop to check your run, you should see that it uses more than one CPU. In my previous experience, running multiple `call_variants` on the same machine tends to make the run slower. If you're running call_variants separately on each shard, and if you can do each of them on different machines, that's probably most ideal. But if you plan to try running multiple `call_variants` on the same machine, you might want to watch out the speed because it will likely not be linearly faster. (If you find otherwise, let me know. I haven't tried it myself for a while now).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:162,deployability,fail,failed,162,"@mclaugsf Thanks for the update! . 1) In terms of failed jobs -- we also noticed that our current recommendation of case studies tend to mask the issues if a run failed in the middle, because we currently pipe all output to log files. We're making some changes so that if anything fails in the middle, it'll be more clear to the users later. I'm still fixing a few more things, hopefully it'll come out in the next release. For now it's a good idea to just check the log files to make sure previous runs were successfully done before proceeding. 2) For call_variants, can you check your `call_variants.log` file and see what the lines look like? In my run for the WGS casestudy, it converges to something like:. ```. I0815 18:49:08.438520 140611550078720 call_variants.py:359] Processed 113665 examples in 223 batches [0.222 sec per 100]. I0815 18:49:09.491303 140611550078720 call_variants.py:359] Processed 114177 examples in 224 batches [0.222 sec per 100]. I0815 18:49:10.535501 140611550078720 call_variants.py:359] Processed 114689 examples in 225 batches [0.221 sec per 100]. ```. In our case study, we recommend just running one `call_variants` per machine. `call_variants` itself does utilize multiple CPUs now, so if you use top or htop to check your run, you should see that it uses more than one CPU. In my previous experience, running multiple `call_variants` on the same machine tends to make the run slower. If you're running call_variants separately on each shard, and if you can do each of them on different machines, that's probably most ideal. But if you plan to try running multiple `call_variants` on the same machine, you might want to watch out the speed because it will likely not be linearly faster. (If you find otherwise, let me know. I haven't tried it myself for a while now).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:224,deployability,log,log,224,"@mclaugsf Thanks for the update! . 1) In terms of failed jobs -- we also noticed that our current recommendation of case studies tend to mask the issues if a run failed in the middle, because we currently pipe all output to log files. We're making some changes so that if anything fails in the middle, it'll be more clear to the users later. I'm still fixing a few more things, hopefully it'll come out in the next release. For now it's a good idea to just check the log files to make sure previous runs were successfully done before proceeding. 2) For call_variants, can you check your `call_variants.log` file and see what the lines look like? In my run for the WGS casestudy, it converges to something like:. ```. I0815 18:49:08.438520 140611550078720 call_variants.py:359] Processed 113665 examples in 223 batches [0.222 sec per 100]. I0815 18:49:09.491303 140611550078720 call_variants.py:359] Processed 114177 examples in 224 batches [0.222 sec per 100]. I0815 18:49:10.535501 140611550078720 call_variants.py:359] Processed 114689 examples in 225 batches [0.221 sec per 100]. ```. In our case study, we recommend just running one `call_variants` per machine. `call_variants` itself does utilize multiple CPUs now, so if you use top or htop to check your run, you should see that it uses more than one CPU. In my previous experience, running multiple `call_variants` on the same machine tends to make the run slower. If you're running call_variants separately on each shard, and if you can do each of them on different machines, that's probably most ideal. But if you plan to try running multiple `call_variants` on the same machine, you might want to watch out the speed because it will likely not be linearly faster. (If you find otherwise, let me know. I haven't tried it myself for a while now).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:281,deployability,fail,fails,281,"@mclaugsf Thanks for the update! . 1) In terms of failed jobs -- we also noticed that our current recommendation of case studies tend to mask the issues if a run failed in the middle, because we currently pipe all output to log files. We're making some changes so that if anything fails in the middle, it'll be more clear to the users later. I'm still fixing a few more things, hopefully it'll come out in the next release. For now it's a good idea to just check the log files to make sure previous runs were successfully done before proceeding. 2) For call_variants, can you check your `call_variants.log` file and see what the lines look like? In my run for the WGS casestudy, it converges to something like:. ```. I0815 18:49:08.438520 140611550078720 call_variants.py:359] Processed 113665 examples in 223 batches [0.222 sec per 100]. I0815 18:49:09.491303 140611550078720 call_variants.py:359] Processed 114177 examples in 224 batches [0.222 sec per 100]. I0815 18:49:10.535501 140611550078720 call_variants.py:359] Processed 114689 examples in 225 batches [0.221 sec per 100]. ```. In our case study, we recommend just running one `call_variants` per machine. `call_variants` itself does utilize multiple CPUs now, so if you use top or htop to check your run, you should see that it uses more than one CPU. In my previous experience, running multiple `call_variants` on the same machine tends to make the run slower. If you're running call_variants separately on each shard, and if you can do each of them on different machines, that's probably most ideal. But if you plan to try running multiple `call_variants` on the same machine, you might want to watch out the speed because it will likely not be linearly faster. (If you find otherwise, let me know. I haven't tried it myself for a while now).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:415,deployability,releas,release,415,"@mclaugsf Thanks for the update! . 1) In terms of failed jobs -- we also noticed that our current recommendation of case studies tend to mask the issues if a run failed in the middle, because we currently pipe all output to log files. We're making some changes so that if anything fails in the middle, it'll be more clear to the users later. I'm still fixing a few more things, hopefully it'll come out in the next release. For now it's a good idea to just check the log files to make sure previous runs were successfully done before proceeding. 2) For call_variants, can you check your `call_variants.log` file and see what the lines look like? In my run for the WGS casestudy, it converges to something like:. ```. I0815 18:49:08.438520 140611550078720 call_variants.py:359] Processed 113665 examples in 223 batches [0.222 sec per 100]. I0815 18:49:09.491303 140611550078720 call_variants.py:359] Processed 114177 examples in 224 batches [0.222 sec per 100]. I0815 18:49:10.535501 140611550078720 call_variants.py:359] Processed 114689 examples in 225 batches [0.221 sec per 100]. ```. In our case study, we recommend just running one `call_variants` per machine. `call_variants` itself does utilize multiple CPUs now, so if you use top or htop to check your run, you should see that it uses more than one CPU. In my previous experience, running multiple `call_variants` on the same machine tends to make the run slower. If you're running call_variants separately on each shard, and if you can do each of them on different machines, that's probably most ideal. But if you plan to try running multiple `call_variants` on the same machine, you might want to watch out the speed because it will likely not be linearly faster. (If you find otherwise, let me know. I haven't tried it myself for a while now).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:467,deployability,log,log,467,"@mclaugsf Thanks for the update! . 1) In terms of failed jobs -- we also noticed that our current recommendation of case studies tend to mask the issues if a run failed in the middle, because we currently pipe all output to log files. We're making some changes so that if anything fails in the middle, it'll be more clear to the users later. I'm still fixing a few more things, hopefully it'll come out in the next release. For now it's a good idea to just check the log files to make sure previous runs were successfully done before proceeding. 2) For call_variants, can you check your `call_variants.log` file and see what the lines look like? In my run for the WGS casestudy, it converges to something like:. ```. I0815 18:49:08.438520 140611550078720 call_variants.py:359] Processed 113665 examples in 223 batches [0.222 sec per 100]. I0815 18:49:09.491303 140611550078720 call_variants.py:359] Processed 114177 examples in 224 batches [0.222 sec per 100]. I0815 18:49:10.535501 140611550078720 call_variants.py:359] Processed 114689 examples in 225 batches [0.221 sec per 100]. ```. In our case study, we recommend just running one `call_variants` per machine. `call_variants` itself does utilize multiple CPUs now, so if you use top or htop to check your run, you should see that it uses more than one CPU. In my previous experience, running multiple `call_variants` on the same machine tends to make the run slower. If you're running call_variants separately on each shard, and if you can do each of them on different machines, that's probably most ideal. But if you plan to try running multiple `call_variants` on the same machine, you might want to watch out the speed because it will likely not be linearly faster. (If you find otherwise, let me know. I haven't tried it myself for a while now).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:602,deployability,log,log,602,"@mclaugsf Thanks for the update! . 1) In terms of failed jobs -- we also noticed that our current recommendation of case studies tend to mask the issues if a run failed in the middle, because we currently pipe all output to log files. We're making some changes so that if anything fails in the middle, it'll be more clear to the users later. I'm still fixing a few more things, hopefully it'll come out in the next release. For now it's a good idea to just check the log files to make sure previous runs were successfully done before proceeding. 2) For call_variants, can you check your `call_variants.log` file and see what the lines look like? In my run for the WGS casestudy, it converges to something like:. ```. I0815 18:49:08.438520 140611550078720 call_variants.py:359] Processed 113665 examples in 223 batches [0.222 sec per 100]. I0815 18:49:09.491303 140611550078720 call_variants.py:359] Processed 114177 examples in 224 batches [0.222 sec per 100]. I0815 18:49:10.535501 140611550078720 call_variants.py:359] Processed 114689 examples in 225 batches [0.221 sec per 100]. ```. In our case study, we recommend just running one `call_variants` per machine. `call_variants` itself does utilize multiple CPUs now, so if you use top or htop to check your run, you should see that it uses more than one CPU. In my previous experience, running multiple `call_variants` on the same machine tends to make the run slower. If you're running call_variants separately on each shard, and if you can do each of them on different machines, that's probably most ideal. But if you plan to try running multiple `call_variants` on the same machine, you might want to watch out the speed because it will likely not be linearly faster. (If you find otherwise, let me know. I haven't tried it myself for a while now).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:90,energy efficiency,current,current,90,"@mclaugsf Thanks for the update! . 1) In terms of failed jobs -- we also noticed that our current recommendation of case studies tend to mask the issues if a run failed in the middle, because we currently pipe all output to log files. We're making some changes so that if anything fails in the middle, it'll be more clear to the users later. I'm still fixing a few more things, hopefully it'll come out in the next release. For now it's a good idea to just check the log files to make sure previous runs were successfully done before proceeding. 2) For call_variants, can you check your `call_variants.log` file and see what the lines look like? In my run for the WGS casestudy, it converges to something like:. ```. I0815 18:49:08.438520 140611550078720 call_variants.py:359] Processed 113665 examples in 223 batches [0.222 sec per 100]. I0815 18:49:09.491303 140611550078720 call_variants.py:359] Processed 114177 examples in 224 batches [0.222 sec per 100]. I0815 18:49:10.535501 140611550078720 call_variants.py:359] Processed 114689 examples in 225 batches [0.221 sec per 100]. ```. In our case study, we recommend just running one `call_variants` per machine. `call_variants` itself does utilize multiple CPUs now, so if you use top or htop to check your run, you should see that it uses more than one CPU. In my previous experience, running multiple `call_variants` on the same machine tends to make the run slower. If you're running call_variants separately on each shard, and if you can do each of them on different machines, that's probably most ideal. But if you plan to try running multiple `call_variants` on the same machine, you might want to watch out the speed because it will likely not be linearly faster. (If you find otherwise, let me know. I haven't tried it myself for a while now).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:195,energy efficiency,current,currently,195,"@mclaugsf Thanks for the update! . 1) In terms of failed jobs -- we also noticed that our current recommendation of case studies tend to mask the issues if a run failed in the middle, because we currently pipe all output to log files. We're making some changes so that if anything fails in the middle, it'll be more clear to the users later. I'm still fixing a few more things, hopefully it'll come out in the next release. For now it's a good idea to just check the log files to make sure previous runs were successfully done before proceeding. 2) For call_variants, can you check your `call_variants.log` file and see what the lines look like? In my run for the WGS casestudy, it converges to something like:. ```. I0815 18:49:08.438520 140611550078720 call_variants.py:359] Processed 113665 examples in 223 batches [0.222 sec per 100]. I0815 18:49:09.491303 140611550078720 call_variants.py:359] Processed 114177 examples in 224 batches [0.222 sec per 100]. I0815 18:49:10.535501 140611550078720 call_variants.py:359] Processed 114689 examples in 225 batches [0.221 sec per 100]. ```. In our case study, we recommend just running one `call_variants` per machine. `call_variants` itself does utilize multiple CPUs now, so if you use top or htop to check your run, you should see that it uses more than one CPU. In my previous experience, running multiple `call_variants` on the same machine tends to make the run slower. If you're running call_variants separately on each shard, and if you can do each of them on different machines, that's probably most ideal. But if you plan to try running multiple `call_variants` on the same machine, you might want to watch out the speed because it will likely not be linearly faster. (If you find otherwise, let me know. I haven't tried it myself for a while now).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:1211,energy efficiency,CPU,CPUs,1211,"@mclaugsf Thanks for the update! . 1) In terms of failed jobs -- we also noticed that our current recommendation of case studies tend to mask the issues if a run failed in the middle, because we currently pipe all output to log files. We're making some changes so that if anything fails in the middle, it'll be more clear to the users later. I'm still fixing a few more things, hopefully it'll come out in the next release. For now it's a good idea to just check the log files to make sure previous runs were successfully done before proceeding. 2) For call_variants, can you check your `call_variants.log` file and see what the lines look like? In my run for the WGS casestudy, it converges to something like:. ```. I0815 18:49:08.438520 140611550078720 call_variants.py:359] Processed 113665 examples in 223 batches [0.222 sec per 100]. I0815 18:49:09.491303 140611550078720 call_variants.py:359] Processed 114177 examples in 224 batches [0.222 sec per 100]. I0815 18:49:10.535501 140611550078720 call_variants.py:359] Processed 114689 examples in 225 batches [0.221 sec per 100]. ```. In our case study, we recommend just running one `call_variants` per machine. `call_variants` itself does utilize multiple CPUs now, so if you use top or htop to check your run, you should see that it uses more than one CPU. In my previous experience, running multiple `call_variants` on the same machine tends to make the run slower. If you're running call_variants separately on each shard, and if you can do each of them on different machines, that's probably most ideal. But if you plan to try running multiple `call_variants` on the same machine, you might want to watch out the speed because it will likely not be linearly faster. (If you find otherwise, let me know. I haven't tried it myself for a while now).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:1308,energy efficiency,CPU,CPU,1308,"@mclaugsf Thanks for the update! . 1) In terms of failed jobs -- we also noticed that our current recommendation of case studies tend to mask the issues if a run failed in the middle, because we currently pipe all output to log files. We're making some changes so that if anything fails in the middle, it'll be more clear to the users later. I'm still fixing a few more things, hopefully it'll come out in the next release. For now it's a good idea to just check the log files to make sure previous runs were successfully done before proceeding. 2) For call_variants, can you check your `call_variants.log` file and see what the lines look like? In my run for the WGS casestudy, it converges to something like:. ```. I0815 18:49:08.438520 140611550078720 call_variants.py:359] Processed 113665 examples in 223 batches [0.222 sec per 100]. I0815 18:49:09.491303 140611550078720 call_variants.py:359] Processed 114177 examples in 224 batches [0.222 sec per 100]. I0815 18:49:10.535501 140611550078720 call_variants.py:359] Processed 114689 examples in 225 batches [0.221 sec per 100]. ```. In our case study, we recommend just running one `call_variants` per machine. `call_variants` itself does utilize multiple CPUs now, so if you use top or htop to check your run, you should see that it uses more than one CPU. In my previous experience, running multiple `call_variants` on the same machine tends to make the run slower. If you're running call_variants separately on each shard, and if you can do each of them on different machines, that's probably most ideal. But if you plan to try running multiple `call_variants` on the same machine, you might want to watch out the speed because it will likely not be linearly faster. (If you find otherwise, let me know. I haven't tried it myself for a while now).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:810,integrability,batch,batches,810,"@mclaugsf Thanks for the update! . 1) In terms of failed jobs -- we also noticed that our current recommendation of case studies tend to mask the issues if a run failed in the middle, because we currently pipe all output to log files. We're making some changes so that if anything fails in the middle, it'll be more clear to the users later. I'm still fixing a few more things, hopefully it'll come out in the next release. For now it's a good idea to just check the log files to make sure previous runs were successfully done before proceeding. 2) For call_variants, can you check your `call_variants.log` file and see what the lines look like? In my run for the WGS casestudy, it converges to something like:. ```. I0815 18:49:08.438520 140611550078720 call_variants.py:359] Processed 113665 examples in 223 batches [0.222 sec per 100]. I0815 18:49:09.491303 140611550078720 call_variants.py:359] Processed 114177 examples in 224 batches [0.222 sec per 100]. I0815 18:49:10.535501 140611550078720 call_variants.py:359] Processed 114689 examples in 225 batches [0.221 sec per 100]. ```. In our case study, we recommend just running one `call_variants` per machine. `call_variants` itself does utilize multiple CPUs now, so if you use top or htop to check your run, you should see that it uses more than one CPU. In my previous experience, running multiple `call_variants` on the same machine tends to make the run slower. If you're running call_variants separately on each shard, and if you can do each of them on different machines, that's probably most ideal. But if you plan to try running multiple `call_variants` on the same machine, you might want to watch out the speed because it will likely not be linearly faster. (If you find otherwise, let me know. I haven't tried it myself for a while now).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:932,integrability,batch,batches,932,"@mclaugsf Thanks for the update! . 1) In terms of failed jobs -- we also noticed that our current recommendation of case studies tend to mask the issues if a run failed in the middle, because we currently pipe all output to log files. We're making some changes so that if anything fails in the middle, it'll be more clear to the users later. I'm still fixing a few more things, hopefully it'll come out in the next release. For now it's a good idea to just check the log files to make sure previous runs were successfully done before proceeding. 2) For call_variants, can you check your `call_variants.log` file and see what the lines look like? In my run for the WGS casestudy, it converges to something like:. ```. I0815 18:49:08.438520 140611550078720 call_variants.py:359] Processed 113665 examples in 223 batches [0.222 sec per 100]. I0815 18:49:09.491303 140611550078720 call_variants.py:359] Processed 114177 examples in 224 batches [0.222 sec per 100]. I0815 18:49:10.535501 140611550078720 call_variants.py:359] Processed 114689 examples in 225 batches [0.221 sec per 100]. ```. In our case study, we recommend just running one `call_variants` per machine. `call_variants` itself does utilize multiple CPUs now, so if you use top or htop to check your run, you should see that it uses more than one CPU. In my previous experience, running multiple `call_variants` on the same machine tends to make the run slower. If you're running call_variants separately on each shard, and if you can do each of them on different machines, that's probably most ideal. But if you plan to try running multiple `call_variants` on the same machine, you might want to watch out the speed because it will likely not be linearly faster. (If you find otherwise, let me know. I haven't tried it myself for a while now).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:1054,integrability,batch,batches,1054,"@mclaugsf Thanks for the update! . 1) In terms of failed jobs -- we also noticed that our current recommendation of case studies tend to mask the issues if a run failed in the middle, because we currently pipe all output to log files. We're making some changes so that if anything fails in the middle, it'll be more clear to the users later. I'm still fixing a few more things, hopefully it'll come out in the next release. For now it's a good idea to just check the log files to make sure previous runs were successfully done before proceeding. 2) For call_variants, can you check your `call_variants.log` file and see what the lines look like? In my run for the WGS casestudy, it converges to something like:. ```. I0815 18:49:08.438520 140611550078720 call_variants.py:359] Processed 113665 examples in 223 batches [0.222 sec per 100]. I0815 18:49:09.491303 140611550078720 call_variants.py:359] Processed 114177 examples in 224 batches [0.222 sec per 100]. I0815 18:49:10.535501 140611550078720 call_variants.py:359] Processed 114689 examples in 225 batches [0.221 sec per 100]. ```. In our case study, we recommend just running one `call_variants` per machine. `call_variants` itself does utilize multiple CPUs now, so if you use top or htop to check your run, you should see that it uses more than one CPU. In my previous experience, running multiple `call_variants` on the same machine tends to make the run slower. If you're running call_variants separately on each shard, and if you can do each of them on different machines, that's probably most ideal. But if you plan to try running multiple `call_variants` on the same machine, you might want to watch out the speed because it will likely not be linearly faster. (If you find otherwise, let me know. I haven't tried it myself for a while now).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:810,performance,batch,batches,810,"@mclaugsf Thanks for the update! . 1) In terms of failed jobs -- we also noticed that our current recommendation of case studies tend to mask the issues if a run failed in the middle, because we currently pipe all output to log files. We're making some changes so that if anything fails in the middle, it'll be more clear to the users later. I'm still fixing a few more things, hopefully it'll come out in the next release. For now it's a good idea to just check the log files to make sure previous runs were successfully done before proceeding. 2) For call_variants, can you check your `call_variants.log` file and see what the lines look like? In my run for the WGS casestudy, it converges to something like:. ```. I0815 18:49:08.438520 140611550078720 call_variants.py:359] Processed 113665 examples in 223 batches [0.222 sec per 100]. I0815 18:49:09.491303 140611550078720 call_variants.py:359] Processed 114177 examples in 224 batches [0.222 sec per 100]. I0815 18:49:10.535501 140611550078720 call_variants.py:359] Processed 114689 examples in 225 batches [0.221 sec per 100]. ```. In our case study, we recommend just running one `call_variants` per machine. `call_variants` itself does utilize multiple CPUs now, so if you use top or htop to check your run, you should see that it uses more than one CPU. In my previous experience, running multiple `call_variants` on the same machine tends to make the run slower. If you're running call_variants separately on each shard, and if you can do each of them on different machines, that's probably most ideal. But if you plan to try running multiple `call_variants` on the same machine, you might want to watch out the speed because it will likely not be linearly faster. (If you find otherwise, let me know. I haven't tried it myself for a while now).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:932,performance,batch,batches,932,"@mclaugsf Thanks for the update! . 1) In terms of failed jobs -- we also noticed that our current recommendation of case studies tend to mask the issues if a run failed in the middle, because we currently pipe all output to log files. We're making some changes so that if anything fails in the middle, it'll be more clear to the users later. I'm still fixing a few more things, hopefully it'll come out in the next release. For now it's a good idea to just check the log files to make sure previous runs were successfully done before proceeding. 2) For call_variants, can you check your `call_variants.log` file and see what the lines look like? In my run for the WGS casestudy, it converges to something like:. ```. I0815 18:49:08.438520 140611550078720 call_variants.py:359] Processed 113665 examples in 223 batches [0.222 sec per 100]. I0815 18:49:09.491303 140611550078720 call_variants.py:359] Processed 114177 examples in 224 batches [0.222 sec per 100]. I0815 18:49:10.535501 140611550078720 call_variants.py:359] Processed 114689 examples in 225 batches [0.221 sec per 100]. ```. In our case study, we recommend just running one `call_variants` per machine. `call_variants` itself does utilize multiple CPUs now, so if you use top or htop to check your run, you should see that it uses more than one CPU. In my previous experience, running multiple `call_variants` on the same machine tends to make the run slower. If you're running call_variants separately on each shard, and if you can do each of them on different machines, that's probably most ideal. But if you plan to try running multiple `call_variants` on the same machine, you might want to watch out the speed because it will likely not be linearly faster. (If you find otherwise, let me know. I haven't tried it myself for a while now).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:1054,performance,batch,batches,1054,"@mclaugsf Thanks for the update! . 1) In terms of failed jobs -- we also noticed that our current recommendation of case studies tend to mask the issues if a run failed in the middle, because we currently pipe all output to log files. We're making some changes so that if anything fails in the middle, it'll be more clear to the users later. I'm still fixing a few more things, hopefully it'll come out in the next release. For now it's a good idea to just check the log files to make sure previous runs were successfully done before proceeding. 2) For call_variants, can you check your `call_variants.log` file and see what the lines look like? In my run for the WGS casestudy, it converges to something like:. ```. I0815 18:49:08.438520 140611550078720 call_variants.py:359] Processed 113665 examples in 223 batches [0.222 sec per 100]. I0815 18:49:09.491303 140611550078720 call_variants.py:359] Processed 114177 examples in 224 batches [0.222 sec per 100]. I0815 18:49:10.535501 140611550078720 call_variants.py:359] Processed 114689 examples in 225 batches [0.221 sec per 100]. ```. In our case study, we recommend just running one `call_variants` per machine. `call_variants` itself does utilize multiple CPUs now, so if you use top or htop to check your run, you should see that it uses more than one CPU. In my previous experience, running multiple `call_variants` on the same machine tends to make the run slower. If you're running call_variants separately on each shard, and if you can do each of them on different machines, that's probably most ideal. But if you plan to try running multiple `call_variants` on the same machine, you might want to watch out the speed because it will likely not be linearly faster. (If you find otherwise, let me know. I haven't tried it myself for a while now).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:1211,performance,CPU,CPUs,1211,"@mclaugsf Thanks for the update! . 1) In terms of failed jobs -- we also noticed that our current recommendation of case studies tend to mask the issues if a run failed in the middle, because we currently pipe all output to log files. We're making some changes so that if anything fails in the middle, it'll be more clear to the users later. I'm still fixing a few more things, hopefully it'll come out in the next release. For now it's a good idea to just check the log files to make sure previous runs were successfully done before proceeding. 2) For call_variants, can you check your `call_variants.log` file and see what the lines look like? In my run for the WGS casestudy, it converges to something like:. ```. I0815 18:49:08.438520 140611550078720 call_variants.py:359] Processed 113665 examples in 223 batches [0.222 sec per 100]. I0815 18:49:09.491303 140611550078720 call_variants.py:359] Processed 114177 examples in 224 batches [0.222 sec per 100]. I0815 18:49:10.535501 140611550078720 call_variants.py:359] Processed 114689 examples in 225 batches [0.221 sec per 100]. ```. In our case study, we recommend just running one `call_variants` per machine. `call_variants` itself does utilize multiple CPUs now, so if you use top or htop to check your run, you should see that it uses more than one CPU. In my previous experience, running multiple `call_variants` on the same machine tends to make the run slower. If you're running call_variants separately on each shard, and if you can do each of them on different machines, that's probably most ideal. But if you plan to try running multiple `call_variants` on the same machine, you might want to watch out the speed because it will likely not be linearly faster. (If you find otherwise, let me know. I haven't tried it myself for a while now).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:1308,performance,CPU,CPU,1308,"@mclaugsf Thanks for the update! . 1) In terms of failed jobs -- we also noticed that our current recommendation of case studies tend to mask the issues if a run failed in the middle, because we currently pipe all output to log files. We're making some changes so that if anything fails in the middle, it'll be more clear to the users later. I'm still fixing a few more things, hopefully it'll come out in the next release. For now it's a good idea to just check the log files to make sure previous runs were successfully done before proceeding. 2) For call_variants, can you check your `call_variants.log` file and see what the lines look like? In my run for the WGS casestudy, it converges to something like:. ```. I0815 18:49:08.438520 140611550078720 call_variants.py:359] Processed 113665 examples in 223 batches [0.222 sec per 100]. I0815 18:49:09.491303 140611550078720 call_variants.py:359] Processed 114177 examples in 224 batches [0.222 sec per 100]. I0815 18:49:10.535501 140611550078720 call_variants.py:359] Processed 114689 examples in 225 batches [0.221 sec per 100]. ```. In our case study, we recommend just running one `call_variants` per machine. `call_variants` itself does utilize multiple CPUs now, so if you use top or htop to check your run, you should see that it uses more than one CPU. In my previous experience, running multiple `call_variants` on the same machine tends to make the run slower. If you're running call_variants separately on each shard, and if you can do each of them on different machines, that's probably most ideal. But if you plan to try running multiple `call_variants` on the same machine, you might want to watch out the speed because it will likely not be linearly faster. (If you find otherwise, let me know. I haven't tried it myself for a while now).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:50,reliability,fail,failed,50,"@mclaugsf Thanks for the update! . 1) In terms of failed jobs -- we also noticed that our current recommendation of case studies tend to mask the issues if a run failed in the middle, because we currently pipe all output to log files. We're making some changes so that if anything fails in the middle, it'll be more clear to the users later. I'm still fixing a few more things, hopefully it'll come out in the next release. For now it's a good idea to just check the log files to make sure previous runs were successfully done before proceeding. 2) For call_variants, can you check your `call_variants.log` file and see what the lines look like? In my run for the WGS casestudy, it converges to something like:. ```. I0815 18:49:08.438520 140611550078720 call_variants.py:359] Processed 113665 examples in 223 batches [0.222 sec per 100]. I0815 18:49:09.491303 140611550078720 call_variants.py:359] Processed 114177 examples in 224 batches [0.222 sec per 100]. I0815 18:49:10.535501 140611550078720 call_variants.py:359] Processed 114689 examples in 225 batches [0.221 sec per 100]. ```. In our case study, we recommend just running one `call_variants` per machine. `call_variants` itself does utilize multiple CPUs now, so if you use top or htop to check your run, you should see that it uses more than one CPU. In my previous experience, running multiple `call_variants` on the same machine tends to make the run slower. If you're running call_variants separately on each shard, and if you can do each of them on different machines, that's probably most ideal. But if you plan to try running multiple `call_variants` on the same machine, you might want to watch out the speed because it will likely not be linearly faster. (If you find otherwise, let me know. I haven't tried it myself for a while now).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:162,reliability,fail,failed,162,"@mclaugsf Thanks for the update! . 1) In terms of failed jobs -- we also noticed that our current recommendation of case studies tend to mask the issues if a run failed in the middle, because we currently pipe all output to log files. We're making some changes so that if anything fails in the middle, it'll be more clear to the users later. I'm still fixing a few more things, hopefully it'll come out in the next release. For now it's a good idea to just check the log files to make sure previous runs were successfully done before proceeding. 2) For call_variants, can you check your `call_variants.log` file and see what the lines look like? In my run for the WGS casestudy, it converges to something like:. ```. I0815 18:49:08.438520 140611550078720 call_variants.py:359] Processed 113665 examples in 223 batches [0.222 sec per 100]. I0815 18:49:09.491303 140611550078720 call_variants.py:359] Processed 114177 examples in 224 batches [0.222 sec per 100]. I0815 18:49:10.535501 140611550078720 call_variants.py:359] Processed 114689 examples in 225 batches [0.221 sec per 100]. ```. In our case study, we recommend just running one `call_variants` per machine. `call_variants` itself does utilize multiple CPUs now, so if you use top or htop to check your run, you should see that it uses more than one CPU. In my previous experience, running multiple `call_variants` on the same machine tends to make the run slower. If you're running call_variants separately on each shard, and if you can do each of them on different machines, that's probably most ideal. But if you plan to try running multiple `call_variants` on the same machine, you might want to watch out the speed because it will likely not be linearly faster. (If you find otherwise, let me know. I haven't tried it myself for a while now).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:281,reliability,fail,fails,281,"@mclaugsf Thanks for the update! . 1) In terms of failed jobs -- we also noticed that our current recommendation of case studies tend to mask the issues if a run failed in the middle, because we currently pipe all output to log files. We're making some changes so that if anything fails in the middle, it'll be more clear to the users later. I'm still fixing a few more things, hopefully it'll come out in the next release. For now it's a good idea to just check the log files to make sure previous runs were successfully done before proceeding. 2) For call_variants, can you check your `call_variants.log` file and see what the lines look like? In my run for the WGS casestudy, it converges to something like:. ```. I0815 18:49:08.438520 140611550078720 call_variants.py:359] Processed 113665 examples in 223 batches [0.222 sec per 100]. I0815 18:49:09.491303 140611550078720 call_variants.py:359] Processed 114177 examples in 224 batches [0.222 sec per 100]. I0815 18:49:10.535501 140611550078720 call_variants.py:359] Processed 114689 examples in 225 batches [0.221 sec per 100]. ```. In our case study, we recommend just running one `call_variants` per machine. `call_variants` itself does utilize multiple CPUs now, so if you use top or htop to check your run, you should see that it uses more than one CPU. In my previous experience, running multiple `call_variants` on the same machine tends to make the run slower. If you're running call_variants separately on each shard, and if you can do each of them on different machines, that's probably most ideal. But if you plan to try running multiple `call_variants` on the same machine, you might want to watch out the speed because it will likely not be linearly faster. (If you find otherwise, let me know. I haven't tried it myself for a while now).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:1189,reliability,doe,does,1189,"@mclaugsf Thanks for the update! . 1) In terms of failed jobs -- we also noticed that our current recommendation of case studies tend to mask the issues if a run failed in the middle, because we currently pipe all output to log files. We're making some changes so that if anything fails in the middle, it'll be more clear to the users later. I'm still fixing a few more things, hopefully it'll come out in the next release. For now it's a good idea to just check the log files to make sure previous runs were successfully done before proceeding. 2) For call_variants, can you check your `call_variants.log` file and see what the lines look like? In my run for the WGS casestudy, it converges to something like:. ```. I0815 18:49:08.438520 140611550078720 call_variants.py:359] Processed 113665 examples in 223 batches [0.222 sec per 100]. I0815 18:49:09.491303 140611550078720 call_variants.py:359] Processed 114177 examples in 224 batches [0.222 sec per 100]. I0815 18:49:10.535501 140611550078720 call_variants.py:359] Processed 114689 examples in 225 batches [0.221 sec per 100]. ```. In our case study, we recommend just running one `call_variants` per machine. `call_variants` itself does utilize multiple CPUs now, so if you use top or htop to check your run, you should see that it uses more than one CPU. In my previous experience, running multiple `call_variants` on the same machine tends to make the run slower. If you're running call_variants separately on each shard, and if you can do each of them on different machines, that's probably most ideal. But if you plan to try running multiple `call_variants` on the same machine, you might want to watch out the speed because it will likely not be linearly faster. (If you find otherwise, let me know. I haven't tried it myself for a while now).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:1415,reliability,slo,slower,1415,"@mclaugsf Thanks for the update! . 1) In terms of failed jobs -- we also noticed that our current recommendation of case studies tend to mask the issues if a run failed in the middle, because we currently pipe all output to log files. We're making some changes so that if anything fails in the middle, it'll be more clear to the users later. I'm still fixing a few more things, hopefully it'll come out in the next release. For now it's a good idea to just check the log files to make sure previous runs were successfully done before proceeding. 2) For call_variants, can you check your `call_variants.log` file and see what the lines look like? In my run for the WGS casestudy, it converges to something like:. ```. I0815 18:49:08.438520 140611550078720 call_variants.py:359] Processed 113665 examples in 223 batches [0.222 sec per 100]. I0815 18:49:09.491303 140611550078720 call_variants.py:359] Processed 114177 examples in 224 batches [0.222 sec per 100]. I0815 18:49:10.535501 140611550078720 call_variants.py:359] Processed 114689 examples in 225 batches [0.221 sec per 100]. ```. In our case study, we recommend just running one `call_variants` per machine. `call_variants` itself does utilize multiple CPUs now, so if you use top or htop to check your run, you should see that it uses more than one CPU. In my previous experience, running multiple `call_variants` on the same machine tends to make the run slower. If you're running call_variants separately on each shard, and if you can do each of them on different machines, that's probably most ideal. But if you plan to try running multiple `call_variants` on the same machine, you might want to watch out the speed because it will likely not be linearly faster. (If you find otherwise, let me know. I haven't tried it myself for a while now).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:25,safety,updat,update,25,"@mclaugsf Thanks for the update! . 1) In terms of failed jobs -- we also noticed that our current recommendation of case studies tend to mask the issues if a run failed in the middle, because we currently pipe all output to log files. We're making some changes so that if anything fails in the middle, it'll be more clear to the users later. I'm still fixing a few more things, hopefully it'll come out in the next release. For now it's a good idea to just check the log files to make sure previous runs were successfully done before proceeding. 2) For call_variants, can you check your `call_variants.log` file and see what the lines look like? In my run for the WGS casestudy, it converges to something like:. ```. I0815 18:49:08.438520 140611550078720 call_variants.py:359] Processed 113665 examples in 223 batches [0.222 sec per 100]. I0815 18:49:09.491303 140611550078720 call_variants.py:359] Processed 114177 examples in 224 batches [0.222 sec per 100]. I0815 18:49:10.535501 140611550078720 call_variants.py:359] Processed 114689 examples in 225 batches [0.221 sec per 100]. ```. In our case study, we recommend just running one `call_variants` per machine. `call_variants` itself does utilize multiple CPUs now, so if you use top or htop to check your run, you should see that it uses more than one CPU. In my previous experience, running multiple `call_variants` on the same machine tends to make the run slower. If you're running call_variants separately on each shard, and if you can do each of them on different machines, that's probably most ideal. But if you plan to try running multiple `call_variants` on the same machine, you might want to watch out the speed because it will likely not be linearly faster. (If you find otherwise, let me know. I haven't tried it myself for a while now).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:224,safety,log,log,224,"@mclaugsf Thanks for the update! . 1) In terms of failed jobs -- we also noticed that our current recommendation of case studies tend to mask the issues if a run failed in the middle, because we currently pipe all output to log files. We're making some changes so that if anything fails in the middle, it'll be more clear to the users later. I'm still fixing a few more things, hopefully it'll come out in the next release. For now it's a good idea to just check the log files to make sure previous runs were successfully done before proceeding. 2) For call_variants, can you check your `call_variants.log` file and see what the lines look like? In my run for the WGS casestudy, it converges to something like:. ```. I0815 18:49:08.438520 140611550078720 call_variants.py:359] Processed 113665 examples in 223 batches [0.222 sec per 100]. I0815 18:49:09.491303 140611550078720 call_variants.py:359] Processed 114177 examples in 224 batches [0.222 sec per 100]. I0815 18:49:10.535501 140611550078720 call_variants.py:359] Processed 114689 examples in 225 batches [0.221 sec per 100]. ```. In our case study, we recommend just running one `call_variants` per machine. `call_variants` itself does utilize multiple CPUs now, so if you use top or htop to check your run, you should see that it uses more than one CPU. In my previous experience, running multiple `call_variants` on the same machine tends to make the run slower. If you're running call_variants separately on each shard, and if you can do each of them on different machines, that's probably most ideal. But if you plan to try running multiple `call_variants` on the same machine, you might want to watch out the speed because it will likely not be linearly faster. (If you find otherwise, let me know. I haven't tried it myself for a while now).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:467,safety,log,log,467,"@mclaugsf Thanks for the update! . 1) In terms of failed jobs -- we also noticed that our current recommendation of case studies tend to mask the issues if a run failed in the middle, because we currently pipe all output to log files. We're making some changes so that if anything fails in the middle, it'll be more clear to the users later. I'm still fixing a few more things, hopefully it'll come out in the next release. For now it's a good idea to just check the log files to make sure previous runs were successfully done before proceeding. 2) For call_variants, can you check your `call_variants.log` file and see what the lines look like? In my run for the WGS casestudy, it converges to something like:. ```. I0815 18:49:08.438520 140611550078720 call_variants.py:359] Processed 113665 examples in 223 batches [0.222 sec per 100]. I0815 18:49:09.491303 140611550078720 call_variants.py:359] Processed 114177 examples in 224 batches [0.222 sec per 100]. I0815 18:49:10.535501 140611550078720 call_variants.py:359] Processed 114689 examples in 225 batches [0.221 sec per 100]. ```. In our case study, we recommend just running one `call_variants` per machine. `call_variants` itself does utilize multiple CPUs now, so if you use top or htop to check your run, you should see that it uses more than one CPU. In my previous experience, running multiple `call_variants` on the same machine tends to make the run slower. If you're running call_variants separately on each shard, and if you can do each of them on different machines, that's probably most ideal. But if you plan to try running multiple `call_variants` on the same machine, you might want to watch out the speed because it will likely not be linearly faster. (If you find otherwise, let me know. I haven't tried it myself for a while now).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:602,safety,log,log,602,"@mclaugsf Thanks for the update! . 1) In terms of failed jobs -- we also noticed that our current recommendation of case studies tend to mask the issues if a run failed in the middle, because we currently pipe all output to log files. We're making some changes so that if anything fails in the middle, it'll be more clear to the users later. I'm still fixing a few more things, hopefully it'll come out in the next release. For now it's a good idea to just check the log files to make sure previous runs were successfully done before proceeding. 2) For call_variants, can you check your `call_variants.log` file and see what the lines look like? In my run for the WGS casestudy, it converges to something like:. ```. I0815 18:49:08.438520 140611550078720 call_variants.py:359] Processed 113665 examples in 223 batches [0.222 sec per 100]. I0815 18:49:09.491303 140611550078720 call_variants.py:359] Processed 114177 examples in 224 batches [0.222 sec per 100]. I0815 18:49:10.535501 140611550078720 call_variants.py:359] Processed 114689 examples in 225 batches [0.221 sec per 100]. ```. In our case study, we recommend just running one `call_variants` per machine. `call_variants` itself does utilize multiple CPUs now, so if you use top or htop to check your run, you should see that it uses more than one CPU. In my previous experience, running multiple `call_variants` on the same machine tends to make the run slower. If you're running call_variants separately on each shard, and if you can do each of them on different machines, that's probably most ideal. But if you plan to try running multiple `call_variants` on the same machine, you might want to watch out the speed because it will likely not be linearly faster. (If you find otherwise, let me know. I haven't tried it myself for a while now).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:25,security,updat,update,25,"@mclaugsf Thanks for the update! . 1) In terms of failed jobs -- we also noticed that our current recommendation of case studies tend to mask the issues if a run failed in the middle, because we currently pipe all output to log files. We're making some changes so that if anything fails in the middle, it'll be more clear to the users later. I'm still fixing a few more things, hopefully it'll come out in the next release. For now it's a good idea to just check the log files to make sure previous runs were successfully done before proceeding. 2) For call_variants, can you check your `call_variants.log` file and see what the lines look like? In my run for the WGS casestudy, it converges to something like:. ```. I0815 18:49:08.438520 140611550078720 call_variants.py:359] Processed 113665 examples in 223 batches [0.222 sec per 100]. I0815 18:49:09.491303 140611550078720 call_variants.py:359] Processed 114177 examples in 224 batches [0.222 sec per 100]. I0815 18:49:10.535501 140611550078720 call_variants.py:359] Processed 114689 examples in 225 batches [0.221 sec per 100]. ```. In our case study, we recommend just running one `call_variants` per machine. `call_variants` itself does utilize multiple CPUs now, so if you use top or htop to check your run, you should see that it uses more than one CPU. In my previous experience, running multiple `call_variants` on the same machine tends to make the run slower. If you're running call_variants separately on each shard, and if you can do each of them on different machines, that's probably most ideal. But if you plan to try running multiple `call_variants` on the same machine, you might want to watch out the speed because it will likely not be linearly faster. (If you find otherwise, let me know. I haven't tried it myself for a while now).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:224,security,log,log,224,"@mclaugsf Thanks for the update! . 1) In terms of failed jobs -- we also noticed that our current recommendation of case studies tend to mask the issues if a run failed in the middle, because we currently pipe all output to log files. We're making some changes so that if anything fails in the middle, it'll be more clear to the users later. I'm still fixing a few more things, hopefully it'll come out in the next release. For now it's a good idea to just check the log files to make sure previous runs were successfully done before proceeding. 2) For call_variants, can you check your `call_variants.log` file and see what the lines look like? In my run for the WGS casestudy, it converges to something like:. ```. I0815 18:49:08.438520 140611550078720 call_variants.py:359] Processed 113665 examples in 223 batches [0.222 sec per 100]. I0815 18:49:09.491303 140611550078720 call_variants.py:359] Processed 114177 examples in 224 batches [0.222 sec per 100]. I0815 18:49:10.535501 140611550078720 call_variants.py:359] Processed 114689 examples in 225 batches [0.221 sec per 100]. ```. In our case study, we recommend just running one `call_variants` per machine. `call_variants` itself does utilize multiple CPUs now, so if you use top or htop to check your run, you should see that it uses more than one CPU. In my previous experience, running multiple `call_variants` on the same machine tends to make the run slower. If you're running call_variants separately on each shard, and if you can do each of them on different machines, that's probably most ideal. But if you plan to try running multiple `call_variants` on the same machine, you might want to watch out the speed because it will likely not be linearly faster. (If you find otherwise, let me know. I haven't tried it myself for a while now).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:467,security,log,log,467,"@mclaugsf Thanks for the update! . 1) In terms of failed jobs -- we also noticed that our current recommendation of case studies tend to mask the issues if a run failed in the middle, because we currently pipe all output to log files. We're making some changes so that if anything fails in the middle, it'll be more clear to the users later. I'm still fixing a few more things, hopefully it'll come out in the next release. For now it's a good idea to just check the log files to make sure previous runs were successfully done before proceeding. 2) For call_variants, can you check your `call_variants.log` file and see what the lines look like? In my run for the WGS casestudy, it converges to something like:. ```. I0815 18:49:08.438520 140611550078720 call_variants.py:359] Processed 113665 examples in 223 batches [0.222 sec per 100]. I0815 18:49:09.491303 140611550078720 call_variants.py:359] Processed 114177 examples in 224 batches [0.222 sec per 100]. I0815 18:49:10.535501 140611550078720 call_variants.py:359] Processed 114689 examples in 225 batches [0.221 sec per 100]. ```. In our case study, we recommend just running one `call_variants` per machine. `call_variants` itself does utilize multiple CPUs now, so if you use top or htop to check your run, you should see that it uses more than one CPU. In my previous experience, running multiple `call_variants` on the same machine tends to make the run slower. If you're running call_variants separately on each shard, and if you can do each of them on different machines, that's probably most ideal. But if you plan to try running multiple `call_variants` on the same machine, you might want to watch out the speed because it will likely not be linearly faster. (If you find otherwise, let me know. I haven't tried it myself for a while now).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:602,security,log,log,602,"@mclaugsf Thanks for the update! . 1) In terms of failed jobs -- we also noticed that our current recommendation of case studies tend to mask the issues if a run failed in the middle, because we currently pipe all output to log files. We're making some changes so that if anything fails in the middle, it'll be more clear to the users later. I'm still fixing a few more things, hopefully it'll come out in the next release. For now it's a good idea to just check the log files to make sure previous runs were successfully done before proceeding. 2) For call_variants, can you check your `call_variants.log` file and see what the lines look like? In my run for the WGS casestudy, it converges to something like:. ```. I0815 18:49:08.438520 140611550078720 call_variants.py:359] Processed 113665 examples in 223 batches [0.222 sec per 100]. I0815 18:49:09.491303 140611550078720 call_variants.py:359] Processed 114177 examples in 224 batches [0.222 sec per 100]. I0815 18:49:10.535501 140611550078720 call_variants.py:359] Processed 114689 examples in 225 batches [0.221 sec per 100]. ```. In our case study, we recommend just running one `call_variants` per machine. `call_variants` itself does utilize multiple CPUs now, so if you use top or htop to check your run, you should see that it uses more than one CPU. In my previous experience, running multiple `call_variants` on the same machine tends to make the run slower. If you're running call_variants separately on each shard, and if you can do each of them on different machines, that's probably most ideal. But if you plan to try running multiple `call_variants` on the same machine, you might want to watch out the speed because it will likely not be linearly faster. (If you find otherwise, let me know. I haven't tried it myself for a while now).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:224,testability,log,log,224,"@mclaugsf Thanks for the update! . 1) In terms of failed jobs -- we also noticed that our current recommendation of case studies tend to mask the issues if a run failed in the middle, because we currently pipe all output to log files. We're making some changes so that if anything fails in the middle, it'll be more clear to the users later. I'm still fixing a few more things, hopefully it'll come out in the next release. For now it's a good idea to just check the log files to make sure previous runs were successfully done before proceeding. 2) For call_variants, can you check your `call_variants.log` file and see what the lines look like? In my run for the WGS casestudy, it converges to something like:. ```. I0815 18:49:08.438520 140611550078720 call_variants.py:359] Processed 113665 examples in 223 batches [0.222 sec per 100]. I0815 18:49:09.491303 140611550078720 call_variants.py:359] Processed 114177 examples in 224 batches [0.222 sec per 100]. I0815 18:49:10.535501 140611550078720 call_variants.py:359] Processed 114689 examples in 225 batches [0.221 sec per 100]. ```. In our case study, we recommend just running one `call_variants` per machine. `call_variants` itself does utilize multiple CPUs now, so if you use top or htop to check your run, you should see that it uses more than one CPU. In my previous experience, running multiple `call_variants` on the same machine tends to make the run slower. If you're running call_variants separately on each shard, and if you can do each of them on different machines, that's probably most ideal. But if you plan to try running multiple `call_variants` on the same machine, you might want to watch out the speed because it will likely not be linearly faster. (If you find otherwise, let me know. I haven't tried it myself for a while now).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:467,testability,log,log,467,"@mclaugsf Thanks for the update! . 1) In terms of failed jobs -- we also noticed that our current recommendation of case studies tend to mask the issues if a run failed in the middle, because we currently pipe all output to log files. We're making some changes so that if anything fails in the middle, it'll be more clear to the users later. I'm still fixing a few more things, hopefully it'll come out in the next release. For now it's a good idea to just check the log files to make sure previous runs were successfully done before proceeding. 2) For call_variants, can you check your `call_variants.log` file and see what the lines look like? In my run for the WGS casestudy, it converges to something like:. ```. I0815 18:49:08.438520 140611550078720 call_variants.py:359] Processed 113665 examples in 223 batches [0.222 sec per 100]. I0815 18:49:09.491303 140611550078720 call_variants.py:359] Processed 114177 examples in 224 batches [0.222 sec per 100]. I0815 18:49:10.535501 140611550078720 call_variants.py:359] Processed 114689 examples in 225 batches [0.221 sec per 100]. ```. In our case study, we recommend just running one `call_variants` per machine. `call_variants` itself does utilize multiple CPUs now, so if you use top or htop to check your run, you should see that it uses more than one CPU. In my previous experience, running multiple `call_variants` on the same machine tends to make the run slower. If you're running call_variants separately on each shard, and if you can do each of them on different machines, that's probably most ideal. But if you plan to try running multiple `call_variants` on the same machine, you might want to watch out the speed because it will likely not be linearly faster. (If you find otherwise, let me know. I haven't tried it myself for a while now).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:602,testability,log,log,602,"@mclaugsf Thanks for the update! . 1) In terms of failed jobs -- we also noticed that our current recommendation of case studies tend to mask the issues if a run failed in the middle, because we currently pipe all output to log files. We're making some changes so that if anything fails in the middle, it'll be more clear to the users later. I'm still fixing a few more things, hopefully it'll come out in the next release. For now it's a good idea to just check the log files to make sure previous runs were successfully done before proceeding. 2) For call_variants, can you check your `call_variants.log` file and see what the lines look like? In my run for the WGS casestudy, it converges to something like:. ```. I0815 18:49:08.438520 140611550078720 call_variants.py:359] Processed 113665 examples in 223 batches [0.222 sec per 100]. I0815 18:49:09.491303 140611550078720 call_variants.py:359] Processed 114177 examples in 224 batches [0.222 sec per 100]. I0815 18:49:10.535501 140611550078720 call_variants.py:359] Processed 114689 examples in 225 batches [0.221 sec per 100]. ```. In our case study, we recommend just running one `call_variants` per machine. `call_variants` itself does utilize multiple CPUs now, so if you use top or htop to check your run, you should see that it uses more than one CPU. In my previous experience, running multiple `call_variants` on the same machine tends to make the run slower. If you're running call_variants separately on each shard, and if you can do each of them on different machines, that's probably most ideal. But if you plan to try running multiple `call_variants` on the same machine, you might want to watch out the speed because it will likely not be linearly faster. (If you find otherwise, let me know. I haven't tried it myself for a while now).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:1574,testability,plan,plan,1574,"@mclaugsf Thanks for the update! . 1) In terms of failed jobs -- we also noticed that our current recommendation of case studies tend to mask the issues if a run failed in the middle, because we currently pipe all output to log files. We're making some changes so that if anything fails in the middle, it'll be more clear to the users later. I'm still fixing a few more things, hopefully it'll come out in the next release. For now it's a good idea to just check the log files to make sure previous runs were successfully done before proceeding. 2) For call_variants, can you check your `call_variants.log` file and see what the lines look like? In my run for the WGS casestudy, it converges to something like:. ```. I0815 18:49:08.438520 140611550078720 call_variants.py:359] Processed 113665 examples in 223 batches [0.222 sec per 100]. I0815 18:49:09.491303 140611550078720 call_variants.py:359] Processed 114177 examples in 224 batches [0.222 sec per 100]. I0815 18:49:10.535501 140611550078720 call_variants.py:359] Processed 114689 examples in 225 batches [0.221 sec per 100]. ```. In our case study, we recommend just running one `call_variants` per machine. `call_variants` itself does utilize multiple CPUs now, so if you use top or htop to check your run, you should see that it uses more than one CPU. In my previous experience, running multiple `call_variants` on the same machine tends to make the run slower. If you're running call_variants separately on each shard, and if you can do each of them on different machines, that's probably most ideal. But if you plan to try running multiple `call_variants` on the same machine, you might want to watch out the speed because it will likely not be linearly faster. (If you find otherwise, let me know. I haven't tried it myself for a while now).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:316,usability,clear,clear,316,"@mclaugsf Thanks for the update! . 1) In terms of failed jobs -- we also noticed that our current recommendation of case studies tend to mask the issues if a run failed in the middle, because we currently pipe all output to log files. We're making some changes so that if anything fails in the middle, it'll be more clear to the users later. I'm still fixing a few more things, hopefully it'll come out in the next release. For now it's a good idea to just check the log files to make sure previous runs were successfully done before proceeding. 2) For call_variants, can you check your `call_variants.log` file and see what the lines look like? In my run for the WGS casestudy, it converges to something like:. ```. I0815 18:49:08.438520 140611550078720 call_variants.py:359] Processed 113665 examples in 223 batches [0.222 sec per 100]. I0815 18:49:09.491303 140611550078720 call_variants.py:359] Processed 114177 examples in 224 batches [0.222 sec per 100]. I0815 18:49:10.535501 140611550078720 call_variants.py:359] Processed 114689 examples in 225 batches [0.221 sec per 100]. ```. In our case study, we recommend just running one `call_variants` per machine. `call_variants` itself does utilize multiple CPUs now, so if you use top or htop to check your run, you should see that it uses more than one CPU. In my previous experience, running multiple `call_variants` on the same machine tends to make the run slower. If you're running call_variants separately on each shard, and if you can do each of them on different machines, that's probably most ideal. But if you plan to try running multiple `call_variants` on the same machine, you might want to watch out the speed because it will likely not be linearly faster. (If you find otherwise, let me know. I haven't tried it myself for a while now).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:329,usability,user,users,329,"@mclaugsf Thanks for the update! . 1) In terms of failed jobs -- we also noticed that our current recommendation of case studies tend to mask the issues if a run failed in the middle, because we currently pipe all output to log files. We're making some changes so that if anything fails in the middle, it'll be more clear to the users later. I'm still fixing a few more things, hopefully it'll come out in the next release. For now it's a good idea to just check the log files to make sure previous runs were successfully done before proceeding. 2) For call_variants, can you check your `call_variants.log` file and see what the lines look like? In my run for the WGS casestudy, it converges to something like:. ```. I0815 18:49:08.438520 140611550078720 call_variants.py:359] Processed 113665 examples in 223 batches [0.222 sec per 100]. I0815 18:49:09.491303 140611550078720 call_variants.py:359] Processed 114177 examples in 224 batches [0.222 sec per 100]. I0815 18:49:10.535501 140611550078720 call_variants.py:359] Processed 114689 examples in 225 batches [0.221 sec per 100]. ```. In our case study, we recommend just running one `call_variants` per machine. `call_variants` itself does utilize multiple CPUs now, so if you use top or htop to check your run, you should see that it uses more than one CPU. In my previous experience, running multiple `call_variants` on the same machine tends to make the run slower. If you're running call_variants separately on each shard, and if you can do each of them on different machines, that's probably most ideal. But if you plan to try running multiple `call_variants` on the same machine, you might want to watch out the speed because it will likely not be linearly faster. (If you find otherwise, let me know. I haven't tried it myself for a while now).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:1328,usability,experien,experience,1328,"@mclaugsf Thanks for the update! . 1) In terms of failed jobs -- we also noticed that our current recommendation of case studies tend to mask the issues if a run failed in the middle, because we currently pipe all output to log files. We're making some changes so that if anything fails in the middle, it'll be more clear to the users later. I'm still fixing a few more things, hopefully it'll come out in the next release. For now it's a good idea to just check the log files to make sure previous runs were successfully done before proceeding. 2) For call_variants, can you check your `call_variants.log` file and see what the lines look like? In my run for the WGS casestudy, it converges to something like:. ```. I0815 18:49:08.438520 140611550078720 call_variants.py:359] Processed 113665 examples in 223 batches [0.222 sec per 100]. I0815 18:49:09.491303 140611550078720 call_variants.py:359] Processed 114177 examples in 224 batches [0.222 sec per 100]. I0815 18:49:10.535501 140611550078720 call_variants.py:359] Processed 114689 examples in 225 batches [0.221 sec per 100]. ```. In our case study, we recommend just running one `call_variants` per machine. `call_variants` itself does utilize multiple CPUs now, so if you use top or htop to check your run, you should see that it uses more than one CPU. In my previous experience, running multiple `call_variants` on the same machine tends to make the run slower. If you're running call_variants separately on each shard, and if you can do each of them on different machines, that's probably most ideal. But if you plan to try running multiple `call_variants` on the same machine, you might want to watch out the speed because it will likely not be linearly faster. (If you find otherwise, let me know. I haven't tried it myself for a while now).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:194,availability,avail,available,194,"It seems like original question is resolved for now. If there are more questions for `call_variants`, feel free to open another issue. I think it's also possible to do with whether Intel MKL is available or not on your machine. If your speed reported in the `call_variants` step is much slower, feel free to open another issue and we can discuss there.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:287,availability,slo,slower,287,"It seems like original question is resolved for now. If there are more questions for `call_variants`, feel free to open another issue. I think it's also possible to do with whether Intel MKL is available or not on your machine. If your speed reported in the `call_variants` step is much slower, feel free to open another issue and we can discuss there.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:194,reliability,availab,available,194,"It seems like original question is resolved for now. If there are more questions for `call_variants`, feel free to open another issue. I think it's also possible to do with whether Intel MKL is available or not on your machine. If your speed reported in the `call_variants` step is much slower, feel free to open another issue and we can discuss there.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:287,reliability,slo,slower,287,"It seems like original question is resolved for now. If there are more questions for `call_variants`, feel free to open another issue. I think it's also possible to do with whether Intel MKL is available or not on your machine. If your speed reported in the `call_variants` step is much slower, feel free to open another issue and we can discuss there.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:194,safety,avail,available,194,"It seems like original question is resolved for now. If there are more questions for `call_variants`, feel free to open another issue. I think it's also possible to do with whether Intel MKL is available or not on your machine. If your speed reported in the `call_variants` step is much slower, feel free to open another issue and we can discuss there.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:194,security,availab,available,194,"It seems like original question is resolved for now. If there are more questions for `call_variants`, feel free to open another issue. I think it's also possible to do with whether Intel MKL is available or not on your machine. If your speed reported in the `call_variants` step is much slower, feel free to open another issue and we can discuss there.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:194,availability,avail,available,194,"Thanks, just to followup: my `call_variants` step eventually finished after 1d2h31m. The last lines in my call_variants.log file read:. <pre>. 2018-10-17T14:30:33.396510159Z statfs 424901734400 available 4378992640 used 429280727040 total -- interval 10.0000 seconds 0 used. 2018-10-17T14:30:33.786897949Z I1017 14:30:33.786412 140161207068416 call_variants.py:359] Processed 10551585 examples in 329738 batches [0.904 sec per 100]. 2018-10-17T14:30:34.069377504Z I1017 14:30:34.068934 140161207068416 call_variants.py:359] Processed 10551617 examples in 329739 batches [0.904 sec per 100]. 2018-10-17T14:30:34.164880374Z I1017 14:30:34.164383 140161207068416 call_variants.py:359] Processed 10551649 examples in 329740 batches [0.904 sec per 100]. 2018-10-17T14:30:34.166325693Z I1017 14:30:34.166042 140161207068416 call_variants.py:361] Done evaluating variants. </pre>.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:120,deployability,log,log,120,"Thanks, just to followup: my `call_variants` step eventually finished after 1d2h31m. The last lines in my call_variants.log file read:. <pre>. 2018-10-17T14:30:33.396510159Z statfs 424901734400 available 4378992640 used 429280727040 total -- interval 10.0000 seconds 0 used. 2018-10-17T14:30:33.786897949Z I1017 14:30:33.786412 140161207068416 call_variants.py:359] Processed 10551585 examples in 329738 batches [0.904 sec per 100]. 2018-10-17T14:30:34.069377504Z I1017 14:30:34.068934 140161207068416 call_variants.py:359] Processed 10551617 examples in 329739 batches [0.904 sec per 100]. 2018-10-17T14:30:34.164880374Z I1017 14:30:34.164383 140161207068416 call_variants.py:359] Processed 10551649 examples in 329740 batches [0.904 sec per 100]. 2018-10-17T14:30:34.166325693Z I1017 14:30:34.166042 140161207068416 call_variants.py:361] Done evaluating variants. </pre>.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:50,integrability,event,eventually,50,"Thanks, just to followup: my `call_variants` step eventually finished after 1d2h31m. The last lines in my call_variants.log file read:. <pre>. 2018-10-17T14:30:33.396510159Z statfs 424901734400 available 4378992640 used 429280727040 total -- interval 10.0000 seconds 0 used. 2018-10-17T14:30:33.786897949Z I1017 14:30:33.786412 140161207068416 call_variants.py:359] Processed 10551585 examples in 329738 batches [0.904 sec per 100]. 2018-10-17T14:30:34.069377504Z I1017 14:30:34.068934 140161207068416 call_variants.py:359] Processed 10551617 examples in 329739 batches [0.904 sec per 100]. 2018-10-17T14:30:34.164880374Z I1017 14:30:34.164383 140161207068416 call_variants.py:359] Processed 10551649 examples in 329740 batches [0.904 sec per 100]. 2018-10-17T14:30:34.166325693Z I1017 14:30:34.166042 140161207068416 call_variants.py:361] Done evaluating variants. </pre>.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:404,integrability,batch,batches,404,"Thanks, just to followup: my `call_variants` step eventually finished after 1d2h31m. The last lines in my call_variants.log file read:. <pre>. 2018-10-17T14:30:33.396510159Z statfs 424901734400 available 4378992640 used 429280727040 total -- interval 10.0000 seconds 0 used. 2018-10-17T14:30:33.786897949Z I1017 14:30:33.786412 140161207068416 call_variants.py:359] Processed 10551585 examples in 329738 batches [0.904 sec per 100]. 2018-10-17T14:30:34.069377504Z I1017 14:30:34.068934 140161207068416 call_variants.py:359] Processed 10551617 examples in 329739 batches [0.904 sec per 100]. 2018-10-17T14:30:34.164880374Z I1017 14:30:34.164383 140161207068416 call_variants.py:359] Processed 10551649 examples in 329740 batches [0.904 sec per 100]. 2018-10-17T14:30:34.166325693Z I1017 14:30:34.166042 140161207068416 call_variants.py:361] Done evaluating variants. </pre>.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:562,integrability,batch,batches,562,"Thanks, just to followup: my `call_variants` step eventually finished after 1d2h31m. The last lines in my call_variants.log file read:. <pre>. 2018-10-17T14:30:33.396510159Z statfs 424901734400 available 4378992640 used 429280727040 total -- interval 10.0000 seconds 0 used. 2018-10-17T14:30:33.786897949Z I1017 14:30:33.786412 140161207068416 call_variants.py:359] Processed 10551585 examples in 329738 batches [0.904 sec per 100]. 2018-10-17T14:30:34.069377504Z I1017 14:30:34.068934 140161207068416 call_variants.py:359] Processed 10551617 examples in 329739 batches [0.904 sec per 100]. 2018-10-17T14:30:34.164880374Z I1017 14:30:34.164383 140161207068416 call_variants.py:359] Processed 10551649 examples in 329740 batches [0.904 sec per 100]. 2018-10-17T14:30:34.166325693Z I1017 14:30:34.166042 140161207068416 call_variants.py:361] Done evaluating variants. </pre>.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:720,integrability,batch,batches,720,"Thanks, just to followup: my `call_variants` step eventually finished after 1d2h31m. The last lines in my call_variants.log file read:. <pre>. 2018-10-17T14:30:33.396510159Z statfs 424901734400 available 4378992640 used 429280727040 total -- interval 10.0000 seconds 0 used. 2018-10-17T14:30:33.786897949Z I1017 14:30:33.786412 140161207068416 call_variants.py:359] Processed 10551585 examples in 329738 batches [0.904 sec per 100]. 2018-10-17T14:30:34.069377504Z I1017 14:30:34.068934 140161207068416 call_variants.py:359] Processed 10551617 examples in 329739 batches [0.904 sec per 100]. 2018-10-17T14:30:34.164880374Z I1017 14:30:34.164383 140161207068416 call_variants.py:359] Processed 10551649 examples in 329740 batches [0.904 sec per 100]. 2018-10-17T14:30:34.166325693Z I1017 14:30:34.166042 140161207068416 call_variants.py:361] Done evaluating variants. </pre>.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:404,performance,batch,batches,404,"Thanks, just to followup: my `call_variants` step eventually finished after 1d2h31m. The last lines in my call_variants.log file read:. <pre>. 2018-10-17T14:30:33.396510159Z statfs 424901734400 available 4378992640 used 429280727040 total -- interval 10.0000 seconds 0 used. 2018-10-17T14:30:33.786897949Z I1017 14:30:33.786412 140161207068416 call_variants.py:359] Processed 10551585 examples in 329738 batches [0.904 sec per 100]. 2018-10-17T14:30:34.069377504Z I1017 14:30:34.068934 140161207068416 call_variants.py:359] Processed 10551617 examples in 329739 batches [0.904 sec per 100]. 2018-10-17T14:30:34.164880374Z I1017 14:30:34.164383 140161207068416 call_variants.py:359] Processed 10551649 examples in 329740 batches [0.904 sec per 100]. 2018-10-17T14:30:34.166325693Z I1017 14:30:34.166042 140161207068416 call_variants.py:361] Done evaluating variants. </pre>.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:562,performance,batch,batches,562,"Thanks, just to followup: my `call_variants` step eventually finished after 1d2h31m. The last lines in my call_variants.log file read:. <pre>. 2018-10-17T14:30:33.396510159Z statfs 424901734400 available 4378992640 used 429280727040 total -- interval 10.0000 seconds 0 used. 2018-10-17T14:30:33.786897949Z I1017 14:30:33.786412 140161207068416 call_variants.py:359] Processed 10551585 examples in 329738 batches [0.904 sec per 100]. 2018-10-17T14:30:34.069377504Z I1017 14:30:34.068934 140161207068416 call_variants.py:359] Processed 10551617 examples in 329739 batches [0.904 sec per 100]. 2018-10-17T14:30:34.164880374Z I1017 14:30:34.164383 140161207068416 call_variants.py:359] Processed 10551649 examples in 329740 batches [0.904 sec per 100]. 2018-10-17T14:30:34.166325693Z I1017 14:30:34.166042 140161207068416 call_variants.py:361] Done evaluating variants. </pre>.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:720,performance,batch,batches,720,"Thanks, just to followup: my `call_variants` step eventually finished after 1d2h31m. The last lines in my call_variants.log file read:. <pre>. 2018-10-17T14:30:33.396510159Z statfs 424901734400 available 4378992640 used 429280727040 total -- interval 10.0000 seconds 0 used. 2018-10-17T14:30:33.786897949Z I1017 14:30:33.786412 140161207068416 call_variants.py:359] Processed 10551585 examples in 329738 batches [0.904 sec per 100]. 2018-10-17T14:30:34.069377504Z I1017 14:30:34.068934 140161207068416 call_variants.py:359] Processed 10551617 examples in 329739 batches [0.904 sec per 100]. 2018-10-17T14:30:34.164880374Z I1017 14:30:34.164383 140161207068416 call_variants.py:359] Processed 10551649 examples in 329740 batches [0.904 sec per 100]. 2018-10-17T14:30:34.166325693Z I1017 14:30:34.166042 140161207068416 call_variants.py:361] Done evaluating variants. </pre>.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:194,reliability,availab,available,194,"Thanks, just to followup: my `call_variants` step eventually finished after 1d2h31m. The last lines in my call_variants.log file read:. <pre>. 2018-10-17T14:30:33.396510159Z statfs 424901734400 available 4378992640 used 429280727040 total -- interval 10.0000 seconds 0 used. 2018-10-17T14:30:33.786897949Z I1017 14:30:33.786412 140161207068416 call_variants.py:359] Processed 10551585 examples in 329738 batches [0.904 sec per 100]. 2018-10-17T14:30:34.069377504Z I1017 14:30:34.068934 140161207068416 call_variants.py:359] Processed 10551617 examples in 329739 batches [0.904 sec per 100]. 2018-10-17T14:30:34.164880374Z I1017 14:30:34.164383 140161207068416 call_variants.py:359] Processed 10551649 examples in 329740 batches [0.904 sec per 100]. 2018-10-17T14:30:34.166325693Z I1017 14:30:34.166042 140161207068416 call_variants.py:361] Done evaluating variants. </pre>.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:120,safety,log,log,120,"Thanks, just to followup: my `call_variants` step eventually finished after 1d2h31m. The last lines in my call_variants.log file read:. <pre>. 2018-10-17T14:30:33.396510159Z statfs 424901734400 available 4378992640 used 429280727040 total -- interval 10.0000 seconds 0 used. 2018-10-17T14:30:33.786897949Z I1017 14:30:33.786412 140161207068416 call_variants.py:359] Processed 10551585 examples in 329738 batches [0.904 sec per 100]. 2018-10-17T14:30:34.069377504Z I1017 14:30:34.068934 140161207068416 call_variants.py:359] Processed 10551617 examples in 329739 batches [0.904 sec per 100]. 2018-10-17T14:30:34.164880374Z I1017 14:30:34.164383 140161207068416 call_variants.py:359] Processed 10551649 examples in 329740 batches [0.904 sec per 100]. 2018-10-17T14:30:34.166325693Z I1017 14:30:34.166042 140161207068416 call_variants.py:361] Done evaluating variants. </pre>.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:194,safety,avail,available,194,"Thanks, just to followup: my `call_variants` step eventually finished after 1d2h31m. The last lines in my call_variants.log file read:. <pre>. 2018-10-17T14:30:33.396510159Z statfs 424901734400 available 4378992640 used 429280727040 total -- interval 10.0000 seconds 0 used. 2018-10-17T14:30:33.786897949Z I1017 14:30:33.786412 140161207068416 call_variants.py:359] Processed 10551585 examples in 329738 batches [0.904 sec per 100]. 2018-10-17T14:30:34.069377504Z I1017 14:30:34.068934 140161207068416 call_variants.py:359] Processed 10551617 examples in 329739 batches [0.904 sec per 100]. 2018-10-17T14:30:34.164880374Z I1017 14:30:34.164383 140161207068416 call_variants.py:359] Processed 10551649 examples in 329740 batches [0.904 sec per 100]. 2018-10-17T14:30:34.166325693Z I1017 14:30:34.166042 140161207068416 call_variants.py:361] Done evaluating variants. </pre>.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:120,security,log,log,120,"Thanks, just to followup: my `call_variants` step eventually finished after 1d2h31m. The last lines in my call_variants.log file read:. <pre>. 2018-10-17T14:30:33.396510159Z statfs 424901734400 available 4378992640 used 429280727040 total -- interval 10.0000 seconds 0 used. 2018-10-17T14:30:33.786897949Z I1017 14:30:33.786412 140161207068416 call_variants.py:359] Processed 10551585 examples in 329738 batches [0.904 sec per 100]. 2018-10-17T14:30:34.069377504Z I1017 14:30:34.068934 140161207068416 call_variants.py:359] Processed 10551617 examples in 329739 batches [0.904 sec per 100]. 2018-10-17T14:30:34.164880374Z I1017 14:30:34.164383 140161207068416 call_variants.py:359] Processed 10551649 examples in 329740 batches [0.904 sec per 100]. 2018-10-17T14:30:34.166325693Z I1017 14:30:34.166042 140161207068416 call_variants.py:361] Done evaluating variants. </pre>.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:194,security,availab,available,194,"Thanks, just to followup: my `call_variants` step eventually finished after 1d2h31m. The last lines in my call_variants.log file read:. <pre>. 2018-10-17T14:30:33.396510159Z statfs 424901734400 available 4378992640 used 429280727040 total -- interval 10.0000 seconds 0 used. 2018-10-17T14:30:33.786897949Z I1017 14:30:33.786412 140161207068416 call_variants.py:359] Processed 10551585 examples in 329738 batches [0.904 sec per 100]. 2018-10-17T14:30:34.069377504Z I1017 14:30:34.068934 140161207068416 call_variants.py:359] Processed 10551617 examples in 329739 batches [0.904 sec per 100]. 2018-10-17T14:30:34.164880374Z I1017 14:30:34.164383 140161207068416 call_variants.py:359] Processed 10551649 examples in 329740 batches [0.904 sec per 100]. 2018-10-17T14:30:34.166325693Z I1017 14:30:34.166042 140161207068416 call_variants.py:361] Done evaluating variants. </pre>.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:120,testability,log,log,120,"Thanks, just to followup: my `call_variants` step eventually finished after 1d2h31m. The last lines in my call_variants.log file read:. <pre>. 2018-10-17T14:30:33.396510159Z statfs 424901734400 available 4378992640 used 429280727040 total -- interval 10.0000 seconds 0 used. 2018-10-17T14:30:33.786897949Z I1017 14:30:33.786412 140161207068416 call_variants.py:359] Processed 10551585 examples in 329738 batches [0.904 sec per 100]. 2018-10-17T14:30:34.069377504Z I1017 14:30:34.068934 140161207068416 call_variants.py:359] Processed 10551617 examples in 329739 batches [0.904 sec per 100]. 2018-10-17T14:30:34.164880374Z I1017 14:30:34.164383 140161207068416 call_variants.py:359] Processed 10551649 examples in 329740 batches [0.904 sec per 100]. 2018-10-17T14:30:34.166325693Z I1017 14:30:34.166042 140161207068416 call_variants.py:361] Done evaluating variants. </pre>.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:416,availability,avail,available,416,"Hi @mclaugsf,. Let me give you a bit more context here on the runtime of call_variants. call_variants is the deep learning component of DeepVariant, so it relies on TensorFlow to execute the inception_v3 model used to evaluate our genotype likelihoods. In the 0.7 case study, make_examples creates 5,847,041 genomic tensors that need to be evaluated. When executing using CPUs, TensorFlow by default uses all of the available cores on the machine. So in our case study, which runs on a 64 core machine, we are using all 64 cores to evaluate these tensors. . So a rough estimate of the core-hours needed for the DeepVariant WGS case is:. 64 cores * 205 minutes of runtime ~= 219 core hours ~= 9 days. So if you are running on a machine with a single core, you should see call_variants take ~9 days. This is a bit of an over-estimate because 64 cores isn't 64x more efficient than 1 core. . Based on your 1 day turn around I'd guess you are running on a machine with 8 cores. Note these numbers assume you are using a modern CPU with AVX etc instruction sets. Not having those can increase the runtime by ~4x or so. Also I want to ask - in your original post are you processing exomes? If so, are you providing a capture regions bed to make_examples? Normally an exome produces < 100k examples (contrast that with 5.8M in a whole genome) so the runtime should be 60x less on an exome. That means instead of 9 days on a single core you are looking at 3.5 hours.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:204,energy efficiency,model,model,204,"Hi @mclaugsf,. Let me give you a bit more context here on the runtime of call_variants. call_variants is the deep learning component of DeepVariant, so it relies on TensorFlow to execute the inception_v3 model used to evaluate our genotype likelihoods. In the 0.7 case study, make_examples creates 5,847,041 genomic tensors that need to be evaluated. When executing using CPUs, TensorFlow by default uses all of the available cores on the machine. So in our case study, which runs on a 64 core machine, we are using all 64 cores to evaluate these tensors. . So a rough estimate of the core-hours needed for the DeepVariant WGS case is:. 64 cores * 205 minutes of runtime ~= 219 core hours ~= 9 days. So if you are running on a machine with a single core, you should see call_variants take ~9 days. This is a bit of an over-estimate because 64 cores isn't 64x more efficient than 1 core. . Based on your 1 day turn around I'd guess you are running on a machine with 8 cores. Note these numbers assume you are using a modern CPU with AVX etc instruction sets. Not having those can increase the runtime by ~4x or so. Also I want to ask - in your original post are you processing exomes? If so, are you providing a capture regions bed to make_examples? Normally an exome produces < 100k examples (contrast that with 5.8M in a whole genome) so the runtime should be 60x less on an exome. That means instead of 9 days on a single core you are looking at 3.5 hours.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:372,energy efficiency,CPU,CPUs,372,"Hi @mclaugsf,. Let me give you a bit more context here on the runtime of call_variants. call_variants is the deep learning component of DeepVariant, so it relies on TensorFlow to execute the inception_v3 model used to evaluate our genotype likelihoods. In the 0.7 case study, make_examples creates 5,847,041 genomic tensors that need to be evaluated. When executing using CPUs, TensorFlow by default uses all of the available cores on the machine. So in our case study, which runs on a 64 core machine, we are using all 64 cores to evaluate these tensors. . So a rough estimate of the core-hours needed for the DeepVariant WGS case is:. 64 cores * 205 minutes of runtime ~= 219 core hours ~= 9 days. So if you are running on a machine with a single core, you should see call_variants take ~9 days. This is a bit of an over-estimate because 64 cores isn't 64x more efficient than 1 core. . Based on your 1 day turn around I'd guess you are running on a machine with 8 cores. Note these numbers assume you are using a modern CPU with AVX etc instruction sets. Not having those can increase the runtime by ~4x or so. Also I want to ask - in your original post are you processing exomes? If so, are you providing a capture regions bed to make_examples? Normally an exome produces < 100k examples (contrast that with 5.8M in a whole genome) so the runtime should be 60x less on an exome. That means instead of 9 days on a single core you are looking at 3.5 hours.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:426,energy efficiency,core,cores,426,"Hi @mclaugsf,. Let me give you a bit more context here on the runtime of call_variants. call_variants is the deep learning component of DeepVariant, so it relies on TensorFlow to execute the inception_v3 model used to evaluate our genotype likelihoods. In the 0.7 case study, make_examples creates 5,847,041 genomic tensors that need to be evaluated. When executing using CPUs, TensorFlow by default uses all of the available cores on the machine. So in our case study, which runs on a 64 core machine, we are using all 64 cores to evaluate these tensors. . So a rough estimate of the core-hours needed for the DeepVariant WGS case is:. 64 cores * 205 minutes of runtime ~= 219 core hours ~= 9 days. So if you are running on a machine with a single core, you should see call_variants take ~9 days. This is a bit of an over-estimate because 64 cores isn't 64x more efficient than 1 core. . Based on your 1 day turn around I'd guess you are running on a machine with 8 cores. Note these numbers assume you are using a modern CPU with AVX etc instruction sets. Not having those can increase the runtime by ~4x or so. Also I want to ask - in your original post are you processing exomes? If so, are you providing a capture regions bed to make_examples? Normally an exome produces < 100k examples (contrast that with 5.8M in a whole genome) so the runtime should be 60x less on an exome. That means instead of 9 days on a single core you are looking at 3.5 hours.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:489,energy efficiency,core,core,489,"Hi @mclaugsf,. Let me give you a bit more context here on the runtime of call_variants. call_variants is the deep learning component of DeepVariant, so it relies on TensorFlow to execute the inception_v3 model used to evaluate our genotype likelihoods. In the 0.7 case study, make_examples creates 5,847,041 genomic tensors that need to be evaluated. When executing using CPUs, TensorFlow by default uses all of the available cores on the machine. So in our case study, which runs on a 64 core machine, we are using all 64 cores to evaluate these tensors. . So a rough estimate of the core-hours needed for the DeepVariant WGS case is:. 64 cores * 205 minutes of runtime ~= 219 core hours ~= 9 days. So if you are running on a machine with a single core, you should see call_variants take ~9 days. This is a bit of an over-estimate because 64 cores isn't 64x more efficient than 1 core. . Based on your 1 day turn around I'd guess you are running on a machine with 8 cores. Note these numbers assume you are using a modern CPU with AVX etc instruction sets. Not having those can increase the runtime by ~4x or so. Also I want to ask - in your original post are you processing exomes? If so, are you providing a capture regions bed to make_examples? Normally an exome produces < 100k examples (contrast that with 5.8M in a whole genome) so the runtime should be 60x less on an exome. That means instead of 9 days on a single core you are looking at 3.5 hours.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:523,energy efficiency,core,cores,523,"Hi @mclaugsf,. Let me give you a bit more context here on the runtime of call_variants. call_variants is the deep learning component of DeepVariant, so it relies on TensorFlow to execute the inception_v3 model used to evaluate our genotype likelihoods. In the 0.7 case study, make_examples creates 5,847,041 genomic tensors that need to be evaluated. When executing using CPUs, TensorFlow by default uses all of the available cores on the machine. So in our case study, which runs on a 64 core machine, we are using all 64 cores to evaluate these tensors. . So a rough estimate of the core-hours needed for the DeepVariant WGS case is:. 64 cores * 205 minutes of runtime ~= 219 core hours ~= 9 days. So if you are running on a machine with a single core, you should see call_variants take ~9 days. This is a bit of an over-estimate because 64 cores isn't 64x more efficient than 1 core. . Based on your 1 day turn around I'd guess you are running on a machine with 8 cores. Note these numbers assume you are using a modern CPU with AVX etc instruction sets. Not having those can increase the runtime by ~4x or so. Also I want to ask - in your original post are you processing exomes? If so, are you providing a capture regions bed to make_examples? Normally an exome produces < 100k examples (contrast that with 5.8M in a whole genome) so the runtime should be 60x less on an exome. That means instead of 9 days on a single core you are looking at 3.5 hours.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:569,energy efficiency,estimat,estimate,569,"Hi @mclaugsf,. Let me give you a bit more context here on the runtime of call_variants. call_variants is the deep learning component of DeepVariant, so it relies on TensorFlow to execute the inception_v3 model used to evaluate our genotype likelihoods. In the 0.7 case study, make_examples creates 5,847,041 genomic tensors that need to be evaluated. When executing using CPUs, TensorFlow by default uses all of the available cores on the machine. So in our case study, which runs on a 64 core machine, we are using all 64 cores to evaluate these tensors. . So a rough estimate of the core-hours needed for the DeepVariant WGS case is:. 64 cores * 205 minutes of runtime ~= 219 core hours ~= 9 days. So if you are running on a machine with a single core, you should see call_variants take ~9 days. This is a bit of an over-estimate because 64 cores isn't 64x more efficient than 1 core. . Based on your 1 day turn around I'd guess you are running on a machine with 8 cores. Note these numbers assume you are using a modern CPU with AVX etc instruction sets. Not having those can increase the runtime by ~4x or so. Also I want to ask - in your original post are you processing exomes? If so, are you providing a capture regions bed to make_examples? Normally an exome produces < 100k examples (contrast that with 5.8M in a whole genome) so the runtime should be 60x less on an exome. That means instead of 9 days on a single core you are looking at 3.5 hours.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:585,energy efficiency,core,core-hours,585,"Hi @mclaugsf,. Let me give you a bit more context here on the runtime of call_variants. call_variants is the deep learning component of DeepVariant, so it relies on TensorFlow to execute the inception_v3 model used to evaluate our genotype likelihoods. In the 0.7 case study, make_examples creates 5,847,041 genomic tensors that need to be evaluated. When executing using CPUs, TensorFlow by default uses all of the available cores on the machine. So in our case study, which runs on a 64 core machine, we are using all 64 cores to evaluate these tensors. . So a rough estimate of the core-hours needed for the DeepVariant WGS case is:. 64 cores * 205 minutes of runtime ~= 219 core hours ~= 9 days. So if you are running on a machine with a single core, you should see call_variants take ~9 days. This is a bit of an over-estimate because 64 cores isn't 64x more efficient than 1 core. . Based on your 1 day turn around I'd guess you are running on a machine with 8 cores. Note these numbers assume you are using a modern CPU with AVX etc instruction sets. Not having those can increase the runtime by ~4x or so. Also I want to ask - in your original post are you processing exomes? If so, are you providing a capture regions bed to make_examples? Normally an exome produces < 100k examples (contrast that with 5.8M in a whole genome) so the runtime should be 60x less on an exome. That means instead of 9 days on a single core you are looking at 3.5 hours.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:640,energy efficiency,core,cores,640,"Hi @mclaugsf,. Let me give you a bit more context here on the runtime of call_variants. call_variants is the deep learning component of DeepVariant, so it relies on TensorFlow to execute the inception_v3 model used to evaluate our genotype likelihoods. In the 0.7 case study, make_examples creates 5,847,041 genomic tensors that need to be evaluated. When executing using CPUs, TensorFlow by default uses all of the available cores on the machine. So in our case study, which runs on a 64 core machine, we are using all 64 cores to evaluate these tensors. . So a rough estimate of the core-hours needed for the DeepVariant WGS case is:. 64 cores * 205 minutes of runtime ~= 219 core hours ~= 9 days. So if you are running on a machine with a single core, you should see call_variants take ~9 days. This is a bit of an over-estimate because 64 cores isn't 64x more efficient than 1 core. . Based on your 1 day turn around I'd guess you are running on a machine with 8 cores. Note these numbers assume you are using a modern CPU with AVX etc instruction sets. Not having those can increase the runtime by ~4x or so. Also I want to ask - in your original post are you processing exomes? If so, are you providing a capture regions bed to make_examples? Normally an exome produces < 100k examples (contrast that with 5.8M in a whole genome) so the runtime should be 60x less on an exome. That means instead of 9 days on a single core you are looking at 3.5 hours.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:678,energy efficiency,core,core,678,"Hi @mclaugsf,. Let me give you a bit more context here on the runtime of call_variants. call_variants is the deep learning component of DeepVariant, so it relies on TensorFlow to execute the inception_v3 model used to evaluate our genotype likelihoods. In the 0.7 case study, make_examples creates 5,847,041 genomic tensors that need to be evaluated. When executing using CPUs, TensorFlow by default uses all of the available cores on the machine. So in our case study, which runs on a 64 core machine, we are using all 64 cores to evaluate these tensors. . So a rough estimate of the core-hours needed for the DeepVariant WGS case is:. 64 cores * 205 minutes of runtime ~= 219 core hours ~= 9 days. So if you are running on a machine with a single core, you should see call_variants take ~9 days. This is a bit of an over-estimate because 64 cores isn't 64x more efficient than 1 core. . Based on your 1 day turn around I'd guess you are running on a machine with 8 cores. Note these numbers assume you are using a modern CPU with AVX etc instruction sets. Not having those can increase the runtime by ~4x or so. Also I want to ask - in your original post are you processing exomes? If so, are you providing a capture regions bed to make_examples? Normally an exome produces < 100k examples (contrast that with 5.8M in a whole genome) so the runtime should be 60x less on an exome. That means instead of 9 days on a single core you are looking at 3.5 hours.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:749,energy efficiency,core,core,749,"Hi @mclaugsf,. Let me give you a bit more context here on the runtime of call_variants. call_variants is the deep learning component of DeepVariant, so it relies on TensorFlow to execute the inception_v3 model used to evaluate our genotype likelihoods. In the 0.7 case study, make_examples creates 5,847,041 genomic tensors that need to be evaluated. When executing using CPUs, TensorFlow by default uses all of the available cores on the machine. So in our case study, which runs on a 64 core machine, we are using all 64 cores to evaluate these tensors. . So a rough estimate of the core-hours needed for the DeepVariant WGS case is:. 64 cores * 205 minutes of runtime ~= 219 core hours ~= 9 days. So if you are running on a machine with a single core, you should see call_variants take ~9 days. This is a bit of an over-estimate because 64 cores isn't 64x more efficient than 1 core. . Based on your 1 day turn around I'd guess you are running on a machine with 8 cores. Note these numbers assume you are using a modern CPU with AVX etc instruction sets. Not having those can increase the runtime by ~4x or so. Also I want to ask - in your original post are you processing exomes? If so, are you providing a capture regions bed to make_examples? Normally an exome produces < 100k examples (contrast that with 5.8M in a whole genome) so the runtime should be 60x less on an exome. That means instead of 9 days on a single core you are looking at 3.5 hours.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:823,energy efficiency,estimat,estimate,823,"Hi @mclaugsf,. Let me give you a bit more context here on the runtime of call_variants. call_variants is the deep learning component of DeepVariant, so it relies on TensorFlow to execute the inception_v3 model used to evaluate our genotype likelihoods. In the 0.7 case study, make_examples creates 5,847,041 genomic tensors that need to be evaluated. When executing using CPUs, TensorFlow by default uses all of the available cores on the machine. So in our case study, which runs on a 64 core machine, we are using all 64 cores to evaluate these tensors. . So a rough estimate of the core-hours needed for the DeepVariant WGS case is:. 64 cores * 205 minutes of runtime ~= 219 core hours ~= 9 days. So if you are running on a machine with a single core, you should see call_variants take ~9 days. This is a bit of an over-estimate because 64 cores isn't 64x more efficient than 1 core. . Based on your 1 day turn around I'd guess you are running on a machine with 8 cores. Note these numbers assume you are using a modern CPU with AVX etc instruction sets. Not having those can increase the runtime by ~4x or so. Also I want to ask - in your original post are you processing exomes? If so, are you providing a capture regions bed to make_examples? Normally an exome produces < 100k examples (contrast that with 5.8M in a whole genome) so the runtime should be 60x less on an exome. That means instead of 9 days on a single core you are looking at 3.5 hours.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:843,energy efficiency,core,cores,843,"Hi @mclaugsf,. Let me give you a bit more context here on the runtime of call_variants. call_variants is the deep learning component of DeepVariant, so it relies on TensorFlow to execute the inception_v3 model used to evaluate our genotype likelihoods. In the 0.7 case study, make_examples creates 5,847,041 genomic tensors that need to be evaluated. When executing using CPUs, TensorFlow by default uses all of the available cores on the machine. So in our case study, which runs on a 64 core machine, we are using all 64 cores to evaluate these tensors. . So a rough estimate of the core-hours needed for the DeepVariant WGS case is:. 64 cores * 205 minutes of runtime ~= 219 core hours ~= 9 days. So if you are running on a machine with a single core, you should see call_variants take ~9 days. This is a bit of an over-estimate because 64 cores isn't 64x more efficient than 1 core. . Based on your 1 day turn around I'd guess you are running on a machine with 8 cores. Note these numbers assume you are using a modern CPU with AVX etc instruction sets. Not having those can increase the runtime by ~4x or so. Also I want to ask - in your original post are you processing exomes? If so, are you providing a capture regions bed to make_examples? Normally an exome produces < 100k examples (contrast that with 5.8M in a whole genome) so the runtime should be 60x less on an exome. That means instead of 9 days on a single core you are looking at 3.5 hours.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:881,energy efficiency,core,core,881,"Hi @mclaugsf,. Let me give you a bit more context here on the runtime of call_variants. call_variants is the deep learning component of DeepVariant, so it relies on TensorFlow to execute the inception_v3 model used to evaluate our genotype likelihoods. In the 0.7 case study, make_examples creates 5,847,041 genomic tensors that need to be evaluated. When executing using CPUs, TensorFlow by default uses all of the available cores on the machine. So in our case study, which runs on a 64 core machine, we are using all 64 cores to evaluate these tensors. . So a rough estimate of the core-hours needed for the DeepVariant WGS case is:. 64 cores * 205 minutes of runtime ~= 219 core hours ~= 9 days. So if you are running on a machine with a single core, you should see call_variants take ~9 days. This is a bit of an over-estimate because 64 cores isn't 64x more efficient than 1 core. . Based on your 1 day turn around I'd guess you are running on a machine with 8 cores. Note these numbers assume you are using a modern CPU with AVX etc instruction sets. Not having those can increase the runtime by ~4x or so. Also I want to ask - in your original post are you processing exomes? If so, are you providing a capture regions bed to make_examples? Normally an exome produces < 100k examples (contrast that with 5.8M in a whole genome) so the runtime should be 60x less on an exome. That means instead of 9 days on a single core you are looking at 3.5 hours.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:967,energy efficiency,core,cores,967,"Hi @mclaugsf,. Let me give you a bit more context here on the runtime of call_variants. call_variants is the deep learning component of DeepVariant, so it relies on TensorFlow to execute the inception_v3 model used to evaluate our genotype likelihoods. In the 0.7 case study, make_examples creates 5,847,041 genomic tensors that need to be evaluated. When executing using CPUs, TensorFlow by default uses all of the available cores on the machine. So in our case study, which runs on a 64 core machine, we are using all 64 cores to evaluate these tensors. . So a rough estimate of the core-hours needed for the DeepVariant WGS case is:. 64 cores * 205 minutes of runtime ~= 219 core hours ~= 9 days. So if you are running on a machine with a single core, you should see call_variants take ~9 days. This is a bit of an over-estimate because 64 cores isn't 64x more efficient than 1 core. . Based on your 1 day turn around I'd guess you are running on a machine with 8 cores. Note these numbers assume you are using a modern CPU with AVX etc instruction sets. Not having those can increase the runtime by ~4x or so. Also I want to ask - in your original post are you processing exomes? If so, are you providing a capture regions bed to make_examples? Normally an exome produces < 100k examples (contrast that with 5.8M in a whole genome) so the runtime should be 60x less on an exome. That means instead of 9 days on a single core you are looking at 3.5 hours.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:1023,energy efficiency,CPU,CPU,1023,"Hi @mclaugsf,. Let me give you a bit more context here on the runtime of call_variants. call_variants is the deep learning component of DeepVariant, so it relies on TensorFlow to execute the inception_v3 model used to evaluate our genotype likelihoods. In the 0.7 case study, make_examples creates 5,847,041 genomic tensors that need to be evaluated. When executing using CPUs, TensorFlow by default uses all of the available cores on the machine. So in our case study, which runs on a 64 core machine, we are using all 64 cores to evaluate these tensors. . So a rough estimate of the core-hours needed for the DeepVariant WGS case is:. 64 cores * 205 minutes of runtime ~= 219 core hours ~= 9 days. So if you are running on a machine with a single core, you should see call_variants take ~9 days. This is a bit of an over-estimate because 64 cores isn't 64x more efficient than 1 core. . Based on your 1 day turn around I'd guess you are running on a machine with 8 cores. Note these numbers assume you are using a modern CPU with AVX etc instruction sets. Not having those can increase the runtime by ~4x or so. Also I want to ask - in your original post are you processing exomes? If so, are you providing a capture regions bed to make_examples? Normally an exome produces < 100k examples (contrast that with 5.8M in a whole genome) so the runtime should be 60x less on an exome. That means instead of 9 days on a single core you are looking at 3.5 hours.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:1424,energy efficiency,core,core,1424,"Hi @mclaugsf,. Let me give you a bit more context here on the runtime of call_variants. call_variants is the deep learning component of DeepVariant, so it relies on TensorFlow to execute the inception_v3 model used to evaluate our genotype likelihoods. In the 0.7 case study, make_examples creates 5,847,041 genomic tensors that need to be evaluated. When executing using CPUs, TensorFlow by default uses all of the available cores on the machine. So in our case study, which runs on a 64 core machine, we are using all 64 cores to evaluate these tensors. . So a rough estimate of the core-hours needed for the DeepVariant WGS case is:. 64 cores * 205 minutes of runtime ~= 219 core hours ~= 9 days. So if you are running on a machine with a single core, you should see call_variants take ~9 days. This is a bit of an over-estimate because 64 cores isn't 64x more efficient than 1 core. . Based on your 1 day turn around I'd guess you are running on a machine with 8 cores. Note these numbers assume you are using a modern CPU with AVX etc instruction sets. Not having those can increase the runtime by ~4x or so. Also I want to ask - in your original post are you processing exomes? If so, are you providing a capture regions bed to make_examples? Normally an exome produces < 100k examples (contrast that with 5.8M in a whole genome) so the runtime should be 60x less on an exome. That means instead of 9 days on a single core you are looking at 3.5 hours.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:123,integrability,compon,component,123,"Hi @mclaugsf,. Let me give you a bit more context here on the runtime of call_variants. call_variants is the deep learning component of DeepVariant, so it relies on TensorFlow to execute the inception_v3 model used to evaluate our genotype likelihoods. In the 0.7 case study, make_examples creates 5,847,041 genomic tensors that need to be evaluated. When executing using CPUs, TensorFlow by default uses all of the available cores on the machine. So in our case study, which runs on a 64 core machine, we are using all 64 cores to evaluate these tensors. . So a rough estimate of the core-hours needed for the DeepVariant WGS case is:. 64 cores * 205 minutes of runtime ~= 219 core hours ~= 9 days. So if you are running on a machine with a single core, you should see call_variants take ~9 days. This is a bit of an over-estimate because 64 cores isn't 64x more efficient than 1 core. . Based on your 1 day turn around I'd guess you are running on a machine with 8 cores. Note these numbers assume you are using a modern CPU with AVX etc instruction sets. Not having those can increase the runtime by ~4x or so. Also I want to ask - in your original post are you processing exomes? If so, are you providing a capture regions bed to make_examples? Normally an exome produces < 100k examples (contrast that with 5.8M in a whole genome) so the runtime should be 60x less on an exome. That means instead of 9 days on a single core you are looking at 3.5 hours.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:123,interoperability,compon,component,123,"Hi @mclaugsf,. Let me give you a bit more context here on the runtime of call_variants. call_variants is the deep learning component of DeepVariant, so it relies on TensorFlow to execute the inception_v3 model used to evaluate our genotype likelihoods. In the 0.7 case study, make_examples creates 5,847,041 genomic tensors that need to be evaluated. When executing using CPUs, TensorFlow by default uses all of the available cores on the machine. So in our case study, which runs on a 64 core machine, we are using all 64 cores to evaluate these tensors. . So a rough estimate of the core-hours needed for the DeepVariant WGS case is:. 64 cores * 205 minutes of runtime ~= 219 core hours ~= 9 days. So if you are running on a machine with a single core, you should see call_variants take ~9 days. This is a bit of an over-estimate because 64 cores isn't 64x more efficient than 1 core. . Based on your 1 day turn around I'd guess you are running on a machine with 8 cores. Note these numbers assume you are using a modern CPU with AVX etc instruction sets. Not having those can increase the runtime by ~4x or so. Also I want to ask - in your original post are you processing exomes? If so, are you providing a capture regions bed to make_examples? Normally an exome produces < 100k examples (contrast that with 5.8M in a whole genome) so the runtime should be 60x less on an exome. That means instead of 9 days on a single core you are looking at 3.5 hours.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:123,modifiability,compon,component,123,"Hi @mclaugsf,. Let me give you a bit more context here on the runtime of call_variants. call_variants is the deep learning component of DeepVariant, so it relies on TensorFlow to execute the inception_v3 model used to evaluate our genotype likelihoods. In the 0.7 case study, make_examples creates 5,847,041 genomic tensors that need to be evaluated. When executing using CPUs, TensorFlow by default uses all of the available cores on the machine. So in our case study, which runs on a 64 core machine, we are using all 64 cores to evaluate these tensors. . So a rough estimate of the core-hours needed for the DeepVariant WGS case is:. 64 cores * 205 minutes of runtime ~= 219 core hours ~= 9 days. So if you are running on a machine with a single core, you should see call_variants take ~9 days. This is a bit of an over-estimate because 64 cores isn't 64x more efficient than 1 core. . Based on your 1 day turn around I'd guess you are running on a machine with 8 cores. Note these numbers assume you are using a modern CPU with AVX etc instruction sets. Not having those can increase the runtime by ~4x or so. Also I want to ask - in your original post are you processing exomes? If so, are you providing a capture regions bed to make_examples? Normally an exome produces < 100k examples (contrast that with 5.8M in a whole genome) so the runtime should be 60x less on an exome. That means instead of 9 days on a single core you are looking at 3.5 hours.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:372,performance,CPU,CPUs,372,"Hi @mclaugsf,. Let me give you a bit more context here on the runtime of call_variants. call_variants is the deep learning component of DeepVariant, so it relies on TensorFlow to execute the inception_v3 model used to evaluate our genotype likelihoods. In the 0.7 case study, make_examples creates 5,847,041 genomic tensors that need to be evaluated. When executing using CPUs, TensorFlow by default uses all of the available cores on the machine. So in our case study, which runs on a 64 core machine, we are using all 64 cores to evaluate these tensors. . So a rough estimate of the core-hours needed for the DeepVariant WGS case is:. 64 cores * 205 minutes of runtime ~= 219 core hours ~= 9 days. So if you are running on a machine with a single core, you should see call_variants take ~9 days. This is a bit of an over-estimate because 64 cores isn't 64x more efficient than 1 core. . Based on your 1 day turn around I'd guess you are running on a machine with 8 cores. Note these numbers assume you are using a modern CPU with AVX etc instruction sets. Not having those can increase the runtime by ~4x or so. Also I want to ask - in your original post are you processing exomes? If so, are you providing a capture regions bed to make_examples? Normally an exome produces < 100k examples (contrast that with 5.8M in a whole genome) so the runtime should be 60x less on an exome. That means instead of 9 days on a single core you are looking at 3.5 hours.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:1023,performance,CPU,CPU,1023,"Hi @mclaugsf,. Let me give you a bit more context here on the runtime of call_variants. call_variants is the deep learning component of DeepVariant, so it relies on TensorFlow to execute the inception_v3 model used to evaluate our genotype likelihoods. In the 0.7 case study, make_examples creates 5,847,041 genomic tensors that need to be evaluated. When executing using CPUs, TensorFlow by default uses all of the available cores on the machine. So in our case study, which runs on a 64 core machine, we are using all 64 cores to evaluate these tensors. . So a rough estimate of the core-hours needed for the DeepVariant WGS case is:. 64 cores * 205 minutes of runtime ~= 219 core hours ~= 9 days. So if you are running on a machine with a single core, you should see call_variants take ~9 days. This is a bit of an over-estimate because 64 cores isn't 64x more efficient than 1 core. . Based on your 1 day turn around I'd guess you are running on a machine with 8 cores. Note these numbers assume you are using a modern CPU with AVX etc instruction sets. Not having those can increase the runtime by ~4x or so. Also I want to ask - in your original post are you processing exomes? If so, are you providing a capture regions bed to make_examples? Normally an exome produces < 100k examples (contrast that with 5.8M in a whole genome) so the runtime should be 60x less on an exome. That means instead of 9 days on a single core you are looking at 3.5 hours.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:416,reliability,availab,available,416,"Hi @mclaugsf,. Let me give you a bit more context here on the runtime of call_variants. call_variants is the deep learning component of DeepVariant, so it relies on TensorFlow to execute the inception_v3 model used to evaluate our genotype likelihoods. In the 0.7 case study, make_examples creates 5,847,041 genomic tensors that need to be evaluated. When executing using CPUs, TensorFlow by default uses all of the available cores on the machine. So in our case study, which runs on a 64 core machine, we are using all 64 cores to evaluate these tensors. . So a rough estimate of the core-hours needed for the DeepVariant WGS case is:. 64 cores * 205 minutes of runtime ~= 219 core hours ~= 9 days. So if you are running on a machine with a single core, you should see call_variants take ~9 days. This is a bit of an over-estimate because 64 cores isn't 64x more efficient than 1 core. . Based on your 1 day turn around I'd guess you are running on a machine with 8 cores. Note these numbers assume you are using a modern CPU with AVX etc instruction sets. Not having those can increase the runtime by ~4x or so. Also I want to ask - in your original post are you processing exomes? If so, are you providing a capture regions bed to make_examples? Normally an exome produces < 100k examples (contrast that with 5.8M in a whole genome) so the runtime should be 60x less on an exome. That means instead of 9 days on a single core you are looking at 3.5 hours.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:416,safety,avail,available,416,"Hi @mclaugsf,. Let me give you a bit more context here on the runtime of call_variants. call_variants is the deep learning component of DeepVariant, so it relies on TensorFlow to execute the inception_v3 model used to evaluate our genotype likelihoods. In the 0.7 case study, make_examples creates 5,847,041 genomic tensors that need to be evaluated. When executing using CPUs, TensorFlow by default uses all of the available cores on the machine. So in our case study, which runs on a 64 core machine, we are using all 64 cores to evaluate these tensors. . So a rough estimate of the core-hours needed for the DeepVariant WGS case is:. 64 cores * 205 minutes of runtime ~= 219 core hours ~= 9 days. So if you are running on a machine with a single core, you should see call_variants take ~9 days. This is a bit of an over-estimate because 64 cores isn't 64x more efficient than 1 core. . Based on your 1 day turn around I'd guess you are running on a machine with 8 cores. Note these numbers assume you are using a modern CPU with AVX etc instruction sets. Not having those can increase the runtime by ~4x or so. Also I want to ask - in your original post are you processing exomes? If so, are you providing a capture regions bed to make_examples? Normally an exome produces < 100k examples (contrast that with 5.8M in a whole genome) so the runtime should be 60x less on an exome. That means instead of 9 days on a single core you are looking at 3.5 hours.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:204,security,model,model,204,"Hi @mclaugsf,. Let me give you a bit more context here on the runtime of call_variants. call_variants is the deep learning component of DeepVariant, so it relies on TensorFlow to execute the inception_v3 model used to evaluate our genotype likelihoods. In the 0.7 case study, make_examples creates 5,847,041 genomic tensors that need to be evaluated. When executing using CPUs, TensorFlow by default uses all of the available cores on the machine. So in our case study, which runs on a 64 core machine, we are using all 64 cores to evaluate these tensors. . So a rough estimate of the core-hours needed for the DeepVariant WGS case is:. 64 cores * 205 minutes of runtime ~= 219 core hours ~= 9 days. So if you are running on a machine with a single core, you should see call_variants take ~9 days. This is a bit of an over-estimate because 64 cores isn't 64x more efficient than 1 core. . Based on your 1 day turn around I'd guess you are running on a machine with 8 cores. Note these numbers assume you are using a modern CPU with AVX etc instruction sets. Not having those can increase the runtime by ~4x or so. Also I want to ask - in your original post are you processing exomes? If so, are you providing a capture regions bed to make_examples? Normally an exome produces < 100k examples (contrast that with 5.8M in a whole genome) so the runtime should be 60x less on an exome. That means instead of 9 days on a single core you are looking at 3.5 hours.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:416,security,availab,available,416,"Hi @mclaugsf,. Let me give you a bit more context here on the runtime of call_variants. call_variants is the deep learning component of DeepVariant, so it relies on TensorFlow to execute the inception_v3 model used to evaluate our genotype likelihoods. In the 0.7 case study, make_examples creates 5,847,041 genomic tensors that need to be evaluated. When executing using CPUs, TensorFlow by default uses all of the available cores on the machine. So in our case study, which runs on a 64 core machine, we are using all 64 cores to evaluate these tensors. . So a rough estimate of the core-hours needed for the DeepVariant WGS case is:. 64 cores * 205 minutes of runtime ~= 219 core hours ~= 9 days. So if you are running on a machine with a single core, you should see call_variants take ~9 days. This is a bit of an over-estimate because 64 cores isn't 64x more efficient than 1 core. . Based on your 1 day turn around I'd guess you are running on a machine with 8 cores. Note these numbers assume you are using a modern CPU with AVX etc instruction sets. Not having those can increase the runtime by ~4x or so. Also I want to ask - in your original post are you processing exomes? If so, are you providing a capture regions bed to make_examples? Normally an exome produces < 100k examples (contrast that with 5.8M in a whole genome) so the runtime should be 60x less on an exome. That means instead of 9 days on a single core you are looking at 3.5 hours.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:42,testability,context,context,42,"Hi @mclaugsf,. Let me give you a bit more context here on the runtime of call_variants. call_variants is the deep learning component of DeepVariant, so it relies on TensorFlow to execute the inception_v3 model used to evaluate our genotype likelihoods. In the 0.7 case study, make_examples creates 5,847,041 genomic tensors that need to be evaluated. When executing using CPUs, TensorFlow by default uses all of the available cores on the machine. So in our case study, which runs on a 64 core machine, we are using all 64 cores to evaluate these tensors. . So a rough estimate of the core-hours needed for the DeepVariant WGS case is:. 64 cores * 205 minutes of runtime ~= 219 core hours ~= 9 days. So if you are running on a machine with a single core, you should see call_variants take ~9 days. This is a bit of an over-estimate because 64 cores isn't 64x more efficient than 1 core. . Based on your 1 day turn around I'd guess you are running on a machine with 8 cores. Note these numbers assume you are using a modern CPU with AVX etc instruction sets. Not having those can increase the runtime by ~4x or so. Also I want to ask - in your original post are you processing exomes? If so, are you providing a capture regions bed to make_examples? Normally an exome produces < 100k examples (contrast that with 5.8M in a whole genome) so the runtime should be 60x less on an exome. That means instead of 9 days on a single core you are looking at 3.5 hours.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:114,usability,learn,learning,114,"Hi @mclaugsf,. Let me give you a bit more context here on the runtime of call_variants. call_variants is the deep learning component of DeepVariant, so it relies on TensorFlow to execute the inception_v3 model used to evaluate our genotype likelihoods. In the 0.7 case study, make_examples creates 5,847,041 genomic tensors that need to be evaluated. When executing using CPUs, TensorFlow by default uses all of the available cores on the machine. So in our case study, which runs on a 64 core machine, we are using all 64 cores to evaluate these tensors. . So a rough estimate of the core-hours needed for the DeepVariant WGS case is:. 64 cores * 205 minutes of runtime ~= 219 core hours ~= 9 days. So if you are running on a machine with a single core, you should see call_variants take ~9 days. This is a bit of an over-estimate because 64 cores isn't 64x more efficient than 1 core. . Based on your 1 day turn around I'd guess you are running on a machine with 8 cores. Note these numbers assume you are using a modern CPU with AVX etc instruction sets. Not having those can increase the runtime by ~4x or so. Also I want to ask - in your original post are you processing exomes? If so, are you providing a capture regions bed to make_examples? Normally an exome produces < 100k examples (contrast that with 5.8M in a whole genome) so the runtime should be 60x less on an exome. That means instead of 9 days on a single core you are looking at 3.5 hours.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:864,usability,efficien,efficient,864,"Hi @mclaugsf,. Let me give you a bit more context here on the runtime of call_variants. call_variants is the deep learning component of DeepVariant, so it relies on TensorFlow to execute the inception_v3 model used to evaluate our genotype likelihoods. In the 0.7 case study, make_examples creates 5,847,041 genomic tensors that need to be evaluated. When executing using CPUs, TensorFlow by default uses all of the available cores on the machine. So in our case study, which runs on a 64 core machine, we are using all 64 cores to evaluate these tensors. . So a rough estimate of the core-hours needed for the DeepVariant WGS case is:. 64 cores * 205 minutes of runtime ~= 219 core hours ~= 9 days. So if you are running on a machine with a single core, you should see call_variants take ~9 days. This is a bit of an over-estimate because 64 cores isn't 64x more efficient than 1 core. . Based on your 1 day turn around I'd guess you are running on a machine with 8 cores. Note these numbers assume you are using a modern CPU with AVX etc instruction sets. Not having those can increase the runtime by ~4x or so. Also I want to ask - in your original post are you processing exomes? If so, are you providing a capture regions bed to make_examples? Normally an exome produces < 100k examples (contrast that with 5.8M in a whole genome) so the runtime should be 60x less on an exome. That means instead of 9 days on a single core you are looking at 3.5 hours.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:66,availability,avail,available,66,">When executing using CPUs, TensorFlow by default uses all of the available cores on the machine. So in our case study, which runs on a 64 core machine, we are using all 64 cores to evaluate these tensors. Thank you! I didn't understand there was a component of this that was automatically determining the number of cores I was using and making use of them. Yes, I was using 8 cores. >Also I want to ask - in your original post are you processing exomes? If so, are you providing a capture regions bed to make_examples? . Yes, I am currently evaluating DeepVariant on Exomes and WGS samples. I did indeed provide a regions bed to `make_examples`. My original issue I was asking about was a shard that seemed to be running forever (I broke up each shard for `make_examples` onto its own single core w/2Gb memory). But, this was probably an issue with the instance I was using for that particular shard and the way our system was mishandling the restart. I can get the exomes to run through DeepVariant reliably now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:1001,availability,reliab,reliably,1001,">When executing using CPUs, TensorFlow by default uses all of the available cores on the machine. So in our case study, which runs on a 64 core machine, we are using all 64 cores to evaluate these tensors. Thank you! I didn't understand there was a component of this that was automatically determining the number of cores I was using and making use of them. Yes, I was using 8 cores. >Also I want to ask - in your original post are you processing exomes? If so, are you providing a capture regions bed to make_examples? . Yes, I am currently evaluating DeepVariant on Exomes and WGS samples. I did indeed provide a regions bed to `make_examples`. My original issue I was asking about was a shard that seemed to be running forever (I broke up each shard for `make_examples` onto its own single core w/2Gb memory). But, this was probably an issue with the instance I was using for that particular shard and the way our system was mishandling the restart. I can get the exomes to run through DeepVariant reliably now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:276,deployability,automat,automatically,276,">When executing using CPUs, TensorFlow by default uses all of the available cores on the machine. So in our case study, which runs on a 64 core machine, we are using all 64 cores to evaluate these tensors. Thank you! I didn't understand there was a component of this that was automatically determining the number of cores I was using and making use of them. Yes, I was using 8 cores. >Also I want to ask - in your original post are you processing exomes? If so, are you providing a capture regions bed to make_examples? . Yes, I am currently evaluating DeepVariant on Exomes and WGS samples. I did indeed provide a regions bed to `make_examples`. My original issue I was asking about was a shard that seemed to be running forever (I broke up each shard for `make_examples` onto its own single core w/2Gb memory). But, this was probably an issue with the instance I was using for that particular shard and the way our system was mishandling the restart. I can get the exomes to run through DeepVariant reliably now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:22,energy efficiency,CPU,CPUs,22,">When executing using CPUs, TensorFlow by default uses all of the available cores on the machine. So in our case study, which runs on a 64 core machine, we are using all 64 cores to evaluate these tensors. Thank you! I didn't understand there was a component of this that was automatically determining the number of cores I was using and making use of them. Yes, I was using 8 cores. >Also I want to ask - in your original post are you processing exomes? If so, are you providing a capture regions bed to make_examples? . Yes, I am currently evaluating DeepVariant on Exomes and WGS samples. I did indeed provide a regions bed to `make_examples`. My original issue I was asking about was a shard that seemed to be running forever (I broke up each shard for `make_examples` onto its own single core w/2Gb memory). But, this was probably an issue with the instance I was using for that particular shard and the way our system was mishandling the restart. I can get the exomes to run through DeepVariant reliably now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:76,energy efficiency,core,cores,76,">When executing using CPUs, TensorFlow by default uses all of the available cores on the machine. So in our case study, which runs on a 64 core machine, we are using all 64 cores to evaluate these tensors. Thank you! I didn't understand there was a component of this that was automatically determining the number of cores I was using and making use of them. Yes, I was using 8 cores. >Also I want to ask - in your original post are you processing exomes? If so, are you providing a capture regions bed to make_examples? . Yes, I am currently evaluating DeepVariant on Exomes and WGS samples. I did indeed provide a regions bed to `make_examples`. My original issue I was asking about was a shard that seemed to be running forever (I broke up each shard for `make_examples` onto its own single core w/2Gb memory). But, this was probably an issue with the instance I was using for that particular shard and the way our system was mishandling the restart. I can get the exomes to run through DeepVariant reliably now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:139,energy efficiency,core,core,139,">When executing using CPUs, TensorFlow by default uses all of the available cores on the machine. So in our case study, which runs on a 64 core machine, we are using all 64 cores to evaluate these tensors. Thank you! I didn't understand there was a component of this that was automatically determining the number of cores I was using and making use of them. Yes, I was using 8 cores. >Also I want to ask - in your original post are you processing exomes? If so, are you providing a capture regions bed to make_examples? . Yes, I am currently evaluating DeepVariant on Exomes and WGS samples. I did indeed provide a regions bed to `make_examples`. My original issue I was asking about was a shard that seemed to be running forever (I broke up each shard for `make_examples` onto its own single core w/2Gb memory). But, this was probably an issue with the instance I was using for that particular shard and the way our system was mishandling the restart. I can get the exomes to run through DeepVariant reliably now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:173,energy efficiency,core,cores,173,">When executing using CPUs, TensorFlow by default uses all of the available cores on the machine. So in our case study, which runs on a 64 core machine, we are using all 64 cores to evaluate these tensors. Thank you! I didn't understand there was a component of this that was automatically determining the number of cores I was using and making use of them. Yes, I was using 8 cores. >Also I want to ask - in your original post are you processing exomes? If so, are you providing a capture regions bed to make_examples? . Yes, I am currently evaluating DeepVariant on Exomes and WGS samples. I did indeed provide a regions bed to `make_examples`. My original issue I was asking about was a shard that seemed to be running forever (I broke up each shard for `make_examples` onto its own single core w/2Gb memory). But, this was probably an issue with the instance I was using for that particular shard and the way our system was mishandling the restart. I can get the exomes to run through DeepVariant reliably now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:316,energy efficiency,core,cores,316,">When executing using CPUs, TensorFlow by default uses all of the available cores on the machine. So in our case study, which runs on a 64 core machine, we are using all 64 cores to evaluate these tensors. Thank you! I didn't understand there was a component of this that was automatically determining the number of cores I was using and making use of them. Yes, I was using 8 cores. >Also I want to ask - in your original post are you processing exomes? If so, are you providing a capture regions bed to make_examples? . Yes, I am currently evaluating DeepVariant on Exomes and WGS samples. I did indeed provide a regions bed to `make_examples`. My original issue I was asking about was a shard that seemed to be running forever (I broke up each shard for `make_examples` onto its own single core w/2Gb memory). But, this was probably an issue with the instance I was using for that particular shard and the way our system was mishandling the restart. I can get the exomes to run through DeepVariant reliably now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:377,energy efficiency,core,cores,377,">When executing using CPUs, TensorFlow by default uses all of the available cores on the machine. So in our case study, which runs on a 64 core machine, we are using all 64 cores to evaluate these tensors. Thank you! I didn't understand there was a component of this that was automatically determining the number of cores I was using and making use of them. Yes, I was using 8 cores. >Also I want to ask - in your original post are you processing exomes? If so, are you providing a capture regions bed to make_examples? . Yes, I am currently evaluating DeepVariant on Exomes and WGS samples. I did indeed provide a regions bed to `make_examples`. My original issue I was asking about was a shard that seemed to be running forever (I broke up each shard for `make_examples` onto its own single core w/2Gb memory). But, this was probably an issue with the instance I was using for that particular shard and the way our system was mishandling the restart. I can get the exomes to run through DeepVariant reliably now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:532,energy efficiency,current,currently,532,">When executing using CPUs, TensorFlow by default uses all of the available cores on the machine. So in our case study, which runs on a 64 core machine, we are using all 64 cores to evaluate these tensors. Thank you! I didn't understand there was a component of this that was automatically determining the number of cores I was using and making use of them. Yes, I was using 8 cores. >Also I want to ask - in your original post are you processing exomes? If so, are you providing a capture regions bed to make_examples? . Yes, I am currently evaluating DeepVariant on Exomes and WGS samples. I did indeed provide a regions bed to `make_examples`. My original issue I was asking about was a shard that seemed to be running forever (I broke up each shard for `make_examples` onto its own single core w/2Gb memory). But, this was probably an issue with the instance I was using for that particular shard and the way our system was mishandling the restart. I can get the exomes to run through DeepVariant reliably now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:793,energy efficiency,core,core,793,">When executing using CPUs, TensorFlow by default uses all of the available cores on the machine. So in our case study, which runs on a 64 core machine, we are using all 64 cores to evaluate these tensors. Thank you! I didn't understand there was a component of this that was automatically determining the number of cores I was using and making use of them. Yes, I was using 8 cores. >Also I want to ask - in your original post are you processing exomes? If so, are you providing a capture regions bed to make_examples? . Yes, I am currently evaluating DeepVariant on Exomes and WGS samples. I did indeed provide a regions bed to `make_examples`. My original issue I was asking about was a shard that seemed to be running forever (I broke up each shard for `make_examples` onto its own single core w/2Gb memory). But, this was probably an issue with the instance I was using for that particular shard and the way our system was mishandling the restart. I can get the exomes to run through DeepVariant reliably now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:249,integrability,compon,component,249,">When executing using CPUs, TensorFlow by default uses all of the available cores on the machine. So in our case study, which runs on a 64 core machine, we are using all 64 cores to evaluate these tensors. Thank you! I didn't understand there was a component of this that was automatically determining the number of cores I was using and making use of them. Yes, I was using 8 cores. >Also I want to ask - in your original post are you processing exomes? If so, are you providing a capture regions bed to make_examples? . Yes, I am currently evaluating DeepVariant on Exomes and WGS samples. I did indeed provide a regions bed to `make_examples`. My original issue I was asking about was a shard that seemed to be running forever (I broke up each shard for `make_examples` onto its own single core w/2Gb memory). But, this was probably an issue with the instance I was using for that particular shard and the way our system was mishandling the restart. I can get the exomes to run through DeepVariant reliably now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:249,interoperability,compon,component,249,">When executing using CPUs, TensorFlow by default uses all of the available cores on the machine. So in our case study, which runs on a 64 core machine, we are using all 64 cores to evaluate these tensors. Thank you! I didn't understand there was a component of this that was automatically determining the number of cores I was using and making use of them. Yes, I was using 8 cores. >Also I want to ask - in your original post are you processing exomes? If so, are you providing a capture regions bed to make_examples? . Yes, I am currently evaluating DeepVariant on Exomes and WGS samples. I did indeed provide a regions bed to `make_examples`. My original issue I was asking about was a shard that seemed to be running forever (I broke up each shard for `make_examples` onto its own single core w/2Gb memory). But, this was probably an issue with the instance I was using for that particular shard and the way our system was mishandling the restart. I can get the exomes to run through DeepVariant reliably now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:249,modifiability,compon,component,249,">When executing using CPUs, TensorFlow by default uses all of the available cores on the machine. So in our case study, which runs on a 64 core machine, we are using all 64 cores to evaluate these tensors. Thank you! I didn't understand there was a component of this that was automatically determining the number of cores I was using and making use of them. Yes, I was using 8 cores. >Also I want to ask - in your original post are you processing exomes? If so, are you providing a capture regions bed to make_examples? . Yes, I am currently evaluating DeepVariant on Exomes and WGS samples. I did indeed provide a regions bed to `make_examples`. My original issue I was asking about was a shard that seemed to be running forever (I broke up each shard for `make_examples` onto its own single core w/2Gb memory). But, this was probably an issue with the instance I was using for that particular shard and the way our system was mishandling the restart. I can get the exomes to run through DeepVariant reliably now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:22,performance,CPU,CPUs,22,">When executing using CPUs, TensorFlow by default uses all of the available cores on the machine. So in our case study, which runs on a 64 core machine, we are using all 64 cores to evaluate these tensors. Thank you! I didn't understand there was a component of this that was automatically determining the number of cores I was using and making use of them. Yes, I was using 8 cores. >Also I want to ask - in your original post are you processing exomes? If so, are you providing a capture regions bed to make_examples? . Yes, I am currently evaluating DeepVariant on Exomes and WGS samples. I did indeed provide a regions bed to `make_examples`. My original issue I was asking about was a shard that seemed to be running forever (I broke up each shard for `make_examples` onto its own single core w/2Gb memory). But, this was probably an issue with the instance I was using for that particular shard and the way our system was mishandling the restart. I can get the exomes to run through DeepVariant reliably now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:804,performance,memor,memory,804,">When executing using CPUs, TensorFlow by default uses all of the available cores on the machine. So in our case study, which runs on a 64 core machine, we are using all 64 cores to evaluate these tensors. Thank you! I didn't understand there was a component of this that was automatically determining the number of cores I was using and making use of them. Yes, I was using 8 cores. >Also I want to ask - in your original post are you processing exomes? If so, are you providing a capture regions bed to make_examples? . Yes, I am currently evaluating DeepVariant on Exomes and WGS samples. I did indeed provide a regions bed to `make_examples`. My original issue I was asking about was a shard that seemed to be running forever (I broke up each shard for `make_examples` onto its own single core w/2Gb memory). But, this was probably an issue with the instance I was using for that particular shard and the way our system was mishandling the restart. I can get the exomes to run through DeepVariant reliably now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:66,reliability,availab,available,66,">When executing using CPUs, TensorFlow by default uses all of the available cores on the machine. So in our case study, which runs on a 64 core machine, we are using all 64 cores to evaluate these tensors. Thank you! I didn't understand there was a component of this that was automatically determining the number of cores I was using and making use of them. Yes, I was using 8 cores. >Also I want to ask - in your original post are you processing exomes? If so, are you providing a capture regions bed to make_examples? . Yes, I am currently evaluating DeepVariant on Exomes and WGS samples. I did indeed provide a regions bed to `make_examples`. My original issue I was asking about was a shard that seemed to be running forever (I broke up each shard for `make_examples` onto its own single core w/2Gb memory). But, this was probably an issue with the instance I was using for that particular shard and the way our system was mishandling the restart. I can get the exomes to run through DeepVariant reliably now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:1001,reliability,reliab,reliably,1001,">When executing using CPUs, TensorFlow by default uses all of the available cores on the machine. So in our case study, which runs on a 64 core machine, we are using all 64 cores to evaluate these tensors. Thank you! I didn't understand there was a component of this that was automatically determining the number of cores I was using and making use of them. Yes, I was using 8 cores. >Also I want to ask - in your original post are you processing exomes? If so, are you providing a capture regions bed to make_examples? . Yes, I am currently evaluating DeepVariant on Exomes and WGS samples. I did indeed provide a regions bed to `make_examples`. My original issue I was asking about was a shard that seemed to be running forever (I broke up each shard for `make_examples` onto its own single core w/2Gb memory). But, this was probably an issue with the instance I was using for that particular shard and the way our system was mishandling the restart. I can get the exomes to run through DeepVariant reliably now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:66,safety,avail,available,66,">When executing using CPUs, TensorFlow by default uses all of the available cores on the machine. So in our case study, which runs on a 64 core machine, we are using all 64 cores to evaluate these tensors. Thank you! I didn't understand there was a component of this that was automatically determining the number of cores I was using and making use of them. Yes, I was using 8 cores. >Also I want to ask - in your original post are you processing exomes? If so, are you providing a capture regions bed to make_examples? . Yes, I am currently evaluating DeepVariant on Exomes and WGS samples. I did indeed provide a regions bed to `make_examples`. My original issue I was asking about was a shard that seemed to be running forever (I broke up each shard for `make_examples` onto its own single core w/2Gb memory). But, this was probably an issue with the instance I was using for that particular shard and the way our system was mishandling the restart. I can get the exomes to run through DeepVariant reliably now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:66,security,availab,available,66,">When executing using CPUs, TensorFlow by default uses all of the available cores on the machine. So in our case study, which runs on a 64 core machine, we are using all 64 cores to evaluate these tensors. Thank you! I didn't understand there was a component of this that was automatically determining the number of cores I was using and making use of them. Yes, I was using 8 cores. >Also I want to ask - in your original post are you processing exomes? If so, are you providing a capture regions bed to make_examples? . Yes, I am currently evaluating DeepVariant on Exomes and WGS samples. I did indeed provide a regions bed to `make_examples`. My original issue I was asking about was a shard that seemed to be running forever (I broke up each shard for `make_examples` onto its own single core w/2Gb memory). But, this was probably an issue with the instance I was using for that particular shard and the way our system was mishandling the restart. I can get the exomes to run through DeepVariant reliably now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:226,testability,understand,understand,226,">When executing using CPUs, TensorFlow by default uses all of the available cores on the machine. So in our case study, which runs on a 64 core machine, we are using all 64 cores to evaluate these tensors. Thank you! I didn't understand there was a component of this that was automatically determining the number of cores I was using and making use of them. Yes, I was using 8 cores. >Also I want to ask - in your original post are you processing exomes? If so, are you providing a capture regions bed to make_examples? . Yes, I am currently evaluating DeepVariant on Exomes and WGS samples. I did indeed provide a regions bed to `make_examples`. My original issue I was asking about was a shard that seemed to be running forever (I broke up each shard for `make_examples` onto its own single core w/2Gb memory). But, this was probably an issue with the instance I was using for that particular shard and the way our system was mishandling the restart. I can get the exomes to run through DeepVariant reliably now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:276,testability,automat,automatically,276,">When executing using CPUs, TensorFlow by default uses all of the available cores on the machine. So in our case study, which runs on a 64 core machine, we are using all 64 cores to evaluate these tensors. Thank you! I didn't understand there was a component of this that was automatically determining the number of cores I was using and making use of them. Yes, I was using 8 cores. >Also I want to ask - in your original post are you processing exomes? If so, are you providing a capture regions bed to make_examples? . Yes, I am currently evaluating DeepVariant on Exomes and WGS samples. I did indeed provide a regions bed to `make_examples`. My original issue I was asking about was a shard that seemed to be running forever (I broke up each shard for `make_examples` onto its own single core w/2Gb memory). But, this was probably an issue with the instance I was using for that particular shard and the way our system was mishandling the restart. I can get the exomes to run through DeepVariant reliably now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/105:804,usability,memor,memory,804,">When executing using CPUs, TensorFlow by default uses all of the available cores on the machine. So in our case study, which runs on a 64 core machine, we are using all 64 cores to evaluate these tensors. Thank you! I didn't understand there was a component of this that was automatically determining the number of cores I was using and making use of them. Yes, I was using 8 cores. >Also I want to ask - in your original post are you processing exomes? If so, are you providing a capture regions bed to make_examples? . Yes, I am currently evaluating DeepVariant on Exomes and WGS samples. I did indeed provide a regions bed to `make_examples`. My original issue I was asking about was a shard that seemed to be running forever (I broke up each shard for `make_examples` onto its own single core w/2Gb memory). But, this was probably an issue with the instance I was using for that particular shard and the way our system was mishandling the restart. I can get the exomes to run through DeepVariant reliably now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/105
https://github.com/google/deepvariant/issues/106:17,availability,error,error,17,"Most likely this error is due to your reference and input BAM/SAM have different contig names. The error you see may happen if for example reference contigs are named ""chr1"", ""chr2"", etc but input input BAM/SAM contigs have names without ""chr"" prefix.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/106
https://github.com/google/deepvariant/issues/106:99,availability,error,error,99,"Most likely this error is due to your reference and input BAM/SAM have different contig names. The error you see may happen if for example reference contigs are named ""chr1"", ""chr2"", etc but input input BAM/SAM contigs have names without ""chr"" prefix.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/106
https://github.com/google/deepvariant/issues/106:17,performance,error,error,17,"Most likely this error is due to your reference and input BAM/SAM have different contig names. The error you see may happen if for example reference contigs are named ""chr1"", ""chr2"", etc but input input BAM/SAM contigs have names without ""chr"" prefix.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/106
https://github.com/google/deepvariant/issues/106:99,performance,error,error,99,"Most likely this error is due to your reference and input BAM/SAM have different contig names. The error you see may happen if for example reference contigs are named ""chr1"", ""chr2"", etc but input input BAM/SAM contigs have names without ""chr"" prefix.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/106
https://github.com/google/deepvariant/issues/106:17,safety,error,error,17,"Most likely this error is due to your reference and input BAM/SAM have different contig names. The error you see may happen if for example reference contigs are named ""chr1"", ""chr2"", etc but input input BAM/SAM contigs have names without ""chr"" prefix.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/106
https://github.com/google/deepvariant/issues/106:52,safety,input,input,52,"Most likely this error is due to your reference and input BAM/SAM have different contig names. The error you see may happen if for example reference contigs are named ""chr1"", ""chr2"", etc but input input BAM/SAM contigs have names without ""chr"" prefix.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/106
https://github.com/google/deepvariant/issues/106:99,safety,error,error,99,"Most likely this error is due to your reference and input BAM/SAM have different contig names. The error you see may happen if for example reference contigs are named ""chr1"", ""chr2"", etc but input input BAM/SAM contigs have names without ""chr"" prefix.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/106
https://github.com/google/deepvariant/issues/106:191,safety,input,input,191,"Most likely this error is due to your reference and input BAM/SAM have different contig names. The error you see may happen if for example reference contigs are named ""chr1"", ""chr2"", etc but input input BAM/SAM contigs have names without ""chr"" prefix.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/106
https://github.com/google/deepvariant/issues/106:197,safety,input,input,197,"Most likely this error is due to your reference and input BAM/SAM have different contig names. The error you see may happen if for example reference contigs are named ""chr1"", ""chr2"", etc but input input BAM/SAM contigs have names without ""chr"" prefix.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/106
https://github.com/google/deepvariant/issues/106:17,usability,error,error,17,"Most likely this error is due to your reference and input BAM/SAM have different contig names. The error you see may happen if for example reference contigs are named ""chr1"", ""chr2"", etc but input input BAM/SAM contigs have names without ""chr"" prefix.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/106
https://github.com/google/deepvariant/issues/106:52,usability,input,input,52,"Most likely this error is due to your reference and input BAM/SAM have different contig names. The error you see may happen if for example reference contigs are named ""chr1"", ""chr2"", etc but input input BAM/SAM contigs have names without ""chr"" prefix.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/106
https://github.com/google/deepvariant/issues/106:99,usability,error,error,99,"Most likely this error is due to your reference and input BAM/SAM have different contig names. The error you see may happen if for example reference contigs are named ""chr1"", ""chr2"", etc but input input BAM/SAM contigs have names without ""chr"" prefix.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/106
https://github.com/google/deepvariant/issues/106:191,usability,input,input,191,"Most likely this error is due to your reference and input BAM/SAM have different contig names. The error you see may happen if for example reference contigs are named ""chr1"", ""chr2"", etc but input input BAM/SAM contigs have names without ""chr"" prefix.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/106
https://github.com/google/deepvariant/issues/106:197,usability,input,input,197,"Most likely this error is due to your reference and input BAM/SAM have different contig names. The error you see may happen if for example reference contigs are named ""chr1"", ""chr2"", etc but input input BAM/SAM contigs have names without ""chr"" prefix.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/106
https://github.com/google/deepvariant/issues/106:95,interoperability,coordinat,coordinate,95,"I have checked, they match well. . This is the bam header file ""SQ"" field:. ```. @HD VN:1.4 SO:coordinate. @SQ SN:1 LN:249250621. @SQ SN:2 LN:243199373. @SQ SN:3 LN:198022430. @SQ SN:4 LN:191154276. @SQ SN:5 LN:180915260. @SQ SN:6 LN:171115067. @SQ SN:7 LN:159138663. @SQ SN:8 LN:146364022. @SQ SN:9 LN:141213431. @SQ SN:10 LN:135534747. @SQ SN:11 LN:135006516. @SQ SN:12 LN:133851895. @SQ SN:13 LN:115169878. @SQ SN:14 LN:107349540. @SQ SN:15 LN:102531392. @SQ SN:16 LN:90354753. @SQ SN:17 LN:81195210. @SQ SN:18 LN:78077248. @SQ SN:19 LN:59128983. @SQ SN:20 LN:63025520. @SQ SN:21 LN:48129895. @SQ SN:22 LN:51304566. @SQ SN:X LN:155270560. @SQ SN:Y LN:59373566. @SQ SN:MT LN:16569. @SQ SN:GL000207.1 LN:4262. @SQ SN:GL000226.1 LN:15008. @SQ SN:GL000229.1 LN:19913. @SQ SN:GL000231.1 LN:27386. @SQ SN:GL000210.1 LN:27682. @SQ SN:GL000239.1 LN:33824. @SQ SN:GL000235.1 LN:34474. @SQ SN:GL000201.1 LN:36148. @SQ SN:GL000247.1 LN:36422. @SQ SN:GL000245.1 LN:36651. @SQ SN:GL000197.1 LN:37175. @SQ SN:GL000203.1 LN:37498. @SQ SN:GL000246.1 LN:38154. @SQ SN:GL000249.1 LN:38502. @SQ SN:GL000196.1 LN:38914. @SQ SN:GL000248.1 LN:39786. @SQ SN:GL000244.1 LN:39929. @SQ SN:GL000238.1 LN:39939. @SQ SN:GL000202.1 LN:40103. @SQ SN:GL000234.1 LN:40531. @SQ SN:GL000232.1 LN:40652. @SQ SN:GL000206.1 LN:41001. @SQ SN:GL000240.1 LN:41933. @SQ SN:GL000236.1 LN:41934. @SQ SN:GL000241.1 LN:42152. @SQ SN:GL000243.1 LN:43341. @SQ SN:GL000242.1 LN:43523. @SQ SN:GL000230.1 LN:43691. @SQ SN:GL000237.1 LN:45867. @SQ SN:GL000233.1 LN:45941. @SQ SN:GL000204.1 LN:81310. @SQ SN:GL000198.1 LN:90085. @SQ SN:GL000208.1 LN:92689. @SQ SN:GL000191.1 LN:106433. @SQ SN:GL000227.1 LN:128374. @SQ SN:GL000228.1 LN:129120. @SQ SN:GL000214.1 LN:137718. @SQ SN:GL000221.1 LN:155397. @SQ SN:GL000209.1 LN:159169. @SQ SN:GL000218.1 LN:161147. @SQ SN:GL000220.1 LN:161802. @SQ SN:GL000213.1 LN:164239. @SQ SN:GL000211.1 LN:166566. @SQ SN:GL000199.1 LN:169874. @SQ SN:GL000217.1 LN:172149. @SQ SN:GL000216.1 LN:172294. @SQ SN:GL000215.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/106
https://github.com/google/deepvariant/issues/106:2456,performance,content,content,2456,43691. @SQ SN:GL000237.1 LN:45867. @SQ SN:GL000233.1 LN:45941. @SQ SN:GL000204.1 LN:81310. @SQ SN:GL000198.1 LN:90085. @SQ SN:GL000208.1 LN:92689. @SQ SN:GL000191.1 LN:106433. @SQ SN:GL000227.1 LN:128374. @SQ SN:GL000228.1 LN:129120. @SQ SN:GL000214.1 LN:137718. @SQ SN:GL000221.1 LN:155397. @SQ SN:GL000209.1 LN:159169. @SQ SN:GL000218.1 LN:161147. @SQ SN:GL000220.1 LN:161802. @SQ SN:GL000213.1 LN:164239. @SQ SN:GL000211.1 LN:166566. @SQ SN:GL000199.1 LN:169874. @SQ SN:GL000217.1 LN:172149. @SQ SN:GL000216.1 LN:172294. @SQ SN:GL000215.1 LN:172545. @SQ SN:GL000205.1 LN:174588. @SQ SN:GL000219.1 LN:179198. @SQ SN:GL000224.1 LN:179693. @SQ SN:GL000223.1 LN:180455. @SQ SN:GL000195.1 LN:182896. @SQ SN:GL000212.1 LN:186858. @SQ SN:GL000222.1 LN:186861. @SQ SN:GL000200.1 LN:187035. @SQ SN:GL000193.1 LN:189789. @SQ SN:GL000194.1 LN:191469. @SQ SN:GL000225.1 LN:211173. @SQ SN:GL000192.1 LN:547496. @SQ SN:NC_007605 LN:171823. @SQ SN:hs37d5 LN:35477943. ```. This is the reference .fa.fai file content:. ```. 1 249250621 52 60 61. 2 243199373 253404903 60 61. 3 198022430 500657651 60 61. 4 191154276 701980507 60 61. 5 180915260 896320740 60 61. 6 171115067 1080251307 60 61. 7 159138663 1254218344 60 61. 8 146364022 1416009371 60 61. 9 141213431 1564812846 60 61. 10 135534747 1708379889 60 61. 11 135006516 1846173603 60 61. 12 133851895 1983430282 60 61. 13 115169878 2119513096 60 61. 14 107349540 2236602526 60 61. 15 102531392 2345741279 60 61. 16 90354753 2449981581 60 61. 17 81195210 2541842300 60 61. 18 78077248 2624390817 60 61. 19 59128983 2703769406 60 61. 20 63025520 2763883926 60 61. 21 48129895 2827959925 60 61. 22 51304566 2876892038 60 61. X 155270560 2929051733 60 61. Y 59373566 3086910193 60 61. MT 16569 3147273397 70 71. GL000207.1 4262 3147290264 60 61. GL000226.1 15008 3147294660 60 61. GL000229.1 19913 3147309981 60 61. GL000231.1 27386 3147330288 60 61. GL000210.1 27682 3147358193 60 61. GL000239.1 33824 3147386399 60 61. GL000235.1 34474 3147420849 60 61. GL000,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/106
https://github.com/google/deepvariant/issues/106:31,reliability,doe,doesn,31,Problem solved. . The vcf file doesn't match. Thanks!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/106
https://github.com/google/deepvariant/issues/107:294,availability,robust,robust,294,"Since DeepVariant isn't trained on this data, it will depend on how different this new datatype is from our existing training data. For the training data we used, you can see: https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details-training-data.md. DeepVariant is known to be robust even on certain new data types. For example, when DNAnexus published this blog post, DeepVariant wasn't trained on NovaSeq, and yet DeepVariant performed well on NovaSeq data:. https://blog.dnanexus.com/2018-01-16-evaluating-the-performance-of-ngs-pipelines-on-noisy-wgs-data/. However, we also know that DeepVariant can be significantly more accurate if it has a chance to see the data in its training set. (For example, see: https://ai.googleblog.com/2018/04/deepvariant-accuracy-improvements-for.html). If you'd like to give it a try and let us know, that will be great, especially if you compare it to other baselines. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/107
https://github.com/google/deepvariant/issues/107:54,deployability,depend,depend,54,"Since DeepVariant isn't trained on this data, it will depend on how different this new datatype is from our existing training data. For the training data we used, you can see: https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details-training-data.md. DeepVariant is known to be robust even on certain new data types. For example, when DNAnexus published this blog post, DeepVariant wasn't trained on NovaSeq, and yet DeepVariant performed well on NovaSeq data:. https://blog.dnanexus.com/2018-01-16-evaluating-the-performance-of-ngs-pipelines-on-noisy-wgs-data/. However, we also know that DeepVariant can be significantly more accurate if it has a chance to see the data in its training set. (For example, see: https://ai.googleblog.com/2018/04/deepvariant-accuracy-improvements-for.html). If you'd like to give it a try and let us know, that will be great, especially if you compare it to other baselines. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/107
https://github.com/google/deepvariant/issues/107:549,deployability,pipelin,pipelines-on-noisy-wgs-data,549,"Since DeepVariant isn't trained on this data, it will depend on how different this new datatype is from our existing training data. For the training data we used, you can see: https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details-training-data.md. DeepVariant is known to be robust even on certain new data types. For example, when DNAnexus published this blog post, DeepVariant wasn't trained on NovaSeq, and yet DeepVariant performed well on NovaSeq data:. https://blog.dnanexus.com/2018-01-16-evaluating-the-performance-of-ngs-pipelines-on-noisy-wgs-data/. However, we also know that DeepVariant can be significantly more accurate if it has a chance to see the data in its training set. (For example, see: https://ai.googleblog.com/2018/04/deepvariant-accuracy-improvements-for.html). If you'd like to give it a try and let us know, that will be great, especially if you compare it to other baselines. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/107
https://github.com/google/deepvariant/issues/107:54,integrability,depend,depend,54,"Since DeepVariant isn't trained on this data, it will depend on how different this new datatype is from our existing training data. For the training data we used, you can see: https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details-training-data.md. DeepVariant is known to be robust even on certain new data types. For example, when DNAnexus published this blog post, DeepVariant wasn't trained on NovaSeq, and yet DeepVariant performed well on NovaSeq data:. https://blog.dnanexus.com/2018-01-16-evaluating-the-performance-of-ngs-pipelines-on-noisy-wgs-data/. However, we also know that DeepVariant can be significantly more accurate if it has a chance to see the data in its training set. (For example, see: https://ai.googleblog.com/2018/04/deepvariant-accuracy-improvements-for.html). If you'd like to give it a try and let us know, that will be great, especially if you compare it to other baselines. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/107
https://github.com/google/deepvariant/issues/107:360,integrability,pub,published,360,"Since DeepVariant isn't trained on this data, it will depend on how different this new datatype is from our existing training data. For the training data we used, you can see: https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details-training-data.md. DeepVariant is known to be robust even on certain new data types. For example, when DNAnexus published this blog post, DeepVariant wasn't trained on NovaSeq, and yet DeepVariant performed well on NovaSeq data:. https://blog.dnanexus.com/2018-01-16-evaluating-the-performance-of-ngs-pipelines-on-noisy-wgs-data/. However, we also know that DeepVariant can be significantly more accurate if it has a chance to see the data in its training set. (For example, see: https://ai.googleblog.com/2018/04/deepvariant-accuracy-improvements-for.html). If you'd like to give it a try and let us know, that will be great, especially if you compare it to other baselines. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/107
https://github.com/google/deepvariant/issues/107:549,integrability,pipelin,pipelines-on-noisy-wgs-data,549,"Since DeepVariant isn't trained on this data, it will depend on how different this new datatype is from our existing training data. For the training data we used, you can see: https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details-training-data.md. DeepVariant is known to be robust even on certain new data types. For example, when DNAnexus published this blog post, DeepVariant wasn't trained on NovaSeq, and yet DeepVariant performed well on NovaSeq data:. https://blog.dnanexus.com/2018-01-16-evaluating-the-performance-of-ngs-pipelines-on-noisy-wgs-data/. However, we also know that DeepVariant can be significantly more accurate if it has a chance to see the data in its training set. (For example, see: https://ai.googleblog.com/2018/04/deepvariant-accuracy-improvements-for.html). If you'd like to give it a try and let us know, that will be great, especially if you compare it to other baselines. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/107
https://github.com/google/deepvariant/issues/107:54,modifiability,depend,depend,54,"Since DeepVariant isn't trained on this data, it will depend on how different this new datatype is from our existing training data. For the training data we used, you can see: https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details-training-data.md. DeepVariant is known to be robust even on certain new data types. For example, when DNAnexus published this blog post, DeepVariant wasn't trained on NovaSeq, and yet DeepVariant performed well on NovaSeq data:. https://blog.dnanexus.com/2018-01-16-evaluating-the-performance-of-ngs-pipelines-on-noisy-wgs-data/. However, we also know that DeepVariant can be significantly more accurate if it has a chance to see the data in its training set. (For example, see: https://ai.googleblog.com/2018/04/deepvariant-accuracy-improvements-for.html). If you'd like to give it a try and let us know, that will be great, especially if you compare it to other baselines. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/107
https://github.com/google/deepvariant/issues/107:445,performance,perform,performed,445,"Since DeepVariant isn't trained on this data, it will depend on how different this new datatype is from our existing training data. For the training data we used, you can see: https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details-training-data.md. DeepVariant is known to be robust even on certain new data types. For example, when DNAnexus published this blog post, DeepVariant wasn't trained on NovaSeq, and yet DeepVariant performed well on NovaSeq data:. https://blog.dnanexus.com/2018-01-16-evaluating-the-performance-of-ngs-pipelines-on-noisy-wgs-data/. However, we also know that DeepVariant can be significantly more accurate if it has a chance to see the data in its training set. (For example, see: https://ai.googleblog.com/2018/04/deepvariant-accuracy-improvements-for.html). If you'd like to give it a try and let us know, that will be great, especially if you compare it to other baselines. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/107
https://github.com/google/deepvariant/issues/107:530,performance,perform,performance-of-ngs-pipelines-on-noisy-wgs-data,530,"Since DeepVariant isn't trained on this data, it will depend on how different this new datatype is from our existing training data. For the training data we used, you can see: https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details-training-data.md. DeepVariant is known to be robust even on certain new data types. For example, when DNAnexus published this blog post, DeepVariant wasn't trained on NovaSeq, and yet DeepVariant performed well on NovaSeq data:. https://blog.dnanexus.com/2018-01-16-evaluating-the-performance-of-ngs-pipelines-on-noisy-wgs-data/. However, we also know that DeepVariant can be significantly more accurate if it has a chance to see the data in its training set. (For example, see: https://ai.googleblog.com/2018/04/deepvariant-accuracy-improvements-for.html). If you'd like to give it a try and let us know, that will be great, especially if you compare it to other baselines. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/107
https://github.com/google/deepvariant/issues/107:294,reliability,robust,robust,294,"Since DeepVariant isn't trained on this data, it will depend on how different this new datatype is from our existing training data. For the training data we used, you can see: https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details-training-data.md. DeepVariant is known to be robust even on certain new data types. For example, when DNAnexus published this blog post, DeepVariant wasn't trained on NovaSeq, and yet DeepVariant performed well on NovaSeq data:. https://blog.dnanexus.com/2018-01-16-evaluating-the-performance-of-ngs-pipelines-on-noisy-wgs-data/. However, we also know that DeepVariant can be significantly more accurate if it has a chance to see the data in its training set. (For example, see: https://ai.googleblog.com/2018/04/deepvariant-accuracy-improvements-for.html). If you'd like to give it a try and let us know, that will be great, especially if you compare it to other baselines. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/107
https://github.com/google/deepvariant/issues/107:54,safety,depend,depend,54,"Since DeepVariant isn't trained on this data, it will depend on how different this new datatype is from our existing training data. For the training data we used, you can see: https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details-training-data.md. DeepVariant is known to be robust even on certain new data types. For example, when DNAnexus published this blog post, DeepVariant wasn't trained on NovaSeq, and yet DeepVariant performed well on NovaSeq data:. https://blog.dnanexus.com/2018-01-16-evaluating-the-performance-of-ngs-pipelines-on-noisy-wgs-data/. However, we also know that DeepVariant can be significantly more accurate if it has a chance to see the data in its training set. (For example, see: https://ai.googleblog.com/2018/04/deepvariant-accuracy-improvements-for.html). If you'd like to give it a try and let us know, that will be great, especially if you compare it to other baselines. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/107
https://github.com/google/deepvariant/issues/107:294,safety,robust,robust,294,"Since DeepVariant isn't trained on this data, it will depend on how different this new datatype is from our existing training data. For the training data we used, you can see: https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details-training-data.md. DeepVariant is known to be robust even on certain new data types. For example, when DNAnexus published this blog post, DeepVariant wasn't trained on NovaSeq, and yet DeepVariant performed well on NovaSeq data:. https://blog.dnanexus.com/2018-01-16-evaluating-the-performance-of-ngs-pipelines-on-noisy-wgs-data/. However, we also know that DeepVariant can be significantly more accurate if it has a chance to see the data in its training set. (For example, see: https://ai.googleblog.com/2018/04/deepvariant-accuracy-improvements-for.html). If you'd like to give it a try and let us know, that will be great, especially if you compare it to other baselines. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/107
https://github.com/google/deepvariant/issues/107:625,security,sign,significantly,625,"Since DeepVariant isn't trained on this data, it will depend on how different this new datatype is from our existing training data. For the training data we used, you can see: https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details-training-data.md. DeepVariant is known to be robust even on certain new data types. For example, when DNAnexus published this blog post, DeepVariant wasn't trained on NovaSeq, and yet DeepVariant performed well on NovaSeq data:. https://blog.dnanexus.com/2018-01-16-evaluating-the-performance-of-ngs-pipelines-on-noisy-wgs-data/. However, we also know that DeepVariant can be significantly more accurate if it has a chance to see the data in its training set. (For example, see: https://ai.googleblog.com/2018/04/deepvariant-accuracy-improvements-for.html). If you'd like to give it a try and let us know, that will be great, especially if you compare it to other baselines. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/107
https://github.com/google/deepvariant/issues/107:54,testability,depend,depend,54,"Since DeepVariant isn't trained on this data, it will depend on how different this new datatype is from our existing training data. For the training data we used, you can see: https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details-training-data.md. DeepVariant is known to be robust even on certain new data types. For example, when DNAnexus published this blog post, DeepVariant wasn't trained on NovaSeq, and yet DeepVariant performed well on NovaSeq data:. https://blog.dnanexus.com/2018-01-16-evaluating-the-performance-of-ngs-pipelines-on-noisy-wgs-data/. However, we also know that DeepVariant can be significantly more accurate if it has a chance to see the data in its training set. (For example, see: https://ai.googleblog.com/2018/04/deepvariant-accuracy-improvements-for.html). If you'd like to give it a try and let us know, that will be great, especially if you compare it to other baselines. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/107
https://github.com/google/deepvariant/issues/107:445,usability,perform,performed,445,"Since DeepVariant isn't trained on this data, it will depend on how different this new datatype is from our existing training data. For the training data we used, you can see: https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details-training-data.md. DeepVariant is known to be robust even on certain new data types. For example, when DNAnexus published this blog post, DeepVariant wasn't trained on NovaSeq, and yet DeepVariant performed well on NovaSeq data:. https://blog.dnanexus.com/2018-01-16-evaluating-the-performance-of-ngs-pipelines-on-noisy-wgs-data/. However, we also know that DeepVariant can be significantly more accurate if it has a chance to see the data in its training set. (For example, see: https://ai.googleblog.com/2018/04/deepvariant-accuracy-improvements-for.html). If you'd like to give it a try and let us know, that will be great, especially if you compare it to other baselines. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/107
https://github.com/google/deepvariant/issues/107:530,usability,perform,performance-of-ngs-pipelines-on-noisy-wgs-data,530,"Since DeepVariant isn't trained on this data, it will depend on how different this new datatype is from our existing training data. For the training data we used, you can see: https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details-training-data.md. DeepVariant is known to be robust even on certain new data types. For example, when DNAnexus published this blog post, DeepVariant wasn't trained on NovaSeq, and yet DeepVariant performed well on NovaSeq data:. https://blog.dnanexus.com/2018-01-16-evaluating-the-performance-of-ngs-pipelines-on-noisy-wgs-data/. However, we also know that DeepVariant can be significantly more accurate if it has a chance to see the data in its training set. (For example, see: https://ai.googleblog.com/2018/04/deepvariant-accuracy-improvements-for.html). If you'd like to give it a try and let us know, that will be great, especially if you compare it to other baselines. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/107
https://github.com/google/deepvariant/issues/107:5,usability,close,close,5,I'll close this issue if there's no follow up actions. Feel free to re-open if you have more questions.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/107
https://github.com/google/deepvariant/issues/108:22,usability,close,close,22,"Yup, no worries. I'll close this issue. If there are other questions, feel to file another one.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/108
https://github.com/google/deepvariant/issues/109:106,deployability,observ,observation,106,"Hi @ssm0808 ,. we also recently noticed similar things that we're actively investigating now. The current observation is that these missed calls are usually in a window where there are a few other variants nearby. [The way DeepVariant works](https://github.com/google/deepvariant#about-deepvariant) (full paper [here](https://rdcu.be/7Dhl)) is that candidate variants are generated first, and then a CNN model decides whether it thinks the candidates are really variants or not. In this case, we suspect that our CNN model currently might make more mistakes in the scenario where there are more variants in the picture. In your case, can you maybe share the other candidate variants near chrX:21886576? (maybe by pasting the results in the VCF that are near that line). That can help us understand whether this falls into the same category. We're actively looking to understand whether this is something we can improve on the training data, the input representation, or something else. Thank you for reporting this!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/109
https://github.com/google/deepvariant/issues/109:98,energy efficiency,current,current,98,"Hi @ssm0808 ,. we also recently noticed similar things that we're actively investigating now. The current observation is that these missed calls are usually in a window where there are a few other variants nearby. [The way DeepVariant works](https://github.com/google/deepvariant#about-deepvariant) (full paper [here](https://rdcu.be/7Dhl)) is that candidate variants are generated first, and then a CNN model decides whether it thinks the candidates are really variants or not. In this case, we suspect that our CNN model currently might make more mistakes in the scenario where there are more variants in the picture. In your case, can you maybe share the other candidate variants near chrX:21886576? (maybe by pasting the results in the VCF that are near that line). That can help us understand whether this falls into the same category. We're actively looking to understand whether this is something we can improve on the training data, the input representation, or something else. Thank you for reporting this!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/109
https://github.com/google/deepvariant/issues/109:404,energy efficiency,model,model,404,"Hi @ssm0808 ,. we also recently noticed similar things that we're actively investigating now. The current observation is that these missed calls are usually in a window where there are a few other variants nearby. [The way DeepVariant works](https://github.com/google/deepvariant#about-deepvariant) (full paper [here](https://rdcu.be/7Dhl)) is that candidate variants are generated first, and then a CNN model decides whether it thinks the candidates are really variants or not. In this case, we suspect that our CNN model currently might make more mistakes in the scenario where there are more variants in the picture. In your case, can you maybe share the other candidate variants near chrX:21886576? (maybe by pasting the results in the VCF that are near that line). That can help us understand whether this falls into the same category. We're actively looking to understand whether this is something we can improve on the training data, the input representation, or something else. Thank you for reporting this!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/109
https://github.com/google/deepvariant/issues/109:517,energy efficiency,model,model,517,"Hi @ssm0808 ,. we also recently noticed similar things that we're actively investigating now. The current observation is that these missed calls are usually in a window where there are a few other variants nearby. [The way DeepVariant works](https://github.com/google/deepvariant#about-deepvariant) (full paper [here](https://rdcu.be/7Dhl)) is that candidate variants are generated first, and then a CNN model decides whether it thinks the candidates are really variants or not. In this case, we suspect that our CNN model currently might make more mistakes in the scenario where there are more variants in the picture. In your case, can you maybe share the other candidate variants near chrX:21886576? (maybe by pasting the results in the VCF that are near that line). That can help us understand whether this falls into the same category. We're actively looking to understand whether this is something we can improve on the training data, the input representation, or something else. Thank you for reporting this!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/109
https://github.com/google/deepvariant/issues/109:523,energy efficiency,current,currently,523,"Hi @ssm0808 ,. we also recently noticed similar things that we're actively investigating now. The current observation is that these missed calls are usually in a window where there are a few other variants nearby. [The way DeepVariant works](https://github.com/google/deepvariant#about-deepvariant) (full paper [here](https://rdcu.be/7Dhl)) is that candidate variants are generated first, and then a CNN model decides whether it thinks the candidates are really variants or not. In this case, we suspect that our CNN model currently might make more mistakes in the scenario where there are more variants in the picture. In your case, can you maybe share the other candidate variants near chrX:21886576? (maybe by pasting the results in the VCF that are near that line). That can help us understand whether this falls into the same category. We're actively looking to understand whether this is something we can improve on the training data, the input representation, or something else. Thank you for reporting this!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/109
https://github.com/google/deepvariant/issues/109:648,interoperability,share,share,648,"Hi @ssm0808 ,. we also recently noticed similar things that we're actively investigating now. The current observation is that these missed calls are usually in a window where there are a few other variants nearby. [The way DeepVariant works](https://github.com/google/deepvariant#about-deepvariant) (full paper [here](https://rdcu.be/7Dhl)) is that candidate variants are generated first, and then a CNN model decides whether it thinks the candidates are really variants or not. In this case, we suspect that our CNN model currently might make more mistakes in the scenario where there are more variants in the picture. In your case, can you maybe share the other candidate variants near chrX:21886576? (maybe by pasting the results in the VCF that are near that line). That can help us understand whether this falls into the same category. We're actively looking to understand whether this is something we can improve on the training data, the input representation, or something else. Thank you for reporting this!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/109
https://github.com/google/deepvariant/issues/109:565,modifiability,scenario,scenario,565,"Hi @ssm0808 ,. we also recently noticed similar things that we're actively investigating now. The current observation is that these missed calls are usually in a window where there are a few other variants nearby. [The way DeepVariant works](https://github.com/google/deepvariant#about-deepvariant) (full paper [here](https://rdcu.be/7Dhl)) is that candidate variants are generated first, and then a CNN model decides whether it thinks the candidates are really variants or not. In this case, we suspect that our CNN model currently might make more mistakes in the scenario where there are more variants in the picture. In your case, can you maybe share the other candidate variants near chrX:21886576? (maybe by pasting the results in the VCF that are near that line). That can help us understand whether this falls into the same category. We're actively looking to understand whether this is something we can improve on the training data, the input representation, or something else. Thank you for reporting this!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/109
https://github.com/google/deepvariant/issues/109:945,safety,input,input,945,"Hi @ssm0808 ,. we also recently noticed similar things that we're actively investigating now. The current observation is that these missed calls are usually in a window where there are a few other variants nearby. [The way DeepVariant works](https://github.com/google/deepvariant#about-deepvariant) (full paper [here](https://rdcu.be/7Dhl)) is that candidate variants are generated first, and then a CNN model decides whether it thinks the candidates are really variants or not. In this case, we suspect that our CNN model currently might make more mistakes in the scenario where there are more variants in the picture. In your case, can you maybe share the other candidate variants near chrX:21886576? (maybe by pasting the results in the VCF that are near that line). That can help us understand whether this falls into the same category. We're actively looking to understand whether this is something we can improve on the training data, the input representation, or something else. Thank you for reporting this!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/109
https://github.com/google/deepvariant/issues/109:404,security,model,model,404,"Hi @ssm0808 ,. we also recently noticed similar things that we're actively investigating now. The current observation is that these missed calls are usually in a window where there are a few other variants nearby. [The way DeepVariant works](https://github.com/google/deepvariant#about-deepvariant) (full paper [here](https://rdcu.be/7Dhl)) is that candidate variants are generated first, and then a CNN model decides whether it thinks the candidates are really variants or not. In this case, we suspect that our CNN model currently might make more mistakes in the scenario where there are more variants in the picture. In your case, can you maybe share the other candidate variants near chrX:21886576? (maybe by pasting the results in the VCF that are near that line). That can help us understand whether this falls into the same category. We're actively looking to understand whether this is something we can improve on the training data, the input representation, or something else. Thank you for reporting this!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/109
https://github.com/google/deepvariant/issues/109:517,security,model,model,517,"Hi @ssm0808 ,. we also recently noticed similar things that we're actively investigating now. The current observation is that these missed calls are usually in a window where there are a few other variants nearby. [The way DeepVariant works](https://github.com/google/deepvariant#about-deepvariant) (full paper [here](https://rdcu.be/7Dhl)) is that candidate variants are generated first, and then a CNN model decides whether it thinks the candidates are really variants or not. In this case, we suspect that our CNN model currently might make more mistakes in the scenario where there are more variants in the picture. In your case, can you maybe share the other candidate variants near chrX:21886576? (maybe by pasting the results in the VCF that are near that line). That can help us understand whether this falls into the same category. We're actively looking to understand whether this is something we can improve on the training data, the input representation, or something else. Thank you for reporting this!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/109
https://github.com/google/deepvariant/issues/109:106,testability,observ,observation,106,"Hi @ssm0808 ,. we also recently noticed similar things that we're actively investigating now. The current observation is that these missed calls are usually in a window where there are a few other variants nearby. [The way DeepVariant works](https://github.com/google/deepvariant#about-deepvariant) (full paper [here](https://rdcu.be/7Dhl)) is that candidate variants are generated first, and then a CNN model decides whether it thinks the candidates are really variants or not. In this case, we suspect that our CNN model currently might make more mistakes in the scenario where there are more variants in the picture. In your case, can you maybe share the other candidate variants near chrX:21886576? (maybe by pasting the results in the VCF that are near that line). That can help us understand whether this falls into the same category. We're actively looking to understand whether this is something we can improve on the training data, the input representation, or something else. Thank you for reporting this!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/109
https://github.com/google/deepvariant/issues/109:787,testability,understand,understand,787,"Hi @ssm0808 ,. we also recently noticed similar things that we're actively investigating now. The current observation is that these missed calls are usually in a window where there are a few other variants nearby. [The way DeepVariant works](https://github.com/google/deepvariant#about-deepvariant) (full paper [here](https://rdcu.be/7Dhl)) is that candidate variants are generated first, and then a CNN model decides whether it thinks the candidates are really variants or not. In this case, we suspect that our CNN model currently might make more mistakes in the scenario where there are more variants in the picture. In your case, can you maybe share the other candidate variants near chrX:21886576? (maybe by pasting the results in the VCF that are near that line). That can help us understand whether this falls into the same category. We're actively looking to understand whether this is something we can improve on the training data, the input representation, or something else. Thank you for reporting this!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/109
https://github.com/google/deepvariant/issues/109:867,testability,understand,understand,867,"Hi @ssm0808 ,. we also recently noticed similar things that we're actively investigating now. The current observation is that these missed calls are usually in a window where there are a few other variants nearby. [The way DeepVariant works](https://github.com/google/deepvariant#about-deepvariant) (full paper [here](https://rdcu.be/7Dhl)) is that candidate variants are generated first, and then a CNN model decides whether it thinks the candidates are really variants or not. In this case, we suspect that our CNN model currently might make more mistakes in the scenario where there are more variants in the picture. In your case, can you maybe share the other candidate variants near chrX:21886576? (maybe by pasting the results in the VCF that are near that line). That can help us understand whether this falls into the same category. We're actively looking to understand whether this is something we can improve on the training data, the input representation, or something else. Thank you for reporting this!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/109
https://github.com/google/deepvariant/issues/109:779,usability,help,help,779,"Hi @ssm0808 ,. we also recently noticed similar things that we're actively investigating now. The current observation is that these missed calls are usually in a window where there are a few other variants nearby. [The way DeepVariant works](https://github.com/google/deepvariant#about-deepvariant) (full paper [here](https://rdcu.be/7Dhl)) is that candidate variants are generated first, and then a CNN model decides whether it thinks the candidates are really variants or not. In this case, we suspect that our CNN model currently might make more mistakes in the scenario where there are more variants in the picture. In your case, can you maybe share the other candidate variants near chrX:21886576? (maybe by pasting the results in the VCF that are near that line). That can help us understand whether this falls into the same category. We're actively looking to understand whether this is something we can improve on the training data, the input representation, or something else. Thank you for reporting this!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/109
https://github.com/google/deepvariant/issues/109:945,usability,input,input,945,"Hi @ssm0808 ,. we also recently noticed similar things that we're actively investigating now. The current observation is that these missed calls are usually in a window where there are a few other variants nearby. [The way DeepVariant works](https://github.com/google/deepvariant#about-deepvariant) (full paper [here](https://rdcu.be/7Dhl)) is that candidate variants are generated first, and then a CNN model decides whether it thinks the candidates are really variants or not. In this case, we suspect that our CNN model currently might make more mistakes in the scenario where there are more variants in the picture. In your case, can you maybe share the other candidate variants near chrX:21886576? (maybe by pasting the results in the VCF that are near that line). That can help us understand whether this falls into the same category. We're actively looking to understand whether this is something we can improve on the training data, the input representation, or something else. Thank you for reporting this!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/109
https://github.com/google/deepvariant/issues/109:113,deployability,build,building,113,@pichuan Can you inspect more closely the convolutions in inception v3 to trace out what is happening? How about building an ensemble model where the prediction runs through multiple learned models by either majority vote or a more fine-grained decision boundary.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/109
https://github.com/google/deepvariant/issues/109:134,energy efficiency,model,model,134,@pichuan Can you inspect more closely the convolutions in inception v3 to trace out what is happening? How about building an ensemble model where the prediction runs through multiple learned models by either majority vote or a more fine-grained decision boundary.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/109
https://github.com/google/deepvariant/issues/109:150,energy efficiency,predict,prediction,150,@pichuan Can you inspect more closely the convolutions in inception v3 to trace out what is happening? How about building an ensemble model where the prediction runs through multiple learned models by either majority vote or a more fine-grained decision boundary.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/109
https://github.com/google/deepvariant/issues/109:191,energy efficiency,model,models,191,@pichuan Can you inspect more closely the convolutions in inception v3 to trace out what is happening? How about building an ensemble model where the prediction runs through multiple learned models by either majority vote or a more fine-grained decision boundary.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/109
https://github.com/google/deepvariant/issues/109:150,safety,predict,prediction,150,@pichuan Can you inspect more closely the convolutions in inception v3 to trace out what is happening? How about building an ensemble model where the prediction runs through multiple learned models by either majority vote or a more fine-grained decision boundary.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/109
https://github.com/google/deepvariant/issues/109:134,security,model,model,134,@pichuan Can you inspect more closely the convolutions in inception v3 to trace out what is happening? How about building an ensemble model where the prediction runs through multiple learned models by either majority vote or a more fine-grained decision boundary.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/109
https://github.com/google/deepvariant/issues/109:191,security,model,models,191,@pichuan Can you inspect more closely the convolutions in inception v3 to trace out what is happening? How about building an ensemble model where the prediction runs through multiple learned models by either majority vote or a more fine-grained decision boundary.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/109
https://github.com/google/deepvariant/issues/109:74,testability,trace,trace,74,@pichuan Can you inspect more closely the convolutions in inception v3 to trace out what is happening? How about building an ensemble model where the prediction runs through multiple learned models by either majority vote or a more fine-grained decision boundary.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/109
https://github.com/google/deepvariant/issues/109:30,usability,close,closely,30,@pichuan Can you inspect more closely the convolutions in inception v3 to trace out what is happening? How about building an ensemble model where the prediction runs through multiple learned models by either majority vote or a more fine-grained decision boundary.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/109
https://github.com/google/deepvariant/issues/109:183,usability,learn,learned,183,@pichuan Can you inspect more closely the convolutions in inception v3 to trace out what is happening? How about building an ensemble model where the prediction runs through multiple learned models by either majority vote or a more fine-grained decision boundary.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/109
https://github.com/google/deepvariant/issues/109:691,deployability,updat,update,691,"@pgrosu . Thanks for the suggestion! We've had the idea of using ensemble model for a while, but haven't really done it because it also means that the inference cost will go up linearly. In this case, we have a few ideas about understanding how confident the model is. . It will require some effort to investigate. Once we have more understanding of the core issue, we'll try to come up with a way to improve the model. And I'm also hoping that we'll share the finding here later as well. . This might take a while though. (More than what we'll need to answer usual github issues :)). But this is definitely a priority for us and we'll start very soon. I'll set a reminder for us to give an update later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/109
https://github.com/google/deepvariant/issues/109:74,energy efficiency,model,model,74,"@pgrosu . Thanks for the suggestion! We've had the idea of using ensemble model for a while, but haven't really done it because it also means that the inference cost will go up linearly. In this case, we have a few ideas about understanding how confident the model is. . It will require some effort to investigate. Once we have more understanding of the core issue, we'll try to come up with a way to improve the model. And I'm also hoping that we'll share the finding here later as well. . This might take a while though. (More than what we'll need to answer usual github issues :)). But this is definitely a priority for us and we'll start very soon. I'll set a reminder for us to give an update later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/109
https://github.com/google/deepvariant/issues/109:259,energy efficiency,model,model,259,"@pgrosu . Thanks for the suggestion! We've had the idea of using ensemble model for a while, but haven't really done it because it also means that the inference cost will go up linearly. In this case, we have a few ideas about understanding how confident the model is. . It will require some effort to investigate. Once we have more understanding of the core issue, we'll try to come up with a way to improve the model. And I'm also hoping that we'll share the finding here later as well. . This might take a while though. (More than what we'll need to answer usual github issues :)). But this is definitely a priority for us and we'll start very soon. I'll set a reminder for us to give an update later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/109
https://github.com/google/deepvariant/issues/109:354,energy efficiency,core,core,354,"@pgrosu . Thanks for the suggestion! We've had the idea of using ensemble model for a while, but haven't really done it because it also means that the inference cost will go up linearly. In this case, we have a few ideas about understanding how confident the model is. . It will require some effort to investigate. Once we have more understanding of the core issue, we'll try to come up with a way to improve the model. And I'm also hoping that we'll share the finding here later as well. . This might take a while though. (More than what we'll need to answer usual github issues :)). But this is definitely a priority for us and we'll start very soon. I'll set a reminder for us to give an update later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/109
https://github.com/google/deepvariant/issues/109:413,energy efficiency,model,model,413,"@pgrosu . Thanks for the suggestion! We've had the idea of using ensemble model for a while, but haven't really done it because it also means that the inference cost will go up linearly. In this case, we have a few ideas about understanding how confident the model is. . It will require some effort to investigate. Once we have more understanding of the core issue, we'll try to come up with a way to improve the model. And I'm also hoping that we'll share the finding here later as well. . This might take a while though. (More than what we'll need to answer usual github issues :)). But this is definitely a priority for us and we'll start very soon. I'll set a reminder for us to give an update later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/109
https://github.com/google/deepvariant/issues/109:451,interoperability,share,share,451,"@pgrosu . Thanks for the suggestion! We've had the idea of using ensemble model for a while, but haven't really done it because it also means that the inference cost will go up linearly. In this case, we have a few ideas about understanding how confident the model is. . It will require some effort to investigate. Once we have more understanding of the core issue, we'll try to come up with a way to improve the model. And I'm also hoping that we'll share the finding here later as well. . This might take a while though. (More than what we'll need to answer usual github issues :)). But this is definitely a priority for us and we'll start very soon. I'll set a reminder for us to give an update later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/109
https://github.com/google/deepvariant/issues/109:691,safety,updat,update,691,"@pgrosu . Thanks for the suggestion! We've had the idea of using ensemble model for a while, but haven't really done it because it also means that the inference cost will go up linearly. In this case, we have a few ideas about understanding how confident the model is. . It will require some effort to investigate. Once we have more understanding of the core issue, we'll try to come up with a way to improve the model. And I'm also hoping that we'll share the finding here later as well. . This might take a while though. (More than what we'll need to answer usual github issues :)). But this is definitely a priority for us and we'll start very soon. I'll set a reminder for us to give an update later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/109
https://github.com/google/deepvariant/issues/109:74,security,model,model,74,"@pgrosu . Thanks for the suggestion! We've had the idea of using ensemble model for a while, but haven't really done it because it also means that the inference cost will go up linearly. In this case, we have a few ideas about understanding how confident the model is. . It will require some effort to investigate. Once we have more understanding of the core issue, we'll try to come up with a way to improve the model. And I'm also hoping that we'll share the finding here later as well. . This might take a while though. (More than what we'll need to answer usual github issues :)). But this is definitely a priority for us and we'll start very soon. I'll set a reminder for us to give an update later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/109
https://github.com/google/deepvariant/issues/109:259,security,model,model,259,"@pgrosu . Thanks for the suggestion! We've had the idea of using ensemble model for a while, but haven't really done it because it also means that the inference cost will go up linearly. In this case, we have a few ideas about understanding how confident the model is. . It will require some effort to investigate. Once we have more understanding of the core issue, we'll try to come up with a way to improve the model. And I'm also hoping that we'll share the finding here later as well. . This might take a while though. (More than what we'll need to answer usual github issues :)). But this is definitely a priority for us and we'll start very soon. I'll set a reminder for us to give an update later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/109
https://github.com/google/deepvariant/issues/109:413,security,model,model,413,"@pgrosu . Thanks for the suggestion! We've had the idea of using ensemble model for a while, but haven't really done it because it also means that the inference cost will go up linearly. In this case, we have a few ideas about understanding how confident the model is. . It will require some effort to investigate. Once we have more understanding of the core issue, we'll try to come up with a way to improve the model. And I'm also hoping that we'll share the finding here later as well. . This might take a while though. (More than what we'll need to answer usual github issues :)). But this is definitely a priority for us and we'll start very soon. I'll set a reminder for us to give an update later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/109
https://github.com/google/deepvariant/issues/109:691,security,updat,update,691,"@pgrosu . Thanks for the suggestion! We've had the idea of using ensemble model for a while, but haven't really done it because it also means that the inference cost will go up linearly. In this case, we have a few ideas about understanding how confident the model is. . It will require some effort to investigate. Once we have more understanding of the core issue, we'll try to come up with a way to improve the model. And I'm also hoping that we'll share the finding here later as well. . This might take a while though. (More than what we'll need to answer usual github issues :)). But this is definitely a priority for us and we'll start very soon. I'll set a reminder for us to give an update later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/109
https://github.com/google/deepvariant/issues/109:227,testability,understand,understanding,227,"@pgrosu . Thanks for the suggestion! We've had the idea of using ensemble model for a while, but haven't really done it because it also means that the inference cost will go up linearly. In this case, we have a few ideas about understanding how confident the model is. . It will require some effort to investigate. Once we have more understanding of the core issue, we'll try to come up with a way to improve the model. And I'm also hoping that we'll share the finding here later as well. . This might take a while though. (More than what we'll need to answer usual github issues :)). But this is definitely a priority for us and we'll start very soon. I'll set a reminder for us to give an update later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/109
https://github.com/google/deepvariant/issues/109:333,testability,understand,understanding,333,"@pgrosu . Thanks for the suggestion! We've had the idea of using ensemble model for a while, but haven't really done it because it also means that the inference cost will go up linearly. In this case, we have a few ideas about understanding how confident the model is. . It will require some effort to investigate. Once we have more understanding of the core issue, we'll try to come up with a way to improve the model. And I'm also hoping that we'll share the finding here later as well. . This might take a while though. (More than what we'll need to answer usual github issues :)). But this is definitely a priority for us and we'll start very soon. I'll set a reminder for us to give an update later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/109
https://github.com/google/deepvariant/issues/109:41,availability,avail,available,41,"@pichuan That sounds reasonable, and I'm available if your team needs extra cycles with investigating this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/109
https://github.com/google/deepvariant/issues/109:41,reliability,availab,available,41,"@pichuan That sounds reasonable, and I'm available if your team needs extra cycles with investigating this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/109
https://github.com/google/deepvariant/issues/109:41,safety,avail,available,41,"@pichuan That sounds reasonable, and I'm available if your team needs extra cycles with investigating this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/109
https://github.com/google/deepvariant/issues/109:41,security,availab,available,41,"@pichuan That sounds reasonable, and I'm available if your team needs extra cycles with investigating this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/109
https://github.com/google/deepvariant/issues/109:59,security,team,team,59,"@pichuan That sounds reasonable, and I'm available if your team needs extra cycles with investigating this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/109
https://github.com/google/deepvariant/issues/109:483,availability,robust,robust,483,"@ssm0808 . Looking at the VCF file, it actually doesn't look like there are variants nearby. So it's not related to the issue I mentioned before. If this data is sharable, it'll certainly be helpful if you can share the data (just chrX). An IGV screenshot at that location can also be helpful. Otherwise, I'm not sure what's a best way that we can help diagnose this issue. Let me know if you're able to provide more information. If not, I hope that one day our future model will be robust enough for your data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/109
https://github.com/google/deepvariant/issues/109:469,energy efficiency,model,model,469,"@ssm0808 . Looking at the VCF file, it actually doesn't look like there are variants nearby. So it's not related to the issue I mentioned before. If this data is sharable, it'll certainly be helpful if you can share the data (just chrX). An IGV screenshot at that location can also be helpful. Otherwise, I'm not sure what's a best way that we can help diagnose this issue. Let me know if you're able to provide more information. If not, I hope that one day our future model will be robust enough for your data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/109
https://github.com/google/deepvariant/issues/109:210,interoperability,share,share,210,"@ssm0808 . Looking at the VCF file, it actually doesn't look like there are variants nearby. So it's not related to the issue I mentioned before. If this data is sharable, it'll certainly be helpful if you can share the data (just chrX). An IGV screenshot at that location can also be helpful. Otherwise, I'm not sure what's a best way that we can help diagnose this issue. Let me know if you're able to provide more information. If not, I hope that one day our future model will be robust enough for your data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/109
https://github.com/google/deepvariant/issues/109:48,reliability,doe,doesn,48,"@ssm0808 . Looking at the VCF file, it actually doesn't look like there are variants nearby. So it's not related to the issue I mentioned before. If this data is sharable, it'll certainly be helpful if you can share the data (just chrX). An IGV screenshot at that location can also be helpful. Otherwise, I'm not sure what's a best way that we can help diagnose this issue. Let me know if you're able to provide more information. If not, I hope that one day our future model will be robust enough for your data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/109
https://github.com/google/deepvariant/issues/109:353,reliability,diagno,diagnose,353,"@ssm0808 . Looking at the VCF file, it actually doesn't look like there are variants nearby. So it's not related to the issue I mentioned before. If this data is sharable, it'll certainly be helpful if you can share the data (just chrX). An IGV screenshot at that location can also be helpful. Otherwise, I'm not sure what's a best way that we can help diagnose this issue. Let me know if you're able to provide more information. If not, I hope that one day our future model will be robust enough for your data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/109
https://github.com/google/deepvariant/issues/109:483,reliability,robust,robust,483,"@ssm0808 . Looking at the VCF file, it actually doesn't look like there are variants nearby. So it's not related to the issue I mentioned before. If this data is sharable, it'll certainly be helpful if you can share the data (just chrX). An IGV screenshot at that location can also be helpful. Otherwise, I'm not sure what's a best way that we can help diagnose this issue. Let me know if you're able to provide more information. If not, I hope that one day our future model will be robust enough for your data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/109
https://github.com/google/deepvariant/issues/109:483,safety,robust,robust,483,"@ssm0808 . Looking at the VCF file, it actually doesn't look like there are variants nearby. So it's not related to the issue I mentioned before. If this data is sharable, it'll certainly be helpful if you can share the data (just chrX). An IGV screenshot at that location can also be helpful. Otherwise, I'm not sure what's a best way that we can help diagnose this issue. Let me know if you're able to provide more information. If not, I hope that one day our future model will be robust enough for your data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/109
https://github.com/google/deepvariant/issues/109:469,security,model,model,469,"@ssm0808 . Looking at the VCF file, it actually doesn't look like there are variants nearby. So it's not related to the issue I mentioned before. If this data is sharable, it'll certainly be helpful if you can share the data (just chrX). An IGV screenshot at that location can also be helpful. Otherwise, I'm not sure what's a best way that we can help diagnose this issue. Let me know if you're able to provide more information. If not, I hope that one day our future model will be robust enough for your data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/109
https://github.com/google/deepvariant/issues/109:353,testability,diagno,diagnose,353,"@ssm0808 . Looking at the VCF file, it actually doesn't look like there are variants nearby. So it's not related to the issue I mentioned before. If this data is sharable, it'll certainly be helpful if you can share the data (just chrX). An IGV screenshot at that location can also be helpful. Otherwise, I'm not sure what's a best way that we can help diagnose this issue. Let me know if you're able to provide more information. If not, I hope that one day our future model will be robust enough for your data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/109
https://github.com/google/deepvariant/issues/109:191,usability,help,helpful,191,"@ssm0808 . Looking at the VCF file, it actually doesn't look like there are variants nearby. So it's not related to the issue I mentioned before. If this data is sharable, it'll certainly be helpful if you can share the data (just chrX). An IGV screenshot at that location can also be helpful. Otherwise, I'm not sure what's a best way that we can help diagnose this issue. Let me know if you're able to provide more information. If not, I hope that one day our future model will be robust enough for your data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/109
https://github.com/google/deepvariant/issues/109:285,usability,help,helpful,285,"@ssm0808 . Looking at the VCF file, it actually doesn't look like there are variants nearby. So it's not related to the issue I mentioned before. If this data is sharable, it'll certainly be helpful if you can share the data (just chrX). An IGV screenshot at that location can also be helpful. Otherwise, I'm not sure what's a best way that we can help diagnose this issue. Let me know if you're able to provide more information. If not, I hope that one day our future model will be robust enough for your data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/109
https://github.com/google/deepvariant/issues/109:348,usability,help,help,348,"@ssm0808 . Looking at the VCF file, it actually doesn't look like there are variants nearby. So it's not related to the issue I mentioned before. If this data is sharable, it'll certainly be helpful if you can share the data (just chrX). An IGV screenshot at that location can also be helpful. Otherwise, I'm not sure what's a best way that we can help diagnose this issue. Let me know if you're able to provide more information. If not, I hope that one day our future model will be robust enough for your data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/109
https://github.com/google/deepvariant/issues/109:36,interoperability,share,share,36,"Hi @fengli-eGen . Is it possible to share a snippet of the BAM file so we could take a closer look. That might help us understand what reason DeepVariant might have to not call the variant (or for us to conclude that it should have). Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/109
https://github.com/google/deepvariant/issues/109:119,testability,understand,understand,119,"Hi @fengli-eGen . Is it possible to share a snippet of the BAM file so we could take a closer look. That might help us understand what reason DeepVariant might have to not call the variant (or for us to conclude that it should have). Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/109
https://github.com/google/deepvariant/issues/109:87,usability,close,closer,87,"Hi @fengli-eGen . Is it possible to share a snippet of the BAM file so we could take a closer look. That might help us understand what reason DeepVariant might have to not call the variant (or for us to conclude that it should have). Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/109
https://github.com/google/deepvariant/issues/109:111,usability,help,help,111,"Hi @fengli-eGen . Is it possible to share a snippet of the BAM file so we could take a closer look. That might help us understand what reason DeepVariant might have to not call the variant (or for us to conclude that it should have). Thank you,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/109
https://github.com/google/deepvariant/issues/109:114,availability,error,error,114,"Hi Andrew,. Thank you for your response. I de-identified the BAM file and wanna upload it here, but I received an error that ""We dont support that file type."" Is there a way to send it? I appreciate your time and help! Best,. Feng.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/109
https://github.com/google/deepvariant/issues/109:114,performance,error,error,114,"Hi Andrew,. Thank you for your response. I de-identified the BAM file and wanna upload it here, but I received an error that ""We dont support that file type."" Is there a way to send it? I appreciate your time and help! Best,. Feng.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/109
https://github.com/google/deepvariant/issues/109:205,performance,time,time,205,"Hi Andrew,. Thank you for your response. I de-identified the BAM file and wanna upload it here, but I received an error that ""We dont support that file type."" Is there a way to send it? I appreciate your time and help! Best,. Feng.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/109
https://github.com/google/deepvariant/issues/109:114,safety,error,error,114,"Hi Andrew,. Thank you for your response. I de-identified the BAM file and wanna upload it here, but I received an error that ""We dont support that file type."" Is there a way to send it? I appreciate your time and help! Best,. Feng.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/109
https://github.com/google/deepvariant/issues/109:46,security,ident,identified,46,"Hi Andrew,. Thank you for your response. I de-identified the BAM file and wanna upload it here, but I received an error that ""We dont support that file type."" Is there a way to send it? I appreciate your time and help! Best,. Feng.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/109
https://github.com/google/deepvariant/issues/109:114,usability,error,error,114,"Hi Andrew,. Thank you for your response. I de-identified the BAM file and wanna upload it here, but I received an error that ""We dont support that file type."" Is there a way to send it? I appreciate your time and help! Best,. Feng.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/109
https://github.com/google/deepvariant/issues/109:135,usability,support,support,135,"Hi Andrew,. Thank you for your response. I de-identified the BAM file and wanna upload it here, but I received an error that ""We dont support that file type."" Is there a way to send it? I appreciate your time and help! Best,. Feng.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/109
https://github.com/google/deepvariant/issues/109:214,usability,help,help,214,"Hi Andrew,. Thank you for your response. I de-identified the BAM file and wanna upload it here, but I received an error that ""We dont support that file type."" Is there a way to send it? I appreciate your time and help! Best,. Feng.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/109
https://github.com/google/deepvariant/issues/109:146,security,access,access,146,I upload it to Google Drive: https://drive.google.com/file/d/1aZnuOlCpcDhudfJa28XhvUggOLHuJAyv/view?usp=sharing. Please let me know if you cannot access it. Thanks Andrew!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/109
https://github.com/google/deepvariant/issues/110:106,deployability,releas,release,106,"Hi,. I found a bug in the visualizing ipynb and fixed it internally already. It will come out in the next release (which I'm hoping will be soon). For now, please update the `channels_to_rgb` function to this:. ```. def channels_to_rgb(channels):. # Reconstruct the original channels. base = channels[0]. qual = np.minimum(channels[1], channels[2]). strand = channels[3]. alpha = np.multiply(. channels[4] / 254.0,. channels[5] / 254.0). return np.multiply(. np.stack([base, qual, strand]),. alpha).astype(np.uint8).transpose([1, 2, 0]). ```. It was actually a pretty obvious mistake. I wish I unit tested my notebook.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/110
https://github.com/google/deepvariant/issues/110:163,deployability,updat,update,163,"Hi,. I found a bug in the visualizing ipynb and fixed it internally already. It will come out in the next release (which I'm hoping will be soon). For now, please update the `channels_to_rgb` function to this:. ```. def channels_to_rgb(channels):. # Reconstruct the original channels. base = channels[0]. qual = np.minimum(channels[1], channels[2]). strand = channels[3]. alpha = np.multiply(. channels[4] / 254.0,. channels[5] / 254.0). return np.multiply(. np.stack([base, qual, strand]),. alpha).astype(np.uint8).transpose([1, 2, 0]). ```. It was actually a pretty obvious mistake. I wish I unit tested my notebook.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/110
https://github.com/google/deepvariant/issues/110:462,deployability,stack,stack,462,"Hi,. I found a bug in the visualizing ipynb and fixed it internally already. It will come out in the next release (which I'm hoping will be soon). For now, please update the `channels_to_rgb` function to this:. ```. def channels_to_rgb(channels):. # Reconstruct the original channels. base = channels[0]. qual = np.minimum(channels[1], channels[2]). strand = channels[3]. alpha = np.multiply(. channels[4] / 254.0,. channels[5] / 254.0). return np.multiply(. np.stack([base, qual, strand]),. alpha).astype(np.uint8).transpose([1, 2, 0]). ```. It was actually a pretty obvious mistake. I wish I unit tested my notebook.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/110
https://github.com/google/deepvariant/issues/110:163,safety,updat,update,163,"Hi,. I found a bug in the visualizing ipynb and fixed it internally already. It will come out in the next release (which I'm hoping will be soon). For now, please update the `channels_to_rgb` function to this:. ```. def channels_to_rgb(channels):. # Reconstruct the original channels. base = channels[0]. qual = np.minimum(channels[1], channels[2]). strand = channels[3]. alpha = np.multiply(. channels[4] / 254.0,. channels[5] / 254.0). return np.multiply(. np.stack([base, qual, strand]),. alpha).astype(np.uint8).transpose([1, 2, 0]). ```. It was actually a pretty obvious mistake. I wish I unit tested my notebook.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/110
https://github.com/google/deepvariant/issues/110:599,safety,test,tested,599,"Hi,. I found a bug in the visualizing ipynb and fixed it internally already. It will come out in the next release (which I'm hoping will be soon). For now, please update the `channels_to_rgb` function to this:. ```. def channels_to_rgb(channels):. # Reconstruct the original channels. base = channels[0]. qual = np.minimum(channels[1], channels[2]). strand = channels[3]. alpha = np.multiply(. channels[4] / 254.0,. channels[5] / 254.0). return np.multiply(. np.stack([base, qual, strand]),. alpha).astype(np.uint8).transpose([1, 2, 0]). ```. It was actually a pretty obvious mistake. I wish I unit tested my notebook.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/110
https://github.com/google/deepvariant/issues/110:163,security,updat,update,163,"Hi,. I found a bug in the visualizing ipynb and fixed it internally already. It will come out in the next release (which I'm hoping will be soon). For now, please update the `channels_to_rgb` function to this:. ```. def channels_to_rgb(channels):. # Reconstruct the original channels. base = channels[0]. qual = np.minimum(channels[1], channels[2]). strand = channels[3]. alpha = np.multiply(. channels[4] / 254.0,. channels[5] / 254.0). return np.multiply(. np.stack([base, qual, strand]),. alpha).astype(np.uint8).transpose([1, 2, 0]). ```. It was actually a pretty obvious mistake. I wish I unit tested my notebook.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/110
https://github.com/google/deepvariant/issues/110:594,testability,unit,unit,594,"Hi,. I found a bug in the visualizing ipynb and fixed it internally already. It will come out in the next release (which I'm hoping will be soon). For now, please update the `channels_to_rgb` function to this:. ```. def channels_to_rgb(channels):. # Reconstruct the original channels. base = channels[0]. qual = np.minimum(channels[1], channels[2]). strand = channels[3]. alpha = np.multiply(. channels[4] / 254.0,. channels[5] / 254.0). return np.multiply(. np.stack([base, qual, strand]),. alpha).astype(np.uint8).transpose([1, 2, 0]). ```. It was actually a pretty obvious mistake. I wish I unit tested my notebook.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/110
https://github.com/google/deepvariant/issues/110:599,testability,test,tested,599,"Hi,. I found a bug in the visualizing ipynb and fixed it internally already. It will come out in the next release (which I'm hoping will be soon). For now, please update the `channels_to_rgb` function to this:. ```. def channels_to_rgb(channels):. # Reconstruct the original channels. base = channels[0]. qual = np.minimum(channels[1], channels[2]). strand = channels[3]. alpha = np.multiply(. channels[4] / 254.0,. channels[5] / 254.0). return np.multiply(. np.stack([base, qual, strand]),. alpha).astype(np.uint8).transpose([1, 2, 0]). ```. It was actually a pretty obvious mistake. I wish I unit tested my notebook.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/110
https://github.com/google/deepvariant/issues/110:26,usability,visual,visualizing,26,"Hi,. I found a bug in the visualizing ipynb and fixed it internally already. It will come out in the next release (which I'm hoping will be soon). For now, please update the `channels_to_rgb` function to this:. ```. def channels_to_rgb(channels):. # Reconstruct the original channels. base = channels[0]. qual = np.minimum(channels[1], channels[2]). strand = channels[3]. alpha = np.multiply(. channels[4] / 254.0,. channels[5] / 254.0). return np.multiply(. np.stack([base, qual, strand]),. alpha).astype(np.uint8).transpose([1, 2, 0]). ```. It was actually a pretty obvious mistake. I wish I unit tested my notebook.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/110
https://github.com/google/deepvariant/issues/110:315,usability,minim,minimum,315,"Hi,. I found a bug in the visualizing ipynb and fixed it internally already. It will come out in the next release (which I'm hoping will be soon). For now, please update the `channels_to_rgb` function to this:. ```. def channels_to_rgb(channels):. # Reconstruct the original channels. base = channels[0]. qual = np.minimum(channels[1], channels[2]). strand = channels[3]. alpha = np.multiply(. channels[4] / 254.0,. channels[5] / 254.0). return np.multiply(. np.stack([base, qual, strand]),. alpha).astype(np.uint8).transpose([1, 2, 0]). ```. It was actually a pretty obvious mistake. I wish I unit tested my notebook.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/110
https://github.com/google/deepvariant/issues/110:28,availability,error,error,28,"After taking a look at your error, it might also be a different problem. You might want to see what came out from you `get_int64_list(example, 'label')`. Is it possible that your example don't have a `'label'`? If you created your examples with `calling` mode in `make_examples`, they probably don't have a label, and might have resulted in that error.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/110
https://github.com/google/deepvariant/issues/110:346,availability,error,error,346,"After taking a look at your error, it might also be a different problem. You might want to see what came out from you `get_int64_list(example, 'label')`. Is it possible that your example don't have a `'label'`? If you created your examples with `calling` mode in `make_examples`, they probably don't have a label, and might have resulted in that error.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/110
https://github.com/google/deepvariant/issues/110:28,performance,error,error,28,"After taking a look at your error, it might also be a different problem. You might want to see what came out from you `get_int64_list(example, 'label')`. Is it possible that your example don't have a `'label'`? If you created your examples with `calling` mode in `make_examples`, they probably don't have a label, and might have resulted in that error.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/110
https://github.com/google/deepvariant/issues/110:346,performance,error,error,346,"After taking a look at your error, it might also be a different problem. You might want to see what came out from you `get_int64_list(example, 'label')`. Is it possible that your example don't have a `'label'`? If you created your examples with `calling` mode in `make_examples`, they probably don't have a label, and might have resulted in that error.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/110
https://github.com/google/deepvariant/issues/110:28,safety,error,error,28,"After taking a look at your error, it might also be a different problem. You might want to see what came out from you `get_int64_list(example, 'label')`. Is it possible that your example don't have a `'label'`? If you created your examples with `calling` mode in `make_examples`, they probably don't have a label, and might have resulted in that error.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/110
https://github.com/google/deepvariant/issues/110:346,safety,error,error,346,"After taking a look at your error, it might also be a different problem. You might want to see what came out from you `get_int64_list(example, 'label')`. Is it possible that your example don't have a `'label'`? If you created your examples with `calling` mode in `make_examples`, they probably don't have a label, and might have resulted in that error.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/110
https://github.com/google/deepvariant/issues/110:28,usability,error,error,28,"After taking a look at your error, it might also be a different problem. You might want to see what came out from you `get_int64_list(example, 'label')`. Is it possible that your example don't have a `'label'`? If you created your examples with `calling` mode in `make_examples`, they probably don't have a label, and might have resulted in that error.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/110
https://github.com/google/deepvariant/issues/110:346,usability,error,error,346,"After taking a look at your error, it might also be a different problem. You might want to see what came out from you `get_int64_list(example, 'label')`. Is it possible that your example don't have a `'label'`? If you created your examples with `calling` mode in `make_examples`, they probably don't have a label, and might have resulted in that error.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/110
https://github.com/google/deepvariant/issues/110:167,deployability,instal,installed,167,"Close :) It's more indicative that `read_tfrecords` is not probably working properly, since `examples` is an empty list to iterate from. So probably an issue with the installed version of Tensorflow.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/110
https://github.com/google/deepvariant/issues/110:177,deployability,version,version,177,"Close :) It's more indicative that `read_tfrecords` is not probably working properly, since `examples` is an empty list to iterate from. So probably an issue with the installed version of Tensorflow.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/110
https://github.com/google/deepvariant/issues/110:177,integrability,version,version,177,"Close :) It's more indicative that `read_tfrecords` is not probably working properly, since `examples` is an empty list to iterate from. So probably an issue with the installed version of Tensorflow.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/110
https://github.com/google/deepvariant/issues/110:177,modifiability,version,version,177,"Close :) It's more indicative that `read_tfrecords` is not probably working properly, since `examples` is an empty list to iterate from. So probably an issue with the installed version of Tensorflow.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/110
https://github.com/google/deepvariant/issues/110:0,usability,Close,Close,0,"Close :) It's more indicative that `read_tfrecords` is not probably working properly, since `examples` is an empty list to iterate from. So probably an issue with the installed version of Tensorflow.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/110
https://github.com/google/deepvariant/issues/110:19,usability,indicat,indicative,19,"Close :) It's more indicative that `read_tfrecords` is not probably working properly, since `examples` is an empty list to iterate from. So probably an issue with the installed version of Tensorflow.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/110
https://github.com/google/deepvariant/issues/111:19,availability,sla,slash,19,"Two of our forward-slash symbol is converted to a `%2F` - notice the one in the middle for `...HG001%2FHG001...` and `...study%2FBGISEQ...`. So the search path is most likely not converted properly, and thus not reachable.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/111
https://github.com/google/deepvariant/issues/111:19,reliability,sla,slash,19,"Two of our forward-slash symbol is converted to a `%2F` - notice the one in the middle for `...HG001%2FHG001...` and `...study%2FBGISEQ...`. So the search path is most likely not converted properly, and thus not reachable.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/111
https://github.com/google/deepvariant/issues/111:120,availability,error,error,120,". thank you, it the same, but don't know what must have gone wrong. i used a different data it ran but gave a different error again, please can you help analyze this for me? . WARNING: Logging before flag parsing goes to stderr. I1024 02:24:26.300854 140364017985280 client.py:1004] Timeout attempting to reach GCE metadata service. W1024 02:24:26.301438 140364017985280 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai. I1024 02:24:26.349014 140364017985280 make_examples.py:911] Preparing inputs. [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai. [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi. [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi. Traceback (most recent call last):.  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1015, in <module>.  tf.app.run().  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run.  _sys.exit(main(_sys.argv[:1] + flags_passthrough)).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1005, in main.  make_examples_runner(options).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 912, in make_examples_runner.  regions = processing_regions_from_options(options).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 892, in processing_regions_from_options.  options.min_shared_contigs_basepairs).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/111
https://github.com/google/deepvariant/issues/111:324,availability,servic,service,324,". thank you, it the same, but don't know what must have gone wrong. i used a different data it ran but gave a different error again, please can you help analyze this for me? . WARNING: Logging before flag parsing goes to stderr. I1024 02:24:26.300854 140364017985280 client.py:1004] Timeout attempting to reach GCE metadata service. W1024 02:24:26.301438 140364017985280 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai. I1024 02:24:26.349014 140364017985280 make_examples.py:911] Preparing inputs. [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai. [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi. [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi. Traceback (most recent call last):.  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1015, in <module>.  tf.app.run().  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run.  _sys.exit(main(_sys.argv[:1] + flags_passthrough)).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1005, in main.  make_examples_runner(options).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 912, in make_examples_runner.  regions = processing_regions_from_options(options).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 892, in processing_regions_from_options.  options.min_shared_contigs_basepairs).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/111
https://github.com/google/deepvariant/issues/111:3229,availability,sla,slash,3229,"unner(options).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 912, in make_examples_runner.  regions = processing_regions_from_options(options).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 892, in processing_regions_from_options.  options.min_shared_contigs_basepairs).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 495, in validate_reference_contig_coverage.  ref_bp, common_bp, coverage, format_contig_matches())). ValueError: Reference contigs span 3107576521 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sources were created on a common genome reference build. Contig matches were: ""chr1"" is 248956422 bp and IS MISSING, ""chr2"" is 242193529 bp and IS MISSING, ""chr3"" is 198295559 bp and IS MISSING, ""chr4"" is 190214555 bp and IS MISSING, ""chr5"" is 181538259 bp and IS MISSING, ""chr6"" is 170805979 bp and IS MISSING, ""chr7"" is 159345973 bp and IS MISSING, ""chr8"" is 145138636 bp and IS MISSING, ""chr9"" is 138394717 bp and IS MISSING, ""chr10"" is 133797422 bp and IS MISSING, ""chr11"" is 135086622 bp and IS MISSING, ""chr12"" is 133275309 bp and IS MISSING, ""chr13"" is 114364328 bp and IS MISSING, ""chr14"" is 107043718 bp and IS MISSING, ""chr15"" is 101991189 bp and IS MISSING, ""chr16"" is 90338345 bp and IS MISSING, ""chr17"" is 83257441 bp and IS MISSING................. it continues like this though but i don't want to paste all of it. i await your response thank you. On Tuesday, October 23, 2018, 10:09:51 PM GMT+8, Paul Grosu <notifications@github.com> wrote: . . . Two of our forward-slash symbol is converted to a %2F - notice the one in the middle for ...HG001%2FHG001... and ...study%2FBGISEQ.... So the search path is most likely not converted properly, and thus not reachable. . You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/111
https://github.com/google/deepvariant/issues/111:185,deployability,Log,Logging,185,". thank you, it the same, but don't know what must have gone wrong. i used a different data it ran but gave a different error again, please can you help analyze this for me? . WARNING: Logging before flag parsing goes to stderr. I1024 02:24:26.300854 140364017985280 client.py:1004] Timeout attempting to reach GCE metadata service. W1024 02:24:26.301438 140364017985280 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai. I1024 02:24:26.349014 140364017985280 make_examples.py:911] Preparing inputs. [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai. [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi. [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi. Traceback (most recent call last):.  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1015, in <module>.  tf.app.run().  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run.  _sys.exit(main(_sys.argv[:1] + flags_passthrough)).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1005, in main.  make_examples_runner(options).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 912, in make_examples_runner.  regions = processing_regions_from_options(options).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 892, in processing_regions_from_options.  options.min_shared_contigs_basepairs).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/111
https://github.com/google/deepvariant/issues/111:324,deployability,servic,service,324,". thank you, it the same, but don't know what must have gone wrong. i used a different data it ran but gave a different error again, please can you help analyze this for me? . WARNING: Logging before flag parsing goes to stderr. I1024 02:24:26.300854 140364017985280 client.py:1004] Timeout attempting to reach GCE metadata service. W1024 02:24:26.301438 140364017985280 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai. I1024 02:24:26.349014 140364017985280 make_examples.py:911] Preparing inputs. [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai. [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi. [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi. Traceback (most recent call last):.  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1015, in <module>.  tf.app.run().  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run.  _sys.exit(main(_sys.argv[:1] + flags_passthrough)).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1005, in main.  make_examples_runner(options).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 912, in make_examples_runner.  regions = processing_regions_from_options(options).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 892, in processing_regions_from_options.  options.min_shared_contigs_basepairs).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/111
https://github.com/google/deepvariant/issues/111:1248,deployability,modul,module,1248,"140364017985280 client.py:1004] Timeout attempting to reach GCE metadata service. W1024 02:24:26.301438 140364017985280 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai. I1024 02:24:26.349014 140364017985280 make_examples.py:911] Preparing inputs. [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai. [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi. [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi. Traceback (most recent call last):.  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1015, in <module>.  tf.app.run().  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run.  _sys.exit(main(_sys.argv[:1] + flags_passthrough)).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1005, in main.  make_examples_runner(options).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 912, in make_examples_runner.  regions = processing_regions_from_options(options).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 892, in processing_regions_from_options.  options.min_shared_contigs_basepairs).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 495, in validate_reference_contig_coverage.  ref_bp, common_bp, coverage, format_contig_matches())). ValueError: Reference contigs span 3107576521 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sour",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/111
https://github.com/google/deepvariant/issues/111:2297,deployability,build,build,2297,"thon2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run.  _sys.exit(main(_sys.argv[:1] + flags_passthrough)).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1005, in main.  make_examples_runner(options).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 912, in make_examples_runner.  regions = processing_regions_from_options(options).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 892, in processing_regions_from_options.  options.min_shared_contigs_basepairs).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 495, in validate_reference_contig_coverage.  ref_bp, common_bp, coverage, format_contig_matches())). ValueError: Reference contigs span 3107576521 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sources were created on a common genome reference build. Contig matches were: ""chr1"" is 248956422 bp and IS MISSING, ""chr2"" is 242193529 bp and IS MISSING, ""chr3"" is 198295559 bp and IS MISSING, ""chr4"" is 190214555 bp and IS MISSING, ""chr5"" is 181538259 bp and IS MISSING, ""chr6"" is 170805979 bp and IS MISSING, ""chr7"" is 159345973 bp and IS MISSING, ""chr8"" is 145138636 bp and IS MISSING, ""chr9"" is 138394717 bp and IS MISSING, ""chr10"" is 133797422 bp and IS MISSING, ""chr11"" is 135086622 bp and IS MISSING, ""chr12"" is 133275309 bp and IS MISSING, ""chr13"" is 114364328 bp and IS MISSING, ""chr14"" is 107043718 bp and IS MISSING, ""chr15"" is 101991189 bp and IS MISSING, ""chr16"" is 90338345 bp and IS MISSING, ""chr17"" is 83257441 bp and IS MISSING................. it continues like this though but i don't want to paste all of it. i await your response thank you. On Tuesday, October 23, 2018, 10:09:51 PM GMT+8, Paul Grosu <notifications@github.com> wrote: . . . Two of our forward-slash symbol is converted to a %2F - notice the one in the middle for .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/111
https://github.com/google/deepvariant/issues/111:3013,deployability,continu,continues,3013,"unner(options).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 912, in make_examples_runner.  regions = processing_regions_from_options(options).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 892, in processing_regions_from_options.  options.min_shared_contigs_basepairs).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 495, in validate_reference_contig_coverage.  ref_bp, common_bp, coverage, format_contig_matches())). ValueError: Reference contigs span 3107576521 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sources were created on a common genome reference build. Contig matches were: ""chr1"" is 248956422 bp and IS MISSING, ""chr2"" is 242193529 bp and IS MISSING, ""chr3"" is 198295559 bp and IS MISSING, ""chr4"" is 190214555 bp and IS MISSING, ""chr5"" is 181538259 bp and IS MISSING, ""chr6"" is 170805979 bp and IS MISSING, ""chr7"" is 159345973 bp and IS MISSING, ""chr8"" is 145138636 bp and IS MISSING, ""chr9"" is 138394717 bp and IS MISSING, ""chr10"" is 133797422 bp and IS MISSING, ""chr11"" is 135086622 bp and IS MISSING, ""chr12"" is 133275309 bp and IS MISSING, ""chr13"" is 114364328 bp and IS MISSING, ""chr14"" is 107043718 bp and IS MISSING, ""chr15"" is 101991189 bp and IS MISSING, ""chr16"" is 90338345 bp and IS MISSING, ""chr17"" is 83257441 bp and IS MISSING................. it continues like this though but i don't want to paste all of it. i await your response thank you. On Tuesday, October 23, 2018, 10:09:51 PM GMT+8, Paul Grosu <notifications@github.com> wrote: . . . Two of our forward-slash symbol is converted to a %2F - notice the one in the middle for ...HG001%2FHG001... and ...study%2FBGISEQ.... So the search path is most likely not converted properly, and thus not reachable. . You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/111
https://github.com/google/deepvariant/issues/111:324,integrability,servic,service,324,". thank you, it the same, but don't know what must have gone wrong. i used a different data it ran but gave a different error again, please can you help analyze this for me? . WARNING: Logging before flag parsing goes to stderr. I1024 02:24:26.300854 140364017985280 client.py:1004] Timeout attempting to reach GCE metadata service. W1024 02:24:26.301438 140364017985280 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai. I1024 02:24:26.349014 140364017985280 make_examples.py:911] Preparing inputs. [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai. [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi. [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi. Traceback (most recent call last):.  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1015, in <module>.  tf.app.run().  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run.  _sys.exit(main(_sys.argv[:1] + flags_passthrough)).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1005, in main.  make_examples_runner(options).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 912, in make_examples_runner.  regions = processing_regions_from_options(options).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 892, in processing_regions_from_options.  options.min_shared_contigs_basepairs).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/111
https://github.com/google/deepvariant/issues/111:443,integrability,pub,public,443,". thank you, it the same, but don't know what must have gone wrong. i used a different data it ran but gave a different error again, please can you help analyze this for me? . WARNING: Logging before flag parsing goes to stderr. I1024 02:24:26.300854 140364017985280 client.py:1004] Timeout attempting to reach GCE metadata service. W1024 02:24:26.301438 140364017985280 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai. I1024 02:24:26.349014 140364017985280 make_examples.py:911] Preparing inputs. [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai. [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi. [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi. Traceback (most recent call last):.  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1015, in <module>.  tf.app.run().  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run.  _sys.exit(main(_sys.argv[:1] + flags_passthrough)).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1005, in main.  make_examples_runner(options).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 912, in make_examples_runner.  regions = processing_regions_from_options(options).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 892, in processing_regions_from_options.  options.min_shared_contigs_basepairs).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/111
https://github.com/google/deepvariant/issues/111:1340,interoperability,platform,platform,1340,":26.301438 140364017985280 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai. I1024 02:24:26.349014 140364017985280 make_examples.py:911] Preparing inputs. [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai. [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi. [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi. Traceback (most recent call last):.  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1015, in <module>.  tf.app.run().  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run.  _sys.exit(main(_sys.argv[:1] + flags_passthrough)).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1005, in main.  make_examples_runner(options).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 912, in make_examples_runner.  regions = processing_regions_from_options(options).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 892, in processing_regions_from_options.  options.min_shared_contigs_basepairs).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 495, in validate_reference_contig_coverage.  ref_bp, common_bp, coverage, format_contig_matches())). ValueError: Reference contigs span 3107576521 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sources were created on a common genome reference build. Contig matches were: ""chr1"" is 248956422",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/111
https://github.com/google/deepvariant/issues/111:324,modifiability,servic,service,324,". thank you, it the same, but don't know what must have gone wrong. i used a different data it ran but gave a different error again, please can you help analyze this for me? . WARNING: Logging before flag parsing goes to stderr. I1024 02:24:26.300854 140364017985280 client.py:1004] Timeout attempting to reach GCE metadata service. W1024 02:24:26.301438 140364017985280 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai. I1024 02:24:26.349014 140364017985280 make_examples.py:911] Preparing inputs. [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai. [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi. [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi. Traceback (most recent call last):.  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1015, in <module>.  tf.app.run().  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run.  _sys.exit(main(_sys.argv[:1] + flags_passthrough)).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1005, in main.  make_examples_runner(options).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 912, in make_examples_runner.  regions = processing_regions_from_options(options).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 892, in processing_regions_from_options.  options.min_shared_contigs_basepairs).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/111
https://github.com/google/deepvariant/issues/111:1248,modifiability,modul,module,1248,"140364017985280 client.py:1004] Timeout attempting to reach GCE metadata service. W1024 02:24:26.301438 140364017985280 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai. I1024 02:24:26.349014 140364017985280 make_examples.py:911] Preparing inputs. [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai. [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi. [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi. Traceback (most recent call last):.  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1015, in <module>.  tf.app.run().  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run.  _sys.exit(main(_sys.argv[:1] + flags_passthrough)).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1005, in main.  make_examples_runner(options).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 912, in make_examples_runner.  regions = processing_regions_from_options(options).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 892, in processing_regions_from_options.  options.min_shared_contigs_basepairs).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 495, in validate_reference_contig_coverage.  ref_bp, common_bp, coverage, format_contig_matches())). ValueError: Reference contigs span 3107576521 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sour",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/111
https://github.com/google/deepvariant/issues/111:1313,modifiability,pac,packages,1313,"tadata service. W1024 02:24:26.301438 140364017985280 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai. I1024 02:24:26.349014 140364017985280 make_examples.py:911] Preparing inputs. [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai. [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi. [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi. Traceback (most recent call last):.  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1015, in <module>.  tf.app.run().  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run.  _sys.exit(main(_sys.argv[:1] + flags_passthrough)).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1005, in main.  make_examples_runner(options).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 912, in make_examples_runner.  regions = processing_regions_from_options(options).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 892, in processing_regions_from_options.  options.min_shared_contigs_basepairs).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 495, in validate_reference_contig_coverage.  ref_bp, common_bp, coverage, format_contig_matches())). ValueError: Reference contigs span 3107576521 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sources were created on a common genome reference build. Contig matche",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/111
https://github.com/google/deepvariant/issues/111:120,performance,error,error,120,". thank you, it the same, but don't know what must have gone wrong. i used a different data it ran but gave a different error again, please can you help analyze this for me? . WARNING: Logging before flag parsing goes to stderr. I1024 02:24:26.300854 140364017985280 client.py:1004] Timeout attempting to reach GCE metadata service. W1024 02:24:26.301438 140364017985280 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai. I1024 02:24:26.349014 140364017985280 make_examples.py:911] Preparing inputs. [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai. [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi. [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi. Traceback (most recent call last):.  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1015, in <module>.  tf.app.run().  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run.  _sys.exit(main(_sys.argv[:1] + flags_passthrough)).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1005, in main.  make_examples_runner(options).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 912, in make_examples_runner.  regions = processing_regions_from_options(options).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 892, in processing_regions_from_options.  options.min_shared_contigs_basepairs).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/111
https://github.com/google/deepvariant/issues/111:283,performance,Time,Timeout,283,". thank you, it the same, but don't know what must have gone wrong. i used a different data it ran but gave a different error again, please can you help analyze this for me? . WARNING: Logging before flag parsing goes to stderr. I1024 02:24:26.300854 140364017985280 client.py:1004] Timeout attempting to reach GCE metadata service. W1024 02:24:26.301438 140364017985280 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai. I1024 02:24:26.349014 140364017985280 make_examples.py:911] Preparing inputs. [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai. [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi. [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi. Traceback (most recent call last):.  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1015, in <module>.  tf.app.run().  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run.  _sys.exit(main(_sys.argv[:1] + flags_passthrough)).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1005, in main.  make_examples_runner(options).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 912, in make_examples_runner.  regions = processing_regions_from_options(options).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 892, in processing_regions_from_options.  options.min_shared_contigs_basepairs).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/111
https://github.com/google/deepvariant/issues/111:829,performance,perform,performance-testdata,829,". thank you, it the same, but don't know what must have gone wrong. i used a different data it ran but gave a different error again, please can you help analyze this for me? . WARNING: Logging before flag parsing goes to stderr. I1024 02:24:26.300854 140364017985280 client.py:1004] Timeout attempting to reach GCE metadata service. W1024 02:24:26.301438 140364017985280 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai. I1024 02:24:26.349014 140364017985280 make_examples.py:911] Preparing inputs. [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai. [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi. [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi. Traceback (most recent call last):.  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1015, in <module>.  tf.app.run().  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run.  _sys.exit(main(_sys.argv[:1] + flags_passthrough)).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1005, in main.  make_examples_runner(options).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 912, in make_examples_runner.  regions = processing_regions_from_options(options).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 892, in processing_regions_from_options.  options.min_shared_contigs_basepairs).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/111
https://github.com/google/deepvariant/issues/111:1003,performance,perform,performance-testdata,1003,"it the same, but don't know what must have gone wrong. i used a different data it ran but gave a different error again, please can you help analyze this for me? . WARNING: Logging before flag parsing goes to stderr. I1024 02:24:26.300854 140364017985280 client.py:1004] Timeout attempting to reach GCE metadata service. W1024 02:24:26.301438 140364017985280 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai. I1024 02:24:26.349014 140364017985280 make_examples.py:911] Preparing inputs. [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai. [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi. [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi. Traceback (most recent call last):.  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1015, in <module>.  tf.app.run().  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run.  _sys.exit(main(_sys.argv[:1] + flags_passthrough)).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1005, in main.  make_examples_runner(options).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 912, in make_examples_runner.  regions = processing_regions_from_options(options).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 892, in processing_regions_from_options.  options.min_shared_contigs_basepairs).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 495, i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/111
https://github.com/google/deepvariant/issues/111:3229,reliability,sla,slash,3229,"unner(options).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 912, in make_examples_runner.  regions = processing_regions_from_options(options).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 892, in processing_regions_from_options.  options.min_shared_contigs_basepairs).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 495, in validate_reference_contig_coverage.  ref_bp, common_bp, coverage, format_contig_matches())). ValueError: Reference contigs span 3107576521 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sources were created on a common genome reference build. Contig matches were: ""chr1"" is 248956422 bp and IS MISSING, ""chr2"" is 242193529 bp and IS MISSING, ""chr3"" is 198295559 bp and IS MISSING, ""chr4"" is 190214555 bp and IS MISSING, ""chr5"" is 181538259 bp and IS MISSING, ""chr6"" is 170805979 bp and IS MISSING, ""chr7"" is 159345973 bp and IS MISSING, ""chr8"" is 145138636 bp and IS MISSING, ""chr9"" is 138394717 bp and IS MISSING, ""chr10"" is 133797422 bp and IS MISSING, ""chr11"" is 135086622 bp and IS MISSING, ""chr12"" is 133275309 bp and IS MISSING, ""chr13"" is 114364328 bp and IS MISSING, ""chr14"" is 107043718 bp and IS MISSING, ""chr15"" is 101991189 bp and IS MISSING, ""chr16"" is 90338345 bp and IS MISSING, ""chr17"" is 83257441 bp and IS MISSING................. it continues like this though but i don't want to paste all of it. i await your response thank you. On Tuesday, October 23, 2018, 10:09:51 PM GMT+8, Paul Grosu <notifications@github.com> wrote: . . . Two of our forward-slash symbol is converted to a %2F - notice the one in the middle for ...HG001%2FHG001... and ...study%2FBGISEQ.... So the search path is most likely not converted properly, and thus not reachable. . You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/111
https://github.com/google/deepvariant/issues/111:120,safety,error,error,120,". thank you, it the same, but don't know what must have gone wrong. i used a different data it ran but gave a different error again, please can you help analyze this for me? . WARNING: Logging before flag parsing goes to stderr. I1024 02:24:26.300854 140364017985280 client.py:1004] Timeout attempting to reach GCE metadata service. W1024 02:24:26.301438 140364017985280 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai. I1024 02:24:26.349014 140364017985280 make_examples.py:911] Preparing inputs. [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai. [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi. [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi. Traceback (most recent call last):.  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1015, in <module>.  tf.app.run().  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run.  _sys.exit(main(_sys.argv[:1] + flags_passthrough)).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1005, in main.  make_examples_runner(options).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 912, in make_examples_runner.  regions = processing_regions_from_options(options).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 892, in processing_regions_from_options.  options.min_shared_contigs_basepairs).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/111
https://github.com/google/deepvariant/issues/111:185,safety,Log,Logging,185,". thank you, it the same, but don't know what must have gone wrong. i used a different data it ran but gave a different error again, please can you help analyze this for me? . WARNING: Logging before flag parsing goes to stderr. I1024 02:24:26.300854 140364017985280 client.py:1004] Timeout attempting to reach GCE metadata service. W1024 02:24:26.301438 140364017985280 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai. I1024 02:24:26.349014 140364017985280 make_examples.py:911] Preparing inputs. [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai. [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi. [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi. Traceback (most recent call last):.  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1015, in <module>.  tf.app.run().  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run.  _sys.exit(main(_sys.argv[:1] + flags_passthrough)).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1005, in main.  make_examples_runner(options).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 912, in make_examples_runner.  regions = processing_regions_from_options(options).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 892, in processing_regions_from_options.  options.min_shared_contigs_basepairs).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/111
https://github.com/google/deepvariant/issues/111:283,safety,Timeout,Timeout,283,". thank you, it the same, but don't know what must have gone wrong. i used a different data it ran but gave a different error again, please can you help analyze this for me? . WARNING: Logging before flag parsing goes to stderr. I1024 02:24:26.300854 140364017985280 client.py:1004] Timeout attempting to reach GCE metadata service. W1024 02:24:26.301438 140364017985280 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai. I1024 02:24:26.349014 140364017985280 make_examples.py:911] Preparing inputs. [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai. [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi. [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi. Traceback (most recent call last):.  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1015, in <module>.  tf.app.run().  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run.  _sys.exit(main(_sys.argv[:1] + flags_passthrough)).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1005, in main.  make_examples_runner(options).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 912, in make_examples_runner.  regions = processing_regions_from_options(options).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 892, in processing_regions_from_options.  options.min_shared_contigs_basepairs).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/111
https://github.com/google/deepvariant/issues/111:660,safety,input,inputs,660,". thank you, it the same, but don't know what must have gone wrong. i used a different data it ran but gave a different error again, please can you help analyze this for me? . WARNING: Logging before flag parsing goes to stderr. I1024 02:24:26.300854 140364017985280 client.py:1004] Timeout attempting to reach GCE metadata service. W1024 02:24:26.301438 140364017985280 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai. I1024 02:24:26.349014 140364017985280 make_examples.py:911] Preparing inputs. [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai. [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi. [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi. Traceback (most recent call last):.  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1015, in <module>.  tf.app.run().  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run.  _sys.exit(main(_sys.argv[:1] + flags_passthrough)).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1005, in main.  make_examples_runner(options).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 912, in make_examples_runner.  regions = processing_regions_from_options(options).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 892, in processing_regions_from_options.  options.min_shared_contigs_basepairs).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/111
https://github.com/google/deepvariant/issues/111:841,safety,test,testdata,841,". thank you, it the same, but don't know what must have gone wrong. i used a different data it ran but gave a different error again, please can you help analyze this for me? . WARNING: Logging before flag parsing goes to stderr. I1024 02:24:26.300854 140364017985280 client.py:1004] Timeout attempting to reach GCE metadata service. W1024 02:24:26.301438 140364017985280 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai. I1024 02:24:26.349014 140364017985280 make_examples.py:911] Preparing inputs. [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai. [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi. [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi. Traceback (most recent call last):.  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1015, in <module>.  tf.app.run().  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run.  _sys.exit(main(_sys.argv[:1] + flags_passthrough)).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1005, in main.  make_examples_runner(options).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 912, in make_examples_runner.  regions = processing_regions_from_options(options).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 892, in processing_regions_from_options.  options.min_shared_contigs_basepairs).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/111
https://github.com/google/deepvariant/issues/111:1015,safety,test,testdata,1015," same, but don't know what must have gone wrong. i used a different data it ran but gave a different error again, please can you help analyze this for me? . WARNING: Logging before flag parsing goes to stderr. I1024 02:24:26.300854 140364017985280 client.py:1004] Timeout attempting to reach GCE metadata service. W1024 02:24:26.301438 140364017985280 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai. I1024 02:24:26.349014 140364017985280 make_examples.py:911] Preparing inputs. [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai. [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi. [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi. Traceback (most recent call last):.  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1015, in <module>.  tf.app.run().  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run.  _sys.exit(main(_sys.argv[:1] + flags_passthrough)).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1005, in main.  make_examples_runner(options).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 912, in make_examples_runner.  regions = processing_regions_from_options(options).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 892, in processing_regions_from_options.  options.min_shared_contigs_basepairs).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 495, in vali",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/111
https://github.com/google/deepvariant/issues/111:1248,safety,modul,module,1248,"140364017985280 client.py:1004] Timeout attempting to reach GCE metadata service. W1024 02:24:26.301438 140364017985280 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai. I1024 02:24:26.349014 140364017985280 make_examples.py:911] Preparing inputs. [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai. [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi. [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi. Traceback (most recent call last):.  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1015, in <module>.  tf.app.run().  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run.  _sys.exit(main(_sys.argv[:1] + flags_passthrough)).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1005, in main.  make_examples_runner(options).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 912, in make_examples_runner.  regions = processing_regions_from_options(options).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 892, in processing_regions_from_options.  options.min_shared_contigs_basepairs).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 495, in validate_reference_contig_coverage.  ref_bp, common_bp, coverage, format_contig_matches())). ValueError: Reference contigs span 3107576521 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sour",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/111
https://github.com/google/deepvariant/issues/111:2219,safety,input,input,2219,"mples.py"", line 1015, in <module>.  tf.app.run().  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run.  _sys.exit(main(_sys.argv[:1] + flags_passthrough)).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1005, in main.  make_examples_runner(options).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 912, in make_examples_runner.  regions = processing_regions_from_options(options).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 892, in processing_regions_from_options.  options.min_shared_contigs_basepairs).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 495, in validate_reference_contig_coverage.  ref_bp, common_bp, coverage, format_contig_matches())). ValueError: Reference contigs span 3107576521 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sources were created on a common genome reference build. Contig matches were: ""chr1"" is 248956422 bp and IS MISSING, ""chr2"" is 242193529 bp and IS MISSING, ""chr3"" is 198295559 bp and IS MISSING, ""chr4"" is 190214555 bp and IS MISSING, ""chr5"" is 181538259 bp and IS MISSING, ""chr6"" is 170805979 bp and IS MISSING, ""chr7"" is 159345973 bp and IS MISSING, ""chr8"" is 145138636 bp and IS MISSING, ""chr9"" is 138394717 bp and IS MISSING, ""chr10"" is 133797422 bp and IS MISSING, ""chr11"" is 135086622 bp and IS MISSING, ""chr12"" is 133275309 bp and IS MISSING, ""chr13"" is 114364328 bp and IS MISSING, ""chr14"" is 107043718 bp and IS MISSING, ""chr15"" is 101991189 bp and IS MISSING, ""chr16"" is 90338345 bp and IS MISSING, ""chr17"" is 83257441 bp and IS MISSING................. it continues like this though but i don't want to paste all of it. i await your response thank you. On Tuesday, October 23, 2018, 10:09:51 PM GMT+8, Paul Grosu <notifications@github.com> wrote: . . . Two of our f",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/111
https://github.com/google/deepvariant/issues/111:185,security,Log,Logging,185,". thank you, it the same, but don't know what must have gone wrong. i used a different data it ran but gave a different error again, please can you help analyze this for me? . WARNING: Logging before flag parsing goes to stderr. I1024 02:24:26.300854 140364017985280 client.py:1004] Timeout attempting to reach GCE metadata service. W1024 02:24:26.301438 140364017985280 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai. I1024 02:24:26.349014 140364017985280 make_examples.py:911] Preparing inputs. [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai. [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi. [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi. Traceback (most recent call last):.  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1015, in <module>.  tf.app.run().  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run.  _sys.exit(main(_sys.argv[:1] + flags_passthrough)).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1005, in main.  make_examples_runner(options).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 912, in make_examples_runner.  regions = processing_regions_from_options(options).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 892, in processing_regions_from_options.  options.min_shared_contigs_basepairs).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/111
https://github.com/google/deepvariant/issues/111:469,security,access,accessible,469,". thank you, it the same, but don't know what must have gone wrong. i used a different data it ran but gave a different error again, please can you help analyze this for me? . WARNING: Logging before flag parsing goes to stderr. I1024 02:24:26.300854 140364017985280 client.py:1004] Timeout attempting to reach GCE metadata service. W1024 02:24:26.301438 140364017985280 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai. I1024 02:24:26.349014 140364017985280 make_examples.py:911] Preparing inputs. [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai. [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi. [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi. Traceback (most recent call last):.  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1015, in <module>.  tf.app.run().  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run.  _sys.exit(main(_sys.argv[:1] + flags_passthrough)).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1005, in main.  make_examples_runner(options).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 912, in make_examples_runner.  regions = processing_regions_from_options(options).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 892, in processing_regions_from_options.  options.min_shared_contigs_basepairs).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/111
https://github.com/google/deepvariant/issues/111:3465,security,auth,authored,3465,"unner(options).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 912, in make_examples_runner.  regions = processing_regions_from_options(options).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 892, in processing_regions_from_options.  options.min_shared_contigs_basepairs).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 495, in validate_reference_contig_coverage.  ref_bp, common_bp, coverage, format_contig_matches())). ValueError: Reference contigs span 3107576521 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sources were created on a common genome reference build. Contig matches were: ""chr1"" is 248956422 bp and IS MISSING, ""chr2"" is 242193529 bp and IS MISSING, ""chr3"" is 198295559 bp and IS MISSING, ""chr4"" is 190214555 bp and IS MISSING, ""chr5"" is 181538259 bp and IS MISSING, ""chr6"" is 170805979 bp and IS MISSING, ""chr7"" is 159345973 bp and IS MISSING, ""chr8"" is 145138636 bp and IS MISSING, ""chr9"" is 138394717 bp and IS MISSING, ""chr10"" is 133797422 bp and IS MISSING, ""chr11"" is 135086622 bp and IS MISSING, ""chr12"" is 133275309 bp and IS MISSING, ""chr13"" is 114364328 bp and IS MISSING, ""chr14"" is 107043718 bp and IS MISSING, ""chr15"" is 101991189 bp and IS MISSING, ""chr16"" is 90338345 bp and IS MISSING, ""chr17"" is 83257441 bp and IS MISSING................. it continues like this though but i don't want to paste all of it. i await your response thank you. On Tuesday, October 23, 2018, 10:09:51 PM GMT+8, Paul Grosu <notifications@github.com> wrote: . . . Two of our forward-slash symbol is converted to a %2F - notice the one in the middle for ...HG001%2FHG001... and ...study%2FBGISEQ.... So the search path is most likely not converted properly, and thus not reachable. . You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/111
https://github.com/google/deepvariant/issues/111:185,testability,Log,Logging,185,". thank you, it the same, but don't know what must have gone wrong. i used a different data it ran but gave a different error again, please can you help analyze this for me? . WARNING: Logging before flag parsing goes to stderr. I1024 02:24:26.300854 140364017985280 client.py:1004] Timeout attempting to reach GCE metadata service. W1024 02:24:26.301438 140364017985280 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai. I1024 02:24:26.349014 140364017985280 make_examples.py:911] Preparing inputs. [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai. [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi. [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi. Traceback (most recent call last):.  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1015, in <module>.  tf.app.run().  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run.  _sys.exit(main(_sys.argv[:1] + flags_passthrough)).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1005, in main.  make_examples_runner(options).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 912, in make_examples_runner.  regions = processing_regions_from_options(options).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 892, in processing_regions_from_options.  options.min_shared_contigs_basepairs).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/111
https://github.com/google/deepvariant/issues/111:841,testability,test,testdata,841,". thank you, it the same, but don't know what must have gone wrong. i used a different data it ran but gave a different error again, please can you help analyze this for me? . WARNING: Logging before flag parsing goes to stderr. I1024 02:24:26.300854 140364017985280 client.py:1004] Timeout attempting to reach GCE metadata service. W1024 02:24:26.301438 140364017985280 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai. I1024 02:24:26.349014 140364017985280 make_examples.py:911] Preparing inputs. [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai. [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi. [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi. Traceback (most recent call last):.  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1015, in <module>.  tf.app.run().  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run.  _sys.exit(main(_sys.argv[:1] + flags_passthrough)).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1005, in main.  make_examples_runner(options).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 912, in make_examples_runner.  regions = processing_regions_from_options(options).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 892, in processing_regions_from_options.  options.min_shared_contigs_basepairs).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/111
https://github.com/google/deepvariant/issues/111:1015,testability,test,testdata,1015," same, but don't know what must have gone wrong. i used a different data it ran but gave a different error again, please can you help analyze this for me? . WARNING: Logging before flag parsing goes to stderr. I1024 02:24:26.300854 140364017985280 client.py:1004] Timeout attempting to reach GCE metadata service. W1024 02:24:26.301438 140364017985280 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai. I1024 02:24:26.349014 140364017985280 make_examples.py:911] Preparing inputs. [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai. [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi. [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi. Traceback (most recent call last):.  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1015, in <module>.  tf.app.run().  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run.  _sys.exit(main(_sys.argv[:1] + flags_passthrough)).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1005, in main.  make_examples_runner(options).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 912, in make_examples_runner.  regions = processing_regions_from_options(options).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 892, in processing_regions_from_options.  options.min_shared_contigs_basepairs).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 495, in vali",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/111
https://github.com/google/deepvariant/issues/111:1113,testability,Trace,Traceback,1113,"t error again, please can you help analyze this for me? . WARNING: Logging before flag parsing goes to stderr. I1024 02:24:26.300854 140364017985280 client.py:1004] Timeout attempting to reach GCE metadata service. W1024 02:24:26.301438 140364017985280 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai. I1024 02:24:26.349014 140364017985280 make_examples.py:911] Preparing inputs. [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai. [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi. [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi. Traceback (most recent call last):.  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1015, in <module>.  tf.app.run().  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run.  _sys.exit(main(_sys.argv[:1] + flags_passthrough)).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1005, in main.  make_examples_runner(options).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 912, in make_examples_runner.  regions = processing_regions_from_options(options).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 892, in processing_regions_from_options.  options.min_shared_contigs_basepairs).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 495, in validate_reference_contig_coverage.  ref_bp, common_bp, coverage, format_contig_matches())). ValueEr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/111
https://github.com/google/deepvariant/issues/111:2074,testability,coverag,coverage,2074,"OM1-22_v3.2.2_highconf.vcf.gz.tbi. Traceback (most recent call last):.  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1015, in <module>.  tf.app.run().  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run.  _sys.exit(main(_sys.argv[:1] + flags_passthrough)).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1005, in main.  make_examples_runner(options).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 912, in make_examples_runner.  regions = processing_regions_from_options(options).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 892, in processing_regions_from_options.  options.min_shared_contigs_basepairs).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 495, in validate_reference_contig_coverage.  ref_bp, common_bp, coverage, format_contig_matches())). ValueError: Reference contigs span 3107576521 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sources were created on a common genome reference build. Contig matches were: ""chr1"" is 248956422 bp and IS MISSING, ""chr2"" is 242193529 bp and IS MISSING, ""chr3"" is 198295559 bp and IS MISSING, ""chr4"" is 190214555 bp and IS MISSING, ""chr5"" is 181538259 bp and IS MISSING, ""chr6"" is 170805979 bp and IS MISSING, ""chr7"" is 159345973 bp and IS MISSING, ""chr8"" is 145138636 bp and IS MISSING, ""chr9"" is 138394717 bp and IS MISSING, ""chr10"" is 133797422 bp and IS MISSING, ""chr11"" is 135086622 bp and IS MISSING, ""chr12"" is 133275309 bp and IS MISSING, ""chr13"" is 114364328 bp and IS MISSING, ""chr14"" is 107043718 bp and IS MISSING, ""chr15"" is 101991189 bp and IS MISSING, ""chr16"" is 90338345 bp and IS MISSING, ""chr17"" is 83257441 bp and IS MISSING................. it continues like this though but i don't want to paste all of it. i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/111
https://github.com/google/deepvariant/issues/111:120,usability,error,error,120,". thank you, it the same, but don't know what must have gone wrong. i used a different data it ran but gave a different error again, please can you help analyze this for me? . WARNING: Logging before flag parsing goes to stderr. I1024 02:24:26.300854 140364017985280 client.py:1004] Timeout attempting to reach GCE metadata service. W1024 02:24:26.301438 140364017985280 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai. I1024 02:24:26.349014 140364017985280 make_examples.py:911] Preparing inputs. [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai. [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi. [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi. Traceback (most recent call last):.  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1015, in <module>.  tf.app.run().  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run.  _sys.exit(main(_sys.argv[:1] + flags_passthrough)).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1005, in main.  make_examples_runner(options).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 912, in make_examples_runner.  regions = processing_regions_from_options(options).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 892, in processing_regions_from_options.  options.min_shared_contigs_basepairs).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/111
https://github.com/google/deepvariant/issues/111:148,usability,help,help,148,". thank you, it the same, but don't know what must have gone wrong. i used a different data it ran but gave a different error again, please can you help analyze this for me? . WARNING: Logging before flag parsing goes to stderr. I1024 02:24:26.300854 140364017985280 client.py:1004] Timeout attempting to reach GCE metadata service. W1024 02:24:26.301438 140364017985280 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai. I1024 02:24:26.349014 140364017985280 make_examples.py:911] Preparing inputs. [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai. [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi. [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi. Traceback (most recent call last):.  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1015, in <module>.  tf.app.run().  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run.  _sys.exit(main(_sys.argv[:1] + flags_passthrough)).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1005, in main.  make_examples_runner(options).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 912, in make_examples_runner.  regions = processing_regions_from_options(options).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 892, in processing_regions_from_options.  options.min_shared_contigs_basepairs).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/111
https://github.com/google/deepvariant/issues/111:660,usability,input,inputs,660,". thank you, it the same, but don't know what must have gone wrong. i used a different data it ran but gave a different error again, please can you help analyze this for me? . WARNING: Logging before flag parsing goes to stderr. I1024 02:24:26.300854 140364017985280 client.py:1004] Timeout attempting to reach GCE metadata service. W1024 02:24:26.301438 140364017985280 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai. I1024 02:24:26.349014 140364017985280 make_examples.py:911] Preparing inputs. [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai. [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi. [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi. Traceback (most recent call last):.  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1015, in <module>.  tf.app.run().  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run.  _sys.exit(main(_sys.argv[:1] + flags_passthrough)).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1005, in main.  make_examples_runner(options).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 912, in make_examples_runner.  regions = processing_regions_from_options(options).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 892, in processing_regions_from_options.  options.min_shared_contigs_basepairs).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/111
https://github.com/google/deepvariant/issues/111:829,usability,perform,performance-testdata,829,". thank you, it the same, but don't know what must have gone wrong. i used a different data it ran but gave a different error again, please can you help analyze this for me? . WARNING: Logging before flag parsing goes to stderr. I1024 02:24:26.300854 140364017985280 client.py:1004] Timeout attempting to reach GCE metadata service. W1024 02:24:26.301438 140364017985280 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai. I1024 02:24:26.349014 140364017985280 make_examples.py:911] Preparing inputs. [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai. [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi. [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi. Traceback (most recent call last):.  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1015, in <module>.  tf.app.run().  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run.  _sys.exit(main(_sys.argv[:1] + flags_passthrough)).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1005, in main.  make_examples_runner(options).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 912, in make_examples_runner.  regions = processing_regions_from_options(options).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 892, in processing_regions_from_options.  options.min_shared_contigs_basepairs).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/111
https://github.com/google/deepvariant/issues/111:1003,usability,perform,performance-testdata,1003,"it the same, but don't know what must have gone wrong. i used a different data it ran but gave a different error again, please can you help analyze this for me? . WARNING: Logging before flag parsing goes to stderr. I1024 02:24:26.300854 140364017985280 client.py:1004] Timeout attempting to reach GCE metadata service. W1024 02:24:26.301438 140364017985280 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib. [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai. I1024 02:24:26.349014 140364017985280 make_examples.py:911] Preparing inputs. [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai. [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi. [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi. Traceback (most recent call last):.  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1015, in <module>.  tf.app.run().  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run.  _sys.exit(main(_sys.argv[:1] + flags_passthrough)).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1005, in main.  make_examples_runner(options).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 912, in make_examples_runner.  regions = processing_regions_from_options(options).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 892, in processing_regions_from_options.  options.min_shared_contigs_basepairs).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 495, i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/111
https://github.com/google/deepvariant/issues/111:2219,usability,input,input,2219,"mples.py"", line 1015, in <module>.  tf.app.run().  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run.  _sys.exit(main(_sys.argv[:1] + flags_passthrough)).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1005, in main.  make_examples_runner(options).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 912, in make_examples_runner.  regions = processing_regions_from_options(options).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 892, in processing_regions_from_options.  options.min_shared_contigs_basepairs).  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 495, in validate_reference_contig_coverage.  ref_bp, common_bp, coverage, format_contig_matches())). ValueError: Reference contigs span 3107576521 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sources were created on a common genome reference build. Contig matches were: ""chr1"" is 248956422 bp and IS MISSING, ""chr2"" is 242193529 bp and IS MISSING, ""chr3"" is 198295559 bp and IS MISSING, ""chr4"" is 190214555 bp and IS MISSING, ""chr5"" is 181538259 bp and IS MISSING, ""chr6"" is 170805979 bp and IS MISSING, ""chr7"" is 159345973 bp and IS MISSING, ""chr8"" is 145138636 bp and IS MISSING, ""chr9"" is 138394717 bp and IS MISSING, ""chr10"" is 133797422 bp and IS MISSING, ""chr11"" is 135086622 bp and IS MISSING, ""chr12"" is 133275309 bp and IS MISSING, ""chr13"" is 114364328 bp and IS MISSING, ""chr14"" is 107043718 bp and IS MISSING, ""chr15"" is 101991189 bp and IS MISSING, ""chr16"" is 90338345 bp and IS MISSING, ""chr17"" is 83257441 bp and IS MISSING................. it continues like this though but i don't want to paste all of it. i await your response thank you. On Tuesday, October 23, 2018, 10:09:51 PM GMT+8, Paul Grosu <notifications@github.com> wrote: . . . Two of our f",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/111
https://github.com/google/deepvariant/issues/112:185,interoperability,architectur,architecture,185,"Hi @chris-sf . thanks for reporting the issue. We really don't expect the variant calling results to differ when we change the number of shards (especially on the same kind of hardware architecture), but it seems like we can also see this issue when we try running WES case study with different number of shards. We'll investigate further to find out the root cause. And to clarify, you're seeing the same results when the number of shards are the same. Is that correct?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/112
https://github.com/google/deepvariant/issues/112:122,deployability,stage,stage,122,"From our experiments so far, it seems like the number of tensorflow.Examples coming out at the end of the `make_examples` stage already differ. So that means even before the data touches the CNN classifier in TensorFlow, there are already some differences. This is still not quite I'd expect, but I could see maybe some differences happen when we partition things in different number of tasks. I'll take a closer look on `make_examples`. For now, if you need to ensure reproducible and deterministic results, please also make sure to use the same number of shards.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/112
https://github.com/google/deepvariant/issues/112:406,usability,close,closer,406,"From our experiments so far, it seems like the number of tensorflow.Examples coming out at the end of the `make_examples` stage already differ. So that means even before the data touches the CNN classifier in TensorFlow, there are already some differences. This is still not quite I'd expect, but I could see maybe some differences happen when we partition things in different number of tasks. I'll take a closer look on `make_examples`. For now, if you need to ensure reproducible and deterministic results, please also make sure to use the same number of shards.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/112
https://github.com/google/deepvariant/issues/112:225,availability,consist,consistent,225,"Yeah, but @pichuan folks will then ask the obvious question of what would be the right number of shards to get the correct variant calls. Probably some validation tests might be required with comparative benchmarks that show consistent predictability. I'm sure you guys have done some of this already, which would be nice if it could be shared for optimally configuration of the settings. Ideally it's something that users can then replicate themselves, in order to run verification tests at different granular stages of the DeepVariant workflow - and would be compared to validated metrics for proper evaluation - just to convince the users all is working properly.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/112
https://github.com/google/deepvariant/issues/112:432,availability,replic,replicate,432,"Yeah, but @pichuan folks will then ask the obvious question of what would be the right number of shards to get the correct variant calls. Probably some validation tests might be required with comparative benchmarks that show consistent predictability. I'm sure you guys have done some of this already, which would be nice if it could be shared for optimally configuration of the settings. Ideally it's something that users can then replicate themselves, in order to run verification tests at different granular stages of the DeepVariant workflow - and would be compared to validated metrics for proper evaluation - just to convince the users all is working properly.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/112
https://github.com/google/deepvariant/issues/112:358,deployability,configurat,configuration,358,"Yeah, but @pichuan folks will then ask the obvious question of what would be the right number of shards to get the correct variant calls. Probably some validation tests might be required with comparative benchmarks that show consistent predictability. I'm sure you guys have done some of this already, which would be nice if it could be shared for optimally configuration of the settings. Ideally it's something that users can then replicate themselves, in order to run verification tests at different granular stages of the DeepVariant workflow - and would be compared to validated metrics for proper evaluation - just to convince the users all is working properly.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/112
https://github.com/google/deepvariant/issues/112:511,deployability,stage,stages,511,"Yeah, but @pichuan folks will then ask the obvious question of what would be the right number of shards to get the correct variant calls. Probably some validation tests might be required with comparative benchmarks that show consistent predictability. I'm sure you guys have done some of this already, which would be nice if it could be shared for optimally configuration of the settings. Ideally it's something that users can then replicate themselves, in order to run verification tests at different granular stages of the DeepVariant workflow - and would be compared to validated metrics for proper evaluation - just to convince the users all is working properly.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/112
https://github.com/google/deepvariant/issues/112:236,energy efficiency,predict,predictability,236,"Yeah, but @pichuan folks will then ask the obvious question of what would be the right number of shards to get the correct variant calls. Probably some validation tests might be required with comparative benchmarks that show consistent predictability. I'm sure you guys have done some of this already, which would be nice if it could be shared for optimally configuration of the settings. Ideally it's something that users can then replicate themselves, in order to run verification tests at different granular stages of the DeepVariant workflow - and would be compared to validated metrics for proper evaluation - just to convince the users all is working properly.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/112
https://github.com/google/deepvariant/issues/112:348,energy efficiency,optim,optimally,348,"Yeah, but @pichuan folks will then ask the obvious question of what would be the right number of shards to get the correct variant calls. Probably some validation tests might be required with comparative benchmarks that show consistent predictability. I'm sure you guys have done some of this already, which would be nice if it could be shared for optimally configuration of the settings. Ideally it's something that users can then replicate themselves, in order to run verification tests at different granular stages of the DeepVariant workflow - and would be compared to validated metrics for proper evaluation - just to convince the users all is working properly.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/112
https://github.com/google/deepvariant/issues/112:358,integrability,configur,configuration,358,"Yeah, but @pichuan folks will then ask the obvious question of what would be the right number of shards to get the correct variant calls. Probably some validation tests might be required with comparative benchmarks that show consistent predictability. I'm sure you guys have done some of this already, which would be nice if it could be shared for optimally configuration of the settings. Ideally it's something that users can then replicate themselves, in order to run verification tests at different granular stages of the DeepVariant workflow - and would be compared to validated metrics for proper evaluation - just to convince the users all is working properly.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/112
https://github.com/google/deepvariant/issues/112:337,interoperability,share,shared,337,"Yeah, but @pichuan folks will then ask the obvious question of what would be the right number of shards to get the correct variant calls. Probably some validation tests might be required with comparative benchmarks that show consistent predictability. I'm sure you guys have done some of this already, which would be nice if it could be shared for optimally configuration of the settings. Ideally it's something that users can then replicate themselves, in order to run verification tests at different granular stages of the DeepVariant workflow - and would be compared to validated metrics for proper evaluation - just to convince the users all is working properly.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/112
https://github.com/google/deepvariant/issues/112:358,modifiability,configur,configuration,358,"Yeah, but @pichuan folks will then ask the obvious question of what would be the right number of shards to get the correct variant calls. Probably some validation tests might be required with comparative benchmarks that show consistent predictability. I'm sure you guys have done some of this already, which would be nice if it could be shared for optimally configuration of the settings. Ideally it's something that users can then replicate themselves, in order to run verification tests at different granular stages of the DeepVariant workflow - and would be compared to validated metrics for proper evaluation - just to convince the users all is working properly.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/112
https://github.com/google/deepvariant/issues/112:152,safety,valid,validation,152,"Yeah, but @pichuan folks will then ask the obvious question of what would be the right number of shards to get the correct variant calls. Probably some validation tests might be required with comparative benchmarks that show consistent predictability. I'm sure you guys have done some of this already, which would be nice if it could be shared for optimally configuration of the settings. Ideally it's something that users can then replicate themselves, in order to run verification tests at different granular stages of the DeepVariant workflow - and would be compared to validated metrics for proper evaluation - just to convince the users all is working properly.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/112
https://github.com/google/deepvariant/issues/112:163,safety,test,tests,163,"Yeah, but @pichuan folks will then ask the obvious question of what would be the right number of shards to get the correct variant calls. Probably some validation tests might be required with comparative benchmarks that show consistent predictability. I'm sure you guys have done some of this already, which would be nice if it could be shared for optimally configuration of the settings. Ideally it's something that users can then replicate themselves, in order to run verification tests at different granular stages of the DeepVariant workflow - and would be compared to validated metrics for proper evaluation - just to convince the users all is working properly.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/112
https://github.com/google/deepvariant/issues/112:236,safety,predict,predictability,236,"Yeah, but @pichuan folks will then ask the obvious question of what would be the right number of shards to get the correct variant calls. Probably some validation tests might be required with comparative benchmarks that show consistent predictability. I'm sure you guys have done some of this already, which would be nice if it could be shared for optimally configuration of the settings. Ideally it's something that users can then replicate themselves, in order to run verification tests at different granular stages of the DeepVariant workflow - and would be compared to validated metrics for proper evaluation - just to convince the users all is working properly.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/112
https://github.com/google/deepvariant/issues/112:483,safety,test,tests,483,"Yeah, but @pichuan folks will then ask the obvious question of what would be the right number of shards to get the correct variant calls. Probably some validation tests might be required with comparative benchmarks that show consistent predictability. I'm sure you guys have done some of this already, which would be nice if it could be shared for optimally configuration of the settings. Ideally it's something that users can then replicate themselves, in order to run verification tests at different granular stages of the DeepVariant workflow - and would be compared to validated metrics for proper evaluation - just to convince the users all is working properly.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/112
https://github.com/google/deepvariant/issues/112:573,safety,valid,validated,573,"Yeah, but @pichuan folks will then ask the obvious question of what would be the right number of shards to get the correct variant calls. Probably some validation tests might be required with comparative benchmarks that show consistent predictability. I'm sure you guys have done some of this already, which would be nice if it could be shared for optimally configuration of the settings. Ideally it's something that users can then replicate themselves, in order to run verification tests at different granular stages of the DeepVariant workflow - and would be compared to validated metrics for proper evaluation - just to convince the users all is working properly.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/112
https://github.com/google/deepvariant/issues/112:152,security,validat,validation,152,"Yeah, but @pichuan folks will then ask the obvious question of what would be the right number of shards to get the correct variant calls. Probably some validation tests might be required with comparative benchmarks that show consistent predictability. I'm sure you guys have done some of this already, which would be nice if it could be shared for optimally configuration of the settings. Ideally it's something that users can then replicate themselves, in order to run verification tests at different granular stages of the DeepVariant workflow - and would be compared to validated metrics for proper evaluation - just to convince the users all is working properly.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/112
https://github.com/google/deepvariant/issues/112:358,security,configur,configuration,358,"Yeah, but @pichuan folks will then ask the obvious question of what would be the right number of shards to get the correct variant calls. Probably some validation tests might be required with comparative benchmarks that show consistent predictability. I'm sure you guys have done some of this already, which would be nice if it could be shared for optimally configuration of the settings. Ideally it's something that users can then replicate themselves, in order to run verification tests at different granular stages of the DeepVariant workflow - and would be compared to validated metrics for proper evaluation - just to convince the users all is working properly.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/112
https://github.com/google/deepvariant/issues/112:573,security,validat,validated,573,"Yeah, but @pichuan folks will then ask the obvious question of what would be the right number of shards to get the correct variant calls. Probably some validation tests might be required with comparative benchmarks that show consistent predictability. I'm sure you guys have done some of this already, which would be nice if it could be shared for optimally configuration of the settings. Ideally it's something that users can then replicate themselves, in order to run verification tests at different granular stages of the DeepVariant workflow - and would be compared to validated metrics for proper evaluation - just to convince the users all is working properly.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/112
https://github.com/google/deepvariant/issues/112:163,testability,test,tests,163,"Yeah, but @pichuan folks will then ask the obvious question of what would be the right number of shards to get the correct variant calls. Probably some validation tests might be required with comparative benchmarks that show consistent predictability. I'm sure you guys have done some of this already, which would be nice if it could be shared for optimally configuration of the settings. Ideally it's something that users can then replicate themselves, in order to run verification tests at different granular stages of the DeepVariant workflow - and would be compared to validated metrics for proper evaluation - just to convince the users all is working properly.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/112
https://github.com/google/deepvariant/issues/112:470,testability,verif,verification,470,"Yeah, but @pichuan folks will then ask the obvious question of what would be the right number of shards to get the correct variant calls. Probably some validation tests might be required with comparative benchmarks that show consistent predictability. I'm sure you guys have done some of this already, which would be nice if it could be shared for optimally configuration of the settings. Ideally it's something that users can then replicate themselves, in order to run verification tests at different granular stages of the DeepVariant workflow - and would be compared to validated metrics for proper evaluation - just to convince the users all is working properly.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/112
https://github.com/google/deepvariant/issues/112:483,testability,test,tests,483,"Yeah, but @pichuan folks will then ask the obvious question of what would be the right number of shards to get the correct variant calls. Probably some validation tests might be required with comparative benchmarks that show consistent predictability. I'm sure you guys have done some of this already, which would be nice if it could be shared for optimally configuration of the settings. Ideally it's something that users can then replicate themselves, in order to run verification tests at different granular stages of the DeepVariant workflow - and would be compared to validated metrics for proper evaluation - just to convince the users all is working properly.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/112
https://github.com/google/deepvariant/issues/112:225,usability,consist,consistent,225,"Yeah, but @pichuan folks will then ask the obvious question of what would be the right number of shards to get the correct variant calls. Probably some validation tests might be required with comparative benchmarks that show consistent predictability. I'm sure you guys have done some of this already, which would be nice if it could be shared for optimally configuration of the settings. Ideally it's something that users can then replicate themselves, in order to run verification tests at different granular stages of the DeepVariant workflow - and would be compared to validated metrics for proper evaluation - just to convince the users all is working properly.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/112
https://github.com/google/deepvariant/issues/112:417,usability,user,users,417,"Yeah, but @pichuan folks will then ask the obvious question of what would be the right number of shards to get the correct variant calls. Probably some validation tests might be required with comparative benchmarks that show consistent predictability. I'm sure you guys have done some of this already, which would be nice if it could be shared for optimally configuration of the settings. Ideally it's something that users can then replicate themselves, in order to run verification tests at different granular stages of the DeepVariant workflow - and would be compared to validated metrics for proper evaluation - just to convince the users all is working properly.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/112
https://github.com/google/deepvariant/issues/112:537,usability,workflow,workflow,537,"Yeah, but @pichuan folks will then ask the obvious question of what would be the right number of shards to get the correct variant calls. Probably some validation tests might be required with comparative benchmarks that show consistent predictability. I'm sure you guys have done some of this already, which would be nice if it could be shared for optimally configuration of the settings. Ideally it's something that users can then replicate themselves, in order to run verification tests at different granular stages of the DeepVariant workflow - and would be compared to validated metrics for proper evaluation - just to convince the users all is working properly.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/112
https://github.com/google/deepvariant/issues/112:636,usability,user,users,636,"Yeah, but @pichuan folks will then ask the obvious question of what would be the right number of shards to get the correct variant calls. Probably some validation tests might be required with comparative benchmarks that show consistent predictability. I'm sure you guys have done some of this already, which would be nice if it could be shared for optimally configuration of the settings. Ideally it's something that users can then replicate themselves, in order to run verification tests at different granular stages of the DeepVariant workflow - and would be compared to validated metrics for proper evaluation - just to convince the users all is working properly.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/112
https://github.com/google/deepvariant/issues/112:384,availability,down,down,384,"This was really puzzling me, so I started with a smallest possible reproducible setting and started debugging. . Now I understand why the results are different. . This update will be about the technical explanation of why different number of shards result in different result:. This is because when we're processing reads in a region, we (by default) first do a sampling of the reads down to `max_reads_per_partition` (default=1500) reads. Here is the code snippets that does that:. https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py#L842-L844. ```. if self.options.max_reads_per_partition > 0:. reads = utils.reservoir_sample(. reads, self.options.max_reads_per_partition, self.random). ```. This was originally an optimization for speed. A while ago we noticed that some regions in the genome took really long time to create examples. And this is because some regions have a lot of reads piled up. Because of the way that germline DeepVariant is set up, the final pileup image has only a image height of 100 anyway, so we don't really need to process all the reads. But at this stage we still need to go through the realigner step later, so we decided to sample enough reads here so that the following steps can still use more information, but we cut off at the number of `max_reads_per_partition` (default=1500) reads. The randomization here is done with `self.random`, which is initialize with the same seed at the beginning. Because the seed is the same at the beginning, we're able to generate deterministic and reproducible results if you fix the number of tasks. However, when you run make_examples with different number of tasks, that means each task has its own `self.random`. Even though each of them started with the same seed, when you have different number of tasks(shards), these RandomState ends up progressing differently. This means when you look at a same region in two different runs (for example, I was looking at `20:3208913-3209912` in the exome), the",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/112
https://github.com/google/deepvariant/issues/112:2807,availability,slo,slower,2807,"that germline DeepVariant is set up, the final pileup image has only a image height of 100 anyway, so we don't really need to process all the reads. But at this stage we still need to go through the realigner step later, so we decided to sample enough reads here so that the following steps can still use more information, but we cut off at the number of `max_reads_per_partition` (default=1500) reads. The randomization here is done with `self.random`, which is initialize with the same seed at the beginning. Because the seed is the same at the beginning, we're able to generate deterministic and reproducible results if you fix the number of tasks. However, when you run make_examples with different number of tasks, that means each task has its own `self.random`. Even though each of them started with the same seed, when you have different number of tasks(shards), these RandomState ends up progressing differently. This means when you look at a same region in two different runs (for example, I was looking at `20:3208913-3209912` in the exome), they might be going through a different place of the random number. That is the reason why if you run with different number of shards, the results are different. @pgrosu , based on the reason above, you can understand that the number of tasks/shards won't actually correlate or explain which variant calling results will be better. Currently our goal was to make sure every run with the same parameters has deterministic and reproducible results. I think we're still achieving that. I need to adjust my previous belief that ""our results coming out from `make_examples would be the same"" is incorrect. The reason is explained above. With the current code, if you want all runs (even with different number of shards) to be the same, you can set `--max_reads_per_partition 0`. This might cause the run to be much slower though. But after that, the results should be the same. If after adding that flag, results are still different, please let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/112
https://github.com/google/deepvariant/issues/112:168,deployability,updat,update,168,"This was really puzzling me, so I started with a smallest possible reproducible setting and started debugging. . Now I understand why the results are different. . This update will be about the technical explanation of why different number of shards result in different result:. This is because when we're processing reads in a region, we (by default) first do a sampling of the reads down to `max_reads_per_partition` (default=1500) reads. Here is the code snippets that does that:. https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py#L842-L844. ```. if self.options.max_reads_per_partition > 0:. reads = utils.reservoir_sample(. reads, self.options.max_reads_per_partition, self.random). ```. This was originally an optimization for speed. A while ago we noticed that some regions in the genome took really long time to create examples. And this is because some regions have a lot of reads piled up. Because of the way that germline DeepVariant is set up, the final pileup image has only a image height of 100 anyway, so we don't really need to process all the reads. But at this stage we still need to go through the realigner step later, so we decided to sample enough reads here so that the following steps can still use more information, but we cut off at the number of `max_reads_per_partition` (default=1500) reads. The randomization here is done with `self.random`, which is initialize with the same seed at the beginning. Because the seed is the same at the beginning, we're able to generate deterministic and reproducible results if you fix the number of tasks. However, when you run make_examples with different number of tasks, that means each task has its own `self.random`. Even though each of them started with the same seed, when you have different number of tasks(shards), these RandomState ends up progressing differently. This means when you look at a same region in two different runs (for example, I was looking at `20:3208913-3209912` in the exome), the",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/112
https://github.com/google/deepvariant/issues/112:1106,deployability,stage,stage,1106,". . Now I understand why the results are different. . This update will be about the technical explanation of why different number of shards result in different result:. This is because when we're processing reads in a region, we (by default) first do a sampling of the reads down to `max_reads_per_partition` (default=1500) reads. Here is the code snippets that does that:. https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py#L842-L844. ```. if self.options.max_reads_per_partition > 0:. reads = utils.reservoir_sample(. reads, self.options.max_reads_per_partition, self.random). ```. This was originally an optimization for speed. A while ago we noticed that some regions in the genome took really long time to create examples. And this is because some regions have a lot of reads piled up. Because of the way that germline DeepVariant is set up, the final pileup image has only a image height of 100 anyway, so we don't really need to process all the reads. But at this stage we still need to go through the realigner step later, so we decided to sample enough reads here so that the following steps can still use more information, but we cut off at the number of `max_reads_per_partition` (default=1500) reads. The randomization here is done with `self.random`, which is initialize with the same seed at the beginning. Because the seed is the same at the beginning, we're able to generate deterministic and reproducible results if you fix the number of tasks. However, when you run make_examples with different number of tasks, that means each task has its own `self.random`. Even though each of them started with the same seed, when you have different number of tasks(shards), these RandomState ends up progressing differently. This means when you look at a same region in two different runs (for example, I was looking at `20:3208913-3209912` in the exome), they might be going through a different place of the random number. That is the reason why if you run with diffe",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/112
https://github.com/google/deepvariant/issues/112:742,energy efficiency,optim,optimization,742,"This was really puzzling me, so I started with a smallest possible reproducible setting and started debugging. . Now I understand why the results are different. . This update will be about the technical explanation of why different number of shards result in different result:. This is because when we're processing reads in a region, we (by default) first do a sampling of the reads down to `max_reads_per_partition` (default=1500) reads. Here is the code snippets that does that:. https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py#L842-L844. ```. if self.options.max_reads_per_partition > 0:. reads = utils.reservoir_sample(. reads, self.options.max_reads_per_partition, self.random). ```. This was originally an optimization for speed. A while ago we noticed that some regions in the genome took really long time to create examples. And this is because some regions have a lot of reads piled up. Because of the way that germline DeepVariant is set up, the final pileup image has only a image height of 100 anyway, so we don't really need to process all the reads. But at this stage we still need to go through the realigner step later, so we decided to sample enough reads here so that the following steps can still use more information, but we cut off at the number of `max_reads_per_partition` (default=1500) reads. The randomization here is done with `self.random`, which is initialize with the same seed at the beginning. Because the seed is the same at the beginning, we're able to generate deterministic and reproducible results if you fix the number of tasks. However, when you run make_examples with different number of tasks, that means each task has its own `self.random`. Even though each of them started with the same seed, when you have different number of tasks(shards), these RandomState ends up progressing differently. This means when you look at a same region in two different runs (for example, I was looking at `20:3208913-3209912` in the exome), the",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/112
https://github.com/google/deepvariant/issues/112:2329,energy efficiency,Current,Currently,2329,"that germline DeepVariant is set up, the final pileup image has only a image height of 100 anyway, so we don't really need to process all the reads. But at this stage we still need to go through the realigner step later, so we decided to sample enough reads here so that the following steps can still use more information, but we cut off at the number of `max_reads_per_partition` (default=1500) reads. The randomization here is done with `self.random`, which is initialize with the same seed at the beginning. Because the seed is the same at the beginning, we're able to generate deterministic and reproducible results if you fix the number of tasks. However, when you run make_examples with different number of tasks, that means each task has its own `self.random`. Even though each of them started with the same seed, when you have different number of tasks(shards), these RandomState ends up progressing differently. This means when you look at a same region in two different runs (for example, I was looking at `20:3208913-3209912` in the exome), they might be going through a different place of the random number. That is the reason why if you run with different number of shards, the results are different. @pgrosu , based on the reason above, you can understand that the number of tasks/shards won't actually correlate or explain which variant calling results will be better. Currently our goal was to make sure every run with the same parameters has deterministic and reproducible results. I think we're still achieving that. I need to adjust my previous belief that ""our results coming out from `make_examples would be the same"" is incorrect. The reason is explained above. With the current code, if you want all runs (even with different number of shards) to be the same, you can set `--max_reads_per_partition 0`. This might cause the run to be much slower though. But after that, the results should be the same. If after adding that flag, results are still different, please let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/112
https://github.com/google/deepvariant/issues/112:2638,energy efficiency,current,current,2638,"that germline DeepVariant is set up, the final pileup image has only a image height of 100 anyway, so we don't really need to process all the reads. But at this stage we still need to go through the realigner step later, so we decided to sample enough reads here so that the following steps can still use more information, but we cut off at the number of `max_reads_per_partition` (default=1500) reads. The randomization here is done with `self.random`, which is initialize with the same seed at the beginning. Because the seed is the same at the beginning, we're able to generate deterministic and reproducible results if you fix the number of tasks. However, when you run make_examples with different number of tasks, that means each task has its own `self.random`. Even though each of them started with the same seed, when you have different number of tasks(shards), these RandomState ends up progressing differently. This means when you look at a same region in two different runs (for example, I was looking at `20:3208913-3209912` in the exome), they might be going through a different place of the random number. That is the reason why if you run with different number of shards, the results are different. @pgrosu , based on the reason above, you can understand that the number of tasks/shards won't actually correlate or explain which variant calling results will be better. Currently our goal was to make sure every run with the same parameters has deterministic and reproducible results. I think we're still achieving that. I need to adjust my previous belief that ""our results coming out from `make_examples would be the same"" is incorrect. The reason is explained above. With the current code, if you want all runs (even with different number of shards) to be the same, you can set `--max_reads_per_partition 0`. This might cause the run to be much slower though. But after that, the results should be the same. If after adding that flag, results are still different, please let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/112
https://github.com/google/deepvariant/issues/112:2389,modifiability,paramet,parameters,2389,"that germline DeepVariant is set up, the final pileup image has only a image height of 100 anyway, so we don't really need to process all the reads. But at this stage we still need to go through the realigner step later, so we decided to sample enough reads here so that the following steps can still use more information, but we cut off at the number of `max_reads_per_partition` (default=1500) reads. The randomization here is done with `self.random`, which is initialize with the same seed at the beginning. Because the seed is the same at the beginning, we're able to generate deterministic and reproducible results if you fix the number of tasks. However, when you run make_examples with different number of tasks, that means each task has its own `self.random`. Even though each of them started with the same seed, when you have different number of tasks(shards), these RandomState ends up progressing differently. This means when you look at a same region in two different runs (for example, I was looking at `20:3208913-3209912` in the exome), they might be going through a different place of the random number. That is the reason why if you run with different number of shards, the results are different. @pgrosu , based on the reason above, you can understand that the number of tasks/shards won't actually correlate or explain which variant calling results will be better. Currently our goal was to make sure every run with the same parameters has deterministic and reproducible results. I think we're still achieving that. I need to adjust my previous belief that ""our results coming out from `make_examples would be the same"" is incorrect. The reason is explained above. With the current code, if you want all runs (even with different number of shards) to be the same, you can set `--max_reads_per_partition 0`. This might cause the run to be much slower though. But after that, the results should be the same. If after adding that flag, results are still different, please let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/112
https://github.com/google/deepvariant/issues/112:742,performance,optimiz,optimization,742,"This was really puzzling me, so I started with a smallest possible reproducible setting and started debugging. . Now I understand why the results are different. . This update will be about the technical explanation of why different number of shards result in different result:. This is because when we're processing reads in a region, we (by default) first do a sampling of the reads down to `max_reads_per_partition` (default=1500) reads. Here is the code snippets that does that:. https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py#L842-L844. ```. if self.options.max_reads_per_partition > 0:. reads = utils.reservoir_sample(. reads, self.options.max_reads_per_partition, self.random). ```. This was originally an optimization for speed. A while ago we noticed that some regions in the genome took really long time to create examples. And this is because some regions have a lot of reads piled up. Because of the way that germline DeepVariant is set up, the final pileup image has only a image height of 100 anyway, so we don't really need to process all the reads. But at this stage we still need to go through the realigner step later, so we decided to sample enough reads here so that the following steps can still use more information, but we cut off at the number of `max_reads_per_partition` (default=1500) reads. The randomization here is done with `self.random`, which is initialize with the same seed at the beginning. Because the seed is the same at the beginning, we're able to generate deterministic and reproducible results if you fix the number of tasks. However, when you run make_examples with different number of tasks, that means each task has its own `self.random`. Even though each of them started with the same seed, when you have different number of tasks(shards), these RandomState ends up progressing differently. This means when you look at a same region in two different runs (for example, I was looking at `20:3208913-3209912` in the exome), the",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/112
https://github.com/google/deepvariant/issues/112:838,performance,time,time,838,"This was really puzzling me, so I started with a smallest possible reproducible setting and started debugging. . Now I understand why the results are different. . This update will be about the technical explanation of why different number of shards result in different result:. This is because when we're processing reads in a region, we (by default) first do a sampling of the reads down to `max_reads_per_partition` (default=1500) reads. Here is the code snippets that does that:. https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py#L842-L844. ```. if self.options.max_reads_per_partition > 0:. reads = utils.reservoir_sample(. reads, self.options.max_reads_per_partition, self.random). ```. This was originally an optimization for speed. A while ago we noticed that some regions in the genome took really long time to create examples. And this is because some regions have a lot of reads piled up. Because of the way that germline DeepVariant is set up, the final pileup image has only a image height of 100 anyway, so we don't really need to process all the reads. But at this stage we still need to go through the realigner step later, so we decided to sample enough reads here so that the following steps can still use more information, but we cut off at the number of `max_reads_per_partition` (default=1500) reads. The randomization here is done with `self.random`, which is initialize with the same seed at the beginning. Because the seed is the same at the beginning, we're able to generate deterministic and reproducible results if you fix the number of tasks. However, when you run make_examples with different number of tasks, that means each task has its own `self.random`. Even though each of them started with the same seed, when you have different number of tasks(shards), these RandomState ends up progressing differently. This means when you look at a same region in two different runs (for example, I was looking at `20:3208913-3209912` in the exome), the",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/112
https://github.com/google/deepvariant/issues/112:471,reliability,doe,does,471,"This was really puzzling me, so I started with a smallest possible reproducible setting and started debugging. . Now I understand why the results are different. . This update will be about the technical explanation of why different number of shards result in different result:. This is because when we're processing reads in a region, we (by default) first do a sampling of the reads down to `max_reads_per_partition` (default=1500) reads. Here is the code snippets that does that:. https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py#L842-L844. ```. if self.options.max_reads_per_partition > 0:. reads = utils.reservoir_sample(. reads, self.options.max_reads_per_partition, self.random). ```. This was originally an optimization for speed. A while ago we noticed that some regions in the genome took really long time to create examples. And this is because some regions have a lot of reads piled up. Because of the way that germline DeepVariant is set up, the final pileup image has only a image height of 100 anyway, so we don't really need to process all the reads. But at this stage we still need to go through the realigner step later, so we decided to sample enough reads here so that the following steps can still use more information, but we cut off at the number of `max_reads_per_partition` (default=1500) reads. The randomization here is done with `self.random`, which is initialize with the same seed at the beginning. Because the seed is the same at the beginning, we're able to generate deterministic and reproducible results if you fix the number of tasks. However, when you run make_examples with different number of tasks, that means each task has its own `self.random`. Even though each of them started with the same seed, when you have different number of tasks(shards), these RandomState ends up progressing differently. This means when you look at a same region in two different runs (for example, I was looking at `20:3208913-3209912` in the exome), the",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/112
https://github.com/google/deepvariant/issues/112:2807,reliability,slo,slower,2807,"that germline DeepVariant is set up, the final pileup image has only a image height of 100 anyway, so we don't really need to process all the reads. But at this stage we still need to go through the realigner step later, so we decided to sample enough reads here so that the following steps can still use more information, but we cut off at the number of `max_reads_per_partition` (default=1500) reads. The randomization here is done with `self.random`, which is initialize with the same seed at the beginning. Because the seed is the same at the beginning, we're able to generate deterministic and reproducible results if you fix the number of tasks. However, when you run make_examples with different number of tasks, that means each task has its own `self.random`. Even though each of them started with the same seed, when you have different number of tasks(shards), these RandomState ends up progressing differently. This means when you look at a same region in two different runs (for example, I was looking at `20:3208913-3209912` in the exome), they might be going through a different place of the random number. That is the reason why if you run with different number of shards, the results are different. @pgrosu , based on the reason above, you can understand that the number of tasks/shards won't actually correlate or explain which variant calling results will be better. Currently our goal was to make sure every run with the same parameters has deterministic and reproducible results. I think we're still achieving that. I need to adjust my previous belief that ""our results coming out from `make_examples would be the same"" is incorrect. The reason is explained above. With the current code, if you want all runs (even with different number of shards) to be the same, you can set `--max_reads_per_partition 0`. This might cause the run to be much slower though. But after that, the results should be the same. If after adding that flag, results are still different, please let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/112
https://github.com/google/deepvariant/issues/112:168,safety,updat,update,168,"This was really puzzling me, so I started with a smallest possible reproducible setting and started debugging. . Now I understand why the results are different. . This update will be about the technical explanation of why different number of shards result in different result:. This is because when we're processing reads in a region, we (by default) first do a sampling of the reads down to `max_reads_per_partition` (default=1500) reads. Here is the code snippets that does that:. https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py#L842-L844. ```. if self.options.max_reads_per_partition > 0:. reads = utils.reservoir_sample(. reads, self.options.max_reads_per_partition, self.random). ```. This was originally an optimization for speed. A while ago we noticed that some regions in the genome took really long time to create examples. And this is because some regions have a lot of reads piled up. Because of the way that germline DeepVariant is set up, the final pileup image has only a image height of 100 anyway, so we don't really need to process all the reads. But at this stage we still need to go through the realigner step later, so we decided to sample enough reads here so that the following steps can still use more information, but we cut off at the number of `max_reads_per_partition` (default=1500) reads. The randomization here is done with `self.random`, which is initialize with the same seed at the beginning. Because the seed is the same at the beginning, we're able to generate deterministic and reproducible results if you fix the number of tasks. However, when you run make_examples with different number of tasks, that means each task has its own `self.random`. Even though each of them started with the same seed, when you have different number of tasks(shards), these RandomState ends up progressing differently. This means when you look at a same region in two different runs (for example, I was looking at `20:3208913-3209912` in the exome), the",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/112
https://github.com/google/deepvariant/issues/112:168,security,updat,update,168,"This was really puzzling me, so I started with a smallest possible reproducible setting and started debugging. . Now I understand why the results are different. . This update will be about the technical explanation of why different number of shards result in different result:. This is because when we're processing reads in a region, we (by default) first do a sampling of the reads down to `max_reads_per_partition` (default=1500) reads. Here is the code snippets that does that:. https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py#L842-L844. ```. if self.options.max_reads_per_partition > 0:. reads = utils.reservoir_sample(. reads, self.options.max_reads_per_partition, self.random). ```. This was originally an optimization for speed. A while ago we noticed that some regions in the genome took really long time to create examples. And this is because some regions have a lot of reads piled up. Because of the way that germline DeepVariant is set up, the final pileup image has only a image height of 100 anyway, so we don't really need to process all the reads. But at this stage we still need to go through the realigner step later, so we decided to sample enough reads here so that the following steps can still use more information, but we cut off at the number of `max_reads_per_partition` (default=1500) reads. The randomization here is done with `self.random`, which is initialize with the same seed at the beginning. Because the seed is the same at the beginning, we're able to generate deterministic and reproducible results if you fix the number of tasks. However, when you run make_examples with different number of tasks, that means each task has its own `self.random`. Even though each of them started with the same seed, when you have different number of tasks(shards), these RandomState ends up progressing differently. This means when you look at a same region in two different runs (for example, I was looking at `20:3208913-3209912` in the exome), the",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/112
https://github.com/google/deepvariant/issues/112:119,testability,understand,understand,119,"This was really puzzling me, so I started with a smallest possible reproducible setting and started debugging. . Now I understand why the results are different. . This update will be about the technical explanation of why different number of shards result in different result:. This is because when we're processing reads in a region, we (by default) first do a sampling of the reads down to `max_reads_per_partition` (default=1500) reads. Here is the code snippets that does that:. https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py#L842-L844. ```. if self.options.max_reads_per_partition > 0:. reads = utils.reservoir_sample(. reads, self.options.max_reads_per_partition, self.random). ```. This was originally an optimization for speed. A while ago we noticed that some regions in the genome took really long time to create examples. And this is because some regions have a lot of reads piled up. Because of the way that germline DeepVariant is set up, the final pileup image has only a image height of 100 anyway, so we don't really need to process all the reads. But at this stage we still need to go through the realigner step later, so we decided to sample enough reads here so that the following steps can still use more information, but we cut off at the number of `max_reads_per_partition` (default=1500) reads. The randomization here is done with `self.random`, which is initialize with the same seed at the beginning. Because the seed is the same at the beginning, we're able to generate deterministic and reproducible results if you fix the number of tasks. However, when you run make_examples with different number of tasks, that means each task has its own `self.random`. Even though each of them started with the same seed, when you have different number of tasks(shards), these RandomState ends up progressing differently. This means when you look at a same region in two different runs (for example, I was looking at `20:3208913-3209912` in the exome), the",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/112
https://github.com/google/deepvariant/issues/112:2204,testability,understand,understand,2204,"that germline DeepVariant is set up, the final pileup image has only a image height of 100 anyway, so we don't really need to process all the reads. But at this stage we still need to go through the realigner step later, so we decided to sample enough reads here so that the following steps can still use more information, but we cut off at the number of `max_reads_per_partition` (default=1500) reads. The randomization here is done with `self.random`, which is initialize with the same seed at the beginning. Because the seed is the same at the beginning, we're able to generate deterministic and reproducible results if you fix the number of tasks. However, when you run make_examples with different number of tasks, that means each task has its own `self.random`. Even though each of them started with the same seed, when you have different number of tasks(shards), these RandomState ends up progressing differently. This means when you look at a same region in two different runs (for example, I was looking at `20:3208913-3209912` in the exome), they might be going through a different place of the random number. That is the reason why if you run with different number of shards, the results are different. @pgrosu , based on the reason above, you can understand that the number of tasks/shards won't actually correlate or explain which variant calling results will be better. Currently our goal was to make sure every run with the same parameters has deterministic and reproducible results. I think we're still achieving that. I need to adjust my previous belief that ""our results coming out from `make_examples would be the same"" is incorrect. The reason is explained above. With the current code, if you want all runs (even with different number of shards) to be the same, you can set `--max_reads_per_partition 0`. This might cause the run to be much slower though. But after that, the results should be the same. If after adding that flag, results are still different, please let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/112
https://github.com/google/deepvariant/issues/112:1841,usability,progress,progressing,1841,"reate examples. And this is because some regions have a lot of reads piled up. Because of the way that germline DeepVariant is set up, the final pileup image has only a image height of 100 anyway, so we don't really need to process all the reads. But at this stage we still need to go through the realigner step later, so we decided to sample enough reads here so that the following steps can still use more information, but we cut off at the number of `max_reads_per_partition` (default=1500) reads. The randomization here is done with `self.random`, which is initialize with the same seed at the beginning. Because the seed is the same at the beginning, we're able to generate deterministic and reproducible results if you fix the number of tasks. However, when you run make_examples with different number of tasks, that means each task has its own `self.random`. Even though each of them started with the same seed, when you have different number of tasks(shards), these RandomState ends up progressing differently. This means when you look at a same region in two different runs (for example, I was looking at `20:3208913-3209912` in the exome), they might be going through a different place of the random number. That is the reason why if you run with different number of shards, the results are different. @pgrosu , based on the reason above, you can understand that the number of tasks/shards won't actually correlate or explain which variant calling results will be better. Currently our goal was to make sure every run with the same parameters has deterministic and reproducible results. I think we're still achieving that. I need to adjust my previous belief that ""our results coming out from `make_examples would be the same"" is incorrect. The reason is explained above. With the current code, if you want all runs (even with different number of shards) to be the same, you can set `--max_reads_per_partition 0`. This might cause the run to be much slower though. But after that, the resul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/112
https://github.com/google/deepvariant/issues/112:138,deployability,releas,release,138,"Hi @chris-sf . Thanks again for reporting this issue. We made an internal change to address this issue, which should come out in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/112
https://github.com/google/deepvariant/issues/114:1299,energy efficiency,measur,measure,1299,"This is a very interesting question, and the answers to it are complex. I assume for the sake of the question that you have FASTQ data from sequencing a single bacterial colony (so this is not a metagenomics question). Let's divide the question between ""can it technically be done"" and ""will the answers be scientifically valid"". To the question ""can it technically be done"", the answer is probably yes. I am not aware that we have specifically attempted this in bacteria. But if you have a FASTA file with a reference for a species and FASTQ reads, you should be able to generate variant calls for it. To the question ""will the answers be scientifically valid"", it is important to note calling variants in bacterial genomes is an area of open research. Using DeepVariant is reasonable, but I don't think you'll able to consider the output of any method (DeepVariant or other) as certain to give you fully correct results on this problem right out of the box. You'll want to use a few methods (use Freebayes and GATK) and compare between them with metrics you can independently validate, then decide what works and doesn't for your use case. One way to do this could be that for a clonal lineage you expect variants to all be called as 1/1 for the non-plasmid genome sequence. Ryan Poplin used this measure in a similar way to compare DeepVariant and other methods on inbred rice strains from the 3000 Rice Genomes Project. We would be quite interested to receive your feedback on how DeepVariant performs in this use case, as this may help us understand the value of DeepVariant and improve it for the community.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:432,interoperability,specif,specifically,432,"This is a very interesting question, and the answers to it are complex. I assume for the sake of the question that you have FASTQ data from sequencing a single bacterial colony (so this is not a metagenomics question). Let's divide the question between ""can it technically be done"" and ""will the answers be scientifically valid"". To the question ""can it technically be done"", the answer is probably yes. I am not aware that we have specifically attempted this in bacteria. But if you have a FASTA file with a reference for a species and FASTQ reads, you should be able to generate variant calls for it. To the question ""will the answers be scientifically valid"", it is important to note calling variants in bacterial genomes is an area of open research. Using DeepVariant is reasonable, but I don't think you'll able to consider the output of any method (DeepVariant or other) as certain to give you fully correct results on this problem right out of the box. You'll want to use a few methods (use Freebayes and GATK) and compare between them with metrics you can independently validate, then decide what works and doesn't for your use case. One way to do this could be that for a clonal lineage you expect variants to all be called as 1/1 for the non-plasmid genome sequence. Ryan Poplin used this measure in a similar way to compare DeepVariant and other methods on inbred rice strains from the 3000 Rice Genomes Project. We would be quite interested to receive your feedback on how DeepVariant performs in this use case, as this may help us understand the value of DeepVariant and improve it for the community.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:1497,performance,perform,performs,1497,"This is a very interesting question, and the answers to it are complex. I assume for the sake of the question that you have FASTQ data from sequencing a single bacterial colony (so this is not a metagenomics question). Let's divide the question between ""can it technically be done"" and ""will the answers be scientifically valid"". To the question ""can it technically be done"", the answer is probably yes. I am not aware that we have specifically attempted this in bacteria. But if you have a FASTA file with a reference for a species and FASTQ reads, you should be able to generate variant calls for it. To the question ""will the answers be scientifically valid"", it is important to note calling variants in bacterial genomes is an area of open research. Using DeepVariant is reasonable, but I don't think you'll able to consider the output of any method (DeepVariant or other) as certain to give you fully correct results on this problem right out of the box. You'll want to use a few methods (use Freebayes and GATK) and compare between them with metrics you can independently validate, then decide what works and doesn't for your use case. One way to do this could be that for a clonal lineage you expect variants to all be called as 1/1 for the non-plasmid genome sequence. Ryan Poplin used this measure in a similar way to compare DeepVariant and other methods on inbred rice strains from the 3000 Rice Genomes Project. We would be quite interested to receive your feedback on how DeepVariant performs in this use case, as this may help us understand the value of DeepVariant and improve it for the community.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:1115,reliability,doe,doesn,1115,"This is a very interesting question, and the answers to it are complex. I assume for the sake of the question that you have FASTQ data from sequencing a single bacterial colony (so this is not a metagenomics question). Let's divide the question between ""can it technically be done"" and ""will the answers be scientifically valid"". To the question ""can it technically be done"", the answer is probably yes. I am not aware that we have specifically attempted this in bacteria. But if you have a FASTA file with a reference for a species and FASTQ reads, you should be able to generate variant calls for it. To the question ""will the answers be scientifically valid"", it is important to note calling variants in bacterial genomes is an area of open research. Using DeepVariant is reasonable, but I don't think you'll able to consider the output of any method (DeepVariant or other) as certain to give you fully correct results on this problem right out of the box. You'll want to use a few methods (use Freebayes and GATK) and compare between them with metrics you can independently validate, then decide what works and doesn't for your use case. One way to do this could be that for a clonal lineage you expect variants to all be called as 1/1 for the non-plasmid genome sequence. Ryan Poplin used this measure in a similar way to compare DeepVariant and other methods on inbred rice strains from the 3000 Rice Genomes Project. We would be quite interested to receive your feedback on how DeepVariant performs in this use case, as this may help us understand the value of DeepVariant and improve it for the community.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:63,safety,compl,complex,63,"This is a very interesting question, and the answers to it are complex. I assume for the sake of the question that you have FASTQ data from sequencing a single bacterial colony (so this is not a metagenomics question). Let's divide the question between ""can it technically be done"" and ""will the answers be scientifically valid"". To the question ""can it technically be done"", the answer is probably yes. I am not aware that we have specifically attempted this in bacteria. But if you have a FASTA file with a reference for a species and FASTQ reads, you should be able to generate variant calls for it. To the question ""will the answers be scientifically valid"", it is important to note calling variants in bacterial genomes is an area of open research. Using DeepVariant is reasonable, but I don't think you'll able to consider the output of any method (DeepVariant or other) as certain to give you fully correct results on this problem right out of the box. You'll want to use a few methods (use Freebayes and GATK) and compare between them with metrics you can independently validate, then decide what works and doesn't for your use case. One way to do this could be that for a clonal lineage you expect variants to all be called as 1/1 for the non-plasmid genome sequence. Ryan Poplin used this measure in a similar way to compare DeepVariant and other methods on inbred rice strains from the 3000 Rice Genomes Project. We would be quite interested to receive your feedback on how DeepVariant performs in this use case, as this may help us understand the value of DeepVariant and improve it for the community.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:322,safety,valid,valid,322,"This is a very interesting question, and the answers to it are complex. I assume for the sake of the question that you have FASTQ data from sequencing a single bacterial colony (so this is not a metagenomics question). Let's divide the question between ""can it technically be done"" and ""will the answers be scientifically valid"". To the question ""can it technically be done"", the answer is probably yes. I am not aware that we have specifically attempted this in bacteria. But if you have a FASTA file with a reference for a species and FASTQ reads, you should be able to generate variant calls for it. To the question ""will the answers be scientifically valid"", it is important to note calling variants in bacterial genomes is an area of open research. Using DeepVariant is reasonable, but I don't think you'll able to consider the output of any method (DeepVariant or other) as certain to give you fully correct results on this problem right out of the box. You'll want to use a few methods (use Freebayes and GATK) and compare between them with metrics you can independently validate, then decide what works and doesn't for your use case. One way to do this could be that for a clonal lineage you expect variants to all be called as 1/1 for the non-plasmid genome sequence. Ryan Poplin used this measure in a similar way to compare DeepVariant and other methods on inbred rice strains from the 3000 Rice Genomes Project. We would be quite interested to receive your feedback on how DeepVariant performs in this use case, as this may help us understand the value of DeepVariant and improve it for the community.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:655,safety,valid,valid,655,"This is a very interesting question, and the answers to it are complex. I assume for the sake of the question that you have FASTQ data from sequencing a single bacterial colony (so this is not a metagenomics question). Let's divide the question between ""can it technically be done"" and ""will the answers be scientifically valid"". To the question ""can it technically be done"", the answer is probably yes. I am not aware that we have specifically attempted this in bacteria. But if you have a FASTA file with a reference for a species and FASTQ reads, you should be able to generate variant calls for it. To the question ""will the answers be scientifically valid"", it is important to note calling variants in bacterial genomes is an area of open research. Using DeepVariant is reasonable, but I don't think you'll able to consider the output of any method (DeepVariant or other) as certain to give you fully correct results on this problem right out of the box. You'll want to use a few methods (use Freebayes and GATK) and compare between them with metrics you can independently validate, then decide what works and doesn't for your use case. One way to do this could be that for a clonal lineage you expect variants to all be called as 1/1 for the non-plasmid genome sequence. Ryan Poplin used this measure in a similar way to compare DeepVariant and other methods on inbred rice strains from the 3000 Rice Genomes Project. We would be quite interested to receive your feedback on how DeepVariant performs in this use case, as this may help us understand the value of DeepVariant and improve it for the community.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:1078,safety,valid,validate,1078,"This is a very interesting question, and the answers to it are complex. I assume for the sake of the question that you have FASTQ data from sequencing a single bacterial colony (so this is not a metagenomics question). Let's divide the question between ""can it technically be done"" and ""will the answers be scientifically valid"". To the question ""can it technically be done"", the answer is probably yes. I am not aware that we have specifically attempted this in bacteria. But if you have a FASTA file with a reference for a species and FASTQ reads, you should be able to generate variant calls for it. To the question ""will the answers be scientifically valid"", it is important to note calling variants in bacterial genomes is an area of open research. Using DeepVariant is reasonable, but I don't think you'll able to consider the output of any method (DeepVariant or other) as certain to give you fully correct results on this problem right out of the box. You'll want to use a few methods (use Freebayes and GATK) and compare between them with metrics you can independently validate, then decide what works and doesn't for your use case. One way to do this could be that for a clonal lineage you expect variants to all be called as 1/1 for the non-plasmid genome sequence. Ryan Poplin used this measure in a similar way to compare DeepVariant and other methods on inbred rice strains from the 3000 Rice Genomes Project. We would be quite interested to receive your feedback on how DeepVariant performs in this use case, as this may help us understand the value of DeepVariant and improve it for the community.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:63,security,compl,complex,63,"This is a very interesting question, and the answers to it are complex. I assume for the sake of the question that you have FASTQ data from sequencing a single bacterial colony (so this is not a metagenomics question). Let's divide the question between ""can it technically be done"" and ""will the answers be scientifically valid"". To the question ""can it technically be done"", the answer is probably yes. I am not aware that we have specifically attempted this in bacteria. But if you have a FASTA file with a reference for a species and FASTQ reads, you should be able to generate variant calls for it. To the question ""will the answers be scientifically valid"", it is important to note calling variants in bacterial genomes is an area of open research. Using DeepVariant is reasonable, but I don't think you'll able to consider the output of any method (DeepVariant or other) as certain to give you fully correct results on this problem right out of the box. You'll want to use a few methods (use Freebayes and GATK) and compare between them with metrics you can independently validate, then decide what works and doesn't for your use case. One way to do this could be that for a clonal lineage you expect variants to all be called as 1/1 for the non-plasmid genome sequence. Ryan Poplin used this measure in a similar way to compare DeepVariant and other methods on inbred rice strains from the 3000 Rice Genomes Project. We would be quite interested to receive your feedback on how DeepVariant performs in this use case, as this may help us understand the value of DeepVariant and improve it for the community.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:1078,security,validat,validate,1078,"This is a very interesting question, and the answers to it are complex. I assume for the sake of the question that you have FASTQ data from sequencing a single bacterial colony (so this is not a metagenomics question). Let's divide the question between ""can it technically be done"" and ""will the answers be scientifically valid"". To the question ""can it technically be done"", the answer is probably yes. I am not aware that we have specifically attempted this in bacteria. But if you have a FASTA file with a reference for a species and FASTQ reads, you should be able to generate variant calls for it. To the question ""will the answers be scientifically valid"", it is important to note calling variants in bacterial genomes is an area of open research. Using DeepVariant is reasonable, but I don't think you'll able to consider the output of any method (DeepVariant or other) as certain to give you fully correct results on this problem right out of the box. You'll want to use a few methods (use Freebayes and GATK) and compare between them with metrics you can independently validate, then decide what works and doesn't for your use case. One way to do this could be that for a clonal lineage you expect variants to all be called as 1/1 for the non-plasmid genome sequence. Ryan Poplin used this measure in a similar way to compare DeepVariant and other methods on inbred rice strains from the 3000 Rice Genomes Project. We would be quite interested to receive your feedback on how DeepVariant performs in this use case, as this may help us understand the value of DeepVariant and improve it for the community.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:1188,security,lineag,lineage,1188,"This is a very interesting question, and the answers to it are complex. I assume for the sake of the question that you have FASTQ data from sequencing a single bacterial colony (so this is not a metagenomics question). Let's divide the question between ""can it technically be done"" and ""will the answers be scientifically valid"". To the question ""can it technically be done"", the answer is probably yes. I am not aware that we have specifically attempted this in bacteria. But if you have a FASTA file with a reference for a species and FASTQ reads, you should be able to generate variant calls for it. To the question ""will the answers be scientifically valid"", it is important to note calling variants in bacterial genomes is an area of open research. Using DeepVariant is reasonable, but I don't think you'll able to consider the output of any method (DeepVariant or other) as certain to give you fully correct results on this problem right out of the box. You'll want to use a few methods (use Freebayes and GATK) and compare between them with metrics you can independently validate, then decide what works and doesn't for your use case. One way to do this could be that for a clonal lineage you expect variants to all be called as 1/1 for the non-plasmid genome sequence. Ryan Poplin used this measure in a similar way to compare DeepVariant and other methods on inbred rice strains from the 3000 Rice Genomes Project. We would be quite interested to receive your feedback on how DeepVariant performs in this use case, as this may help us understand the value of DeepVariant and improve it for the community.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:1544,testability,understand,understand,1544,"This is a very interesting question, and the answers to it are complex. I assume for the sake of the question that you have FASTQ data from sequencing a single bacterial colony (so this is not a metagenomics question). Let's divide the question between ""can it technically be done"" and ""will the answers be scientifically valid"". To the question ""can it technically be done"", the answer is probably yes. I am not aware that we have specifically attempted this in bacteria. But if you have a FASTA file with a reference for a species and FASTQ reads, you should be able to generate variant calls for it. To the question ""will the answers be scientifically valid"", it is important to note calling variants in bacterial genomes is an area of open research. Using DeepVariant is reasonable, but I don't think you'll able to consider the output of any method (DeepVariant or other) as certain to give you fully correct results on this problem right out of the box. You'll want to use a few methods (use Freebayes and GATK) and compare between them with metrics you can independently validate, then decide what works and doesn't for your use case. One way to do this could be that for a clonal lineage you expect variants to all be called as 1/1 for the non-plasmid genome sequence. Ryan Poplin used this measure in a similar way to compare DeepVariant and other methods on inbred rice strains from the 3000 Rice Genomes Project. We would be quite interested to receive your feedback on how DeepVariant performs in this use case, as this may help us understand the value of DeepVariant and improve it for the community.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:1469,usability,feedback,feedback,1469,"This is a very interesting question, and the answers to it are complex. I assume for the sake of the question that you have FASTQ data from sequencing a single bacterial colony (so this is not a metagenomics question). Let's divide the question between ""can it technically be done"" and ""will the answers be scientifically valid"". To the question ""can it technically be done"", the answer is probably yes. I am not aware that we have specifically attempted this in bacteria. But if you have a FASTA file with a reference for a species and FASTQ reads, you should be able to generate variant calls for it. To the question ""will the answers be scientifically valid"", it is important to note calling variants in bacterial genomes is an area of open research. Using DeepVariant is reasonable, but I don't think you'll able to consider the output of any method (DeepVariant or other) as certain to give you fully correct results on this problem right out of the box. You'll want to use a few methods (use Freebayes and GATK) and compare between them with metrics you can independently validate, then decide what works and doesn't for your use case. One way to do this could be that for a clonal lineage you expect variants to all be called as 1/1 for the non-plasmid genome sequence. Ryan Poplin used this measure in a similar way to compare DeepVariant and other methods on inbred rice strains from the 3000 Rice Genomes Project. We would be quite interested to receive your feedback on how DeepVariant performs in this use case, as this may help us understand the value of DeepVariant and improve it for the community.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:1497,usability,perform,performs,1497,"This is a very interesting question, and the answers to it are complex. I assume for the sake of the question that you have FASTQ data from sequencing a single bacterial colony (so this is not a metagenomics question). Let's divide the question between ""can it technically be done"" and ""will the answers be scientifically valid"". To the question ""can it technically be done"", the answer is probably yes. I am not aware that we have specifically attempted this in bacteria. But if you have a FASTA file with a reference for a species and FASTQ reads, you should be able to generate variant calls for it. To the question ""will the answers be scientifically valid"", it is important to note calling variants in bacterial genomes is an area of open research. Using DeepVariant is reasonable, but I don't think you'll able to consider the output of any method (DeepVariant or other) as certain to give you fully correct results on this problem right out of the box. You'll want to use a few methods (use Freebayes and GATK) and compare between them with metrics you can independently validate, then decide what works and doesn't for your use case. One way to do this could be that for a clonal lineage you expect variants to all be called as 1/1 for the non-plasmid genome sequence. Ryan Poplin used this measure in a similar way to compare DeepVariant and other methods on inbred rice strains from the 3000 Rice Genomes Project. We would be quite interested to receive your feedback on how DeepVariant performs in this use case, as this may help us understand the value of DeepVariant and improve it for the community.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:1536,usability,help,help,1536,"This is a very interesting question, and the answers to it are complex. I assume for the sake of the question that you have FASTQ data from sequencing a single bacterial colony (so this is not a metagenomics question). Let's divide the question between ""can it technically be done"" and ""will the answers be scientifically valid"". To the question ""can it technically be done"", the answer is probably yes. I am not aware that we have specifically attempted this in bacteria. But if you have a FASTA file with a reference for a species and FASTQ reads, you should be able to generate variant calls for it. To the question ""will the answers be scientifically valid"", it is important to note calling variants in bacterial genomes is an area of open research. Using DeepVariant is reasonable, but I don't think you'll able to consider the output of any method (DeepVariant or other) as certain to give you fully correct results on this problem right out of the box. You'll want to use a few methods (use Freebayes and GATK) and compare between them with metrics you can independently validate, then decide what works and doesn't for your use case. One way to do this could be that for a clonal lineage you expect variants to all be called as 1/1 for the non-plasmid genome sequence. Ryan Poplin used this measure in a similar way to compare DeepVariant and other methods on inbred rice strains from the 3000 Rice Genomes Project. We would be quite interested to receive your feedback on how DeepVariant performs in this use case, as this may help us understand the value of DeepVariant and improve it for the community.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:397,availability,error,error,397,"Hi Andrew,. Thanks for answering my questions.It is very informative. Currently I am using the GATK pipeline to identify any novel strains for a. particular bacteria genus. So far it makes sense, meaning the known mutated. locations on the genome can be found using the pipeline. However, there are. a lot of possible false positives, which could be either potential new. SNPs, or just sequencing error. That's what prompted me to explore. DeepVarient in the first place, to minimize false positives from sequencing. error. As I understand that GATK has a step to incorporate known variants to. correct for downstream analysis. But bacteria in general do not have those. information. I am wondering do you have any toolbox on this topic? I'd love to perform some comparisons between different variant detection of. bacteria genomes in the near future. Will keep you posted if I find. anything interesting. On Wed, Oct 31, 2018 at 7:13 PM Andrew Carroll <notifications@github.com>. wrote:. > This is a very interesting question, and the answers to it are complex. I. > assume for the sake of the question that you have FASTQ data from. > sequencing a single bacterial colony (so this is not a metagenomics. > question). >. > Let's divide the question between ""can it technically be done"" and ""will. > the answers be scientifically valid"". >. > To the question ""can it technically be done"", the answer is probably yes. > I am not aware that we have specifically attempted this in bacteria. But if. > you have a FASTA file with a reference for a species and FASTQ reads, you. > should be able to generate variant calls for it. >. > To the question ""will the answers be scientifically valid"", it is. > important to note calling variants in bacterial genomes is an area of open. > research. Using DeepVariant is reasonable, but I don't think you'll able to. > consider the output of any method (DeepVariant or other) as certain to give. > you fully correct results on this problem right out of the box. Yo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:517,availability,error,error,517,"Hi Andrew,. Thanks for answering my questions.It is very informative. Currently I am using the GATK pipeline to identify any novel strains for a. particular bacteria genus. So far it makes sense, meaning the known mutated. locations on the genome can be found using the pipeline. However, there are. a lot of possible false positives, which could be either potential new. SNPs, or just sequencing error. That's what prompted me to explore. DeepVarient in the first place, to minimize false positives from sequencing. error. As I understand that GATK has a step to incorporate known variants to. correct for downstream analysis. But bacteria in general do not have those. information. I am wondering do you have any toolbox on this topic? I'd love to perform some comparisons between different variant detection of. bacteria genomes in the near future. Will keep you posted if I find. anything interesting. On Wed, Oct 31, 2018 at 7:13 PM Andrew Carroll <notifications@github.com>. wrote:. > This is a very interesting question, and the answers to it are complex. I. > assume for the sake of the question that you have FASTQ data from. > sequencing a single bacterial colony (so this is not a metagenomics. > question). >. > Let's divide the question between ""can it technically be done"" and ""will. > the answers be scientifically valid"". >. > To the question ""can it technically be done"", the answer is probably yes. > I am not aware that we have specifically attempted this in bacteria. But if. > you have a FASTA file with a reference for a species and FASTQ reads, you. > should be able to generate variant calls for it. >. > To the question ""will the answers be scientifically valid"", it is. > important to note calling variants in bacterial genomes is an area of open. > research. Using DeepVariant is reasonable, but I don't think you'll able to. > consider the output of any method (DeepVariant or other) as certain to give. > you fully correct results on this problem right out of the box. Yo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:607,availability,down,downstream,607,"Hi Andrew,. Thanks for answering my questions.It is very informative. Currently I am using the GATK pipeline to identify any novel strains for a. particular bacteria genus. So far it makes sense, meaning the known mutated. locations on the genome can be found using the pipeline. However, there are. a lot of possible false positives, which could be either potential new. SNPs, or just sequencing error. That's what prompted me to explore. DeepVarient in the first place, to minimize false positives from sequencing. error. As I understand that GATK has a step to incorporate known variants to. correct for downstream analysis. But bacteria in general do not have those. information. I am wondering do you have any toolbox on this topic? I'd love to perform some comparisons between different variant detection of. bacteria genomes in the near future. Will keep you posted if I find. anything interesting. On Wed, Oct 31, 2018 at 7:13 PM Andrew Carroll <notifications@github.com>. wrote:. > This is a very interesting question, and the answers to it are complex. I. > assume for the sake of the question that you have FASTQ data from. > sequencing a single bacterial colony (so this is not a metagenomics. > question). >. > Let's divide the question between ""can it technically be done"" and ""will. > the answers be scientifically valid"". >. > To the question ""can it technically be done"", the answer is probably yes. > I am not aware that we have specifically attempted this in bacteria. But if. > you have a FASTA file with a reference for a species and FASTQ reads, you. > should be able to generate variant calls for it. >. > To the question ""will the answers be scientifically valid"", it is. > important to note calling variants in bacterial genomes is an area of open. > research. Using DeepVariant is reasonable, but I don't think you'll able to. > consider the output of any method (DeepVariant or other) as certain to give. > you fully correct results on this problem right out of the box. Yo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:100,deployability,pipelin,pipeline,100,"Hi Andrew,. Thanks for answering my questions.It is very informative. Currently I am using the GATK pipeline to identify any novel strains for a. particular bacteria genus. So far it makes sense, meaning the known mutated. locations on the genome can be found using the pipeline. However, there are. a lot of possible false positives, which could be either potential new. SNPs, or just sequencing error. That's what prompted me to explore. DeepVarient in the first place, to minimize false positives from sequencing. error. As I understand that GATK has a step to incorporate known variants to. correct for downstream analysis. But bacteria in general do not have those. information. I am wondering do you have any toolbox on this topic? I'd love to perform some comparisons between different variant detection of. bacteria genomes in the near future. Will keep you posted if I find. anything interesting. On Wed, Oct 31, 2018 at 7:13 PM Andrew Carroll <notifications@github.com>. wrote:. > This is a very interesting question, and the answers to it are complex. I. > assume for the sake of the question that you have FASTQ data from. > sequencing a single bacterial colony (so this is not a metagenomics. > question). >. > Let's divide the question between ""can it technically be done"" and ""will. > the answers be scientifically valid"". >. > To the question ""can it technically be done"", the answer is probably yes. > I am not aware that we have specifically attempted this in bacteria. But if. > you have a FASTA file with a reference for a species and FASTQ reads, you. > should be able to generate variant calls for it. >. > To the question ""will the answers be scientifically valid"", it is. > important to note calling variants in bacterial genomes is an area of open. > research. Using DeepVariant is reasonable, but I don't think you'll able to. > consider the output of any method (DeepVariant or other) as certain to give. > you fully correct results on this problem right out of the box. Yo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:270,deployability,pipelin,pipeline,270,"Hi Andrew,. Thanks for answering my questions.It is very informative. Currently I am using the GATK pipeline to identify any novel strains for a. particular bacteria genus. So far it makes sense, meaning the known mutated. locations on the genome can be found using the pipeline. However, there are. a lot of possible false positives, which could be either potential new. SNPs, or just sequencing error. That's what prompted me to explore. DeepVarient in the first place, to minimize false positives from sequencing. error. As I understand that GATK has a step to incorporate known variants to. correct for downstream analysis. But bacteria in general do not have those. information. I am wondering do you have any toolbox on this topic? I'd love to perform some comparisons between different variant detection of. bacteria genomes in the near future. Will keep you posted if I find. anything interesting. On Wed, Oct 31, 2018 at 7:13 PM Andrew Carroll <notifications@github.com>. wrote:. > This is a very interesting question, and the answers to it are complex. I. > assume for the sake of the question that you have FASTQ data from. > sequencing a single bacterial colony (so this is not a metagenomics. > question). >. > Let's divide the question between ""can it technically be done"" and ""will. > the answers be scientifically valid"". >. > To the question ""can it technically be done"", the answer is probably yes. > I am not aware that we have specifically attempted this in bacteria. But if. > you have a FASTA file with a reference for a species and FASTQ reads, you. > should be able to generate variant calls for it. >. > To the question ""will the answers be scientifically valid"", it is. > important to note calling variants in bacterial genomes is an area of open. > research. Using DeepVariant is reasonable, but I don't think you'll able to. > consider the output of any method (DeepVariant or other) as certain to give. > you fully correct results on this problem right out of the box. Yo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:70,energy efficiency,Current,Currently,70,"Hi Andrew,. Thanks for answering my questions.It is very informative. Currently I am using the GATK pipeline to identify any novel strains for a. particular bacteria genus. So far it makes sense, meaning the known mutated. locations on the genome can be found using the pipeline. However, there are. a lot of possible false positives, which could be either potential new. SNPs, or just sequencing error. That's what prompted me to explore. DeepVarient in the first place, to minimize false positives from sequencing. error. As I understand that GATK has a step to incorporate known variants to. correct for downstream analysis. But bacteria in general do not have those. information. I am wondering do you have any toolbox on this topic? I'd love to perform some comparisons between different variant detection of. bacteria genomes in the near future. Will keep you posted if I find. anything interesting. On Wed, Oct 31, 2018 at 7:13 PM Andrew Carroll <notifications@github.com>. wrote:. > This is a very interesting question, and the answers to it are complex. I. > assume for the sake of the question that you have FASTQ data from. > sequencing a single bacterial colony (so this is not a metagenomics. > question). >. > Let's divide the question between ""can it technically be done"" and ""will. > the answers be scientifically valid"". >. > To the question ""can it technically be done"", the answer is probably yes. > I am not aware that we have specifically attempted this in bacteria. But if. > you have a FASTA file with a reference for a species and FASTQ reads, you. > should be able to generate variant calls for it. >. > To the question ""will the answers be scientifically valid"", it is. > important to note calling variants in bacterial genomes is an area of open. > research. Using DeepVariant is reasonable, but I don't think you'll able to. > consider the output of any method (DeepVariant or other) as certain to give. > you fully correct results on this problem right out of the box. Yo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:2357,energy efficiency,measur,measure,2357," question, and the answers to it are complex. I. > assume for the sake of the question that you have FASTQ data from. > sequencing a single bacterial colony (so this is not a metagenomics. > question). >. > Let's divide the question between ""can it technically be done"" and ""will. > the answers be scientifically valid"". >. > To the question ""can it technically be done"", the answer is probably yes. > I am not aware that we have specifically attempted this in bacteria. But if. > you have a FASTA file with a reference for a species and FASTQ reads, you. > should be able to generate variant calls for it. >. > To the question ""will the answers be scientifically valid"", it is. > important to note calling variants in bacterial genomes is an area of open. > research. Using DeepVariant is reasonable, but I don't think you'll able to. > consider the output of any method (DeepVariant or other) as certain to give. > you fully correct results on this problem right out of the box. You'll want. > to use a few methods (use Freebayes and GATK) and compare between them with. > metrics you can independently validate, then decide what works and doesn't. > for your use case. >. > One way to do this could be that for a clonal lineage you expect variants. > to all be called as 1/1 for the non-plasmid genome sequence. Ryan Poplin. > used this measure in a similar way to compare DeepVariant and other methods. > on inbred rice strains from the 3000 Rice Genomes Project. >. > We would be quite interested to receive your feedback on how DeepVariant. > performs in this use case, as this may help us understand the value of. > DeepVariant and improve it for the community. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/114#issuecomment-434889612>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AHCP08xLJBFnbpQzdBn9MXeFKOyacKs9ks5uqjzDgaJpZM4YEtb1>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:100,integrability,pipelin,pipeline,100,"Hi Andrew,. Thanks for answering my questions.It is very informative. Currently I am using the GATK pipeline to identify any novel strains for a. particular bacteria genus. So far it makes sense, meaning the known mutated. locations on the genome can be found using the pipeline. However, there are. a lot of possible false positives, which could be either potential new. SNPs, or just sequencing error. That's what prompted me to explore. DeepVarient in the first place, to minimize false positives from sequencing. error. As I understand that GATK has a step to incorporate known variants to. correct for downstream analysis. But bacteria in general do not have those. information. I am wondering do you have any toolbox on this topic? I'd love to perform some comparisons between different variant detection of. bacteria genomes in the near future. Will keep you posted if I find. anything interesting. On Wed, Oct 31, 2018 at 7:13 PM Andrew Carroll <notifications@github.com>. wrote:. > This is a very interesting question, and the answers to it are complex. I. > assume for the sake of the question that you have FASTQ data from. > sequencing a single bacterial colony (so this is not a metagenomics. > question). >. > Let's divide the question between ""can it technically be done"" and ""will. > the answers be scientifically valid"". >. > To the question ""can it technically be done"", the answer is probably yes. > I am not aware that we have specifically attempted this in bacteria. But if. > you have a FASTA file with a reference for a species and FASTQ reads, you. > should be able to generate variant calls for it. >. > To the question ""will the answers be scientifically valid"", it is. > important to note calling variants in bacterial genomes is an area of open. > research. Using DeepVariant is reasonable, but I don't think you'll able to. > consider the output of any method (DeepVariant or other) as certain to give. > you fully correct results on this problem right out of the box. Yo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:270,integrability,pipelin,pipeline,270,"Hi Andrew,. Thanks for answering my questions.It is very informative. Currently I am using the GATK pipeline to identify any novel strains for a. particular bacteria genus. So far it makes sense, meaning the known mutated. locations on the genome can be found using the pipeline. However, there are. a lot of possible false positives, which could be either potential new. SNPs, or just sequencing error. That's what prompted me to explore. DeepVarient in the first place, to minimize false positives from sequencing. error. As I understand that GATK has a step to incorporate known variants to. correct for downstream analysis. But bacteria in general do not have those. information. I am wondering do you have any toolbox on this topic? I'd love to perform some comparisons between different variant detection of. bacteria genomes in the near future. Will keep you posted if I find. anything interesting. On Wed, Oct 31, 2018 at 7:13 PM Andrew Carroll <notifications@github.com>. wrote:. > This is a very interesting question, and the answers to it are complex. I. > assume for the sake of the question that you have FASTQ data from. > sequencing a single bacterial colony (so this is not a metagenomics. > question). >. > Let's divide the question between ""can it technically be done"" and ""will. > the answers be scientifically valid"". >. > To the question ""can it technically be done"", the answer is probably yes. > I am not aware that we have specifically attempted this in bacteria. But if. > you have a FASTA file with a reference for a species and FASTQ reads, you. > should be able to generate variant calls for it. >. > To the question ""will the answers be scientifically valid"", it is. > important to note calling variants in bacterial genomes is an area of open. > research. Using DeepVariant is reasonable, but I don't think you'll able to. > consider the output of any method (DeepVariant or other) as certain to give. > you fully correct results on this problem right out of the box. Yo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:731,integrability,topic,topic,731,"Hi Andrew,. Thanks for answering my questions.It is very informative. Currently I am using the GATK pipeline to identify any novel strains for a. particular bacteria genus. So far it makes sense, meaning the known mutated. locations on the genome can be found using the pipeline. However, there are. a lot of possible false positives, which could be either potential new. SNPs, or just sequencing error. That's what prompted me to explore. DeepVarient in the first place, to minimize false positives from sequencing. error. As I understand that GATK has a step to incorporate known variants to. correct for downstream analysis. But bacteria in general do not have those. information. I am wondering do you have any toolbox on this topic? I'd love to perform some comparisons between different variant detection of. bacteria genomes in the near future. Will keep you posted if I find. anything interesting. On Wed, Oct 31, 2018 at 7:13 PM Andrew Carroll <notifications@github.com>. wrote:. > This is a very interesting question, and the answers to it are complex. I. > assume for the sake of the question that you have FASTQ data from. > sequencing a single bacterial colony (so this is not a metagenomics. > question). >. > Let's divide the question between ""can it technically be done"" and ""will. > the answers be scientifically valid"". >. > To the question ""can it technically be done"", the answer is probably yes. > I am not aware that we have specifically attempted this in bacteria. But if. > you have a FASTA file with a reference for a species and FASTQ reads, you. > should be able to generate variant calls for it. >. > To the question ""will the answers be scientifically valid"", it is. > important to note calling variants in bacterial genomes is an area of open. > research. Using DeepVariant is reasonable, but I don't think you'll able to. > consider the output of any method (DeepVariant or other) as certain to give. > you fully correct results on this problem right out of the box. Yo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:1447,interoperability,specif,specifically,1447,"n the first place, to minimize false positives from sequencing. error. As I understand that GATK has a step to incorporate known variants to. correct for downstream analysis. But bacteria in general do not have those. information. I am wondering do you have any toolbox on this topic? I'd love to perform some comparisons between different variant detection of. bacteria genomes in the near future. Will keep you posted if I find. anything interesting. On Wed, Oct 31, 2018 at 7:13 PM Andrew Carroll <notifications@github.com>. wrote:. > This is a very interesting question, and the answers to it are complex. I. > assume for the sake of the question that you have FASTQ data from. > sequencing a single bacterial colony (so this is not a metagenomics. > question). >. > Let's divide the question between ""can it technically be done"" and ""will. > the answers be scientifically valid"". >. > To the question ""can it technically be done"", the answer is probably yes. > I am not aware that we have specifically attempted this in bacteria. But if. > you have a FASTA file with a reference for a species and FASTQ reads, you. > should be able to generate variant calls for it. >. > To the question ""will the answers be scientifically valid"", it is. > important to note calling variants in bacterial genomes is an area of open. > research. Using DeepVariant is reasonable, but I don't think you'll able to. > consider the output of any method (DeepVariant or other) as certain to give. > you fully correct results on this problem right out of the box. You'll want. > to use a few methods (use Freebayes and GATK) and compare between them with. > metrics you can independently validate, then decide what works and doesn't. > for your use case. >. > One way to do this could be that for a clonal lineage you expect variants. > to all be called as 1/1 for the non-plasmid genome sequence. Ryan Poplin. > used this measure in a similar way to compare DeepVariant and other methods. > on inbred rice strains from",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:397,performance,error,error,397,"Hi Andrew,. Thanks for answering my questions.It is very informative. Currently I am using the GATK pipeline to identify any novel strains for a. particular bacteria genus. So far it makes sense, meaning the known mutated. locations on the genome can be found using the pipeline. However, there are. a lot of possible false positives, which could be either potential new. SNPs, or just sequencing error. That's what prompted me to explore. DeepVarient in the first place, to minimize false positives from sequencing. error. As I understand that GATK has a step to incorporate known variants to. correct for downstream analysis. But bacteria in general do not have those. information. I am wondering do you have any toolbox on this topic? I'd love to perform some comparisons between different variant detection of. bacteria genomes in the near future. Will keep you posted if I find. anything interesting. On Wed, Oct 31, 2018 at 7:13 PM Andrew Carroll <notifications@github.com>. wrote:. > This is a very interesting question, and the answers to it are complex. I. > assume for the sake of the question that you have FASTQ data from. > sequencing a single bacterial colony (so this is not a metagenomics. > question). >. > Let's divide the question between ""can it technically be done"" and ""will. > the answers be scientifically valid"". >. > To the question ""can it technically be done"", the answer is probably yes. > I am not aware that we have specifically attempted this in bacteria. But if. > you have a FASTA file with a reference for a species and FASTQ reads, you. > should be able to generate variant calls for it. >. > To the question ""will the answers be scientifically valid"", it is. > important to note calling variants in bacterial genomes is an area of open. > research. Using DeepVariant is reasonable, but I don't think you'll able to. > consider the output of any method (DeepVariant or other) as certain to give. > you fully correct results on this problem right out of the box. Yo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:517,performance,error,error,517,"Hi Andrew,. Thanks for answering my questions.It is very informative. Currently I am using the GATK pipeline to identify any novel strains for a. particular bacteria genus. So far it makes sense, meaning the known mutated. locations on the genome can be found using the pipeline. However, there are. a lot of possible false positives, which could be either potential new. SNPs, or just sequencing error. That's what prompted me to explore. DeepVarient in the first place, to minimize false positives from sequencing. error. As I understand that GATK has a step to incorporate known variants to. correct for downstream analysis. But bacteria in general do not have those. information. I am wondering do you have any toolbox on this topic? I'd love to perform some comparisons between different variant detection of. bacteria genomes in the near future. Will keep you posted if I find. anything interesting. On Wed, Oct 31, 2018 at 7:13 PM Andrew Carroll <notifications@github.com>. wrote:. > This is a very interesting question, and the answers to it are complex. I. > assume for the sake of the question that you have FASTQ data from. > sequencing a single bacterial colony (so this is not a metagenomics. > question). >. > Let's divide the question between ""can it technically be done"" and ""will. > the answers be scientifically valid"". >. > To the question ""can it technically be done"", the answer is probably yes. > I am not aware that we have specifically attempted this in bacteria. But if. > you have a FASTA file with a reference for a species and FASTQ reads, you. > should be able to generate variant calls for it. >. > To the question ""will the answers be scientifically valid"", it is. > important to note calling variants in bacterial genomes is an area of open. > research. Using DeepVariant is reasonable, but I don't think you'll able to. > consider the output of any method (DeepVariant or other) as certain to give. > you fully correct results on this problem right out of the box. Yo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:750,performance,perform,perform,750,"Hi Andrew,. Thanks for answering my questions.It is very informative. Currently I am using the GATK pipeline to identify any novel strains for a. particular bacteria genus. So far it makes sense, meaning the known mutated. locations on the genome can be found using the pipeline. However, there are. a lot of possible false positives, which could be either potential new. SNPs, or just sequencing error. That's what prompted me to explore. DeepVarient in the first place, to minimize false positives from sequencing. error. As I understand that GATK has a step to incorporate known variants to. correct for downstream analysis. But bacteria in general do not have those. information. I am wondering do you have any toolbox on this topic? I'd love to perform some comparisons between different variant detection of. bacteria genomes in the near future. Will keep you posted if I find. anything interesting. On Wed, Oct 31, 2018 at 7:13 PM Andrew Carroll <notifications@github.com>. wrote:. > This is a very interesting question, and the answers to it are complex. I. > assume for the sake of the question that you have FASTQ data from. > sequencing a single bacterial colony (so this is not a metagenomics. > question). >. > Let's divide the question between ""can it technically be done"" and ""will. > the answers be scientifically valid"". >. > To the question ""can it technically be done"", the answer is probably yes. > I am not aware that we have specifically attempted this in bacteria. But if. > you have a FASTA file with a reference for a species and FASTQ reads, you. > should be able to generate variant calls for it. >. > To the question ""will the answers be scientifically valid"", it is. > important to note calling variants in bacterial genomes is an area of open. > research. Using DeepVariant is reasonable, but I don't think you'll able to. > consider the output of any method (DeepVariant or other) as certain to give. > you fully correct results on this problem right out of the box. Yo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:2566,performance,perform,performs,2566," question, and the answers to it are complex. I. > assume for the sake of the question that you have FASTQ data from. > sequencing a single bacterial colony (so this is not a metagenomics. > question). >. > Let's divide the question between ""can it technically be done"" and ""will. > the answers be scientifically valid"". >. > To the question ""can it technically be done"", the answer is probably yes. > I am not aware that we have specifically attempted this in bacteria. But if. > you have a FASTA file with a reference for a species and FASTQ reads, you. > should be able to generate variant calls for it. >. > To the question ""will the answers be scientifically valid"", it is. > important to note calling variants in bacterial genomes is an area of open. > research. Using DeepVariant is reasonable, but I don't think you'll able to. > consider the output of any method (DeepVariant or other) as certain to give. > you fully correct results on this problem right out of the box. You'll want. > to use a few methods (use Freebayes and GATK) and compare between them with. > metrics you can independently validate, then decide what works and doesn't. > for your use case. >. > One way to do this could be that for a clonal lineage you expect variants. > to all be called as 1/1 for the non-plasmid genome sequence. Ryan Poplin. > used this measure in a similar way to compare DeepVariant and other methods. > on inbred rice strains from the 3000 Rice Genomes Project. >. > We would be quite interested to receive your feedback on how DeepVariant. > performs in this use case, as this may help us understand the value of. > DeepVariant and improve it for the community. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/114#issuecomment-434889612>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AHCP08xLJBFnbpQzdBn9MXeFKOyacKs9ks5uqjzDgaJpZM4YEtb1>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:2159,reliability,doe,doesn,2159," question, and the answers to it are complex. I. > assume for the sake of the question that you have FASTQ data from. > sequencing a single bacterial colony (so this is not a metagenomics. > question). >. > Let's divide the question between ""can it technically be done"" and ""will. > the answers be scientifically valid"". >. > To the question ""can it technically be done"", the answer is probably yes. > I am not aware that we have specifically attempted this in bacteria. But if. > you have a FASTA file with a reference for a species and FASTQ reads, you. > should be able to generate variant calls for it. >. > To the question ""will the answers be scientifically valid"", it is. > important to note calling variants in bacterial genomes is an area of open. > research. Using DeepVariant is reasonable, but I don't think you'll able to. > consider the output of any method (DeepVariant or other) as certain to give. > you fully correct results on this problem right out of the box. You'll want. > to use a few methods (use Freebayes and GATK) and compare between them with. > metrics you can independently validate, then decide what works and doesn't. > for your use case. >. > One way to do this could be that for a clonal lineage you expect variants. > to all be called as 1/1 for the non-plasmid genome sequence. Ryan Poplin. > used this measure in a similar way to compare DeepVariant and other methods. > on inbred rice strains from the 3000 Rice Genomes Project. >. > We would be quite interested to receive your feedback on how DeepVariant. > performs in this use case, as this may help us understand the value of. > DeepVariant and improve it for the community. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/114#issuecomment-434889612>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AHCP08xLJBFnbpQzdBn9MXeFKOyacKs9ks5uqjzDgaJpZM4YEtb1>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:397,safety,error,error,397,"Hi Andrew,. Thanks for answering my questions.It is very informative. Currently I am using the GATK pipeline to identify any novel strains for a. particular bacteria genus. So far it makes sense, meaning the known mutated. locations on the genome can be found using the pipeline. However, there are. a lot of possible false positives, which could be either potential new. SNPs, or just sequencing error. That's what prompted me to explore. DeepVarient in the first place, to minimize false positives from sequencing. error. As I understand that GATK has a step to incorporate known variants to. correct for downstream analysis. But bacteria in general do not have those. information. I am wondering do you have any toolbox on this topic? I'd love to perform some comparisons between different variant detection of. bacteria genomes in the near future. Will keep you posted if I find. anything interesting. On Wed, Oct 31, 2018 at 7:13 PM Andrew Carroll <notifications@github.com>. wrote:. > This is a very interesting question, and the answers to it are complex. I. > assume for the sake of the question that you have FASTQ data from. > sequencing a single bacterial colony (so this is not a metagenomics. > question). >. > Let's divide the question between ""can it technically be done"" and ""will. > the answers be scientifically valid"". >. > To the question ""can it technically be done"", the answer is probably yes. > I am not aware that we have specifically attempted this in bacteria. But if. > you have a FASTA file with a reference for a species and FASTQ reads, you. > should be able to generate variant calls for it. >. > To the question ""will the answers be scientifically valid"", it is. > important to note calling variants in bacterial genomes is an area of open. > research. Using DeepVariant is reasonable, but I don't think you'll able to. > consider the output of any method (DeepVariant or other) as certain to give. > you fully correct results on this problem right out of the box. Yo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:517,safety,error,error,517,"Hi Andrew,. Thanks for answering my questions.It is very informative. Currently I am using the GATK pipeline to identify any novel strains for a. particular bacteria genus. So far it makes sense, meaning the known mutated. locations on the genome can be found using the pipeline. However, there are. a lot of possible false positives, which could be either potential new. SNPs, or just sequencing error. That's what prompted me to explore. DeepVarient in the first place, to minimize false positives from sequencing. error. As I understand that GATK has a step to incorporate known variants to. correct for downstream analysis. But bacteria in general do not have those. information. I am wondering do you have any toolbox on this topic? I'd love to perform some comparisons between different variant detection of. bacteria genomes in the near future. Will keep you posted if I find. anything interesting. On Wed, Oct 31, 2018 at 7:13 PM Andrew Carroll <notifications@github.com>. wrote:. > This is a very interesting question, and the answers to it are complex. I. > assume for the sake of the question that you have FASTQ data from. > sequencing a single bacterial colony (so this is not a metagenomics. > question). >. > Let's divide the question between ""can it technically be done"" and ""will. > the answers be scientifically valid"". >. > To the question ""can it technically be done"", the answer is probably yes. > I am not aware that we have specifically attempted this in bacteria. But if. > you have a FASTA file with a reference for a species and FASTQ reads, you. > should be able to generate variant calls for it. >. > To the question ""will the answers be scientifically valid"", it is. > important to note calling variants in bacterial genomes is an area of open. > research. Using DeepVariant is reasonable, but I don't think you'll able to. > consider the output of any method (DeepVariant or other) as certain to give. > you fully correct results on this problem right out of the box. Yo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:801,safety,detect,detection,801,"Hi Andrew,. Thanks for answering my questions.It is very informative. Currently I am using the GATK pipeline to identify any novel strains for a. particular bacteria genus. So far it makes sense, meaning the known mutated. locations on the genome can be found using the pipeline. However, there are. a lot of possible false positives, which could be either potential new. SNPs, or just sequencing error. That's what prompted me to explore. DeepVarient in the first place, to minimize false positives from sequencing. error. As I understand that GATK has a step to incorporate known variants to. correct for downstream analysis. But bacteria in general do not have those. information. I am wondering do you have any toolbox on this topic? I'd love to perform some comparisons between different variant detection of. bacteria genomes in the near future. Will keep you posted if I find. anything interesting. On Wed, Oct 31, 2018 at 7:13 PM Andrew Carroll <notifications@github.com>. wrote:. > This is a very interesting question, and the answers to it are complex. I. > assume for the sake of the question that you have FASTQ data from. > sequencing a single bacterial colony (so this is not a metagenomics. > question). >. > Let's divide the question between ""can it technically be done"" and ""will. > the answers be scientifically valid"". >. > To the question ""can it technically be done"", the answer is probably yes. > I am not aware that we have specifically attempted this in bacteria. But if. > you have a FASTA file with a reference for a species and FASTQ reads, you. > should be able to generate variant calls for it. >. > To the question ""will the answers be scientifically valid"", it is. > important to note calling variants in bacterial genomes is an area of open. > research. Using DeepVariant is reasonable, but I don't think you'll able to. > consider the output of any method (DeepVariant or other) as certain to give. > you fully correct results on this problem right out of the box. Yo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:1054,safety,compl,complex,1054,"nformative. Currently I am using the GATK pipeline to identify any novel strains for a. particular bacteria genus. So far it makes sense, meaning the known mutated. locations on the genome can be found using the pipeline. However, there are. a lot of possible false positives, which could be either potential new. SNPs, or just sequencing error. That's what prompted me to explore. DeepVarient in the first place, to minimize false positives from sequencing. error. As I understand that GATK has a step to incorporate known variants to. correct for downstream analysis. But bacteria in general do not have those. information. I am wondering do you have any toolbox on this topic? I'd love to perform some comparisons between different variant detection of. bacteria genomes in the near future. Will keep you posted if I find. anything interesting. On Wed, Oct 31, 2018 at 7:13 PM Andrew Carroll <notifications@github.com>. wrote:. > This is a very interesting question, and the answers to it are complex. I. > assume for the sake of the question that you have FASTQ data from. > sequencing a single bacterial colony (so this is not a metagenomics. > question). >. > Let's divide the question between ""can it technically be done"" and ""will. > the answers be scientifically valid"". >. > To the question ""can it technically be done"", the answer is probably yes. > I am not aware that we have specifically attempted this in bacteria. But if. > you have a FASTA file with a reference for a species and FASTQ reads, you. > should be able to generate variant calls for it. >. > To the question ""will the answers be scientifically valid"", it is. > important to note calling variants in bacterial genomes is an area of open. > research. Using DeepVariant is reasonable, but I don't think you'll able to. > consider the output of any method (DeepVariant or other) as certain to give. > you fully correct results on this problem right out of the box. You'll want. > to use a few methods (use Freebayes and GATK)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:1330,safety,valid,valid,1330,", which could be either potential new. SNPs, or just sequencing error. That's what prompted me to explore. DeepVarient in the first place, to minimize false positives from sequencing. error. As I understand that GATK has a step to incorporate known variants to. correct for downstream analysis. But bacteria in general do not have those. information. I am wondering do you have any toolbox on this topic? I'd love to perform some comparisons between different variant detection of. bacteria genomes in the near future. Will keep you posted if I find. anything interesting. On Wed, Oct 31, 2018 at 7:13 PM Andrew Carroll <notifications@github.com>. wrote:. > This is a very interesting question, and the answers to it are complex. I. > assume for the sake of the question that you have FASTQ data from. > sequencing a single bacterial colony (so this is not a metagenomics. > question). >. > Let's divide the question between ""can it technically be done"" and ""will. > the answers be scientifically valid"". >. > To the question ""can it technically be done"", the answer is probably yes. > I am not aware that we have specifically attempted this in bacteria. But if. > you have a FASTA file with a reference for a species and FASTQ reads, you. > should be able to generate variant calls for it. >. > To the question ""will the answers be scientifically valid"", it is. > important to note calling variants in bacterial genomes is an area of open. > research. Using DeepVariant is reasonable, but I don't think you'll able to. > consider the output of any method (DeepVariant or other) as certain to give. > you fully correct results on this problem right out of the box. You'll want. > to use a few methods (use Freebayes and GATK) and compare between them with. > metrics you can independently validate, then decide what works and doesn't. > for your use case. >. > One way to do this could be that for a clonal lineage you expect variants. > to all be called as 1/1 for the non-plasmid genome sequence. R",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:1681,safety,valid,valid,1681,"I am wondering do you have any toolbox on this topic? I'd love to perform some comparisons between different variant detection of. bacteria genomes in the near future. Will keep you posted if I find. anything interesting. On Wed, Oct 31, 2018 at 7:13 PM Andrew Carroll <notifications@github.com>. wrote:. > This is a very interesting question, and the answers to it are complex. I. > assume for the sake of the question that you have FASTQ data from. > sequencing a single bacterial colony (so this is not a metagenomics. > question). >. > Let's divide the question between ""can it technically be done"" and ""will. > the answers be scientifically valid"". >. > To the question ""can it technically be done"", the answer is probably yes. > I am not aware that we have specifically attempted this in bacteria. But if. > you have a FASTA file with a reference for a species and FASTQ reads, you. > should be able to generate variant calls for it. >. > To the question ""will the answers be scientifically valid"", it is. > important to note calling variants in bacterial genomes is an area of open. > research. Using DeepVariant is reasonable, but I don't think you'll able to. > consider the output of any method (DeepVariant or other) as certain to give. > you fully correct results on this problem right out of the box. You'll want. > to use a few methods (use Freebayes and GATK) and compare between them with. > metrics you can independently validate, then decide what works and doesn't. > for your use case. >. > One way to do this could be that for a clonal lineage you expect variants. > to all be called as 1/1 for the non-plasmid genome sequence. Ryan Poplin. > used this measure in a similar way to compare DeepVariant and other methods. > on inbred rice strains from the 3000 Rice Genomes Project. >. > We would be quite interested to receive your feedback on how DeepVariant. > performs in this use case, as this may help us understand the value of. > DeepVariant and improve it for the community",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:2122,safety,valid,validate,2122," question, and the answers to it are complex. I. > assume for the sake of the question that you have FASTQ data from. > sequencing a single bacterial colony (so this is not a metagenomics. > question). >. > Let's divide the question between ""can it technically be done"" and ""will. > the answers be scientifically valid"". >. > To the question ""can it technically be done"", the answer is probably yes. > I am not aware that we have specifically attempted this in bacteria. But if. > you have a FASTA file with a reference for a species and FASTQ reads, you. > should be able to generate variant calls for it. >. > To the question ""will the answers be scientifically valid"", it is. > important to note calling variants in bacterial genomes is an area of open. > research. Using DeepVariant is reasonable, but I don't think you'll able to. > consider the output of any method (DeepVariant or other) as certain to give. > you fully correct results on this problem right out of the box. You'll want. > to use a few methods (use Freebayes and GATK) and compare between them with. > metrics you can independently validate, then decide what works and doesn't. > for your use case. >. > One way to do this could be that for a clonal lineage you expect variants. > to all be called as 1/1 for the non-plasmid genome sequence. Ryan Poplin. > used this measure in a similar way to compare DeepVariant and other methods. > on inbred rice strains from the 3000 Rice Genomes Project. >. > We would be quite interested to receive your feedback on how DeepVariant. > performs in this use case, as this may help us understand the value of. > DeepVariant and improve it for the community. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/114#issuecomment-434889612>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AHCP08xLJBFnbpQzdBn9MXeFKOyacKs9ks5uqjzDgaJpZM4YEtb1>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:112,security,ident,identify,112,"Hi Andrew,. Thanks for answering my questions.It is very informative. Currently I am using the GATK pipeline to identify any novel strains for a. particular bacteria genus. So far it makes sense, meaning the known mutated. locations on the genome can be found using the pipeline. However, there are. a lot of possible false positives, which could be either potential new. SNPs, or just sequencing error. That's what prompted me to explore. DeepVarient in the first place, to minimize false positives from sequencing. error. As I understand that GATK has a step to incorporate known variants to. correct for downstream analysis. But bacteria in general do not have those. information. I am wondering do you have any toolbox on this topic? I'd love to perform some comparisons between different variant detection of. bacteria genomes in the near future. Will keep you posted if I find. anything interesting. On Wed, Oct 31, 2018 at 7:13 PM Andrew Carroll <notifications@github.com>. wrote:. > This is a very interesting question, and the answers to it are complex. I. > assume for the sake of the question that you have FASTQ data from. > sequencing a single bacterial colony (so this is not a metagenomics. > question). >. > Let's divide the question between ""can it technically be done"" and ""will. > the answers be scientifically valid"". >. > To the question ""can it technically be done"", the answer is probably yes. > I am not aware that we have specifically attempted this in bacteria. But if. > you have a FASTA file with a reference for a species and FASTQ reads, you. > should be able to generate variant calls for it. >. > To the question ""will the answers be scientifically valid"", it is. > important to note calling variants in bacterial genomes is an area of open. > research. Using DeepVariant is reasonable, but I don't think you'll able to. > consider the output of any method (DeepVariant or other) as certain to give. > you fully correct results on this problem right out of the box. Yo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:801,security,detect,detection,801,"Hi Andrew,. Thanks for answering my questions.It is very informative. Currently I am using the GATK pipeline to identify any novel strains for a. particular bacteria genus. So far it makes sense, meaning the known mutated. locations on the genome can be found using the pipeline. However, there are. a lot of possible false positives, which could be either potential new. SNPs, or just sequencing error. That's what prompted me to explore. DeepVarient in the first place, to minimize false positives from sequencing. error. As I understand that GATK has a step to incorporate known variants to. correct for downstream analysis. But bacteria in general do not have those. information. I am wondering do you have any toolbox on this topic? I'd love to perform some comparisons between different variant detection of. bacteria genomes in the near future. Will keep you posted if I find. anything interesting. On Wed, Oct 31, 2018 at 7:13 PM Andrew Carroll <notifications@github.com>. wrote:. > This is a very interesting question, and the answers to it are complex. I. > assume for the sake of the question that you have FASTQ data from. > sequencing a single bacterial colony (so this is not a metagenomics. > question). >. > Let's divide the question between ""can it technically be done"" and ""will. > the answers be scientifically valid"". >. > To the question ""can it technically be done"", the answer is probably yes. > I am not aware that we have specifically attempted this in bacteria. But if. > you have a FASTA file with a reference for a species and FASTQ reads, you. > should be able to generate variant calls for it. >. > To the question ""will the answers be scientifically valid"", it is. > important to note calling variants in bacterial genomes is an area of open. > research. Using DeepVariant is reasonable, but I don't think you'll able to. > consider the output of any method (DeepVariant or other) as certain to give. > you fully correct results on this problem right out of the box. Yo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:1054,security,compl,complex,1054,"nformative. Currently I am using the GATK pipeline to identify any novel strains for a. particular bacteria genus. So far it makes sense, meaning the known mutated. locations on the genome can be found using the pipeline. However, there are. a lot of possible false positives, which could be either potential new. SNPs, or just sequencing error. That's what prompted me to explore. DeepVarient in the first place, to minimize false positives from sequencing. error. As I understand that GATK has a step to incorporate known variants to. correct for downstream analysis. But bacteria in general do not have those. information. I am wondering do you have any toolbox on this topic? I'd love to perform some comparisons between different variant detection of. bacteria genomes in the near future. Will keep you posted if I find. anything interesting. On Wed, Oct 31, 2018 at 7:13 PM Andrew Carroll <notifications@github.com>. wrote:. > This is a very interesting question, and the answers to it are complex. I. > assume for the sake of the question that you have FASTQ data from. > sequencing a single bacterial colony (so this is not a metagenomics. > question). >. > Let's divide the question between ""can it technically be done"" and ""will. > the answers be scientifically valid"". >. > To the question ""can it technically be done"", the answer is probably yes. > I am not aware that we have specifically attempted this in bacteria. But if. > you have a FASTA file with a reference for a species and FASTQ reads, you. > should be able to generate variant calls for it. >. > To the question ""will the answers be scientifically valid"", it is. > important to note calling variants in bacterial genomes is an area of open. > research. Using DeepVariant is reasonable, but I don't think you'll able to. > consider the output of any method (DeepVariant or other) as certain to give. > you fully correct results on this problem right out of the box. You'll want. > to use a few methods (use Freebayes and GATK)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:2122,security,validat,validate,2122," question, and the answers to it are complex. I. > assume for the sake of the question that you have FASTQ data from. > sequencing a single bacterial colony (so this is not a metagenomics. > question). >. > Let's divide the question between ""can it technically be done"" and ""will. > the answers be scientifically valid"". >. > To the question ""can it technically be done"", the answer is probably yes. > I am not aware that we have specifically attempted this in bacteria. But if. > you have a FASTA file with a reference for a species and FASTQ reads, you. > should be able to generate variant calls for it. >. > To the question ""will the answers be scientifically valid"", it is. > important to note calling variants in bacterial genomes is an area of open. > research. Using DeepVariant is reasonable, but I don't think you'll able to. > consider the output of any method (DeepVariant or other) as certain to give. > you fully correct results on this problem right out of the box. You'll want. > to use a few methods (use Freebayes and GATK) and compare between them with. > metrics you can independently validate, then decide what works and doesn't. > for your use case. >. > One way to do this could be that for a clonal lineage you expect variants. > to all be called as 1/1 for the non-plasmid genome sequence. Ryan Poplin. > used this measure in a similar way to compare DeepVariant and other methods. > on inbred rice strains from the 3000 Rice Genomes Project. >. > We would be quite interested to receive your feedback on how DeepVariant. > performs in this use case, as this may help us understand the value of. > DeepVariant and improve it for the community. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/114#issuecomment-434889612>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AHCP08xLJBFnbpQzdBn9MXeFKOyacKs9ks5uqjzDgaJpZM4YEtb1>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:2240,security,lineag,lineage,2240," question, and the answers to it are complex. I. > assume for the sake of the question that you have FASTQ data from. > sequencing a single bacterial colony (so this is not a metagenomics. > question). >. > Let's divide the question between ""can it technically be done"" and ""will. > the answers be scientifically valid"". >. > To the question ""can it technically be done"", the answer is probably yes. > I am not aware that we have specifically attempted this in bacteria. But if. > you have a FASTA file with a reference for a species and FASTQ reads, you. > should be able to generate variant calls for it. >. > To the question ""will the answers be scientifically valid"", it is. > important to note calling variants in bacterial genomes is an area of open. > research. Using DeepVariant is reasonable, but I don't think you'll able to. > consider the output of any method (DeepVariant or other) as certain to give. > you fully correct results on this problem right out of the box. You'll want. > to use a few methods (use Freebayes and GATK) and compare between them with. > metrics you can independently validate, then decide what works and doesn't. > for your use case. >. > One way to do this could be that for a clonal lineage you expect variants. > to all be called as 1/1 for the non-plasmid genome sequence. Ryan Poplin. > used this measure in a similar way to compare DeepVariant and other methods. > on inbred rice strains from the 3000 Rice Genomes Project. >. > We would be quite interested to receive your feedback on how DeepVariant. > performs in this use case, as this may help us understand the value of. > DeepVariant and improve it for the community. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/114#issuecomment-434889612>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AHCP08xLJBFnbpQzdBn9MXeFKOyacKs9ks5uqjzDgaJpZM4YEtb1>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:2731,security,auth,authored,2731," question, and the answers to it are complex. I. > assume for the sake of the question that you have FASTQ data from. > sequencing a single bacterial colony (so this is not a metagenomics. > question). >. > Let's divide the question between ""can it technically be done"" and ""will. > the answers be scientifically valid"". >. > To the question ""can it technically be done"", the answer is probably yes. > I am not aware that we have specifically attempted this in bacteria. But if. > you have a FASTA file with a reference for a species and FASTQ reads, you. > should be able to generate variant calls for it. >. > To the question ""will the answers be scientifically valid"", it is. > important to note calling variants in bacterial genomes is an area of open. > research. Using DeepVariant is reasonable, but I don't think you'll able to. > consider the output of any method (DeepVariant or other) as certain to give. > you fully correct results on this problem right out of the box. You'll want. > to use a few methods (use Freebayes and GATK) and compare between them with. > metrics you can independently validate, then decide what works and doesn't. > for your use case. >. > One way to do this could be that for a clonal lineage you expect variants. > to all be called as 1/1 for the non-plasmid genome sequence. Ryan Poplin. > used this measure in a similar way to compare DeepVariant and other methods. > on inbred rice strains from the 3000 Rice Genomes Project. >. > We would be quite interested to receive your feedback on how DeepVariant. > performs in this use case, as this may help us understand the value of. > DeepVariant and improve it for the community. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/114#issuecomment-434889612>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AHCP08xLJBFnbpQzdBn9MXeFKOyacKs9ks5uqjzDgaJpZM4YEtb1>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:2951,security,auth,auth,2951," question, and the answers to it are complex. I. > assume for the sake of the question that you have FASTQ data from. > sequencing a single bacterial colony (so this is not a metagenomics. > question). >. > Let's divide the question between ""can it technically be done"" and ""will. > the answers be scientifically valid"". >. > To the question ""can it technically be done"", the answer is probably yes. > I am not aware that we have specifically attempted this in bacteria. But if. > you have a FASTA file with a reference for a species and FASTQ reads, you. > should be able to generate variant calls for it. >. > To the question ""will the answers be scientifically valid"", it is. > important to note calling variants in bacterial genomes is an area of open. > research. Using DeepVariant is reasonable, but I don't think you'll able to. > consider the output of any method (DeepVariant or other) as certain to give. > you fully correct results on this problem right out of the box. You'll want. > to use a few methods (use Freebayes and GATK) and compare between them with. > metrics you can independently validate, then decide what works and doesn't. > for your use case. >. > One way to do this could be that for a clonal lineage you expect variants. > to all be called as 1/1 for the non-plasmid genome sequence. Ryan Poplin. > used this measure in a similar way to compare DeepVariant and other methods. > on inbred rice strains from the 3000 Rice Genomes Project. >. > We would be quite interested to receive your feedback on how DeepVariant. > performs in this use case, as this may help us understand the value of. > DeepVariant and improve it for the community. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/114#issuecomment-434889612>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AHCP08xLJBFnbpQzdBn9MXeFKOyacKs9ks5uqjzDgaJpZM4YEtb1>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:529,testability,understand,understand,529,"Hi Andrew,. Thanks for answering my questions.It is very informative. Currently I am using the GATK pipeline to identify any novel strains for a. particular bacteria genus. So far it makes sense, meaning the known mutated. locations on the genome can be found using the pipeline. However, there are. a lot of possible false positives, which could be either potential new. SNPs, or just sequencing error. That's what prompted me to explore. DeepVarient in the first place, to minimize false positives from sequencing. error. As I understand that GATK has a step to incorporate known variants to. correct for downstream analysis. But bacteria in general do not have those. information. I am wondering do you have any toolbox on this topic? I'd love to perform some comparisons between different variant detection of. bacteria genomes in the near future. Will keep you posted if I find. anything interesting. On Wed, Oct 31, 2018 at 7:13 PM Andrew Carroll <notifications@github.com>. wrote:. > This is a very interesting question, and the answers to it are complex. I. > assume for the sake of the question that you have FASTQ data from. > sequencing a single bacterial colony (so this is not a metagenomics. > question). >. > Let's divide the question between ""can it technically be done"" and ""will. > the answers be scientifically valid"". >. > To the question ""can it technically be done"", the answer is probably yes. > I am not aware that we have specifically attempted this in bacteria. But if. > you have a FASTA file with a reference for a species and FASTQ reads, you. > should be able to generate variant calls for it. >. > To the question ""will the answers be scientifically valid"", it is. > important to note calling variants in bacterial genomes is an area of open. > research. Using DeepVariant is reasonable, but I don't think you'll able to. > consider the output of any method (DeepVariant or other) as certain to give. > you fully correct results on this problem right out of the box. Yo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:2613,testability,understand,understand,2613," question, and the answers to it are complex. I. > assume for the sake of the question that you have FASTQ data from. > sequencing a single bacterial colony (so this is not a metagenomics. > question). >. > Let's divide the question between ""can it technically be done"" and ""will. > the answers be scientifically valid"". >. > To the question ""can it technically be done"", the answer is probably yes. > I am not aware that we have specifically attempted this in bacteria. But if. > you have a FASTA file with a reference for a species and FASTQ reads, you. > should be able to generate variant calls for it. >. > To the question ""will the answers be scientifically valid"", it is. > important to note calling variants in bacterial genomes is an area of open. > research. Using DeepVariant is reasonable, but I don't think you'll able to. > consider the output of any method (DeepVariant or other) as certain to give. > you fully correct results on this problem right out of the box. You'll want. > to use a few methods (use Freebayes and GATK) and compare between them with. > metrics you can independently validate, then decide what works and doesn't. > for your use case. >. > One way to do this could be that for a clonal lineage you expect variants. > to all be called as 1/1 for the non-plasmid genome sequence. Ryan Poplin. > used this measure in a similar way to compare DeepVariant and other methods. > on inbred rice strains from the 3000 Rice Genomes Project. >. > We would be quite interested to receive your feedback on how DeepVariant. > performs in this use case, as this may help us understand the value of. > DeepVariant and improve it for the community. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/114#issuecomment-434889612>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AHCP08xLJBFnbpQzdBn9MXeFKOyacKs9ks5uqjzDgaJpZM4YEtb1>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:397,usability,error,error,397,"Hi Andrew,. Thanks for answering my questions.It is very informative. Currently I am using the GATK pipeline to identify any novel strains for a. particular bacteria genus. So far it makes sense, meaning the known mutated. locations on the genome can be found using the pipeline. However, there are. a lot of possible false positives, which could be either potential new. SNPs, or just sequencing error. That's what prompted me to explore. DeepVarient in the first place, to minimize false positives from sequencing. error. As I understand that GATK has a step to incorporate known variants to. correct for downstream analysis. But bacteria in general do not have those. information. I am wondering do you have any toolbox on this topic? I'd love to perform some comparisons between different variant detection of. bacteria genomes in the near future. Will keep you posted if I find. anything interesting. On Wed, Oct 31, 2018 at 7:13 PM Andrew Carroll <notifications@github.com>. wrote:. > This is a very interesting question, and the answers to it are complex. I. > assume for the sake of the question that you have FASTQ data from. > sequencing a single bacterial colony (so this is not a metagenomics. > question). >. > Let's divide the question between ""can it technically be done"" and ""will. > the answers be scientifically valid"". >. > To the question ""can it technically be done"", the answer is probably yes. > I am not aware that we have specifically attempted this in bacteria. But if. > you have a FASTA file with a reference for a species and FASTQ reads, you. > should be able to generate variant calls for it. >. > To the question ""will the answers be scientifically valid"", it is. > important to note calling variants in bacterial genomes is an area of open. > research. Using DeepVariant is reasonable, but I don't think you'll able to. > consider the output of any method (DeepVariant or other) as certain to give. > you fully correct results on this problem right out of the box. Yo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:475,usability,minim,minimize,475,"Hi Andrew,. Thanks for answering my questions.It is very informative. Currently I am using the GATK pipeline to identify any novel strains for a. particular bacteria genus. So far it makes sense, meaning the known mutated. locations on the genome can be found using the pipeline. However, there are. a lot of possible false positives, which could be either potential new. SNPs, or just sequencing error. That's what prompted me to explore. DeepVarient in the first place, to minimize false positives from sequencing. error. As I understand that GATK has a step to incorporate known variants to. correct for downstream analysis. But bacteria in general do not have those. information. I am wondering do you have any toolbox on this topic? I'd love to perform some comparisons between different variant detection of. bacteria genomes in the near future. Will keep you posted if I find. anything interesting. On Wed, Oct 31, 2018 at 7:13 PM Andrew Carroll <notifications@github.com>. wrote:. > This is a very interesting question, and the answers to it are complex. I. > assume for the sake of the question that you have FASTQ data from. > sequencing a single bacterial colony (so this is not a metagenomics. > question). >. > Let's divide the question between ""can it technically be done"" and ""will. > the answers be scientifically valid"". >. > To the question ""can it technically be done"", the answer is probably yes. > I am not aware that we have specifically attempted this in bacteria. But if. > you have a FASTA file with a reference for a species and FASTQ reads, you. > should be able to generate variant calls for it. >. > To the question ""will the answers be scientifically valid"", it is. > important to note calling variants in bacterial genomes is an area of open. > research. Using DeepVariant is reasonable, but I don't think you'll able to. > consider the output of any method (DeepVariant or other) as certain to give. > you fully correct results on this problem right out of the box. Yo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:517,usability,error,error,517,"Hi Andrew,. Thanks for answering my questions.It is very informative. Currently I am using the GATK pipeline to identify any novel strains for a. particular bacteria genus. So far it makes sense, meaning the known mutated. locations on the genome can be found using the pipeline. However, there are. a lot of possible false positives, which could be either potential new. SNPs, or just sequencing error. That's what prompted me to explore. DeepVarient in the first place, to minimize false positives from sequencing. error. As I understand that GATK has a step to incorporate known variants to. correct for downstream analysis. But bacteria in general do not have those. information. I am wondering do you have any toolbox on this topic? I'd love to perform some comparisons between different variant detection of. bacteria genomes in the near future. Will keep you posted if I find. anything interesting. On Wed, Oct 31, 2018 at 7:13 PM Andrew Carroll <notifications@github.com>. wrote:. > This is a very interesting question, and the answers to it are complex. I. > assume for the sake of the question that you have FASTQ data from. > sequencing a single bacterial colony (so this is not a metagenomics. > question). >. > Let's divide the question between ""can it technically be done"" and ""will. > the answers be scientifically valid"". >. > To the question ""can it technically be done"", the answer is probably yes. > I am not aware that we have specifically attempted this in bacteria. But if. > you have a FASTA file with a reference for a species and FASTQ reads, you. > should be able to generate variant calls for it. >. > To the question ""will the answers be scientifically valid"", it is. > important to note calling variants in bacterial genomes is an area of open. > research. Using DeepVariant is reasonable, but I don't think you'll able to. > consider the output of any method (DeepVariant or other) as certain to give. > you fully correct results on this problem right out of the box. Yo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:715,usability,tool,toolbox,715,"Hi Andrew,. Thanks for answering my questions.It is very informative. Currently I am using the GATK pipeline to identify any novel strains for a. particular bacteria genus. So far it makes sense, meaning the known mutated. locations on the genome can be found using the pipeline. However, there are. a lot of possible false positives, which could be either potential new. SNPs, or just sequencing error. That's what prompted me to explore. DeepVarient in the first place, to minimize false positives from sequencing. error. As I understand that GATK has a step to incorporate known variants to. correct for downstream analysis. But bacteria in general do not have those. information. I am wondering do you have any toolbox on this topic? I'd love to perform some comparisons between different variant detection of. bacteria genomes in the near future. Will keep you posted if I find. anything interesting. On Wed, Oct 31, 2018 at 7:13 PM Andrew Carroll <notifications@github.com>. wrote:. > This is a very interesting question, and the answers to it are complex. I. > assume for the sake of the question that you have FASTQ data from. > sequencing a single bacterial colony (so this is not a metagenomics. > question). >. > Let's divide the question between ""can it technically be done"" and ""will. > the answers be scientifically valid"". >. > To the question ""can it technically be done"", the answer is probably yes. > I am not aware that we have specifically attempted this in bacteria. But if. > you have a FASTA file with a reference for a species and FASTQ reads, you. > should be able to generate variant calls for it. >. > To the question ""will the answers be scientifically valid"", it is. > important to note calling variants in bacterial genomes is an area of open. > research. Using DeepVariant is reasonable, but I don't think you'll able to. > consider the output of any method (DeepVariant or other) as certain to give. > you fully correct results on this problem right out of the box. Yo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:750,usability,perform,perform,750,"Hi Andrew,. Thanks for answering my questions.It is very informative. Currently I am using the GATK pipeline to identify any novel strains for a. particular bacteria genus. So far it makes sense, meaning the known mutated. locations on the genome can be found using the pipeline. However, there are. a lot of possible false positives, which could be either potential new. SNPs, or just sequencing error. That's what prompted me to explore. DeepVarient in the first place, to minimize false positives from sequencing. error. As I understand that GATK has a step to incorporate known variants to. correct for downstream analysis. But bacteria in general do not have those. information. I am wondering do you have any toolbox on this topic? I'd love to perform some comparisons between different variant detection of. bacteria genomes in the near future. Will keep you posted if I find. anything interesting. On Wed, Oct 31, 2018 at 7:13 PM Andrew Carroll <notifications@github.com>. wrote:. > This is a very interesting question, and the answers to it are complex. I. > assume for the sake of the question that you have FASTQ data from. > sequencing a single bacterial colony (so this is not a metagenomics. > question). >. > Let's divide the question between ""can it technically be done"" and ""will. > the answers be scientifically valid"". >. > To the question ""can it technically be done"", the answer is probably yes. > I am not aware that we have specifically attempted this in bacteria. But if. > you have a FASTA file with a reference for a species and FASTQ reads, you. > should be able to generate variant calls for it. >. > To the question ""will the answers be scientifically valid"", it is. > important to note calling variants in bacterial genomes is an area of open. > research. Using DeepVariant is reasonable, but I don't think you'll able to. > consider the output of any method (DeepVariant or other) as certain to give. > you fully correct results on this problem right out of the box. Yo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:2535,usability,feedback,feedback,2535," question, and the answers to it are complex. I. > assume for the sake of the question that you have FASTQ data from. > sequencing a single bacterial colony (so this is not a metagenomics. > question). >. > Let's divide the question between ""can it technically be done"" and ""will. > the answers be scientifically valid"". >. > To the question ""can it technically be done"", the answer is probably yes. > I am not aware that we have specifically attempted this in bacteria. But if. > you have a FASTA file with a reference for a species and FASTQ reads, you. > should be able to generate variant calls for it. >. > To the question ""will the answers be scientifically valid"", it is. > important to note calling variants in bacterial genomes is an area of open. > research. Using DeepVariant is reasonable, but I don't think you'll able to. > consider the output of any method (DeepVariant or other) as certain to give. > you fully correct results on this problem right out of the box. You'll want. > to use a few methods (use Freebayes and GATK) and compare between them with. > metrics you can independently validate, then decide what works and doesn't. > for your use case. >. > One way to do this could be that for a clonal lineage you expect variants. > to all be called as 1/1 for the non-plasmid genome sequence. Ryan Poplin. > used this measure in a similar way to compare DeepVariant and other methods. > on inbred rice strains from the 3000 Rice Genomes Project. >. > We would be quite interested to receive your feedback on how DeepVariant. > performs in this use case, as this may help us understand the value of. > DeepVariant and improve it for the community. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/114#issuecomment-434889612>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AHCP08xLJBFnbpQzdBn9MXeFKOyacKs9ks5uqjzDgaJpZM4YEtb1>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:2566,usability,perform,performs,2566," question, and the answers to it are complex. I. > assume for the sake of the question that you have FASTQ data from. > sequencing a single bacterial colony (so this is not a metagenomics. > question). >. > Let's divide the question between ""can it technically be done"" and ""will. > the answers be scientifically valid"". >. > To the question ""can it technically be done"", the answer is probably yes. > I am not aware that we have specifically attempted this in bacteria. But if. > you have a FASTA file with a reference for a species and FASTQ reads, you. > should be able to generate variant calls for it. >. > To the question ""will the answers be scientifically valid"", it is. > important to note calling variants in bacterial genomes is an area of open. > research. Using DeepVariant is reasonable, but I don't think you'll able to. > consider the output of any method (DeepVariant or other) as certain to give. > you fully correct results on this problem right out of the box. You'll want. > to use a few methods (use Freebayes and GATK) and compare between them with. > metrics you can independently validate, then decide what works and doesn't. > for your use case. >. > One way to do this could be that for a clonal lineage you expect variants. > to all be called as 1/1 for the non-plasmid genome sequence. Ryan Poplin. > used this measure in a similar way to compare DeepVariant and other methods. > on inbred rice strains from the 3000 Rice Genomes Project. >. > We would be quite interested to receive your feedback on how DeepVariant. > performs in this use case, as this may help us understand the value of. > DeepVariant and improve it for the community. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/114#issuecomment-434889612>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AHCP08xLJBFnbpQzdBn9MXeFKOyacKs9ks5uqjzDgaJpZM4YEtb1>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:2605,usability,help,help,2605," question, and the answers to it are complex. I. > assume for the sake of the question that you have FASTQ data from. > sequencing a single bacterial colony (so this is not a metagenomics. > question). >. > Let's divide the question between ""can it technically be done"" and ""will. > the answers be scientifically valid"". >. > To the question ""can it technically be done"", the answer is probably yes. > I am not aware that we have specifically attempted this in bacteria. But if. > you have a FASTA file with a reference for a species and FASTQ reads, you. > should be able to generate variant calls for it. >. > To the question ""will the answers be scientifically valid"", it is. > important to note calling variants in bacterial genomes is an area of open. > research. Using DeepVariant is reasonable, but I don't think you'll able to. > consider the output of any method (DeepVariant or other) as certain to give. > you fully correct results on this problem right out of the box. You'll want. > to use a few methods (use Freebayes and GATK) and compare between them with. > metrics you can independently validate, then decide what works and doesn't. > for your use case. >. > One way to do this could be that for a clonal lineage you expect variants. > to all be called as 1/1 for the non-plasmid genome sequence. Ryan Poplin. > used this measure in a similar way to compare DeepVariant and other methods. > on inbred rice strains from the 3000 Rice Genomes Project. >. > We would be quite interested to receive your feedback on how DeepVariant. > performs in this use case, as this may help us understand the value of. > DeepVariant and improve it for the community. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/114#issuecomment-434889612>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AHCP08xLJBFnbpQzdBn9MXeFKOyacKs9ks5uqjzDgaJpZM4YEtb1>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:361,availability,avail,available,361,"When you refer to the method GATK has to incorporate known variants for quality control, I believe that you are referring to VQSR (variant quality score recalibrator). You are correct that this requires a set of ground truth information. It seems likely to me that VQSR and similar systems would not behave correctly on bacterial data even if ground truth were available. To understand whether events are false positives or real SNPs, I think you will want to get some orthogonal measure of whether they are accurate. It probably makes sense to look at the reads data for some of these sites in a tool like IGV to get a feel for whether they look reliable or not. One nice thing about DeepVariant is that the quality scores for variants calls are generally normally distributed and predict the human variant quality confidence quite well. One approach you can take is to see what the distribution of DeepVariant quality scores is for the variants you know are real and see whether the new variants have a similar confidence distribution.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:647,availability,reliab,reliable,647,"When you refer to the method GATK has to incorporate known variants for quality control, I believe that you are referring to VQSR (variant quality score recalibrator). You are correct that this requires a set of ground truth information. It seems likely to me that VQSR and similar systems would not behave correctly on bacterial data even if ground truth were available. To understand whether events are false positives or real SNPs, I think you will want to get some orthogonal measure of whether they are accurate. It probably makes sense to look at the reads data for some of these sites in a tool like IGV to get a feel for whether they look reliable or not. One nice thing about DeepVariant is that the quality scores for variants calls are generally normally distributed and predict the human variant quality confidence quite well. One approach you can take is to see what the distribution of DeepVariant quality scores is for the variants you know are real and see whether the new variants have a similar confidence distribution.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:480,energy efficiency,measur,measure,480,"When you refer to the method GATK has to incorporate known variants for quality control, I believe that you are referring to VQSR (variant quality score recalibrator). You are correct that this requires a set of ground truth information. It seems likely to me that VQSR and similar systems would not behave correctly on bacterial data even if ground truth were available. To understand whether events are false positives or real SNPs, I think you will want to get some orthogonal measure of whether they are accurate. It probably makes sense to look at the reads data for some of these sites in a tool like IGV to get a feel for whether they look reliable or not. One nice thing about DeepVariant is that the quality scores for variants calls are generally normally distributed and predict the human variant quality confidence quite well. One approach you can take is to see what the distribution of DeepVariant quality scores is for the variants you know are real and see whether the new variants have a similar confidence distribution.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:782,energy efficiency,predict,predict,782,"When you refer to the method GATK has to incorporate known variants for quality control, I believe that you are referring to VQSR (variant quality score recalibrator). You are correct that this requires a set of ground truth information. It seems likely to me that VQSR and similar systems would not behave correctly on bacterial data even if ground truth were available. To understand whether events are false positives or real SNPs, I think you will want to get some orthogonal measure of whether they are accurate. It probably makes sense to look at the reads data for some of these sites in a tool like IGV to get a feel for whether they look reliable or not. One nice thing about DeepVariant is that the quality scores for variants calls are generally normally distributed and predict the human variant quality confidence quite well. One approach you can take is to see what the distribution of DeepVariant quality scores is for the variants you know are real and see whether the new variants have a similar confidence distribution.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:394,integrability,event,events,394,"When you refer to the method GATK has to incorporate known variants for quality control, I believe that you are referring to VQSR (variant quality score recalibrator). You are correct that this requires a set of ground truth information. It seems likely to me that VQSR and similar systems would not behave correctly on bacterial data even if ground truth were available. To understand whether events are false positives or real SNPs, I think you will want to get some orthogonal measure of whether they are accurate. It probably makes sense to look at the reads data for some of these sites in a tool like IGV to get a feel for whether they look reliable or not. One nice thing about DeepVariant is that the quality scores for variants calls are generally normally distributed and predict the human variant quality confidence quite well. One approach you can take is to see what the distribution of DeepVariant quality scores is for the variants you know are real and see whether the new variants have a similar confidence distribution.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:766,interoperability,distribut,distributed,766,"When you refer to the method GATK has to incorporate known variants for quality control, I believe that you are referring to VQSR (variant quality score recalibrator). You are correct that this requires a set of ground truth information. It seems likely to me that VQSR and similar systems would not behave correctly on bacterial data even if ground truth were available. To understand whether events are false positives or real SNPs, I think you will want to get some orthogonal measure of whether they are accurate. It probably makes sense to look at the reads data for some of these sites in a tool like IGV to get a feel for whether they look reliable or not. One nice thing about DeepVariant is that the quality scores for variants calls are generally normally distributed and predict the human variant quality confidence quite well. One approach you can take is to see what the distribution of DeepVariant quality scores is for the variants you know are real and see whether the new variants have a similar confidence distribution.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:884,interoperability,distribut,distribution,884,"When you refer to the method GATK has to incorporate known variants for quality control, I believe that you are referring to VQSR (variant quality score recalibrator). You are correct that this requires a set of ground truth information. It seems likely to me that VQSR and similar systems would not behave correctly on bacterial data even if ground truth were available. To understand whether events are false positives or real SNPs, I think you will want to get some orthogonal measure of whether they are accurate. It probably makes sense to look at the reads data for some of these sites in a tool like IGV to get a feel for whether they look reliable or not. One nice thing about DeepVariant is that the quality scores for variants calls are generally normally distributed and predict the human variant quality confidence quite well. One approach you can take is to see what the distribution of DeepVariant quality scores is for the variants you know are real and see whether the new variants have a similar confidence distribution.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:1024,interoperability,distribut,distribution,1024,"When you refer to the method GATK has to incorporate known variants for quality control, I believe that you are referring to VQSR (variant quality score recalibrator). You are correct that this requires a set of ground truth information. It seems likely to me that VQSR and similar systems would not behave correctly on bacterial data even if ground truth were available. To understand whether events are false positives or real SNPs, I think you will want to get some orthogonal measure of whether they are accurate. It probably makes sense to look at the reads data for some of these sites in a tool like IGV to get a feel for whether they look reliable or not. One nice thing about DeepVariant is that the quality scores for variants calls are generally normally distributed and predict the human variant quality confidence quite well. One approach you can take is to see what the distribution of DeepVariant quality scores is for the variants you know are real and see whether the new variants have a similar confidence distribution.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:361,reliability,availab,available,361,"When you refer to the method GATK has to incorporate known variants for quality control, I believe that you are referring to VQSR (variant quality score recalibrator). You are correct that this requires a set of ground truth information. It seems likely to me that VQSR and similar systems would not behave correctly on bacterial data even if ground truth were available. To understand whether events are false positives or real SNPs, I think you will want to get some orthogonal measure of whether they are accurate. It probably makes sense to look at the reads data for some of these sites in a tool like IGV to get a feel for whether they look reliable or not. One nice thing about DeepVariant is that the quality scores for variants calls are generally normally distributed and predict the human variant quality confidence quite well. One approach you can take is to see what the distribution of DeepVariant quality scores is for the variants you know are real and see whether the new variants have a similar confidence distribution.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:647,reliability,reliab,reliable,647,"When you refer to the method GATK has to incorporate known variants for quality control, I believe that you are referring to VQSR (variant quality score recalibrator). You are correct that this requires a set of ground truth information. It seems likely to me that VQSR and similar systems would not behave correctly on bacterial data even if ground truth were available. To understand whether events are false positives or real SNPs, I think you will want to get some orthogonal measure of whether they are accurate. It probably makes sense to look at the reads data for some of these sites in a tool like IGV to get a feel for whether they look reliable or not. One nice thing about DeepVariant is that the quality scores for variants calls are generally normally distributed and predict the human variant quality confidence quite well. One approach you can take is to see what the distribution of DeepVariant quality scores is for the variants you know are real and see whether the new variants have a similar confidence distribution.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:361,safety,avail,available,361,"When you refer to the method GATK has to incorporate known variants for quality control, I believe that you are referring to VQSR (variant quality score recalibrator). You are correct that this requires a set of ground truth information. It seems likely to me that VQSR and similar systems would not behave correctly on bacterial data even if ground truth were available. To understand whether events are false positives or real SNPs, I think you will want to get some orthogonal measure of whether they are accurate. It probably makes sense to look at the reads data for some of these sites in a tool like IGV to get a feel for whether they look reliable or not. One nice thing about DeepVariant is that the quality scores for variants calls are generally normally distributed and predict the human variant quality confidence quite well. One approach you can take is to see what the distribution of DeepVariant quality scores is for the variants you know are real and see whether the new variants have a similar confidence distribution.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:782,safety,predict,predict,782,"When you refer to the method GATK has to incorporate known variants for quality control, I believe that you are referring to VQSR (variant quality score recalibrator). You are correct that this requires a set of ground truth information. It seems likely to me that VQSR and similar systems would not behave correctly on bacterial data even if ground truth were available. To understand whether events are false positives or real SNPs, I think you will want to get some orthogonal measure of whether they are accurate. It probably makes sense to look at the reads data for some of these sites in a tool like IGV to get a feel for whether they look reliable or not. One nice thing about DeepVariant is that the quality scores for variants calls are generally normally distributed and predict the human variant quality confidence quite well. One approach you can take is to see what the distribution of DeepVariant quality scores is for the variants you know are real and see whether the new variants have a similar confidence distribution.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:80,security,control,control,80,"When you refer to the method GATK has to incorporate known variants for quality control, I believe that you are referring to VQSR (variant quality score recalibrator). You are correct that this requires a set of ground truth information. It seems likely to me that VQSR and similar systems would not behave correctly on bacterial data even if ground truth were available. To understand whether events are false positives or real SNPs, I think you will want to get some orthogonal measure of whether they are accurate. It probably makes sense to look at the reads data for some of these sites in a tool like IGV to get a feel for whether they look reliable or not. One nice thing about DeepVariant is that the quality scores for variants calls are generally normally distributed and predict the human variant quality confidence quite well. One approach you can take is to see what the distribution of DeepVariant quality scores is for the variants you know are real and see whether the new variants have a similar confidence distribution.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:361,security,availab,available,361,"When you refer to the method GATK has to incorporate known variants for quality control, I believe that you are referring to VQSR (variant quality score recalibrator). You are correct that this requires a set of ground truth information. It seems likely to me that VQSR and similar systems would not behave correctly on bacterial data even if ground truth were available. To understand whether events are false positives or real SNPs, I think you will want to get some orthogonal measure of whether they are accurate. It probably makes sense to look at the reads data for some of these sites in a tool like IGV to get a feel for whether they look reliable or not. One nice thing about DeepVariant is that the quality scores for variants calls are generally normally distributed and predict the human variant quality confidence quite well. One approach you can take is to see what the distribution of DeepVariant quality scores is for the variants you know are real and see whether the new variants have a similar confidence distribution.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:80,testability,control,control,80,"When you refer to the method GATK has to incorporate known variants for quality control, I believe that you are referring to VQSR (variant quality score recalibrator). You are correct that this requires a set of ground truth information. It seems likely to me that VQSR and similar systems would not behave correctly on bacterial data even if ground truth were available. To understand whether events are false positives or real SNPs, I think you will want to get some orthogonal measure of whether they are accurate. It probably makes sense to look at the reads data for some of these sites in a tool like IGV to get a feel for whether they look reliable or not. One nice thing about DeepVariant is that the quality scores for variants calls are generally normally distributed and predict the human variant quality confidence quite well. One approach you can take is to see what the distribution of DeepVariant quality scores is for the variants you know are real and see whether the new variants have a similar confidence distribution.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:375,testability,understand,understand,375,"When you refer to the method GATK has to incorporate known variants for quality control, I believe that you are referring to VQSR (variant quality score recalibrator). You are correct that this requires a set of ground truth information. It seems likely to me that VQSR and similar systems would not behave correctly on bacterial data even if ground truth were available. To understand whether events are false positives or real SNPs, I think you will want to get some orthogonal measure of whether they are accurate. It probably makes sense to look at the reads data for some of these sites in a tool like IGV to get a feel for whether they look reliable or not. One nice thing about DeepVariant is that the quality scores for variants calls are generally normally distributed and predict the human variant quality confidence quite well. One approach you can take is to see what the distribution of DeepVariant quality scores is for the variants you know are real and see whether the new variants have a similar confidence distribution.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:597,usability,tool,tool,597,"When you refer to the method GATK has to incorporate known variants for quality control, I believe that you are referring to VQSR (variant quality score recalibrator). You are correct that this requires a set of ground truth information. It seems likely to me that VQSR and similar systems would not behave correctly on bacterial data even if ground truth were available. To understand whether events are false positives or real SNPs, I think you will want to get some orthogonal measure of whether they are accurate. It probably makes sense to look at the reads data for some of these sites in a tool like IGV to get a feel for whether they look reliable or not. One nice thing about DeepVariant is that the quality scores for variants calls are generally normally distributed and predict the human variant quality confidence quite well. One approach you can take is to see what the distribution of DeepVariant quality scores is for the variants you know are real and see whether the new variants have a similar confidence distribution.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:124,deployability,pipelin,pipeline,124,We are also interested in exploring the possibility of using DeepVariant to call SNPs in bacterial genomes. For our current pipeline we are using BWA to map reads to the reference and then PILON from the Broad to call variants. Specifically we are working on the Tuberculosis genome.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:116,energy efficiency,current,current,116,We are also interested in exploring the possibility of using DeepVariant to call SNPs in bacterial genomes. For our current pipeline we are using BWA to map reads to the reference and then PILON from the Broad to call variants. Specifically we are working on the Tuberculosis genome.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:124,integrability,pipelin,pipeline,124,We are also interested in exploring the possibility of using DeepVariant to call SNPs in bacterial genomes. For our current pipeline we are using BWA to map reads to the reference and then PILON from the Broad to call variants. Specifically we are working on the Tuberculosis genome.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:228,interoperability,Specif,Specifically,228,We are also interested in exploring the possibility of using DeepVariant to call SNPs in bacterial genomes. For our current pipeline we are using BWA to map reads to the reference and then PILON from the Broad to call variants. Specifically we are working on the Tuberculosis genome.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:504,deployability,pipelin,pipeline,504,"Thanks Michael, I'd love to know what conclusions about how the accuracy. and background noise detection you have on the TB genome. If it is not too. much trouble, can you keep me posted? Or you have some blog/post or paper I. can follow up on? You guys are totally rocking it!! Very exciting study!! On Thu, Nov 8, 2018 at 2:47 PM Michael <notifications@github.com> wrote:. > We are also interested in exploring the possibility of using DeepVariant. > to call SNPs in bacterial genomes. For our current pipeline we are using. > BWA to map reads to the reference and then PILON from the Broad to call. > variants. Specifically we are working on the Tuberculosis genome. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/114#issuecomment-437149890>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AHCP00NuLoTAyzg9zh_dKzzgjzby9ytMks5utJhlgaJpZM4YEtb1>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:496,energy efficiency,current,current,496,"Thanks Michael, I'd love to know what conclusions about how the accuracy. and background noise detection you have on the TB genome. If it is not too. much trouble, can you keep me posted? Or you have some blog/post or paper I. can follow up on? You guys are totally rocking it!! Very exciting study!! On Thu, Nov 8, 2018 at 2:47 PM Michael <notifications@github.com> wrote:. > We are also interested in exploring the possibility of using DeepVariant. > to call SNPs in bacterial genomes. For our current pipeline we are using. > BWA to map reads to the reference and then PILON from the Broad to call. > variants. Specifically we are working on the Tuberculosis genome. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/114#issuecomment-437149890>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AHCP00NuLoTAyzg9zh_dKzzgjzby9ytMks5utJhlgaJpZM4YEtb1>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:504,integrability,pipelin,pipeline,504,"Thanks Michael, I'd love to know what conclusions about how the accuracy. and background noise detection you have on the TB genome. If it is not too. much trouble, can you keep me posted? Or you have some blog/post or paper I. can follow up on? You guys are totally rocking it!! Very exciting study!! On Thu, Nov 8, 2018 at 2:47 PM Michael <notifications@github.com> wrote:. > We are also interested in exploring the possibility of using DeepVariant. > to call SNPs in bacterial genomes. For our current pipeline we are using. > BWA to map reads to the reference and then PILON from the Broad to call. > variants. Specifically we are working on the Tuberculosis genome. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/114#issuecomment-437149890>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AHCP00NuLoTAyzg9zh_dKzzgjzby9ytMks5utJhlgaJpZM4YEtb1>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:614,interoperability,Specif,Specifically,614,"Thanks Michael, I'd love to know what conclusions about how the accuracy. and background noise detection you have on the TB genome. If it is not too. much trouble, can you keep me posted? Or you have some blog/post or paper I. can follow up on? You guys are totally rocking it!! Very exciting study!! On Thu, Nov 8, 2018 at 2:47 PM Michael <notifications@github.com> wrote:. > We are also interested in exploring the possibility of using DeepVariant. > to call SNPs in bacterial genomes. For our current pipeline we are using. > BWA to map reads to the reference and then PILON from the Broad to call. > variants. Specifically we are working on the Tuberculosis genome. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/114#issuecomment-437149890>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AHCP00NuLoTAyzg9zh_dKzzgjzby9ytMks5utJhlgaJpZM4YEtb1>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:95,safety,detect,detection,95,"Thanks Michael, I'd love to know what conclusions about how the accuracy. and background noise detection you have on the TB genome. If it is not too. much trouble, can you keep me posted? Or you have some blog/post or paper I. can follow up on? You guys are totally rocking it!! Very exciting study!! On Thu, Nov 8, 2018 at 2:47 PM Michael <notifications@github.com> wrote:. > We are also interested in exploring the possibility of using DeepVariant. > to call SNPs in bacterial genomes. For our current pipeline we are using. > BWA to map reads to the reference and then PILON from the Broad to call. > variants. Specifically we are working on the Tuberculosis genome. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/114#issuecomment-437149890>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AHCP00NuLoTAyzg9zh_dKzzgjzby9ytMks5utJhlgaJpZM4YEtb1>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:95,security,detect,detection,95,"Thanks Michael, I'd love to know what conclusions about how the accuracy. and background noise detection you have on the TB genome. If it is not too. much trouble, can you keep me posted? Or you have some blog/post or paper I. can follow up on? You guys are totally rocking it!! Very exciting study!! On Thu, Nov 8, 2018 at 2:47 PM Michael <notifications@github.com> wrote:. > We are also interested in exploring the possibility of using DeepVariant. > to call SNPs in bacterial genomes. For our current pipeline we are using. > BWA to map reads to the reference and then PILON from the Broad to call. > variants. Specifically we are working on the Tuberculosis genome. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/114#issuecomment-437149890>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AHCP00NuLoTAyzg9zh_dKzzgjzby9ytMks5utJhlgaJpZM4YEtb1>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:715,security,auth,authored,715,"Thanks Michael, I'd love to know what conclusions about how the accuracy. and background noise detection you have on the TB genome. If it is not too. much trouble, can you keep me posted? Or you have some blog/post or paper I. can follow up on? You guys are totally rocking it!! Very exciting study!! On Thu, Nov 8, 2018 at 2:47 PM Michael <notifications@github.com> wrote:. > We are also interested in exploring the possibility of using DeepVariant. > to call SNPs in bacterial genomes. For our current pipeline we are using. > BWA to map reads to the reference and then PILON from the Broad to call. > variants. Specifically we are working on the Tuberculosis genome. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/114#issuecomment-437149890>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AHCP00NuLoTAyzg9zh_dKzzgjzby9ytMks5utJhlgaJpZM4YEtb1>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:935,security,auth,auth,935,"Thanks Michael, I'd love to know what conclusions about how the accuracy. and background noise detection you have on the TB genome. If it is not too. much trouble, can you keep me posted? Or you have some blog/post or paper I. can follow up on? You guys are totally rocking it!! Very exciting study!! On Thu, Nov 8, 2018 at 2:47 PM Michael <notifications@github.com> wrote:. > We are also interested in exploring the possibility of using DeepVariant. > to call SNPs in bacterial genomes. For our current pipeline we are using. > BWA to map reads to the reference and then PILON from the Broad to call. > variants. Specifically we are working on the Tuberculosis genome. >. > . > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/114#issuecomment-437149890>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AHCP00NuLoTAyzg9zh_dKzzgjzby9ytMks5utJhlgaJpZM4YEtb1>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:213,deployability,updat,updates,213,"I'm happy to see that this forum has sparked potential research collaborations! I'm going to close this issue for now so it's easier for me to track what issues still need our attention. Feel free to post further updates on this topic if you like. If you have more questions for the DeepVariant team, also feel free to open more issues. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:229,integrability,topic,topic,229,"I'm happy to see that this forum has sparked potential research collaborations! I'm going to close this issue for now so it's easier for me to track what issues still need our attention. Feel free to post further updates on this topic if you like. If you have more questions for the DeepVariant team, also feel free to open more issues. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:213,safety,updat,updates,213,"I'm happy to see that this forum has sparked potential research collaborations! I'm going to close this issue for now so it's easier for me to track what issues still need our attention. Feel free to post further updates on this topic if you like. If you have more questions for the DeepVariant team, also feel free to open more issues. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:213,security,updat,updates,213,"I'm happy to see that this forum has sparked potential research collaborations! I'm going to close this issue for now so it's easier for me to track what issues still need our attention. Feel free to post further updates on this topic if you like. If you have more questions for the DeepVariant team, also feel free to open more issues. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:295,security,team,team,295,"I'm happy to see that this forum has sparked potential research collaborations! I'm going to close this issue for now so it's easier for me to track what issues still need our attention. Feel free to post further updates on this topic if you like. If you have more questions for the DeepVariant team, also feel free to open more issues. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/114:93,usability,close,close,93,"I'm happy to see that this forum has sparked potential research collaborations! I'm going to close this issue for now so it's easier for me to track what issues still need our attention. Feel free to post further updates on this topic if you like. If you have more questions for the DeepVariant team, also feel free to open more issues. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/114
https://github.com/google/deepvariant/issues/115:46,energy efficiency,current,currently,46,"Hi @melkerdawy ,. the DeepVariant codebase is currently designed to for DNA data only. The underlying tool and principle (of converting genomic data into a machine learning problem) could be generalized. But the existing tool as is isn't designed or used for RNA-seq data. In another word - it could work, but it will be open-ended research. I'd recommend you looking into how DeepVariant is done, and look into the [Nucleus](https://github.com/google/nucleus) library as well. We just recently announced a pip package for Nucleus. . Feel free to share your experimental results and discuss any issues you've encountered. We'll try our best to answer and discuss with you here. (Closing for now. Feel free to re-open)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/115
https://github.com/google/deepvariant/issues/115:547,interoperability,share,share,547,"Hi @melkerdawy ,. the DeepVariant codebase is currently designed to for DNA data only. The underlying tool and principle (of converting genomic data into a machine learning problem) could be generalized. But the existing tool as is isn't designed or used for RNA-seq data. In another word - it could work, but it will be open-ended research. I'd recommend you looking into how DeepVariant is done, and look into the [Nucleus](https://github.com/google/nucleus) library as well. We just recently announced a pip package for Nucleus. . Feel free to share your experimental results and discuss any issues you've encountered. We'll try our best to answer and discuss with you here. (Closing for now. Feel free to re-open)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/115
https://github.com/google/deepvariant/issues/115:511,modifiability,pac,package,511,"Hi @melkerdawy ,. the DeepVariant codebase is currently designed to for DNA data only. The underlying tool and principle (of converting genomic data into a machine learning problem) could be generalized. But the existing tool as is isn't designed or used for RNA-seq data. In another word - it could work, but it will be open-ended research. I'd recommend you looking into how DeepVariant is done, and look into the [Nucleus](https://github.com/google/nucleus) library as well. We just recently announced a pip package for Nucleus. . Feel free to share your experimental results and discuss any issues you've encountered. We'll try our best to answer and discuss with you here. (Closing for now. Feel free to re-open)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/115
https://github.com/google/deepvariant/issues/115:102,usability,tool,tool,102,"Hi @melkerdawy ,. the DeepVariant codebase is currently designed to for DNA data only. The underlying tool and principle (of converting genomic data into a machine learning problem) could be generalized. But the existing tool as is isn't designed or used for RNA-seq data. In another word - it could work, but it will be open-ended research. I'd recommend you looking into how DeepVariant is done, and look into the [Nucleus](https://github.com/google/nucleus) library as well. We just recently announced a pip package for Nucleus. . Feel free to share your experimental results and discuss any issues you've encountered. We'll try our best to answer and discuss with you here. (Closing for now. Feel free to re-open)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/115
https://github.com/google/deepvariant/issues/115:164,usability,learn,learning,164,"Hi @melkerdawy ,. the DeepVariant codebase is currently designed to for DNA data only. The underlying tool and principle (of converting genomic data into a machine learning problem) could be generalized. But the existing tool as is isn't designed or used for RNA-seq data. In another word - it could work, but it will be open-ended research. I'd recommend you looking into how DeepVariant is done, and look into the [Nucleus](https://github.com/google/nucleus) library as well. We just recently announced a pip package for Nucleus. . Feel free to share your experimental results and discuss any issues you've encountered. We'll try our best to answer and discuss with you here. (Closing for now. Feel free to re-open)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/115
https://github.com/google/deepvariant/issues/115:221,usability,tool,tool,221,"Hi @melkerdawy ,. the DeepVariant codebase is currently designed to for DNA data only. The underlying tool and principle (of converting genomic data into a machine learning problem) could be generalized. But the existing tool as is isn't designed or used for RNA-seq data. In another word - it could work, but it will be open-ended research. I'd recommend you looking into how DeepVariant is done, and look into the [Nucleus](https://github.com/google/nucleus) library as well. We just recently announced a pip package for Nucleus. . Feel free to share your experimental results and discuss any issues you've encountered. We'll try our best to answer and discuss with you here. (Closing for now. Feel free to re-open)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/115
https://github.com/google/deepvariant/issues/115:274,interoperability,specif,specific,274,"Sort of a follow-up question, on a related application: can DeepVariant take an RNA-seq bam file obtained via GATK's best practices (link [here](https://software.broadinstitute.org/gatk/documentation/article.php?id=3891)), and get a trustworthy output? . A quick check on a specific locus indicates that DeepVariant's WGS applied to RNA-seq retrieves a few more variants than GATK's ""HaplotypeCaller"". Can we trust it? Has anybody done more comprehensive testing of this? .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/115
https://github.com/google/deepvariant/issues/115:122,reliability,pra,practices,122,"Sort of a follow-up question, on a related application: can DeepVariant take an RNA-seq bam file obtained via GATK's best practices (link [here](https://software.broadinstitute.org/gatk/documentation/article.php?id=3891)), and get a trustworthy output? . A quick check on a specific locus indicates that DeepVariant's WGS applied to RNA-seq retrieves a few more variants than GATK's ""HaplotypeCaller"". Can we trust it? Has anybody done more comprehensive testing of this? .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/115
https://github.com/google/deepvariant/issues/115:455,safety,test,testing,455,"Sort of a follow-up question, on a related application: can DeepVariant take an RNA-seq bam file obtained via GATK's best practices (link [here](https://software.broadinstitute.org/gatk/documentation/article.php?id=3891)), and get a trustworthy output? . A quick check on a specific locus indicates that DeepVariant's WGS applied to RNA-seq retrieves a few more variants than GATK's ""HaplotypeCaller"". Can we trust it? Has anybody done more comprehensive testing of this? .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/115
https://github.com/google/deepvariant/issues/115:233,security,trust,trustworthy,233,"Sort of a follow-up question, on a related application: can DeepVariant take an RNA-seq bam file obtained via GATK's best practices (link [here](https://software.broadinstitute.org/gatk/documentation/article.php?id=3891)), and get a trustworthy output? . A quick check on a specific locus indicates that DeepVariant's WGS applied to RNA-seq retrieves a few more variants than GATK's ""HaplotypeCaller"". Can we trust it? Has anybody done more comprehensive testing of this? .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/115
https://github.com/google/deepvariant/issues/115:409,security,trust,trust,409,"Sort of a follow-up question, on a related application: can DeepVariant take an RNA-seq bam file obtained via GATK's best practices (link [here](https://software.broadinstitute.org/gatk/documentation/article.php?id=3891)), and get a trustworthy output? . A quick check on a specific locus indicates that DeepVariant's WGS applied to RNA-seq retrieves a few more variants than GATK's ""HaplotypeCaller"". Can we trust it? Has anybody done more comprehensive testing of this? .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/115
https://github.com/google/deepvariant/issues/115:455,testability,test,testing,455,"Sort of a follow-up question, on a related application: can DeepVariant take an RNA-seq bam file obtained via GATK's best practices (link [here](https://software.broadinstitute.org/gatk/documentation/article.php?id=3891)), and get a trustworthy output? . A quick check on a specific locus indicates that DeepVariant's WGS applied to RNA-seq retrieves a few more variants than GATK's ""HaplotypeCaller"". Can we trust it? Has anybody done more comprehensive testing of this? .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/115
https://github.com/google/deepvariant/issues/115:186,usability,document,documentation,186,"Sort of a follow-up question, on a related application: can DeepVariant take an RNA-seq bam file obtained via GATK's best practices (link [here](https://software.broadinstitute.org/gatk/documentation/article.php?id=3891)), and get a trustworthy output? . A quick check on a specific locus indicates that DeepVariant's WGS applied to RNA-seq retrieves a few more variants than GATK's ""HaplotypeCaller"". Can we trust it? Has anybody done more comprehensive testing of this? .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/115
https://github.com/google/deepvariant/issues/115:289,usability,indicat,indicates,289,"Sort of a follow-up question, on a related application: can DeepVariant take an RNA-seq bam file obtained via GATK's best practices (link [here](https://software.broadinstitute.org/gatk/documentation/article.php?id=3891)), and get a trustworthy output? . A quick check on a specific locus indicates that DeepVariant's WGS applied to RNA-seq retrieves a few more variants than GATK's ""HaplotypeCaller"". Can we trust it? Has anybody done more comprehensive testing of this? .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/115
https://github.com/google/deepvariant/issues/115:70,interoperability,specif,specifically,70,"Adding @AndrewCarroll to see if he has more thoughts. I suspect if we specifically train an rna seq data with the same underlying algorithm, it will work well. However, we have not yet done anything on RNA data like I mentioned in my previous comment.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/115
https://github.com/google/deepvariant/issues/115:518,deployability,observ,observe,518,"Hi @AldoCP,. We have not comprehensively investigated performance on RNA-Seq samples yet. Because some components of the RNA-Seq problem more closely resemble variant calling in DNA-Seq exomes, I recommend that you use the exome model instead of the WGS one. A proper investigation would probably involve looking at the variant calls that result from the same sample with both DNA-Seq and RNA-Seq and investigating the concordance of calls. I expect that sort of comparison is probably worth a paper. . In general, we observe that when DeepVariant is applied in other domains beyond its training data, it tends to undercall as opposed to call false positive events, so if you are getting calls in DeepVariant and not HaplotypeCaller, those are worth looking into. If you do so, we would be interested in your feedback.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/115
https://github.com/google/deepvariant/issues/115:229,energy efficiency,model,model,229,"Hi @AldoCP,. We have not comprehensively investigated performance on RNA-Seq samples yet. Because some components of the RNA-Seq problem more closely resemble variant calling in DNA-Seq exomes, I recommend that you use the exome model instead of the WGS one. A proper investigation would probably involve looking at the variant calls that result from the same sample with both DNA-Seq and RNA-Seq and investigating the concordance of calls. I expect that sort of comparison is probably worth a paper. . In general, we observe that when DeepVariant is applied in other domains beyond its training data, it tends to undercall as opposed to call false positive events, so if you are getting calls in DeepVariant and not HaplotypeCaller, those are worth looking into. If you do so, we would be interested in your feedback.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/115
https://github.com/google/deepvariant/issues/115:103,integrability,compon,components,103,"Hi @AldoCP,. We have not comprehensively investigated performance on RNA-Seq samples yet. Because some components of the RNA-Seq problem more closely resemble variant calling in DNA-Seq exomes, I recommend that you use the exome model instead of the WGS one. A proper investigation would probably involve looking at the variant calls that result from the same sample with both DNA-Seq and RNA-Seq and investigating the concordance of calls. I expect that sort of comparison is probably worth a paper. . In general, we observe that when DeepVariant is applied in other domains beyond its training data, it tends to undercall as opposed to call false positive events, so if you are getting calls in DeepVariant and not HaplotypeCaller, those are worth looking into. If you do so, we would be interested in your feedback.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/115
https://github.com/google/deepvariant/issues/115:658,integrability,event,events,658,"Hi @AldoCP,. We have not comprehensively investigated performance on RNA-Seq samples yet. Because some components of the RNA-Seq problem more closely resemble variant calling in DNA-Seq exomes, I recommend that you use the exome model instead of the WGS one. A proper investigation would probably involve looking at the variant calls that result from the same sample with both DNA-Seq and RNA-Seq and investigating the concordance of calls. I expect that sort of comparison is probably worth a paper. . In general, we observe that when DeepVariant is applied in other domains beyond its training data, it tends to undercall as opposed to call false positive events, so if you are getting calls in DeepVariant and not HaplotypeCaller, those are worth looking into. If you do so, we would be interested in your feedback.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/115
https://github.com/google/deepvariant/issues/115:103,interoperability,compon,components,103,"Hi @AldoCP,. We have not comprehensively investigated performance on RNA-Seq samples yet. Because some components of the RNA-Seq problem more closely resemble variant calling in DNA-Seq exomes, I recommend that you use the exome model instead of the WGS one. A proper investigation would probably involve looking at the variant calls that result from the same sample with both DNA-Seq and RNA-Seq and investigating the concordance of calls. I expect that sort of comparison is probably worth a paper. . In general, we observe that when DeepVariant is applied in other domains beyond its training data, it tends to undercall as opposed to call false positive events, so if you are getting calls in DeepVariant and not HaplotypeCaller, those are worth looking into. If you do so, we would be interested in your feedback.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/115
https://github.com/google/deepvariant/issues/115:103,modifiability,compon,components,103,"Hi @AldoCP,. We have not comprehensively investigated performance on RNA-Seq samples yet. Because some components of the RNA-Seq problem more closely resemble variant calling in DNA-Seq exomes, I recommend that you use the exome model instead of the WGS one. A proper investigation would probably involve looking at the variant calls that result from the same sample with both DNA-Seq and RNA-Seq and investigating the concordance of calls. I expect that sort of comparison is probably worth a paper. . In general, we observe that when DeepVariant is applied in other domains beyond its training data, it tends to undercall as opposed to call false positive events, so if you are getting calls in DeepVariant and not HaplotypeCaller, those are worth looking into. If you do so, we would be interested in your feedback.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/115
https://github.com/google/deepvariant/issues/115:54,performance,perform,performance,54,"Hi @AldoCP,. We have not comprehensively investigated performance on RNA-Seq samples yet. Because some components of the RNA-Seq problem more closely resemble variant calling in DNA-Seq exomes, I recommend that you use the exome model instead of the WGS one. A proper investigation would probably involve looking at the variant calls that result from the same sample with both DNA-Seq and RNA-Seq and investigating the concordance of calls. I expect that sort of comparison is probably worth a paper. . In general, we observe that when DeepVariant is applied in other domains beyond its training data, it tends to undercall as opposed to call false positive events, so if you are getting calls in DeepVariant and not HaplotypeCaller, those are worth looking into. If you do so, we would be interested in your feedback.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/115
https://github.com/google/deepvariant/issues/115:229,security,model,model,229,"Hi @AldoCP,. We have not comprehensively investigated performance on RNA-Seq samples yet. Because some components of the RNA-Seq problem more closely resemble variant calling in DNA-Seq exomes, I recommend that you use the exome model instead of the WGS one. A proper investigation would probably involve looking at the variant calls that result from the same sample with both DNA-Seq and RNA-Seq and investigating the concordance of calls. I expect that sort of comparison is probably worth a paper. . In general, we observe that when DeepVariant is applied in other domains beyond its training data, it tends to undercall as opposed to call false positive events, so if you are getting calls in DeepVariant and not HaplotypeCaller, those are worth looking into. If you do so, we would be interested in your feedback.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/115
https://github.com/google/deepvariant/issues/115:518,testability,observ,observe,518,"Hi @AldoCP,. We have not comprehensively investigated performance on RNA-Seq samples yet. Because some components of the RNA-Seq problem more closely resemble variant calling in DNA-Seq exomes, I recommend that you use the exome model instead of the WGS one. A proper investigation would probably involve looking at the variant calls that result from the same sample with both DNA-Seq and RNA-Seq and investigating the concordance of calls. I expect that sort of comparison is probably worth a paper. . In general, we observe that when DeepVariant is applied in other domains beyond its training data, it tends to undercall as opposed to call false positive events, so if you are getting calls in DeepVariant and not HaplotypeCaller, those are worth looking into. If you do so, we would be interested in your feedback.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/115
https://github.com/google/deepvariant/issues/115:54,usability,perform,performance,54,"Hi @AldoCP,. We have not comprehensively investigated performance on RNA-Seq samples yet. Because some components of the RNA-Seq problem more closely resemble variant calling in DNA-Seq exomes, I recommend that you use the exome model instead of the WGS one. A proper investigation would probably involve looking at the variant calls that result from the same sample with both DNA-Seq and RNA-Seq and investigating the concordance of calls. I expect that sort of comparison is probably worth a paper. . In general, we observe that when DeepVariant is applied in other domains beyond its training data, it tends to undercall as opposed to call false positive events, so if you are getting calls in DeepVariant and not HaplotypeCaller, those are worth looking into. If you do so, we would be interested in your feedback.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/115
https://github.com/google/deepvariant/issues/115:142,usability,close,closely,142,"Hi @AldoCP,. We have not comprehensively investigated performance on RNA-Seq samples yet. Because some components of the RNA-Seq problem more closely resemble variant calling in DNA-Seq exomes, I recommend that you use the exome model instead of the WGS one. A proper investigation would probably involve looking at the variant calls that result from the same sample with both DNA-Seq and RNA-Seq and investigating the concordance of calls. I expect that sort of comparison is probably worth a paper. . In general, we observe that when DeepVariant is applied in other domains beyond its training data, it tends to undercall as opposed to call false positive events, so if you are getting calls in DeepVariant and not HaplotypeCaller, those are worth looking into. If you do so, we would be interested in your feedback.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/115
https://github.com/google/deepvariant/issues/115:809,usability,feedback,feedback,809,"Hi @AldoCP,. We have not comprehensively investigated performance on RNA-Seq samples yet. Because some components of the RNA-Seq problem more closely resemble variant calling in DNA-Seq exomes, I recommend that you use the exome model instead of the WGS one. A proper investigation would probably involve looking at the variant calls that result from the same sample with both DNA-Seq and RNA-Seq and investigating the concordance of calls. I expect that sort of comparison is probably worth a paper. . In general, we observe that when DeepVariant is applied in other domains beyond its training data, it tends to undercall as opposed to call false positive events, so if you are getting calls in DeepVariant and not HaplotypeCaller, those are worth looking into. If you do so, we would be interested in your feedback.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/115
https://github.com/google/deepvariant/issues/115:262,energy efficiency,model,model,262,"Thanks for the feedback. I went back to my files and just realized that my previous comment was inaccurate: the locus I analyzed on RNASeq was ""chr20:10,000,000-10,040,000""; the same exonic variant (chr20:10019093) was detected by both GATK and DeepVariant (WGS model) in my sample. As mentioned, I didn't do extensive tests at all (it was just that one locus) -- I'm happy to do further analysis if relevant,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/115
https://github.com/google/deepvariant/issues/115:309,modifiability,extens,extensive,309,"Thanks for the feedback. I went back to my files and just realized that my previous comment was inaccurate: the locus I analyzed on RNASeq was ""chr20:10,000,000-10,040,000""; the same exonic variant (chr20:10019093) was detected by both GATK and DeepVariant (WGS model) in my sample. As mentioned, I didn't do extensive tests at all (it was just that one locus) -- I'm happy to do further analysis if relevant,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/115
https://github.com/google/deepvariant/issues/115:219,safety,detect,detected,219,"Thanks for the feedback. I went back to my files and just realized that my previous comment was inaccurate: the locus I analyzed on RNASeq was ""chr20:10,000,000-10,040,000""; the same exonic variant (chr20:10019093) was detected by both GATK and DeepVariant (WGS model) in my sample. As mentioned, I didn't do extensive tests at all (it was just that one locus) -- I'm happy to do further analysis if relevant,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/115
https://github.com/google/deepvariant/issues/115:319,safety,test,tests,319,"Thanks for the feedback. I went back to my files and just realized that my previous comment was inaccurate: the locus I analyzed on RNASeq was ""chr20:10,000,000-10,040,000""; the same exonic variant (chr20:10019093) was detected by both GATK and DeepVariant (WGS model) in my sample. As mentioned, I didn't do extensive tests at all (it was just that one locus) -- I'm happy to do further analysis if relevant,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/115
https://github.com/google/deepvariant/issues/115:219,security,detect,detected,219,"Thanks for the feedback. I went back to my files and just realized that my previous comment was inaccurate: the locus I analyzed on RNASeq was ""chr20:10,000,000-10,040,000""; the same exonic variant (chr20:10019093) was detected by both GATK and DeepVariant (WGS model) in my sample. As mentioned, I didn't do extensive tests at all (it was just that one locus) -- I'm happy to do further analysis if relevant,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/115
https://github.com/google/deepvariant/issues/115:262,security,model,model,262,"Thanks for the feedback. I went back to my files and just realized that my previous comment was inaccurate: the locus I analyzed on RNASeq was ""chr20:10,000,000-10,040,000""; the same exonic variant (chr20:10019093) was detected by both GATK and DeepVariant (WGS model) in my sample. As mentioned, I didn't do extensive tests at all (it was just that one locus) -- I'm happy to do further analysis if relevant,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/115
https://github.com/google/deepvariant/issues/115:319,testability,test,tests,319,"Thanks for the feedback. I went back to my files and just realized that my previous comment was inaccurate: the locus I analyzed on RNASeq was ""chr20:10,000,000-10,040,000""; the same exonic variant (chr20:10019093) was detected by both GATK and DeepVariant (WGS model) in my sample. As mentioned, I didn't do extensive tests at all (it was just that one locus) -- I'm happy to do further analysis if relevant,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/115
https://github.com/google/deepvariant/issues/115:15,usability,feedback,feedback,15,"Thanks for the feedback. I went back to my files and just realized that my previous comment was inaccurate: the locus I analyzed on RNASeq was ""chr20:10,000,000-10,040,000""; the same exonic variant (chr20:10019093) was detected by both GATK and DeepVariant (WGS model) in my sample. As mentioned, I didn't do extensive tests at all (it was just that one locus) -- I'm happy to do further analysis if relevant,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/115
https://github.com/google/deepvariant/issues/115:70,deployability,updat,update,70,"@AldoCP @melkerdawy although this was closed some time ago, I have an update related to this question. We have successfully trained and released an Illumina RNA-seq model and [case study](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md). . DeepVariant now has a flag (`--split_skip_reads`) that can be used in conjunction with training or inference with RNA-seq data. The flag is required to efficiently process RNA-seq data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/115
https://github.com/google/deepvariant/issues/115:136,deployability,releas,released,136,"@AldoCP @melkerdawy although this was closed some time ago, I have an update related to this question. We have successfully trained and released an Illumina RNA-seq model and [case study](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md). . DeepVariant now has a flag (`--split_skip_reads`) that can be used in conjunction with training or inference with RNA-seq data. The flag is required to efficiently process RNA-seq data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/115
https://github.com/google/deepvariant/issues/115:165,energy efficiency,model,model,165,"@AldoCP @melkerdawy although this was closed some time ago, I have an update related to this question. We have successfully trained and released an Illumina RNA-seq model and [case study](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md). . DeepVariant now has a flag (`--split_skip_reads`) that can be used in conjunction with training or inference with RNA-seq data. The flag is required to efficiently process RNA-seq data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/115
https://github.com/google/deepvariant/issues/115:50,performance,time,time,50,"@AldoCP @melkerdawy although this was closed some time ago, I have an update related to this question. We have successfully trained and released an Illumina RNA-seq model and [case study](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md). . DeepVariant now has a flag (`--split_skip_reads`) that can be used in conjunction with training or inference with RNA-seq data. The flag is required to efficiently process RNA-seq data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/115
https://github.com/google/deepvariant/issues/115:70,safety,updat,update,70,"@AldoCP @melkerdawy although this was closed some time ago, I have an update related to this question. We have successfully trained and released an Illumina RNA-seq model and [case study](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md). . DeepVariant now has a flag (`--split_skip_reads`) that can be used in conjunction with training or inference with RNA-seq data. The flag is required to efficiently process RNA-seq data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/115
https://github.com/google/deepvariant/issues/115:70,security,updat,update,70,"@AldoCP @melkerdawy although this was closed some time ago, I have an update related to this question. We have successfully trained and released an Illumina RNA-seq model and [case study](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md). . DeepVariant now has a flag (`--split_skip_reads`) that can be used in conjunction with training or inference with RNA-seq data. The flag is required to efficiently process RNA-seq data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/115
https://github.com/google/deepvariant/issues/115:165,security,model,model,165,"@AldoCP @melkerdawy although this was closed some time ago, I have an update related to this question. We have successfully trained and released an Illumina RNA-seq model and [case study](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md). . DeepVariant now has a flag (`--split_skip_reads`) that can be used in conjunction with training or inference with RNA-seq data. The flag is required to efficiently process RNA-seq data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/115
https://github.com/google/deepvariant/issues/115:38,usability,close,closed,38,"@AldoCP @melkerdawy although this was closed some time ago, I have an update related to this question. We have successfully trained and released an Illumina RNA-seq model and [case study](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md). . DeepVariant now has a flag (`--split_skip_reads`) that can be used in conjunction with training or inference with RNA-seq data. The flag is required to efficiently process RNA-seq data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/115
https://github.com/google/deepvariant/issues/115:430,usability,efficien,efficiently,430,"@AldoCP @melkerdawy although this was closed some time ago, I have an update related to this question. We have successfully trained and released an Illumina RNA-seq model and [case study](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md). . DeepVariant now has a flag (`--split_skip_reads`) that can be used in conjunction with training or inference with RNA-seq data. The flag is required to efficiently process RNA-seq data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/115
https://github.com/google/deepvariant/issues/116:309,availability,error,error,309,"Hello Salik,. I am sorry you are encountering issues with the Cloud Runner. I can't definitively tell you what has gone wrong, but I can spot a few things that will be issues (there could also be others). First, I think the log snippet here comes from the main log of the cloud runner. The underlying program error can often be found in another folder of the same run. You should see a ""logs"" folder and this should contain a program-specific log (like make_examples.log). This can be more informative. From your run, I can see that you provide your FASTA as an uncompressed .FA file. I believe that the cloud runner requires a BGZIP compressed reference, a samtools FAIDX, and a GZI index all in the same place. You will need to do the following operations on the FASTA file in a bucket that you control:. bgzip -i GRCh38_Verily_v1.genome.fa. samtools faidx GRCh38_Verily_v1.genome.fa. afterward, you will need to put the resulting:. GRCh38_Verily_v1.genome.fa.gz. GRCh38_Verily_v1.genome.fa.gz.gzi. GRCh38_Verily_v1.genome.fa.gz.fai. in the same bucket that the cloud runner can access. . Hopefully these instructions seem reasonable and this unblocks you from this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:747,availability,operat,operations,747,"Hello Salik,. I am sorry you are encountering issues with the Cloud Runner. I can't definitively tell you what has gone wrong, but I can spot a few things that will be issues (there could also be others). First, I think the log snippet here comes from the main log of the cloud runner. The underlying program error can often be found in another folder of the same run. You should see a ""logs"" folder and this should contain a program-specific log (like make_examples.log). This can be more informative. From your run, I can see that you provide your FASTA as an uncompressed .FA file. I believe that the cloud runner requires a BGZIP compressed reference, a samtools FAIDX, and a GZI index all in the same place. You will need to do the following operations on the FASTA file in a bucket that you control:. bgzip -i GRCh38_Verily_v1.genome.fa. samtools faidx GRCh38_Verily_v1.genome.fa. afterward, you will need to put the resulting:. GRCh38_Verily_v1.genome.fa.gz. GRCh38_Verily_v1.genome.fa.gz.gzi. GRCh38_Verily_v1.genome.fa.gz.fai. in the same bucket that the cloud runner can access. . Hopefully these instructions seem reasonable and this unblocks you from this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:224,deployability,log,log,224,"Hello Salik,. I am sorry you are encountering issues with the Cloud Runner. I can't definitively tell you what has gone wrong, but I can spot a few things that will be issues (there could also be others). First, I think the log snippet here comes from the main log of the cloud runner. The underlying program error can often be found in another folder of the same run. You should see a ""logs"" folder and this should contain a program-specific log (like make_examples.log). This can be more informative. From your run, I can see that you provide your FASTA as an uncompressed .FA file. I believe that the cloud runner requires a BGZIP compressed reference, a samtools FAIDX, and a GZI index all in the same place. You will need to do the following operations on the FASTA file in a bucket that you control:. bgzip -i GRCh38_Verily_v1.genome.fa. samtools faidx GRCh38_Verily_v1.genome.fa. afterward, you will need to put the resulting:. GRCh38_Verily_v1.genome.fa.gz. GRCh38_Verily_v1.genome.fa.gz.gzi. GRCh38_Verily_v1.genome.fa.gz.fai. in the same bucket that the cloud runner can access. . Hopefully these instructions seem reasonable and this unblocks you from this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:261,deployability,log,log,261,"Hello Salik,. I am sorry you are encountering issues with the Cloud Runner. I can't definitively tell you what has gone wrong, but I can spot a few things that will be issues (there could also be others). First, I think the log snippet here comes from the main log of the cloud runner. The underlying program error can often be found in another folder of the same run. You should see a ""logs"" folder and this should contain a program-specific log (like make_examples.log). This can be more informative. From your run, I can see that you provide your FASTA as an uncompressed .FA file. I believe that the cloud runner requires a BGZIP compressed reference, a samtools FAIDX, and a GZI index all in the same place. You will need to do the following operations on the FASTA file in a bucket that you control:. bgzip -i GRCh38_Verily_v1.genome.fa. samtools faidx GRCh38_Verily_v1.genome.fa. afterward, you will need to put the resulting:. GRCh38_Verily_v1.genome.fa.gz. GRCh38_Verily_v1.genome.fa.gz.gzi. GRCh38_Verily_v1.genome.fa.gz.fai. in the same bucket that the cloud runner can access. . Hopefully these instructions seem reasonable and this unblocks you from this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:387,deployability,log,logs,387,"Hello Salik,. I am sorry you are encountering issues with the Cloud Runner. I can't definitively tell you what has gone wrong, but I can spot a few things that will be issues (there could also be others). First, I think the log snippet here comes from the main log of the cloud runner. The underlying program error can often be found in another folder of the same run. You should see a ""logs"" folder and this should contain a program-specific log (like make_examples.log). This can be more informative. From your run, I can see that you provide your FASTA as an uncompressed .FA file. I believe that the cloud runner requires a BGZIP compressed reference, a samtools FAIDX, and a GZI index all in the same place. You will need to do the following operations on the FASTA file in a bucket that you control:. bgzip -i GRCh38_Verily_v1.genome.fa. samtools faidx GRCh38_Verily_v1.genome.fa. afterward, you will need to put the resulting:. GRCh38_Verily_v1.genome.fa.gz. GRCh38_Verily_v1.genome.fa.gz.gzi. GRCh38_Verily_v1.genome.fa.gz.fai. in the same bucket that the cloud runner can access. . Hopefully these instructions seem reasonable and this unblocks you from this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:416,deployability,contain,contain,416,"Hello Salik,. I am sorry you are encountering issues with the Cloud Runner. I can't definitively tell you what has gone wrong, but I can spot a few things that will be issues (there could also be others). First, I think the log snippet here comes from the main log of the cloud runner. The underlying program error can often be found in another folder of the same run. You should see a ""logs"" folder and this should contain a program-specific log (like make_examples.log). This can be more informative. From your run, I can see that you provide your FASTA as an uncompressed .FA file. I believe that the cloud runner requires a BGZIP compressed reference, a samtools FAIDX, and a GZI index all in the same place. You will need to do the following operations on the FASTA file in a bucket that you control:. bgzip -i GRCh38_Verily_v1.genome.fa. samtools faidx GRCh38_Verily_v1.genome.fa. afterward, you will need to put the resulting:. GRCh38_Verily_v1.genome.fa.gz. GRCh38_Verily_v1.genome.fa.gz.gzi. GRCh38_Verily_v1.genome.fa.gz.fai. in the same bucket that the cloud runner can access. . Hopefully these instructions seem reasonable and this unblocks you from this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:443,deployability,log,log,443,"Hello Salik,. I am sorry you are encountering issues with the Cloud Runner. I can't definitively tell you what has gone wrong, but I can spot a few things that will be issues (there could also be others). First, I think the log snippet here comes from the main log of the cloud runner. The underlying program error can often be found in another folder of the same run. You should see a ""logs"" folder and this should contain a program-specific log (like make_examples.log). This can be more informative. From your run, I can see that you provide your FASTA as an uncompressed .FA file. I believe that the cloud runner requires a BGZIP compressed reference, a samtools FAIDX, and a GZI index all in the same place. You will need to do the following operations on the FASTA file in a bucket that you control:. bgzip -i GRCh38_Verily_v1.genome.fa. samtools faidx GRCh38_Verily_v1.genome.fa. afterward, you will need to put the resulting:. GRCh38_Verily_v1.genome.fa.gz. GRCh38_Verily_v1.genome.fa.gz.gzi. GRCh38_Verily_v1.genome.fa.gz.fai. in the same bucket that the cloud runner can access. . Hopefully these instructions seem reasonable and this unblocks you from this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:467,deployability,log,log,467,"Hello Salik,. I am sorry you are encountering issues with the Cloud Runner. I can't definitively tell you what has gone wrong, but I can spot a few things that will be issues (there could also be others). First, I think the log snippet here comes from the main log of the cloud runner. The underlying program error can often be found in another folder of the same run. You should see a ""logs"" folder and this should contain a program-specific log (like make_examples.log). This can be more informative. From your run, I can see that you provide your FASTA as an uncompressed .FA file. I believe that the cloud runner requires a BGZIP compressed reference, a samtools FAIDX, and a GZI index all in the same place. You will need to do the following operations on the FASTA file in a bucket that you control:. bgzip -i GRCh38_Verily_v1.genome.fa. samtools faidx GRCh38_Verily_v1.genome.fa. afterward, you will need to put the resulting:. GRCh38_Verily_v1.genome.fa.gz. GRCh38_Verily_v1.genome.fa.gz.gzi. GRCh38_Verily_v1.genome.fa.gz.fai. in the same bucket that the cloud runner can access. . Hopefully these instructions seem reasonable and this unblocks you from this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:62,energy efficiency,Cloud,Cloud,62,"Hello Salik,. I am sorry you are encountering issues with the Cloud Runner. I can't definitively tell you what has gone wrong, but I can spot a few things that will be issues (there could also be others). First, I think the log snippet here comes from the main log of the cloud runner. The underlying program error can often be found in another folder of the same run. You should see a ""logs"" folder and this should contain a program-specific log (like make_examples.log). This can be more informative. From your run, I can see that you provide your FASTA as an uncompressed .FA file. I believe that the cloud runner requires a BGZIP compressed reference, a samtools FAIDX, and a GZI index all in the same place. You will need to do the following operations on the FASTA file in a bucket that you control:. bgzip -i GRCh38_Verily_v1.genome.fa. samtools faidx GRCh38_Verily_v1.genome.fa. afterward, you will need to put the resulting:. GRCh38_Verily_v1.genome.fa.gz. GRCh38_Verily_v1.genome.fa.gz.gzi. GRCh38_Verily_v1.genome.fa.gz.fai. in the same bucket that the cloud runner can access. . Hopefully these instructions seem reasonable and this unblocks you from this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:272,energy efficiency,cloud,cloud,272,"Hello Salik,. I am sorry you are encountering issues with the Cloud Runner. I can't definitively tell you what has gone wrong, but I can spot a few things that will be issues (there could also be others). First, I think the log snippet here comes from the main log of the cloud runner. The underlying program error can often be found in another folder of the same run. You should see a ""logs"" folder and this should contain a program-specific log (like make_examples.log). This can be more informative. From your run, I can see that you provide your FASTA as an uncompressed .FA file. I believe that the cloud runner requires a BGZIP compressed reference, a samtools FAIDX, and a GZI index all in the same place. You will need to do the following operations on the FASTA file in a bucket that you control:. bgzip -i GRCh38_Verily_v1.genome.fa. samtools faidx GRCh38_Verily_v1.genome.fa. afterward, you will need to put the resulting:. GRCh38_Verily_v1.genome.fa.gz. GRCh38_Verily_v1.genome.fa.gz.gzi. GRCh38_Verily_v1.genome.fa.gz.fai. in the same bucket that the cloud runner can access. . Hopefully these instructions seem reasonable and this unblocks you from this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:604,energy efficiency,cloud,cloud,604,"Hello Salik,. I am sorry you are encountering issues with the Cloud Runner. I can't definitively tell you what has gone wrong, but I can spot a few things that will be issues (there could also be others). First, I think the log snippet here comes from the main log of the cloud runner. The underlying program error can often be found in another folder of the same run. You should see a ""logs"" folder and this should contain a program-specific log (like make_examples.log). This can be more informative. From your run, I can see that you provide your FASTA as an uncompressed .FA file. I believe that the cloud runner requires a BGZIP compressed reference, a samtools FAIDX, and a GZI index all in the same place. You will need to do the following operations on the FASTA file in a bucket that you control:. bgzip -i GRCh38_Verily_v1.genome.fa. samtools faidx GRCh38_Verily_v1.genome.fa. afterward, you will need to put the resulting:. GRCh38_Verily_v1.genome.fa.gz. GRCh38_Verily_v1.genome.fa.gz.gzi. GRCh38_Verily_v1.genome.fa.gz.fai. in the same bucket that the cloud runner can access. . Hopefully these instructions seem reasonable and this unblocks you from this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:1064,energy efficiency,cloud,cloud,1064,"Hello Salik,. I am sorry you are encountering issues with the Cloud Runner. I can't definitively tell you what has gone wrong, but I can spot a few things that will be issues (there could also be others). First, I think the log snippet here comes from the main log of the cloud runner. The underlying program error can often be found in another folder of the same run. You should see a ""logs"" folder and this should contain a program-specific log (like make_examples.log). This can be more informative. From your run, I can see that you provide your FASTA as an uncompressed .FA file. I believe that the cloud runner requires a BGZIP compressed reference, a samtools FAIDX, and a GZI index all in the same place. You will need to do the following operations on the FASTA file in a bucket that you control:. bgzip -i GRCh38_Verily_v1.genome.fa. samtools faidx GRCh38_Verily_v1.genome.fa. afterward, you will need to put the resulting:. GRCh38_Verily_v1.genome.fa.gz. GRCh38_Verily_v1.genome.fa.gz.gzi. GRCh38_Verily_v1.genome.fa.gz.fai. in the same bucket that the cloud runner can access. . Hopefully these instructions seem reasonable and this unblocks you from this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:434,interoperability,specif,specific,434,"Hello Salik,. I am sorry you are encountering issues with the Cloud Runner. I can't definitively tell you what has gone wrong, but I can spot a few things that will be issues (there could also be others). First, I think the log snippet here comes from the main log of the cloud runner. The underlying program error can often be found in another folder of the same run. You should see a ""logs"" folder and this should contain a program-specific log (like make_examples.log). This can be more informative. From your run, I can see that you provide your FASTA as an uncompressed .FA file. I believe that the cloud runner requires a BGZIP compressed reference, a samtools FAIDX, and a GZI index all in the same place. You will need to do the following operations on the FASTA file in a bucket that you control:. bgzip -i GRCh38_Verily_v1.genome.fa. samtools faidx GRCh38_Verily_v1.genome.fa. afterward, you will need to put the resulting:. GRCh38_Verily_v1.genome.fa.gz. GRCh38_Verily_v1.genome.fa.gz.gzi. GRCh38_Verily_v1.genome.fa.gz.fai. in the same bucket that the cloud runner can access. . Hopefully these instructions seem reasonable and this unblocks you from this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:309,performance,error,error,309,"Hello Salik,. I am sorry you are encountering issues with the Cloud Runner. I can't definitively tell you what has gone wrong, but I can spot a few things that will be issues (there could also be others). First, I think the log snippet here comes from the main log of the cloud runner. The underlying program error can often be found in another folder of the same run. You should see a ""logs"" folder and this should contain a program-specific log (like make_examples.log). This can be more informative. From your run, I can see that you provide your FASTA as an uncompressed .FA file. I believe that the cloud runner requires a BGZIP compressed reference, a samtools FAIDX, and a GZI index all in the same place. You will need to do the following operations on the FASTA file in a bucket that you control:. bgzip -i GRCh38_Verily_v1.genome.fa. samtools faidx GRCh38_Verily_v1.genome.fa. afterward, you will need to put the resulting:. GRCh38_Verily_v1.genome.fa.gz. GRCh38_Verily_v1.genome.fa.gz.gzi. GRCh38_Verily_v1.genome.fa.gz.fai. in the same bucket that the cloud runner can access. . Hopefully these instructions seem reasonable and this unblocks you from this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:224,safety,log,log,224,"Hello Salik,. I am sorry you are encountering issues with the Cloud Runner. I can't definitively tell you what has gone wrong, but I can spot a few things that will be issues (there could also be others). First, I think the log snippet here comes from the main log of the cloud runner. The underlying program error can often be found in another folder of the same run. You should see a ""logs"" folder and this should contain a program-specific log (like make_examples.log). This can be more informative. From your run, I can see that you provide your FASTA as an uncompressed .FA file. I believe that the cloud runner requires a BGZIP compressed reference, a samtools FAIDX, and a GZI index all in the same place. You will need to do the following operations on the FASTA file in a bucket that you control:. bgzip -i GRCh38_Verily_v1.genome.fa. samtools faidx GRCh38_Verily_v1.genome.fa. afterward, you will need to put the resulting:. GRCh38_Verily_v1.genome.fa.gz. GRCh38_Verily_v1.genome.fa.gz.gzi. GRCh38_Verily_v1.genome.fa.gz.fai. in the same bucket that the cloud runner can access. . Hopefully these instructions seem reasonable and this unblocks you from this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:261,safety,log,log,261,"Hello Salik,. I am sorry you are encountering issues with the Cloud Runner. I can't definitively tell you what has gone wrong, but I can spot a few things that will be issues (there could also be others). First, I think the log snippet here comes from the main log of the cloud runner. The underlying program error can often be found in another folder of the same run. You should see a ""logs"" folder and this should contain a program-specific log (like make_examples.log). This can be more informative. From your run, I can see that you provide your FASTA as an uncompressed .FA file. I believe that the cloud runner requires a BGZIP compressed reference, a samtools FAIDX, and a GZI index all in the same place. You will need to do the following operations on the FASTA file in a bucket that you control:. bgzip -i GRCh38_Verily_v1.genome.fa. samtools faidx GRCh38_Verily_v1.genome.fa. afterward, you will need to put the resulting:. GRCh38_Verily_v1.genome.fa.gz. GRCh38_Verily_v1.genome.fa.gz.gzi. GRCh38_Verily_v1.genome.fa.gz.fai. in the same bucket that the cloud runner can access. . Hopefully these instructions seem reasonable and this unblocks you from this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:309,safety,error,error,309,"Hello Salik,. I am sorry you are encountering issues with the Cloud Runner. I can't definitively tell you what has gone wrong, but I can spot a few things that will be issues (there could also be others). First, I think the log snippet here comes from the main log of the cloud runner. The underlying program error can often be found in another folder of the same run. You should see a ""logs"" folder and this should contain a program-specific log (like make_examples.log). This can be more informative. From your run, I can see that you provide your FASTA as an uncompressed .FA file. I believe that the cloud runner requires a BGZIP compressed reference, a samtools FAIDX, and a GZI index all in the same place. You will need to do the following operations on the FASTA file in a bucket that you control:. bgzip -i GRCh38_Verily_v1.genome.fa. samtools faidx GRCh38_Verily_v1.genome.fa. afterward, you will need to put the resulting:. GRCh38_Verily_v1.genome.fa.gz. GRCh38_Verily_v1.genome.fa.gz.gzi. GRCh38_Verily_v1.genome.fa.gz.fai. in the same bucket that the cloud runner can access. . Hopefully these instructions seem reasonable and this unblocks you from this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:387,safety,log,logs,387,"Hello Salik,. I am sorry you are encountering issues with the Cloud Runner. I can't definitively tell you what has gone wrong, but I can spot a few things that will be issues (there could also be others). First, I think the log snippet here comes from the main log of the cloud runner. The underlying program error can often be found in another folder of the same run. You should see a ""logs"" folder and this should contain a program-specific log (like make_examples.log). This can be more informative. From your run, I can see that you provide your FASTA as an uncompressed .FA file. I believe that the cloud runner requires a BGZIP compressed reference, a samtools FAIDX, and a GZI index all in the same place. You will need to do the following operations on the FASTA file in a bucket that you control:. bgzip -i GRCh38_Verily_v1.genome.fa. samtools faidx GRCh38_Verily_v1.genome.fa. afterward, you will need to put the resulting:. GRCh38_Verily_v1.genome.fa.gz. GRCh38_Verily_v1.genome.fa.gz.gzi. GRCh38_Verily_v1.genome.fa.gz.fai. in the same bucket that the cloud runner can access. . Hopefully these instructions seem reasonable and this unblocks you from this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:443,safety,log,log,443,"Hello Salik,. I am sorry you are encountering issues with the Cloud Runner. I can't definitively tell you what has gone wrong, but I can spot a few things that will be issues (there could also be others). First, I think the log snippet here comes from the main log of the cloud runner. The underlying program error can often be found in another folder of the same run. You should see a ""logs"" folder and this should contain a program-specific log (like make_examples.log). This can be more informative. From your run, I can see that you provide your FASTA as an uncompressed .FA file. I believe that the cloud runner requires a BGZIP compressed reference, a samtools FAIDX, and a GZI index all in the same place. You will need to do the following operations on the FASTA file in a bucket that you control:. bgzip -i GRCh38_Verily_v1.genome.fa. samtools faidx GRCh38_Verily_v1.genome.fa. afterward, you will need to put the resulting:. GRCh38_Verily_v1.genome.fa.gz. GRCh38_Verily_v1.genome.fa.gz.gzi. GRCh38_Verily_v1.genome.fa.gz.fai. in the same bucket that the cloud runner can access. . Hopefully these instructions seem reasonable and this unblocks you from this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:467,safety,log,log,467,"Hello Salik,. I am sorry you are encountering issues with the Cloud Runner. I can't definitively tell you what has gone wrong, but I can spot a few things that will be issues (there could also be others). First, I think the log snippet here comes from the main log of the cloud runner. The underlying program error can often be found in another folder of the same run. You should see a ""logs"" folder and this should contain a program-specific log (like make_examples.log). This can be more informative. From your run, I can see that you provide your FASTA as an uncompressed .FA file. I believe that the cloud runner requires a BGZIP compressed reference, a samtools FAIDX, and a GZI index all in the same place. You will need to do the following operations on the FASTA file in a bucket that you control:. bgzip -i GRCh38_Verily_v1.genome.fa. samtools faidx GRCh38_Verily_v1.genome.fa. afterward, you will need to put the resulting:. GRCh38_Verily_v1.genome.fa.gz. GRCh38_Verily_v1.genome.fa.gz.gzi. GRCh38_Verily_v1.genome.fa.gz.fai. in the same bucket that the cloud runner can access. . Hopefully these instructions seem reasonable and this unblocks you from this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:224,security,log,log,224,"Hello Salik,. I am sorry you are encountering issues with the Cloud Runner. I can't definitively tell you what has gone wrong, but I can spot a few things that will be issues (there could also be others). First, I think the log snippet here comes from the main log of the cloud runner. The underlying program error can often be found in another folder of the same run. You should see a ""logs"" folder and this should contain a program-specific log (like make_examples.log). This can be more informative. From your run, I can see that you provide your FASTA as an uncompressed .FA file. I believe that the cloud runner requires a BGZIP compressed reference, a samtools FAIDX, and a GZI index all in the same place. You will need to do the following operations on the FASTA file in a bucket that you control:. bgzip -i GRCh38_Verily_v1.genome.fa. samtools faidx GRCh38_Verily_v1.genome.fa. afterward, you will need to put the resulting:. GRCh38_Verily_v1.genome.fa.gz. GRCh38_Verily_v1.genome.fa.gz.gzi. GRCh38_Verily_v1.genome.fa.gz.fai. in the same bucket that the cloud runner can access. . Hopefully these instructions seem reasonable and this unblocks you from this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:261,security,log,log,261,"Hello Salik,. I am sorry you are encountering issues with the Cloud Runner. I can't definitively tell you what has gone wrong, but I can spot a few things that will be issues (there could also be others). First, I think the log snippet here comes from the main log of the cloud runner. The underlying program error can often be found in another folder of the same run. You should see a ""logs"" folder and this should contain a program-specific log (like make_examples.log). This can be more informative. From your run, I can see that you provide your FASTA as an uncompressed .FA file. I believe that the cloud runner requires a BGZIP compressed reference, a samtools FAIDX, and a GZI index all in the same place. You will need to do the following operations on the FASTA file in a bucket that you control:. bgzip -i GRCh38_Verily_v1.genome.fa. samtools faidx GRCh38_Verily_v1.genome.fa. afterward, you will need to put the resulting:. GRCh38_Verily_v1.genome.fa.gz. GRCh38_Verily_v1.genome.fa.gz.gzi. GRCh38_Verily_v1.genome.fa.gz.fai. in the same bucket that the cloud runner can access. . Hopefully these instructions seem reasonable and this unblocks you from this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:387,security,log,logs,387,"Hello Salik,. I am sorry you are encountering issues with the Cloud Runner. I can't definitively tell you what has gone wrong, but I can spot a few things that will be issues (there could also be others). First, I think the log snippet here comes from the main log of the cloud runner. The underlying program error can often be found in another folder of the same run. You should see a ""logs"" folder and this should contain a program-specific log (like make_examples.log). This can be more informative. From your run, I can see that you provide your FASTA as an uncompressed .FA file. I believe that the cloud runner requires a BGZIP compressed reference, a samtools FAIDX, and a GZI index all in the same place. You will need to do the following operations on the FASTA file in a bucket that you control:. bgzip -i GRCh38_Verily_v1.genome.fa. samtools faidx GRCh38_Verily_v1.genome.fa. afterward, you will need to put the resulting:. GRCh38_Verily_v1.genome.fa.gz. GRCh38_Verily_v1.genome.fa.gz.gzi. GRCh38_Verily_v1.genome.fa.gz.fai. in the same bucket that the cloud runner can access. . Hopefully these instructions seem reasonable and this unblocks you from this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:443,security,log,log,443,"Hello Salik,. I am sorry you are encountering issues with the Cloud Runner. I can't definitively tell you what has gone wrong, but I can spot a few things that will be issues (there could also be others). First, I think the log snippet here comes from the main log of the cloud runner. The underlying program error can often be found in another folder of the same run. You should see a ""logs"" folder and this should contain a program-specific log (like make_examples.log). This can be more informative. From your run, I can see that you provide your FASTA as an uncompressed .FA file. I believe that the cloud runner requires a BGZIP compressed reference, a samtools FAIDX, and a GZI index all in the same place. You will need to do the following operations on the FASTA file in a bucket that you control:. bgzip -i GRCh38_Verily_v1.genome.fa. samtools faidx GRCh38_Verily_v1.genome.fa. afterward, you will need to put the resulting:. GRCh38_Verily_v1.genome.fa.gz. GRCh38_Verily_v1.genome.fa.gz.gzi. GRCh38_Verily_v1.genome.fa.gz.fai. in the same bucket that the cloud runner can access. . Hopefully these instructions seem reasonable and this unblocks you from this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:467,security,log,log,467,"Hello Salik,. I am sorry you are encountering issues with the Cloud Runner. I can't definitively tell you what has gone wrong, but I can spot a few things that will be issues (there could also be others). First, I think the log snippet here comes from the main log of the cloud runner. The underlying program error can often be found in another folder of the same run. You should see a ""logs"" folder and this should contain a program-specific log (like make_examples.log). This can be more informative. From your run, I can see that you provide your FASTA as an uncompressed .FA file. I believe that the cloud runner requires a BGZIP compressed reference, a samtools FAIDX, and a GZI index all in the same place. You will need to do the following operations on the FASTA file in a bucket that you control:. bgzip -i GRCh38_Verily_v1.genome.fa. samtools faidx GRCh38_Verily_v1.genome.fa. afterward, you will need to put the resulting:. GRCh38_Verily_v1.genome.fa.gz. GRCh38_Verily_v1.genome.fa.gz.gzi. GRCh38_Verily_v1.genome.fa.gz.fai. in the same bucket that the cloud runner can access. . Hopefully these instructions seem reasonable and this unblocks you from this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:797,security,control,control,797,"Hello Salik,. I am sorry you are encountering issues with the Cloud Runner. I can't definitively tell you what has gone wrong, but I can spot a few things that will be issues (there could also be others). First, I think the log snippet here comes from the main log of the cloud runner. The underlying program error can often be found in another folder of the same run. You should see a ""logs"" folder and this should contain a program-specific log (like make_examples.log). This can be more informative. From your run, I can see that you provide your FASTA as an uncompressed .FA file. I believe that the cloud runner requires a BGZIP compressed reference, a samtools FAIDX, and a GZI index all in the same place. You will need to do the following operations on the FASTA file in a bucket that you control:. bgzip -i GRCh38_Verily_v1.genome.fa. samtools faidx GRCh38_Verily_v1.genome.fa. afterward, you will need to put the resulting:. GRCh38_Verily_v1.genome.fa.gz. GRCh38_Verily_v1.genome.fa.gz.gzi. GRCh38_Verily_v1.genome.fa.gz.fai. in the same bucket that the cloud runner can access. . Hopefully these instructions seem reasonable and this unblocks you from this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:1081,security,access,access,1081,"Hello Salik,. I am sorry you are encountering issues with the Cloud Runner. I can't definitively tell you what has gone wrong, but I can spot a few things that will be issues (there could also be others). First, I think the log snippet here comes from the main log of the cloud runner. The underlying program error can often be found in another folder of the same run. You should see a ""logs"" folder and this should contain a program-specific log (like make_examples.log). This can be more informative. From your run, I can see that you provide your FASTA as an uncompressed .FA file. I believe that the cloud runner requires a BGZIP compressed reference, a samtools FAIDX, and a GZI index all in the same place. You will need to do the following operations on the FASTA file in a bucket that you control:. bgzip -i GRCh38_Verily_v1.genome.fa. samtools faidx GRCh38_Verily_v1.genome.fa. afterward, you will need to put the resulting:. GRCh38_Verily_v1.genome.fa.gz. GRCh38_Verily_v1.genome.fa.gz.gzi. GRCh38_Verily_v1.genome.fa.gz.fai. in the same bucket that the cloud runner can access. . Hopefully these instructions seem reasonable and this unblocks you from this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:224,testability,log,log,224,"Hello Salik,. I am sorry you are encountering issues with the Cloud Runner. I can't definitively tell you what has gone wrong, but I can spot a few things that will be issues (there could also be others). First, I think the log snippet here comes from the main log of the cloud runner. The underlying program error can often be found in another folder of the same run. You should see a ""logs"" folder and this should contain a program-specific log (like make_examples.log). This can be more informative. From your run, I can see that you provide your FASTA as an uncompressed .FA file. I believe that the cloud runner requires a BGZIP compressed reference, a samtools FAIDX, and a GZI index all in the same place. You will need to do the following operations on the FASTA file in a bucket that you control:. bgzip -i GRCh38_Verily_v1.genome.fa. samtools faidx GRCh38_Verily_v1.genome.fa. afterward, you will need to put the resulting:. GRCh38_Verily_v1.genome.fa.gz. GRCh38_Verily_v1.genome.fa.gz.gzi. GRCh38_Verily_v1.genome.fa.gz.fai. in the same bucket that the cloud runner can access. . Hopefully these instructions seem reasonable and this unblocks you from this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:261,testability,log,log,261,"Hello Salik,. I am sorry you are encountering issues with the Cloud Runner. I can't definitively tell you what has gone wrong, but I can spot a few things that will be issues (there could also be others). First, I think the log snippet here comes from the main log of the cloud runner. The underlying program error can often be found in another folder of the same run. You should see a ""logs"" folder and this should contain a program-specific log (like make_examples.log). This can be more informative. From your run, I can see that you provide your FASTA as an uncompressed .FA file. I believe that the cloud runner requires a BGZIP compressed reference, a samtools FAIDX, and a GZI index all in the same place. You will need to do the following operations on the FASTA file in a bucket that you control:. bgzip -i GRCh38_Verily_v1.genome.fa. samtools faidx GRCh38_Verily_v1.genome.fa. afterward, you will need to put the resulting:. GRCh38_Verily_v1.genome.fa.gz. GRCh38_Verily_v1.genome.fa.gz.gzi. GRCh38_Verily_v1.genome.fa.gz.fai. in the same bucket that the cloud runner can access. . Hopefully these instructions seem reasonable and this unblocks you from this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:387,testability,log,logs,387,"Hello Salik,. I am sorry you are encountering issues with the Cloud Runner. I can't definitively tell you what has gone wrong, but I can spot a few things that will be issues (there could also be others). First, I think the log snippet here comes from the main log of the cloud runner. The underlying program error can often be found in another folder of the same run. You should see a ""logs"" folder and this should contain a program-specific log (like make_examples.log). This can be more informative. From your run, I can see that you provide your FASTA as an uncompressed .FA file. I believe that the cloud runner requires a BGZIP compressed reference, a samtools FAIDX, and a GZI index all in the same place. You will need to do the following operations on the FASTA file in a bucket that you control:. bgzip -i GRCh38_Verily_v1.genome.fa. samtools faidx GRCh38_Verily_v1.genome.fa. afterward, you will need to put the resulting:. GRCh38_Verily_v1.genome.fa.gz. GRCh38_Verily_v1.genome.fa.gz.gzi. GRCh38_Verily_v1.genome.fa.gz.fai. in the same bucket that the cloud runner can access. . Hopefully these instructions seem reasonable and this unblocks you from this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:443,testability,log,log,443,"Hello Salik,. I am sorry you are encountering issues with the Cloud Runner. I can't definitively tell you what has gone wrong, but I can spot a few things that will be issues (there could also be others). First, I think the log snippet here comes from the main log of the cloud runner. The underlying program error can often be found in another folder of the same run. You should see a ""logs"" folder and this should contain a program-specific log (like make_examples.log). This can be more informative. From your run, I can see that you provide your FASTA as an uncompressed .FA file. I believe that the cloud runner requires a BGZIP compressed reference, a samtools FAIDX, and a GZI index all in the same place. You will need to do the following operations on the FASTA file in a bucket that you control:. bgzip -i GRCh38_Verily_v1.genome.fa. samtools faidx GRCh38_Verily_v1.genome.fa. afterward, you will need to put the resulting:. GRCh38_Verily_v1.genome.fa.gz. GRCh38_Verily_v1.genome.fa.gz.gzi. GRCh38_Verily_v1.genome.fa.gz.fai. in the same bucket that the cloud runner can access. . Hopefully these instructions seem reasonable and this unblocks you from this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:467,testability,log,log,467,"Hello Salik,. I am sorry you are encountering issues with the Cloud Runner. I can't definitively tell you what has gone wrong, but I can spot a few things that will be issues (there could also be others). First, I think the log snippet here comes from the main log of the cloud runner. The underlying program error can often be found in another folder of the same run. You should see a ""logs"" folder and this should contain a program-specific log (like make_examples.log). This can be more informative. From your run, I can see that you provide your FASTA as an uncompressed .FA file. I believe that the cloud runner requires a BGZIP compressed reference, a samtools FAIDX, and a GZI index all in the same place. You will need to do the following operations on the FASTA file in a bucket that you control:. bgzip -i GRCh38_Verily_v1.genome.fa. samtools faidx GRCh38_Verily_v1.genome.fa. afterward, you will need to put the resulting:. GRCh38_Verily_v1.genome.fa.gz. GRCh38_Verily_v1.genome.fa.gz.gzi. GRCh38_Verily_v1.genome.fa.gz.fai. in the same bucket that the cloud runner can access. . Hopefully these instructions seem reasonable and this unblocks you from this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:797,testability,control,control,797,"Hello Salik,. I am sorry you are encountering issues with the Cloud Runner. I can't definitively tell you what has gone wrong, but I can spot a few things that will be issues (there could also be others). First, I think the log snippet here comes from the main log of the cloud runner. The underlying program error can often be found in another folder of the same run. You should see a ""logs"" folder and this should contain a program-specific log (like make_examples.log). This can be more informative. From your run, I can see that you provide your FASTA as an uncompressed .FA file. I believe that the cloud runner requires a BGZIP compressed reference, a samtools FAIDX, and a GZI index all in the same place. You will need to do the following operations on the FASTA file in a bucket that you control:. bgzip -i GRCh38_Verily_v1.genome.fa. samtools faidx GRCh38_Verily_v1.genome.fa. afterward, you will need to put the resulting:. GRCh38_Verily_v1.genome.fa.gz. GRCh38_Verily_v1.genome.fa.gz.gzi. GRCh38_Verily_v1.genome.fa.gz.fai. in the same bucket that the cloud runner can access. . Hopefully these instructions seem reasonable and this unblocks you from this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:309,usability,error,error,309,"Hello Salik,. I am sorry you are encountering issues with the Cloud Runner. I can't definitively tell you what has gone wrong, but I can spot a few things that will be issues (there could also be others). First, I think the log snippet here comes from the main log of the cloud runner. The underlying program error can often be found in another folder of the same run. You should see a ""logs"" folder and this should contain a program-specific log (like make_examples.log). This can be more informative. From your run, I can see that you provide your FASTA as an uncompressed .FA file. I believe that the cloud runner requires a BGZIP compressed reference, a samtools FAIDX, and a GZI index all in the same place. You will need to do the following operations on the FASTA file in a bucket that you control:. bgzip -i GRCh38_Verily_v1.genome.fa. samtools faidx GRCh38_Verily_v1.genome.fa. afterward, you will need to put the resulting:. GRCh38_Verily_v1.genome.fa.gz. GRCh38_Verily_v1.genome.fa.gz.gzi. GRCh38_Verily_v1.genome.fa.gz.fai. in the same bucket that the cloud runner can access. . Hopefully these instructions seem reasonable and this unblocks you from this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:1312,availability,servic,service-account-scopes,1312,"I have generated those files and added those using the '--ref_fai' and '--ref_gzi' flags respectively and let you know the results. Here is my new configuration:. ```. #!/bin/bash. set -euo pipefail. # Set common settings. PROJECT_ID=valis-194104. OUTPUT_BUCKET=gs://canis/CNR-data. STAGING_FOLDER_NAME=deep_variant_files. OUTPUT_FILE_NAME=TLE_a_001_deep_variant.vcf. # Model for calling whole exome sequencing data. MODEL=gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard. IMAGE_VERSION=0.7.0. DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}"". COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \. --project ${PROJECT_ID} \. --zones us-west1-b \. --docker_image ${DOCKER_IMAGE} \. --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \. --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \. --model ${MODEL} \. --regions gs://canis/CNR-data/CDS-canonical.bed \. --bam gs://canis/CNR-data/TLE_a_001.bam \. --bai gs://canis/CNR-data/TLE_a_001.bam.bai \. --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz \. --ref_fai gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai \. --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi \. --gcsfuse"". # Run the pipeline. gcloud alpha genomics pipelines run \. --project ""${PROJECT_ID}"" \. --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \. --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \. --zones us-west1-b \. --docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \. --command-line ""${COMMAND}"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:147,deployability,configurat,configuration,147,"I have generated those files and added those using the '--ref_fai' and '--ref_gzi' flags respectively and let you know the results. Here is my new configuration:. ```. #!/bin/bash. set -euo pipefail. # Set common settings. PROJECT_ID=valis-194104. OUTPUT_BUCKET=gs://canis/CNR-data. STAGING_FOLDER_NAME=deep_variant_files. OUTPUT_FILE_NAME=TLE_a_001_deep_variant.vcf. # Model for calling whole exome sequencing data. MODEL=gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard. IMAGE_VERSION=0.7.0. DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}"". COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \. --project ${PROJECT_ID} \. --zones us-west1-b \. --docker_image ${DOCKER_IMAGE} \. --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \. --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \. --model ${MODEL} \. --regions gs://canis/CNR-data/CDS-canonical.bed \. --bam gs://canis/CNR-data/TLE_a_001.bam \. --bai gs://canis/CNR-data/TLE_a_001.bam.bai \. --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz \. --ref_fai gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai \. --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi \. --gcsfuse"". # Run the pipeline. gcloud alpha genomics pipelines run \. --project ""${PROJECT_ID}"" \. --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \. --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \. --zones us-west1-b \. --docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \. --command-line ""${COMMAND}"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:1232,deployability,pipelin,pipeline,1232,"I have generated those files and added those using the '--ref_fai' and '--ref_gzi' flags respectively and let you know the results. Here is my new configuration:. ```. #!/bin/bash. set -euo pipefail. # Set common settings. PROJECT_ID=valis-194104. OUTPUT_BUCKET=gs://canis/CNR-data. STAGING_FOLDER_NAME=deep_variant_files. OUTPUT_FILE_NAME=TLE_a_001_deep_variant.vcf. # Model for calling whole exome sequencing data. MODEL=gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard. IMAGE_VERSION=0.7.0. DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}"". COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \. --project ${PROJECT_ID} \. --zones us-west1-b \. --docker_image ${DOCKER_IMAGE} \. --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \. --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \. --model ${MODEL} \. --regions gs://canis/CNR-data/CDS-canonical.bed \. --bam gs://canis/CNR-data/TLE_a_001.bam \. --bai gs://canis/CNR-data/TLE_a_001.bam.bai \. --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz \. --ref_fai gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai \. --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi \. --gcsfuse"". # Run the pipeline. gcloud alpha genomics pipelines run \. --project ""${PROJECT_ID}"" \. --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \. --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \. --zones us-west1-b \. --docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \. --command-line ""${COMMAND}"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:1264,deployability,pipelin,pipelines,1264,"I have generated those files and added those using the '--ref_fai' and '--ref_gzi' flags respectively and let you know the results. Here is my new configuration:. ```. #!/bin/bash. set -euo pipefail. # Set common settings. PROJECT_ID=valis-194104. OUTPUT_BUCKET=gs://canis/CNR-data. STAGING_FOLDER_NAME=deep_variant_files. OUTPUT_FILE_NAME=TLE_a_001_deep_variant.vcf. # Model for calling whole exome sequencing data. MODEL=gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard. IMAGE_VERSION=0.7.0. DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}"". COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \. --project ${PROJECT_ID} \. --zones us-west1-b \. --docker_image ${DOCKER_IMAGE} \. --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \. --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \. --model ${MODEL} \. --regions gs://canis/CNR-data/CDS-canonical.bed \. --bam gs://canis/CNR-data/TLE_a_001.bam \. --bai gs://canis/CNR-data/TLE_a_001.bam.bai \. --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz \. --ref_fai gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai \. --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi \. --gcsfuse"". # Run the pipeline. gcloud alpha genomics pipelines run \. --project ""${PROJECT_ID}"" \. --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \. --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \. --zones us-west1-b \. --docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \. --command-line ""${COMMAND}"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:1312,deployability,servic,service-account-scopes,1312,"I have generated those files and added those using the '--ref_fai' and '--ref_gzi' flags respectively and let you know the results. Here is my new configuration:. ```. #!/bin/bash. set -euo pipefail. # Set common settings. PROJECT_ID=valis-194104. OUTPUT_BUCKET=gs://canis/CNR-data. STAGING_FOLDER_NAME=deep_variant_files. OUTPUT_FILE_NAME=TLE_a_001_deep_variant.vcf. # Model for calling whole exome sequencing data. MODEL=gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard. IMAGE_VERSION=0.7.0. DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}"". COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \. --project ${PROJECT_ID} \. --zones us-west1-b \. --docker_image ${DOCKER_IMAGE} \. --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \. --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \. --model ${MODEL} \. --regions gs://canis/CNR-data/CDS-canonical.bed \. --bam gs://canis/CNR-data/TLE_a_001.bam \. --bai gs://canis/CNR-data/TLE_a_001.bam.bai \. --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz \. --ref_fai gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai \. --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi \. --gcsfuse"". # Run the pipeline. gcloud alpha genomics pipelines run \. --project ""${PROJECT_ID}"" \. --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \. --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \. --zones us-west1-b \. --docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \. --command-line ""${COMMAND}"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:1389,deployability,log,logging,1389,"I have generated those files and added those using the '--ref_fai' and '--ref_gzi' flags respectively and let you know the results. Here is my new configuration:. ```. #!/bin/bash. set -euo pipefail. # Set common settings. PROJECT_ID=valis-194104. OUTPUT_BUCKET=gs://canis/CNR-data. STAGING_FOLDER_NAME=deep_variant_files. OUTPUT_FILE_NAME=TLE_a_001_deep_variant.vcf. # Model for calling whole exome sequencing data. MODEL=gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard. IMAGE_VERSION=0.7.0. DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}"". COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \. --project ${PROJECT_ID} \. --zones us-west1-b \. --docker_image ${DOCKER_IMAGE} \. --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \. --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \. --model ${MODEL} \. --regions gs://canis/CNR-data/CDS-canonical.bed \. --bam gs://canis/CNR-data/TLE_a_001.bam \. --bai gs://canis/CNR-data/TLE_a_001.bam.bai \. --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz \. --ref_fai gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai \. --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi \. --gcsfuse"". # Run the pipeline. gcloud alpha genomics pipelines run \. --project ""${PROJECT_ID}"" \. --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \. --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \. --zones us-west1-b \. --docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \. --command-line ""${COMMAND}"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:1473,deployability,log,log,1473,"I have generated those files and added those using the '--ref_fai' and '--ref_gzi' flags respectively and let you know the results. Here is my new configuration:. ```. #!/bin/bash. set -euo pipefail. # Set common settings. PROJECT_ID=valis-194104. OUTPUT_BUCKET=gs://canis/CNR-data. STAGING_FOLDER_NAME=deep_variant_files. OUTPUT_FILE_NAME=TLE_a_001_deep_variant.vcf. # Model for calling whole exome sequencing data. MODEL=gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard. IMAGE_VERSION=0.7.0. DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}"". COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \. --project ${PROJECT_ID} \. --zones us-west1-b \. --docker_image ${DOCKER_IMAGE} \. --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \. --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \. --model ${MODEL} \. --regions gs://canis/CNR-data/CDS-canonical.bed \. --bam gs://canis/CNR-data/TLE_a_001.bam \. --bai gs://canis/CNR-data/TLE_a_001.bam.bai \. --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz \. --ref_fai gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai \. --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi \. --gcsfuse"". # Run the pipeline. gcloud alpha genomics pipelines run \. --project ""${PROJECT_ID}"" \. --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \. --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \. --zones us-west1-b \. --docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \. --command-line ""${COMMAND}"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:370,energy efficiency,Model,Model,370,"I have generated those files and added those using the '--ref_fai' and '--ref_gzi' flags respectively and let you know the results. Here is my new configuration:. ```. #!/bin/bash. set -euo pipefail. # Set common settings. PROJECT_ID=valis-194104. OUTPUT_BUCKET=gs://canis/CNR-data. STAGING_FOLDER_NAME=deep_variant_files. OUTPUT_FILE_NAME=TLE_a_001_deep_variant.vcf. # Model for calling whole exome sequencing data. MODEL=gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard. IMAGE_VERSION=0.7.0. DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}"". COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \. --project ${PROJECT_ID} \. --zones us-west1-b \. --docker_image ${DOCKER_IMAGE} \. --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \. --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \. --model ${MODEL} \. --regions gs://canis/CNR-data/CDS-canonical.bed \. --bam gs://canis/CNR-data/TLE_a_001.bam \. --bai gs://canis/CNR-data/TLE_a_001.bam.bai \. --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz \. --ref_fai gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai \. --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi \. --gcsfuse"". # Run the pipeline. gcloud alpha genomics pipelines run \. --project ""${PROJECT_ID}"" \. --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \. --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \. --zones us-west1-b \. --docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \. --command-line ""${COMMAND}"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:417,energy efficiency,MODEL,MODEL,417,"I have generated those files and added those using the '--ref_fai' and '--ref_gzi' flags respectively and let you know the results. Here is my new configuration:. ```. #!/bin/bash. set -euo pipefail. # Set common settings. PROJECT_ID=valis-194104. OUTPUT_BUCKET=gs://canis/CNR-data. STAGING_FOLDER_NAME=deep_variant_files. OUTPUT_FILE_NAME=TLE_a_001_deep_variant.vcf. # Model for calling whole exome sequencing data. MODEL=gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard. IMAGE_VERSION=0.7.0. DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}"". COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \. --project ${PROJECT_ID} \. --zones us-west1-b \. --docker_image ${DOCKER_IMAGE} \. --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \. --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \. --model ${MODEL} \. --regions gs://canis/CNR-data/CDS-canonical.bed \. --bam gs://canis/CNR-data/TLE_a_001.bam \. --bai gs://canis/CNR-data/TLE_a_001.bam.bai \. --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz \. --ref_fai gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai \. --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi \. --gcsfuse"". # Run the pipeline. gcloud alpha genomics pipelines run \. --project ""${PROJECT_ID}"" \. --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \. --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \. --zones us-west1-b \. --docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \. --command-line ""${COMMAND}"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:440,energy efficiency,model,models,440,"I have generated those files and added those using the '--ref_fai' and '--ref_gzi' flags respectively and let you know the results. Here is my new configuration:. ```. #!/bin/bash. set -euo pipefail. # Set common settings. PROJECT_ID=valis-194104. OUTPUT_BUCKET=gs://canis/CNR-data. STAGING_FOLDER_NAME=deep_variant_files. OUTPUT_FILE_NAME=TLE_a_001_deep_variant.vcf. # Model for calling whole exome sequencing data. MODEL=gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard. IMAGE_VERSION=0.7.0. DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}"". COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \. --project ${PROJECT_ID} \. --zones us-west1-b \. --docker_image ${DOCKER_IMAGE} \. --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \. --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \. --model ${MODEL} \. --regions gs://canis/CNR-data/CDS-canonical.bed \. --bam gs://canis/CNR-data/TLE_a_001.bam \. --bai gs://canis/CNR-data/TLE_a_001.bam.bai \. --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz \. --ref_fai gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai \. --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi \. --gcsfuse"". # Run the pipeline. gcloud alpha genomics pipelines run \. --project ""${PROJECT_ID}"" \. --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \. --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \. --zones us-west1-b \. --docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \. --command-line ""${COMMAND}"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:858,energy efficiency,model,model,858,"I have generated those files and added those using the '--ref_fai' and '--ref_gzi' flags respectively and let you know the results. Here is my new configuration:. ```. #!/bin/bash. set -euo pipefail. # Set common settings. PROJECT_ID=valis-194104. OUTPUT_BUCKET=gs://canis/CNR-data. STAGING_FOLDER_NAME=deep_variant_files. OUTPUT_FILE_NAME=TLE_a_001_deep_variant.vcf. # Model for calling whole exome sequencing data. MODEL=gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard. IMAGE_VERSION=0.7.0. DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}"". COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \. --project ${PROJECT_ID} \. --zones us-west1-b \. --docker_image ${DOCKER_IMAGE} \. --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \. --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \. --model ${MODEL} \. --regions gs://canis/CNR-data/CDS-canonical.bed \. --bam gs://canis/CNR-data/TLE_a_001.bam \. --bai gs://canis/CNR-data/TLE_a_001.bam.bai \. --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz \. --ref_fai gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai \. --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi \. --gcsfuse"". # Run the pipeline. gcloud alpha genomics pipelines run \. --project ""${PROJECT_ID}"" \. --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \. --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \. --zones us-west1-b \. --docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \. --command-line ""${COMMAND}"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:866,energy efficiency,MODEL,MODEL,866,"I have generated those files and added those using the '--ref_fai' and '--ref_gzi' flags respectively and let you know the results. Here is my new configuration:. ```. #!/bin/bash. set -euo pipefail. # Set common settings. PROJECT_ID=valis-194104. OUTPUT_BUCKET=gs://canis/CNR-data. STAGING_FOLDER_NAME=deep_variant_files. OUTPUT_FILE_NAME=TLE_a_001_deep_variant.vcf. # Model for calling whole exome sequencing data. MODEL=gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard. IMAGE_VERSION=0.7.0. DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}"". COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \. --project ${PROJECT_ID} \. --zones us-west1-b \. --docker_image ${DOCKER_IMAGE} \. --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \. --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \. --model ${MODEL} \. --regions gs://canis/CNR-data/CDS-canonical.bed \. --bam gs://canis/CNR-data/TLE_a_001.bam \. --bai gs://canis/CNR-data/TLE_a_001.bam.bai \. --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz \. --ref_fai gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai \. --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi \. --gcsfuse"". # Run the pipeline. gcloud alpha genomics pipelines run \. --project ""${PROJECT_ID}"" \. --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \. --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \. --zones us-west1-b \. --docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \. --command-line ""${COMMAND}"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:1368,energy efficiency,cloud,cloud-platform,1368,"I have generated those files and added those using the '--ref_fai' and '--ref_gzi' flags respectively and let you know the results. Here is my new configuration:. ```. #!/bin/bash. set -euo pipefail. # Set common settings. PROJECT_ID=valis-194104. OUTPUT_BUCKET=gs://canis/CNR-data. STAGING_FOLDER_NAME=deep_variant_files. OUTPUT_FILE_NAME=TLE_a_001_deep_variant.vcf. # Model for calling whole exome sequencing data. MODEL=gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard. IMAGE_VERSION=0.7.0. DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}"". COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \. --project ${PROJECT_ID} \. --zones us-west1-b \. --docker_image ${DOCKER_IMAGE} \. --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \. --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \. --model ${MODEL} \. --regions gs://canis/CNR-data/CDS-canonical.bed \. --bam gs://canis/CNR-data/TLE_a_001.bam \. --bai gs://canis/CNR-data/TLE_a_001.bam.bai \. --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz \. --ref_fai gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai \. --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi \. --gcsfuse"". # Run the pipeline. gcloud alpha genomics pipelines run \. --project ""${PROJECT_ID}"" \. --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \. --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \. --zones us-west1-b \. --docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \. --command-line ""${COMMAND}"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:147,integrability,configur,configuration,147,"I have generated those files and added those using the '--ref_fai' and '--ref_gzi' flags respectively and let you know the results. Here is my new configuration:. ```. #!/bin/bash. set -euo pipefail. # Set common settings. PROJECT_ID=valis-194104. OUTPUT_BUCKET=gs://canis/CNR-data. STAGING_FOLDER_NAME=deep_variant_files. OUTPUT_FILE_NAME=TLE_a_001_deep_variant.vcf. # Model for calling whole exome sequencing data. MODEL=gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard. IMAGE_VERSION=0.7.0. DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}"". COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \. --project ${PROJECT_ID} \. --zones us-west1-b \. --docker_image ${DOCKER_IMAGE} \. --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \. --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \. --model ${MODEL} \. --regions gs://canis/CNR-data/CDS-canonical.bed \. --bam gs://canis/CNR-data/TLE_a_001.bam \. --bai gs://canis/CNR-data/TLE_a_001.bam.bai \. --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz \. --ref_fai gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai \. --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi \. --gcsfuse"". # Run the pipeline. gcloud alpha genomics pipelines run \. --project ""${PROJECT_ID}"" \. --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \. --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \. --zones us-west1-b \. --docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \. --command-line ""${COMMAND}"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:1232,integrability,pipelin,pipeline,1232,"I have generated those files and added those using the '--ref_fai' and '--ref_gzi' flags respectively and let you know the results. Here is my new configuration:. ```. #!/bin/bash. set -euo pipefail. # Set common settings. PROJECT_ID=valis-194104. OUTPUT_BUCKET=gs://canis/CNR-data. STAGING_FOLDER_NAME=deep_variant_files. OUTPUT_FILE_NAME=TLE_a_001_deep_variant.vcf. # Model for calling whole exome sequencing data. MODEL=gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard. IMAGE_VERSION=0.7.0. DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}"". COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \. --project ${PROJECT_ID} \. --zones us-west1-b \. --docker_image ${DOCKER_IMAGE} \. --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \. --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \. --model ${MODEL} \. --regions gs://canis/CNR-data/CDS-canonical.bed \. --bam gs://canis/CNR-data/TLE_a_001.bam \. --bai gs://canis/CNR-data/TLE_a_001.bam.bai \. --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz \. --ref_fai gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai \. --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi \. --gcsfuse"". # Run the pipeline. gcloud alpha genomics pipelines run \. --project ""${PROJECT_ID}"" \. --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \. --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \. --zones us-west1-b \. --docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \. --command-line ""${COMMAND}"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:1264,integrability,pipelin,pipelines,1264,"I have generated those files and added those using the '--ref_fai' and '--ref_gzi' flags respectively and let you know the results. Here is my new configuration:. ```. #!/bin/bash. set -euo pipefail. # Set common settings. PROJECT_ID=valis-194104. OUTPUT_BUCKET=gs://canis/CNR-data. STAGING_FOLDER_NAME=deep_variant_files. OUTPUT_FILE_NAME=TLE_a_001_deep_variant.vcf. # Model for calling whole exome sequencing data. MODEL=gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard. IMAGE_VERSION=0.7.0. DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}"". COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \. --project ${PROJECT_ID} \. --zones us-west1-b \. --docker_image ${DOCKER_IMAGE} \. --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \. --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \. --model ${MODEL} \. --regions gs://canis/CNR-data/CDS-canonical.bed \. --bam gs://canis/CNR-data/TLE_a_001.bam \. --bai gs://canis/CNR-data/TLE_a_001.bam.bai \. --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz \. --ref_fai gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai \. --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi \. --gcsfuse"". # Run the pipeline. gcloud alpha genomics pipelines run \. --project ""${PROJECT_ID}"" \. --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \. --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \. --zones us-west1-b \. --docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \. --command-line ""${COMMAND}"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:1312,integrability,servic,service-account-scopes,1312,"I have generated those files and added those using the '--ref_fai' and '--ref_gzi' flags respectively and let you know the results. Here is my new configuration:. ```. #!/bin/bash. set -euo pipefail. # Set common settings. PROJECT_ID=valis-194104. OUTPUT_BUCKET=gs://canis/CNR-data. STAGING_FOLDER_NAME=deep_variant_files. OUTPUT_FILE_NAME=TLE_a_001_deep_variant.vcf. # Model for calling whole exome sequencing data. MODEL=gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard. IMAGE_VERSION=0.7.0. DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}"". COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \. --project ${PROJECT_ID} \. --zones us-west1-b \. --docker_image ${DOCKER_IMAGE} \. --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \. --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \. --model ${MODEL} \. --regions gs://canis/CNR-data/CDS-canonical.bed \. --bam gs://canis/CNR-data/TLE_a_001.bam \. --bai gs://canis/CNR-data/TLE_a_001.bam.bai \. --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz \. --ref_fai gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai \. --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi \. --gcsfuse"". # Run the pipeline. gcloud alpha genomics pipelines run \. --project ""${PROJECT_ID}"" \. --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \. --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \. --zones us-west1-b \. --docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \. --command-line ""${COMMAND}"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:1374,interoperability,platform,platform,1374,"I have generated those files and added those using the '--ref_fai' and '--ref_gzi' flags respectively and let you know the results. Here is my new configuration:. ```. #!/bin/bash. set -euo pipefail. # Set common settings. PROJECT_ID=valis-194104. OUTPUT_BUCKET=gs://canis/CNR-data. STAGING_FOLDER_NAME=deep_variant_files. OUTPUT_FILE_NAME=TLE_a_001_deep_variant.vcf. # Model for calling whole exome sequencing data. MODEL=gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard. IMAGE_VERSION=0.7.0. DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}"". COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \. --project ${PROJECT_ID} \. --zones us-west1-b \. --docker_image ${DOCKER_IMAGE} \. --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \. --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \. --model ${MODEL} \. --regions gs://canis/CNR-data/CDS-canonical.bed \. --bam gs://canis/CNR-data/TLE_a_001.bam \. --bai gs://canis/CNR-data/TLE_a_001.bam.bai \. --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz \. --ref_fai gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai \. --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi \. --gcsfuse"". # Run the pipeline. gcloud alpha genomics pipelines run \. --project ""${PROJECT_ID}"" \. --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \. --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \. --zones us-west1-b \. --docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \. --command-line ""${COMMAND}"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:147,modifiability,configur,configuration,147,"I have generated those files and added those using the '--ref_fai' and '--ref_gzi' flags respectively and let you know the results. Here is my new configuration:. ```. #!/bin/bash. set -euo pipefail. # Set common settings. PROJECT_ID=valis-194104. OUTPUT_BUCKET=gs://canis/CNR-data. STAGING_FOLDER_NAME=deep_variant_files. OUTPUT_FILE_NAME=TLE_a_001_deep_variant.vcf. # Model for calling whole exome sequencing data. MODEL=gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard. IMAGE_VERSION=0.7.0. DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}"". COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \. --project ${PROJECT_ID} \. --zones us-west1-b \. --docker_image ${DOCKER_IMAGE} \. --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \. --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \. --model ${MODEL} \. --regions gs://canis/CNR-data/CDS-canonical.bed \. --bam gs://canis/CNR-data/TLE_a_001.bam \. --bai gs://canis/CNR-data/TLE_a_001.bam.bai \. --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz \. --ref_fai gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai \. --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi \. --gcsfuse"". # Run the pipeline. gcloud alpha genomics pipelines run \. --project ""${PROJECT_ID}"" \. --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \. --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \. --zones us-west1-b \. --docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \. --command-line ""${COMMAND}"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:1312,modifiability,servic,service-account-scopes,1312,"I have generated those files and added those using the '--ref_fai' and '--ref_gzi' flags respectively and let you know the results. Here is my new configuration:. ```. #!/bin/bash. set -euo pipefail. # Set common settings. PROJECT_ID=valis-194104. OUTPUT_BUCKET=gs://canis/CNR-data. STAGING_FOLDER_NAME=deep_variant_files. OUTPUT_FILE_NAME=TLE_a_001_deep_variant.vcf. # Model for calling whole exome sequencing data. MODEL=gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard. IMAGE_VERSION=0.7.0. DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}"". COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \. --project ${PROJECT_ID} \. --zones us-west1-b \. --docker_image ${DOCKER_IMAGE} \. --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \. --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \. --model ${MODEL} \. --regions gs://canis/CNR-data/CDS-canonical.bed \. --bam gs://canis/CNR-data/TLE_a_001.bam \. --bai gs://canis/CNR-data/TLE_a_001.bam.bai \. --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz \. --ref_fai gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai \. --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi \. --gcsfuse"". # Run the pipeline. gcloud alpha genomics pipelines run \. --project ""${PROJECT_ID}"" \. --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \. --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \. --zones us-west1-b \. --docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \. --command-line ""${COMMAND}"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:1389,safety,log,logging,1389,"I have generated those files and added those using the '--ref_fai' and '--ref_gzi' flags respectively and let you know the results. Here is my new configuration:. ```. #!/bin/bash. set -euo pipefail. # Set common settings. PROJECT_ID=valis-194104. OUTPUT_BUCKET=gs://canis/CNR-data. STAGING_FOLDER_NAME=deep_variant_files. OUTPUT_FILE_NAME=TLE_a_001_deep_variant.vcf. # Model for calling whole exome sequencing data. MODEL=gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard. IMAGE_VERSION=0.7.0. DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}"". COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \. --project ${PROJECT_ID} \. --zones us-west1-b \. --docker_image ${DOCKER_IMAGE} \. --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \. --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \. --model ${MODEL} \. --regions gs://canis/CNR-data/CDS-canonical.bed \. --bam gs://canis/CNR-data/TLE_a_001.bam \. --bai gs://canis/CNR-data/TLE_a_001.bam.bai \. --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz \. --ref_fai gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai \. --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi \. --gcsfuse"". # Run the pipeline. gcloud alpha genomics pipelines run \. --project ""${PROJECT_ID}"" \. --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \. --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \. --zones us-west1-b \. --docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \. --command-line ""${COMMAND}"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:1473,safety,log,log,1473,"I have generated those files and added those using the '--ref_fai' and '--ref_gzi' flags respectively and let you know the results. Here is my new configuration:. ```. #!/bin/bash. set -euo pipefail. # Set common settings. PROJECT_ID=valis-194104. OUTPUT_BUCKET=gs://canis/CNR-data. STAGING_FOLDER_NAME=deep_variant_files. OUTPUT_FILE_NAME=TLE_a_001_deep_variant.vcf. # Model for calling whole exome sequencing data. MODEL=gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard. IMAGE_VERSION=0.7.0. DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}"". COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \. --project ${PROJECT_ID} \. --zones us-west1-b \. --docker_image ${DOCKER_IMAGE} \. --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \. --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \. --model ${MODEL} \. --regions gs://canis/CNR-data/CDS-canonical.bed \. --bam gs://canis/CNR-data/TLE_a_001.bam \. --bai gs://canis/CNR-data/TLE_a_001.bam.bai \. --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz \. --ref_fai gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai \. --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi \. --gcsfuse"". # Run the pipeline. gcloud alpha genomics pipelines run \. --project ""${PROJECT_ID}"" \. --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \. --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \. --zones us-west1-b \. --docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \. --command-line ""${COMMAND}"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:147,security,configur,configuration,147,"I have generated those files and added those using the '--ref_fai' and '--ref_gzi' flags respectively and let you know the results. Here is my new configuration:. ```. #!/bin/bash. set -euo pipefail. # Set common settings. PROJECT_ID=valis-194104. OUTPUT_BUCKET=gs://canis/CNR-data. STAGING_FOLDER_NAME=deep_variant_files. OUTPUT_FILE_NAME=TLE_a_001_deep_variant.vcf. # Model for calling whole exome sequencing data. MODEL=gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard. IMAGE_VERSION=0.7.0. DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}"". COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \. --project ${PROJECT_ID} \. --zones us-west1-b \. --docker_image ${DOCKER_IMAGE} \. --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \. --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \. --model ${MODEL} \. --regions gs://canis/CNR-data/CDS-canonical.bed \. --bam gs://canis/CNR-data/TLE_a_001.bam \. --bai gs://canis/CNR-data/TLE_a_001.bam.bai \. --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz \. --ref_fai gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai \. --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi \. --gcsfuse"". # Run the pipeline. gcloud alpha genomics pipelines run \. --project ""${PROJECT_ID}"" \. --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \. --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \. --zones us-west1-b \. --docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \. --command-line ""${COMMAND}"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:370,security,Model,Model,370,"I have generated those files and added those using the '--ref_fai' and '--ref_gzi' flags respectively and let you know the results. Here is my new configuration:. ```. #!/bin/bash. set -euo pipefail. # Set common settings. PROJECT_ID=valis-194104. OUTPUT_BUCKET=gs://canis/CNR-data. STAGING_FOLDER_NAME=deep_variant_files. OUTPUT_FILE_NAME=TLE_a_001_deep_variant.vcf. # Model for calling whole exome sequencing data. MODEL=gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard. IMAGE_VERSION=0.7.0. DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}"". COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \. --project ${PROJECT_ID} \. --zones us-west1-b \. --docker_image ${DOCKER_IMAGE} \. --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \. --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \. --model ${MODEL} \. --regions gs://canis/CNR-data/CDS-canonical.bed \. --bam gs://canis/CNR-data/TLE_a_001.bam \. --bai gs://canis/CNR-data/TLE_a_001.bam.bai \. --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz \. --ref_fai gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai \. --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi \. --gcsfuse"". # Run the pipeline. gcloud alpha genomics pipelines run \. --project ""${PROJECT_ID}"" \. --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \. --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \. --zones us-west1-b \. --docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \. --command-line ""${COMMAND}"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:417,security,MODEL,MODEL,417,"I have generated those files and added those using the '--ref_fai' and '--ref_gzi' flags respectively and let you know the results. Here is my new configuration:. ```. #!/bin/bash. set -euo pipefail. # Set common settings. PROJECT_ID=valis-194104. OUTPUT_BUCKET=gs://canis/CNR-data. STAGING_FOLDER_NAME=deep_variant_files. OUTPUT_FILE_NAME=TLE_a_001_deep_variant.vcf. # Model for calling whole exome sequencing data. MODEL=gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard. IMAGE_VERSION=0.7.0. DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}"". COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \. --project ${PROJECT_ID} \. --zones us-west1-b \. --docker_image ${DOCKER_IMAGE} \. --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \. --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \. --model ${MODEL} \. --regions gs://canis/CNR-data/CDS-canonical.bed \. --bam gs://canis/CNR-data/TLE_a_001.bam \. --bai gs://canis/CNR-data/TLE_a_001.bam.bai \. --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz \. --ref_fai gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai \. --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi \. --gcsfuse"". # Run the pipeline. gcloud alpha genomics pipelines run \. --project ""${PROJECT_ID}"" \. --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \. --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \. --zones us-west1-b \. --docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \. --command-line ""${COMMAND}"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:440,security,model,models,440,"I have generated those files and added those using the '--ref_fai' and '--ref_gzi' flags respectively and let you know the results. Here is my new configuration:. ```. #!/bin/bash. set -euo pipefail. # Set common settings. PROJECT_ID=valis-194104. OUTPUT_BUCKET=gs://canis/CNR-data. STAGING_FOLDER_NAME=deep_variant_files. OUTPUT_FILE_NAME=TLE_a_001_deep_variant.vcf. # Model for calling whole exome sequencing data. MODEL=gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard. IMAGE_VERSION=0.7.0. DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}"". COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \. --project ${PROJECT_ID} \. --zones us-west1-b \. --docker_image ${DOCKER_IMAGE} \. --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \. --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \. --model ${MODEL} \. --regions gs://canis/CNR-data/CDS-canonical.bed \. --bam gs://canis/CNR-data/TLE_a_001.bam \. --bai gs://canis/CNR-data/TLE_a_001.bam.bai \. --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz \. --ref_fai gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai \. --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi \. --gcsfuse"". # Run the pipeline. gcloud alpha genomics pipelines run \. --project ""${PROJECT_ID}"" \. --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \. --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \. --zones us-west1-b \. --docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \. --command-line ""${COMMAND}"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:858,security,model,model,858,"I have generated those files and added those using the '--ref_fai' and '--ref_gzi' flags respectively and let you know the results. Here is my new configuration:. ```. #!/bin/bash. set -euo pipefail. # Set common settings. PROJECT_ID=valis-194104. OUTPUT_BUCKET=gs://canis/CNR-data. STAGING_FOLDER_NAME=deep_variant_files. OUTPUT_FILE_NAME=TLE_a_001_deep_variant.vcf. # Model for calling whole exome sequencing data. MODEL=gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard. IMAGE_VERSION=0.7.0. DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}"". COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \. --project ${PROJECT_ID} \. --zones us-west1-b \. --docker_image ${DOCKER_IMAGE} \. --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \. --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \. --model ${MODEL} \. --regions gs://canis/CNR-data/CDS-canonical.bed \. --bam gs://canis/CNR-data/TLE_a_001.bam \. --bai gs://canis/CNR-data/TLE_a_001.bam.bai \. --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz \. --ref_fai gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai \. --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi \. --gcsfuse"". # Run the pipeline. gcloud alpha genomics pipelines run \. --project ""${PROJECT_ID}"" \. --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \. --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \. --zones us-west1-b \. --docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \. --command-line ""${COMMAND}"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:866,security,MODEL,MODEL,866,"I have generated those files and added those using the '--ref_fai' and '--ref_gzi' flags respectively and let you know the results. Here is my new configuration:. ```. #!/bin/bash. set -euo pipefail. # Set common settings. PROJECT_ID=valis-194104. OUTPUT_BUCKET=gs://canis/CNR-data. STAGING_FOLDER_NAME=deep_variant_files. OUTPUT_FILE_NAME=TLE_a_001_deep_variant.vcf. # Model for calling whole exome sequencing data. MODEL=gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard. IMAGE_VERSION=0.7.0. DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}"". COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \. --project ${PROJECT_ID} \. --zones us-west1-b \. --docker_image ${DOCKER_IMAGE} \. --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \. --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \. --model ${MODEL} \. --regions gs://canis/CNR-data/CDS-canonical.bed \. --bam gs://canis/CNR-data/TLE_a_001.bam \. --bai gs://canis/CNR-data/TLE_a_001.bam.bai \. --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz \. --ref_fai gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai \. --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi \. --gcsfuse"". # Run the pipeline. gcloud alpha genomics pipelines run \. --project ""${PROJECT_ID}"" \. --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \. --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \. --zones us-west1-b \. --docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \. --command-line ""${COMMAND}"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:1363,security,auth,auth,1363,"I have generated those files and added those using the '--ref_fai' and '--ref_gzi' flags respectively and let you know the results. Here is my new configuration:. ```. #!/bin/bash. set -euo pipefail. # Set common settings. PROJECT_ID=valis-194104. OUTPUT_BUCKET=gs://canis/CNR-data. STAGING_FOLDER_NAME=deep_variant_files. OUTPUT_FILE_NAME=TLE_a_001_deep_variant.vcf. # Model for calling whole exome sequencing data. MODEL=gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard. IMAGE_VERSION=0.7.0. DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}"". COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \. --project ${PROJECT_ID} \. --zones us-west1-b \. --docker_image ${DOCKER_IMAGE} \. --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \. --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \. --model ${MODEL} \. --regions gs://canis/CNR-data/CDS-canonical.bed \. --bam gs://canis/CNR-data/TLE_a_001.bam \. --bai gs://canis/CNR-data/TLE_a_001.bam.bai \. --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz \. --ref_fai gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai \. --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi \. --gcsfuse"". # Run the pipeline. gcloud alpha genomics pipelines run \. --project ""${PROJECT_ID}"" \. --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \. --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \. --zones us-west1-b \. --docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \. --command-line ""${COMMAND}"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:1389,security,log,logging,1389,"I have generated those files and added those using the '--ref_fai' and '--ref_gzi' flags respectively and let you know the results. Here is my new configuration:. ```. #!/bin/bash. set -euo pipefail. # Set common settings. PROJECT_ID=valis-194104. OUTPUT_BUCKET=gs://canis/CNR-data. STAGING_FOLDER_NAME=deep_variant_files. OUTPUT_FILE_NAME=TLE_a_001_deep_variant.vcf. # Model for calling whole exome sequencing data. MODEL=gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard. IMAGE_VERSION=0.7.0. DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}"". COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \. --project ${PROJECT_ID} \. --zones us-west1-b \. --docker_image ${DOCKER_IMAGE} \. --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \. --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \. --model ${MODEL} \. --regions gs://canis/CNR-data/CDS-canonical.bed \. --bam gs://canis/CNR-data/TLE_a_001.bam \. --bai gs://canis/CNR-data/TLE_a_001.bam.bai \. --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz \. --ref_fai gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai \. --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi \. --gcsfuse"". # Run the pipeline. gcloud alpha genomics pipelines run \. --project ""${PROJECT_ID}"" \. --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \. --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \. --zones us-west1-b \. --docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \. --command-line ""${COMMAND}"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:1473,security,log,log,1473,"I have generated those files and added those using the '--ref_fai' and '--ref_gzi' flags respectively and let you know the results. Here is my new configuration:. ```. #!/bin/bash. set -euo pipefail. # Set common settings. PROJECT_ID=valis-194104. OUTPUT_BUCKET=gs://canis/CNR-data. STAGING_FOLDER_NAME=deep_variant_files. OUTPUT_FILE_NAME=TLE_a_001_deep_variant.vcf. # Model for calling whole exome sequencing data. MODEL=gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard. IMAGE_VERSION=0.7.0. DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}"". COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \. --project ${PROJECT_ID} \. --zones us-west1-b \. --docker_image ${DOCKER_IMAGE} \. --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \. --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \. --model ${MODEL} \. --regions gs://canis/CNR-data/CDS-canonical.bed \. --bam gs://canis/CNR-data/TLE_a_001.bam \. --bai gs://canis/CNR-data/TLE_a_001.bam.bai \. --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz \. --ref_fai gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai \. --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi \. --gcsfuse"". # Run the pipeline. gcloud alpha genomics pipelines run \. --project ""${PROJECT_ID}"" \. --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \. --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \. --zones us-west1-b \. --docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \. --command-line ""${COMMAND}"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:1587,security,command-lin,command-line,1587,"I have generated those files and added those using the '--ref_fai' and '--ref_gzi' flags respectively and let you know the results. Here is my new configuration:. ```. #!/bin/bash. set -euo pipefail. # Set common settings. PROJECT_ID=valis-194104. OUTPUT_BUCKET=gs://canis/CNR-data. STAGING_FOLDER_NAME=deep_variant_files. OUTPUT_FILE_NAME=TLE_a_001_deep_variant.vcf. # Model for calling whole exome sequencing data. MODEL=gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard. IMAGE_VERSION=0.7.0. DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}"". COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \. --project ${PROJECT_ID} \. --zones us-west1-b \. --docker_image ${DOCKER_IMAGE} \. --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \. --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \. --model ${MODEL} \. --regions gs://canis/CNR-data/CDS-canonical.bed \. --bam gs://canis/CNR-data/TLE_a_001.bam \. --bai gs://canis/CNR-data/TLE_a_001.bam.bai \. --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz \. --ref_fai gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai \. --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi \. --gcsfuse"". # Run the pipeline. gcloud alpha genomics pipelines run \. --project ""${PROJECT_ID}"" \. --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \. --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \. --zones us-west1-b \. --docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \. --command-line ""${COMMAND}"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:1389,testability,log,logging,1389,"I have generated those files and added those using the '--ref_fai' and '--ref_gzi' flags respectively and let you know the results. Here is my new configuration:. ```. #!/bin/bash. set -euo pipefail. # Set common settings. PROJECT_ID=valis-194104. OUTPUT_BUCKET=gs://canis/CNR-data. STAGING_FOLDER_NAME=deep_variant_files. OUTPUT_FILE_NAME=TLE_a_001_deep_variant.vcf. # Model for calling whole exome sequencing data. MODEL=gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard. IMAGE_VERSION=0.7.0. DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}"". COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \. --project ${PROJECT_ID} \. --zones us-west1-b \. --docker_image ${DOCKER_IMAGE} \. --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \. --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \. --model ${MODEL} \. --regions gs://canis/CNR-data/CDS-canonical.bed \. --bam gs://canis/CNR-data/TLE_a_001.bam \. --bai gs://canis/CNR-data/TLE_a_001.bam.bai \. --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz \. --ref_fai gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai \. --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi \. --gcsfuse"". # Run the pipeline. gcloud alpha genomics pipelines run \. --project ""${PROJECT_ID}"" \. --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \. --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \. --zones us-west1-b \. --docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \. --command-line ""${COMMAND}"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:1473,testability,log,log,1473,"I have generated those files and added those using the '--ref_fai' and '--ref_gzi' flags respectively and let you know the results. Here is my new configuration:. ```. #!/bin/bash. set -euo pipefail. # Set common settings. PROJECT_ID=valis-194104. OUTPUT_BUCKET=gs://canis/CNR-data. STAGING_FOLDER_NAME=deep_variant_files. OUTPUT_FILE_NAME=TLE_a_001_deep_variant.vcf. # Model for calling whole exome sequencing data. MODEL=gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard. IMAGE_VERSION=0.7.0. DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}"". COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \. --project ${PROJECT_ID} \. --zones us-west1-b \. --docker_image ${DOCKER_IMAGE} \. --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \. --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \. --model ${MODEL} \. --regions gs://canis/CNR-data/CDS-canonical.bed \. --bam gs://canis/CNR-data/TLE_a_001.bam \. --bai gs://canis/CNR-data/TLE_a_001.bam.bai \. --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz \. --ref_fai gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai \. --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi \. --gcsfuse"". # Run the pipeline. gcloud alpha genomics pipelines run \. --project ""${PROJECT_ID}"" \. --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \. --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \. --zones us-west1-b \. --docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \. --command-line ""${COMMAND}"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:607,usability,COMMAND,COMMAND,607,"I have generated those files and added those using the '--ref_fai' and '--ref_gzi' flags respectively and let you know the results. Here is my new configuration:. ```. #!/bin/bash. set -euo pipefail. # Set common settings. PROJECT_ID=valis-194104. OUTPUT_BUCKET=gs://canis/CNR-data. STAGING_FOLDER_NAME=deep_variant_files. OUTPUT_FILE_NAME=TLE_a_001_deep_variant.vcf. # Model for calling whole exome sequencing data. MODEL=gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard. IMAGE_VERSION=0.7.0. DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}"". COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \. --project ${PROJECT_ID} \. --zones us-west1-b \. --docker_image ${DOCKER_IMAGE} \. --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \. --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \. --model ${MODEL} \. --regions gs://canis/CNR-data/CDS-canonical.bed \. --bam gs://canis/CNR-data/TLE_a_001.bam \. --bai gs://canis/CNR-data/TLE_a_001.bam.bai \. --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz \. --ref_fai gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai \. --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi \. --gcsfuse"". # Run the pipeline. gcloud alpha genomics pipelines run \. --project ""${PROJECT_ID}"" \. --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \. --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \. --zones us-west1-b \. --docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \. --command-line ""${COMMAND}"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:1587,usability,command,command-line,1587,"I have generated those files and added those using the '--ref_fai' and '--ref_gzi' flags respectively and let you know the results. Here is my new configuration:. ```. #!/bin/bash. set -euo pipefail. # Set common settings. PROJECT_ID=valis-194104. OUTPUT_BUCKET=gs://canis/CNR-data. STAGING_FOLDER_NAME=deep_variant_files. OUTPUT_FILE_NAME=TLE_a_001_deep_variant.vcf. # Model for calling whole exome sequencing data. MODEL=gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard. IMAGE_VERSION=0.7.0. DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}"". COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \. --project ${PROJECT_ID} \. --zones us-west1-b \. --docker_image ${DOCKER_IMAGE} \. --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \. --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \. --model ${MODEL} \. --regions gs://canis/CNR-data/CDS-canonical.bed \. --bam gs://canis/CNR-data/TLE_a_001.bam \. --bai gs://canis/CNR-data/TLE_a_001.bam.bai \. --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz \. --ref_fai gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai \. --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi \. --gcsfuse"". # Run the pipeline. gcloud alpha genomics pipelines run \. --project ""${PROJECT_ID}"" \. --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \. --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \. --zones us-west1-b \. --docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \. --command-line ""${COMMAND}"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:1603,usability,COMMAND,COMMAND,1603,"I have generated those files and added those using the '--ref_fai' and '--ref_gzi' flags respectively and let you know the results. Here is my new configuration:. ```. #!/bin/bash. set -euo pipefail. # Set common settings. PROJECT_ID=valis-194104. OUTPUT_BUCKET=gs://canis/CNR-data. STAGING_FOLDER_NAME=deep_variant_files. OUTPUT_FILE_NAME=TLE_a_001_deep_variant.vcf. # Model for calling whole exome sequencing data. MODEL=gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard. IMAGE_VERSION=0.7.0. DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}"". COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \. --project ${PROJECT_ID} \. --zones us-west1-b \. --docker_image ${DOCKER_IMAGE} \. --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \. --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \. --model ${MODEL} \. --regions gs://canis/CNR-data/CDS-canonical.bed \. --bam gs://canis/CNR-data/TLE_a_001.bam \. --bai gs://canis/CNR-data/TLE_a_001.bam.bai \. --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz \. --ref_fai gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai \. --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi \. --gcsfuse"". # Run the pipeline. gcloud alpha genomics pipelines run \. --project ""${PROJECT_ID}"" \. --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \. --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \. --zones us-west1-b \. --docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \. --command-line ""${COMMAND}"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:32,availability,failur,failures,32,"Ran it again, and still hitting failures:. ```. done: true. error:. code: 9. message: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. metadata:. '@type': type.googleapis.com/google.genomics.v2alpha1.Metadata. createTime: '2018-11-08T14:27:06.016940Z'. endTime: '2018-11-08T14:30:59.324697Z'. events:. - description: Worker released. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.WorkerReleasedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:30:59.324697Z'. - description: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.FailedEvent. cause: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. code: FAILED_PRECONDITION. timestamp: '2018-11-08T14:30:58.518326Z'. - description: Stopped running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 2. exitStatus: 0. stderr: ''. timestamp: '2018-11-08T14:30:58.416239Z'. - description: Started running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent. actionId: 2. ipAddress: ''. portMappings: {}. timestamp: '2018-11-08T14:30:55.929647Z'. - description: Unexpected exit status 1 while running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project. valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:60,availability,error,error,60,"Ran it again, and still hitting failures:. ```. done: true. error:. code: 9. message: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. metadata:. '@type': type.googleapis.com/google.genomics.v2alpha1.Metadata. createTime: '2018-11-08T14:27:06.016940Z'. endTime: '2018-11-08T14:30:59.324697Z'. events:. - description: Worker released. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.WorkerReleasedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:30:59.324697Z'. - description: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.FailedEvent. cause: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. code: FAILED_PRECONDITION. timestamp: '2018-11-08T14:30:58.518326Z'. - description: Stopped running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 2. exitStatus: 0. stderr: ''. timestamp: '2018-11-08T14:30:58.416239Z'. - description: Started running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent. actionId: 2. ipAddress: ''. portMappings: {}. timestamp: '2018-11-08T14:30:55.929647Z'. - description: Unexpected exit status 1 while running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project. valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:3958,availability,error,error,3958," --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse"": exit status 1: /examples_output.tfrecord@""${SHARDS}"".gz\n --reads ""/input-gcsfused-{}/${BAM}""\n --ref ""${INPUT_REF}""\n --task {}\n --regions gs://canis/CNR-data/CDS-canonical.bed"" # ENABLE_FUSE\n']. Traceback (most recent call last):. File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>. run(). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run. _run_make_examples(pipeline_args). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples. _wait_for_results(threads, results). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results. result.get(). File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get. raise self._value. RuntimeError: Job failed with error ""run"": operation ""projects/valis-194104/operations/781565864516461293"" failed: executing pipeline: Execution failed: action 6: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION). details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 1. exitStatus: 1. stderr: |+. /examples_output.tfrecord@""${SHARDS}"".gz\n --reads ""/input-gcsfused-{}/${BAM}""\n --ref ""${INPUT_REF}""\n --task {}\n --regions gs://canis/CNR-data/CDS-canonical.bed"" # ENABLE_FUSE\n']. Traceback (most recent call last):. File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>. run(). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run. _run_make_examples(pipeline_args). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples. _wait_for_results(threads, results). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results. result.get(). File",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:3971,availability,operat,operation,3971,"is/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse"": exit status 1: /examples_output.tfrecord@""${SHARDS}"".gz\n --reads ""/input-gcsfused-{}/${BAM}""\n --ref ""${INPUT_REF}""\n --task {}\n --regions gs://canis/CNR-data/CDS-canonical.bed"" # ENABLE_FUSE\n']. Traceback (most recent call last):. File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>. run(). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run. _run_make_examples(pipeline_args). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples. _wait_for_results(threads, results). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results. result.get(). File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get. raise self._value. RuntimeError: Job failed with error ""run"": operation ""projects/valis-194104/operations/781565864516461293"" failed: executing pipeline: Execution failed: action 6: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION). details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 1. exitStatus: 1. stderr: |+. /examples_output.tfrecord@""${SHARDS}"".gz\n --reads ""/input-gcsfused-{}/${BAM}""\n --ref ""${INPUT_REF}""\n --task {}\n --regions gs://canis/CNR-data/CDS-canonical.bed"" # ENABLE_FUSE\n']. Traceback (most recent call last):. File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>. run(). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run. _run_make_examples(pipeline_args). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples. _wait_for_results(threads, results). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results. result.get(). File ""/usr/lib/pyth",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:4004,availability,operat,operations,4004,"me.fa.gz --ref_fai gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse"": exit status 1: /examples_output.tfrecord@""${SHARDS}"".gz\n --reads ""/input-gcsfused-{}/${BAM}""\n --ref ""${INPUT_REF}""\n --task {}\n --regions gs://canis/CNR-data/CDS-canonical.bed"" # ENABLE_FUSE\n']. Traceback (most recent call last):. File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>. run(). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run. _run_make_examples(pipeline_args). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples. _wait_for_results(threads, results). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results. result.get(). File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get. raise self._value. RuntimeError: Job failed with error ""run"": operation ""projects/valis-194104/operations/781565864516461293"" failed: executing pipeline: Execution failed: action 6: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION). details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 1. exitStatus: 1. stderr: |+. /examples_output.tfrecord@""${SHARDS}"".gz\n --reads ""/input-gcsfused-{}/${BAM}""\n --ref ""${INPUT_REF}""\n --task {}\n --regions gs://canis/CNR-data/CDS-canonical.bed"" # ENABLE_FUSE\n']. Traceback (most recent call last):. File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>. run(). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run. _run_make_examples(pipeline_args). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples. _wait_for_results(threads, results). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results. result.get(). File ""/usr/lib/python2.7/multiprocessing/pool.py"", l",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:5075,availability,error,error,5075,"d: action 6: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION). details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 1. exitStatus: 1. stderr: |+. /examples_output.tfrecord@""${SHARDS}"".gz\n --reads ""/input-gcsfused-{}/${BAM}""\n --ref ""${INPUT_REF}""\n --task {}\n --regions gs://canis/CNR-data/CDS-canonical.bed"" # ENABLE_FUSE\n']. Traceback (most recent call last):. File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>. run(). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run. _run_make_examples(pipeline_args). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples. _wait_for_results(threads, results). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results. result.get(). File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get. raise self._value. RuntimeError: Job failed with error ""run"": operation ""projects/valis-194104/operations/781565864516461293"" failed: executing pipeline: Execution failed: action 6: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION). timestamp: '2018-11-08T14:30:55.372655Z'. - description: Started running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project. valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse"". details:. '@type': type.google",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:5088,availability,operat,operation,5088,"expected exit status 1 was not ignored (reason: FAILED_PRECONDITION). details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 1. exitStatus: 1. stderr: |+. /examples_output.tfrecord@""${SHARDS}"".gz\n --reads ""/input-gcsfused-{}/${BAM}""\n --ref ""${INPUT_REF}""\n --task {}\n --regions gs://canis/CNR-data/CDS-canonical.bed"" # ENABLE_FUSE\n']. Traceback (most recent call last):. File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>. run(). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run. _run_make_examples(pipeline_args). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples. _wait_for_results(threads, results). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results. result.get(). File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get. raise self._value. RuntimeError: Job failed with error ""run"": operation ""projects/valis-194104/operations/781565864516461293"" failed: executing pipeline: Execution failed: action 6: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION). timestamp: '2018-11-08T14:30:55.372655Z'. - description: Started running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project. valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse"". details:. '@type': type.googleapis.com/google",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:5121,availability,operat,operations,5121,"nored (reason: FAILED_PRECONDITION). details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 1. exitStatus: 1. stderr: |+. /examples_output.tfrecord@""${SHARDS}"".gz\n --reads ""/input-gcsfused-{}/${BAM}""\n --ref ""${INPUT_REF}""\n --task {}\n --regions gs://canis/CNR-data/CDS-canonical.bed"" # ENABLE_FUSE\n']. Traceback (most recent call last):. File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>. run(). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run. _run_make_examples(pipeline_args). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples. _wait_for_results(threads, results). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results. result.get(). File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get. raise self._value. RuntimeError: Job failed with error ""run"": operation ""projects/valis-194104/operations/781565864516461293"" failed: executing pipeline: Execution failed: action 6: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION). timestamp: '2018-11-08T14:30:55.372655Z'. - description: Started running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project. valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStart",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:9037,availability,servic,serviceAccount,9037,rAssignedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:27:06.604193Z'. labels: {}. pipeline:. actions:. - commands:. - -c. - /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project valis-194104 --zones. us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse. entrypoint: bash. environment: {}. flags: []. imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0. labels: {}. mounts: []. name: ''. pidNamespace: ''. portMappings: {}. - commands:. - /bin/sh. - -c. - gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log. entrypoint: ''. environment: {}. flags:. - ALWAYS_RUN. imageUri: google/cloud-sdk:alpine. labels: {}. mounts: []. name: ''. pidNamespace: ''. portMappings: {}. environment: {}. resources:. projectId: valis-194104. regions: []. virtualMachine:. accelerators: []. bootDiskSizeGb: 10. bootImage: projects/cos-cloud/global/images/family/cos-stable. cpuPlatform: ''. disks: []. labels: {}. machineType: n1-standard-1. nvidiaDriverVersion: ''. preemptible: false. serviceAccount:. email: default. scopes:. - https://www.googleapis.com/auth/cloud-platform. - https://www.googleapis.com/auth/devstorage.read_write. - https://www.googleapis.com/auth/genomics. zones:. - us-west1-b. timeout: 604800s. startTime: '2018-11-08T14:27:06.604193Z'. name: projects/valis-194104/operations/12097970745380060156. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:9340,availability,operat,operations,9340,rAssignedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:27:06.604193Z'. labels: {}. pipeline:. actions:. - commands:. - -c. - /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project valis-194104 --zones. us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse. entrypoint: bash. environment: {}. flags: []. imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0. labels: {}. mounts: []. name: ''. pidNamespace: ''. portMappings: {}. - commands:. - /bin/sh. - -c. - gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log. entrypoint: ''. environment: {}. flags:. - ALWAYS_RUN. imageUri: google/cloud-sdk:alpine. labels: {}. mounts: []. name: ''. pidNamespace: ''. portMappings: {}. environment: {}. resources:. projectId: valis-194104. regions: []. virtualMachine:. accelerators: []. bootDiskSizeGb: 10. bootImage: projects/cos-cloud/global/images/family/cos-stable. cpuPlatform: ''. disks: []. labels: {}. machineType: n1-standard-1. nvidiaDriverVersion: ''. preemptible: false. serviceAccount:. email: default. scopes:. - https://www.googleapis.com/auth/cloud-platform. - https://www.googleapis.com/auth/devstorage.read_write. - https://www.googleapis.com/auth/genomics. zones:. - us-west1-b. timeout: 604800s. startTime: '2018-11-08T14:27:06.604193Z'. name: projects/valis-194104/operations/12097970745380060156. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:32,deployability,fail,failures,32,"Ran it again, and still hitting failures:. ```. done: true. error:. code: 9. message: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. metadata:. '@type': type.googleapis.com/google.genomics.v2alpha1.Metadata. createTime: '2018-11-08T14:27:06.016940Z'. endTime: '2018-11-08T14:30:59.324697Z'. events:. - description: Worker released. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.WorkerReleasedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:30:59.324697Z'. - description: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.FailedEvent. cause: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. code: FAILED_PRECONDITION. timestamp: '2018-11-08T14:30:58.518326Z'. - description: Stopped running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 2. exitStatus: 0. stderr: ''. timestamp: '2018-11-08T14:30:58.416239Z'. - description: Started running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent. actionId: 2. ipAddress: ''. portMappings: {}. timestamp: '2018-11-08T14:30:55.929647Z'. - description: Unexpected exit status 1 while running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project. valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:97,deployability,fail,failed,97,"Ran it again, and still hitting failures:. ```. done: true. error:. code: 9. message: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. metadata:. '@type': type.googleapis.com/google.genomics.v2alpha1.Metadata. createTime: '2018-11-08T14:27:06.016940Z'. endTime: '2018-11-08T14:30:59.324697Z'. events:. - description: Worker released. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.WorkerReleasedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:30:59.324697Z'. - description: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.FailedEvent. cause: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. code: FAILED_PRECONDITION. timestamp: '2018-11-08T14:30:58.518326Z'. - description: Stopped running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 2. exitStatus: 0. stderr: ''. timestamp: '2018-11-08T14:30:58.416239Z'. - description: Started running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent. actionId: 2. ipAddress: ''. portMappings: {}. timestamp: '2018-11-08T14:30:55.929647Z'. - description: Unexpected exit status 1 while running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project. valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:347,deployability,releas,released,347,"Ran it again, and still hitting failures:. ```. done: true. error:. code: 9. message: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. metadata:. '@type': type.googleapis.com/google.genomics.v2alpha1.Metadata. createTime: '2018-11-08T14:27:06.016940Z'. endTime: '2018-11-08T14:30:59.324697Z'. events:. - description: Worker released. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.WorkerReleasedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:30:59.324697Z'. - description: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.FailedEvent. cause: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. code: FAILED_PRECONDITION. timestamp: '2018-11-08T14:30:58.518326Z'. - description: Stopped running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 2. exitStatus: 0. stderr: ''. timestamp: '2018-11-08T14:30:58.416239Z'. - description: Started running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent. actionId: 2. ipAddress: ''. portMappings: {}. timestamp: '2018-11-08T14:30:55.929647Z'. - description: Unexpected exit status 1 while running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project. valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:459,deployability,pipelin,pipelines-worker-,459,"Ran it again, and still hitting failures:. ```. done: true. error:. code: 9. message: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. metadata:. '@type': type.googleapis.com/google.genomics.v2alpha1.Metadata. createTime: '2018-11-08T14:27:06.016940Z'. endTime: '2018-11-08T14:30:59.324697Z'. events:. - description: Worker released. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.WorkerReleasedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:30:59.324697Z'. - description: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.FailedEvent. cause: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. code: FAILED_PRECONDITION. timestamp: '2018-11-08T14:30:58.518326Z'. - description: Stopped running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 2. exitStatus: 0. stderr: ''. timestamp: '2018-11-08T14:30:58.416239Z'. - description: Started running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent. actionId: 2. ipAddress: ''. portMappings: {}. timestamp: '2018-11-08T14:30:55.929647Z'. - description: Unexpected exit status 1 while running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project. valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:596,deployability,fail,failed,596,"Ran it again, and still hitting failures:. ```. done: true. error:. code: 9. message: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. metadata:. '@type': type.googleapis.com/google.genomics.v2alpha1.Metadata. createTime: '2018-11-08T14:27:06.016940Z'. endTime: '2018-11-08T14:30:59.324697Z'. events:. - description: Worker released. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.WorkerReleasedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:30:59.324697Z'. - description: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.FailedEvent. cause: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. code: FAILED_PRECONDITION. timestamp: '2018-11-08T14:30:58.518326Z'. - description: Stopped running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 2. exitStatus: 0. stderr: ''. timestamp: '2018-11-08T14:30:58.416239Z'. - description: Started running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent. actionId: 2. ipAddress: ''. portMappings: {}. timestamp: '2018-11-08T14:30:55.929647Z'. - description: Unexpected exit status 1 while running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project. valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:721,deployability,Fail,FailedEvent,721,"Ran it again, and still hitting failures:. ```. done: true. error:. code: 9. message: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. metadata:. '@type': type.googleapis.com/google.genomics.v2alpha1.Metadata. createTime: '2018-11-08T14:27:06.016940Z'. endTime: '2018-11-08T14:30:59.324697Z'. events:. - description: Worker released. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.WorkerReleasedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:30:59.324697Z'. - description: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.FailedEvent. cause: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. code: FAILED_PRECONDITION. timestamp: '2018-11-08T14:30:58.518326Z'. - description: Stopped running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 2. exitStatus: 0. stderr: ''. timestamp: '2018-11-08T14:30:58.416239Z'. - description: Started running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent. actionId: 2. ipAddress: ''. portMappings: {}. timestamp: '2018-11-08T14:30:55.929647Z'. - description: Unexpected exit status 1 while running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project. valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:752,deployability,fail,failed,752,"Ran it again, and still hitting failures:. ```. done: true. error:. code: 9. message: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. metadata:. '@type': type.googleapis.com/google.genomics.v2alpha1.Metadata. createTime: '2018-11-08T14:27:06.016940Z'. endTime: '2018-11-08T14:30:59.324697Z'. events:. - description: Worker released. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.WorkerReleasedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:30:59.324697Z'. - description: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.FailedEvent. cause: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. code: FAILED_PRECONDITION. timestamp: '2018-11-08T14:30:58.518326Z'. - description: Stopped running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 2. exitStatus: 0. stderr: ''. timestamp: '2018-11-08T14:30:58.416239Z'. - description: Started running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent. actionId: 2. ipAddress: ''. portMappings: {}. timestamp: '2018-11-08T14:30:55.929647Z'. - description: Unexpected exit status 1 while running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project. valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:946,deployability,log,logs,946,"Ran it again, and still hitting failures:. ```. done: true. error:. code: 9. message: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. metadata:. '@type': type.googleapis.com/google.genomics.v2alpha1.Metadata. createTime: '2018-11-08T14:27:06.016940Z'. endTime: '2018-11-08T14:30:59.324697Z'. events:. - description: Worker released. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.WorkerReleasedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:30:59.324697Z'. - description: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.FailedEvent. cause: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. code: FAILED_PRECONDITION. timestamp: '2018-11-08T14:30:58.518326Z'. - description: Stopped running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 2. exitStatus: 0. stderr: ''. timestamp: '2018-11-08T14:30:58.416239Z'. - description: Started running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent. actionId: 2. ipAddress: ''. portMappings: {}. timestamp: '2018-11-08T14:30:55.929647Z'. - description: Unexpected exit status 1 while running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project. valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:1025,deployability,log,log,1025,"ting failures:. ```. done: true. error:. code: 9. message: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. metadata:. '@type': type.googleapis.com/google.genomics.v2alpha1.Metadata. createTime: '2018-11-08T14:27:06.016940Z'. endTime: '2018-11-08T14:30:59.324697Z'. events:. - description: Worker released. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.WorkerReleasedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:30:59.324697Z'. - description: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.FailedEvent. cause: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. code: FAILED_PRECONDITION. timestamp: '2018-11-08T14:30:58.518326Z'. - description: Stopped running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 2. exitStatus: 0. stderr: ''. timestamp: '2018-11-08T14:30:58.416239Z'. - description: Started running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent. actionId: 2. ipAddress: ''. portMappings: {}. timestamp: '2018-11-08T14:30:55.929647Z'. - description: Unexpected exit status 1 while running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project. valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:1095,deployability,Contain,ContainerStoppedEvent,1095,"ction 1: unexpected exit status 1 was not ignored'. metadata:. '@type': type.googleapis.com/google.genomics.v2alpha1.Metadata. createTime: '2018-11-08T14:27:06.016940Z'. endTime: '2018-11-08T14:30:59.324697Z'. events:. - description: Worker released. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.WorkerReleasedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:30:59.324697Z'. - description: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.FailedEvent. cause: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. code: FAILED_PRECONDITION. timestamp: '2018-11-08T14:30:58.518326Z'. - description: Stopped running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 2. exitStatus: 0. stderr: ''. timestamp: '2018-11-08T14:30:58.416239Z'. - description: Started running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent. actionId: 2. ipAddress: ''. portMappings: {}. timestamp: '2018-11-08T14:30:55.929647Z'. - description: Unexpected exit status 1 while running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project. valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:1264,deployability,log,logs,1264,"016940Z'. endTime: '2018-11-08T14:30:59.324697Z'. events:. - description: Worker released. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.WorkerReleasedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:30:59.324697Z'. - description: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.FailedEvent. cause: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. code: FAILED_PRECONDITION. timestamp: '2018-11-08T14:30:58.518326Z'. - description: Stopped running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 2. exitStatus: 0. stderr: ''. timestamp: '2018-11-08T14:30:58.416239Z'. - description: Started running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent. actionId: 2. ipAddress: ''. portMappings: {}. timestamp: '2018-11-08T14:30:55.929647Z'. - description: Unexpected exit status 1 while running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project. valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcs",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:1343,deployability,log,log,1343,"r released. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.WorkerReleasedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:30:59.324697Z'. - description: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.FailedEvent. cause: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. code: FAILED_PRECONDITION. timestamp: '2018-11-08T14:30:58.518326Z'. - description: Stopped running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 2. exitStatus: 0. stderr: ''. timestamp: '2018-11-08T14:30:58.416239Z'. - description: Started running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent. actionId: 2. ipAddress: ''. portMappings: {}. timestamp: '2018-11-08T14:30:55.929647Z'. - description: Unexpected exit status 1 while running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project. valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.Unexpect",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:1413,deployability,Contain,ContainerStartedEvent,1413,"kerReleasedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:30:59.324697Z'. - description: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.FailedEvent. cause: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. code: FAILED_PRECONDITION. timestamp: '2018-11-08T14:30:58.518326Z'. - description: Stopped running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 2. exitStatus: 0. stderr: ''. timestamp: '2018-11-08T14:30:58.416239Z'. - description: Started running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent. actionId: 2. ipAddress: ''. portMappings: {}. timestamp: '2018-11-08T14:30:55.929647Z'. - description: Unexpected exit status 1 while running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project. valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.UnexpectedExitStatusEvent. actionId: 1. exitStatus: 1. timestamp: '2018-11-08T14:30:55.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:3469,deployability,modul,module,3469," /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse"": exit status 1: /examples_output.tfrecord@""${SHARDS}"".gz\n --reads ""/input-gcsfused-{}/${BAM}""\n --ref ""${INPUT_REF}""\n --task {}\n --regions gs://canis/CNR-data/CDS-canonical.bed"" # ENABLE_FUSE\n']. Traceback (most recent call last):. File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>. run(). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run. _run_make_examples(pipeline_args). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples. _wait_for_results(threads, results). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results. result.get(). File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get. raise self._value. RuntimeError: Job failed with error ""run"": operation ""projects/valis-194104/operations/781565864516461293"" failed: executing pipeline: Execution failed: action 6: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION). details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 1. exitStatus: 1. stderr: |+. /examples_output.tfrecord@""${SHARDS}"".gz\n --reads ""/input-gcsfused-{}/${BAM}""\n --ref ""${INPUT_REF}""\n --task {}\n --regions gs://canis/CNR-data/CDS-canonical.bed"" # ENABLE_FUSE\n']",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:3946,deployability,fail,failed,3946,"_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse"": exit status 1: /examples_output.tfrecord@""${SHARDS}"".gz\n --reads ""/input-gcsfused-{}/${BAM}""\n --ref ""${INPUT_REF}""\n --task {}\n --regions gs://canis/CNR-data/CDS-canonical.bed"" # ENABLE_FUSE\n']. Traceback (most recent call last):. File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>. run(). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run. _run_make_examples(pipeline_args). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples. _wait_for_results(threads, results). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results. result.get(). File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get. raise self._value. RuntimeError: Job failed with error ""run"": operation ""projects/valis-194104/operations/781565864516461293"" failed: executing pipeline: Execution failed: action 6: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION). details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 1. exitStatus: 1. stderr: |+. /examples_output.tfrecord@""${SHARDS}"".gz\n --reads ""/input-gcsfused-{}/${BAM}""\n --ref ""${INPUT_REF}""\n --task {}\n --regions gs://canis/CNR-data/CDS-canonical.bed"" # ENABLE_FUSE\n']. Traceback (most recent call last):. File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>. run(). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run. _run_make_examples(pipeline_args). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples. _wait_for_results(threads, results). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results. result",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:4035,deployability,fail,failed,4035,"/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse"": exit status 1: /examples_output.tfrecord@""${SHARDS}"".gz\n --reads ""/input-gcsfused-{}/${BAM}""\n --ref ""${INPUT_REF}""\n --task {}\n --regions gs://canis/CNR-data/CDS-canonical.bed"" # ENABLE_FUSE\n']. Traceback (most recent call last):. File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>. run(). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run. _run_make_examples(pipeline_args). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples. _wait_for_results(threads, results). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results. result.get(). File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get. raise self._value. RuntimeError: Job failed with error ""run"": operation ""projects/valis-194104/operations/781565864516461293"" failed: executing pipeline: Execution failed: action 6: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION). details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 1. exitStatus: 1. stderr: |+. /examples_output.tfrecord@""${SHARDS}"".gz\n --reads ""/input-gcsfused-{}/${BAM}""\n --ref ""${INPUT_REF}""\n --task {}\n --regions gs://canis/CNR-data/CDS-canonical.bed"" # ENABLE_FUSE\n']. Traceback (most recent call last):. File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>. run(). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run. _run_make_examples(pipeline_args). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples. _wait_for_results(threads, results). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results. result.get(). File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get. raise self._",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:4053,deployability,pipelin,pipeline,4053,"rily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse"": exit status 1: /examples_output.tfrecord@""${SHARDS}"".gz\n --reads ""/input-gcsfused-{}/${BAM}""\n --ref ""${INPUT_REF}""\n --task {}\n --regions gs://canis/CNR-data/CDS-canonical.bed"" # ENABLE_FUSE\n']. Traceback (most recent call last):. File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>. run(). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run. _run_make_examples(pipeline_args). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples. _wait_for_results(threads, results). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results. result.get(). File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get. raise self._value. RuntimeError: Job failed with error ""run"": operation ""projects/valis-194104/operations/781565864516461293"" failed: executing pipeline: Execution failed: action 6: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION). details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 1. exitStatus: 1. stderr: |+. /examples_output.tfrecord@""${SHARDS}"".gz\n --reads ""/input-gcsfused-{}/${BAM}""\n --ref ""${INPUT_REF}""\n --task {}\n --regions gs://canis/CNR-data/CDS-canonical.bed"" # ENABLE_FUSE\n']. Traceback (most recent call last):. File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>. run(). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run. _run_make_examples(pipeline_args). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples. _wait_for_results(threads, results). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results. result.get(). File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get. raise self._value. RuntimeError",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:4073,deployability,fail,failed,4073,"z.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse"": exit status 1: /examples_output.tfrecord@""${SHARDS}"".gz\n --reads ""/input-gcsfused-{}/${BAM}""\n --ref ""${INPUT_REF}""\n --task {}\n --regions gs://canis/CNR-data/CDS-canonical.bed"" # ENABLE_FUSE\n']. Traceback (most recent call last):. File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>. run(). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run. _run_make_examples(pipeline_args). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples. _wait_for_results(threads, results). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results. result.get(). File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get. raise self._value. RuntimeError: Job failed with error ""run"": operation ""projects/valis-194104/operations/781565864516461293"" failed: executing pipeline: Execution failed: action 6: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION). details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 1. exitStatus: 1. stderr: |+. /examples_output.tfrecord@""${SHARDS}"".gz\n --reads ""/input-gcsfused-{}/${BAM}""\n --ref ""${INPUT_REF}""\n --task {}\n --regions gs://canis/CNR-data/CDS-canonical.bed"" # ENABLE_FUSE\n']. Traceback (most recent call last):. File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>. run(). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run. _run_make_examples(pipeline_args). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples. _wait_for_results(threads, results). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results. result.get(). File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get. raise self._value. RuntimeError: Job failed with e",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:4227,deployability,Contain,ContainerStoppedEvent,4227,"ed-{}/${BAM}""\n --ref ""${INPUT_REF}""\n --task {}\n --regions gs://canis/CNR-data/CDS-canonical.bed"" # ENABLE_FUSE\n']. Traceback (most recent call last):. File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>. run(). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run. _run_make_examples(pipeline_args). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples. _wait_for_results(threads, results). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results. result.get(). File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get. raise self._value. RuntimeError: Job failed with error ""run"": operation ""projects/valis-194104/operations/781565864516461293"" failed: executing pipeline: Execution failed: action 6: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION). details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 1. exitStatus: 1. stderr: |+. /examples_output.tfrecord@""${SHARDS}"".gz\n --reads ""/input-gcsfused-{}/${BAM}""\n --ref ""${INPUT_REF}""\n --task {}\n --regions gs://canis/CNR-data/CDS-canonical.bed"" # ENABLE_FUSE\n']. Traceback (most recent call last):. File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>. run(). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run. _run_make_examples(pipeline_args). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples. _wait_for_results(threads, results). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results. result.get(). File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get. raise self._value. RuntimeError: Job failed with error ""run"": operation ""projects/valis-194104/operations/781565864516461293"" failed: executing pipeline: Execution failed: action 6: unexpected exit status 1 was n",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:4586,deployability,modul,module,4586,"ine_args). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples. _wait_for_results(threads, results). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results. result.get(). File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get. raise self._value. RuntimeError: Job failed with error ""run"": operation ""projects/valis-194104/operations/781565864516461293"" failed: executing pipeline: Execution failed: action 6: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION). details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 1. exitStatus: 1. stderr: |+. /examples_output.tfrecord@""${SHARDS}"".gz\n --reads ""/input-gcsfused-{}/${BAM}""\n --ref ""${INPUT_REF}""\n --task {}\n --regions gs://canis/CNR-data/CDS-canonical.bed"" # ENABLE_FUSE\n']. Traceback (most recent call last):. File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>. run(). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run. _run_make_examples(pipeline_args). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples. _wait_for_results(threads, results). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results. result.get(). File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get. raise self._value. RuntimeError: Job failed with error ""run"": operation ""projects/valis-194104/operations/781565864516461293"" failed: executing pipeline: Execution failed: action 6: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION). timestamp: '2018-11-08T14:30:55.372655Z'. - description: Started running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project. valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:5063,deployability,fail,failed,5063,"cution failed: action 6: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION). details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 1. exitStatus: 1. stderr: |+. /examples_output.tfrecord@""${SHARDS}"".gz\n --reads ""/input-gcsfused-{}/${BAM}""\n --ref ""${INPUT_REF}""\n --task {}\n --regions gs://canis/CNR-data/CDS-canonical.bed"" # ENABLE_FUSE\n']. Traceback (most recent call last):. File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>. run(). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run. _run_make_examples(pipeline_args). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples. _wait_for_results(threads, results). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results. result.get(). File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get. raise self._value. RuntimeError: Job failed with error ""run"": operation ""projects/valis-194104/operations/781565864516461293"" failed: executing pipeline: Execution failed: action 6: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION). timestamp: '2018-11-08T14:30:55.372655Z'. - description: Started running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project. valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse"". details:. '@type':",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:5152,deployability,fail,failed,5152,"ITION). details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 1. exitStatus: 1. stderr: |+. /examples_output.tfrecord@""${SHARDS}"".gz\n --reads ""/input-gcsfused-{}/${BAM}""\n --ref ""${INPUT_REF}""\n --task {}\n --regions gs://canis/CNR-data/CDS-canonical.bed"" # ENABLE_FUSE\n']. Traceback (most recent call last):. File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>. run(). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run. _run_make_examples(pipeline_args). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples. _wait_for_results(threads, results). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results. result.get(). File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get. raise self._value. RuntimeError: Job failed with error ""run"": operation ""projects/valis-194104/operations/781565864516461293"" failed: executing pipeline: Execution failed: action 6: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION). timestamp: '2018-11-08T14:30:55.372655Z'. - description: Started running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project. valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent. actionId: 1. ipAddre",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:5170,deployability,pipelin,pipeline,5170,"@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 1. exitStatus: 1. stderr: |+. /examples_output.tfrecord@""${SHARDS}"".gz\n --reads ""/input-gcsfused-{}/${BAM}""\n --ref ""${INPUT_REF}""\n --task {}\n --regions gs://canis/CNR-data/CDS-canonical.bed"" # ENABLE_FUSE\n']. Traceback (most recent call last):. File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>. run(). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run. _run_make_examples(pipeline_args). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples. _wait_for_results(threads, results). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results. result.get(). File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get. raise self._value. RuntimeError: Job failed with error ""run"": operation ""projects/valis-194104/operations/781565864516461293"" failed: executing pipeline: Execution failed: action 6: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION). timestamp: '2018-11-08T14:30:55.372655Z'. - description: Started running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project. valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent. actionId: 1. ipAddress: ''. portMapping",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:5190,deployability,fail,failed,5190,"apis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 1. exitStatus: 1. stderr: |+. /examples_output.tfrecord@""${SHARDS}"".gz\n --reads ""/input-gcsfused-{}/${BAM}""\n --ref ""${INPUT_REF}""\n --task {}\n --regions gs://canis/CNR-data/CDS-canonical.bed"" # ENABLE_FUSE\n']. Traceback (most recent call last):. File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>. run(). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run. _run_make_examples(pipeline_args). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples. _wait_for_results(threads, results). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results. result.get(). File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get. raise self._value. RuntimeError: Job failed with error ""run"": operation ""projects/valis-194104/operations/781565864516461293"" failed: executing pipeline: Execution failed: action 6: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION). timestamp: '2018-11-08T14:30:55.372655Z'. - description: Started running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project. valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent. actionId: 1. ipAddress: ''. portMappings: {}. timestamp: '",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:6112,deployability,Contain,ContainerStartedEvent,6112,"erations/781565864516461293"" failed: executing pipeline: Execution failed: action 6: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION). timestamp: '2018-11-08T14:30:55.372655Z'. - description: Started running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project. valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent. actionId: 1. ipAddress: ''. portMappings: {}. timestamp: '2018-11-08T14:28:16.559791Z'. - description: Stopped pulling ""google/cloud-sdk:alpine"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.PullStoppedEvent. imageUri: google/cloud-sdk:alpine. timestamp: '2018-11-08T14:28:12.427407Z'. - description: Started pulling ""google/cloud-sdk:alpine"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.PullStartedEvent. imageUri: google/cloud-sdk:alpine. timestamp: '2018-11-08T14:28:05.897359Z'. - description: Stopped pulling ""gcr.io/deepvariant-docker/deepvariant_runner:0.7.0"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.PullStoppedEvent. imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0. timestamp: '2018-11-08T14:28:05.747135Z'. - description: Started pulling ""gcr.io/deepvariant-docker/deepvariant_runner:0.7.0"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.PullStartedEvent. imageUri: gcr.io/deepvariant",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:7229,deployability,pipelin,pipelines-worker-,7229,"Stopped pulling ""google/cloud-sdk:alpine"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.PullStoppedEvent. imageUri: google/cloud-sdk:alpine. timestamp: '2018-11-08T14:28:12.427407Z'. - description: Started pulling ""google/cloud-sdk:alpine"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.PullStartedEvent. imageUri: google/cloud-sdk:alpine. timestamp: '2018-11-08T14:28:05.897359Z'. - description: Stopped pulling ""gcr.io/deepvariant-docker/deepvariant_runner:0.7.0"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.PullStoppedEvent. imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0. timestamp: '2018-11-08T14:28:05.747135Z'. - description: Started pulling ""gcr.io/deepvariant-docker/deepvariant_runner:0.7.0"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.PullStartedEvent. imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0. timestamp: '2018-11-08T14:27:34.961215Z'. - description: Worker ""google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7"". assigned in ""us-west1-b"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.WorkerAssignedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:27:06.604193Z'. labels: {}. pipeline:. actions:. - commands:. - -c. - /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project valis-194104 --zones. us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.ge",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:7409,deployability,pipelin,pipelines-worker-,7409,"08T14:28:12.427407Z'. - description: Started pulling ""google/cloud-sdk:alpine"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.PullStartedEvent. imageUri: google/cloud-sdk:alpine. timestamp: '2018-11-08T14:28:05.897359Z'. - description: Stopped pulling ""gcr.io/deepvariant-docker/deepvariant_runner:0.7.0"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.PullStoppedEvent. imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0. timestamp: '2018-11-08T14:28:05.747135Z'. - description: Started pulling ""gcr.io/deepvariant-docker/deepvariant_runner:0.7.0"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.PullStartedEvent. imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0. timestamp: '2018-11-08T14:27:34.961215Z'. - description: Worker ""google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7"". assigned in ""us-west1-b"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.WorkerAssignedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:27:06.604193Z'. labels: {}. pipeline:. actions:. - commands:. - -c. - /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project valis-194104 --zones. us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse. entrypoint: bash. environment: {}. flags: []. imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0. labels: {}. mounts: []. name: ''. pidNamespace",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:7532,deployability,pipelin,pipeline,7532,"/google.genomics.v2alpha1.PullStartedEvent. imageUri: google/cloud-sdk:alpine. timestamp: '2018-11-08T14:28:05.897359Z'. - description: Stopped pulling ""gcr.io/deepvariant-docker/deepvariant_runner:0.7.0"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.PullStoppedEvent. imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0. timestamp: '2018-11-08T14:28:05.747135Z'. - description: Started pulling ""gcr.io/deepvariant-docker/deepvariant_runner:0.7.0"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.PullStartedEvent. imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0. timestamp: '2018-11-08T14:27:34.961215Z'. - description: Worker ""google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7"". assigned in ""us-west1-b"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.WorkerAssignedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:27:06.604193Z'. labels: {}. pipeline:. actions:. - commands:. - -c. - /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project valis-194104 --zones. us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse. entrypoint: bash. environment: {}. flags: []. imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0. labels: {}. mounts: []. name: ''. pidNamespace: ''. portMappings: {}. - commands:. - /bin/sh. - -c. - gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_vari",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:8495,deployability,log,logs,8495,rAssignedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:27:06.604193Z'. labels: {}. pipeline:. actions:. - commands:. - -c. - /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project valis-194104 --zones. us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse. entrypoint: bash. environment: {}. flags: []. imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0. labels: {}. mounts: []. name: ''. pidNamespace: ''. portMappings: {}. - commands:. - /bin/sh. - -c. - gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log. entrypoint: ''. environment: {}. flags:. - ALWAYS_RUN. imageUri: google/cloud-sdk:alpine. labels: {}. mounts: []. name: ''. pidNamespace: ''. portMappings: {}. environment: {}. resources:. projectId: valis-194104. regions: []. virtualMachine:. accelerators: []. bootDiskSizeGb: 10. bootImage: projects/cos-cloud/global/images/family/cos-stable. cpuPlatform: ''. disks: []. labels: {}. machineType: n1-standard-1. nvidiaDriverVersion: ''. preemptible: false. serviceAccount:. email: default. scopes:. - https://www.googleapis.com/auth/cloud-platform. - https://www.googleapis.com/auth/devstorage.read_write. - https://www.googleapis.com/auth/genomics. zones:. - us-west1-b. timeout: 604800s. startTime: '2018-11-08T14:27:06.604193Z'. name: projects/valis-194104/operations/12097970745380060156. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:8574,deployability,log,log,8574,rAssignedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:27:06.604193Z'. labels: {}. pipeline:. actions:. - commands:. - -c. - /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project valis-194104 --zones. us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse. entrypoint: bash. environment: {}. flags: []. imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0. labels: {}. mounts: []. name: ''. pidNamespace: ''. portMappings: {}. - commands:. - /bin/sh. - -c. - gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log. entrypoint: ''. environment: {}. flags:. - ALWAYS_RUN. imageUri: google/cloud-sdk:alpine. labels: {}. mounts: []. name: ''. pidNamespace: ''. portMappings: {}. environment: {}. resources:. projectId: valis-194104. regions: []. virtualMachine:. accelerators: []. bootDiskSizeGb: 10. bootImage: projects/cos-cloud/global/images/family/cos-stable. cpuPlatform: ''. disks: []. labels: {}. machineType: n1-standard-1. nvidiaDriverVersion: ''. preemptible: false. serviceAccount:. email: default. scopes:. - https://www.googleapis.com/auth/cloud-platform. - https://www.googleapis.com/auth/devstorage.read_write. - https://www.googleapis.com/auth/genomics. zones:. - us-west1-b. timeout: 604800s. startTime: '2018-11-08T14:27:06.604193Z'. name: projects/valis-194104/operations/12097970745380060156. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:8756,deployability,resourc,resources,8756,rAssignedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:27:06.604193Z'. labels: {}. pipeline:. actions:. - commands:. - -c. - /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project valis-194104 --zones. us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse. entrypoint: bash. environment: {}. flags: []. imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0. labels: {}. mounts: []. name: ''. pidNamespace: ''. portMappings: {}. - commands:. - /bin/sh. - -c. - gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log. entrypoint: ''. environment: {}. flags:. - ALWAYS_RUN. imageUri: google/cloud-sdk:alpine. labels: {}. mounts: []. name: ''. pidNamespace: ''. portMappings: {}. environment: {}. resources:. projectId: valis-194104. regions: []. virtualMachine:. accelerators: []. bootDiskSizeGb: 10. bootImage: projects/cos-cloud/global/images/family/cos-stable. cpuPlatform: ''. disks: []. labels: {}. machineType: n1-standard-1. nvidiaDriverVersion: ''. preemptible: false. serviceAccount:. email: default. scopes:. - https://www.googleapis.com/auth/cloud-platform. - https://www.googleapis.com/auth/devstorage.read_write. - https://www.googleapis.com/auth/genomics. zones:. - us-west1-b. timeout: 604800s. startTime: '2018-11-08T14:27:06.604193Z'. name: projects/valis-194104/operations/12097970745380060156. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:9037,deployability,servic,serviceAccount,9037,rAssignedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:27:06.604193Z'. labels: {}. pipeline:. actions:. - commands:. - -c. - /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project valis-194104 --zones. us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse. entrypoint: bash. environment: {}. flags: []. imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0. labels: {}. mounts: []. name: ''. pidNamespace: ''. portMappings: {}. - commands:. - /bin/sh. - -c. - gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log. entrypoint: ''. environment: {}. flags:. - ALWAYS_RUN. imageUri: google/cloud-sdk:alpine. labels: {}. mounts: []. name: ''. pidNamespace: ''. portMappings: {}. environment: {}. resources:. projectId: valis-194104. regions: []. virtualMachine:. accelerators: []. bootDiskSizeGb: 10. bootImage: projects/cos-cloud/global/images/family/cos-stable. cpuPlatform: ''. disks: []. labels: {}. machineType: n1-standard-1. nvidiaDriverVersion: ''. preemptible: false. serviceAccount:. email: default. scopes:. - https://www.googleapis.com/auth/cloud-platform. - https://www.googleapis.com/auth/devstorage.read_write. - https://www.googleapis.com/auth/genomics. zones:. - us-west1-b. timeout: 604800s. startTime: '2018-11-08T14:27:06.604193Z'. name: projects/valis-194104/operations/12097970745380060156. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:1844,energy efficiency,model,model,1844,"mp: '2018-11-08T14:30:58.518326Z'. - description: Stopped running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 2. exitStatus: 0. stderr: ''. timestamp: '2018-11-08T14:30:58.416239Z'. - description: Started running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent. actionId: 2. ipAddress: ''. portMappings: {}. timestamp: '2018-11-08T14:30:55.929647Z'. - description: Unexpected exit status 1 while running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project. valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.UnexpectedExitStatusEvent. actionId: 1. exitStatus: 1. timestamp: '2018-11-08T14:30:55.434175Z'. - description: |-. Stopped running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions gs://ca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:1868,energy efficiency,model,models,1868,".518326Z'. - description: Stopped running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 2. exitStatus: 0. stderr: ''. timestamp: '2018-11-08T14:30:58.416239Z'. - description: Started running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent. actionId: 2. ipAddress: ''. portMappings: {}. timestamp: '2018-11-08T14:30:55.929647Z'. - description: Unexpected exit status 1 while running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project. valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.UnexpectedExitStatusEvent. actionId: 1. exitStatus: 1. timestamp: '2018-11-08T14:30:55.434175Z'. - description: |-. Stopped running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions gs://canis/CNR-data/CDS-canonic",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:2733,energy efficiency,model,model,2733,"-outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.UnexpectedExitStatusEvent. actionId: 1. exitStatus: 1. timestamp: '2018-11-08T14:30:55.434175Z'. - description: |-. Stopped running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse"": exit status 1: /examples_output.tfrecord@""${SHARDS}"".gz\n --reads ""/input-gcsfused-{}/${BAM}""\n --ref ""${INPUT_REF}""\n --task {}\n --regions gs://canis/CNR-data/CDS-canonical.bed"" # ENABLE_FUSE\n']. Traceback (most recent call last):. File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>. run(). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run. _run_make_examples(pipeline_args). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples. _wait_for_results(threads, results). File",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:2756,energy efficiency,model,models,2756,"R-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.UnexpectedExitStatusEvent. actionId: 1. exitStatus: 1. timestamp: '2018-11-08T14:30:55.434175Z'. - description: |-. Stopped running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse"": exit status 1: /examples_output.tfrecord@""${SHARDS}"".gz\n --reads ""/input-gcsfused-{}/${BAM}""\n --ref ""${INPUT_REF}""\n --task {}\n --regions gs://canis/CNR-data/CDS-canonical.bed"" # ENABLE_FUSE\n']. Traceback (most recent call last):. File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>. run(). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run. _run_make_examples(pipeline_args). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples. _wait_for_results(threads, results). File ""/opt/deepvariant_runn",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:5619,energy efficiency,model,model,5619,"nt_runner/src/gcp_deepvariant_runner.py"", line 845, in run. _run_make_examples(pipeline_args). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples. _wait_for_results(threads, results). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results. result.get(). File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get. raise self._value. RuntimeError: Job failed with error ""run"": operation ""projects/valis-194104/operations/781565864516461293"" failed: executing pipeline: Execution failed: action 6: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION). timestamp: '2018-11-08T14:30:55.372655Z'. - description: Started running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project. valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent. actionId: 1. ipAddress: ''. portMappings: {}. timestamp: '2018-11-08T14:28:16.559791Z'. - description: Stopped pulling ""google/cloud-sdk:alpine"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.PullStoppedEvent. imageUri: google/cloud-sdk:alpine. timestamp: '2018-11-08T14:28:12.427407Z'. - description: Started pulling ""google/cloud-sdk:alpine"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.PullStartedEvent. imageUri: google/cloud-sdk:alpine. timesta",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:5643,energy efficiency,model,models,5643,"riant_runner.py"", line 845, in run. _run_make_examples(pipeline_args). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples. _wait_for_results(threads, results). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results. result.get(). File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get. raise self._value. RuntimeError: Job failed with error ""run"": operation ""projects/valis-194104/operations/781565864516461293"" failed: executing pipeline: Execution failed: action 6: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION). timestamp: '2018-11-08T14:30:55.372655Z'. - description: Started running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project. valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent. actionId: 1. ipAddress: ''. portMappings: {}. timestamp: '2018-11-08T14:28:16.559791Z'. - description: Stopped pulling ""google/cloud-sdk:alpine"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.PullStoppedEvent. imageUri: google/cloud-sdk:alpine. timestamp: '2018-11-08T14:28:12.427407Z'. - description: Started pulling ""google/cloud-sdk:alpine"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.PullStartedEvent. imageUri: google/cloud-sdk:alpine. timestamp: '2018-11-08T14:28:05",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:6262,energy efficiency,cloud,cloud-sdk,6262,"ECONDITION). timestamp: '2018-11-08T14:30:55.372655Z'. - description: Started running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project. valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent. actionId: 1. ipAddress: ''. portMappings: {}. timestamp: '2018-11-08T14:28:16.559791Z'. - description: Stopped pulling ""google/cloud-sdk:alpine"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.PullStoppedEvent. imageUri: google/cloud-sdk:alpine. timestamp: '2018-11-08T14:28:12.427407Z'. - description: Started pulling ""google/cloud-sdk:alpine"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.PullStartedEvent. imageUri: google/cloud-sdk:alpine. timestamp: '2018-11-08T14:28:05.897359Z'. - description: Stopped pulling ""gcr.io/deepvariant-docker/deepvariant_runner:0.7.0"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.PullStoppedEvent. imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0. timestamp: '2018-11-08T14:28:05.747135Z'. - description: Started pulling ""gcr.io/deepvariant-docker/deepvariant_runner:0.7.0"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.PullStartedEvent. imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0. timestamp: '2018-11-08T14:27:34.961215Z'. - description: Worker ""google-pipelines-worker-4b16fd95b691baddc54b0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:6380,energy efficiency,cloud,cloud-sdk,6380,"gcp_deepvariant_runner --project. valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent. actionId: 1. ipAddress: ''. portMappings: {}. timestamp: '2018-11-08T14:28:16.559791Z'. - description: Stopped pulling ""google/cloud-sdk:alpine"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.PullStoppedEvent. imageUri: google/cloud-sdk:alpine. timestamp: '2018-11-08T14:28:12.427407Z'. - description: Started pulling ""google/cloud-sdk:alpine"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.PullStartedEvent. imageUri: google/cloud-sdk:alpine. timestamp: '2018-11-08T14:28:05.897359Z'. - description: Stopped pulling ""gcr.io/deepvariant-docker/deepvariant_runner:0.7.0"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.PullStoppedEvent. imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0. timestamp: '2018-11-08T14:28:05.747135Z'. - description: Started pulling ""gcr.io/deepvariant-docker/deepvariant_runner:0.7.0"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.PullStartedEvent. imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0. timestamp: '2018-11-08T14:27:34.961215Z'. - description: Worker ""google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7"". assigned in ""us-west1-b"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.WorkerAssigned",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:6479,energy efficiency,cloud,cloud-sdk,6479,"-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent. actionId: 1. ipAddress: ''. portMappings: {}. timestamp: '2018-11-08T14:28:16.559791Z'. - description: Stopped pulling ""google/cloud-sdk:alpine"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.PullStoppedEvent. imageUri: google/cloud-sdk:alpine. timestamp: '2018-11-08T14:28:12.427407Z'. - description: Started pulling ""google/cloud-sdk:alpine"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.PullStartedEvent. imageUri: google/cloud-sdk:alpine. timestamp: '2018-11-08T14:28:05.897359Z'. - description: Stopped pulling ""gcr.io/deepvariant-docker/deepvariant_runner:0.7.0"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.PullStoppedEvent. imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0. timestamp: '2018-11-08T14:28:05.747135Z'. - description: Started pulling ""gcr.io/deepvariant-docker/deepvariant_runner:0.7.0"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.PullStartedEvent. imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0. timestamp: '2018-11-08T14:27:34.961215Z'. - description: Worker ""google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7"". assigned in ""us-west1-b"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.WorkerAssignedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timest",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:6597,energy efficiency,cloud,cloud-sdk,6597,"_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent. actionId: 1. ipAddress: ''. portMappings: {}. timestamp: '2018-11-08T14:28:16.559791Z'. - description: Stopped pulling ""google/cloud-sdk:alpine"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.PullStoppedEvent. imageUri: google/cloud-sdk:alpine. timestamp: '2018-11-08T14:28:12.427407Z'. - description: Started pulling ""google/cloud-sdk:alpine"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.PullStartedEvent. imageUri: google/cloud-sdk:alpine. timestamp: '2018-11-08T14:28:05.897359Z'. - description: Stopped pulling ""gcr.io/deepvariant-docker/deepvariant_runner:0.7.0"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.PullStoppedEvent. imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0. timestamp: '2018-11-08T14:28:05.747135Z'. - description: Started pulling ""gcr.io/deepvariant-docker/deepvariant_runner:0.7.0"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.PullStartedEvent. imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0. timestamp: '2018-11-08T14:27:34.961215Z'. - description: Worker ""google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7"". assigned in ""us-west1-b"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.WorkerAssignedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:27:06.604193Z'. labels: {}. pipeline:. actions:. - commands:. - -c. - /opt/deepvariant_runner/bin/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:7836,energy efficiency,model,model,7836,"o/deepvariant-docker/deepvariant_runner:0.7.0. timestamp: '2018-11-08T14:28:05.747135Z'. - description: Started pulling ""gcr.io/deepvariant-docker/deepvariant_runner:0.7.0"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.PullStartedEvent. imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0. timestamp: '2018-11-08T14:27:34.961215Z'. - description: Worker ""google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7"". assigned in ""us-west1-b"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.WorkerAssignedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:27:06.604193Z'. labels: {}. pipeline:. actions:. - commands:. - -c. - /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project valis-194104 --zones. us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse. entrypoint: bash. environment: {}. flags: []. imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0. labels: {}. mounts: []. name: ''. pidNamespace: ''. portMappings: {}. - commands:. - /bin/sh. - -c. - gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log. entrypoint: ''. environment: {}. flags:. - ALWAYS_RUN. imageUri: google/cloud-sdk:alpine. labels: {}. mounts: []. name: ''. pidNamespace: ''. portMappings: {}. environment: {}. resources:. projectId: valis-194104. regions: []. virtualMachine:. accelerators: []",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:7860,energy efficiency,model,models,7860,"pvariant_runner:0.7.0. timestamp: '2018-11-08T14:28:05.747135Z'. - description: Started pulling ""gcr.io/deepvariant-docker/deepvariant_runner:0.7.0"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.PullStartedEvent. imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0. timestamp: '2018-11-08T14:27:34.961215Z'. - description: Worker ""google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7"". assigned in ""us-west1-b"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.WorkerAssignedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:27:06.604193Z'. labels: {}. pipeline:. actions:. - commands:. - -c. - /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project valis-194104 --zones. us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse. entrypoint: bash. environment: {}. flags: []. imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0. labels: {}. mounts: []. name: ''. pidNamespace: ''. portMappings: {}. - commands:. - /bin/sh. - -c. - gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log. entrypoint: ''. environment: {}. flags:. - ALWAYS_RUN. imageUri: google/cloud-sdk:alpine. labels: {}. mounts: []. name: ''. pidNamespace: ''. portMappings: {}. environment: {}. resources:. projectId: valis-194104. regions: []. virtualMachine:. accelerators: []. bootDiskSizeGb: 10. bo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:8651,energy efficiency,cloud,cloud-sdk,8651,rAssignedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:27:06.604193Z'. labels: {}. pipeline:. actions:. - commands:. - -c. - /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project valis-194104 --zones. us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse. entrypoint: bash. environment: {}. flags: []. imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0. labels: {}. mounts: []. name: ''. pidNamespace: ''. portMappings: {}. - commands:. - /bin/sh. - -c. - gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log. entrypoint: ''. environment: {}. flags:. - ALWAYS_RUN. imageUri: google/cloud-sdk:alpine. labels: {}. mounts: []. name: ''. pidNamespace: ''. portMappings: {}. environment: {}. resources:. projectId: valis-194104. regions: []. virtualMachine:. accelerators: []. bootDiskSizeGb: 10. bootImage: projects/cos-cloud/global/images/family/cos-stable. cpuPlatform: ''. disks: []. labels: {}. machineType: n1-standard-1. nvidiaDriverVersion: ''. preemptible: false. serviceAccount:. email: default. scopes:. - https://www.googleapis.com/auth/cloud-platform. - https://www.googleapis.com/auth/devstorage.read_write. - https://www.googleapis.com/auth/genomics. zones:. - us-west1-b. timeout: 604800s. startTime: '2018-11-08T14:27:06.604193Z'. name: projects/valis-194104/operations/12097970745380060156. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:8756,energy efficiency,resourc,resources,8756,rAssignedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:27:06.604193Z'. labels: {}. pipeline:. actions:. - commands:. - -c. - /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project valis-194104 --zones. us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse. entrypoint: bash. environment: {}. flags: []. imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0. labels: {}. mounts: []. name: ''. pidNamespace: ''. portMappings: {}. - commands:. - /bin/sh. - -c. - gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log. entrypoint: ''. environment: {}. flags:. - ALWAYS_RUN. imageUri: google/cloud-sdk:alpine. labels: {}. mounts: []. name: ''. pidNamespace: ''. portMappings: {}. environment: {}. resources:. projectId: valis-194104. regions: []. virtualMachine:. accelerators: []. bootDiskSizeGb: 10. bootImage: projects/cos-cloud/global/images/family/cos-stable. cpuPlatform: ''. disks: []. labels: {}. machineType: n1-standard-1. nvidiaDriverVersion: ''. preemptible: false. serviceAccount:. email: default. scopes:. - https://www.googleapis.com/auth/cloud-platform. - https://www.googleapis.com/auth/devstorage.read_write. - https://www.googleapis.com/auth/genomics. zones:. - us-west1-b. timeout: 604800s. startTime: '2018-11-08T14:27:06.604193Z'. name: projects/valis-194104/operations/12097970745380060156. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:8885,energy efficiency,cloud,cloud,8885,rAssignedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:27:06.604193Z'. labels: {}. pipeline:. actions:. - commands:. - -c. - /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project valis-194104 --zones. us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse. entrypoint: bash. environment: {}. flags: []. imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0. labels: {}. mounts: []. name: ''. pidNamespace: ''. portMappings: {}. - commands:. - /bin/sh. - -c. - gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log. entrypoint: ''. environment: {}. flags:. - ALWAYS_RUN. imageUri: google/cloud-sdk:alpine. labels: {}. mounts: []. name: ''. pidNamespace: ''. portMappings: {}. environment: {}. resources:. projectId: valis-194104. regions: []. virtualMachine:. accelerators: []. bootDiskSizeGb: 10. bootImage: projects/cos-cloud/global/images/family/cos-stable. cpuPlatform: ''. disks: []. labels: {}. machineType: n1-standard-1. nvidiaDriverVersion: ''. preemptible: false. serviceAccount:. email: default. scopes:. - https://www.googleapis.com/auth/cloud-platform. - https://www.googleapis.com/auth/devstorage.read_write. - https://www.googleapis.com/auth/genomics. zones:. - us-west1-b. timeout: 604800s. startTime: '2018-11-08T14:27:06.604193Z'. name: projects/valis-194104/operations/12097970745380060156. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:8924,energy efficiency,cpu,cpuPlatform,8924,rAssignedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:27:06.604193Z'. labels: {}. pipeline:. actions:. - commands:. - -c. - /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project valis-194104 --zones. us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse. entrypoint: bash. environment: {}. flags: []. imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0. labels: {}. mounts: []. name: ''. pidNamespace: ''. portMappings: {}. - commands:. - /bin/sh. - -c. - gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log. entrypoint: ''. environment: {}. flags:. - ALWAYS_RUN. imageUri: google/cloud-sdk:alpine. labels: {}. mounts: []. name: ''. pidNamespace: ''. portMappings: {}. environment: {}. resources:. projectId: valis-194104. regions: []. virtualMachine:. accelerators: []. bootDiskSizeGb: 10. bootImage: projects/cos-cloud/global/images/family/cos-stable. cpuPlatform: ''. disks: []. labels: {}. machineType: n1-standard-1. nvidiaDriverVersion: ''. preemptible: false. serviceAccount:. email: default. scopes:. - https://www.googleapis.com/auth/cloud-platform. - https://www.googleapis.com/auth/devstorage.read_write. - https://www.googleapis.com/auth/genomics. zones:. - us-west1-b. timeout: 604800s. startTime: '2018-11-08T14:27:06.604193Z'. name: projects/valis-194104/operations/12097970745380060156. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:9113,energy efficiency,cloud,cloud-platform,9113,rAssignedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:27:06.604193Z'. labels: {}. pipeline:. actions:. - commands:. - -c. - /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project valis-194104 --zones. us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse. entrypoint: bash. environment: {}. flags: []. imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0. labels: {}. mounts: []. name: ''. pidNamespace: ''. portMappings: {}. - commands:. - /bin/sh. - -c. - gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log. entrypoint: ''. environment: {}. flags:. - ALWAYS_RUN. imageUri: google/cloud-sdk:alpine. labels: {}. mounts: []. name: ''. pidNamespace: ''. portMappings: {}. environment: {}. resources:. projectId: valis-194104. regions: []. virtualMachine:. accelerators: []. bootDiskSizeGb: 10. bootImage: projects/cos-cloud/global/images/family/cos-stable. cpuPlatform: ''. disks: []. labels: {}. machineType: n1-standard-1. nvidiaDriverVersion: ''. preemptible: false. serviceAccount:. email: default. scopes:. - https://www.googleapis.com/auth/cloud-platform. - https://www.googleapis.com/auth/devstorage.read_write. - https://www.googleapis.com/auth/genomics. zones:. - us-west1-b. timeout: 604800s. startTime: '2018-11-08T14:27:06.604193Z'. name: projects/valis-194104/operations/12097970745380060156. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:77,integrability,messag,message,77,"Ran it again, and still hitting failures:. ```. done: true. error:. code: 9. message: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. metadata:. '@type': type.googleapis.com/google.genomics.v2alpha1.Metadata. createTime: '2018-11-08T14:27:06.016940Z'. endTime: '2018-11-08T14:30:59.324697Z'. events:. - description: Worker released. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.WorkerReleasedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:30:59.324697Z'. - description: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.FailedEvent. cause: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. code: FAILED_PRECONDITION. timestamp: '2018-11-08T14:30:58.518326Z'. - description: Stopped running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 2. exitStatus: 0. stderr: ''. timestamp: '2018-11-08T14:30:58.416239Z'. - description: Started running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent. actionId: 2. ipAddress: ''. portMappings: {}. timestamp: '2018-11-08T14:30:55.929647Z'. - description: Unexpected exit status 1 while running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project. valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:316,integrability,event,events,316,"Ran it again, and still hitting failures:. ```. done: true. error:. code: 9. message: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. metadata:. '@type': type.googleapis.com/google.genomics.v2alpha1.Metadata. createTime: '2018-11-08T14:27:06.016940Z'. endTime: '2018-11-08T14:30:59.324697Z'. events:. - description: Worker released. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.WorkerReleasedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:30:59.324697Z'. - description: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.FailedEvent. cause: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. code: FAILED_PRECONDITION. timestamp: '2018-11-08T14:30:58.518326Z'. - description: Stopped running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 2. exitStatus: 0. stderr: ''. timestamp: '2018-11-08T14:30:58.416239Z'. - description: Started running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent. actionId: 2. ipAddress: ''. portMappings: {}. timestamp: '2018-11-08T14:30:55.929647Z'. - description: Unexpected exit status 1 while running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project. valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:459,integrability,pipelin,pipelines-worker-,459,"Ran it again, and still hitting failures:. ```. done: true. error:. code: 9. message: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. metadata:. '@type': type.googleapis.com/google.genomics.v2alpha1.Metadata. createTime: '2018-11-08T14:27:06.016940Z'. endTime: '2018-11-08T14:30:59.324697Z'. events:. - description: Worker released. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.WorkerReleasedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:30:59.324697Z'. - description: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.FailedEvent. cause: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. code: FAILED_PRECONDITION. timestamp: '2018-11-08T14:30:58.518326Z'. - description: Stopped running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 2. exitStatus: 0. stderr: ''. timestamp: '2018-11-08T14:30:58.416239Z'. - description: Started running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent. actionId: 2. ipAddress: ''. portMappings: {}. timestamp: '2018-11-08T14:30:55.929647Z'. - description: Unexpected exit status 1 while running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project. valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:4053,integrability,pipelin,pipeline,4053,"rily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse"": exit status 1: /examples_output.tfrecord@""${SHARDS}"".gz\n --reads ""/input-gcsfused-{}/${BAM}""\n --ref ""${INPUT_REF}""\n --task {}\n --regions gs://canis/CNR-data/CDS-canonical.bed"" # ENABLE_FUSE\n']. Traceback (most recent call last):. File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>. run(). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run. _run_make_examples(pipeline_args). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples. _wait_for_results(threads, results). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results. result.get(). File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get. raise self._value. RuntimeError: Job failed with error ""run"": operation ""projects/valis-194104/operations/781565864516461293"" failed: executing pipeline: Execution failed: action 6: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION). details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 1. exitStatus: 1. stderr: |+. /examples_output.tfrecord@""${SHARDS}"".gz\n --reads ""/input-gcsfused-{}/${BAM}""\n --ref ""${INPUT_REF}""\n --task {}\n --regions gs://canis/CNR-data/CDS-canonical.bed"" # ENABLE_FUSE\n']. Traceback (most recent call last):. File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>. run(). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run. _run_make_examples(pipeline_args). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples. _wait_for_results(threads, results). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results. result.get(). File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get. raise self._value. RuntimeError",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:5170,integrability,pipelin,pipeline,5170,"@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 1. exitStatus: 1. stderr: |+. /examples_output.tfrecord@""${SHARDS}"".gz\n --reads ""/input-gcsfused-{}/${BAM}""\n --ref ""${INPUT_REF}""\n --task {}\n --regions gs://canis/CNR-data/CDS-canonical.bed"" # ENABLE_FUSE\n']. Traceback (most recent call last):. File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>. run(). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run. _run_make_examples(pipeline_args). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples. _wait_for_results(threads, results). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results. result.get(). File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get. raise self._value. RuntimeError: Job failed with error ""run"": operation ""projects/valis-194104/operations/781565864516461293"" failed: executing pipeline: Execution failed: action 6: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION). timestamp: '2018-11-08T14:30:55.372655Z'. - description: Started running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project. valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent. actionId: 1. ipAddress: ''. portMapping",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:7229,integrability,pipelin,pipelines-worker-,7229,"Stopped pulling ""google/cloud-sdk:alpine"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.PullStoppedEvent. imageUri: google/cloud-sdk:alpine. timestamp: '2018-11-08T14:28:12.427407Z'. - description: Started pulling ""google/cloud-sdk:alpine"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.PullStartedEvent. imageUri: google/cloud-sdk:alpine. timestamp: '2018-11-08T14:28:05.897359Z'. - description: Stopped pulling ""gcr.io/deepvariant-docker/deepvariant_runner:0.7.0"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.PullStoppedEvent. imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0. timestamp: '2018-11-08T14:28:05.747135Z'. - description: Started pulling ""gcr.io/deepvariant-docker/deepvariant_runner:0.7.0"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.PullStartedEvent. imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0. timestamp: '2018-11-08T14:27:34.961215Z'. - description: Worker ""google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7"". assigned in ""us-west1-b"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.WorkerAssignedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:27:06.604193Z'. labels: {}. pipeline:. actions:. - commands:. - -c. - /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project valis-194104 --zones. us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.ge",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:7409,integrability,pipelin,pipelines-worker-,7409,"08T14:28:12.427407Z'. - description: Started pulling ""google/cloud-sdk:alpine"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.PullStartedEvent. imageUri: google/cloud-sdk:alpine. timestamp: '2018-11-08T14:28:05.897359Z'. - description: Stopped pulling ""gcr.io/deepvariant-docker/deepvariant_runner:0.7.0"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.PullStoppedEvent. imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0. timestamp: '2018-11-08T14:28:05.747135Z'. - description: Started pulling ""gcr.io/deepvariant-docker/deepvariant_runner:0.7.0"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.PullStartedEvent. imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0. timestamp: '2018-11-08T14:27:34.961215Z'. - description: Worker ""google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7"". assigned in ""us-west1-b"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.WorkerAssignedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:27:06.604193Z'. labels: {}. pipeline:. actions:. - commands:. - -c. - /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project valis-194104 --zones. us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse. entrypoint: bash. environment: {}. flags: []. imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0. labels: {}. mounts: []. name: ''. pidNamespace",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:7532,integrability,pipelin,pipeline,7532,"/google.genomics.v2alpha1.PullStartedEvent. imageUri: google/cloud-sdk:alpine. timestamp: '2018-11-08T14:28:05.897359Z'. - description: Stopped pulling ""gcr.io/deepvariant-docker/deepvariant_runner:0.7.0"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.PullStoppedEvent. imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0. timestamp: '2018-11-08T14:28:05.747135Z'. - description: Started pulling ""gcr.io/deepvariant-docker/deepvariant_runner:0.7.0"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.PullStartedEvent. imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0. timestamp: '2018-11-08T14:27:34.961215Z'. - description: Worker ""google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7"". assigned in ""us-west1-b"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.WorkerAssignedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:27:06.604193Z'. labels: {}. pipeline:. actions:. - commands:. - -c. - /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project valis-194104 --zones. us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse. entrypoint: bash. environment: {}. flags: []. imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0. labels: {}. mounts: []. name: ''. pidNamespace: ''. portMappings: {}. - commands:. - /bin/sh. - -c. - gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_vari",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:9037,integrability,servic,serviceAccount,9037,rAssignedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:27:06.604193Z'. labels: {}. pipeline:. actions:. - commands:. - -c. - /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project valis-194104 --zones. us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse. entrypoint: bash. environment: {}. flags: []. imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0. labels: {}. mounts: []. name: ''. pidNamespace: ''. portMappings: {}. - commands:. - /bin/sh. - -c. - gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log. entrypoint: ''. environment: {}. flags:. - ALWAYS_RUN. imageUri: google/cloud-sdk:alpine. labels: {}. mounts: []. name: ''. pidNamespace: ''. portMappings: {}. environment: {}. resources:. projectId: valis-194104. regions: []. virtualMachine:. accelerators: []. bootDiskSizeGb: 10. bootImage: projects/cos-cloud/global/images/family/cos-stable. cpuPlatform: ''. disks: []. labels: {}. machineType: n1-standard-1. nvidiaDriverVersion: ''. preemptible: false. serviceAccount:. email: default. scopes:. - https://www.googleapis.com/auth/cloud-platform. - https://www.googleapis.com/auth/devstorage.read_write. - https://www.googleapis.com/auth/genomics. zones:. - us-west1-b. timeout: 604800s. startTime: '2018-11-08T14:27:06.604193Z'. name: projects/valis-194104/operations/12097970745380060156. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:77,interoperability,messag,message,77,"Ran it again, and still hitting failures:. ```. done: true. error:. code: 9. message: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. metadata:. '@type': type.googleapis.com/google.genomics.v2alpha1.Metadata. createTime: '2018-11-08T14:27:06.016940Z'. endTime: '2018-11-08T14:30:59.324697Z'. events:. - description: Worker released. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.WorkerReleasedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:30:59.324697Z'. - description: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.FailedEvent. cause: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. code: FAILED_PRECONDITION. timestamp: '2018-11-08T14:30:58.518326Z'. - description: Stopped running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 2. exitStatus: 0. stderr: ''. timestamp: '2018-11-08T14:30:58.416239Z'. - description: Started running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent. actionId: 2. ipAddress: ''. portMappings: {}. timestamp: '2018-11-08T14:30:55.929647Z'. - description: Unexpected exit status 1 while running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project. valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:8980,interoperability,standard,standard-,8980,rAssignedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:27:06.604193Z'. labels: {}. pipeline:. actions:. - commands:. - -c. - /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project valis-194104 --zones. us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse. entrypoint: bash. environment: {}. flags: []. imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0. labels: {}. mounts: []. name: ''. pidNamespace: ''. portMappings: {}. - commands:. - /bin/sh. - -c. - gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log. entrypoint: ''. environment: {}. flags:. - ALWAYS_RUN. imageUri: google/cloud-sdk:alpine. labels: {}. mounts: []. name: ''. pidNamespace: ''. portMappings: {}. environment: {}. resources:. projectId: valis-194104. regions: []. virtualMachine:. accelerators: []. bootDiskSizeGb: 10. bootImage: projects/cos-cloud/global/images/family/cos-stable. cpuPlatform: ''. disks: []. labels: {}. machineType: n1-standard-1. nvidiaDriverVersion: ''. preemptible: false. serviceAccount:. email: default. scopes:. - https://www.googleapis.com/auth/cloud-platform. - https://www.googleapis.com/auth/devstorage.read_write. - https://www.googleapis.com/auth/genomics. zones:. - us-west1-b. timeout: 604800s. startTime: '2018-11-08T14:27:06.604193Z'. name: projects/valis-194104/operations/12097970745380060156. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:9119,interoperability,platform,platform,9119,rAssignedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:27:06.604193Z'. labels: {}. pipeline:. actions:. - commands:. - -c. - /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project valis-194104 --zones. us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse. entrypoint: bash. environment: {}. flags: []. imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0. labels: {}. mounts: []. name: ''. pidNamespace: ''. portMappings: {}. - commands:. - /bin/sh. - -c. - gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log. entrypoint: ''. environment: {}. flags:. - ALWAYS_RUN. imageUri: google/cloud-sdk:alpine. labels: {}. mounts: []. name: ''. pidNamespace: ''. portMappings: {}. environment: {}. resources:. projectId: valis-194104. regions: []. virtualMachine:. accelerators: []. bootDiskSizeGb: 10. bootImage: projects/cos-cloud/global/images/family/cos-stable. cpuPlatform: ''. disks: []. labels: {}. machineType: n1-standard-1. nvidiaDriverVersion: ''. preemptible: false. serviceAccount:. email: default. scopes:. - https://www.googleapis.com/auth/cloud-platform. - https://www.googleapis.com/auth/devstorage.read_write. - https://www.googleapis.com/auth/genomics. zones:. - us-west1-b. timeout: 604800s. startTime: '2018-11-08T14:27:06.604193Z'. name: projects/valis-194104/operations/12097970745380060156. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:3469,modifiability,modul,module,3469," /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse"": exit status 1: /examples_output.tfrecord@""${SHARDS}"".gz\n --reads ""/input-gcsfused-{}/${BAM}""\n --ref ""${INPUT_REF}""\n --task {}\n --regions gs://canis/CNR-data/CDS-canonical.bed"" # ENABLE_FUSE\n']. Traceback (most recent call last):. File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>. run(). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run. _run_make_examples(pipeline_args). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples. _wait_for_results(threads, results). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results. result.get(). File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get. raise self._value. RuntimeError: Job failed with error ""run"": operation ""projects/valis-194104/operations/781565864516461293"" failed: executing pipeline: Execution failed: action 6: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION). details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 1. exitStatus: 1. stderr: |+. /examples_output.tfrecord@""${SHARDS}"".gz\n --reads ""/input-gcsfused-{}/${BAM}""\n --ref ""${INPUT_REF}""\n --task {}\n --regions gs://canis/CNR-data/CDS-canonical.bed"" # ENABLE_FUSE\n']",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:4586,modifiability,modul,module,4586,"ine_args). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples. _wait_for_results(threads, results). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results. result.get(). File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get. raise self._value. RuntimeError: Job failed with error ""run"": operation ""projects/valis-194104/operations/781565864516461293"" failed: executing pipeline: Execution failed: action 6: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION). details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 1. exitStatus: 1. stderr: |+. /examples_output.tfrecord@""${SHARDS}"".gz\n --reads ""/input-gcsfused-{}/${BAM}""\n --ref ""${INPUT_REF}""\n --task {}\n --regions gs://canis/CNR-data/CDS-canonical.bed"" # ENABLE_FUSE\n']. Traceback (most recent call last):. File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>. run(). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run. _run_make_examples(pipeline_args). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples. _wait_for_results(threads, results). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results. result.get(). File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get. raise self._value. RuntimeError: Job failed with error ""run"": operation ""projects/valis-194104/operations/781565864516461293"" failed: executing pipeline: Execution failed: action 6: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION). timestamp: '2018-11-08T14:30:55.372655Z'. - description: Started running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project. valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:9037,modifiability,servic,serviceAccount,9037,rAssignedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:27:06.604193Z'. labels: {}. pipeline:. actions:. - commands:. - -c. - /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project valis-194104 --zones. us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse. entrypoint: bash. environment: {}. flags: []. imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0. labels: {}. mounts: []. name: ''. pidNamespace: ''. portMappings: {}. - commands:. - /bin/sh. - -c. - gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log. entrypoint: ''. environment: {}. flags:. - ALWAYS_RUN. imageUri: google/cloud-sdk:alpine. labels: {}. mounts: []. name: ''. pidNamespace: ''. portMappings: {}. environment: {}. resources:. projectId: valis-194104. regions: []. virtualMachine:. accelerators: []. bootDiskSizeGb: 10. bootImage: projects/cos-cloud/global/images/family/cos-stable. cpuPlatform: ''. disks: []. labels: {}. machineType: n1-standard-1. nvidiaDriverVersion: ''. preemptible: false. serviceAccount:. email: default. scopes:. - https://www.googleapis.com/auth/cloud-platform. - https://www.googleapis.com/auth/devstorage.read_write. - https://www.googleapis.com/auth/genomics. zones:. - us-west1-b. timeout: 604800s. startTime: '2018-11-08T14:27:06.604193Z'. name: projects/valis-194104/operations/12097970745380060156. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:32,performance,failur,failures,32,"Ran it again, and still hitting failures:. ```. done: true. error:. code: 9. message: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. metadata:. '@type': type.googleapis.com/google.genomics.v2alpha1.Metadata. createTime: '2018-11-08T14:27:06.016940Z'. endTime: '2018-11-08T14:30:59.324697Z'. events:. - description: Worker released. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.WorkerReleasedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:30:59.324697Z'. - description: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.FailedEvent. cause: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. code: FAILED_PRECONDITION. timestamp: '2018-11-08T14:30:58.518326Z'. - description: Stopped running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 2. exitStatus: 0. stderr: ''. timestamp: '2018-11-08T14:30:58.416239Z'. - description: Started running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent. actionId: 2. ipAddress: ''. portMappings: {}. timestamp: '2018-11-08T14:30:55.929647Z'. - description: Unexpected exit status 1 while running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project. valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:60,performance,error,error,60,"Ran it again, and still hitting failures:. ```. done: true. error:. code: 9. message: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. metadata:. '@type': type.googleapis.com/google.genomics.v2alpha1.Metadata. createTime: '2018-11-08T14:27:06.016940Z'. endTime: '2018-11-08T14:30:59.324697Z'. events:. - description: Worker released. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.WorkerReleasedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:30:59.324697Z'. - description: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.FailedEvent. cause: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. code: FAILED_PRECONDITION. timestamp: '2018-11-08T14:30:58.518326Z'. - description: Stopped running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 2. exitStatus: 0. stderr: ''. timestamp: '2018-11-08T14:30:58.416239Z'. - description: Started running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent. actionId: 2. ipAddress: ''. portMappings: {}. timestamp: '2018-11-08T14:30:55.929647Z'. - description: Unexpected exit status 1 while running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project. valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:528,performance,time,timestamp,528,"Ran it again, and still hitting failures:. ```. done: true. error:. code: 9. message: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. metadata:. '@type': type.googleapis.com/google.genomics.v2alpha1.Metadata. createTime: '2018-11-08T14:27:06.016940Z'. endTime: '2018-11-08T14:30:59.324697Z'. events:. - description: Worker released. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.WorkerReleasedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:30:59.324697Z'. - description: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.FailedEvent. cause: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. code: FAILED_PRECONDITION. timestamp: '2018-11-08T14:30:58.518326Z'. - description: Stopped running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 2. exitStatus: 0. stderr: ''. timestamp: '2018-11-08T14:30:58.416239Z'. - description: Started running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent. actionId: 2. ipAddress: ''. portMappings: {}. timestamp: '2018-11-08T14:30:55.929647Z'. - description: Unexpected exit status 1 while running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project. valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:840,performance,time,timestamp,840,"Ran it again, and still hitting failures:. ```. done: true. error:. code: 9. message: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. metadata:. '@type': type.googleapis.com/google.genomics.v2alpha1.Metadata. createTime: '2018-11-08T14:27:06.016940Z'. endTime: '2018-11-08T14:30:59.324697Z'. events:. - description: Worker released. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.WorkerReleasedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:30:59.324697Z'. - description: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.FailedEvent. cause: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. code: FAILED_PRECONDITION. timestamp: '2018-11-08T14:30:58.518326Z'. - description: Stopped running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 2. exitStatus: 0. stderr: ''. timestamp: '2018-11-08T14:30:58.416239Z'. - description: Started running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent. actionId: 2. ipAddress: ''. portMappings: {}. timestamp: '2018-11-08T14:30:55.929647Z'. - description: Unexpected exit status 1 while running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project. valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:1158,performance,time,timestamp,1158,"ata:. '@type': type.googleapis.com/google.genomics.v2alpha1.Metadata. createTime: '2018-11-08T14:27:06.016940Z'. endTime: '2018-11-08T14:30:59.324697Z'. events:. - description: Worker released. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.WorkerReleasedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:30:59.324697Z'. - description: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.FailedEvent. cause: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. code: FAILED_PRECONDITION. timestamp: '2018-11-08T14:30:58.518326Z'. - description: Stopped running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 2. exitStatus: 0. stderr: ''. timestamp: '2018-11-08T14:30:58.416239Z'. - description: Started running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent. actionId: 2. ipAddress: ''. portMappings: {}. timestamp: '2018-11-08T14:30:55.929647Z'. - description: Unexpected exit status 1 while running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project. valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:1482,performance,time,timestamp,1482,"1baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:30:59.324697Z'. - description: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.FailedEvent. cause: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. code: FAILED_PRECONDITION. timestamp: '2018-11-08T14:30:58.518326Z'. - description: Stopped running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 2. exitStatus: 0. stderr: ''. timestamp: '2018-11-08T14:30:58.416239Z'. - description: Started running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent. actionId: 2. ipAddress: ''. portMappings: {}. timestamp: '2018-11-08T14:30:55.929647Z'. - description: Unexpected exit status 1 while running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project. valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.UnexpectedExitStatusEvent. actionId: 1. exitStatus: 1. timestamp: '2018-11-08T14:30:55.434175Z'. - description: |-. Stopped running ""-c /opt/deepvaria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:2392,performance,time,timestamp,2392,"nomics.v2alpha1.ContainerStartedEvent. actionId: 2. ipAddress: ''. portMappings: {}. timestamp: '2018-11-08T14:30:55.929647Z'. - description: Unexpected exit status 1 while running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project. valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.UnexpectedExitStatusEvent. actionId: 1. exitStatus: 1. timestamp: '2018-11-08T14:30:55.434175Z'. - description: |-. Stopped running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse"": exit status 1: /examples_output.tfrecord@""${SHARDS}"".gz\n --reads ""/input-gcsfused-{}/${BAM}""\n --ref ""${INPUT_REF}""\n --task {}\n --regions gs://canis/CNR-data/CDS-canonical.bed"" # ENABLE_FUSE\n']. Traceback (most recent call last):. File",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:3958,performance,error,error,3958," --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse"": exit status 1: /examples_output.tfrecord@""${SHARDS}"".gz\n --reads ""/input-gcsfused-{}/${BAM}""\n --ref ""${INPUT_REF}""\n --task {}\n --regions gs://canis/CNR-data/CDS-canonical.bed"" # ENABLE_FUSE\n']. Traceback (most recent call last):. File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>. run(). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run. _run_make_examples(pipeline_args). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples. _wait_for_results(threads, results). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results. result.get(). File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get. raise self._value. RuntimeError: Job failed with error ""run"": operation ""projects/valis-194104/operations/781565864516461293"" failed: executing pipeline: Execution failed: action 6: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION). details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 1. exitStatus: 1. stderr: |+. /examples_output.tfrecord@""${SHARDS}"".gz\n --reads ""/input-gcsfused-{}/${BAM}""\n --ref ""${INPUT_REF}""\n --task {}\n --regions gs://canis/CNR-data/CDS-canonical.bed"" # ENABLE_FUSE\n']. Traceback (most recent call last):. File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>. run(). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run. _run_make_examples(pipeline_args). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples. _wait_for_results(threads, results). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results. result.get(). File",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:5075,performance,error,error,5075,"d: action 6: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION). details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 1. exitStatus: 1. stderr: |+. /examples_output.tfrecord@""${SHARDS}"".gz\n --reads ""/input-gcsfused-{}/${BAM}""\n --ref ""${INPUT_REF}""\n --task {}\n --regions gs://canis/CNR-data/CDS-canonical.bed"" # ENABLE_FUSE\n']. Traceback (most recent call last):. File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>. run(). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run. _run_make_examples(pipeline_args). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples. _wait_for_results(threads, results). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results. result.get(). File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get. raise self._value. RuntimeError: Job failed with error ""run"": operation ""projects/valis-194104/operations/781565864516461293"" failed: executing pipeline: Execution failed: action 6: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION). timestamp: '2018-11-08T14:30:55.372655Z'. - description: Started running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project. valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse"". details:. '@type': type.google",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:5280,performance,time,timestamp,5280," |+. /examples_output.tfrecord@""${SHARDS}"".gz\n --reads ""/input-gcsfused-{}/${BAM}""\n --ref ""${INPUT_REF}""\n --task {}\n --regions gs://canis/CNR-data/CDS-canonical.bed"" # ENABLE_FUSE\n']. Traceback (most recent call last):. File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>. run(). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run. _run_make_examples(pipeline_args). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples. _wait_for_results(threads, results). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results. result.get(). File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get. raise self._value. RuntimeError: Job failed with error ""run"": operation ""projects/valis-194104/operations/781565864516461293"" failed: executing pipeline: Execution failed: action 6: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION). timestamp: '2018-11-08T14:30:55.372655Z'. - description: Started running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project. valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent. actionId: 1. ipAddress: ''. portMappings: {}. timestamp: '2018-11-08T14:28:16.559791Z'. - description: Stopped pulling ""google/cloud-sdk:alpine"". deta",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:6181,performance,time,timestamp,6181,"ion failed: action 6: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION). timestamp: '2018-11-08T14:30:55.372655Z'. - description: Started running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project. valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent. actionId: 1. ipAddress: ''. portMappings: {}. timestamp: '2018-11-08T14:28:16.559791Z'. - description: Stopped pulling ""google/cloud-sdk:alpine"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.PullStoppedEvent. imageUri: google/cloud-sdk:alpine. timestamp: '2018-11-08T14:28:12.427407Z'. - description: Started pulling ""google/cloud-sdk:alpine"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.PullStartedEvent. imageUri: google/cloud-sdk:alpine. timestamp: '2018-11-08T14:28:05.897359Z'. - description: Stopped pulling ""gcr.io/deepvariant-docker/deepvariant_runner:0.7.0"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.PullStoppedEvent. imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0. timestamp: '2018-11-08T14:28:05.747135Z'. - description: Started pulling ""gcr.io/deepvariant-docker/deepvariant_runner:0.7.0"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.PullStartedEvent. imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0. timestamp: '2018-11-08T14:27:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:6398,performance,time,timestamp,6398,"nner --project. valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent. actionId: 1. ipAddress: ''. portMappings: {}. timestamp: '2018-11-08T14:28:16.559791Z'. - description: Stopped pulling ""google/cloud-sdk:alpine"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.PullStoppedEvent. imageUri: google/cloud-sdk:alpine. timestamp: '2018-11-08T14:28:12.427407Z'. - description: Started pulling ""google/cloud-sdk:alpine"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.PullStartedEvent. imageUri: google/cloud-sdk:alpine. timestamp: '2018-11-08T14:28:05.897359Z'. - description: Stopped pulling ""gcr.io/deepvariant-docker/deepvariant_runner:0.7.0"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.PullStoppedEvent. imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0. timestamp: '2018-11-08T14:28:05.747135Z'. - description: Started pulling ""gcr.io/deepvariant-docker/deepvariant_runner:0.7.0"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.PullStartedEvent. imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0. timestamp: '2018-11-08T14:27:34.961215Z'. - description: Worker ""google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7"". assigned in ""us-west1-b"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.WorkerAssignedEvent. instance: g",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:6615,performance,time,timestamp,6615,"odel. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent. actionId: 1. ipAddress: ''. portMappings: {}. timestamp: '2018-11-08T14:28:16.559791Z'. - description: Stopped pulling ""google/cloud-sdk:alpine"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.PullStoppedEvent. imageUri: google/cloud-sdk:alpine. timestamp: '2018-11-08T14:28:12.427407Z'. - description: Started pulling ""google/cloud-sdk:alpine"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.PullStartedEvent. imageUri: google/cloud-sdk:alpine. timestamp: '2018-11-08T14:28:05.897359Z'. - description: Stopped pulling ""gcr.io/deepvariant-docker/deepvariant_runner:0.7.0"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.PullStoppedEvent. imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0. timestamp: '2018-11-08T14:28:05.747135Z'. - description: Started pulling ""gcr.io/deepvariant-docker/deepvariant_runner:0.7.0"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.PullStartedEvent. imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0. timestamp: '2018-11-08T14:27:34.961215Z'. - description: Worker ""google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7"". assigned in ""us-west1-b"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.WorkerAssignedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:27:06.604193Z'. labels: {}. pipeline:. actions:. - commands:. - -c. - /opt/deepvariant_runner/bin/gcp_deepvariant_ru",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:6886,performance,time,timestamp,6886,"v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent. actionId: 1. ipAddress: ''. portMappings: {}. timestamp: '2018-11-08T14:28:16.559791Z'. - description: Stopped pulling ""google/cloud-sdk:alpine"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.PullStoppedEvent. imageUri: google/cloud-sdk:alpine. timestamp: '2018-11-08T14:28:12.427407Z'. - description: Started pulling ""google/cloud-sdk:alpine"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.PullStartedEvent. imageUri: google/cloud-sdk:alpine. timestamp: '2018-11-08T14:28:05.897359Z'. - description: Stopped pulling ""gcr.io/deepvariant-docker/deepvariant_runner:0.7.0"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.PullStoppedEvent. imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0. timestamp: '2018-11-08T14:28:05.747135Z'. - description: Started pulling ""gcr.io/deepvariant-docker/deepvariant_runner:0.7.0"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.PullStartedEvent. imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0. timestamp: '2018-11-08T14:27:34.961215Z'. - description: Worker ""google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7"". assigned in ""us-west1-b"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.WorkerAssignedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:27:06.604193Z'. labels: {}. pipeline:. actions:. - commands:. - -c. - /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project valis-194104 --zones. us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:7157,performance,time,timestamp,7157," portMappings: {}. timestamp: '2018-11-08T14:28:16.559791Z'. - description: Stopped pulling ""google/cloud-sdk:alpine"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.PullStoppedEvent. imageUri: google/cloud-sdk:alpine. timestamp: '2018-11-08T14:28:12.427407Z'. - description: Started pulling ""google/cloud-sdk:alpine"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.PullStartedEvent. imageUri: google/cloud-sdk:alpine. timestamp: '2018-11-08T14:28:05.897359Z'. - description: Stopped pulling ""gcr.io/deepvariant-docker/deepvariant_runner:0.7.0"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.PullStoppedEvent. imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0. timestamp: '2018-11-08T14:28:05.747135Z'. - description: Started pulling ""gcr.io/deepvariant-docker/deepvariant_runner:0.7.0"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.PullStartedEvent. imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0. timestamp: '2018-11-08T14:27:34.961215Z'. - description: Worker ""google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7"". assigned in ""us-west1-b"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.WorkerAssignedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:27:06.604193Z'. labels: {}. pipeline:. actions:. - commands:. - -c. - /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project valis-194104 --zones. us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:7478,performance,time,timestamp,7478,"d-sdk:alpine"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.PullStartedEvent. imageUri: google/cloud-sdk:alpine. timestamp: '2018-11-08T14:28:05.897359Z'. - description: Stopped pulling ""gcr.io/deepvariant-docker/deepvariant_runner:0.7.0"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.PullStoppedEvent. imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0. timestamp: '2018-11-08T14:28:05.747135Z'. - description: Started pulling ""gcr.io/deepvariant-docker/deepvariant_runner:0.7.0"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.PullStartedEvent. imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0. timestamp: '2018-11-08T14:27:34.961215Z'. - description: Worker ""google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7"". assigned in ""us-west1-b"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.WorkerAssignedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:27:06.604193Z'. labels: {}. pipeline:. actions:. - commands:. - -c. - /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project valis-194104 --zones. us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse. entrypoint: bash. environment: {}. flags: []. imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0. labels: {}. mounts: []. name: ''. pidNamespace: ''. portMappings: {}. - commands:. - /bin/sh. - -c. - gsutil -q",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:8756,performance,resourc,resources,8756,rAssignedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:27:06.604193Z'. labels: {}. pipeline:. actions:. - commands:. - -c. - /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project valis-194104 --zones. us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse. entrypoint: bash. environment: {}. flags: []. imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0. labels: {}. mounts: []. name: ''. pidNamespace: ''. portMappings: {}. - commands:. - /bin/sh. - -c. - gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log. entrypoint: ''. environment: {}. flags:. - ALWAYS_RUN. imageUri: google/cloud-sdk:alpine. labels: {}. mounts: []. name: ''. pidNamespace: ''. portMappings: {}. environment: {}. resources:. projectId: valis-194104. regions: []. virtualMachine:. accelerators: []. bootDiskSizeGb: 10. bootImage: projects/cos-cloud/global/images/family/cos-stable. cpuPlatform: ''. disks: []. labels: {}. machineType: n1-standard-1. nvidiaDriverVersion: ''. preemptible: false. serviceAccount:. email: default. scopes:. - https://www.googleapis.com/auth/cloud-platform. - https://www.googleapis.com/auth/devstorage.read_write. - https://www.googleapis.com/auth/genomics. zones:. - us-west1-b. timeout: 604800s. startTime: '2018-11-08T14:27:06.604193Z'. name: projects/valis-194104/operations/12097970745380060156. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:8924,performance,cpu,cpuPlatform,8924,rAssignedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:27:06.604193Z'. labels: {}. pipeline:. actions:. - commands:. - -c. - /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project valis-194104 --zones. us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse. entrypoint: bash. environment: {}. flags: []. imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0. labels: {}. mounts: []. name: ''. pidNamespace: ''. portMappings: {}. - commands:. - /bin/sh. - -c. - gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log. entrypoint: ''. environment: {}. flags:. - ALWAYS_RUN. imageUri: google/cloud-sdk:alpine. labels: {}. mounts: []. name: ''. pidNamespace: ''. portMappings: {}. environment: {}. resources:. projectId: valis-194104. regions: []. virtualMachine:. accelerators: []. bootDiskSizeGb: 10. bootImage: projects/cos-cloud/global/images/family/cos-stable. cpuPlatform: ''. disks: []. labels: {}. machineType: n1-standard-1. nvidiaDriverVersion: ''. preemptible: false. serviceAccount:. email: default. scopes:. - https://www.googleapis.com/auth/cloud-platform. - https://www.googleapis.com/auth/devstorage.read_write. - https://www.googleapis.com/auth/genomics. zones:. - us-west1-b. timeout: 604800s. startTime: '2018-11-08T14:27:06.604193Z'. name: projects/valis-194104/operations/12097970745380060156. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:8941,performance,disk,disks,8941,rAssignedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:27:06.604193Z'. labels: {}. pipeline:. actions:. - commands:. - -c. - /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project valis-194104 --zones. us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse. entrypoint: bash. environment: {}. flags: []. imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0. labels: {}. mounts: []. name: ''. pidNamespace: ''. portMappings: {}. - commands:. - /bin/sh. - -c. - gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log. entrypoint: ''. environment: {}. flags:. - ALWAYS_RUN. imageUri: google/cloud-sdk:alpine. labels: {}. mounts: []. name: ''. pidNamespace: ''. portMappings: {}. environment: {}. resources:. projectId: valis-194104. regions: []. virtualMachine:. accelerators: []. bootDiskSizeGb: 10. bootImage: projects/cos-cloud/global/images/family/cos-stable. cpuPlatform: ''. disks: []. labels: {}. machineType: n1-standard-1. nvidiaDriverVersion: ''. preemptible: false. serviceAccount:. email: default. scopes:. - https://www.googleapis.com/auth/cloud-platform. - https://www.googleapis.com/auth/devstorage.read_write. - https://www.googleapis.com/auth/genomics. zones:. - us-west1-b. timeout: 604800s. startTime: '2018-11-08T14:27:06.604193Z'. name: projects/valis-194104/operations/12097970745380060156. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:9252,performance,time,timeout,9252,rAssignedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:27:06.604193Z'. labels: {}. pipeline:. actions:. - commands:. - -c. - /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project valis-194104 --zones. us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse. entrypoint: bash. environment: {}. flags: []. imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0. labels: {}. mounts: []. name: ''. pidNamespace: ''. portMappings: {}. - commands:. - /bin/sh. - -c. - gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log. entrypoint: ''. environment: {}. flags:. - ALWAYS_RUN. imageUri: google/cloud-sdk:alpine. labels: {}. mounts: []. name: ''. pidNamespace: ''. portMappings: {}. environment: {}. resources:. projectId: valis-194104. regions: []. virtualMachine:. accelerators: []. bootDiskSizeGb: 10. bootImage: projects/cos-cloud/global/images/family/cos-stable. cpuPlatform: ''. disks: []. labels: {}. machineType: n1-standard-1. nvidiaDriverVersion: ''. preemptible: false. serviceAccount:. email: default. scopes:. - https://www.googleapis.com/auth/cloud-platform. - https://www.googleapis.com/auth/devstorage.read_write. - https://www.googleapis.com/auth/genomics. zones:. - us-west1-b. timeout: 604800s. startTime: '2018-11-08T14:27:06.604193Z'. name: projects/valis-194104/operations/12097970745380060156. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:32,reliability,fail,failures,32,"Ran it again, and still hitting failures:. ```. done: true. error:. code: 9. message: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. metadata:. '@type': type.googleapis.com/google.genomics.v2alpha1.Metadata. createTime: '2018-11-08T14:27:06.016940Z'. endTime: '2018-11-08T14:30:59.324697Z'. events:. - description: Worker released. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.WorkerReleasedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:30:59.324697Z'. - description: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.FailedEvent. cause: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. code: FAILED_PRECONDITION. timestamp: '2018-11-08T14:30:58.518326Z'. - description: Stopped running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 2. exitStatus: 0. stderr: ''. timestamp: '2018-11-08T14:30:58.416239Z'. - description: Started running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent. actionId: 2. ipAddress: ''. portMappings: {}. timestamp: '2018-11-08T14:30:55.929647Z'. - description: Unexpected exit status 1 while running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project. valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:97,reliability,fail,failed,97,"Ran it again, and still hitting failures:. ```. done: true. error:. code: 9. message: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. metadata:. '@type': type.googleapis.com/google.genomics.v2alpha1.Metadata. createTime: '2018-11-08T14:27:06.016940Z'. endTime: '2018-11-08T14:30:59.324697Z'. events:. - description: Worker released. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.WorkerReleasedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:30:59.324697Z'. - description: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.FailedEvent. cause: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. code: FAILED_PRECONDITION. timestamp: '2018-11-08T14:30:58.518326Z'. - description: Stopped running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 2. exitStatus: 0. stderr: ''. timestamp: '2018-11-08T14:30:58.416239Z'. - description: Started running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent. actionId: 2. ipAddress: ''. portMappings: {}. timestamp: '2018-11-08T14:30:55.929647Z'. - description: Unexpected exit status 1 while running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project. valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:596,reliability,fail,failed,596,"Ran it again, and still hitting failures:. ```. done: true. error:. code: 9. message: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. metadata:. '@type': type.googleapis.com/google.genomics.v2alpha1.Metadata. createTime: '2018-11-08T14:27:06.016940Z'. endTime: '2018-11-08T14:30:59.324697Z'. events:. - description: Worker released. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.WorkerReleasedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:30:59.324697Z'. - description: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.FailedEvent. cause: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. code: FAILED_PRECONDITION. timestamp: '2018-11-08T14:30:58.518326Z'. - description: Stopped running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 2. exitStatus: 0. stderr: ''. timestamp: '2018-11-08T14:30:58.416239Z'. - description: Started running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent. actionId: 2. ipAddress: ''. portMappings: {}. timestamp: '2018-11-08T14:30:55.929647Z'. - description: Unexpected exit status 1 while running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project. valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:721,reliability,Fail,FailedEvent,721,"Ran it again, and still hitting failures:. ```. done: true. error:. code: 9. message: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. metadata:. '@type': type.googleapis.com/google.genomics.v2alpha1.Metadata. createTime: '2018-11-08T14:27:06.016940Z'. endTime: '2018-11-08T14:30:59.324697Z'. events:. - description: Worker released. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.WorkerReleasedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:30:59.324697Z'. - description: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.FailedEvent. cause: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. code: FAILED_PRECONDITION. timestamp: '2018-11-08T14:30:58.518326Z'. - description: Stopped running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 2. exitStatus: 0. stderr: ''. timestamp: '2018-11-08T14:30:58.416239Z'. - description: Started running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent. actionId: 2. ipAddress: ''. portMappings: {}. timestamp: '2018-11-08T14:30:55.929647Z'. - description: Unexpected exit status 1 while running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project. valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:752,reliability,fail,failed,752,"Ran it again, and still hitting failures:. ```. done: true. error:. code: 9. message: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. metadata:. '@type': type.googleapis.com/google.genomics.v2alpha1.Metadata. createTime: '2018-11-08T14:27:06.016940Z'. endTime: '2018-11-08T14:30:59.324697Z'. events:. - description: Worker released. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.WorkerReleasedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:30:59.324697Z'. - description: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.FailedEvent. cause: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. code: FAILED_PRECONDITION. timestamp: '2018-11-08T14:30:58.518326Z'. - description: Stopped running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 2. exitStatus: 0. stderr: ''. timestamp: '2018-11-08T14:30:58.416239Z'. - description: Started running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent. actionId: 2. ipAddress: ''. portMappings: {}. timestamp: '2018-11-08T14:30:55.929647Z'. - description: Unexpected exit status 1 while running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project. valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:3946,reliability,fail,failed,3946,"_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse"": exit status 1: /examples_output.tfrecord@""${SHARDS}"".gz\n --reads ""/input-gcsfused-{}/${BAM}""\n --ref ""${INPUT_REF}""\n --task {}\n --regions gs://canis/CNR-data/CDS-canonical.bed"" # ENABLE_FUSE\n']. Traceback (most recent call last):. File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>. run(). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run. _run_make_examples(pipeline_args). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples. _wait_for_results(threads, results). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results. result.get(). File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get. raise self._value. RuntimeError: Job failed with error ""run"": operation ""projects/valis-194104/operations/781565864516461293"" failed: executing pipeline: Execution failed: action 6: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION). details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 1. exitStatus: 1. stderr: |+. /examples_output.tfrecord@""${SHARDS}"".gz\n --reads ""/input-gcsfused-{}/${BAM}""\n --ref ""${INPUT_REF}""\n --task {}\n --regions gs://canis/CNR-data/CDS-canonical.bed"" # ENABLE_FUSE\n']. Traceback (most recent call last):. File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>. run(). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run. _run_make_examples(pipeline_args). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples. _wait_for_results(threads, results). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results. result",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:4035,reliability,fail,failed,4035,"/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse"": exit status 1: /examples_output.tfrecord@""${SHARDS}"".gz\n --reads ""/input-gcsfused-{}/${BAM}""\n --ref ""${INPUT_REF}""\n --task {}\n --regions gs://canis/CNR-data/CDS-canonical.bed"" # ENABLE_FUSE\n']. Traceback (most recent call last):. File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>. run(). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run. _run_make_examples(pipeline_args). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples. _wait_for_results(threads, results). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results. result.get(). File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get. raise self._value. RuntimeError: Job failed with error ""run"": operation ""projects/valis-194104/operations/781565864516461293"" failed: executing pipeline: Execution failed: action 6: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION). details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 1. exitStatus: 1. stderr: |+. /examples_output.tfrecord@""${SHARDS}"".gz\n --reads ""/input-gcsfused-{}/${BAM}""\n --ref ""${INPUT_REF}""\n --task {}\n --regions gs://canis/CNR-data/CDS-canonical.bed"" # ENABLE_FUSE\n']. Traceback (most recent call last):. File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>. run(). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run. _run_make_examples(pipeline_args). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples. _wait_for_results(threads, results). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results. result.get(). File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get. raise self._",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:4073,reliability,fail,failed,4073,"z.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse"": exit status 1: /examples_output.tfrecord@""${SHARDS}"".gz\n --reads ""/input-gcsfused-{}/${BAM}""\n --ref ""${INPUT_REF}""\n --task {}\n --regions gs://canis/CNR-data/CDS-canonical.bed"" # ENABLE_FUSE\n']. Traceback (most recent call last):. File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>. run(). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run. _run_make_examples(pipeline_args). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples. _wait_for_results(threads, results). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results. result.get(). File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get. raise self._value. RuntimeError: Job failed with error ""run"": operation ""projects/valis-194104/operations/781565864516461293"" failed: executing pipeline: Execution failed: action 6: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION). details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 1. exitStatus: 1. stderr: |+. /examples_output.tfrecord@""${SHARDS}"".gz\n --reads ""/input-gcsfused-{}/${BAM}""\n --ref ""${INPUT_REF}""\n --task {}\n --regions gs://canis/CNR-data/CDS-canonical.bed"" # ENABLE_FUSE\n']. Traceback (most recent call last):. File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>. run(). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run. _run_make_examples(pipeline_args). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples. _wait_for_results(threads, results). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results. result.get(). File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get. raise self._value. RuntimeError: Job failed with e",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:5063,reliability,fail,failed,5063,"cution failed: action 6: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION). details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 1. exitStatus: 1. stderr: |+. /examples_output.tfrecord@""${SHARDS}"".gz\n --reads ""/input-gcsfused-{}/${BAM}""\n --ref ""${INPUT_REF}""\n --task {}\n --regions gs://canis/CNR-data/CDS-canonical.bed"" # ENABLE_FUSE\n']. Traceback (most recent call last):. File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>. run(). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run. _run_make_examples(pipeline_args). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples. _wait_for_results(threads, results). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results. result.get(). File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get. raise self._value. RuntimeError: Job failed with error ""run"": operation ""projects/valis-194104/operations/781565864516461293"" failed: executing pipeline: Execution failed: action 6: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION). timestamp: '2018-11-08T14:30:55.372655Z'. - description: Started running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project. valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse"". details:. '@type':",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:5152,reliability,fail,failed,5152,"ITION). details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 1. exitStatus: 1. stderr: |+. /examples_output.tfrecord@""${SHARDS}"".gz\n --reads ""/input-gcsfused-{}/${BAM}""\n --ref ""${INPUT_REF}""\n --task {}\n --regions gs://canis/CNR-data/CDS-canonical.bed"" # ENABLE_FUSE\n']. Traceback (most recent call last):. File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>. run(). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run. _run_make_examples(pipeline_args). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples. _wait_for_results(threads, results). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results. result.get(). File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get. raise self._value. RuntimeError: Job failed with error ""run"": operation ""projects/valis-194104/operations/781565864516461293"" failed: executing pipeline: Execution failed: action 6: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION). timestamp: '2018-11-08T14:30:55.372655Z'. - description: Started running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project. valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent. actionId: 1. ipAddre",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:5190,reliability,fail,failed,5190,"apis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 1. exitStatus: 1. stderr: |+. /examples_output.tfrecord@""${SHARDS}"".gz\n --reads ""/input-gcsfused-{}/${BAM}""\n --ref ""${INPUT_REF}""\n --task {}\n --regions gs://canis/CNR-data/CDS-canonical.bed"" # ENABLE_FUSE\n']. Traceback (most recent call last):. File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>. run(). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run. _run_make_examples(pipeline_args). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples. _wait_for_results(threads, results). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results. result.get(). File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get. raise self._value. RuntimeError: Job failed with error ""run"": operation ""projects/valis-194104/operations/781565864516461293"" failed: executing pipeline: Execution failed: action 6: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION). timestamp: '2018-11-08T14:30:55.372655Z'. - description: Started running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project. valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent. actionId: 1. ipAddress: ''. portMappings: {}. timestamp: '",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:60,safety,error,error,60,"Ran it again, and still hitting failures:. ```. done: true. error:. code: 9. message: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. metadata:. '@type': type.googleapis.com/google.genomics.v2alpha1.Metadata. createTime: '2018-11-08T14:27:06.016940Z'. endTime: '2018-11-08T14:30:59.324697Z'. events:. - description: Worker released. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.WorkerReleasedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:30:59.324697Z'. - description: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.FailedEvent. cause: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. code: FAILED_PRECONDITION. timestamp: '2018-11-08T14:30:58.518326Z'. - description: Stopped running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 2. exitStatus: 0. stderr: ''. timestamp: '2018-11-08T14:30:58.416239Z'. - description: Started running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent. actionId: 2. ipAddress: ''. portMappings: {}. timestamp: '2018-11-08T14:30:55.929647Z'. - description: Unexpected exit status 1 while running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project. valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:946,safety,log,logs,946,"Ran it again, and still hitting failures:. ```. done: true. error:. code: 9. message: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. metadata:. '@type': type.googleapis.com/google.genomics.v2alpha1.Metadata. createTime: '2018-11-08T14:27:06.016940Z'. endTime: '2018-11-08T14:30:59.324697Z'. events:. - description: Worker released. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.WorkerReleasedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:30:59.324697Z'. - description: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.FailedEvent. cause: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. code: FAILED_PRECONDITION. timestamp: '2018-11-08T14:30:58.518326Z'. - description: Stopped running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 2. exitStatus: 0. stderr: ''. timestamp: '2018-11-08T14:30:58.416239Z'. - description: Started running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent. actionId: 2. ipAddress: ''. portMappings: {}. timestamp: '2018-11-08T14:30:55.929647Z'. - description: Unexpected exit status 1 while running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project. valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:1025,safety,log,log,1025,"ting failures:. ```. done: true. error:. code: 9. message: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. metadata:. '@type': type.googleapis.com/google.genomics.v2alpha1.Metadata. createTime: '2018-11-08T14:27:06.016940Z'. endTime: '2018-11-08T14:30:59.324697Z'. events:. - description: Worker released. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.WorkerReleasedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:30:59.324697Z'. - description: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.FailedEvent. cause: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. code: FAILED_PRECONDITION. timestamp: '2018-11-08T14:30:58.518326Z'. - description: Stopped running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 2. exitStatus: 0. stderr: ''. timestamp: '2018-11-08T14:30:58.416239Z'. - description: Started running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent. actionId: 2. ipAddress: ''. portMappings: {}. timestamp: '2018-11-08T14:30:55.929647Z'. - description: Unexpected exit status 1 while running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project. valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:1264,safety,log,logs,1264,"016940Z'. endTime: '2018-11-08T14:30:59.324697Z'. events:. - description: Worker released. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.WorkerReleasedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:30:59.324697Z'. - description: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.FailedEvent. cause: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. code: FAILED_PRECONDITION. timestamp: '2018-11-08T14:30:58.518326Z'. - description: Stopped running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 2. exitStatus: 0. stderr: ''. timestamp: '2018-11-08T14:30:58.416239Z'. - description: Started running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent. actionId: 2. ipAddress: ''. portMappings: {}. timestamp: '2018-11-08T14:30:55.929647Z'. - description: Unexpected exit status 1 while running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project. valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcs",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:1343,safety,log,log,1343,"r released. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.WorkerReleasedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:30:59.324697Z'. - description: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.FailedEvent. cause: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. code: FAILED_PRECONDITION. timestamp: '2018-11-08T14:30:58.518326Z'. - description: Stopped running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 2. exitStatus: 0. stderr: ''. timestamp: '2018-11-08T14:30:58.416239Z'. - description: Started running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent. actionId: 2. ipAddress: ''. portMappings: {}. timestamp: '2018-11-08T14:30:55.929647Z'. - description: Unexpected exit status 1 while running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project. valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.Unexpect",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:3226,safety,input,input-gcsfused,3226,"_Verily_v1.genome.fa.gz.gzi --gcsfuse"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.UnexpectedExitStatusEvent. actionId: 1. exitStatus: 1. timestamp: '2018-11-08T14:30:55.434175Z'. - description: |-. Stopped running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse"": exit status 1: /examples_output.tfrecord@""${SHARDS}"".gz\n --reads ""/input-gcsfused-{}/${BAM}""\n --ref ""${INPUT_REF}""\n --task {}\n --regions gs://canis/CNR-data/CDS-canonical.bed"" # ENABLE_FUSE\n']. Traceback (most recent call last):. File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>. run(). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run. _run_make_examples(pipeline_args). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples. _wait_for_results(threads, results). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results. result.get(). File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get. raise self._value. RuntimeError: Job failed with error ""run"": operation ""projects/valis-194104/operations/781565864516461293"" failed: executing pipeline: Execution failed: action 6: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION). details:. '@type': type.googleapis.com/google.genomics.v2alpha1.Contai",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:3469,safety,modul,module,3469," /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse"": exit status 1: /examples_output.tfrecord@""${SHARDS}"".gz\n --reads ""/input-gcsfused-{}/${BAM}""\n --ref ""${INPUT_REF}""\n --task {}\n --regions gs://canis/CNR-data/CDS-canonical.bed"" # ENABLE_FUSE\n']. Traceback (most recent call last):. File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>. run(). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run. _run_make_examples(pipeline_args). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples. _wait_for_results(threads, results). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results. result.get(). File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get. raise self._value. RuntimeError: Job failed with error ""run"": operation ""projects/valis-194104/operations/781565864516461293"" failed: executing pipeline: Execution failed: action 6: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION). details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 1. exitStatus: 1. stderr: |+. /examples_output.tfrecord@""${SHARDS}"".gz\n --reads ""/input-gcsfused-{}/${BAM}""\n --ref ""${INPUT_REF}""\n --task {}\n --regions gs://canis/CNR-data/CDS-canonical.bed"" # ENABLE_FUSE\n']",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:3958,safety,error,error,3958," --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse"": exit status 1: /examples_output.tfrecord@""${SHARDS}"".gz\n --reads ""/input-gcsfused-{}/${BAM}""\n --ref ""${INPUT_REF}""\n --task {}\n --regions gs://canis/CNR-data/CDS-canonical.bed"" # ENABLE_FUSE\n']. Traceback (most recent call last):. File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>. run(). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run. _run_make_examples(pipeline_args). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples. _wait_for_results(threads, results). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results. result.get(). File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get. raise self._value. RuntimeError: Job failed with error ""run"": operation ""projects/valis-194104/operations/781565864516461293"" failed: executing pipeline: Execution failed: action 6: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION). details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 1. exitStatus: 1. stderr: |+. /examples_output.tfrecord@""${SHARDS}"".gz\n --reads ""/input-gcsfused-{}/${BAM}""\n --ref ""${INPUT_REF}""\n --task {}\n --regions gs://canis/CNR-data/CDS-canonical.bed"" # ENABLE_FUSE\n']. Traceback (most recent call last):. File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>. run(). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run. _run_make_examples(pipeline_args). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples. _wait_for_results(threads, results). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results. result.get(). File",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:4343,safety,input,input-gcsfused,4343,"E\n']. Traceback (most recent call last):. File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>. run(). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run. _run_make_examples(pipeline_args). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples. _wait_for_results(threads, results). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results. result.get(). File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get. raise self._value. RuntimeError: Job failed with error ""run"": operation ""projects/valis-194104/operations/781565864516461293"" failed: executing pipeline: Execution failed: action 6: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION). details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 1. exitStatus: 1. stderr: |+. /examples_output.tfrecord@""${SHARDS}"".gz\n --reads ""/input-gcsfused-{}/${BAM}""\n --ref ""${INPUT_REF}""\n --task {}\n --regions gs://canis/CNR-data/CDS-canonical.bed"" # ENABLE_FUSE\n']. Traceback (most recent call last):. File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>. run(). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run. _run_make_examples(pipeline_args). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples. _wait_for_results(threads, results). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results. result.get(). File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get. raise self._value. RuntimeError: Job failed with error ""run"": operation ""projects/valis-194104/operations/781565864516461293"" failed: executing pipeline: Execution failed: action 6: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION). timestamp: '2018-11-08T14:30:55.372655Z'. - description: Started runni",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:4586,safety,modul,module,4586,"ine_args). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples. _wait_for_results(threads, results). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results. result.get(). File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get. raise self._value. RuntimeError: Job failed with error ""run"": operation ""projects/valis-194104/operations/781565864516461293"" failed: executing pipeline: Execution failed: action 6: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION). details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 1. exitStatus: 1. stderr: |+. /examples_output.tfrecord@""${SHARDS}"".gz\n --reads ""/input-gcsfused-{}/${BAM}""\n --ref ""${INPUT_REF}""\n --task {}\n --regions gs://canis/CNR-data/CDS-canonical.bed"" # ENABLE_FUSE\n']. Traceback (most recent call last):. File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>. run(). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run. _run_make_examples(pipeline_args). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples. _wait_for_results(threads, results). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results. result.get(). File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get. raise self._value. RuntimeError: Job failed with error ""run"": operation ""projects/valis-194104/operations/781565864516461293"" failed: executing pipeline: Execution failed: action 6: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION). timestamp: '2018-11-08T14:30:55.372655Z'. - description: Started running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project. valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:5075,safety,error,error,5075,"d: action 6: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION). details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 1. exitStatus: 1. stderr: |+. /examples_output.tfrecord@""${SHARDS}"".gz\n --reads ""/input-gcsfused-{}/${BAM}""\n --ref ""${INPUT_REF}""\n --task {}\n --regions gs://canis/CNR-data/CDS-canonical.bed"" # ENABLE_FUSE\n']. Traceback (most recent call last):. File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>. run(). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run. _run_make_examples(pipeline_args). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples. _wait_for_results(threads, results). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results. result.get(). File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get. raise self._value. RuntimeError: Job failed with error ""run"": operation ""projects/valis-194104/operations/781565864516461293"" failed: executing pipeline: Execution failed: action 6: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION). timestamp: '2018-11-08T14:30:55.372655Z'. - description: Started running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project. valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse"". details:. '@type': type.google",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:8495,safety,log,logs,8495,rAssignedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:27:06.604193Z'. labels: {}. pipeline:. actions:. - commands:. - -c. - /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project valis-194104 --zones. us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse. entrypoint: bash. environment: {}. flags: []. imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0. labels: {}. mounts: []. name: ''. pidNamespace: ''. portMappings: {}. - commands:. - /bin/sh. - -c. - gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log. entrypoint: ''. environment: {}. flags:. - ALWAYS_RUN. imageUri: google/cloud-sdk:alpine. labels: {}. mounts: []. name: ''. pidNamespace: ''. portMappings: {}. environment: {}. resources:. projectId: valis-194104. regions: []. virtualMachine:. accelerators: []. bootDiskSizeGb: 10. bootImage: projects/cos-cloud/global/images/family/cos-stable. cpuPlatform: ''. disks: []. labels: {}. machineType: n1-standard-1. nvidiaDriverVersion: ''. preemptible: false. serviceAccount:. email: default. scopes:. - https://www.googleapis.com/auth/cloud-platform. - https://www.googleapis.com/auth/devstorage.read_write. - https://www.googleapis.com/auth/genomics. zones:. - us-west1-b. timeout: 604800s. startTime: '2018-11-08T14:27:06.604193Z'. name: projects/valis-194104/operations/12097970745380060156. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:8574,safety,log,log,8574,rAssignedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:27:06.604193Z'. labels: {}. pipeline:. actions:. - commands:. - -c. - /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project valis-194104 --zones. us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse. entrypoint: bash. environment: {}. flags: []. imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0. labels: {}. mounts: []. name: ''. pidNamespace: ''. portMappings: {}. - commands:. - /bin/sh. - -c. - gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log. entrypoint: ''. environment: {}. flags:. - ALWAYS_RUN. imageUri: google/cloud-sdk:alpine. labels: {}. mounts: []. name: ''. pidNamespace: ''. portMappings: {}. environment: {}. resources:. projectId: valis-194104. regions: []. virtualMachine:. accelerators: []. bootDiskSizeGb: 10. bootImage: projects/cos-cloud/global/images/family/cos-stable. cpuPlatform: ''. disks: []. labels: {}. machineType: n1-standard-1. nvidiaDriverVersion: ''. preemptible: false. serviceAccount:. email: default. scopes:. - https://www.googleapis.com/auth/cloud-platform. - https://www.googleapis.com/auth/devstorage.read_write. - https://www.googleapis.com/auth/genomics. zones:. - us-west1-b. timeout: 604800s. startTime: '2018-11-08T14:27:06.604193Z'. name: projects/valis-194104/operations/12097970745380060156. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:8756,safety,resourc,resources,8756,rAssignedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:27:06.604193Z'. labels: {}. pipeline:. actions:. - commands:. - -c. - /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project valis-194104 --zones. us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse. entrypoint: bash. environment: {}. flags: []. imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0. labels: {}. mounts: []. name: ''. pidNamespace: ''. portMappings: {}. - commands:. - /bin/sh. - -c. - gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log. entrypoint: ''. environment: {}. flags:. - ALWAYS_RUN. imageUri: google/cloud-sdk:alpine. labels: {}. mounts: []. name: ''. pidNamespace: ''. portMappings: {}. environment: {}. resources:. projectId: valis-194104. regions: []. virtualMachine:. accelerators: []. bootDiskSizeGb: 10. bootImage: projects/cos-cloud/global/images/family/cos-stable. cpuPlatform: ''. disks: []. labels: {}. machineType: n1-standard-1. nvidiaDriverVersion: ''. preemptible: false. serviceAccount:. email: default. scopes:. - https://www.googleapis.com/auth/cloud-platform. - https://www.googleapis.com/auth/devstorage.read_write. - https://www.googleapis.com/auth/genomics. zones:. - us-west1-b. timeout: 604800s. startTime: '2018-11-08T14:27:06.604193Z'. name: projects/valis-194104/operations/12097970745380060156. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:9252,safety,timeout,timeout,9252,rAssignedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:27:06.604193Z'. labels: {}. pipeline:. actions:. - commands:. - -c. - /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project valis-194104 --zones. us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse. entrypoint: bash. environment: {}. flags: []. imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0. labels: {}. mounts: []. name: ''. pidNamespace: ''. portMappings: {}. - commands:. - /bin/sh. - -c. - gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log. entrypoint: ''. environment: {}. flags:. - ALWAYS_RUN. imageUri: google/cloud-sdk:alpine. labels: {}. mounts: []. name: ''. pidNamespace: ''. portMappings: {}. environment: {}. resources:. projectId: valis-194104. regions: []. virtualMachine:. accelerators: []. bootDiskSizeGb: 10. bootImage: projects/cos-cloud/global/images/family/cos-stable. cpuPlatform: ''. disks: []. labels: {}. machineType: n1-standard-1. nvidiaDriverVersion: ''. preemptible: false. serviceAccount:. email: default. scopes:. - https://www.googleapis.com/auth/cloud-platform. - https://www.googleapis.com/auth/devstorage.read_write. - https://www.googleapis.com/auth/genomics. zones:. - us-west1-b. timeout: 604800s. startTime: '2018-11-08T14:27:06.604193Z'. name: projects/valis-194104/operations/12097970745380060156. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:946,security,log,logs,946,"Ran it again, and still hitting failures:. ```. done: true. error:. code: 9. message: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. metadata:. '@type': type.googleapis.com/google.genomics.v2alpha1.Metadata. createTime: '2018-11-08T14:27:06.016940Z'. endTime: '2018-11-08T14:30:59.324697Z'. events:. - description: Worker released. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.WorkerReleasedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:30:59.324697Z'. - description: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.FailedEvent. cause: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. code: FAILED_PRECONDITION. timestamp: '2018-11-08T14:30:58.518326Z'. - description: Stopped running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 2. exitStatus: 0. stderr: ''. timestamp: '2018-11-08T14:30:58.416239Z'. - description: Started running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent. actionId: 2. ipAddress: ''. portMappings: {}. timestamp: '2018-11-08T14:30:55.929647Z'. - description: Unexpected exit status 1 while running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project. valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:1025,security,log,log,1025,"ting failures:. ```. done: true. error:. code: 9. message: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. metadata:. '@type': type.googleapis.com/google.genomics.v2alpha1.Metadata. createTime: '2018-11-08T14:27:06.016940Z'. endTime: '2018-11-08T14:30:59.324697Z'. events:. - description: Worker released. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.WorkerReleasedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:30:59.324697Z'. - description: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.FailedEvent. cause: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. code: FAILED_PRECONDITION. timestamp: '2018-11-08T14:30:58.518326Z'. - description: Stopped running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 2. exitStatus: 0. stderr: ''. timestamp: '2018-11-08T14:30:58.416239Z'. - description: Started running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent. actionId: 2. ipAddress: ''. portMappings: {}. timestamp: '2018-11-08T14:30:55.929647Z'. - description: Unexpected exit status 1 while running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project. valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:1264,security,log,logs,1264,"016940Z'. endTime: '2018-11-08T14:30:59.324697Z'. events:. - description: Worker released. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.WorkerReleasedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:30:59.324697Z'. - description: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.FailedEvent. cause: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. code: FAILED_PRECONDITION. timestamp: '2018-11-08T14:30:58.518326Z'. - description: Stopped running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 2. exitStatus: 0. stderr: ''. timestamp: '2018-11-08T14:30:58.416239Z'. - description: Started running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent. actionId: 2. ipAddress: ''. portMappings: {}. timestamp: '2018-11-08T14:30:55.929647Z'. - description: Unexpected exit status 1 while running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project. valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcs",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:1343,security,log,log,1343,"r released. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.WorkerReleasedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:30:59.324697Z'. - description: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.FailedEvent. cause: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. code: FAILED_PRECONDITION. timestamp: '2018-11-08T14:30:58.518326Z'. - description: Stopped running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 2. exitStatus: 0. stderr: ''. timestamp: '2018-11-08T14:30:58.416239Z'. - description: Started running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent. actionId: 2. ipAddress: ''. portMappings: {}. timestamp: '2018-11-08T14:30:55.929647Z'. - description: Unexpected exit status 1 while running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project. valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.Unexpect",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:1844,security,model,model,1844,"mp: '2018-11-08T14:30:58.518326Z'. - description: Stopped running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 2. exitStatus: 0. stderr: ''. timestamp: '2018-11-08T14:30:58.416239Z'. - description: Started running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent. actionId: 2. ipAddress: ''. portMappings: {}. timestamp: '2018-11-08T14:30:55.929647Z'. - description: Unexpected exit status 1 while running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project. valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.UnexpectedExitStatusEvent. actionId: 1. exitStatus: 1. timestamp: '2018-11-08T14:30:55.434175Z'. - description: |-. Stopped running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions gs://ca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:1868,security,model,models,1868,".518326Z'. - description: Stopped running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 2. exitStatus: 0. stderr: ''. timestamp: '2018-11-08T14:30:58.416239Z'. - description: Started running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent. actionId: 2. ipAddress: ''. portMappings: {}. timestamp: '2018-11-08T14:30:55.929647Z'. - description: Unexpected exit status 1 while running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project. valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.UnexpectedExitStatusEvent. actionId: 1. exitStatus: 1. timestamp: '2018-11-08T14:30:55.434175Z'. - description: |-. Stopped running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions gs://canis/CNR-data/CDS-canonic",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:2733,security,model,model,2733,"-outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.UnexpectedExitStatusEvent. actionId: 1. exitStatus: 1. timestamp: '2018-11-08T14:30:55.434175Z'. - description: |-. Stopped running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse"": exit status 1: /examples_output.tfrecord@""${SHARDS}"".gz\n --reads ""/input-gcsfused-{}/${BAM}""\n --ref ""${INPUT_REF}""\n --task {}\n --regions gs://canis/CNR-data/CDS-canonical.bed"" # ENABLE_FUSE\n']. Traceback (most recent call last):. File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>. run(). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run. _run_make_examples(pipeline_args). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples. _wait_for_results(threads, results). File",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:2756,security,model,models,2756,"R-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.UnexpectedExitStatusEvent. actionId: 1. exitStatus: 1. timestamp: '2018-11-08T14:30:55.434175Z'. - description: |-. Stopped running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse"": exit status 1: /examples_output.tfrecord@""${SHARDS}"".gz\n --reads ""/input-gcsfused-{}/${BAM}""\n --ref ""${INPUT_REF}""\n --task {}\n --regions gs://canis/CNR-data/CDS-canonical.bed"" # ENABLE_FUSE\n']. Traceback (most recent call last):. File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>. run(). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run. _run_make_examples(pipeline_args). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples. _wait_for_results(threads, results). File ""/opt/deepvariant_runn",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:5619,security,model,model,5619,"nt_runner/src/gcp_deepvariant_runner.py"", line 845, in run. _run_make_examples(pipeline_args). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples. _wait_for_results(threads, results). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results. result.get(). File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get. raise self._value. RuntimeError: Job failed with error ""run"": operation ""projects/valis-194104/operations/781565864516461293"" failed: executing pipeline: Execution failed: action 6: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION). timestamp: '2018-11-08T14:30:55.372655Z'. - description: Started running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project. valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent. actionId: 1. ipAddress: ''. portMappings: {}. timestamp: '2018-11-08T14:28:16.559791Z'. - description: Stopped pulling ""google/cloud-sdk:alpine"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.PullStoppedEvent. imageUri: google/cloud-sdk:alpine. timestamp: '2018-11-08T14:28:12.427407Z'. - description: Started pulling ""google/cloud-sdk:alpine"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.PullStartedEvent. imageUri: google/cloud-sdk:alpine. timesta",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:5643,security,model,models,5643,"riant_runner.py"", line 845, in run. _run_make_examples(pipeline_args). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples. _wait_for_results(threads, results). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results. result.get(). File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get. raise self._value. RuntimeError: Job failed with error ""run"": operation ""projects/valis-194104/operations/781565864516461293"" failed: executing pipeline: Execution failed: action 6: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION). timestamp: '2018-11-08T14:30:55.372655Z'. - description: Started running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project. valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent. actionId: 1. ipAddress: ''. portMappings: {}. timestamp: '2018-11-08T14:28:16.559791Z'. - description: Stopped pulling ""google/cloud-sdk:alpine"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.PullStoppedEvent. imageUri: google/cloud-sdk:alpine. timestamp: '2018-11-08T14:28:12.427407Z'. - description: Started pulling ""google/cloud-sdk:alpine"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.PullStartedEvent. imageUri: google/cloud-sdk:alpine. timestamp: '2018-11-08T14:28:05",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:7836,security,model,model,7836,"o/deepvariant-docker/deepvariant_runner:0.7.0. timestamp: '2018-11-08T14:28:05.747135Z'. - description: Started pulling ""gcr.io/deepvariant-docker/deepvariant_runner:0.7.0"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.PullStartedEvent. imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0. timestamp: '2018-11-08T14:27:34.961215Z'. - description: Worker ""google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7"". assigned in ""us-west1-b"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.WorkerAssignedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:27:06.604193Z'. labels: {}. pipeline:. actions:. - commands:. - -c. - /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project valis-194104 --zones. us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse. entrypoint: bash. environment: {}. flags: []. imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0. labels: {}. mounts: []. name: ''. pidNamespace: ''. portMappings: {}. - commands:. - /bin/sh. - -c. - gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log. entrypoint: ''. environment: {}. flags:. - ALWAYS_RUN. imageUri: google/cloud-sdk:alpine. labels: {}. mounts: []. name: ''. pidNamespace: ''. portMappings: {}. environment: {}. resources:. projectId: valis-194104. regions: []. virtualMachine:. accelerators: []",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:7860,security,model,models,7860,"pvariant_runner:0.7.0. timestamp: '2018-11-08T14:28:05.747135Z'. - description: Started pulling ""gcr.io/deepvariant-docker/deepvariant_runner:0.7.0"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.PullStartedEvent. imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0. timestamp: '2018-11-08T14:27:34.961215Z'. - description: Worker ""google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7"". assigned in ""us-west1-b"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.WorkerAssignedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:27:06.604193Z'. labels: {}. pipeline:. actions:. - commands:. - -c. - /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project valis-194104 --zones. us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse. entrypoint: bash. environment: {}. flags: []. imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0. labels: {}. mounts: []. name: ''. pidNamespace: ''. portMappings: {}. - commands:. - /bin/sh. - -c. - gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log. entrypoint: ''. environment: {}. flags:. - ALWAYS_RUN. imageUri: google/cloud-sdk:alpine. labels: {}. mounts: []. name: ''. pidNamespace: ''. portMappings: {}. environment: {}. resources:. projectId: valis-194104. regions: []. virtualMachine:. accelerators: []. bootDiskSizeGb: 10. bo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:8495,security,log,logs,8495,rAssignedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:27:06.604193Z'. labels: {}. pipeline:. actions:. - commands:. - -c. - /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project valis-194104 --zones. us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse. entrypoint: bash. environment: {}. flags: []. imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0. labels: {}. mounts: []. name: ''. pidNamespace: ''. portMappings: {}. - commands:. - /bin/sh. - -c. - gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log. entrypoint: ''. environment: {}. flags:. - ALWAYS_RUN. imageUri: google/cloud-sdk:alpine. labels: {}. mounts: []. name: ''. pidNamespace: ''. portMappings: {}. environment: {}. resources:. projectId: valis-194104. regions: []. virtualMachine:. accelerators: []. bootDiskSizeGb: 10. bootImage: projects/cos-cloud/global/images/family/cos-stable. cpuPlatform: ''. disks: []. labels: {}. machineType: n1-standard-1. nvidiaDriverVersion: ''. preemptible: false. serviceAccount:. email: default. scopes:. - https://www.googleapis.com/auth/cloud-platform. - https://www.googleapis.com/auth/devstorage.read_write. - https://www.googleapis.com/auth/genomics. zones:. - us-west1-b. timeout: 604800s. startTime: '2018-11-08T14:27:06.604193Z'. name: projects/valis-194104/operations/12097970745380060156. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:8574,security,log,log,8574,rAssignedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:27:06.604193Z'. labels: {}. pipeline:. actions:. - commands:. - -c. - /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project valis-194104 --zones. us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse. entrypoint: bash. environment: {}. flags: []. imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0. labels: {}. mounts: []. name: ''. pidNamespace: ''. portMappings: {}. - commands:. - /bin/sh. - -c. - gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log. entrypoint: ''. environment: {}. flags:. - ALWAYS_RUN. imageUri: google/cloud-sdk:alpine. labels: {}. mounts: []. name: ''. pidNamespace: ''. portMappings: {}. environment: {}. resources:. projectId: valis-194104. regions: []. virtualMachine:. accelerators: []. bootDiskSizeGb: 10. bootImage: projects/cos-cloud/global/images/family/cos-stable. cpuPlatform: ''. disks: []. labels: {}. machineType: n1-standard-1. nvidiaDriverVersion: ''. preemptible: false. serviceAccount:. email: default. scopes:. - https://www.googleapis.com/auth/cloud-platform. - https://www.googleapis.com/auth/devstorage.read_write. - https://www.googleapis.com/auth/genomics. zones:. - us-west1-b. timeout: 604800s. startTime: '2018-11-08T14:27:06.604193Z'. name: projects/valis-194104/operations/12097970745380060156. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:9108,security,auth,auth,9108,rAssignedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:27:06.604193Z'. labels: {}. pipeline:. actions:. - commands:. - -c. - /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project valis-194104 --zones. us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse. entrypoint: bash. environment: {}. flags: []. imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0. labels: {}. mounts: []. name: ''. pidNamespace: ''. portMappings: {}. - commands:. - /bin/sh. - -c. - gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log. entrypoint: ''. environment: {}. flags:. - ALWAYS_RUN. imageUri: google/cloud-sdk:alpine. labels: {}. mounts: []. name: ''. pidNamespace: ''. portMappings: {}. environment: {}. resources:. projectId: valis-194104. regions: []. virtualMachine:. accelerators: []. bootDiskSizeGb: 10. bootImage: projects/cos-cloud/global/images/family/cos-stable. cpuPlatform: ''. disks: []. labels: {}. machineType: n1-standard-1. nvidiaDriverVersion: ''. preemptible: false. serviceAccount:. email: default. scopes:. - https://www.googleapis.com/auth/cloud-platform. - https://www.googleapis.com/auth/devstorage.read_write. - https://www.googleapis.com/auth/genomics. zones:. - us-west1-b. timeout: 604800s. startTime: '2018-11-08T14:27:06.604193Z'. name: projects/valis-194104/operations/12097970745380060156. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:9158,security,auth,auth,9158,rAssignedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:27:06.604193Z'. labels: {}. pipeline:. actions:. - commands:. - -c. - /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project valis-194104 --zones. us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse. entrypoint: bash. environment: {}. flags: []. imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0. labels: {}. mounts: []. name: ''. pidNamespace: ''. portMappings: {}. - commands:. - /bin/sh. - -c. - gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log. entrypoint: ''. environment: {}. flags:. - ALWAYS_RUN. imageUri: google/cloud-sdk:alpine. labels: {}. mounts: []. name: ''. pidNamespace: ''. portMappings: {}. environment: {}. resources:. projectId: valis-194104. regions: []. virtualMachine:. accelerators: []. bootDiskSizeGb: 10. bootImage: projects/cos-cloud/global/images/family/cos-stable. cpuPlatform: ''. disks: []. labels: {}. machineType: n1-standard-1. nvidiaDriverVersion: ''. preemptible: false. serviceAccount:. email: default. scopes:. - https://www.googleapis.com/auth/cloud-platform. - https://www.googleapis.com/auth/devstorage.read_write. - https://www.googleapis.com/auth/genomics. zones:. - us-west1-b. timeout: 604800s. startTime: '2018-11-08T14:27:06.604193Z'. name: projects/valis-194104/operations/12097970745380060156. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:9215,security,auth,auth,9215,rAssignedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:27:06.604193Z'. labels: {}. pipeline:. actions:. - commands:. - -c. - /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project valis-194104 --zones. us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse. entrypoint: bash. environment: {}. flags: []. imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0. labels: {}. mounts: []. name: ''. pidNamespace: ''. portMappings: {}. - commands:. - /bin/sh. - -c. - gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log. entrypoint: ''. environment: {}. flags:. - ALWAYS_RUN. imageUri: google/cloud-sdk:alpine. labels: {}. mounts: []. name: ''. pidNamespace: ''. portMappings: {}. environment: {}. resources:. projectId: valis-194104. regions: []. virtualMachine:. accelerators: []. bootDiskSizeGb: 10. bootImage: projects/cos-cloud/global/images/family/cos-stable. cpuPlatform: ''. disks: []. labels: {}. machineType: n1-standard-1. nvidiaDriverVersion: ''. preemptible: false. serviceAccount:. email: default. scopes:. - https://www.googleapis.com/auth/cloud-platform. - https://www.googleapis.com/auth/devstorage.read_write. - https://www.googleapis.com/auth/genomics. zones:. - us-west1-b. timeout: 604800s. startTime: '2018-11-08T14:27:06.604193Z'. name: projects/valis-194104/operations/12097970745380060156. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:946,testability,log,logs,946,"Ran it again, and still hitting failures:. ```. done: true. error:. code: 9. message: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. metadata:. '@type': type.googleapis.com/google.genomics.v2alpha1.Metadata. createTime: '2018-11-08T14:27:06.016940Z'. endTime: '2018-11-08T14:30:59.324697Z'. events:. - description: Worker released. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.WorkerReleasedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:30:59.324697Z'. - description: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.FailedEvent. cause: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. code: FAILED_PRECONDITION. timestamp: '2018-11-08T14:30:58.518326Z'. - description: Stopped running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 2. exitStatus: 0. stderr: ''. timestamp: '2018-11-08T14:30:58.416239Z'. - description: Started running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent. actionId: 2. ipAddress: ''. portMappings: {}. timestamp: '2018-11-08T14:30:55.929647Z'. - description: Unexpected exit status 1 while running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project. valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:1025,testability,log,log,1025,"ting failures:. ```. done: true. error:. code: 9. message: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. metadata:. '@type': type.googleapis.com/google.genomics.v2alpha1.Metadata. createTime: '2018-11-08T14:27:06.016940Z'. endTime: '2018-11-08T14:30:59.324697Z'. events:. - description: Worker released. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.WorkerReleasedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:30:59.324697Z'. - description: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.FailedEvent. cause: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. code: FAILED_PRECONDITION. timestamp: '2018-11-08T14:30:58.518326Z'. - description: Stopped running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 2. exitStatus: 0. stderr: ''. timestamp: '2018-11-08T14:30:58.416239Z'. - description: Started running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent. actionId: 2. ipAddress: ''. portMappings: {}. timestamp: '2018-11-08T14:30:55.929647Z'. - description: Unexpected exit status 1 while running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project. valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:1264,testability,log,logs,1264,"016940Z'. endTime: '2018-11-08T14:30:59.324697Z'. events:. - description: Worker released. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.WorkerReleasedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:30:59.324697Z'. - description: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.FailedEvent. cause: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. code: FAILED_PRECONDITION. timestamp: '2018-11-08T14:30:58.518326Z'. - description: Stopped running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 2. exitStatus: 0. stderr: ''. timestamp: '2018-11-08T14:30:58.416239Z'. - description: Started running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent. actionId: 2. ipAddress: ''. portMappings: {}. timestamp: '2018-11-08T14:30:55.929647Z'. - description: Unexpected exit status 1 while running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project. valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcs",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:1343,testability,log,log,1343,"r released. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.WorkerReleasedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:30:59.324697Z'. - description: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.FailedEvent. cause: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. code: FAILED_PRECONDITION. timestamp: '2018-11-08T14:30:58.518326Z'. - description: Stopped running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 2. exitStatus: 0. stderr: ''. timestamp: '2018-11-08T14:30:58.416239Z'. - description: Started running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent. actionId: 2. ipAddress: ''. portMappings: {}. timestamp: '2018-11-08T14:30:55.929647Z'. - description: Unexpected exit status 1 while running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project. valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.Unexpect",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:3357,testability,Trace,Traceback,3357,". actionId: 1. exitStatus: 1. timestamp: '2018-11-08T14:30:55.434175Z'. - description: |-. Stopped running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse"": exit status 1: /examples_output.tfrecord@""${SHARDS}"".gz\n --reads ""/input-gcsfused-{}/${BAM}""\n --ref ""${INPUT_REF}""\n --task {}\n --regions gs://canis/CNR-data/CDS-canonical.bed"" # ENABLE_FUSE\n']. Traceback (most recent call last):. File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>. run(). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run. _run_make_examples(pipeline_args). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples. _wait_for_results(threads, results). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results. result.get(). File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get. raise self._value. RuntimeError: Job failed with error ""run"": operation ""projects/valis-194104/operations/781565864516461293"" failed: executing pipeline: Execution failed: action 6: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION). details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 1. exitStatus: 1. stderr: |+. /examples_output.tfrecord@""${SHARDS}"".gz\n --reads ""/input-gcsfused-{}/$",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:4474,testability,Trace,Traceback,4474,"un(). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run. _run_make_examples(pipeline_args). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples. _wait_for_results(threads, results). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results. result.get(). File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get. raise self._value. RuntimeError: Job failed with error ""run"": operation ""projects/valis-194104/operations/781565864516461293"" failed: executing pipeline: Execution failed: action 6: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION). details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 1. exitStatus: 1. stderr: |+. /examples_output.tfrecord@""${SHARDS}"".gz\n --reads ""/input-gcsfused-{}/${BAM}""\n --ref ""${INPUT_REF}""\n --task {}\n --regions gs://canis/CNR-data/CDS-canonical.bed"" # ENABLE_FUSE\n']. Traceback (most recent call last):. File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>. run(). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run. _run_make_examples(pipeline_args). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples. _wait_for_results(threads, results). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results. result.get(). File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get. raise self._value. RuntimeError: Job failed with error ""run"": operation ""projects/valis-194104/operations/781565864516461293"" failed: executing pipeline: Execution failed: action 6: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION). timestamp: '2018-11-08T14:30:55.372655Z'. - description: Started running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project. valis-194104 --zones us-west1-b --docker_image gcr.io/deepva",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:8495,testability,log,logs,8495,rAssignedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:27:06.604193Z'. labels: {}. pipeline:. actions:. - commands:. - -c. - /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project valis-194104 --zones. us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse. entrypoint: bash. environment: {}. flags: []. imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0. labels: {}. mounts: []. name: ''. pidNamespace: ''. portMappings: {}. - commands:. - /bin/sh. - -c. - gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log. entrypoint: ''. environment: {}. flags:. - ALWAYS_RUN. imageUri: google/cloud-sdk:alpine. labels: {}. mounts: []. name: ''. pidNamespace: ''. portMappings: {}. environment: {}. resources:. projectId: valis-194104. regions: []. virtualMachine:. accelerators: []. bootDiskSizeGb: 10. bootImage: projects/cos-cloud/global/images/family/cos-stable. cpuPlatform: ''. disks: []. labels: {}. machineType: n1-standard-1. nvidiaDriverVersion: ''. preemptible: false. serviceAccount:. email: default. scopes:. - https://www.googleapis.com/auth/cloud-platform. - https://www.googleapis.com/auth/devstorage.read_write. - https://www.googleapis.com/auth/genomics. zones:. - us-west1-b. timeout: 604800s. startTime: '2018-11-08T14:27:06.604193Z'. name: projects/valis-194104/operations/12097970745380060156. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:8574,testability,log,log,8574,rAssignedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:27:06.604193Z'. labels: {}. pipeline:. actions:. - commands:. - -c. - /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project valis-194104 --zones. us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse. entrypoint: bash. environment: {}. flags: []. imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0. labels: {}. mounts: []. name: ''. pidNamespace: ''. portMappings: {}. - commands:. - /bin/sh. - -c. - gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log. entrypoint: ''. environment: {}. flags:. - ALWAYS_RUN. imageUri: google/cloud-sdk:alpine. labels: {}. mounts: []. name: ''. pidNamespace: ''. portMappings: {}. environment: {}. resources:. projectId: valis-194104. regions: []. virtualMachine:. accelerators: []. bootDiskSizeGb: 10. bootImage: projects/cos-cloud/global/images/family/cos-stable. cpuPlatform: ''. disks: []. labels: {}. machineType: n1-standard-1. nvidiaDriverVersion: ''. preemptible: false. serviceAccount:. email: default. scopes:. - https://www.googleapis.com/auth/cloud-platform. - https://www.googleapis.com/auth/devstorage.read_write. - https://www.googleapis.com/auth/genomics. zones:. - us-west1-b. timeout: 604800s. startTime: '2018-11-08T14:27:06.604193Z'. name: projects/valis-194104/operations/12097970745380060156. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:8756,testability,resourc,resources,8756,rAssignedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:27:06.604193Z'. labels: {}. pipeline:. actions:. - commands:. - -c. - /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project valis-194104 --zones. us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse. entrypoint: bash. environment: {}. flags: []. imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0. labels: {}. mounts: []. name: ''. pidNamespace: ''. portMappings: {}. - commands:. - /bin/sh. - -c. - gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log. entrypoint: ''. environment: {}. flags:. - ALWAYS_RUN. imageUri: google/cloud-sdk:alpine. labels: {}. mounts: []. name: ''. pidNamespace: ''. portMappings: {}. environment: {}. resources:. projectId: valis-194104. regions: []. virtualMachine:. accelerators: []. bootDiskSizeGb: 10. bootImage: projects/cos-cloud/global/images/family/cos-stable. cpuPlatform: ''. disks: []. labels: {}. machineType: n1-standard-1. nvidiaDriverVersion: ''. preemptible: false. serviceAccount:. email: default. scopes:. - https://www.googleapis.com/auth/cloud-platform. - https://www.googleapis.com/auth/devstorage.read_write. - https://www.googleapis.com/auth/genomics. zones:. - us-west1-b. timeout: 604800s. startTime: '2018-11-08T14:27:06.604193Z'. name: projects/valis-194104/operations/12097970745380060156. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:60,usability,error,error,60,"Ran it again, and still hitting failures:. ```. done: true. error:. code: 9. message: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. metadata:. '@type': type.googleapis.com/google.genomics.v2alpha1.Metadata. createTime: '2018-11-08T14:27:06.016940Z'. endTime: '2018-11-08T14:30:59.324697Z'. events:. - description: Worker released. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.WorkerReleasedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:30:59.324697Z'. - description: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.FailedEvent. cause: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. code: FAILED_PRECONDITION. timestamp: '2018-11-08T14:30:58.518326Z'. - description: Stopped running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 2. exitStatus: 0. stderr: ''. timestamp: '2018-11-08T14:30:58.416239Z'. - description: Started running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent. actionId: 2. ipAddress: ''. portMappings: {}. timestamp: '2018-11-08T14:30:55.929647Z'. - description: Unexpected exit status 1 while running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project. valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:131,usability,statu,status,131,"Ran it again, and still hitting failures:. ```. done: true. error:. code: 9. message: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. metadata:. '@type': type.googleapis.com/google.genomics.v2alpha1.Metadata. createTime: '2018-11-08T14:27:06.016940Z'. endTime: '2018-11-08T14:30:59.324697Z'. events:. - description: Worker released. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.WorkerReleasedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:30:59.324697Z'. - description: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.FailedEvent. cause: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. code: FAILED_PRECONDITION. timestamp: '2018-11-08T14:30:58.518326Z'. - description: Stopped running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 2. exitStatus: 0. stderr: ''. timestamp: '2018-11-08T14:30:58.416239Z'. - description: Started running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent. actionId: 2. ipAddress: ''. portMappings: {}. timestamp: '2018-11-08T14:30:55.929647Z'. - description: Unexpected exit status 1 while running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project. valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:630,usability,statu,status,630,"Ran it again, and still hitting failures:. ```. done: true. error:. code: 9. message: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. metadata:. '@type': type.googleapis.com/google.genomics.v2alpha1.Metadata. createTime: '2018-11-08T14:27:06.016940Z'. endTime: '2018-11-08T14:30:59.324697Z'. events:. - description: Worker released. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.WorkerReleasedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:30:59.324697Z'. - description: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.FailedEvent. cause: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. code: FAILED_PRECONDITION. timestamp: '2018-11-08T14:30:58.518326Z'. - description: Stopped running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 2. exitStatus: 0. stderr: ''. timestamp: '2018-11-08T14:30:58.416239Z'. - description: Started running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent. actionId: 2. ipAddress: ''. portMappings: {}. timestamp: '2018-11-08T14:30:55.929647Z'. - description: Unexpected exit status 1 while running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project. valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:786,usability,statu,status,786,"Ran it again, and still hitting failures:. ```. done: true. error:. code: 9. message: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. metadata:. '@type': type.googleapis.com/google.genomics.v2alpha1.Metadata. createTime: '2018-11-08T14:27:06.016940Z'. endTime: '2018-11-08T14:30:59.324697Z'. events:. - description: Worker released. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.WorkerReleasedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:30:59.324697Z'. - description: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.FailedEvent. cause: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. code: FAILED_PRECONDITION. timestamp: '2018-11-08T14:30:58.518326Z'. - description: Stopped running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 2. exitStatus: 0. stderr: ''. timestamp: '2018-11-08T14:30:58.416239Z'. - description: Started running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent. actionId: 2. ipAddress: ''. portMappings: {}. timestamp: '2018-11-08T14:30:55.929647Z'. - description: Unexpected exit status 1 while running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project. valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:897,usability,Stop,Stopped,897,"Ran it again, and still hitting failures:. ```. done: true. error:. code: 9. message: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. metadata:. '@type': type.googleapis.com/google.genomics.v2alpha1.Metadata. createTime: '2018-11-08T14:27:06.016940Z'. endTime: '2018-11-08T14:30:59.324697Z'. events:. - description: Worker released. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.WorkerReleasedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:30:59.324697Z'. - description: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.FailedEvent. cause: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. code: FAILED_PRECONDITION. timestamp: '2018-11-08T14:30:58.518326Z'. - description: Stopped running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 2. exitStatus: 0. stderr: ''. timestamp: '2018-11-08T14:30:58.416239Z'. - description: Started running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent. actionId: 2. ipAddress: ''. portMappings: {}. timestamp: '2018-11-08T14:30:55.929647Z'. - description: Unexpected exit status 1 while running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project. valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:1555,usability,statu,status,1555,"9.324697Z'. - description: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. details:. '@type': type.googleapis.com/google.genomics.v2alpha1.FailedEvent. cause: 'Execution failed: action 1: unexpected exit status 1 was not ignored'. code: FAILED_PRECONDITION. timestamp: '2018-11-08T14:30:58.518326Z'. - description: Stopped running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 2. exitStatus: 0. stderr: ''. timestamp: '2018-11-08T14:30:58.416239Z'. - description: Started running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent. actionId: 2. ipAddress: ''. portMappings: {}. timestamp: '2018-11-08T14:30:55.929647Z'. - description: Unexpected exit status 1 while running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project. valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.UnexpectedExitStatusEvent. actionId: 1. exitStatus: 1. timestamp: '2018-11-08T14:30:55.434175Z'. - description: |-. Stopped running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project valis-194104 --zones us-",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:2453,usability,Stop,Stopped,2453,"s: ''. portMappings: {}. timestamp: '2018-11-08T14:30:55.929647Z'. - description: Unexpected exit status 1 while running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project. valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.UnexpectedExitStatusEvent. actionId: 1. exitStatus: 1. timestamp: '2018-11-08T14:30:55.434175Z'. - description: |-. Stopped running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse"": exit status 1: /examples_output.tfrecord@""${SHARDS}"".gz\n --reads ""/input-gcsfused-{}/${BAM}""\n --ref ""${INPUT_REF}""\n --task {}\n --regions gs://canis/CNR-data/CDS-canonical.bed"" # ENABLE_FUSE\n']. Traceback (most recent call last):. File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", li",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:3163,usability,statu,status,3163,"h38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.UnexpectedExitStatusEvent. actionId: 1. exitStatus: 1. timestamp: '2018-11-08T14:30:55.434175Z'. - description: |-. Stopped running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse"": exit status 1: /examples_output.tfrecord@""${SHARDS}"".gz\n --reads ""/input-gcsfused-{}/${BAM}""\n --ref ""${INPUT_REF}""\n --task {}\n --regions gs://canis/CNR-data/CDS-canonical.bed"" # ENABLE_FUSE\n']. Traceback (most recent call last):. File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>. run(). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run. _run_make_examples(pipeline_args). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples. _wait_for_results(threads, results). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results. result.get(). File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get. raise self._value. RuntimeError: Job failed with error ""run"": operation ""projects/valis-194104/operations/781565864516461293"" failed: executing pipeline: Execution failed: action 6: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION). det",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:3226,usability,input,input-gcsfused,3226,"_Verily_v1.genome.fa.gz.gzi --gcsfuse"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.UnexpectedExitStatusEvent. actionId: 1. exitStatus: 1. timestamp: '2018-11-08T14:30:55.434175Z'. - description: |-. Stopped running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse"": exit status 1: /examples_output.tfrecord@""${SHARDS}"".gz\n --reads ""/input-gcsfused-{}/${BAM}""\n --ref ""${INPUT_REF}""\n --task {}\n --regions gs://canis/CNR-data/CDS-canonical.bed"" # ENABLE_FUSE\n']. Traceback (most recent call last):. File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>. run(). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run. _run_make_examples(pipeline_args). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples. _wait_for_results(threads, results). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results. result.get(). File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get. raise self._value. RuntimeError: Job failed with error ""run"": operation ""projects/valis-194104/operations/781565864516461293"" failed: executing pipeline: Execution failed: action 6: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION). details:. '@type': type.googleapis.com/google.genomics.v2alpha1.Contai",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:3958,usability,error,error,3958," --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse"": exit status 1: /examples_output.tfrecord@""${SHARDS}"".gz\n --reads ""/input-gcsfused-{}/${BAM}""\n --ref ""${INPUT_REF}""\n --task {}\n --regions gs://canis/CNR-data/CDS-canonical.bed"" # ENABLE_FUSE\n']. Traceback (most recent call last):. File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>. run(). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run. _run_make_examples(pipeline_args). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples. _wait_for_results(threads, results). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results. result.get(). File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get. raise self._value. RuntimeError: Job failed with error ""run"": operation ""projects/valis-194104/operations/781565864516461293"" failed: executing pipeline: Execution failed: action 6: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION). details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 1. exitStatus: 1. stderr: |+. /examples_output.tfrecord@""${SHARDS}"".gz\n --reads ""/input-gcsfused-{}/${BAM}""\n --ref ""${INPUT_REF}""\n --task {}\n --regions gs://canis/CNR-data/CDS-canonical.bed"" # ENABLE_FUSE\n']. Traceback (most recent call last):. File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>. run(). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run. _run_make_examples(pipeline_args). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples. _wait_for_results(threads, results). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results. result.get(). File",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:4107,usability,statu,status,4107,"a/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse"": exit status 1: /examples_output.tfrecord@""${SHARDS}"".gz\n --reads ""/input-gcsfused-{}/${BAM}""\n --ref ""${INPUT_REF}""\n --task {}\n --regions gs://canis/CNR-data/CDS-canonical.bed"" # ENABLE_FUSE\n']. Traceback (most recent call last):. File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>. run(). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run. _run_make_examples(pipeline_args). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples. _wait_for_results(threads, results). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results. result.get(). File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get. raise self._value. RuntimeError: Job failed with error ""run"": operation ""projects/valis-194104/operations/781565864516461293"" failed: executing pipeline: Execution failed: action 6: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION). details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 1. exitStatus: 1. stderr: |+. /examples_output.tfrecord@""${SHARDS}"".gz\n --reads ""/input-gcsfused-{}/${BAM}""\n --ref ""${INPUT_REF}""\n --task {}\n --regions gs://canis/CNR-data/CDS-canonical.bed"" # ENABLE_FUSE\n']. Traceback (most recent call last):. File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>. run(). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run. _run_make_examples(pipeline_args). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples. _wait_for_results(threads, results). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results. result.get(). File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get. raise self._value. RuntimeError: Job failed with error ""run"": operation ""projects/va",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:4343,usability,input,input-gcsfused,4343,"E\n']. Traceback (most recent call last):. File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>. run(). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run. _run_make_examples(pipeline_args). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples. _wait_for_results(threads, results). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results. result.get(). File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get. raise self._value. RuntimeError: Job failed with error ""run"": operation ""projects/valis-194104/operations/781565864516461293"" failed: executing pipeline: Execution failed: action 6: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION). details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 1. exitStatus: 1. stderr: |+. /examples_output.tfrecord@""${SHARDS}"".gz\n --reads ""/input-gcsfused-{}/${BAM}""\n --ref ""${INPUT_REF}""\n --task {}\n --regions gs://canis/CNR-data/CDS-canonical.bed"" # ENABLE_FUSE\n']. Traceback (most recent call last):. File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>. run(). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run. _run_make_examples(pipeline_args). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples. _wait_for_results(threads, results). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results. result.get(). File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get. raise self._value. RuntimeError: Job failed with error ""run"": operation ""projects/valis-194104/operations/781565864516461293"" failed: executing pipeline: Execution failed: action 6: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION). timestamp: '2018-11-08T14:30:55.372655Z'. - description: Started runni",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:5075,usability,error,error,5075,"d: action 6: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION). details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent. actionId: 1. exitStatus: 1. stderr: |+. /examples_output.tfrecord@""${SHARDS}"".gz\n --reads ""/input-gcsfused-{}/${BAM}""\n --ref ""${INPUT_REF}""\n --task {}\n --regions gs://canis/CNR-data/CDS-canonical.bed"" # ENABLE_FUSE\n']. Traceback (most recent call last):. File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>. run(). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run. _run_make_examples(pipeline_args). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples. _wait_for_results(threads, results). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results. result.get(). File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get. raise self._value. RuntimeError: Job failed with error ""run"": operation ""projects/valis-194104/operations/781565864516461293"" failed: executing pipeline: Execution failed: action 6: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION). timestamp: '2018-11-08T14:30:55.372655Z'. - description: Started running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project. valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse"". details:. '@type': type.google",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:5224,usability,statu,status,5224,"ContainerStoppedEvent. actionId: 1. exitStatus: 1. stderr: |+. /examples_output.tfrecord@""${SHARDS}"".gz\n --reads ""/input-gcsfused-{}/${BAM}""\n --ref ""${INPUT_REF}""\n --task {}\n --regions gs://canis/CNR-data/CDS-canonical.bed"" # ENABLE_FUSE\n']. Traceback (most recent call last):. File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>. run(). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run. _run_make_examples(pipeline_args). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples. _wait_for_results(threads, results). File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results. result.get(). File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get. raise self._value. RuntimeError: Job failed with error ""run"": operation ""projects/valis-194104/operations/781565864516461293"" failed: executing pipeline: Execution failed: action 6: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION). timestamp: '2018-11-08T14:30:55.372655Z'. - description: Started running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project. valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent. actionId: 1. ipAddress: ''. portMappings: {}. timestamp: '2018-11-08T14:28:16.559791Z'. - de",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:6238,usability,Stop,Stopped,6238,"gnored (reason: FAILED_PRECONDITION). timestamp: '2018-11-08T14:30:55.372655Z'. - description: Started running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project. valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent. actionId: 1. ipAddress: ''. portMappings: {}. timestamp: '2018-11-08T14:28:16.559791Z'. - description: Stopped pulling ""google/cloud-sdk:alpine"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.PullStoppedEvent. imageUri: google/cloud-sdk:alpine. timestamp: '2018-11-08T14:28:12.427407Z'. - description: Started pulling ""google/cloud-sdk:alpine"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.PullStartedEvent. imageUri: google/cloud-sdk:alpine. timestamp: '2018-11-08T14:28:05.897359Z'. - description: Stopped pulling ""gcr.io/deepvariant-docker/deepvariant_runner:0.7.0"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.PullStoppedEvent. imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0. timestamp: '2018-11-08T14:28:05.747135Z'. - description: Started pulling ""gcr.io/deepvariant-docker/deepvariant_runner:0.7.0"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.PullStartedEvent. imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0. timestamp: '2018-11-08T14:27:34.961215Z'. - description: Worker ""google-pipelines-wor",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:6672,usability,Stop,Stopped,6672,"ant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent. actionId: 1. ipAddress: ''. portMappings: {}. timestamp: '2018-11-08T14:28:16.559791Z'. - description: Stopped pulling ""google/cloud-sdk:alpine"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.PullStoppedEvent. imageUri: google/cloud-sdk:alpine. timestamp: '2018-11-08T14:28:12.427407Z'. - description: Started pulling ""google/cloud-sdk:alpine"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.PullStartedEvent. imageUri: google/cloud-sdk:alpine. timestamp: '2018-11-08T14:28:05.897359Z'. - description: Stopped pulling ""gcr.io/deepvariant-docker/deepvariant_runner:0.7.0"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.PullStoppedEvent. imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0. timestamp: '2018-11-08T14:28:05.747135Z'. - description: Started pulling ""gcr.io/deepvariant-docker/deepvariant_runner:0.7.0"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.PullStartedEvent. imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0. timestamp: '2018-11-08T14:27:34.961215Z'. - description: Worker ""google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7"". assigned in ""us-west1-b"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.WorkerAssignedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:27:06.604193Z'. labels: {}. pipeline:. actions:. - commands:. - -c. - /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project valis-194104 --zones. us-west1-b --docker",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:7555,usability,command,commands,7555,"a1.PullStartedEvent. imageUri: google/cloud-sdk:alpine. timestamp: '2018-11-08T14:28:05.897359Z'. - description: Stopped pulling ""gcr.io/deepvariant-docker/deepvariant_runner:0.7.0"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.PullStoppedEvent. imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0. timestamp: '2018-11-08T14:28:05.747135Z'. - description: Started pulling ""gcr.io/deepvariant-docker/deepvariant_runner:0.7.0"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.PullStartedEvent. imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0. timestamp: '2018-11-08T14:27:34.961215Z'. - description: Worker ""google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7"". assigned in ""us-west1-b"". details:. '@type': type.googleapis.com/google.genomics.v2alpha1.WorkerAssignedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:27:06.604193Z'. labels: {}. pipeline:. actions:. - commands:. - -c. - /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project valis-194104 --zones. us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse. entrypoint: bash. environment: {}. flags: []. imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0. labels: {}. mounts: []. name: ''. pidNamespace: ''. portMappings: {}. - commands:. - /bin/sh. - -c. - gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_2",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:8444,usability,command,commands,8444,rAssignedEvent. instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7. zone: us-west1-b. timestamp: '2018-11-08T14:27:06.604193Z'. labels: {}. pipeline:. actions:. - commands:. - -c. - /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project valis-194104 --zones. us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile. gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model. gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions. gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai. gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai. gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse. entrypoint: bash. environment: {}. flags: []. imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0. labels: {}. mounts: []. name: ''. pidNamespace: ''. portMappings: {}. - commands:. - /bin/sh. - -c. - gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log. entrypoint: ''. environment: {}. flags:. - ALWAYS_RUN. imageUri: google/cloud-sdk:alpine. labels: {}. mounts: []. name: ''. pidNamespace: ''. portMappings: {}. environment: {}. resources:. projectId: valis-194104. regions: []. virtualMachine:. accelerators: []. bootDiskSizeGb: 10. bootImage: projects/cos-cloud/global/images/family/cos-stable. cpuPlatform: ''. disks: []. labels: {}. machineType: n1-standard-1. nvidiaDriverVersion: ''. preemptible: false. serviceAccount:. email: default. scopes:. - https://www.googleapis.com/auth/cloud-platform. - https://www.googleapis.com/auth/devstorage.read_write. - https://www.googleapis.com/auth/genomics. zones:. - us-west1-b. timeout: 604800s. startTime: '2018-11-08T14:27:06.604193Z'. name: projects/valis-194104/operations/12097970745380060156. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:26,energy efficiency,Cloud,Cloud,26,@nmousavi from the Google Cloud team to take a look. I'll also take a look later today when I find some more time.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:109,performance,time,time,109,@nmousavi from the Google Cloud team to take a look. I'll also take a look later today when I find some more time.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:32,security,team,team,32,@nmousavi from the Google Cloud team to take a look. I'll also take a look later today when I find some more time.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:63,availability,failur,failure,63,"As @AndrewCarroll said, we need workers log to investigate the failure. Please share workers log (you should be able to find them under `gs://canis/CNR-data/deep_variant_files/logs`).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:40,deployability,log,log,40,"As @AndrewCarroll said, we need workers log to investigate the failure. Please share workers log (you should be able to find them under `gs://canis/CNR-data/deep_variant_files/logs`).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:63,deployability,fail,failure,63,"As @AndrewCarroll said, we need workers log to investigate the failure. Please share workers log (you should be able to find them under `gs://canis/CNR-data/deep_variant_files/logs`).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:93,deployability,log,log,93,"As @AndrewCarroll said, we need workers log to investigate the failure. Please share workers log (you should be able to find them under `gs://canis/CNR-data/deep_variant_files/logs`).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:176,deployability,log,logs,176,"As @AndrewCarroll said, we need workers log to investigate the failure. Please share workers log (you should be able to find them under `gs://canis/CNR-data/deep_variant_files/logs`).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:79,interoperability,share,share,79,"As @AndrewCarroll said, we need workers log to investigate the failure. Please share workers log (you should be able to find them under `gs://canis/CNR-data/deep_variant_files/logs`).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:63,performance,failur,failure,63,"As @AndrewCarroll said, we need workers log to investigate the failure. Please share workers log (you should be able to find them under `gs://canis/CNR-data/deep_variant_files/logs`).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:63,reliability,fail,failure,63,"As @AndrewCarroll said, we need workers log to investigate the failure. Please share workers log (you should be able to find them under `gs://canis/CNR-data/deep_variant_files/logs`).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:40,safety,log,log,40,"As @AndrewCarroll said, we need workers log to investigate the failure. Please share workers log (you should be able to find them under `gs://canis/CNR-data/deep_variant_files/logs`).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:93,safety,log,log,93,"As @AndrewCarroll said, we need workers log to investigate the failure. Please share workers log (you should be able to find them under `gs://canis/CNR-data/deep_variant_files/logs`).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:176,safety,log,logs,176,"As @AndrewCarroll said, we need workers log to investigate the failure. Please share workers log (you should be able to find them under `gs://canis/CNR-data/deep_variant_files/logs`).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:40,security,log,log,40,"As @AndrewCarroll said, we need workers log to investigate the failure. Please share workers log (you should be able to find them under `gs://canis/CNR-data/deep_variant_files/logs`).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:93,security,log,log,93,"As @AndrewCarroll said, we need workers log to investigate the failure. Please share workers log (you should be able to find them under `gs://canis/CNR-data/deep_variant_files/logs`).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:176,security,log,logs,176,"As @AndrewCarroll said, we need workers log to investigate the failure. Please share workers log (you should be able to find them under `gs://canis/CNR-data/deep_variant_files/logs`).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:40,testability,log,log,40,"As @AndrewCarroll said, we need workers log to investigate the failure. Please share workers log (you should be able to find them under `gs://canis/CNR-data/deep_variant_files/logs`).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:93,testability,log,log,93,"As @AndrewCarroll said, we need workers log to investigate the failure. Please share workers log (you should be able to find them under `gs://canis/CNR-data/deep_variant_files/logs`).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:176,testability,log,logs,176,"As @AndrewCarroll said, we need workers log to investigate the failure. Please share workers log (you should be able to find them under `gs://canis/CNR-data/deep_variant_files/logs`).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:66,deployability,log,logs,66,Actually the answer is right in front of you - you don't need the logs. The prize goes to the first one who sees it :),MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:66,safety,log,logs,66,Actually the answer is right in front of you - you don't need the logs. The prize goes to the first one who sees it :),MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:66,security,log,logs,66,Actually the answer is right in front of you - you don't need the logs. The prize goes to the first one who sees it :),MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:66,testability,log,logs,66,Actually the answer is right in front of you - you don't need the logs. The prize goes to the first one who sees it :),MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:74,availability,sli,slim,74,"@saliksyed Try replacing `google/cloud-sdk:alpine` with `google/cloud-sdk:slim`, and then run it again to see if that fixes it. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:33,energy efficiency,cloud,cloud-sdk,33,"@saliksyed Try replacing `google/cloud-sdk:alpine` with `google/cloud-sdk:slim`, and then run it again to see if that fixes it. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:64,energy efficiency,cloud,cloud-sdk,64,"@saliksyed Try replacing `google/cloud-sdk:alpine` with `google/cloud-sdk:slim`, and then run it again to see if that fixes it. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:74,reliability,sli,slim,74,"@saliksyed Try replacing `google/cloud-sdk:alpine` with `google/cloud-sdk:slim`, and then run it again to see if that fixes it. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:43,deployability,version,version,43,I'm not sure how to specify the gcloud sdk version in the launch script -- is there an option for that / where would I find that information?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:43,integrability,version,version,43,I'm not sure how to specify the gcloud sdk version in the launch script -- is there an option for that / where would I find that information?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:20,interoperability,specif,specify,20,I'm not sure how to specify the gcloud sdk version in the launch script -- is there an option for that / where would I find that information?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:43,modifiability,version,version,43,I'm not sure how to specify the gcloud sdk version in the launch script -- is there an option for that / where would I find that information?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:128,availability,sli,slim,128,"The Docker container for the Google pipeline backend would need to be fixed. Take a look, I can cause an exit status of 1 with `slim` and `alpine`, but I can only get a clean exit from `slim` - basically it's getting stuck with `alpine` and not exiting:. ```. paul$ sudo docker run google/cloud-sdk:slim gsutil -m cp /not/there gs://my-bucket/. CommandException: No URLs matched: /not/there. CommandException: 1 file/object could not be transferred. paul$ sudo docker run google/cloud-sdk:alpine gsutil -m cp /not/there gs://my-bucket/. CommandException: No URLs matched: /not/there. CommandException: 1 file/object could not be transferred. ```. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:186,availability,sli,slim,186,"The Docker container for the Google pipeline backend would need to be fixed. Take a look, I can cause an exit status of 1 with `slim` and `alpine`, but I can only get a clean exit from `slim` - basically it's getting stuck with `alpine` and not exiting:. ```. paul$ sudo docker run google/cloud-sdk:slim gsutil -m cp /not/there gs://my-bucket/. CommandException: No URLs matched: /not/there. CommandException: 1 file/object could not be transferred. paul$ sudo docker run google/cloud-sdk:alpine gsutil -m cp /not/there gs://my-bucket/. CommandException: No URLs matched: /not/there. CommandException: 1 file/object could not be transferred. ```. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:299,availability,sli,slim,299,"The Docker container for the Google pipeline backend would need to be fixed. Take a look, I can cause an exit status of 1 with `slim` and `alpine`, but I can only get a clean exit from `slim` - basically it's getting stuck with `alpine` and not exiting:. ```. paul$ sudo docker run google/cloud-sdk:slim gsutil -m cp /not/there gs://my-bucket/. CommandException: No URLs matched: /not/there. CommandException: 1 file/object could not be transferred. paul$ sudo docker run google/cloud-sdk:alpine gsutil -m cp /not/there gs://my-bucket/. CommandException: No URLs matched: /not/there. CommandException: 1 file/object could not be transferred. ```. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:11,deployability,contain,container,11,"The Docker container for the Google pipeline backend would need to be fixed. Take a look, I can cause an exit status of 1 with `slim` and `alpine`, but I can only get a clean exit from `slim` - basically it's getting stuck with `alpine` and not exiting:. ```. paul$ sudo docker run google/cloud-sdk:slim gsutil -m cp /not/there gs://my-bucket/. CommandException: No URLs matched: /not/there. CommandException: 1 file/object could not be transferred. paul$ sudo docker run google/cloud-sdk:alpine gsutil -m cp /not/there gs://my-bucket/. CommandException: No URLs matched: /not/there. CommandException: 1 file/object could not be transferred. ```. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:36,deployability,pipelin,pipeline,36,"The Docker container for the Google pipeline backend would need to be fixed. Take a look, I can cause an exit status of 1 with `slim` and `alpine`, but I can only get a clean exit from `slim` - basically it's getting stuck with `alpine` and not exiting:. ```. paul$ sudo docker run google/cloud-sdk:slim gsutil -m cp /not/there gs://my-bucket/. CommandException: No URLs matched: /not/there. CommandException: 1 file/object could not be transferred. paul$ sudo docker run google/cloud-sdk:alpine gsutil -m cp /not/there gs://my-bucket/. CommandException: No URLs matched: /not/there. CommandException: 1 file/object could not be transferred. ```. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:289,energy efficiency,cloud,cloud-sdk,289,"The Docker container for the Google pipeline backend would need to be fixed. Take a look, I can cause an exit status of 1 with `slim` and `alpine`, but I can only get a clean exit from `slim` - basically it's getting stuck with `alpine` and not exiting:. ```. paul$ sudo docker run google/cloud-sdk:slim gsutil -m cp /not/there gs://my-bucket/. CommandException: No URLs matched: /not/there. CommandException: 1 file/object could not be transferred. paul$ sudo docker run google/cloud-sdk:alpine gsutil -m cp /not/there gs://my-bucket/. CommandException: No URLs matched: /not/there. CommandException: 1 file/object could not be transferred. ```. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:479,energy efficiency,cloud,cloud-sdk,479,"The Docker container for the Google pipeline backend would need to be fixed. Take a look, I can cause an exit status of 1 with `slim` and `alpine`, but I can only get a clean exit from `slim` - basically it's getting stuck with `alpine` and not exiting:. ```. paul$ sudo docker run google/cloud-sdk:slim gsutil -m cp /not/there gs://my-bucket/. CommandException: No URLs matched: /not/there. CommandException: 1 file/object could not be transferred. paul$ sudo docker run google/cloud-sdk:alpine gsutil -m cp /not/there gs://my-bucket/. CommandException: No URLs matched: /not/there. CommandException: 1 file/object could not be transferred. ```. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:36,integrability,pipelin,pipeline,36,"The Docker container for the Google pipeline backend would need to be fixed. Take a look, I can cause an exit status of 1 with `slim` and `alpine`, but I can only get a clean exit from `slim` - basically it's getting stuck with `alpine` and not exiting:. ```. paul$ sudo docker run google/cloud-sdk:slim gsutil -m cp /not/there gs://my-bucket/. CommandException: No URLs matched: /not/there. CommandException: 1 file/object could not be transferred. paul$ sudo docker run google/cloud-sdk:alpine gsutil -m cp /not/there gs://my-bucket/. CommandException: No URLs matched: /not/there. CommandException: 1 file/object could not be transferred. ```. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:128,reliability,sli,slim,128,"The Docker container for the Google pipeline backend would need to be fixed. Take a look, I can cause an exit status of 1 with `slim` and `alpine`, but I can only get a clean exit from `slim` - basically it's getting stuck with `alpine` and not exiting:. ```. paul$ sudo docker run google/cloud-sdk:slim gsutil -m cp /not/there gs://my-bucket/. CommandException: No URLs matched: /not/there. CommandException: 1 file/object could not be transferred. paul$ sudo docker run google/cloud-sdk:alpine gsutil -m cp /not/there gs://my-bucket/. CommandException: No URLs matched: /not/there. CommandException: 1 file/object could not be transferred. ```. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:186,reliability,sli,slim,186,"The Docker container for the Google pipeline backend would need to be fixed. Take a look, I can cause an exit status of 1 with `slim` and `alpine`, but I can only get a clean exit from `slim` - basically it's getting stuck with `alpine` and not exiting:. ```. paul$ sudo docker run google/cloud-sdk:slim gsutil -m cp /not/there gs://my-bucket/. CommandException: No URLs matched: /not/there. CommandException: 1 file/object could not be transferred. paul$ sudo docker run google/cloud-sdk:alpine gsutil -m cp /not/there gs://my-bucket/. CommandException: No URLs matched: /not/there. CommandException: 1 file/object could not be transferred. ```. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:299,reliability,sli,slim,299,"The Docker container for the Google pipeline backend would need to be fixed. Take a look, I can cause an exit status of 1 with `slim` and `alpine`, but I can only get a clean exit from `slim` - basically it's getting stuck with `alpine` and not exiting:. ```. paul$ sudo docker run google/cloud-sdk:slim gsutil -m cp /not/there gs://my-bucket/. CommandException: No URLs matched: /not/there. CommandException: 1 file/object could not be transferred. paul$ sudo docker run google/cloud-sdk:alpine gsutil -m cp /not/there gs://my-bucket/. CommandException: No URLs matched: /not/there. CommandException: 1 file/object could not be transferred. ```. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:110,usability,statu,status,110,"The Docker container for the Google pipeline backend would need to be fixed. Take a look, I can cause an exit status of 1 with `slim` and `alpine`, but I can only get a clean exit from `slim` - basically it's getting stuck with `alpine` and not exiting:. ```. paul$ sudo docker run google/cloud-sdk:slim gsutil -m cp /not/there gs://my-bucket/. CommandException: No URLs matched: /not/there. CommandException: 1 file/object could not be transferred. paul$ sudo docker run google/cloud-sdk:alpine gsutil -m cp /not/there gs://my-bucket/. CommandException: No URLs matched: /not/there. CommandException: 1 file/object could not be transferred. ```. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:345,usability,Command,CommandException,345,"The Docker container for the Google pipeline backend would need to be fixed. Take a look, I can cause an exit status of 1 with `slim` and `alpine`, but I can only get a clean exit from `slim` - basically it's getting stuck with `alpine` and not exiting:. ```. paul$ sudo docker run google/cloud-sdk:slim gsutil -m cp /not/there gs://my-bucket/. CommandException: No URLs matched: /not/there. CommandException: 1 file/object could not be transferred. paul$ sudo docker run google/cloud-sdk:alpine gsutil -m cp /not/there gs://my-bucket/. CommandException: No URLs matched: /not/there. CommandException: 1 file/object could not be transferred. ```. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:392,usability,Command,CommandException,392,"The Docker container for the Google pipeline backend would need to be fixed. Take a look, I can cause an exit status of 1 with `slim` and `alpine`, but I can only get a clean exit from `slim` - basically it's getting stuck with `alpine` and not exiting:. ```. paul$ sudo docker run google/cloud-sdk:slim gsutil -m cp /not/there gs://my-bucket/. CommandException: No URLs matched: /not/there. CommandException: 1 file/object could not be transferred. paul$ sudo docker run google/cloud-sdk:alpine gsutil -m cp /not/there gs://my-bucket/. CommandException: No URLs matched: /not/there. CommandException: 1 file/object could not be transferred. ```. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:537,usability,Command,CommandException,537,"The Docker container for the Google pipeline backend would need to be fixed. Take a look, I can cause an exit status of 1 with `slim` and `alpine`, but I can only get a clean exit from `slim` - basically it's getting stuck with `alpine` and not exiting:. ```. paul$ sudo docker run google/cloud-sdk:slim gsutil -m cp /not/there gs://my-bucket/. CommandException: No URLs matched: /not/there. CommandException: 1 file/object could not be transferred. paul$ sudo docker run google/cloud-sdk:alpine gsutil -m cp /not/there gs://my-bucket/. CommandException: No URLs matched: /not/there. CommandException: 1 file/object could not be transferred. ```. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:584,usability,Command,CommandException,584,"The Docker container for the Google pipeline backend would need to be fixed. Take a look, I can cause an exit status of 1 with `slim` and `alpine`, but I can only get a clean exit from `slim` - basically it's getting stuck with `alpine` and not exiting:. ```. paul$ sudo docker run google/cloud-sdk:slim gsutil -m cp /not/there gs://my-bucket/. CommandException: No URLs matched: /not/there. CommandException: 1 file/object could not be transferred. paul$ sudo docker run google/cloud-sdk:alpine gsutil -m cp /not/there gs://my-bucket/. CommandException: No URLs matched: /not/there. CommandException: 1 file/object could not be transferred. ```. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:655,usability,help,helps,655,"The Docker container for the Google pipeline backend would need to be fixed. Take a look, I can cause an exit status of 1 with `slim` and `alpine`, but I can only get a clean exit from `slim` - basically it's getting stuck with `alpine` and not exiting:. ```. paul$ sudo docker run google/cloud-sdk:slim gsutil -m cp /not/there gs://my-bucket/. CommandException: No URLs matched: /not/there. CommandException: 1 file/object could not be transferred. paul$ sudo docker run google/cloud-sdk:alpine gsutil -m cp /not/there gs://my-bucket/. CommandException: No URLs matched: /not/there. CommandException: 1 file/object could not be transferred. ```. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:28,deployability,log,log,28,"Again, please share workers log so we can help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:14,interoperability,share,share,14,"Again, please share workers log so we can help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:28,safety,log,log,28,"Again, please share workers log so we can help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:28,security,log,log,28,"Again, please share workers log so we can help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:28,testability,log,log,28,"Again, please share workers log so we can help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:42,usability,help,help,42,"Again, please share workers log so we can help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:243,availability,sli,slim,243,"@nmousavi I made a readonly public bucket you can access here:. http://storage.googleapis.com/public-debug. @pgrosu I am not using Docker. I am running the shell script from my local machine. Do I need to update my google-cloud-sdk version to slim? Or are you referring to the DeepVariant container image itself? If so, I am using the Cloud Genomics API -- not hosting my own image of DeepVariant... see:. https://cloud.google.com/genomics/docs/tutorials/deepvariant",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:205,deployability,updat,update,205,"@nmousavi I made a readonly public bucket you can access here:. http://storage.googleapis.com/public-debug. @pgrosu I am not using Docker. I am running the shell script from my local machine. Do I need to update my google-cloud-sdk version to slim? Or are you referring to the DeepVariant container image itself? If so, I am using the Cloud Genomics API -- not hosting my own image of DeepVariant... see:. https://cloud.google.com/genomics/docs/tutorials/deepvariant",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:232,deployability,version,version,232,"@nmousavi I made a readonly public bucket you can access here:. http://storage.googleapis.com/public-debug. @pgrosu I am not using Docker. I am running the shell script from my local machine. Do I need to update my google-cloud-sdk version to slim? Or are you referring to the DeepVariant container image itself? If so, I am using the Cloud Genomics API -- not hosting my own image of DeepVariant... see:. https://cloud.google.com/genomics/docs/tutorials/deepvariant",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:289,deployability,contain,container,289,"@nmousavi I made a readonly public bucket you can access here:. http://storage.googleapis.com/public-debug. @pgrosu I am not using Docker. I am running the shell script from my local machine. Do I need to update my google-cloud-sdk version to slim? Or are you referring to the DeepVariant container image itself? If so, I am using the Cloud Genomics API -- not hosting my own image of DeepVariant... see:. https://cloud.google.com/genomics/docs/tutorials/deepvariant",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:350,deployability,API,API,350,"@nmousavi I made a readonly public bucket you can access here:. http://storage.googleapis.com/public-debug. @pgrosu I am not using Docker. I am running the shell script from my local machine. Do I need to update my google-cloud-sdk version to slim? Or are you referring to the DeepVariant container image itself? If so, I am using the Cloud Genomics API -- not hosting my own image of DeepVariant... see:. https://cloud.google.com/genomics/docs/tutorials/deepvariant",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:222,energy efficiency,cloud,cloud-sdk,222,"@nmousavi I made a readonly public bucket you can access here:. http://storage.googleapis.com/public-debug. @pgrosu I am not using Docker. I am running the shell script from my local machine. Do I need to update my google-cloud-sdk version to slim? Or are you referring to the DeepVariant container image itself? If so, I am using the Cloud Genomics API -- not hosting my own image of DeepVariant... see:. https://cloud.google.com/genomics/docs/tutorials/deepvariant",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:335,energy efficiency,Cloud,Cloud,335,"@nmousavi I made a readonly public bucket you can access here:. http://storage.googleapis.com/public-debug. @pgrosu I am not using Docker. I am running the shell script from my local machine. Do I need to update my google-cloud-sdk version to slim? Or are you referring to the DeepVariant container image itself? If so, I am using the Cloud Genomics API -- not hosting my own image of DeepVariant... see:. https://cloud.google.com/genomics/docs/tutorials/deepvariant",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:414,energy efficiency,cloud,cloud,414,"@nmousavi I made a readonly public bucket you can access here:. http://storage.googleapis.com/public-debug. @pgrosu I am not using Docker. I am running the shell script from my local machine. Do I need to update my google-cloud-sdk version to slim? Or are you referring to the DeepVariant container image itself? If so, I am using the Cloud Genomics API -- not hosting my own image of DeepVariant... see:. https://cloud.google.com/genomics/docs/tutorials/deepvariant",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:28,integrability,pub,public,28,"@nmousavi I made a readonly public bucket you can access here:. http://storage.googleapis.com/public-debug. @pgrosu I am not using Docker. I am running the shell script from my local machine. Do I need to update my google-cloud-sdk version to slim? Or are you referring to the DeepVariant container image itself? If so, I am using the Cloud Genomics API -- not hosting my own image of DeepVariant... see:. https://cloud.google.com/genomics/docs/tutorials/deepvariant",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:94,integrability,pub,public-debug,94,"@nmousavi I made a readonly public bucket you can access here:. http://storage.googleapis.com/public-debug. @pgrosu I am not using Docker. I am running the shell script from my local machine. Do I need to update my google-cloud-sdk version to slim? Or are you referring to the DeepVariant container image itself? If so, I am using the Cloud Genomics API -- not hosting my own image of DeepVariant... see:. https://cloud.google.com/genomics/docs/tutorials/deepvariant",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:232,integrability,version,version,232,"@nmousavi I made a readonly public bucket you can access here:. http://storage.googleapis.com/public-debug. @pgrosu I am not using Docker. I am running the shell script from my local machine. Do I need to update my google-cloud-sdk version to slim? Or are you referring to the DeepVariant container image itself? If so, I am using the Cloud Genomics API -- not hosting my own image of DeepVariant... see:. https://cloud.google.com/genomics/docs/tutorials/deepvariant",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:350,integrability,API,API,350,"@nmousavi I made a readonly public bucket you can access here:. http://storage.googleapis.com/public-debug. @pgrosu I am not using Docker. I am running the shell script from my local machine. Do I need to update my google-cloud-sdk version to slim? Or are you referring to the DeepVariant container image itself? If so, I am using the Cloud Genomics API -- not hosting my own image of DeepVariant... see:. https://cloud.google.com/genomics/docs/tutorials/deepvariant",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:350,interoperability,API,API,350,"@nmousavi I made a readonly public bucket you can access here:. http://storage.googleapis.com/public-debug. @pgrosu I am not using Docker. I am running the shell script from my local machine. Do I need to update my google-cloud-sdk version to slim? Or are you referring to the DeepVariant container image itself? If so, I am using the Cloud Genomics API -- not hosting my own image of DeepVariant... see:. https://cloud.google.com/genomics/docs/tutorials/deepvariant",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:232,modifiability,version,version,232,"@nmousavi I made a readonly public bucket you can access here:. http://storage.googleapis.com/public-debug. @pgrosu I am not using Docker. I am running the shell script from my local machine. Do I need to update my google-cloud-sdk version to slim? Or are you referring to the DeepVariant container image itself? If so, I am using the Cloud Genomics API -- not hosting my own image of DeepVariant... see:. https://cloud.google.com/genomics/docs/tutorials/deepvariant",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:243,reliability,sli,slim,243,"@nmousavi I made a readonly public bucket you can access here:. http://storage.googleapis.com/public-debug. @pgrosu I am not using Docker. I am running the shell script from my local machine. Do I need to update my google-cloud-sdk version to slim? Or are you referring to the DeepVariant container image itself? If so, I am using the Cloud Genomics API -- not hosting my own image of DeepVariant... see:. https://cloud.google.com/genomics/docs/tutorials/deepvariant",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:205,safety,updat,update,205,"@nmousavi I made a readonly public bucket you can access here:. http://storage.googleapis.com/public-debug. @pgrosu I am not using Docker. I am running the shell script from my local machine. Do I need to update my google-cloud-sdk version to slim? Or are you referring to the DeepVariant container image itself? If so, I am using the Cloud Genomics API -- not hosting my own image of DeepVariant... see:. https://cloud.google.com/genomics/docs/tutorials/deepvariant",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:50,security,access,access,50,"@nmousavi I made a readonly public bucket you can access here:. http://storage.googleapis.com/public-debug. @pgrosu I am not using Docker. I am running the shell script from my local machine. Do I need to update my google-cloud-sdk version to slim? Or are you referring to the DeepVariant container image itself? If so, I am using the Cloud Genomics API -- not hosting my own image of DeepVariant... see:. https://cloud.google.com/genomics/docs/tutorials/deepvariant",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:205,security,updat,update,205,"@nmousavi I made a readonly public bucket you can access here:. http://storage.googleapis.com/public-debug. @pgrosu I am not using Docker. I am running the shell script from my local machine. Do I need to update my google-cloud-sdk version to slim? Or are you referring to the DeepVariant container image itself? If so, I am using the Cloud Genomics API -- not hosting my own image of DeepVariant... see:. https://cloud.google.com/genomics/docs/tutorials/deepvariant",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:142,deployability,log,log,142,"It has nothing to do with docker image. The problem is with input file (with err `Unrecognized SAM header type`). Pasting make_example worker log:. ```. Unrecognized SAM header type, ignoring:. I1108 21:36:46.455516 140363989006080 genomics_reader.py:213] Reading /input-gcsfused-2/CNR-data/TLE_a_001.bam with NativeSamReader. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1178, in main. make_examples_runner(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1076, in make_examples_runner. regions = processing_regions_from_options(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 990, in processing_regions_from_options. options.min_shared_contigs_basepairs). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 485, in _ensure_consistent_contigs. min_coverage_fraction). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 557, in validate_reference_contig_coverage. ref_bp, common_bp, coverage, format_contig_matches())). ValueError: Reference contigs span 3088269832 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sources were created on a common genome reference build. Contig matches were: ""chr1"" is 248956422 bp and IS MISSING, ""chr2"" is 242193529 bp and IS MISSING, ""chr3"" is 198295559 bp and IS MISSING, ""chr4"" is 190214555 bp and IS MISSING, ""chr5"" is 181538259 bp ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:493,deployability,modul,module,493,"It has nothing to do with docker image. The problem is with input file (with err `Unrecognized SAM header type`). Pasting make_example worker log:. ```. Unrecognized SAM header type, ignoring:. I1108 21:36:46.455516 140363989006080 genomics_reader.py:213] Reading /input-gcsfused-2/CNR-data/TLE_a_001.bam with NativeSamReader. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1178, in main. make_examples_runner(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1076, in make_examples_runner. regions = processing_regions_from_options(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 990, in processing_regions_from_options. options.min_shared_contigs_basepairs). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 485, in _ensure_consistent_contigs. min_coverage_fraction). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 557, in validate_reference_contig_coverage. ref_bp, common_bp, coverage, format_contig_matches())). ValueError: Reference contigs span 3088269832 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sources were created on a common genome reference build. Contig matches were: ""chr1"" is 248956422 bp and IS MISSING, ""chr2"" is 242193529 bp and IS MISSING, ""chr3"" is 198295559 bp and IS MISSING, ""chr4"" is 190214555 bp and IS MISSING, ""chr5"" is 181538259 bp ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:1793,deployability,build,build,1793,"tions). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1076, in make_examples_runner. regions = processing_regions_from_options(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 990, in processing_regions_from_options. options.min_shared_contigs_basepairs). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 485, in _ensure_consistent_contigs. min_coverage_fraction). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 557, in validate_reference_contig_coverage. ref_bp, common_bp, coverage, format_contig_matches())). ValueError: Reference contigs span 3088269832 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sources were created on a common genome reference build. Contig matches were: ""chr1"" is 248956422 bp and IS MISSING, ""chr2"" is 242193529 bp and IS MISSING, ""chr3"" is 198295559 bp and IS MISSING, ""chr4"" is 190214555 bp and IS MISSING, ""chr5"" is 181538259 bp and IS MISSING, ""chr6"" is 170805979 bp and IS MISSING, ""chr7"" is 159345973 bp and IS MISSING, ""chr8"" is 145138636 bp and IS MISSING, ""chr9"" is 138394717 bp and IS MISSING, ""chr10"" is 133797422 bp and IS MISSING, ""chr11"" is 135086622 bp and IS MISSING, ""chr12"" is 133275309 bp and IS MISSING, ""chr13"" is 114364328 bp and IS MISSING, ""chr14"" is 107043718 bp and IS MISSING, ""chr15"" is 101991189 bp and IS MISSING, ""chr16"" is 90338345 bp and IS MISSING, ""chr17"" is 83257441 bp and IS MISSING, ""chr18"" is 80373285 bp and IS MISSING, ""chr19"" is 58617616 bp and IS MISSING, ""chr20"" is 64444167 bp and IS MISSING, ""chr21"" is 46709983 bp and IS MISSING, ""chr22"" is 50818468 bp and IS MISSING, ""chrX"" is 156040895 bp and IS MISSING, ""chrY"" is 57227415 bp and IS MISSING. parallel: This job failed:. mkdir ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:2781,deployability,fail,failed,2781,"eepvariant/deepvariant/make_examples.py"", line 485, in _ensure_consistent_contigs. min_coverage_fraction). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 557, in validate_reference_contig_coverage. ref_bp, common_bp, coverage, format_contig_matches())). ValueError: Reference contigs span 3088269832 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sources were created on a common genome reference build. Contig matches were: ""chr1"" is 248956422 bp and IS MISSING, ""chr2"" is 242193529 bp and IS MISSING, ""chr3"" is 198295559 bp and IS MISSING, ""chr4"" is 190214555 bp and IS MISSING, ""chr5"" is 181538259 bp and IS MISSING, ""chr6"" is 170805979 bp and IS MISSING, ""chr7"" is 159345973 bp and IS MISSING, ""chr8"" is 145138636 bp and IS MISSING, ""chr9"" is 138394717 bp and IS MISSING, ""chr10"" is 133797422 bp and IS MISSING, ""chr11"" is 135086622 bp and IS MISSING, ""chr12"" is 133275309 bp and IS MISSING, ""chr13"" is 114364328 bp and IS MISSING, ""chr14"" is 107043718 bp and IS MISSING, ""chr15"" is 101991189 bp and IS MISSING, ""chr16"" is 90338345 bp and IS MISSING, ""chr17"" is 83257441 bp and IS MISSING, ""chr18"" is 80373285 bp and IS MISSING, ""chr19"" is 58617616 bp and IS MISSING, ""chr20"" is 64444167 bp and IS MISSING, ""chr21"" is 46709983 bp and IS MISSING, ""chr22"" is 50818468 bp and IS MISSING, ""chrX"" is 156040895 bp and IS MISSING, ""chrY"" is 57227415 bp and IS MISSING. parallel: This job failed:. mkdir -p ./input-gcsfused-2 && gcsfuse --implicit-dirs canis /input-gcsfused-2 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/canis/CNR-data/deep_variant_files/examples/0/examples_output.tfrecord@8.gz --reads /input-gcsfused-2/CNR-data/TLE_a_001.bam --ref /mnt/google/.google/input/canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --task 2 --regions gs://canis/CNR-data/exomes.bed. Using mount point: /input-gcsfused-2. ```. Assigning it to Pi-Chuan.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:579,interoperability,platform,platform,579,"It has nothing to do with docker image. The problem is with input file (with err `Unrecognized SAM header type`). Pasting make_example worker log:. ```. Unrecognized SAM header type, ignoring:. I1108 21:36:46.455516 140363989006080 genomics_reader.py:213] Reading /input-gcsfused-2/CNR-data/TLE_a_001.bam with NativeSamReader. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1178, in main. make_examples_runner(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1076, in make_examples_runner. regions = processing_regions_from_options(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 990, in processing_regions_from_options. options.min_shared_contigs_basepairs). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 485, in _ensure_consistent_contigs. min_coverage_fraction). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 557, in validate_reference_contig_coverage. ref_bp, common_bp, coverage, format_contig_matches())). ValueError: Reference contigs span 3088269832 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sources were created on a common genome reference build. Contig matches were: ""chr1"" is 248956422 bp and IS MISSING, ""chr2"" is 242193529 bp and IS MISSING, ""chr3"" is 198295559 bp and IS MISSING, ""chr4"" is 190214555 bp and IS MISSING, ""chr5"" is 181538259 bp ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:493,modifiability,modul,module,493,"It has nothing to do with docker image. The problem is with input file (with err `Unrecognized SAM header type`). Pasting make_example worker log:. ```. Unrecognized SAM header type, ignoring:. I1108 21:36:46.455516 140363989006080 genomics_reader.py:213] Reading /input-gcsfused-2/CNR-data/TLE_a_001.bam with NativeSamReader. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1178, in main. make_examples_runner(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1076, in make_examples_runner. regions = processing_regions_from_options(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 990, in processing_regions_from_options. options.min_shared_contigs_basepairs). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 485, in _ensure_consistent_contigs. min_coverage_fraction). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 557, in validate_reference_contig_coverage. ref_bp, common_bp, coverage, format_contig_matches())). ValueError: Reference contigs span 3088269832 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sources were created on a common genome reference build. Contig matches were: ""chr1"" is 248956422 bp and IS MISSING, ""chr2"" is 242193529 bp and IS MISSING, ""chr3"" is 198295559 bp and IS MISSING, ""chr4"" is 190214555 bp and IS MISSING, ""chr5"" is 181538259 bp ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:552,modifiability,pac,packages,552,"It has nothing to do with docker image. The problem is with input file (with err `Unrecognized SAM header type`). Pasting make_example worker log:. ```. Unrecognized SAM header type, ignoring:. I1108 21:36:46.455516 140363989006080 genomics_reader.py:213] Reading /input-gcsfused-2/CNR-data/TLE_a_001.bam with NativeSamReader. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1178, in main. make_examples_runner(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1076, in make_examples_runner. regions = processing_regions_from_options(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 990, in processing_regions_from_options. options.min_shared_contigs_basepairs). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 485, in _ensure_consistent_contigs. min_coverage_fraction). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 557, in validate_reference_contig_coverage. ref_bp, common_bp, coverage, format_contig_matches())). ValueError: Reference contigs span 3088269832 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sources were created on a common genome reference build. Contig matches were: ""chr1"" is 248956422 bp and IS MISSING, ""chr2"" is 242193529 bp and IS MISSING, ""chr3"" is 198295559 bp and IS MISSING, ""chr4"" is 190214555 bp and IS MISSING, ""chr5"" is 181538259 bp ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:2762,performance,parallel,parallel,2762,"eepvariant/deepvariant/make_examples.py"", line 485, in _ensure_consistent_contigs. min_coverage_fraction). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 557, in validate_reference_contig_coverage. ref_bp, common_bp, coverage, format_contig_matches())). ValueError: Reference contigs span 3088269832 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sources were created on a common genome reference build. Contig matches were: ""chr1"" is 248956422 bp and IS MISSING, ""chr2"" is 242193529 bp and IS MISSING, ""chr3"" is 198295559 bp and IS MISSING, ""chr4"" is 190214555 bp and IS MISSING, ""chr5"" is 181538259 bp and IS MISSING, ""chr6"" is 170805979 bp and IS MISSING, ""chr7"" is 159345973 bp and IS MISSING, ""chr8"" is 145138636 bp and IS MISSING, ""chr9"" is 138394717 bp and IS MISSING, ""chr10"" is 133797422 bp and IS MISSING, ""chr11"" is 135086622 bp and IS MISSING, ""chr12"" is 133275309 bp and IS MISSING, ""chr13"" is 114364328 bp and IS MISSING, ""chr14"" is 107043718 bp and IS MISSING, ""chr15"" is 101991189 bp and IS MISSING, ""chr16"" is 90338345 bp and IS MISSING, ""chr17"" is 83257441 bp and IS MISSING, ""chr18"" is 80373285 bp and IS MISSING, ""chr19"" is 58617616 bp and IS MISSING, ""chr20"" is 64444167 bp and IS MISSING, ""chr21"" is 46709983 bp and IS MISSING, ""chr22"" is 50818468 bp and IS MISSING, ""chrX"" is 156040895 bp and IS MISSING, ""chrY"" is 57227415 bp and IS MISSING. parallel: This job failed:. mkdir -p ./input-gcsfused-2 && gcsfuse --implicit-dirs canis /input-gcsfused-2 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/canis/CNR-data/deep_variant_files/examples/0/examples_output.tfrecord@8.gz --reads /input-gcsfused-2/CNR-data/TLE_a_001.bam --ref /mnt/google/.google/input/canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --task 2 --regions gs://canis/CNR-data/exomes.bed. Using mount point: /input-gcsfused-2. ```. Assigning it to Pi-Chuan.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:2781,reliability,fail,failed,2781,"eepvariant/deepvariant/make_examples.py"", line 485, in _ensure_consistent_contigs. min_coverage_fraction). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 557, in validate_reference_contig_coverage. ref_bp, common_bp, coverage, format_contig_matches())). ValueError: Reference contigs span 3088269832 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sources were created on a common genome reference build. Contig matches were: ""chr1"" is 248956422 bp and IS MISSING, ""chr2"" is 242193529 bp and IS MISSING, ""chr3"" is 198295559 bp and IS MISSING, ""chr4"" is 190214555 bp and IS MISSING, ""chr5"" is 181538259 bp and IS MISSING, ""chr6"" is 170805979 bp and IS MISSING, ""chr7"" is 159345973 bp and IS MISSING, ""chr8"" is 145138636 bp and IS MISSING, ""chr9"" is 138394717 bp and IS MISSING, ""chr10"" is 133797422 bp and IS MISSING, ""chr11"" is 135086622 bp and IS MISSING, ""chr12"" is 133275309 bp and IS MISSING, ""chr13"" is 114364328 bp and IS MISSING, ""chr14"" is 107043718 bp and IS MISSING, ""chr15"" is 101991189 bp and IS MISSING, ""chr16"" is 90338345 bp and IS MISSING, ""chr17"" is 83257441 bp and IS MISSING, ""chr18"" is 80373285 bp and IS MISSING, ""chr19"" is 58617616 bp and IS MISSING, ""chr20"" is 64444167 bp and IS MISSING, ""chr21"" is 46709983 bp and IS MISSING, ""chr22"" is 50818468 bp and IS MISSING, ""chrX"" is 156040895 bp and IS MISSING, ""chrY"" is 57227415 bp and IS MISSING. parallel: This job failed:. mkdir -p ./input-gcsfused-2 && gcsfuse --implicit-dirs canis /input-gcsfused-2 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/canis/CNR-data/deep_variant_files/examples/0/examples_output.tfrecord@8.gz --reads /input-gcsfused-2/CNR-data/TLE_a_001.bam --ref /mnt/google/.google/input/canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --task 2 --regions gs://canis/CNR-data/exomes.bed. Using mount point: /input-gcsfused-2. ```. Assigning it to Pi-Chuan.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:60,safety,input,input,60,"It has nothing to do with docker image. The problem is with input file (with err `Unrecognized SAM header type`). Pasting make_example worker log:. ```. Unrecognized SAM header type, ignoring:. I1108 21:36:46.455516 140363989006080 genomics_reader.py:213] Reading /input-gcsfused-2/CNR-data/TLE_a_001.bam with NativeSamReader. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1178, in main. make_examples_runner(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1076, in make_examples_runner. regions = processing_regions_from_options(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 990, in processing_regions_from_options. options.min_shared_contigs_basepairs). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 485, in _ensure_consistent_contigs. min_coverage_fraction). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 557, in validate_reference_contig_coverage. ref_bp, common_bp, coverage, format_contig_matches())). ValueError: Reference contigs span 3088269832 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sources were created on a common genome reference build. Contig matches were: ""chr1"" is 248956422 bp and IS MISSING, ""chr2"" is 242193529 bp and IS MISSING, ""chr3"" is 198295559 bp and IS MISSING, ""chr4"" is 190214555 bp and IS MISSING, ""chr5"" is 181538259 bp ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:142,safety,log,log,142,"It has nothing to do with docker image. The problem is with input file (with err `Unrecognized SAM header type`). Pasting make_example worker log:. ```. Unrecognized SAM header type, ignoring:. I1108 21:36:46.455516 140363989006080 genomics_reader.py:213] Reading /input-gcsfused-2/CNR-data/TLE_a_001.bam with NativeSamReader. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1178, in main. make_examples_runner(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1076, in make_examples_runner. regions = processing_regions_from_options(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 990, in processing_regions_from_options. options.min_shared_contigs_basepairs). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 485, in _ensure_consistent_contigs. min_coverage_fraction). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 557, in validate_reference_contig_coverage. ref_bp, common_bp, coverage, format_contig_matches())). ValueError: Reference contigs span 3088269832 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sources were created on a common genome reference build. Contig matches were: ""chr1"" is 248956422 bp and IS MISSING, ""chr2"" is 242193529 bp and IS MISSING, ""chr3"" is 198295559 bp and IS MISSING, ""chr4"" is 190214555 bp and IS MISSING, ""chr5"" is 181538259 bp ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:265,safety,input,input-gcsfused-,265,"It has nothing to do with docker image. The problem is with input file (with err `Unrecognized SAM header type`). Pasting make_example worker log:. ```. Unrecognized SAM header type, ignoring:. I1108 21:36:46.455516 140363989006080 genomics_reader.py:213] Reading /input-gcsfused-2/CNR-data/TLE_a_001.bam with NativeSamReader. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1178, in main. make_examples_runner(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1076, in make_examples_runner. regions = processing_regions_from_options(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 990, in processing_regions_from_options. options.min_shared_contigs_basepairs). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 485, in _ensure_consistent_contigs. min_coverage_fraction). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 557, in validate_reference_contig_coverage. ref_bp, common_bp, coverage, format_contig_matches())). ValueError: Reference contigs span 3088269832 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sources were created on a common genome reference build. Contig matches were: ""chr1"" is 248956422 bp and IS MISSING, ""chr2"" is 242193529 bp and IS MISSING, ""chr3"" is 198295559 bp and IS MISSING, ""chr4"" is 190214555 bp and IS MISSING, ""chr5"" is 181538259 bp ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:493,safety,modul,module,493,"It has nothing to do with docker image. The problem is with input file (with err `Unrecognized SAM header type`). Pasting make_example worker log:. ```. Unrecognized SAM header type, ignoring:. I1108 21:36:46.455516 140363989006080 genomics_reader.py:213] Reading /input-gcsfused-2/CNR-data/TLE_a_001.bam with NativeSamReader. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1178, in main. make_examples_runner(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1076, in make_examples_runner. regions = processing_regions_from_options(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 990, in processing_regions_from_options. options.min_shared_contigs_basepairs). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 485, in _ensure_consistent_contigs. min_coverage_fraction). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 557, in validate_reference_contig_coverage. ref_bp, common_bp, coverage, format_contig_matches())). ValueError: Reference contigs span 3088269832 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sources were created on a common genome reference build. Contig matches were: ""chr1"" is 248956422 bp and IS MISSING, ""chr2"" is 242193529 bp and IS MISSING, ""chr3"" is 198295559 bp and IS MISSING, ""chr4"" is 190214555 bp and IS MISSING, ""chr5"" is 181538259 bp ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:1715,safety,input,input,1715,"ant/deepvariant/make_examples.py"", line 1178, in main. make_examples_runner(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1076, in make_examples_runner. regions = processing_regions_from_options(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 990, in processing_regions_from_options. options.min_shared_contigs_basepairs). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 485, in _ensure_consistent_contigs. min_coverage_fraction). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 557, in validate_reference_contig_coverage. ref_bp, common_bp, coverage, format_contig_matches())). ValueError: Reference contigs span 3088269832 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sources were created on a common genome reference build. Contig matches were: ""chr1"" is 248956422 bp and IS MISSING, ""chr2"" is 242193529 bp and IS MISSING, ""chr3"" is 198295559 bp and IS MISSING, ""chr4"" is 190214555 bp and IS MISSING, ""chr5"" is 181538259 bp and IS MISSING, ""chr6"" is 170805979 bp and IS MISSING, ""chr7"" is 159345973 bp and IS MISSING, ""chr8"" is 145138636 bp and IS MISSING, ""chr9"" is 138394717 bp and IS MISSING, ""chr10"" is 133797422 bp and IS MISSING, ""chr11"" is 135086622 bp and IS MISSING, ""chr12"" is 133275309 bp and IS MISSING, ""chr13"" is 114364328 bp and IS MISSING, ""chr14"" is 107043718 bp and IS MISSING, ""chr15"" is 101991189 bp and IS MISSING, ""chr16"" is 90338345 bp and IS MISSING, ""chr17"" is 83257441 bp and IS MISSING, ""chr18"" is 80373285 bp and IS MISSING, ""chr19"" is 58617616 bp and IS MISSING, ""chr20"" is 64444167 bp and IS MISSING, ""chr21"" is 46709983 bp and IS MISSING, ""chr22"" is 50818468 bp and IS MISSING, ""chrX"" is 156040895 bp and IS MIS",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:2801,safety,input,input-gcsfused-,2801,"eepvariant/deepvariant/make_examples.py"", line 485, in _ensure_consistent_contigs. min_coverage_fraction). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 557, in validate_reference_contig_coverage. ref_bp, common_bp, coverage, format_contig_matches())). ValueError: Reference contigs span 3088269832 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sources were created on a common genome reference build. Contig matches were: ""chr1"" is 248956422 bp and IS MISSING, ""chr2"" is 242193529 bp and IS MISSING, ""chr3"" is 198295559 bp and IS MISSING, ""chr4"" is 190214555 bp and IS MISSING, ""chr5"" is 181538259 bp and IS MISSING, ""chr6"" is 170805979 bp and IS MISSING, ""chr7"" is 159345973 bp and IS MISSING, ""chr8"" is 145138636 bp and IS MISSING, ""chr9"" is 138394717 bp and IS MISSING, ""chr10"" is 133797422 bp and IS MISSING, ""chr11"" is 135086622 bp and IS MISSING, ""chr12"" is 133275309 bp and IS MISSING, ""chr13"" is 114364328 bp and IS MISSING, ""chr14"" is 107043718 bp and IS MISSING, ""chr15"" is 101991189 bp and IS MISSING, ""chr16"" is 90338345 bp and IS MISSING, ""chr17"" is 83257441 bp and IS MISSING, ""chr18"" is 80373285 bp and IS MISSING, ""chr19"" is 58617616 bp and IS MISSING, ""chr20"" is 64444167 bp and IS MISSING, ""chr21"" is 46709983 bp and IS MISSING, ""chr22"" is 50818468 bp and IS MISSING, ""chrX"" is 156040895 bp and IS MISSING, ""chrY"" is 57227415 bp and IS MISSING. parallel: This job failed:. mkdir -p ./input-gcsfused-2 && gcsfuse --implicit-dirs canis /input-gcsfused-2 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/canis/CNR-data/deep_variant_files/examples/0/examples_output.tfrecord@8.gz --reads /input-gcsfused-2/CNR-data/TLE_a_001.bam --ref /mnt/google/.google/input/canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --task 2 --regions gs://canis/CNR-data/exomes.bed. Using mount point: /input-gcsfused-2. ```. Assigning it to Pi-Chuan.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:2852,safety,input,input-gcsfused-,2852,"eepvariant/deepvariant/make_examples.py"", line 485, in _ensure_consistent_contigs. min_coverage_fraction). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 557, in validate_reference_contig_coverage. ref_bp, common_bp, coverage, format_contig_matches())). ValueError: Reference contigs span 3088269832 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sources were created on a common genome reference build. Contig matches were: ""chr1"" is 248956422 bp and IS MISSING, ""chr2"" is 242193529 bp and IS MISSING, ""chr3"" is 198295559 bp and IS MISSING, ""chr4"" is 190214555 bp and IS MISSING, ""chr5"" is 181538259 bp and IS MISSING, ""chr6"" is 170805979 bp and IS MISSING, ""chr7"" is 159345973 bp and IS MISSING, ""chr8"" is 145138636 bp and IS MISSING, ""chr9"" is 138394717 bp and IS MISSING, ""chr10"" is 133797422 bp and IS MISSING, ""chr11"" is 135086622 bp and IS MISSING, ""chr12"" is 133275309 bp and IS MISSING, ""chr13"" is 114364328 bp and IS MISSING, ""chr14"" is 107043718 bp and IS MISSING, ""chr15"" is 101991189 bp and IS MISSING, ""chr16"" is 90338345 bp and IS MISSING, ""chr17"" is 83257441 bp and IS MISSING, ""chr18"" is 80373285 bp and IS MISSING, ""chr19"" is 58617616 bp and IS MISSING, ""chr20"" is 64444167 bp and IS MISSING, ""chr21"" is 46709983 bp and IS MISSING, ""chr22"" is 50818468 bp and IS MISSING, ""chrX"" is 156040895 bp and IS MISSING, ""chrY"" is 57227415 bp and IS MISSING. parallel: This job failed:. mkdir -p ./input-gcsfused-2 && gcsfuse --implicit-dirs canis /input-gcsfused-2 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/canis/CNR-data/deep_variant_files/examples/0/examples_output.tfrecord@8.gz --reads /input-gcsfused-2/CNR-data/TLE_a_001.bam --ref /mnt/google/.google/input/canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --task 2 --regions gs://canis/CNR-data/exomes.bed. Using mount point: /input-gcsfused-2. ```. Assigning it to Pi-Chuan.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:3044,safety,input,input-gcsfused-,3044,"eepvariant/deepvariant/make_examples.py"", line 485, in _ensure_consistent_contigs. min_coverage_fraction). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 557, in validate_reference_contig_coverage. ref_bp, common_bp, coverage, format_contig_matches())). ValueError: Reference contigs span 3088269832 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sources were created on a common genome reference build. Contig matches were: ""chr1"" is 248956422 bp and IS MISSING, ""chr2"" is 242193529 bp and IS MISSING, ""chr3"" is 198295559 bp and IS MISSING, ""chr4"" is 190214555 bp and IS MISSING, ""chr5"" is 181538259 bp and IS MISSING, ""chr6"" is 170805979 bp and IS MISSING, ""chr7"" is 159345973 bp and IS MISSING, ""chr8"" is 145138636 bp and IS MISSING, ""chr9"" is 138394717 bp and IS MISSING, ""chr10"" is 133797422 bp and IS MISSING, ""chr11"" is 135086622 bp and IS MISSING, ""chr12"" is 133275309 bp and IS MISSING, ""chr13"" is 114364328 bp and IS MISSING, ""chr14"" is 107043718 bp and IS MISSING, ""chr15"" is 101991189 bp and IS MISSING, ""chr16"" is 90338345 bp and IS MISSING, ""chr17"" is 83257441 bp and IS MISSING, ""chr18"" is 80373285 bp and IS MISSING, ""chr19"" is 58617616 bp and IS MISSING, ""chr20"" is 64444167 bp and IS MISSING, ""chr21"" is 46709983 bp and IS MISSING, ""chr22"" is 50818468 bp and IS MISSING, ""chrX"" is 156040895 bp and IS MISSING, ""chrY"" is 57227415 bp and IS MISSING. parallel: This job failed:. mkdir -p ./input-gcsfused-2 && gcsfuse --implicit-dirs canis /input-gcsfused-2 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/canis/CNR-data/deep_variant_files/examples/0/examples_output.tfrecord@8.gz --reads /input-gcsfused-2/CNR-data/TLE_a_001.bam --ref /mnt/google/.google/input/canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --task 2 --regions gs://canis/CNR-data/exomes.bed. Using mount point: /input-gcsfused-2. ```. Assigning it to Pi-Chuan.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:3110,safety,input,input,3110,"eepvariant/deepvariant/make_examples.py"", line 485, in _ensure_consistent_contigs. min_coverage_fraction). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 557, in validate_reference_contig_coverage. ref_bp, common_bp, coverage, format_contig_matches())). ValueError: Reference contigs span 3088269832 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sources were created on a common genome reference build. Contig matches were: ""chr1"" is 248956422 bp and IS MISSING, ""chr2"" is 242193529 bp and IS MISSING, ""chr3"" is 198295559 bp and IS MISSING, ""chr4"" is 190214555 bp and IS MISSING, ""chr5"" is 181538259 bp and IS MISSING, ""chr6"" is 170805979 bp and IS MISSING, ""chr7"" is 159345973 bp and IS MISSING, ""chr8"" is 145138636 bp and IS MISSING, ""chr9"" is 138394717 bp and IS MISSING, ""chr10"" is 133797422 bp and IS MISSING, ""chr11"" is 135086622 bp and IS MISSING, ""chr12"" is 133275309 bp and IS MISSING, ""chr13"" is 114364328 bp and IS MISSING, ""chr14"" is 107043718 bp and IS MISSING, ""chr15"" is 101991189 bp and IS MISSING, ""chr16"" is 90338345 bp and IS MISSING, ""chr17"" is 83257441 bp and IS MISSING, ""chr18"" is 80373285 bp and IS MISSING, ""chr19"" is 58617616 bp and IS MISSING, ""chr20"" is 64444167 bp and IS MISSING, ""chr21"" is 46709983 bp and IS MISSING, ""chr22"" is 50818468 bp and IS MISSING, ""chrX"" is 156040895 bp and IS MISSING, ""chrY"" is 57227415 bp and IS MISSING. parallel: This job failed:. mkdir -p ./input-gcsfused-2 && gcsfuse --implicit-dirs canis /input-gcsfused-2 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/canis/CNR-data/deep_variant_files/examples/0/examples_output.tfrecord@8.gz --reads /input-gcsfused-2/CNR-data/TLE_a_001.bam --ref /mnt/google/.google/input/canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --task 2 --regions gs://canis/CNR-data/exomes.bed. Using mount point: /input-gcsfused-2. ```. Assigning it to Pi-Chuan.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:3232,safety,input,input-gcsfused-,3232,"eepvariant/deepvariant/make_examples.py"", line 485, in _ensure_consistent_contigs. min_coverage_fraction). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 557, in validate_reference_contig_coverage. ref_bp, common_bp, coverage, format_contig_matches())). ValueError: Reference contigs span 3088269832 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sources were created on a common genome reference build. Contig matches were: ""chr1"" is 248956422 bp and IS MISSING, ""chr2"" is 242193529 bp and IS MISSING, ""chr3"" is 198295559 bp and IS MISSING, ""chr4"" is 190214555 bp and IS MISSING, ""chr5"" is 181538259 bp and IS MISSING, ""chr6"" is 170805979 bp and IS MISSING, ""chr7"" is 159345973 bp and IS MISSING, ""chr8"" is 145138636 bp and IS MISSING, ""chr9"" is 138394717 bp and IS MISSING, ""chr10"" is 133797422 bp and IS MISSING, ""chr11"" is 135086622 bp and IS MISSING, ""chr12"" is 133275309 bp and IS MISSING, ""chr13"" is 114364328 bp and IS MISSING, ""chr14"" is 107043718 bp and IS MISSING, ""chr15"" is 101991189 bp and IS MISSING, ""chr16"" is 90338345 bp and IS MISSING, ""chr17"" is 83257441 bp and IS MISSING, ""chr18"" is 80373285 bp and IS MISSING, ""chr19"" is 58617616 bp and IS MISSING, ""chr20"" is 64444167 bp and IS MISSING, ""chr21"" is 46709983 bp and IS MISSING, ""chr22"" is 50818468 bp and IS MISSING, ""chrX"" is 156040895 bp and IS MISSING, ""chrY"" is 57227415 bp and IS MISSING. parallel: This job failed:. mkdir -p ./input-gcsfused-2 && gcsfuse --implicit-dirs canis /input-gcsfused-2 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/canis/CNR-data/deep_variant_files/examples/0/examples_output.tfrecord@8.gz --reads /input-gcsfused-2/CNR-data/TLE_a_001.bam --ref /mnt/google/.google/input/canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --task 2 --regions gs://canis/CNR-data/exomes.bed. Using mount point: /input-gcsfused-2. ```. Assigning it to Pi-Chuan.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:142,security,log,log,142,"It has nothing to do with docker image. The problem is with input file (with err `Unrecognized SAM header type`). Pasting make_example worker log:. ```. Unrecognized SAM header type, ignoring:. I1108 21:36:46.455516 140363989006080 genomics_reader.py:213] Reading /input-gcsfused-2/CNR-data/TLE_a_001.bam with NativeSamReader. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1178, in main. make_examples_runner(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1076, in make_examples_runner. regions = processing_regions_from_options(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 990, in processing_regions_from_options. options.min_shared_contigs_basepairs). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 485, in _ensure_consistent_contigs. min_coverage_fraction). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 557, in validate_reference_contig_coverage. ref_bp, common_bp, coverage, format_contig_matches())). ValueError: Reference contigs span 3088269832 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sources were created on a common genome reference build. Contig matches were: ""chr1"" is 248956422 bp and IS MISSING, ""chr2"" is 242193529 bp and IS MISSING, ""chr3"" is 198295559 bp and IS MISSING, ""chr4"" is 190214555 bp and IS MISSING, ""chr5"" is 181538259 bp ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:142,testability,log,log,142,"It has nothing to do with docker image. The problem is with input file (with err `Unrecognized SAM header type`). Pasting make_example worker log:. ```. Unrecognized SAM header type, ignoring:. I1108 21:36:46.455516 140363989006080 genomics_reader.py:213] Reading /input-gcsfused-2/CNR-data/TLE_a_001.bam with NativeSamReader. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1178, in main. make_examples_runner(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1076, in make_examples_runner. regions = processing_regions_from_options(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 990, in processing_regions_from_options. options.min_shared_contigs_basepairs). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 485, in _ensure_consistent_contigs. min_coverage_fraction). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 557, in validate_reference_contig_coverage. ref_bp, common_bp, coverage, format_contig_matches())). ValueError: Reference contigs span 3088269832 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sources were created on a common genome reference build. Contig matches were: ""chr1"" is 248956422 bp and IS MISSING, ""chr2"" is 242193529 bp and IS MISSING, ""chr3"" is 198295559 bp and IS MISSING, ""chr4"" is 190214555 bp and IS MISSING, ""chr5"" is 181538259 bp ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:327,testability,Trace,Traceback,327,"It has nothing to do with docker image. The problem is with input file (with err `Unrecognized SAM header type`). Pasting make_example worker log:. ```. Unrecognized SAM header type, ignoring:. I1108 21:36:46.455516 140363989006080 genomics_reader.py:213] Reading /input-gcsfused-2/CNR-data/TLE_a_001.bam with NativeSamReader. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1178, in main. make_examples_runner(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1076, in make_examples_runner. regions = processing_regions_from_options(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 990, in processing_regions_from_options. options.min_shared_contigs_basepairs). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 485, in _ensure_consistent_contigs. min_coverage_fraction). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 557, in validate_reference_contig_coverage. ref_bp, common_bp, coverage, format_contig_matches())). ValueError: Reference contigs span 3088269832 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sources were created on a common genome reference build. Contig matches were: ""chr1"" is 248956422 bp and IS MISSING, ""chr2"" is 242193529 bp and IS MISSING, ""chr3"" is 198295559 bp and IS MISSING, ""chr4"" is 190214555 bp and IS MISSING, ""chr5"" is 181538259 bp ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:1570,testability,coverag,coverage,1570,"thon/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1178, in main. make_examples_runner(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1076, in make_examples_runner. regions = processing_regions_from_options(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 990, in processing_regions_from_options. options.min_shared_contigs_basepairs). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 485, in _ensure_consistent_contigs. min_coverage_fraction). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 557, in validate_reference_contig_coverage. ref_bp, common_bp, coverage, format_contig_matches())). ValueError: Reference contigs span 3088269832 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sources were created on a common genome reference build. Contig matches were: ""chr1"" is 248956422 bp and IS MISSING, ""chr2"" is 242193529 bp and IS MISSING, ""chr3"" is 198295559 bp and IS MISSING, ""chr4"" is 190214555 bp and IS MISSING, ""chr5"" is 181538259 bp and IS MISSING, ""chr6"" is 170805979 bp and IS MISSING, ""chr7"" is 159345973 bp and IS MISSING, ""chr8"" is 145138636 bp and IS MISSING, ""chr9"" is 138394717 bp and IS MISSING, ""chr10"" is 133797422 bp and IS MISSING, ""chr11"" is 135086622 bp and IS MISSING, ""chr12"" is 133275309 bp and IS MISSING, ""chr13"" is 114364328 bp and IS MISSING, ""chr14"" is 107043718 bp and IS MISSING, ""chr15"" is 101991189 bp and IS MISSING, ""chr16"" is 90338345 bp and IS MISSING, ""chr17"" is 83257441 bp and IS MISSING, ""chr18"" is 80373285 bp and IS MISSING, ""chr19"" is 58617616 bp and IS MISSING, ""chr20",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:60,usability,input,input,60,"It has nothing to do with docker image. The problem is with input file (with err `Unrecognized SAM header type`). Pasting make_example worker log:. ```. Unrecognized SAM header type, ignoring:. I1108 21:36:46.455516 140363989006080 genomics_reader.py:213] Reading /input-gcsfused-2/CNR-data/TLE_a_001.bam with NativeSamReader. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1178, in main. make_examples_runner(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1076, in make_examples_runner. regions = processing_regions_from_options(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 990, in processing_regions_from_options. options.min_shared_contigs_basepairs). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 485, in _ensure_consistent_contigs. min_coverage_fraction). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 557, in validate_reference_contig_coverage. ref_bp, common_bp, coverage, format_contig_matches())). ValueError: Reference contigs span 3088269832 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sources were created on a common genome reference build. Contig matches were: ""chr1"" is 248956422 bp and IS MISSING, ""chr2"" is 242193529 bp and IS MISSING, ""chr3"" is 198295559 bp and IS MISSING, ""chr4"" is 190214555 bp and IS MISSING, ""chr5"" is 181538259 bp ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:265,usability,input,input-gcsfused-,265,"It has nothing to do with docker image. The problem is with input file (with err `Unrecognized SAM header type`). Pasting make_example worker log:. ```. Unrecognized SAM header type, ignoring:. I1108 21:36:46.455516 140363989006080 genomics_reader.py:213] Reading /input-gcsfused-2/CNR-data/TLE_a_001.bam with NativeSamReader. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1178, in main. make_examples_runner(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1076, in make_examples_runner. regions = processing_regions_from_options(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 990, in processing_regions_from_options. options.min_shared_contigs_basepairs). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 485, in _ensure_consistent_contigs. min_coverage_fraction). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 557, in validate_reference_contig_coverage. ref_bp, common_bp, coverage, format_contig_matches())). ValueError: Reference contigs span 3088269832 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sources were created on a common genome reference build. Contig matches were: ""chr1"" is 248956422 bp and IS MISSING, ""chr2"" is 242193529 bp and IS MISSING, ""chr3"" is 198295559 bp and IS MISSING, ""chr4"" is 190214555 bp and IS MISSING, ""chr5"" is 181538259 bp ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:1715,usability,input,input,1715,"ant/deepvariant/make_examples.py"", line 1178, in main. make_examples_runner(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1076, in make_examples_runner. regions = processing_regions_from_options(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 990, in processing_regions_from_options. options.min_shared_contigs_basepairs). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 485, in _ensure_consistent_contigs. min_coverage_fraction). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 557, in validate_reference_contig_coverage. ref_bp, common_bp, coverage, format_contig_matches())). ValueError: Reference contigs span 3088269832 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sources were created on a common genome reference build. Contig matches were: ""chr1"" is 248956422 bp and IS MISSING, ""chr2"" is 242193529 bp and IS MISSING, ""chr3"" is 198295559 bp and IS MISSING, ""chr4"" is 190214555 bp and IS MISSING, ""chr5"" is 181538259 bp and IS MISSING, ""chr6"" is 170805979 bp and IS MISSING, ""chr7"" is 159345973 bp and IS MISSING, ""chr8"" is 145138636 bp and IS MISSING, ""chr9"" is 138394717 bp and IS MISSING, ""chr10"" is 133797422 bp and IS MISSING, ""chr11"" is 135086622 bp and IS MISSING, ""chr12"" is 133275309 bp and IS MISSING, ""chr13"" is 114364328 bp and IS MISSING, ""chr14"" is 107043718 bp and IS MISSING, ""chr15"" is 101991189 bp and IS MISSING, ""chr16"" is 90338345 bp and IS MISSING, ""chr17"" is 83257441 bp and IS MISSING, ""chr18"" is 80373285 bp and IS MISSING, ""chr19"" is 58617616 bp and IS MISSING, ""chr20"" is 64444167 bp and IS MISSING, ""chr21"" is 46709983 bp and IS MISSING, ""chr22"" is 50818468 bp and IS MISSING, ""chrX"" is 156040895 bp and IS MIS",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:2801,usability,input,input-gcsfused-,2801,"eepvariant/deepvariant/make_examples.py"", line 485, in _ensure_consistent_contigs. min_coverage_fraction). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 557, in validate_reference_contig_coverage. ref_bp, common_bp, coverage, format_contig_matches())). ValueError: Reference contigs span 3088269832 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sources were created on a common genome reference build. Contig matches were: ""chr1"" is 248956422 bp and IS MISSING, ""chr2"" is 242193529 bp and IS MISSING, ""chr3"" is 198295559 bp and IS MISSING, ""chr4"" is 190214555 bp and IS MISSING, ""chr5"" is 181538259 bp and IS MISSING, ""chr6"" is 170805979 bp and IS MISSING, ""chr7"" is 159345973 bp and IS MISSING, ""chr8"" is 145138636 bp and IS MISSING, ""chr9"" is 138394717 bp and IS MISSING, ""chr10"" is 133797422 bp and IS MISSING, ""chr11"" is 135086622 bp and IS MISSING, ""chr12"" is 133275309 bp and IS MISSING, ""chr13"" is 114364328 bp and IS MISSING, ""chr14"" is 107043718 bp and IS MISSING, ""chr15"" is 101991189 bp and IS MISSING, ""chr16"" is 90338345 bp and IS MISSING, ""chr17"" is 83257441 bp and IS MISSING, ""chr18"" is 80373285 bp and IS MISSING, ""chr19"" is 58617616 bp and IS MISSING, ""chr20"" is 64444167 bp and IS MISSING, ""chr21"" is 46709983 bp and IS MISSING, ""chr22"" is 50818468 bp and IS MISSING, ""chrX"" is 156040895 bp and IS MISSING, ""chrY"" is 57227415 bp and IS MISSING. parallel: This job failed:. mkdir -p ./input-gcsfused-2 && gcsfuse --implicit-dirs canis /input-gcsfused-2 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/canis/CNR-data/deep_variant_files/examples/0/examples_output.tfrecord@8.gz --reads /input-gcsfused-2/CNR-data/TLE_a_001.bam --ref /mnt/google/.google/input/canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --task 2 --regions gs://canis/CNR-data/exomes.bed. Using mount point: /input-gcsfused-2. ```. Assigning it to Pi-Chuan.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:2852,usability,input,input-gcsfused-,2852,"eepvariant/deepvariant/make_examples.py"", line 485, in _ensure_consistent_contigs. min_coverage_fraction). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 557, in validate_reference_contig_coverage. ref_bp, common_bp, coverage, format_contig_matches())). ValueError: Reference contigs span 3088269832 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sources were created on a common genome reference build. Contig matches were: ""chr1"" is 248956422 bp and IS MISSING, ""chr2"" is 242193529 bp and IS MISSING, ""chr3"" is 198295559 bp and IS MISSING, ""chr4"" is 190214555 bp and IS MISSING, ""chr5"" is 181538259 bp and IS MISSING, ""chr6"" is 170805979 bp and IS MISSING, ""chr7"" is 159345973 bp and IS MISSING, ""chr8"" is 145138636 bp and IS MISSING, ""chr9"" is 138394717 bp and IS MISSING, ""chr10"" is 133797422 bp and IS MISSING, ""chr11"" is 135086622 bp and IS MISSING, ""chr12"" is 133275309 bp and IS MISSING, ""chr13"" is 114364328 bp and IS MISSING, ""chr14"" is 107043718 bp and IS MISSING, ""chr15"" is 101991189 bp and IS MISSING, ""chr16"" is 90338345 bp and IS MISSING, ""chr17"" is 83257441 bp and IS MISSING, ""chr18"" is 80373285 bp and IS MISSING, ""chr19"" is 58617616 bp and IS MISSING, ""chr20"" is 64444167 bp and IS MISSING, ""chr21"" is 46709983 bp and IS MISSING, ""chr22"" is 50818468 bp and IS MISSING, ""chrX"" is 156040895 bp and IS MISSING, ""chrY"" is 57227415 bp and IS MISSING. parallel: This job failed:. mkdir -p ./input-gcsfused-2 && gcsfuse --implicit-dirs canis /input-gcsfused-2 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/canis/CNR-data/deep_variant_files/examples/0/examples_output.tfrecord@8.gz --reads /input-gcsfused-2/CNR-data/TLE_a_001.bam --ref /mnt/google/.google/input/canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --task 2 --regions gs://canis/CNR-data/exomes.bed. Using mount point: /input-gcsfused-2. ```. Assigning it to Pi-Chuan.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:3044,usability,input,input-gcsfused-,3044,"eepvariant/deepvariant/make_examples.py"", line 485, in _ensure_consistent_contigs. min_coverage_fraction). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 557, in validate_reference_contig_coverage. ref_bp, common_bp, coverage, format_contig_matches())). ValueError: Reference contigs span 3088269832 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sources were created on a common genome reference build. Contig matches were: ""chr1"" is 248956422 bp and IS MISSING, ""chr2"" is 242193529 bp and IS MISSING, ""chr3"" is 198295559 bp and IS MISSING, ""chr4"" is 190214555 bp and IS MISSING, ""chr5"" is 181538259 bp and IS MISSING, ""chr6"" is 170805979 bp and IS MISSING, ""chr7"" is 159345973 bp and IS MISSING, ""chr8"" is 145138636 bp and IS MISSING, ""chr9"" is 138394717 bp and IS MISSING, ""chr10"" is 133797422 bp and IS MISSING, ""chr11"" is 135086622 bp and IS MISSING, ""chr12"" is 133275309 bp and IS MISSING, ""chr13"" is 114364328 bp and IS MISSING, ""chr14"" is 107043718 bp and IS MISSING, ""chr15"" is 101991189 bp and IS MISSING, ""chr16"" is 90338345 bp and IS MISSING, ""chr17"" is 83257441 bp and IS MISSING, ""chr18"" is 80373285 bp and IS MISSING, ""chr19"" is 58617616 bp and IS MISSING, ""chr20"" is 64444167 bp and IS MISSING, ""chr21"" is 46709983 bp and IS MISSING, ""chr22"" is 50818468 bp and IS MISSING, ""chrX"" is 156040895 bp and IS MISSING, ""chrY"" is 57227415 bp and IS MISSING. parallel: This job failed:. mkdir -p ./input-gcsfused-2 && gcsfuse --implicit-dirs canis /input-gcsfused-2 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/canis/CNR-data/deep_variant_files/examples/0/examples_output.tfrecord@8.gz --reads /input-gcsfused-2/CNR-data/TLE_a_001.bam --ref /mnt/google/.google/input/canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --task 2 --regions gs://canis/CNR-data/exomes.bed. Using mount point: /input-gcsfused-2. ```. Assigning it to Pi-Chuan.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:3110,usability,input,input,3110,"eepvariant/deepvariant/make_examples.py"", line 485, in _ensure_consistent_contigs. min_coverage_fraction). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 557, in validate_reference_contig_coverage. ref_bp, common_bp, coverage, format_contig_matches())). ValueError: Reference contigs span 3088269832 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sources were created on a common genome reference build. Contig matches were: ""chr1"" is 248956422 bp and IS MISSING, ""chr2"" is 242193529 bp and IS MISSING, ""chr3"" is 198295559 bp and IS MISSING, ""chr4"" is 190214555 bp and IS MISSING, ""chr5"" is 181538259 bp and IS MISSING, ""chr6"" is 170805979 bp and IS MISSING, ""chr7"" is 159345973 bp and IS MISSING, ""chr8"" is 145138636 bp and IS MISSING, ""chr9"" is 138394717 bp and IS MISSING, ""chr10"" is 133797422 bp and IS MISSING, ""chr11"" is 135086622 bp and IS MISSING, ""chr12"" is 133275309 bp and IS MISSING, ""chr13"" is 114364328 bp and IS MISSING, ""chr14"" is 107043718 bp and IS MISSING, ""chr15"" is 101991189 bp and IS MISSING, ""chr16"" is 90338345 bp and IS MISSING, ""chr17"" is 83257441 bp and IS MISSING, ""chr18"" is 80373285 bp and IS MISSING, ""chr19"" is 58617616 bp and IS MISSING, ""chr20"" is 64444167 bp and IS MISSING, ""chr21"" is 46709983 bp and IS MISSING, ""chr22"" is 50818468 bp and IS MISSING, ""chrX"" is 156040895 bp and IS MISSING, ""chrY"" is 57227415 bp and IS MISSING. parallel: This job failed:. mkdir -p ./input-gcsfused-2 && gcsfuse --implicit-dirs canis /input-gcsfused-2 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/canis/CNR-data/deep_variant_files/examples/0/examples_output.tfrecord@8.gz --reads /input-gcsfused-2/CNR-data/TLE_a_001.bam --ref /mnt/google/.google/input/canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --task 2 --regions gs://canis/CNR-data/exomes.bed. Using mount point: /input-gcsfused-2. ```. Assigning it to Pi-Chuan.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:3232,usability,input,input-gcsfused-,3232,"eepvariant/deepvariant/make_examples.py"", line 485, in _ensure_consistent_contigs. min_coverage_fraction). File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 557, in validate_reference_contig_coverage. ref_bp, common_bp, coverage, format_contig_matches())). ValueError: Reference contigs span 3088269832 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sources were created on a common genome reference build. Contig matches were: ""chr1"" is 248956422 bp and IS MISSING, ""chr2"" is 242193529 bp and IS MISSING, ""chr3"" is 198295559 bp and IS MISSING, ""chr4"" is 190214555 bp and IS MISSING, ""chr5"" is 181538259 bp and IS MISSING, ""chr6"" is 170805979 bp and IS MISSING, ""chr7"" is 159345973 bp and IS MISSING, ""chr8"" is 145138636 bp and IS MISSING, ""chr9"" is 138394717 bp and IS MISSING, ""chr10"" is 133797422 bp and IS MISSING, ""chr11"" is 135086622 bp and IS MISSING, ""chr12"" is 133275309 bp and IS MISSING, ""chr13"" is 114364328 bp and IS MISSING, ""chr14"" is 107043718 bp and IS MISSING, ""chr15"" is 101991189 bp and IS MISSING, ""chr16"" is 90338345 bp and IS MISSING, ""chr17"" is 83257441 bp and IS MISSING, ""chr18"" is 80373285 bp and IS MISSING, ""chr19"" is 58617616 bp and IS MISSING, ""chr20"" is 64444167 bp and IS MISSING, ""chr21"" is 46709983 bp and IS MISSING, ""chr22"" is 50818468 bp and IS MISSING, ""chrX"" is 156040895 bp and IS MISSING, ""chrY"" is 57227415 bp and IS MISSING. parallel: This job failed:. mkdir -p ./input-gcsfused-2 && gcsfuse --implicit-dirs canis /input-gcsfused-2 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/canis/CNR-data/deep_variant_files/examples/0/examples_output.tfrecord@8.gz --reads /input-gcsfused-2/CNR-data/TLE_a_001.bam --ref /mnt/google/.google/input/canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --task 2 --regions gs://canis/CNR-data/exomes.bed. Using mount point: /input-gcsfused-2. ```. Assigning it to Pi-Chuan.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:188,availability,error,error,188,"@saliksyed The real issue is that your BAM file does not contain those chromosomes/contigs as based on the reference file. That would probably will fix it. Sorry for the confusion, as the error was giving me a different signal. @nmousavi Not exactly, but I understand what's the trigger, and it probably would be helpful to provide more direct causes directly in the description of the events. The real thing that's happening here is that he doesn't have the right chromosome labels in his BAM files as compared to the reference genome, which misses all of them and triggers the error on the following lines of code:. https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py#L556-L561. The `Unrecognized SAM header type, ignoring:` is just the following warning-line of code that doesn't trigger the exit status of 1:. https://github.com/google/deepvariant/blob/aba55537eec832e6cea678349422124ef50680f4/third_party/nucleus/io/sam_reader.cc#L525. Thanks,. ~p.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:579,availability,error,error,579,"@saliksyed The real issue is that your BAM file does not contain those chromosomes/contigs as based on the reference file. That would probably will fix it. Sorry for the confusion, as the error was giving me a different signal. @nmousavi Not exactly, but I understand what's the trigger, and it probably would be helpful to provide more direct causes directly in the description of the events. The real thing that's happening here is that he doesn't have the right chromosome labels in his BAM files as compared to the reference genome, which misses all of them and triggers the error on the following lines of code:. https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py#L556-L561. The `Unrecognized SAM header type, ignoring:` is just the following warning-line of code that doesn't trigger the exit status of 1:. https://github.com/google/deepvariant/blob/aba55537eec832e6cea678349422124ef50680f4/third_party/nucleus/io/sam_reader.cc#L525. Thanks,. ~p.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:57,deployability,contain,contain,57,"@saliksyed The real issue is that your BAM file does not contain those chromosomes/contigs as based on the reference file. That would probably will fix it. Sorry for the confusion, as the error was giving me a different signal. @nmousavi Not exactly, but I understand what's the trigger, and it probably would be helpful to provide more direct causes directly in the description of the events. The real thing that's happening here is that he doesn't have the right chromosome labels in his BAM files as compared to the reference genome, which misses all of them and triggers the error on the following lines of code:. https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py#L556-L561. The `Unrecognized SAM header type, ignoring:` is just the following warning-line of code that doesn't trigger the exit status of 1:. https://github.com/google/deepvariant/blob/aba55537eec832e6cea678349422124ef50680f4/third_party/nucleus/io/sam_reader.cc#L525. Thanks,. ~p.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:386,integrability,event,events,386,"@saliksyed The real issue is that your BAM file does not contain those chromosomes/contigs as based on the reference file. That would probably will fix it. Sorry for the confusion, as the error was giving me a different signal. @nmousavi Not exactly, but I understand what's the trigger, and it probably would be helpful to provide more direct causes directly in the description of the events. The real thing that's happening here is that he doesn't have the right chromosome labels in his BAM files as compared to the reference genome, which misses all of them and triggers the error on the following lines of code:. https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py#L556-L561. The `Unrecognized SAM header type, ignoring:` is just the following warning-line of code that doesn't trigger the exit status of 1:. https://github.com/google/deepvariant/blob/aba55537eec832e6cea678349422124ef50680f4/third_party/nucleus/io/sam_reader.cc#L525. Thanks,. ~p.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:188,performance,error,error,188,"@saliksyed The real issue is that your BAM file does not contain those chromosomes/contigs as based on the reference file. That would probably will fix it. Sorry for the confusion, as the error was giving me a different signal. @nmousavi Not exactly, but I understand what's the trigger, and it probably would be helpful to provide more direct causes directly in the description of the events. The real thing that's happening here is that he doesn't have the right chromosome labels in his BAM files as compared to the reference genome, which misses all of them and triggers the error on the following lines of code:. https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py#L556-L561. The `Unrecognized SAM header type, ignoring:` is just the following warning-line of code that doesn't trigger the exit status of 1:. https://github.com/google/deepvariant/blob/aba55537eec832e6cea678349422124ef50680f4/third_party/nucleus/io/sam_reader.cc#L525. Thanks,. ~p.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:579,performance,error,error,579,"@saliksyed The real issue is that your BAM file does not contain those chromosomes/contigs as based on the reference file. That would probably will fix it. Sorry for the confusion, as the error was giving me a different signal. @nmousavi Not exactly, but I understand what's the trigger, and it probably would be helpful to provide more direct causes directly in the description of the events. The real thing that's happening here is that he doesn't have the right chromosome labels in his BAM files as compared to the reference genome, which misses all of them and triggers the error on the following lines of code:. https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py#L556-L561. The `Unrecognized SAM header type, ignoring:` is just the following warning-line of code that doesn't trigger the exit status of 1:. https://github.com/google/deepvariant/blob/aba55537eec832e6cea678349422124ef50680f4/third_party/nucleus/io/sam_reader.cc#L525. Thanks,. ~p.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:48,reliability,doe,does,48,"@saliksyed The real issue is that your BAM file does not contain those chromosomes/contigs as based on the reference file. That would probably will fix it. Sorry for the confusion, as the error was giving me a different signal. @nmousavi Not exactly, but I understand what's the trigger, and it probably would be helpful to provide more direct causes directly in the description of the events. The real thing that's happening here is that he doesn't have the right chromosome labels in his BAM files as compared to the reference genome, which misses all of them and triggers the error on the following lines of code:. https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py#L556-L561. The `Unrecognized SAM header type, ignoring:` is just the following warning-line of code that doesn't trigger the exit status of 1:. https://github.com/google/deepvariant/blob/aba55537eec832e6cea678349422124ef50680f4/third_party/nucleus/io/sam_reader.cc#L525. Thanks,. ~p.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:442,reliability,doe,doesn,442,"@saliksyed The real issue is that your BAM file does not contain those chromosomes/contigs as based on the reference file. That would probably will fix it. Sorry for the confusion, as the error was giving me a different signal. @nmousavi Not exactly, but I understand what's the trigger, and it probably would be helpful to provide more direct causes directly in the description of the events. The real thing that's happening here is that he doesn't have the right chromosome labels in his BAM files as compared to the reference genome, which misses all of them and triggers the error on the following lines of code:. https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py#L556-L561. The `Unrecognized SAM header type, ignoring:` is just the following warning-line of code that doesn't trigger the exit status of 1:. https://github.com/google/deepvariant/blob/aba55537eec832e6cea678349422124ef50680f4/third_party/nucleus/io/sam_reader.cc#L525. Thanks,. ~p.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:800,reliability,doe,doesn,800,"@saliksyed The real issue is that your BAM file does not contain those chromosomes/contigs as based on the reference file. That would probably will fix it. Sorry for the confusion, as the error was giving me a different signal. @nmousavi Not exactly, but I understand what's the trigger, and it probably would be helpful to provide more direct causes directly in the description of the events. The real thing that's happening here is that he doesn't have the right chromosome labels in his BAM files as compared to the reference genome, which misses all of them and triggers the error on the following lines of code:. https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py#L556-L561. The `Unrecognized SAM header type, ignoring:` is just the following warning-line of code that doesn't trigger the exit status of 1:. https://github.com/google/deepvariant/blob/aba55537eec832e6cea678349422124ef50680f4/third_party/nucleus/io/sam_reader.cc#L525. Thanks,. ~p.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:188,safety,error,error,188,"@saliksyed The real issue is that your BAM file does not contain those chromosomes/contigs as based on the reference file. That would probably will fix it. Sorry for the confusion, as the error was giving me a different signal. @nmousavi Not exactly, but I understand what's the trigger, and it probably would be helpful to provide more direct causes directly in the description of the events. The real thing that's happening here is that he doesn't have the right chromosome labels in his BAM files as compared to the reference genome, which misses all of them and triggers the error on the following lines of code:. https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py#L556-L561. The `Unrecognized SAM header type, ignoring:` is just the following warning-line of code that doesn't trigger the exit status of 1:. https://github.com/google/deepvariant/blob/aba55537eec832e6cea678349422124ef50680f4/third_party/nucleus/io/sam_reader.cc#L525. Thanks,. ~p.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:579,safety,error,error,579,"@saliksyed The real issue is that your BAM file does not contain those chromosomes/contigs as based on the reference file. That would probably will fix it. Sorry for the confusion, as the error was giving me a different signal. @nmousavi Not exactly, but I understand what's the trigger, and it probably would be helpful to provide more direct causes directly in the description of the events. The real thing that's happening here is that he doesn't have the right chromosome labels in his BAM files as compared to the reference genome, which misses all of them and triggers the error on the following lines of code:. https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py#L556-L561. The `Unrecognized SAM header type, ignoring:` is just the following warning-line of code that doesn't trigger the exit status of 1:. https://github.com/google/deepvariant/blob/aba55537eec832e6cea678349422124ef50680f4/third_party/nucleus/io/sam_reader.cc#L525. Thanks,. ~p.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:220,security,sign,signal,220,"@saliksyed The real issue is that your BAM file does not contain those chromosomes/contigs as based on the reference file. That would probably will fix it. Sorry for the confusion, as the error was giving me a different signal. @nmousavi Not exactly, but I understand what's the trigger, and it probably would be helpful to provide more direct causes directly in the description of the events. The real thing that's happening here is that he doesn't have the right chromosome labels in his BAM files as compared to the reference genome, which misses all of them and triggers the error on the following lines of code:. https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py#L556-L561. The `Unrecognized SAM header type, ignoring:` is just the following warning-line of code that doesn't trigger the exit status of 1:. https://github.com/google/deepvariant/blob/aba55537eec832e6cea678349422124ef50680f4/third_party/nucleus/io/sam_reader.cc#L525. Thanks,. ~p.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:257,testability,understand,understand,257,"@saliksyed The real issue is that your BAM file does not contain those chromosomes/contigs as based on the reference file. That would probably will fix it. Sorry for the confusion, as the error was giving me a different signal. @nmousavi Not exactly, but I understand what's the trigger, and it probably would be helpful to provide more direct causes directly in the description of the events. The real thing that's happening here is that he doesn't have the right chromosome labels in his BAM files as compared to the reference genome, which misses all of them and triggers the error on the following lines of code:. https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py#L556-L561. The `Unrecognized SAM header type, ignoring:` is just the following warning-line of code that doesn't trigger the exit status of 1:. https://github.com/google/deepvariant/blob/aba55537eec832e6cea678349422124ef50680f4/third_party/nucleus/io/sam_reader.cc#L525. Thanks,. ~p.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:188,usability,error,error,188,"@saliksyed The real issue is that your BAM file does not contain those chromosomes/contigs as based on the reference file. That would probably will fix it. Sorry for the confusion, as the error was giving me a different signal. @nmousavi Not exactly, but I understand what's the trigger, and it probably would be helpful to provide more direct causes directly in the description of the events. The real thing that's happening here is that he doesn't have the right chromosome labels in his BAM files as compared to the reference genome, which misses all of them and triggers the error on the following lines of code:. https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py#L556-L561. The `Unrecognized SAM header type, ignoring:` is just the following warning-line of code that doesn't trigger the exit status of 1:. https://github.com/google/deepvariant/blob/aba55537eec832e6cea678349422124ef50680f4/third_party/nucleus/io/sam_reader.cc#L525. Thanks,. ~p.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:313,usability,help,helpful,313,"@saliksyed The real issue is that your BAM file does not contain those chromosomes/contigs as based on the reference file. That would probably will fix it. Sorry for the confusion, as the error was giving me a different signal. @nmousavi Not exactly, but I understand what's the trigger, and it probably would be helpful to provide more direct causes directly in the description of the events. The real thing that's happening here is that he doesn't have the right chromosome labels in his BAM files as compared to the reference genome, which misses all of them and triggers the error on the following lines of code:. https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py#L556-L561. The `Unrecognized SAM header type, ignoring:` is just the following warning-line of code that doesn't trigger the exit status of 1:. https://github.com/google/deepvariant/blob/aba55537eec832e6cea678349422124ef50680f4/third_party/nucleus/io/sam_reader.cc#L525. Thanks,. ~p.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:579,usability,error,error,579,"@saliksyed The real issue is that your BAM file does not contain those chromosomes/contigs as based on the reference file. That would probably will fix it. Sorry for the confusion, as the error was giving me a different signal. @nmousavi Not exactly, but I understand what's the trigger, and it probably would be helpful to provide more direct causes directly in the description of the events. The real thing that's happening here is that he doesn't have the right chromosome labels in his BAM files as compared to the reference genome, which misses all of them and triggers the error on the following lines of code:. https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py#L556-L561. The `Unrecognized SAM header type, ignoring:` is just the following warning-line of code that doesn't trigger the exit status of 1:. https://github.com/google/deepvariant/blob/aba55537eec832e6cea678349422124ef50680f4/third_party/nucleus/io/sam_reader.cc#L525. Thanks,. ~p.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:825,usability,statu,status,825,"@saliksyed The real issue is that your BAM file does not contain those chromosomes/contigs as based on the reference file. That would probably will fix it. Sorry for the confusion, as the error was giving me a different signal. @nmousavi Not exactly, but I understand what's the trigger, and it probably would be helpful to provide more direct causes directly in the description of the events. The real thing that's happening here is that he doesn't have the right chromosome labels in his BAM files as compared to the reference genome, which misses all of them and triggers the error on the following lines of code:. https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py#L556-L561. The `Unrecognized SAM header type, ignoring:` is just the following warning-line of code that doesn't trigger the exit status of 1:. https://github.com/google/deepvariant/blob/aba55537eec832e6cea678349422124ef50680f4/third_party/nucleus/io/sam_reader.cc#L525. Thanks,. ~p.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:47,usability,indicat,indicates,47,"Okay so I get the following information, which indicates the reference genome used for alignment is GRCh37 (not 38 as was assumed). So looks like I need to get a different reference genome. . ```. @SQ SN:chr1 LN:249250621. @SQ SN:chr2 LN:243199373. @SQ SN:chr3 LN:198022430. @SQ SN:chr4 LN:191154276. @SQ SN:chr5 LN:180915260. @SQ SN:chr6 LN:171115067. @SQ SN:chr7 LN:159138663. @SQ SN:chr8 LN:146364022. @SQ SN:chr9 LN:141213431. @SQ SN:chr10 LN:135534747. @SQ SN:chr11 LN:135006516. @SQ SN:chr12 LN:133851895. @SQ SN:chr13 LN:115169878. @SQ SN:chr14 LN:107349540. @SQ SN:chr15 LN:102531392. @SQ SN:chr16 LN:90354753. @SQ SN:chr17 LN:81195210. @SQ SN:chr18 LN:78077248. @SQ SN:chr19 LN:59128983. @SQ SN:chr20 LN:63025520. @SQ SN:chr21 LN:48129895. @SQ SN:chr22 LN:51304566. @SQ SN:chrX LN:155270560. @SQ SN:chrY LN:59373566. @SQ SN:chrM LN:16569. ```. Thanks for the help @nmousavi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:868,usability,help,help,868,"Okay so I get the following information, which indicates the reference genome used for alignment is GRCh37 (not 38 as was assumed). So looks like I need to get a different reference genome. . ```. @SQ SN:chr1 LN:249250621. @SQ SN:chr2 LN:243199373. @SQ SN:chr3 LN:198022430. @SQ SN:chr4 LN:191154276. @SQ SN:chr5 LN:180915260. @SQ SN:chr6 LN:171115067. @SQ SN:chr7 LN:159138663. @SQ SN:chr8 LN:146364022. @SQ SN:chr9 LN:141213431. @SQ SN:chr10 LN:135534747. @SQ SN:chr11 LN:135006516. @SQ SN:chr12 LN:133851895. @SQ SN:chr13 LN:115169878. @SQ SN:chr14 LN:107349540. @SQ SN:chr15 LN:102531392. @SQ SN:chr16 LN:90354753. @SQ SN:chr17 LN:81195210. @SQ SN:chr18 LN:78077248. @SQ SN:chr19 LN:59128983. @SQ SN:chr20 LN:63025520. @SQ SN:chr21 LN:48129895. @SQ SN:chr22 LN:51304566. @SQ SN:chrX LN:155270560. @SQ SN:chrY LN:59373566. @SQ SN:chrM LN:16569. ```. Thanks for the help @nmousavi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:176,deployability,pipelin,pipeline,176,Thank you also to @pgrosu for taking a look. I think the next steps for me are:. 1 -- disassemble the BAM file and re-align using BWA-MEM to the Verily Genome. 2 -- Re run the pipeline with the GRCh38 aligned file.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:176,integrability,pipelin,pipeline,176,Thank you also to @pgrosu for taking a look. I think the next steps for me are:. 1 -- disassemble the BAM file and re-align using BWA-MEM to the Verily Genome. 2 -- Re run the pipeline with the GRCh38 aligned file.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:21,deployability,pipelin,pipeline,21,Why not just run the pipeline using the b37 version of the genome? The gcp runner pipeline should support any reference genome,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:44,deployability,version,version,44,Why not just run the pipeline using the b37 version of the genome? The gcp runner pipeline should support any reference genome,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:82,deployability,pipelin,pipeline,82,Why not just run the pipeline using the b37 version of the genome? The gcp runner pipeline should support any reference genome,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:21,integrability,pipelin,pipeline,21,Why not just run the pipeline using the b37 version of the genome? The gcp runner pipeline should support any reference genome,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:44,integrability,version,version,44,Why not just run the pipeline using the b37 version of the genome? The gcp runner pipeline should support any reference genome,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:82,integrability,pipelin,pipeline,82,Why not just run the pipeline using the b37 version of the genome? The gcp runner pipeline should support any reference genome,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:44,modifiability,version,version,44,Why not just run the pipeline using the b37 version of the genome? The gcp runner pipeline should support any reference genome,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:98,usability,support,support,98,Why not just run the pipeline using the b37 version of the genome? The gcp runner pipeline should support any reference genome,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:30,energy efficiency,Cloud,Cloud,30,@saliksyed the example on the Cloud runner page is actually a b37 genome:. ```. --ref gs://deepvariant/performance-testdata/hs37d5.fa.gz \. ```. See if that works for you? It should also already come with the corresponding indexed file and ready to go.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:103,performance,perform,performance-testdata,103,@saliksyed the example on the Cloud runner page is actually a b37 genome:. ```. --ref gs://deepvariant/performance-testdata/hs37d5.fa.gz \. ```. See if that works for you? It should also already come with the corresponding indexed file and ready to go.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:115,safety,test,testdata,115,@saliksyed the example on the Cloud runner page is actually a b37 genome:. ```. --ref gs://deepvariant/performance-testdata/hs37d5.fa.gz \. ```. See if that works for you? It should also already come with the corresponding indexed file and ready to go.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:115,testability,test,testdata,115,@saliksyed the example on the Cloud runner page is actually a b37 genome:. ```. --ref gs://deepvariant/performance-testdata/hs37d5.fa.gz \. ```. See if that works for you? It should also already come with the corresponding indexed file and ready to go.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:103,usability,perform,performance-testdata,103,@saliksyed the example on the Cloud runner page is actually a b37 genome:. ```. --ref gs://deepvariant/performance-testdata/hs37d5.fa.gz \. ```. See if that works for you? It should also already come with the corresponding indexed file and ready to go.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:243,deployability,automat,automatically,243,"I ran using the reference genome mentioned earlier, but it seems that the reference has the chromosomes named without the ""chr"" prefix, which the BAM file has for the contig names, it would be nice if the package could detect and correct this automatically -- do you think this makes sense, if so I am happy to submit a patch:. ```. 2018-11-10 01:31:37.340729: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1110 01:31:37.359072 140105147934464 genomics_reader.py:213] Reading /input-gcsfused-0/CNR-data/TLE_a_001.bam with NativeSamReader. I1110 01:31:37.383352 140105147934464 make_examples.py:1075] Preparing inputs. 2018-11-10 01:31:38.214366: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1110 01:31:38.223048 140105147934464 genomics_reader.py:213] Reading /input-gcsfused-0/CNR-data/TLE_a_001.bam with NativeSamReader. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1178, in main. make_examples_runner(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1076, in make_examples_runner. regions = processing_regions_from_options(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 990, in processing_regions_from_options. options.min_shared_contigs_basepairs). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 485, in _ensure_consistent_contigs. min_coverage_fraction). File ""/mnt/google/.google/tmp/Baze",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:320,deployability,patch,patch,320,"I ran using the reference genome mentioned earlier, but it seems that the reference has the chromosomes named without the ""chr"" prefix, which the BAM file has for the contig names, it would be nice if the package could detect and correct this automatically -- do you think this makes sense, if so I am happy to submit a patch:. ```. 2018-11-10 01:31:37.340729: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1110 01:31:37.359072 140105147934464 genomics_reader.py:213] Reading /input-gcsfused-0/CNR-data/TLE_a_001.bam with NativeSamReader. I1110 01:31:37.383352 140105147934464 make_examples.py:1075] Preparing inputs. 2018-11-10 01:31:38.214366: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1110 01:31:38.223048 140105147934464 genomics_reader.py:213] Reading /input-gcsfused-0/CNR-data/TLE_a_001.bam with NativeSamReader. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1178, in main. make_examples_runner(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1076, in make_examples_runner. regions = processing_regions_from_options(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 990, in processing_regions_from_options. options.min_shared_contigs_basepairs). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 485, in _ensure_consistent_contigs. min_coverage_fraction). File ""/mnt/google/.google/tmp/Baze",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:1072,deployability,modul,module,1072,"eference has the chromosomes named without the ""chr"" prefix, which the BAM file has for the contig names, it would be nice if the package could detect and correct this automatically -- do you think this makes sense, if so I am happy to submit a patch:. ```. 2018-11-10 01:31:37.340729: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1110 01:31:37.359072 140105147934464 genomics_reader.py:213] Reading /input-gcsfused-0/CNR-data/TLE_a_001.bam with NativeSamReader. I1110 01:31:37.383352 140105147934464 make_examples.py:1075] Preparing inputs. 2018-11-10 01:31:38.214366: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1110 01:31:38.223048 140105147934464 genomics_reader.py:213] Reading /input-gcsfused-0/CNR-data/TLE_a_001.bam with NativeSamReader. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1178, in main. make_examples_runner(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1076, in make_examples_runner. regions = processing_regions_from_options(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 990, in processing_regions_from_options. options.min_shared_contigs_basepairs). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 485, in _ensure_consistent_contigs. min_coverage_fraction). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:2372,deployability,build,build,2372,"tions). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1076, in make_examples_runner. regions = processing_regions_from_options(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 990, in processing_regions_from_options. options.min_shared_contigs_basepairs). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 485, in _ensure_consistent_contigs. min_coverage_fraction). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 557, in validate_reference_contig_coverage. ref_bp, common_bp, coverage, format_contig_matches())). ValueError: Reference contigs span 3095677412 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sources were created on a common genome reference build. Contig matches were: ""1"" is 249250621 bp and IS MISSING, ""2"" is 243199373 bp and IS MISSING, ""3"" is 198022430 bp and IS MISSING, ""4"" is 191154276 bp and IS MISSING, ""5"" is 180915260 bp and IS MISSING, ""6"" is 171115067 bp and IS MISSING, ""7"" is 159138663 bp and IS MISSING, ""8"" is 146364022 bp and IS MISSING, ""9"" is 141213431 bp and IS MISSING, ""10"" is 135534747 bp and IS MISSING, ""11"" is 135006516 bp and IS MISSING, ""12"" is 133851895 bp and IS MISSING, ""13"" is 115169878 bp and IS MISSING, ""14"" is 107349540 bp and IS MISSING, ""15"" is 102531392 bp and IS MISSING, ""16"" is 90354753 bp and IS MISSING, ""17"" is 81195210 bp and IS MISSING, ""18"" is 78077248 bp and IS MISSING, ""19"" is 59128983 bp and IS MISSING, ""20"" is 63025520 bp and IS MISSING, ""21"" is 48129895 bp and IS MISSING, ""22"" is 51304566 bp and IS MISSING, ""X"" is 155270560 bp and IS MISSING, ""Y"" is 59373566 bp and IS MISSING. parallel: This job failed:. mkdir -p ./input-gcsfused-0 && gcsfuse --implicit-dirs canis /input-gcsfused-0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:3288,deployability,fail,failed,3288,"epvariant/make_examples.py"", line 485, in _ensure_consistent_contigs. min_coverage_fraction). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 557, in validate_reference_contig_coverage. ref_bp, common_bp, coverage, format_contig_matches())). ValueError: Reference contigs span 3095677412 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sources were created on a common genome reference build. Contig matches were: ""1"" is 249250621 bp and IS MISSING, ""2"" is 243199373 bp and IS MISSING, ""3"" is 198022430 bp and IS MISSING, ""4"" is 191154276 bp and IS MISSING, ""5"" is 180915260 bp and IS MISSING, ""6"" is 171115067 bp and IS MISSING, ""7"" is 159138663 bp and IS MISSING, ""8"" is 146364022 bp and IS MISSING, ""9"" is 141213431 bp and IS MISSING, ""10"" is 135534747 bp and IS MISSING, ""11"" is 135006516 bp and IS MISSING, ""12"" is 133851895 bp and IS MISSING, ""13"" is 115169878 bp and IS MISSING, ""14"" is 107349540 bp and IS MISSING, ""15"" is 102531392 bp and IS MISSING, ""16"" is 90354753 bp and IS MISSING, ""17"" is 81195210 bp and IS MISSING, ""18"" is 78077248 bp and IS MISSING, ""19"" is 59128983 bp and IS MISSING, ""20"" is 63025520 bp and IS MISSING, ""21"" is 48129895 bp and IS MISSING, ""22"" is 51304566 bp and IS MISSING, ""X"" is 155270560 bp and IS MISSING, ""Y"" is 59373566 bp and IS MISSING. parallel: This job failed:. mkdir -p ./input-gcsfused-0 && gcsfuse --implicit-dirs canis /input-gcsfused-0 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/canis/CNR-data/deep_variant_files/examples/0/examples_output.tfrecord@8.gz --reads /input-gcsfused-0/CNR-data/TLE_a_001.bam --ref /mnt/google/.google/input/deepvariant/performance-testdata/hs37d5.fa.gz --task 0 --regions gs://canis/CNR-data/exomes.bed. Using mount point: /input-gcsfused-0. Opening GCS connection... Opening bucket... Mounting file system... File system has been successfully mounted. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:311,integrability,sub,submit,311,"I ran using the reference genome mentioned earlier, but it seems that the reference has the chromosomes named without the ""chr"" prefix, which the BAM file has for the contig names, it would be nice if the package could detect and correct this automatically -- do you think this makes sense, if so I am happy to submit a patch:. ```. 2018-11-10 01:31:37.340729: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1110 01:31:37.359072 140105147934464 genomics_reader.py:213] Reading /input-gcsfused-0/CNR-data/TLE_a_001.bam with NativeSamReader. I1110 01:31:37.383352 140105147934464 make_examples.py:1075] Preparing inputs. 2018-11-10 01:31:38.214366: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1110 01:31:38.223048 140105147934464 genomics_reader.py:213] Reading /input-gcsfused-0/CNR-data/TLE_a_001.bam with NativeSamReader. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1178, in main. make_examples_runner(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1076, in make_examples_runner. regions = processing_regions_from_options(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 990, in processing_regions_from_options. options.min_shared_contigs_basepairs). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 485, in _ensure_consistent_contigs. min_coverage_fraction). File ""/mnt/google/.google/tmp/Baze",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:1158,interoperability,platform,platform,1158," the contig names, it would be nice if the package could detect and correct this automatically -- do you think this makes sense, if so I am happy to submit a patch:. ```. 2018-11-10 01:31:37.340729: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1110 01:31:37.359072 140105147934464 genomics_reader.py:213] Reading /input-gcsfused-0/CNR-data/TLE_a_001.bam with NativeSamReader. I1110 01:31:37.383352 140105147934464 make_examples.py:1075] Preparing inputs. 2018-11-10 01:31:38.214366: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1110 01:31:38.223048 140105147934464 genomics_reader.py:213] Reading /input-gcsfused-0/CNR-data/TLE_a_001.bam with NativeSamReader. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1178, in main. make_examples_runner(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1076, in make_examples_runner. regions = processing_regions_from_options(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 990, in processing_regions_from_options. options.min_shared_contigs_basepairs). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 485, in _ensure_consistent_contigs. min_coverage_fraction). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 557, in validate_reference_contig_coverage. ref_bp, common_bp, coverage, for",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:205,modifiability,pac,package,205,"I ran using the reference genome mentioned earlier, but it seems that the reference has the chromosomes named without the ""chr"" prefix, which the BAM file has for the contig names, it would be nice if the package could detect and correct this automatically -- do you think this makes sense, if so I am happy to submit a patch:. ```. 2018-11-10 01:31:37.340729: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1110 01:31:37.359072 140105147934464 genomics_reader.py:213] Reading /input-gcsfused-0/CNR-data/TLE_a_001.bam with NativeSamReader. I1110 01:31:37.383352 140105147934464 make_examples.py:1075] Preparing inputs. 2018-11-10 01:31:38.214366: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1110 01:31:38.223048 140105147934464 genomics_reader.py:213] Reading /input-gcsfused-0/CNR-data/TLE_a_001.bam with NativeSamReader. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1178, in main. make_examples_runner(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1076, in make_examples_runner. regions = processing_regions_from_options(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 990, in processing_regions_from_options. options.min_shared_contigs_basepairs). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 485, in _ensure_consistent_contigs. min_coverage_fraction). File ""/mnt/google/.google/tmp/Baze",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:1072,modifiability,modul,module,1072,"eference has the chromosomes named without the ""chr"" prefix, which the BAM file has for the contig names, it would be nice if the package could detect and correct this automatically -- do you think this makes sense, if so I am happy to submit a patch:. ```. 2018-11-10 01:31:37.340729: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1110 01:31:37.359072 140105147934464 genomics_reader.py:213] Reading /input-gcsfused-0/CNR-data/TLE_a_001.bam with NativeSamReader. I1110 01:31:37.383352 140105147934464 make_examples.py:1075] Preparing inputs. 2018-11-10 01:31:38.214366: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1110 01:31:38.223048 140105147934464 genomics_reader.py:213] Reading /input-gcsfused-0/CNR-data/TLE_a_001.bam with NativeSamReader. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1178, in main. make_examples_runner(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1076, in make_examples_runner. regions = processing_regions_from_options(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 990, in processing_regions_from_options. options.min_shared_contigs_basepairs). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 485, in _ensure_consistent_contigs. min_coverage_fraction). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:1131,modifiability,pac,packages,1131," which the BAM file has for the contig names, it would be nice if the package could detect and correct this automatically -- do you think this makes sense, if so I am happy to submit a patch:. ```. 2018-11-10 01:31:37.340729: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1110 01:31:37.359072 140105147934464 genomics_reader.py:213] Reading /input-gcsfused-0/CNR-data/TLE_a_001.bam with NativeSamReader. I1110 01:31:37.383352 140105147934464 make_examples.py:1075] Preparing inputs. 2018-11-10 01:31:38.214366: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1110 01:31:38.223048 140105147934464 genomics_reader.py:213] Reading /input-gcsfused-0/CNR-data/TLE_a_001.bam with NativeSamReader. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1178, in main. make_examples_runner(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1076, in make_examples_runner. regions = processing_regions_from_options(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 990, in processing_regions_from_options. options.min_shared_contigs_basepairs). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 485, in _ensure_consistent_contigs. min_coverage_fraction). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 557, in validate_reference_contig_coverage. ref_b",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:3269,performance,parallel,parallel,3269,"epvariant/make_examples.py"", line 485, in _ensure_consistent_contigs. min_coverage_fraction). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 557, in validate_reference_contig_coverage. ref_bp, common_bp, coverage, format_contig_matches())). ValueError: Reference contigs span 3095677412 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sources were created on a common genome reference build. Contig matches were: ""1"" is 249250621 bp and IS MISSING, ""2"" is 243199373 bp and IS MISSING, ""3"" is 198022430 bp and IS MISSING, ""4"" is 191154276 bp and IS MISSING, ""5"" is 180915260 bp and IS MISSING, ""6"" is 171115067 bp and IS MISSING, ""7"" is 159138663 bp and IS MISSING, ""8"" is 146364022 bp and IS MISSING, ""9"" is 141213431 bp and IS MISSING, ""10"" is 135534747 bp and IS MISSING, ""11"" is 135006516 bp and IS MISSING, ""12"" is 133851895 bp and IS MISSING, ""13"" is 115169878 bp and IS MISSING, ""14"" is 107349540 bp and IS MISSING, ""15"" is 102531392 bp and IS MISSING, ""16"" is 90354753 bp and IS MISSING, ""17"" is 81195210 bp and IS MISSING, ""18"" is 78077248 bp and IS MISSING, ""19"" is 59128983 bp and IS MISSING, ""20"" is 63025520 bp and IS MISSING, ""21"" is 48129895 bp and IS MISSING, ""22"" is 51304566 bp and IS MISSING, ""X"" is 155270560 bp and IS MISSING, ""Y"" is 59373566 bp and IS MISSING. parallel: This job failed:. mkdir -p ./input-gcsfused-0 && gcsfuse --implicit-dirs canis /input-gcsfused-0 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/canis/CNR-data/deep_variant_files/examples/0/examples_output.tfrecord@8.gz --reads /input-gcsfused-0/CNR-data/TLE_a_001.bam --ref /mnt/google/.google/input/deepvariant/performance-testdata/hs37d5.fa.gz --task 0 --regions gs://canis/CNR-data/exomes.bed. Using mount point: /input-gcsfused-0. Opening GCS connection... Opening bucket... Mounting file system... File system has been successfully mounted. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:3635,performance,perform,performance-testdata,3635,"epvariant/make_examples.py"", line 485, in _ensure_consistent_contigs. min_coverage_fraction). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 557, in validate_reference_contig_coverage. ref_bp, common_bp, coverage, format_contig_matches())). ValueError: Reference contigs span 3095677412 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sources were created on a common genome reference build. Contig matches were: ""1"" is 249250621 bp and IS MISSING, ""2"" is 243199373 bp and IS MISSING, ""3"" is 198022430 bp and IS MISSING, ""4"" is 191154276 bp and IS MISSING, ""5"" is 180915260 bp and IS MISSING, ""6"" is 171115067 bp and IS MISSING, ""7"" is 159138663 bp and IS MISSING, ""8"" is 146364022 bp and IS MISSING, ""9"" is 141213431 bp and IS MISSING, ""10"" is 135534747 bp and IS MISSING, ""11"" is 135006516 bp and IS MISSING, ""12"" is 133851895 bp and IS MISSING, ""13"" is 115169878 bp and IS MISSING, ""14"" is 107349540 bp and IS MISSING, ""15"" is 102531392 bp and IS MISSING, ""16"" is 90354753 bp and IS MISSING, ""17"" is 81195210 bp and IS MISSING, ""18"" is 78077248 bp and IS MISSING, ""19"" is 59128983 bp and IS MISSING, ""20"" is 63025520 bp and IS MISSING, ""21"" is 48129895 bp and IS MISSING, ""22"" is 51304566 bp and IS MISSING, ""X"" is 155270560 bp and IS MISSING, ""Y"" is 59373566 bp and IS MISSING. parallel: This job failed:. mkdir -p ./input-gcsfused-0 && gcsfuse --implicit-dirs canis /input-gcsfused-0 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/canis/CNR-data/deep_variant_files/examples/0/examples_output.tfrecord@8.gz --reads /input-gcsfused-0/CNR-data/TLE_a_001.bam --ref /mnt/google/.google/input/deepvariant/performance-testdata/hs37d5.fa.gz --task 0 --regions gs://canis/CNR-data/exomes.bed. Using mount point: /input-gcsfused-0. Opening GCS connection... Opening bucket... Mounting file system... File system has been successfully mounted. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:3288,reliability,fail,failed,3288,"epvariant/make_examples.py"", line 485, in _ensure_consistent_contigs. min_coverage_fraction). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 557, in validate_reference_contig_coverage. ref_bp, common_bp, coverage, format_contig_matches())). ValueError: Reference contigs span 3095677412 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sources were created on a common genome reference build. Contig matches were: ""1"" is 249250621 bp and IS MISSING, ""2"" is 243199373 bp and IS MISSING, ""3"" is 198022430 bp and IS MISSING, ""4"" is 191154276 bp and IS MISSING, ""5"" is 180915260 bp and IS MISSING, ""6"" is 171115067 bp and IS MISSING, ""7"" is 159138663 bp and IS MISSING, ""8"" is 146364022 bp and IS MISSING, ""9"" is 141213431 bp and IS MISSING, ""10"" is 135534747 bp and IS MISSING, ""11"" is 135006516 bp and IS MISSING, ""12"" is 133851895 bp and IS MISSING, ""13"" is 115169878 bp and IS MISSING, ""14"" is 107349540 bp and IS MISSING, ""15"" is 102531392 bp and IS MISSING, ""16"" is 90354753 bp and IS MISSING, ""17"" is 81195210 bp and IS MISSING, ""18"" is 78077248 bp and IS MISSING, ""19"" is 59128983 bp and IS MISSING, ""20"" is 63025520 bp and IS MISSING, ""21"" is 48129895 bp and IS MISSING, ""22"" is 51304566 bp and IS MISSING, ""X"" is 155270560 bp and IS MISSING, ""Y"" is 59373566 bp and IS MISSING. parallel: This job failed:. mkdir -p ./input-gcsfused-0 && gcsfuse --implicit-dirs canis /input-gcsfused-0 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/canis/CNR-data/deep_variant_files/examples/0/examples_output.tfrecord@8.gz --reads /input-gcsfused-0/CNR-data/TLE_a_001.bam --ref /mnt/google/.google/input/deepvariant/performance-testdata/hs37d5.fa.gz --task 0 --regions gs://canis/CNR-data/exomes.bed. Using mount point: /input-gcsfused-0. Opening GCS connection... Opening bucket... Mounting file system... File system has been successfully mounted. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:219,safety,detect,detect,219,"I ran using the reference genome mentioned earlier, but it seems that the reference has the chromosomes named without the ""chr"" prefix, which the BAM file has for the contig names, it would be nice if the package could detect and correct this automatically -- do you think this makes sense, if so I am happy to submit a patch:. ```. 2018-11-10 01:31:37.340729: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1110 01:31:37.359072 140105147934464 genomics_reader.py:213] Reading /input-gcsfused-0/CNR-data/TLE_a_001.bam with NativeSamReader. I1110 01:31:37.383352 140105147934464 make_examples.py:1075] Preparing inputs. 2018-11-10 01:31:38.214366: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1110 01:31:38.223048 140105147934464 genomics_reader.py:213] Reading /input-gcsfused-0/CNR-data/TLE_a_001.bam with NativeSamReader. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1178, in main. make_examples_runner(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1076, in make_examples_runner. regions = processing_regions_from_options(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 990, in processing_regions_from_options. options.min_shared_contigs_basepairs). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 485, in _ensure_consistent_contigs. min_coverage_fraction). File ""/mnt/google/.google/tmp/Baze",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:320,safety,patch,patch,320,"I ran using the reference genome mentioned earlier, but it seems that the reference has the chromosomes named without the ""chr"" prefix, which the BAM file has for the contig names, it would be nice if the package could detect and correct this automatically -- do you think this makes sense, if so I am happy to submit a patch:. ```. 2018-11-10 01:31:37.340729: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1110 01:31:37.359072 140105147934464 genomics_reader.py:213] Reading /input-gcsfused-0/CNR-data/TLE_a_001.bam with NativeSamReader. I1110 01:31:37.383352 140105147934464 make_examples.py:1075] Preparing inputs. 2018-11-10 01:31:38.214366: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1110 01:31:38.223048 140105147934464 genomics_reader.py:213] Reading /input-gcsfused-0/CNR-data/TLE_a_001.bam with NativeSamReader. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1178, in main. make_examples_runner(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1076, in make_examples_runner. regions = processing_regions_from_options(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 990, in processing_regions_from_options. options.min_shared_contigs_basepairs). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 485, in _ensure_consistent_contigs. min_coverage_fraction). File ""/mnt/google/.google/tmp/Baze",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:518,safety,input,input-gcsfused-,518,"I ran using the reference genome mentioned earlier, but it seems that the reference has the chromosomes named without the ""chr"" prefix, which the BAM file has for the contig names, it would be nice if the package could detect and correct this automatically -- do you think this makes sense, if so I am happy to submit a patch:. ```. 2018-11-10 01:31:37.340729: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1110 01:31:37.359072 140105147934464 genomics_reader.py:213] Reading /input-gcsfused-0/CNR-data/TLE_a_001.bam with NativeSamReader. I1110 01:31:37.383352 140105147934464 make_examples.py:1075] Preparing inputs. 2018-11-10 01:31:38.214366: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1110 01:31:38.223048 140105147934464 genomics_reader.py:213] Reading /input-gcsfused-0/CNR-data/TLE_a_001.bam with NativeSamReader. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1178, in main. make_examples_runner(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1076, in make_examples_runner. regions = processing_regions_from_options(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 990, in processing_regions_from_options. options.min_shared_contigs_basepairs). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 485, in _ensure_consistent_contigs. min_coverage_fraction). File ""/mnt/google/.google/tmp/Baze",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:651,safety,input,inputs,651,"I ran using the reference genome mentioned earlier, but it seems that the reference has the chromosomes named without the ""chr"" prefix, which the BAM file has for the contig names, it would be nice if the package could detect and correct this automatically -- do you think this makes sense, if so I am happy to submit a patch:. ```. 2018-11-10 01:31:37.340729: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1110 01:31:37.359072 140105147934464 genomics_reader.py:213] Reading /input-gcsfused-0/CNR-data/TLE_a_001.bam with NativeSamReader. I1110 01:31:37.383352 140105147934464 make_examples.py:1075] Preparing inputs. 2018-11-10 01:31:38.214366: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1110 01:31:38.223048 140105147934464 genomics_reader.py:213] Reading /input-gcsfused-0/CNR-data/TLE_a_001.bam with NativeSamReader. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1178, in main. make_examples_runner(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1076, in make_examples_runner. regions = processing_regions_from_options(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 990, in processing_regions_from_options. options.min_shared_contigs_basepairs). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 485, in _ensure_consistent_contigs. min_coverage_fraction). File ""/mnt/google/.google/tmp/Baze",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:844,safety,input,input-gcsfused-,844,"I ran using the reference genome mentioned earlier, but it seems that the reference has the chromosomes named without the ""chr"" prefix, which the BAM file has for the contig names, it would be nice if the package could detect and correct this automatically -- do you think this makes sense, if so I am happy to submit a patch:. ```. 2018-11-10 01:31:37.340729: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1110 01:31:37.359072 140105147934464 genomics_reader.py:213] Reading /input-gcsfused-0/CNR-data/TLE_a_001.bam with NativeSamReader. I1110 01:31:37.383352 140105147934464 make_examples.py:1075] Preparing inputs. 2018-11-10 01:31:38.214366: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1110 01:31:38.223048 140105147934464 genomics_reader.py:213] Reading /input-gcsfused-0/CNR-data/TLE_a_001.bam with NativeSamReader. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1178, in main. make_examples_runner(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1076, in make_examples_runner. regions = processing_regions_from_options(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 990, in processing_regions_from_options. options.min_shared_contigs_basepairs). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 485, in _ensure_consistent_contigs. min_coverage_fraction). File ""/mnt/google/.google/tmp/Baze",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:1072,safety,modul,module,1072,"eference has the chromosomes named without the ""chr"" prefix, which the BAM file has for the contig names, it would be nice if the package could detect and correct this automatically -- do you think this makes sense, if so I am happy to submit a patch:. ```. 2018-11-10 01:31:37.340729: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1110 01:31:37.359072 140105147934464 genomics_reader.py:213] Reading /input-gcsfused-0/CNR-data/TLE_a_001.bam with NativeSamReader. I1110 01:31:37.383352 140105147934464 make_examples.py:1075] Preparing inputs. 2018-11-10 01:31:38.214366: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1110 01:31:38.223048 140105147934464 genomics_reader.py:213] Reading /input-gcsfused-0/CNR-data/TLE_a_001.bam with NativeSamReader. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1178, in main. make_examples_runner(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1076, in make_examples_runner. regions = processing_regions_from_options(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 990, in processing_regions_from_options. options.min_shared_contigs_basepairs). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 485, in _ensure_consistent_contigs. min_coverage_fraction). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:2294,safety,input,input,2294,"ant/deepvariant/make_examples.py"", line 1178, in main. make_examples_runner(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1076, in make_examples_runner. regions = processing_regions_from_options(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 990, in processing_regions_from_options. options.min_shared_contigs_basepairs). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 485, in _ensure_consistent_contigs. min_coverage_fraction). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 557, in validate_reference_contig_coverage. ref_bp, common_bp, coverage, format_contig_matches())). ValueError: Reference contigs span 3095677412 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sources were created on a common genome reference build. Contig matches were: ""1"" is 249250621 bp and IS MISSING, ""2"" is 243199373 bp and IS MISSING, ""3"" is 198022430 bp and IS MISSING, ""4"" is 191154276 bp and IS MISSING, ""5"" is 180915260 bp and IS MISSING, ""6"" is 171115067 bp and IS MISSING, ""7"" is 159138663 bp and IS MISSING, ""8"" is 146364022 bp and IS MISSING, ""9"" is 141213431 bp and IS MISSING, ""10"" is 135534747 bp and IS MISSING, ""11"" is 135006516 bp and IS MISSING, ""12"" is 133851895 bp and IS MISSING, ""13"" is 115169878 bp and IS MISSING, ""14"" is 107349540 bp and IS MISSING, ""15"" is 102531392 bp and IS MISSING, ""16"" is 90354753 bp and IS MISSING, ""17"" is 81195210 bp and IS MISSING, ""18"" is 78077248 bp and IS MISSING, ""19"" is 59128983 bp and IS MISSING, ""20"" is 63025520 bp and IS MISSING, ""21"" is 48129895 bp and IS MISSING, ""22"" is 51304566 bp and IS MISSING, ""X"" is 155270560 bp and IS MISSING, ""Y"" is 59373566 bp and IS MISSING. parallel: This job failed:. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:3308,safety,input,input-gcsfused-,3308,"epvariant/make_examples.py"", line 485, in _ensure_consistent_contigs. min_coverage_fraction). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 557, in validate_reference_contig_coverage. ref_bp, common_bp, coverage, format_contig_matches())). ValueError: Reference contigs span 3095677412 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sources were created on a common genome reference build. Contig matches were: ""1"" is 249250621 bp and IS MISSING, ""2"" is 243199373 bp and IS MISSING, ""3"" is 198022430 bp and IS MISSING, ""4"" is 191154276 bp and IS MISSING, ""5"" is 180915260 bp and IS MISSING, ""6"" is 171115067 bp and IS MISSING, ""7"" is 159138663 bp and IS MISSING, ""8"" is 146364022 bp and IS MISSING, ""9"" is 141213431 bp and IS MISSING, ""10"" is 135534747 bp and IS MISSING, ""11"" is 135006516 bp and IS MISSING, ""12"" is 133851895 bp and IS MISSING, ""13"" is 115169878 bp and IS MISSING, ""14"" is 107349540 bp and IS MISSING, ""15"" is 102531392 bp and IS MISSING, ""16"" is 90354753 bp and IS MISSING, ""17"" is 81195210 bp and IS MISSING, ""18"" is 78077248 bp and IS MISSING, ""19"" is 59128983 bp and IS MISSING, ""20"" is 63025520 bp and IS MISSING, ""21"" is 48129895 bp and IS MISSING, ""22"" is 51304566 bp and IS MISSING, ""X"" is 155270560 bp and IS MISSING, ""Y"" is 59373566 bp and IS MISSING. parallel: This job failed:. mkdir -p ./input-gcsfused-0 && gcsfuse --implicit-dirs canis /input-gcsfused-0 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/canis/CNR-data/deep_variant_files/examples/0/examples_output.tfrecord@8.gz --reads /input-gcsfused-0/CNR-data/TLE_a_001.bam --ref /mnt/google/.google/input/deepvariant/performance-testdata/hs37d5.fa.gz --task 0 --regions gs://canis/CNR-data/exomes.bed. Using mount point: /input-gcsfused-0. Opening GCS connection... Opening bucket... Mounting file system... File system has been successfully mounted. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:3359,safety,input,input-gcsfused-,3359,"epvariant/make_examples.py"", line 485, in _ensure_consistent_contigs. min_coverage_fraction). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 557, in validate_reference_contig_coverage. ref_bp, common_bp, coverage, format_contig_matches())). ValueError: Reference contigs span 3095677412 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sources were created on a common genome reference build. Contig matches were: ""1"" is 249250621 bp and IS MISSING, ""2"" is 243199373 bp and IS MISSING, ""3"" is 198022430 bp and IS MISSING, ""4"" is 191154276 bp and IS MISSING, ""5"" is 180915260 bp and IS MISSING, ""6"" is 171115067 bp and IS MISSING, ""7"" is 159138663 bp and IS MISSING, ""8"" is 146364022 bp and IS MISSING, ""9"" is 141213431 bp and IS MISSING, ""10"" is 135534747 bp and IS MISSING, ""11"" is 135006516 bp and IS MISSING, ""12"" is 133851895 bp and IS MISSING, ""13"" is 115169878 bp and IS MISSING, ""14"" is 107349540 bp and IS MISSING, ""15"" is 102531392 bp and IS MISSING, ""16"" is 90354753 bp and IS MISSING, ""17"" is 81195210 bp and IS MISSING, ""18"" is 78077248 bp and IS MISSING, ""19"" is 59128983 bp and IS MISSING, ""20"" is 63025520 bp and IS MISSING, ""21"" is 48129895 bp and IS MISSING, ""22"" is 51304566 bp and IS MISSING, ""X"" is 155270560 bp and IS MISSING, ""Y"" is 59373566 bp and IS MISSING. parallel: This job failed:. mkdir -p ./input-gcsfused-0 && gcsfuse --implicit-dirs canis /input-gcsfused-0 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/canis/CNR-data/deep_variant_files/examples/0/examples_output.tfrecord@8.gz --reads /input-gcsfused-0/CNR-data/TLE_a_001.bam --ref /mnt/google/.google/input/deepvariant/performance-testdata/hs37d5.fa.gz --task 0 --regions gs://canis/CNR-data/exomes.bed. Using mount point: /input-gcsfused-0. Opening GCS connection... Opening bucket... Mounting file system... File system has been successfully mounted. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:3551,safety,input,input-gcsfused-,3551,"epvariant/make_examples.py"", line 485, in _ensure_consistent_contigs. min_coverage_fraction). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 557, in validate_reference_contig_coverage. ref_bp, common_bp, coverage, format_contig_matches())). ValueError: Reference contigs span 3095677412 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sources were created on a common genome reference build. Contig matches were: ""1"" is 249250621 bp and IS MISSING, ""2"" is 243199373 bp and IS MISSING, ""3"" is 198022430 bp and IS MISSING, ""4"" is 191154276 bp and IS MISSING, ""5"" is 180915260 bp and IS MISSING, ""6"" is 171115067 bp and IS MISSING, ""7"" is 159138663 bp and IS MISSING, ""8"" is 146364022 bp and IS MISSING, ""9"" is 141213431 bp and IS MISSING, ""10"" is 135534747 bp and IS MISSING, ""11"" is 135006516 bp and IS MISSING, ""12"" is 133851895 bp and IS MISSING, ""13"" is 115169878 bp and IS MISSING, ""14"" is 107349540 bp and IS MISSING, ""15"" is 102531392 bp and IS MISSING, ""16"" is 90354753 bp and IS MISSING, ""17"" is 81195210 bp and IS MISSING, ""18"" is 78077248 bp and IS MISSING, ""19"" is 59128983 bp and IS MISSING, ""20"" is 63025520 bp and IS MISSING, ""21"" is 48129895 bp and IS MISSING, ""22"" is 51304566 bp and IS MISSING, ""X"" is 155270560 bp and IS MISSING, ""Y"" is 59373566 bp and IS MISSING. parallel: This job failed:. mkdir -p ./input-gcsfused-0 && gcsfuse --implicit-dirs canis /input-gcsfused-0 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/canis/CNR-data/deep_variant_files/examples/0/examples_output.tfrecord@8.gz --reads /input-gcsfused-0/CNR-data/TLE_a_001.bam --ref /mnt/google/.google/input/deepvariant/performance-testdata/hs37d5.fa.gz --task 0 --regions gs://canis/CNR-data/exomes.bed. Using mount point: /input-gcsfused-0. Opening GCS connection... Opening bucket... Mounting file system... File system has been successfully mounted. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:3617,safety,input,input,3617,"epvariant/make_examples.py"", line 485, in _ensure_consistent_contigs. min_coverage_fraction). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 557, in validate_reference_contig_coverage. ref_bp, common_bp, coverage, format_contig_matches())). ValueError: Reference contigs span 3095677412 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sources were created on a common genome reference build. Contig matches were: ""1"" is 249250621 bp and IS MISSING, ""2"" is 243199373 bp and IS MISSING, ""3"" is 198022430 bp and IS MISSING, ""4"" is 191154276 bp and IS MISSING, ""5"" is 180915260 bp and IS MISSING, ""6"" is 171115067 bp and IS MISSING, ""7"" is 159138663 bp and IS MISSING, ""8"" is 146364022 bp and IS MISSING, ""9"" is 141213431 bp and IS MISSING, ""10"" is 135534747 bp and IS MISSING, ""11"" is 135006516 bp and IS MISSING, ""12"" is 133851895 bp and IS MISSING, ""13"" is 115169878 bp and IS MISSING, ""14"" is 107349540 bp and IS MISSING, ""15"" is 102531392 bp and IS MISSING, ""16"" is 90354753 bp and IS MISSING, ""17"" is 81195210 bp and IS MISSING, ""18"" is 78077248 bp and IS MISSING, ""19"" is 59128983 bp and IS MISSING, ""20"" is 63025520 bp and IS MISSING, ""21"" is 48129895 bp and IS MISSING, ""22"" is 51304566 bp and IS MISSING, ""X"" is 155270560 bp and IS MISSING, ""Y"" is 59373566 bp and IS MISSING. parallel: This job failed:. mkdir -p ./input-gcsfused-0 && gcsfuse --implicit-dirs canis /input-gcsfused-0 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/canis/CNR-data/deep_variant_files/examples/0/examples_output.tfrecord@8.gz --reads /input-gcsfused-0/CNR-data/TLE_a_001.bam --ref /mnt/google/.google/input/deepvariant/performance-testdata/hs37d5.fa.gz --task 0 --regions gs://canis/CNR-data/exomes.bed. Using mount point: /input-gcsfused-0. Opening GCS connection... Opening bucket... Mounting file system... File system has been successfully mounted. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:3647,safety,test,testdata,3647,"epvariant/make_examples.py"", line 485, in _ensure_consistent_contigs. min_coverage_fraction). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 557, in validate_reference_contig_coverage. ref_bp, common_bp, coverage, format_contig_matches())). ValueError: Reference contigs span 3095677412 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sources were created on a common genome reference build. Contig matches were: ""1"" is 249250621 bp and IS MISSING, ""2"" is 243199373 bp and IS MISSING, ""3"" is 198022430 bp and IS MISSING, ""4"" is 191154276 bp and IS MISSING, ""5"" is 180915260 bp and IS MISSING, ""6"" is 171115067 bp and IS MISSING, ""7"" is 159138663 bp and IS MISSING, ""8"" is 146364022 bp and IS MISSING, ""9"" is 141213431 bp and IS MISSING, ""10"" is 135534747 bp and IS MISSING, ""11"" is 135006516 bp and IS MISSING, ""12"" is 133851895 bp and IS MISSING, ""13"" is 115169878 bp and IS MISSING, ""14"" is 107349540 bp and IS MISSING, ""15"" is 102531392 bp and IS MISSING, ""16"" is 90354753 bp and IS MISSING, ""17"" is 81195210 bp and IS MISSING, ""18"" is 78077248 bp and IS MISSING, ""19"" is 59128983 bp and IS MISSING, ""20"" is 63025520 bp and IS MISSING, ""21"" is 48129895 bp and IS MISSING, ""22"" is 51304566 bp and IS MISSING, ""X"" is 155270560 bp and IS MISSING, ""Y"" is 59373566 bp and IS MISSING. parallel: This job failed:. mkdir -p ./input-gcsfused-0 && gcsfuse --implicit-dirs canis /input-gcsfused-0 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/canis/CNR-data/deep_variant_files/examples/0/examples_output.tfrecord@8.gz --reads /input-gcsfused-0/CNR-data/TLE_a_001.bam --ref /mnt/google/.google/input/deepvariant/performance-testdata/hs37d5.fa.gz --task 0 --regions gs://canis/CNR-data/exomes.bed. Using mount point: /input-gcsfused-0. Opening GCS connection... Opening bucket... Mounting file system... File system has been successfully mounted. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:3740,safety,input,input-gcsfused-,3740,"epvariant/make_examples.py"", line 485, in _ensure_consistent_contigs. min_coverage_fraction). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 557, in validate_reference_contig_coverage. ref_bp, common_bp, coverage, format_contig_matches())). ValueError: Reference contigs span 3095677412 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sources were created on a common genome reference build. Contig matches were: ""1"" is 249250621 bp and IS MISSING, ""2"" is 243199373 bp and IS MISSING, ""3"" is 198022430 bp and IS MISSING, ""4"" is 191154276 bp and IS MISSING, ""5"" is 180915260 bp and IS MISSING, ""6"" is 171115067 bp and IS MISSING, ""7"" is 159138663 bp and IS MISSING, ""8"" is 146364022 bp and IS MISSING, ""9"" is 141213431 bp and IS MISSING, ""10"" is 135534747 bp and IS MISSING, ""11"" is 135006516 bp and IS MISSING, ""12"" is 133851895 bp and IS MISSING, ""13"" is 115169878 bp and IS MISSING, ""14"" is 107349540 bp and IS MISSING, ""15"" is 102531392 bp and IS MISSING, ""16"" is 90354753 bp and IS MISSING, ""17"" is 81195210 bp and IS MISSING, ""18"" is 78077248 bp and IS MISSING, ""19"" is 59128983 bp and IS MISSING, ""20"" is 63025520 bp and IS MISSING, ""21"" is 48129895 bp and IS MISSING, ""22"" is 51304566 bp and IS MISSING, ""X"" is 155270560 bp and IS MISSING, ""Y"" is 59373566 bp and IS MISSING. parallel: This job failed:. mkdir -p ./input-gcsfused-0 && gcsfuse --implicit-dirs canis /input-gcsfused-0 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/canis/CNR-data/deep_variant_files/examples/0/examples_output.tfrecord@8.gz --reads /input-gcsfused-0/CNR-data/TLE_a_001.bam --ref /mnt/google/.google/input/deepvariant/performance-testdata/hs37d5.fa.gz --task 0 --regions gs://canis/CNR-data/exomes.bed. Using mount point: /input-gcsfused-0. Opening GCS connection... Opening bucket... Mounting file system... File system has been successfully mounted. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:219,security,detect,detect,219,"I ran using the reference genome mentioned earlier, but it seems that the reference has the chromosomes named without the ""chr"" prefix, which the BAM file has for the contig names, it would be nice if the package could detect and correct this automatically -- do you think this makes sense, if so I am happy to submit a patch:. ```. 2018-11-10 01:31:37.340729: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1110 01:31:37.359072 140105147934464 genomics_reader.py:213] Reading /input-gcsfused-0/CNR-data/TLE_a_001.bam with NativeSamReader. I1110 01:31:37.383352 140105147934464 make_examples.py:1075] Preparing inputs. 2018-11-10 01:31:38.214366: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1110 01:31:38.223048 140105147934464 genomics_reader.py:213] Reading /input-gcsfused-0/CNR-data/TLE_a_001.bam with NativeSamReader. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1178, in main. make_examples_runner(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1076, in make_examples_runner. regions = processing_regions_from_options(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 990, in processing_regions_from_options. options.min_shared_contigs_basepairs). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 485, in _ensure_consistent_contigs. min_coverage_fraction). File ""/mnt/google/.google/tmp/Baze",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:320,security,patch,patch,320,"I ran using the reference genome mentioned earlier, but it seems that the reference has the chromosomes named without the ""chr"" prefix, which the BAM file has for the contig names, it would be nice if the package could detect and correct this automatically -- do you think this makes sense, if so I am happy to submit a patch:. ```. 2018-11-10 01:31:37.340729: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1110 01:31:37.359072 140105147934464 genomics_reader.py:213] Reading /input-gcsfused-0/CNR-data/TLE_a_001.bam with NativeSamReader. I1110 01:31:37.383352 140105147934464 make_examples.py:1075] Preparing inputs. 2018-11-10 01:31:38.214366: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1110 01:31:38.223048 140105147934464 genomics_reader.py:213] Reading /input-gcsfused-0/CNR-data/TLE_a_001.bam with NativeSamReader. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1178, in main. make_examples_runner(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1076, in make_examples_runner. regions = processing_regions_from_options(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 990, in processing_regions_from_options. options.min_shared_contigs_basepairs). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 485, in _ensure_consistent_contigs. min_coverage_fraction). File ""/mnt/google/.google/tmp/Baze",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:243,testability,automat,automatically,243,"I ran using the reference genome mentioned earlier, but it seems that the reference has the chromosomes named without the ""chr"" prefix, which the BAM file has for the contig names, it would be nice if the package could detect and correct this automatically -- do you think this makes sense, if so I am happy to submit a patch:. ```. 2018-11-10 01:31:37.340729: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1110 01:31:37.359072 140105147934464 genomics_reader.py:213] Reading /input-gcsfused-0/CNR-data/TLE_a_001.bam with NativeSamReader. I1110 01:31:37.383352 140105147934464 make_examples.py:1075] Preparing inputs. 2018-11-10 01:31:38.214366: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1110 01:31:38.223048 140105147934464 genomics_reader.py:213] Reading /input-gcsfused-0/CNR-data/TLE_a_001.bam with NativeSamReader. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1178, in main. make_examples_runner(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1076, in make_examples_runner. regions = processing_regions_from_options(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 990, in processing_regions_from_options. options.min_shared_contigs_basepairs). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 485, in _ensure_consistent_contigs. min_coverage_fraction). File ""/mnt/google/.google/tmp/Baze",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:906,testability,Trace,Traceback,906,"I ran using the reference genome mentioned earlier, but it seems that the reference has the chromosomes named without the ""chr"" prefix, which the BAM file has for the contig names, it would be nice if the package could detect and correct this automatically -- do you think this makes sense, if so I am happy to submit a patch:. ```. 2018-11-10 01:31:37.340729: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1110 01:31:37.359072 140105147934464 genomics_reader.py:213] Reading /input-gcsfused-0/CNR-data/TLE_a_001.bam with NativeSamReader. I1110 01:31:37.383352 140105147934464 make_examples.py:1075] Preparing inputs. 2018-11-10 01:31:38.214366: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1110 01:31:38.223048 140105147934464 genomics_reader.py:213] Reading /input-gcsfused-0/CNR-data/TLE_a_001.bam with NativeSamReader. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1178, in main. make_examples_runner(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1076, in make_examples_runner. regions = processing_regions_from_options(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 990, in processing_regions_from_options. options.min_shared_contigs_basepairs). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 485, in _ensure_consistent_contigs. min_coverage_fraction). File ""/mnt/google/.google/tmp/Baze",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:2149,testability,coverag,coverage,2149,"thon/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1178, in main. make_examples_runner(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1076, in make_examples_runner. regions = processing_regions_from_options(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 990, in processing_regions_from_options. options.min_shared_contigs_basepairs). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 485, in _ensure_consistent_contigs. min_coverage_fraction). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 557, in validate_reference_contig_coverage. ref_bp, common_bp, coverage, format_contig_matches())). ValueError: Reference contigs span 3095677412 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sources were created on a common genome reference build. Contig matches were: ""1"" is 249250621 bp and IS MISSING, ""2"" is 243199373 bp and IS MISSING, ""3"" is 198022430 bp and IS MISSING, ""4"" is 191154276 bp and IS MISSING, ""5"" is 180915260 bp and IS MISSING, ""6"" is 171115067 bp and IS MISSING, ""7"" is 159138663 bp and IS MISSING, ""8"" is 146364022 bp and IS MISSING, ""9"" is 141213431 bp and IS MISSING, ""10"" is 135534747 bp and IS MISSING, ""11"" is 135006516 bp and IS MISSING, ""12"" is 133851895 bp and IS MISSING, ""13"" is 115169878 bp and IS MISSING, ""14"" is 107349540 bp and IS MISSING, ""15"" is 102531392 bp and IS MISSING, ""16"" is 90354753 bp and IS MISSING, ""17"" is 81195210 bp and IS MISSING, ""18"" is 78077248 bp and IS MISSING, ""19"" is 59128983 bp and IS MISSING, ""20"" is 63025520 bp and IS MISSING, ""21"" is 48129895 bp and IS ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:3647,testability,test,testdata,3647,"epvariant/make_examples.py"", line 485, in _ensure_consistent_contigs. min_coverage_fraction). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 557, in validate_reference_contig_coverage. ref_bp, common_bp, coverage, format_contig_matches())). ValueError: Reference contigs span 3095677412 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sources were created on a common genome reference build. Contig matches were: ""1"" is 249250621 bp and IS MISSING, ""2"" is 243199373 bp and IS MISSING, ""3"" is 198022430 bp and IS MISSING, ""4"" is 191154276 bp and IS MISSING, ""5"" is 180915260 bp and IS MISSING, ""6"" is 171115067 bp and IS MISSING, ""7"" is 159138663 bp and IS MISSING, ""8"" is 146364022 bp and IS MISSING, ""9"" is 141213431 bp and IS MISSING, ""10"" is 135534747 bp and IS MISSING, ""11"" is 135006516 bp and IS MISSING, ""12"" is 133851895 bp and IS MISSING, ""13"" is 115169878 bp and IS MISSING, ""14"" is 107349540 bp and IS MISSING, ""15"" is 102531392 bp and IS MISSING, ""16"" is 90354753 bp and IS MISSING, ""17"" is 81195210 bp and IS MISSING, ""18"" is 78077248 bp and IS MISSING, ""19"" is 59128983 bp and IS MISSING, ""20"" is 63025520 bp and IS MISSING, ""21"" is 48129895 bp and IS MISSING, ""22"" is 51304566 bp and IS MISSING, ""X"" is 155270560 bp and IS MISSING, ""Y"" is 59373566 bp and IS MISSING. parallel: This job failed:. mkdir -p ./input-gcsfused-0 && gcsfuse --implicit-dirs canis /input-gcsfused-0 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/canis/CNR-data/deep_variant_files/examples/0/examples_output.tfrecord@8.gz --reads /input-gcsfused-0/CNR-data/TLE_a_001.bam --ref /mnt/google/.google/input/deepvariant/performance-testdata/hs37d5.fa.gz --task 0 --regions gs://canis/CNR-data/exomes.bed. Using mount point: /input-gcsfused-0. Opening GCS connection... Opening bucket... Mounting file system... File system has been successfully mounted. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:518,usability,input,input-gcsfused-,518,"I ran using the reference genome mentioned earlier, but it seems that the reference has the chromosomes named without the ""chr"" prefix, which the BAM file has for the contig names, it would be nice if the package could detect and correct this automatically -- do you think this makes sense, if so I am happy to submit a patch:. ```. 2018-11-10 01:31:37.340729: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1110 01:31:37.359072 140105147934464 genomics_reader.py:213] Reading /input-gcsfused-0/CNR-data/TLE_a_001.bam with NativeSamReader. I1110 01:31:37.383352 140105147934464 make_examples.py:1075] Preparing inputs. 2018-11-10 01:31:38.214366: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1110 01:31:38.223048 140105147934464 genomics_reader.py:213] Reading /input-gcsfused-0/CNR-data/TLE_a_001.bam with NativeSamReader. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1178, in main. make_examples_runner(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1076, in make_examples_runner. regions = processing_regions_from_options(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 990, in processing_regions_from_options. options.min_shared_contigs_basepairs). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 485, in _ensure_consistent_contigs. min_coverage_fraction). File ""/mnt/google/.google/tmp/Baze",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:651,usability,input,inputs,651,"I ran using the reference genome mentioned earlier, but it seems that the reference has the chromosomes named without the ""chr"" prefix, which the BAM file has for the contig names, it would be nice if the package could detect and correct this automatically -- do you think this makes sense, if so I am happy to submit a patch:. ```. 2018-11-10 01:31:37.340729: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1110 01:31:37.359072 140105147934464 genomics_reader.py:213] Reading /input-gcsfused-0/CNR-data/TLE_a_001.bam with NativeSamReader. I1110 01:31:37.383352 140105147934464 make_examples.py:1075] Preparing inputs. 2018-11-10 01:31:38.214366: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1110 01:31:38.223048 140105147934464 genomics_reader.py:213] Reading /input-gcsfused-0/CNR-data/TLE_a_001.bam with NativeSamReader. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1178, in main. make_examples_runner(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1076, in make_examples_runner. regions = processing_regions_from_options(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 990, in processing_regions_from_options. options.min_shared_contigs_basepairs). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 485, in _ensure_consistent_contigs. min_coverage_fraction). File ""/mnt/google/.google/tmp/Baze",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:844,usability,input,input-gcsfused-,844,"I ran using the reference genome mentioned earlier, but it seems that the reference has the chromosomes named without the ""chr"" prefix, which the BAM file has for the contig names, it would be nice if the package could detect and correct this automatically -- do you think this makes sense, if so I am happy to submit a patch:. ```. 2018-11-10 01:31:37.340729: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1110 01:31:37.359072 140105147934464 genomics_reader.py:213] Reading /input-gcsfused-0/CNR-data/TLE_a_001.bam with NativeSamReader. I1110 01:31:37.383352 140105147934464 make_examples.py:1075] Preparing inputs. 2018-11-10 01:31:38.214366: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1110 01:31:38.223048 140105147934464 genomics_reader.py:213] Reading /input-gcsfused-0/CNR-data/TLE_a_001.bam with NativeSamReader. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1178, in main. make_examples_runner(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1076, in make_examples_runner. regions = processing_regions_from_options(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 990, in processing_regions_from_options. options.min_shared_contigs_basepairs). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 485, in _ensure_consistent_contigs. min_coverage_fraction). File ""/mnt/google/.google/tmp/Baze",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:2294,usability,input,input,2294,"ant/deepvariant/make_examples.py"", line 1178, in main. make_examples_runner(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1076, in make_examples_runner. regions = processing_regions_from_options(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 990, in processing_regions_from_options. options.min_shared_contigs_basepairs). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 485, in _ensure_consistent_contigs. min_coverage_fraction). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 557, in validate_reference_contig_coverage. ref_bp, common_bp, coverage, format_contig_matches())). ValueError: Reference contigs span 3095677412 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sources were created on a common genome reference build. Contig matches were: ""1"" is 249250621 bp and IS MISSING, ""2"" is 243199373 bp and IS MISSING, ""3"" is 198022430 bp and IS MISSING, ""4"" is 191154276 bp and IS MISSING, ""5"" is 180915260 bp and IS MISSING, ""6"" is 171115067 bp and IS MISSING, ""7"" is 159138663 bp and IS MISSING, ""8"" is 146364022 bp and IS MISSING, ""9"" is 141213431 bp and IS MISSING, ""10"" is 135534747 bp and IS MISSING, ""11"" is 135006516 bp and IS MISSING, ""12"" is 133851895 bp and IS MISSING, ""13"" is 115169878 bp and IS MISSING, ""14"" is 107349540 bp and IS MISSING, ""15"" is 102531392 bp and IS MISSING, ""16"" is 90354753 bp and IS MISSING, ""17"" is 81195210 bp and IS MISSING, ""18"" is 78077248 bp and IS MISSING, ""19"" is 59128983 bp and IS MISSING, ""20"" is 63025520 bp and IS MISSING, ""21"" is 48129895 bp and IS MISSING, ""22"" is 51304566 bp and IS MISSING, ""X"" is 155270560 bp and IS MISSING, ""Y"" is 59373566 bp and IS MISSING. parallel: This job failed:. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:3308,usability,input,input-gcsfused-,3308,"epvariant/make_examples.py"", line 485, in _ensure_consistent_contigs. min_coverage_fraction). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 557, in validate_reference_contig_coverage. ref_bp, common_bp, coverage, format_contig_matches())). ValueError: Reference contigs span 3095677412 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sources were created on a common genome reference build. Contig matches were: ""1"" is 249250621 bp and IS MISSING, ""2"" is 243199373 bp and IS MISSING, ""3"" is 198022430 bp and IS MISSING, ""4"" is 191154276 bp and IS MISSING, ""5"" is 180915260 bp and IS MISSING, ""6"" is 171115067 bp and IS MISSING, ""7"" is 159138663 bp and IS MISSING, ""8"" is 146364022 bp and IS MISSING, ""9"" is 141213431 bp and IS MISSING, ""10"" is 135534747 bp and IS MISSING, ""11"" is 135006516 bp and IS MISSING, ""12"" is 133851895 bp and IS MISSING, ""13"" is 115169878 bp and IS MISSING, ""14"" is 107349540 bp and IS MISSING, ""15"" is 102531392 bp and IS MISSING, ""16"" is 90354753 bp and IS MISSING, ""17"" is 81195210 bp and IS MISSING, ""18"" is 78077248 bp and IS MISSING, ""19"" is 59128983 bp and IS MISSING, ""20"" is 63025520 bp and IS MISSING, ""21"" is 48129895 bp and IS MISSING, ""22"" is 51304566 bp and IS MISSING, ""X"" is 155270560 bp and IS MISSING, ""Y"" is 59373566 bp and IS MISSING. parallel: This job failed:. mkdir -p ./input-gcsfused-0 && gcsfuse --implicit-dirs canis /input-gcsfused-0 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/canis/CNR-data/deep_variant_files/examples/0/examples_output.tfrecord@8.gz --reads /input-gcsfused-0/CNR-data/TLE_a_001.bam --ref /mnt/google/.google/input/deepvariant/performance-testdata/hs37d5.fa.gz --task 0 --regions gs://canis/CNR-data/exomes.bed. Using mount point: /input-gcsfused-0. Opening GCS connection... Opening bucket... Mounting file system... File system has been successfully mounted. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:3359,usability,input,input-gcsfused-,3359,"epvariant/make_examples.py"", line 485, in _ensure_consistent_contigs. min_coverage_fraction). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 557, in validate_reference_contig_coverage. ref_bp, common_bp, coverage, format_contig_matches())). ValueError: Reference contigs span 3095677412 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sources were created on a common genome reference build. Contig matches were: ""1"" is 249250621 bp and IS MISSING, ""2"" is 243199373 bp and IS MISSING, ""3"" is 198022430 bp and IS MISSING, ""4"" is 191154276 bp and IS MISSING, ""5"" is 180915260 bp and IS MISSING, ""6"" is 171115067 bp and IS MISSING, ""7"" is 159138663 bp and IS MISSING, ""8"" is 146364022 bp and IS MISSING, ""9"" is 141213431 bp and IS MISSING, ""10"" is 135534747 bp and IS MISSING, ""11"" is 135006516 bp and IS MISSING, ""12"" is 133851895 bp and IS MISSING, ""13"" is 115169878 bp and IS MISSING, ""14"" is 107349540 bp and IS MISSING, ""15"" is 102531392 bp and IS MISSING, ""16"" is 90354753 bp and IS MISSING, ""17"" is 81195210 bp and IS MISSING, ""18"" is 78077248 bp and IS MISSING, ""19"" is 59128983 bp and IS MISSING, ""20"" is 63025520 bp and IS MISSING, ""21"" is 48129895 bp and IS MISSING, ""22"" is 51304566 bp and IS MISSING, ""X"" is 155270560 bp and IS MISSING, ""Y"" is 59373566 bp and IS MISSING. parallel: This job failed:. mkdir -p ./input-gcsfused-0 && gcsfuse --implicit-dirs canis /input-gcsfused-0 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/canis/CNR-data/deep_variant_files/examples/0/examples_output.tfrecord@8.gz --reads /input-gcsfused-0/CNR-data/TLE_a_001.bam --ref /mnt/google/.google/input/deepvariant/performance-testdata/hs37d5.fa.gz --task 0 --regions gs://canis/CNR-data/exomes.bed. Using mount point: /input-gcsfused-0. Opening GCS connection... Opening bucket... Mounting file system... File system has been successfully mounted. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:3551,usability,input,input-gcsfused-,3551,"epvariant/make_examples.py"", line 485, in _ensure_consistent_contigs. min_coverage_fraction). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 557, in validate_reference_contig_coverage. ref_bp, common_bp, coverage, format_contig_matches())). ValueError: Reference contigs span 3095677412 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sources were created on a common genome reference build. Contig matches were: ""1"" is 249250621 bp and IS MISSING, ""2"" is 243199373 bp and IS MISSING, ""3"" is 198022430 bp and IS MISSING, ""4"" is 191154276 bp and IS MISSING, ""5"" is 180915260 bp and IS MISSING, ""6"" is 171115067 bp and IS MISSING, ""7"" is 159138663 bp and IS MISSING, ""8"" is 146364022 bp and IS MISSING, ""9"" is 141213431 bp and IS MISSING, ""10"" is 135534747 bp and IS MISSING, ""11"" is 135006516 bp and IS MISSING, ""12"" is 133851895 bp and IS MISSING, ""13"" is 115169878 bp and IS MISSING, ""14"" is 107349540 bp and IS MISSING, ""15"" is 102531392 bp and IS MISSING, ""16"" is 90354753 bp and IS MISSING, ""17"" is 81195210 bp and IS MISSING, ""18"" is 78077248 bp and IS MISSING, ""19"" is 59128983 bp and IS MISSING, ""20"" is 63025520 bp and IS MISSING, ""21"" is 48129895 bp and IS MISSING, ""22"" is 51304566 bp and IS MISSING, ""X"" is 155270560 bp and IS MISSING, ""Y"" is 59373566 bp and IS MISSING. parallel: This job failed:. mkdir -p ./input-gcsfused-0 && gcsfuse --implicit-dirs canis /input-gcsfused-0 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/canis/CNR-data/deep_variant_files/examples/0/examples_output.tfrecord@8.gz --reads /input-gcsfused-0/CNR-data/TLE_a_001.bam --ref /mnt/google/.google/input/deepvariant/performance-testdata/hs37d5.fa.gz --task 0 --regions gs://canis/CNR-data/exomes.bed. Using mount point: /input-gcsfused-0. Opening GCS connection... Opening bucket... Mounting file system... File system has been successfully mounted. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:3617,usability,input,input,3617,"epvariant/make_examples.py"", line 485, in _ensure_consistent_contigs. min_coverage_fraction). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 557, in validate_reference_contig_coverage. ref_bp, common_bp, coverage, format_contig_matches())). ValueError: Reference contigs span 3095677412 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sources were created on a common genome reference build. Contig matches were: ""1"" is 249250621 bp and IS MISSING, ""2"" is 243199373 bp and IS MISSING, ""3"" is 198022430 bp and IS MISSING, ""4"" is 191154276 bp and IS MISSING, ""5"" is 180915260 bp and IS MISSING, ""6"" is 171115067 bp and IS MISSING, ""7"" is 159138663 bp and IS MISSING, ""8"" is 146364022 bp and IS MISSING, ""9"" is 141213431 bp and IS MISSING, ""10"" is 135534747 bp and IS MISSING, ""11"" is 135006516 bp and IS MISSING, ""12"" is 133851895 bp and IS MISSING, ""13"" is 115169878 bp and IS MISSING, ""14"" is 107349540 bp and IS MISSING, ""15"" is 102531392 bp and IS MISSING, ""16"" is 90354753 bp and IS MISSING, ""17"" is 81195210 bp and IS MISSING, ""18"" is 78077248 bp and IS MISSING, ""19"" is 59128983 bp and IS MISSING, ""20"" is 63025520 bp and IS MISSING, ""21"" is 48129895 bp and IS MISSING, ""22"" is 51304566 bp and IS MISSING, ""X"" is 155270560 bp and IS MISSING, ""Y"" is 59373566 bp and IS MISSING. parallel: This job failed:. mkdir -p ./input-gcsfused-0 && gcsfuse --implicit-dirs canis /input-gcsfused-0 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/canis/CNR-data/deep_variant_files/examples/0/examples_output.tfrecord@8.gz --reads /input-gcsfused-0/CNR-data/TLE_a_001.bam --ref /mnt/google/.google/input/deepvariant/performance-testdata/hs37d5.fa.gz --task 0 --regions gs://canis/CNR-data/exomes.bed. Using mount point: /input-gcsfused-0. Opening GCS connection... Opening bucket... Mounting file system... File system has been successfully mounted. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:3635,usability,perform,performance-testdata,3635,"epvariant/make_examples.py"", line 485, in _ensure_consistent_contigs. min_coverage_fraction). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 557, in validate_reference_contig_coverage. ref_bp, common_bp, coverage, format_contig_matches())). ValueError: Reference contigs span 3095677412 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sources were created on a common genome reference build. Contig matches were: ""1"" is 249250621 bp and IS MISSING, ""2"" is 243199373 bp and IS MISSING, ""3"" is 198022430 bp and IS MISSING, ""4"" is 191154276 bp and IS MISSING, ""5"" is 180915260 bp and IS MISSING, ""6"" is 171115067 bp and IS MISSING, ""7"" is 159138663 bp and IS MISSING, ""8"" is 146364022 bp and IS MISSING, ""9"" is 141213431 bp and IS MISSING, ""10"" is 135534747 bp and IS MISSING, ""11"" is 135006516 bp and IS MISSING, ""12"" is 133851895 bp and IS MISSING, ""13"" is 115169878 bp and IS MISSING, ""14"" is 107349540 bp and IS MISSING, ""15"" is 102531392 bp and IS MISSING, ""16"" is 90354753 bp and IS MISSING, ""17"" is 81195210 bp and IS MISSING, ""18"" is 78077248 bp and IS MISSING, ""19"" is 59128983 bp and IS MISSING, ""20"" is 63025520 bp and IS MISSING, ""21"" is 48129895 bp and IS MISSING, ""22"" is 51304566 bp and IS MISSING, ""X"" is 155270560 bp and IS MISSING, ""Y"" is 59373566 bp and IS MISSING. parallel: This job failed:. mkdir -p ./input-gcsfused-0 && gcsfuse --implicit-dirs canis /input-gcsfused-0 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/canis/CNR-data/deep_variant_files/examples/0/examples_output.tfrecord@8.gz --reads /input-gcsfused-0/CNR-data/TLE_a_001.bam --ref /mnt/google/.google/input/deepvariant/performance-testdata/hs37d5.fa.gz --task 0 --regions gs://canis/CNR-data/exomes.bed. Using mount point: /input-gcsfused-0. Opening GCS connection... Opening bucket... Mounting file system... File system has been successfully mounted. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:3740,usability,input,input-gcsfused-,3740,"epvariant/make_examples.py"", line 485, in _ensure_consistent_contigs. min_coverage_fraction). File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 557, in validate_reference_contig_coverage. ref_bp, common_bp, coverage, format_contig_matches())). ValueError: Reference contigs span 3095677412 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sources were created on a common genome reference build. Contig matches were: ""1"" is 249250621 bp and IS MISSING, ""2"" is 243199373 bp and IS MISSING, ""3"" is 198022430 bp and IS MISSING, ""4"" is 191154276 bp and IS MISSING, ""5"" is 180915260 bp and IS MISSING, ""6"" is 171115067 bp and IS MISSING, ""7"" is 159138663 bp and IS MISSING, ""8"" is 146364022 bp and IS MISSING, ""9"" is 141213431 bp and IS MISSING, ""10"" is 135534747 bp and IS MISSING, ""11"" is 135006516 bp and IS MISSING, ""12"" is 133851895 bp and IS MISSING, ""13"" is 115169878 bp and IS MISSING, ""14"" is 107349540 bp and IS MISSING, ""15"" is 102531392 bp and IS MISSING, ""16"" is 90354753 bp and IS MISSING, ""17"" is 81195210 bp and IS MISSING, ""18"" is 78077248 bp and IS MISSING, ""19"" is 59128983 bp and IS MISSING, ""20"" is 63025520 bp and IS MISSING, ""21"" is 48129895 bp and IS MISSING, ""22"" is 51304566 bp and IS MISSING, ""X"" is 155270560 bp and IS MISSING, ""Y"" is 59373566 bp and IS MISSING. parallel: This job failed:. mkdir -p ./input-gcsfused-0 && gcsfuse --implicit-dirs canis /input-gcsfused-0 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/canis/CNR-data/deep_variant_files/examples/0/examples_output.tfrecord@8.gz --reads /input-gcsfused-0/CNR-data/TLE_a_001.bam --ref /mnt/google/.google/input/deepvariant/performance-testdata/hs37d5.fa.gz --task 0 --regions gs://canis/CNR-data/exomes.bed. Using mount point: /input-gcsfused-0. Opening GCS connection... Opening bucket... Mounting file system... File system has been successfully mounted. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:68,deployability,fail,fails,68,"I was able to re-header the BAM file using samtools, now the script fails at a new step: reading the BED file. I am not sure what the problem is here:. ```. Using mount point: /input-gcsfused-0. Opening GCS connection... Opening bucket... Mounting file system... File system has been successfully mounted. 2018-11-11 17:32:17.333150: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1111 17:32:17.342442 140289241822976 genomics_reader.py:213] Reading /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam with NativeSamReader. I1111 17:32:17.361879 140289241822976 make_examples.py:1075] Preparing inputs. 2018-11-11 17:32:18.174362: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1111 17:32:18.184726 140289241822976 genomics_reader.py:213] Reading /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam with NativeSamReader. I1111 17:32:18.227871 140289241822976 make_examples.py:991] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']. [E::hts_open_format] Failed to open file gs://canis/CNR-data/exomes.bed. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1178, in main. make_examples_runner(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1076, in make_examples_runner. regions = processing_regions_from_options(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/deepvariant/make_examples.py",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:1156,deployability,Fail,Failed,1156,"ing mount point: /input-gcsfused-0. Opening GCS connection... Opening bucket... Mounting file system... File system has been successfully mounted. 2018-11-11 17:32:17.333150: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1111 17:32:17.342442 140289241822976 genomics_reader.py:213] Reading /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam with NativeSamReader. I1111 17:32:17.361879 140289241822976 make_examples.py:1075] Preparing inputs. 2018-11-11 17:32:18.174362: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1111 17:32:18.184726 140289241822976 genomics_reader.py:213] Reading /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam with NativeSamReader. I1111 17:32:18.227871 140289241822976 make_examples.py:991] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']. [E::hts_open_format] Failed to open file gs://canis/CNR-data/exomes.bed. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1178, in main. make_examples_runner(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1076, in make_examples_runner. regions = processing_regions_from_options(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 993, in processing_regions_from_options. options.exclude_calling_regions). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deep",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:1374,deployability,modul,module,1374," Unrecognized SAM header type, ignoring: . I1111 17:32:17.342442 140289241822976 genomics_reader.py:213] Reading /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam with NativeSamReader. I1111 17:32:17.361879 140289241822976 make_examples.py:1075] Preparing inputs. 2018-11-11 17:32:18.174362: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1111 17:32:18.184726 140289241822976 genomics_reader.py:213] Reading /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam with NativeSamReader. I1111 17:32:18.227871 140289241822976 make_examples.py:991] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']. [E::hts_open_format] Failed to open file gs://canis/CNR-data/exomes.bed. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1178, in main. make_examples_runner(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1076, in make_examples_runner. regions = processing_regions_from_options(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 993, in processing_regions_from_options. options.exclude_calling_regions). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 587, in build_calling_regions. ranges.RangeSet.from_regions(regions_to_include, contig_dict)). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvar",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:3746,deployability,fail,failed,3746,"ns_to_include, contig_dict)). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 161, in from_regions. return cls(ranges=from_regions(regions, contig_map=contig_map)). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 113, in __init__. for i, range_ in enumerate(ranges):. File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 464, in from_regions. for elt in reader(region):. File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 429, in bed_parser. with bed.BedReader(filename) as fin:. File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 211, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 127, in _native_reader. return NativeBedReader(input_path, **kwargs). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 104, in __init__. self._reader = bed_reader.BedReader.from_file(bed_path, options). ValueError: Not found: Could not open gs://canis/CNR-data/exomes.bed. parallel: This job failed:. mkdir -p ./input-gcsfused-0 && gcsfuse --implicit-dirs canis /input-gcsfused-0 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/canis/CNR-data/deep_variant_files/examples/0/examples_output.tfrecord@8.gz --reads /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam --ref /mnt/google/.google/input/deepvariant/performance-testdata/hs37d5.fa.gz --task 0 --regions gs://canis/CNR-data/exomes.bed. ```. I've added the BED file to the public bucket:. gs://public-debug/exomes.bed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:4223,integrability,pub,public,4223,"ns_to_include, contig_dict)). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 161, in from_regions. return cls(ranges=from_regions(regions, contig_map=contig_map)). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 113, in __init__. for i, range_ in enumerate(ranges):. File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 464, in from_regions. for elt in reader(region):. File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 429, in bed_parser. with bed.BedReader(filename) as fin:. File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 211, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 127, in _native_reader. return NativeBedReader(input_path, **kwargs). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 104, in __init__. self._reader = bed_reader.BedReader.from_file(bed_path, options). ValueError: Not found: Could not open gs://canis/CNR-data/exomes.bed. parallel: This job failed:. mkdir -p ./input-gcsfused-0 && gcsfuse --implicit-dirs canis /input-gcsfused-0 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/canis/CNR-data/deep_variant_files/examples/0/examples_output.tfrecord@8.gz --reads /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam --ref /mnt/google/.google/input/deepvariant/performance-testdata/hs37d5.fa.gz --task 0 --regions gs://canis/CNR-data/exomes.bed. ```. I've added the BED file to the public bucket:. gs://public-debug/exomes.bed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:4244,integrability,pub,public-debug,4244,"ns_to_include, contig_dict)). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 161, in from_regions. return cls(ranges=from_regions(regions, contig_map=contig_map)). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 113, in __init__. for i, range_ in enumerate(ranges):. File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 464, in from_regions. for elt in reader(region):. File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 429, in bed_parser. with bed.BedReader(filename) as fin:. File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 211, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 127, in _native_reader. return NativeBedReader(input_path, **kwargs). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 104, in __init__. self._reader = bed_reader.BedReader.from_file(bed_path, options). ValueError: Not found: Could not open gs://canis/CNR-data/exomes.bed. parallel: This job failed:. mkdir -p ./input-gcsfused-0 && gcsfuse --implicit-dirs canis /input-gcsfused-0 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/canis/CNR-data/deep_variant_files/examples/0/examples_output.tfrecord@8.gz --reads /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam --ref /mnt/google/.google/input/deepvariant/performance-testdata/hs37d5.fa.gz --task 0 --regions gs://canis/CNR-data/exomes.bed. ```. I've added the BED file to the public bucket:. gs://public-debug/exomes.bed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:1460,interoperability,platform,platform,1460,"cs_reader.py:213] Reading /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam with NativeSamReader. I1111 17:32:17.361879 140289241822976 make_examples.py:1075] Preparing inputs. 2018-11-11 17:32:18.174362: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1111 17:32:18.184726 140289241822976 genomics_reader.py:213] Reading /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam with NativeSamReader. I1111 17:32:18.227871 140289241822976 make_examples.py:991] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']. [E::hts_open_format] Failed to open file gs://canis/CNR-data/exomes.bed. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1178, in main. make_examples_runner(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1076, in make_examples_runner. regions = processing_regions_from_options(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 993, in processing_regions_from_options. options.exclude_calling_regions). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 587, in build_calling_regions. ranges.RangeSet.from_regions(regions_to_include, contig_dict)). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 161, in from_regions. return cls(ranges=",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:1374,modifiability,modul,module,1374," Unrecognized SAM header type, ignoring: . I1111 17:32:17.342442 140289241822976 genomics_reader.py:213] Reading /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam with NativeSamReader. I1111 17:32:17.361879 140289241822976 make_examples.py:1075] Preparing inputs. 2018-11-11 17:32:18.174362: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1111 17:32:18.184726 140289241822976 genomics_reader.py:213] Reading /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam with NativeSamReader. I1111 17:32:18.227871 140289241822976 make_examples.py:991] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']. [E::hts_open_format] Failed to open file gs://canis/CNR-data/exomes.bed. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1178, in main. make_examples_runner(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1076, in make_examples_runner. regions = processing_regions_from_options(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 993, in processing_regions_from_options. options.exclude_calling_regions). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 587, in build_calling_regions. ranges.RangeSet.from_regions(regions_to_include, contig_dict)). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvar",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:1433,modifiability,pac,packages,1433,"2442 140289241822976 genomics_reader.py:213] Reading /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam with NativeSamReader. I1111 17:32:17.361879 140289241822976 make_examples.py:1075] Preparing inputs. 2018-11-11 17:32:18.174362: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1111 17:32:18.184726 140289241822976 genomics_reader.py:213] Reading /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam with NativeSamReader. I1111 17:32:18.227871 140289241822976 make_examples.py:991] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']. [E::hts_open_format] Failed to open file gs://canis/CNR-data/exomes.bed. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1178, in main. make_examples_runner(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1076, in make_examples_runner. regions = processing_regions_from_options(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 993, in processing_regions_from_options. options.exclude_calling_regions). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 587, in build_calling_regions. ranges.RangeSet.from_regions(regions_to_include, contig_dict)). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 161, in from_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:3727,performance,parallel,parallel,3727,"ns_to_include, contig_dict)). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 161, in from_regions. return cls(ranges=from_regions(regions, contig_map=contig_map)). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 113, in __init__. for i, range_ in enumerate(ranges):. File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 464, in from_regions. for elt in reader(region):. File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 429, in bed_parser. with bed.BedReader(filename) as fin:. File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 211, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 127, in _native_reader. return NativeBedReader(input_path, **kwargs). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 104, in __init__. self._reader = bed_reader.BedReader.from_file(bed_path, options). ValueError: Not found: Could not open gs://canis/CNR-data/exomes.bed. parallel: This job failed:. mkdir -p ./input-gcsfused-0 && gcsfuse --implicit-dirs canis /input-gcsfused-0 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/canis/CNR-data/deep_variant_files/examples/0/examples_output.tfrecord@8.gz --reads /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam --ref /mnt/google/.google/input/deepvariant/performance-testdata/hs37d5.fa.gz --task 0 --regions gs://canis/CNR-data/exomes.bed. ```. I've added the BED file to the public bucket:. gs://public-debug/exomes.bed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:4102,performance,perform,performance-testdata,4102,"ns_to_include, contig_dict)). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 161, in from_regions. return cls(ranges=from_regions(regions, contig_map=contig_map)). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 113, in __init__. for i, range_ in enumerate(ranges):. File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 464, in from_regions. for elt in reader(region):. File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 429, in bed_parser. with bed.BedReader(filename) as fin:. File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 211, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 127, in _native_reader. return NativeBedReader(input_path, **kwargs). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 104, in __init__. self._reader = bed_reader.BedReader.from_file(bed_path, options). ValueError: Not found: Could not open gs://canis/CNR-data/exomes.bed. parallel: This job failed:. mkdir -p ./input-gcsfused-0 && gcsfuse --implicit-dirs canis /input-gcsfused-0 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/canis/CNR-data/deep_variant_files/examples/0/examples_output.tfrecord@8.gz --reads /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam --ref /mnt/google/.google/input/deepvariant/performance-testdata/hs37d5.fa.gz --task 0 --regions gs://canis/CNR-data/exomes.bed. ```. I've added the BED file to the public bucket:. gs://public-debug/exomes.bed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:68,reliability,fail,fails,68,"I was able to re-header the BAM file using samtools, now the script fails at a new step: reading the BED file. I am not sure what the problem is here:. ```. Using mount point: /input-gcsfused-0. Opening GCS connection... Opening bucket... Mounting file system... File system has been successfully mounted. 2018-11-11 17:32:17.333150: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1111 17:32:17.342442 140289241822976 genomics_reader.py:213] Reading /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam with NativeSamReader. I1111 17:32:17.361879 140289241822976 make_examples.py:1075] Preparing inputs. 2018-11-11 17:32:18.174362: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1111 17:32:18.184726 140289241822976 genomics_reader.py:213] Reading /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam with NativeSamReader. I1111 17:32:18.227871 140289241822976 make_examples.py:991] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']. [E::hts_open_format] Failed to open file gs://canis/CNR-data/exomes.bed. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1178, in main. make_examples_runner(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1076, in make_examples_runner. regions = processing_regions_from_options(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/deepvariant/make_examples.py",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:1156,reliability,Fail,Failed,1156,"ing mount point: /input-gcsfused-0. Opening GCS connection... Opening bucket... Mounting file system... File system has been successfully mounted. 2018-11-11 17:32:17.333150: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1111 17:32:17.342442 140289241822976 genomics_reader.py:213] Reading /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam with NativeSamReader. I1111 17:32:17.361879 140289241822976 make_examples.py:1075] Preparing inputs. 2018-11-11 17:32:18.174362: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1111 17:32:18.184726 140289241822976 genomics_reader.py:213] Reading /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam with NativeSamReader. I1111 17:32:18.227871 140289241822976 make_examples.py:991] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']. [E::hts_open_format] Failed to open file gs://canis/CNR-data/exomes.bed. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1178, in main. make_examples_runner(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1076, in make_examples_runner. regions = processing_regions_from_options(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 993, in processing_regions_from_options. options.exclude_calling_regions). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deep",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:3746,reliability,fail,failed,3746,"ns_to_include, contig_dict)). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 161, in from_regions. return cls(ranges=from_regions(regions, contig_map=contig_map)). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 113, in __init__. for i, range_ in enumerate(ranges):. File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 464, in from_regions. for elt in reader(region):. File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 429, in bed_parser. with bed.BedReader(filename) as fin:. File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 211, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 127, in _native_reader. return NativeBedReader(input_path, **kwargs). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 104, in __init__. self._reader = bed_reader.BedReader.from_file(bed_path, options). ValueError: Not found: Could not open gs://canis/CNR-data/exomes.bed. parallel: This job failed:. mkdir -p ./input-gcsfused-0 && gcsfuse --implicit-dirs canis /input-gcsfused-0 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/canis/CNR-data/deep_variant_files/examples/0/examples_output.tfrecord@8.gz --reads /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam --ref /mnt/google/.google/input/deepvariant/performance-testdata/hs37d5.fa.gz --task 0 --regions gs://canis/CNR-data/exomes.bed. ```. I've added the BED file to the public bucket:. gs://public-debug/exomes.bed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:177,safety,input,input-gcsfused-,177,"I was able to re-header the BAM file using samtools, now the script fails at a new step: reading the BED file. I am not sure what the problem is here:. ```. Using mount point: /input-gcsfused-0. Opening GCS connection... Opening bucket... Mounting file system... File system has been successfully mounted. 2018-11-11 17:32:17.333150: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1111 17:32:17.342442 140289241822976 genomics_reader.py:213] Reading /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam with NativeSamReader. I1111 17:32:17.361879 140289241822976 make_examples.py:1075] Preparing inputs. 2018-11-11 17:32:18.174362: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1111 17:32:18.184726 140289241822976 genomics_reader.py:213] Reading /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam with NativeSamReader. I1111 17:32:18.227871 140289241822976 make_examples.py:991] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']. [E::hts_open_format] Failed to open file gs://canis/CNR-data/exomes.bed. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1178, in main. make_examples_runner(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1076, in make_examples_runner. regions = processing_regions_from_options(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/deepvariant/make_examples.py",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:491,safety,input,input-gcsfused-,491,"I was able to re-header the BAM file using samtools, now the script fails at a new step: reading the BED file. I am not sure what the problem is here:. ```. Using mount point: /input-gcsfused-0. Opening GCS connection... Opening bucket... Mounting file system... File system has been successfully mounted. 2018-11-11 17:32:17.333150: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1111 17:32:17.342442 140289241822976 genomics_reader.py:213] Reading /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam with NativeSamReader. I1111 17:32:17.361879 140289241822976 make_examples.py:1075] Preparing inputs. 2018-11-11 17:32:18.174362: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1111 17:32:18.184726 140289241822976 genomics_reader.py:213] Reading /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam with NativeSamReader. I1111 17:32:18.227871 140289241822976 make_examples.py:991] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']. [E::hts_open_format] Failed to open file gs://canis/CNR-data/exomes.bed. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1178, in main. make_examples_runner(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1076, in make_examples_runner. regions = processing_regions_from_options(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/deepvariant/make_examples.py",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:633,safety,input,inputs,633,"I was able to re-header the BAM file using samtools, now the script fails at a new step: reading the BED file. I am not sure what the problem is here:. ```. Using mount point: /input-gcsfused-0. Opening GCS connection... Opening bucket... Mounting file system... File system has been successfully mounted. 2018-11-11 17:32:17.333150: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1111 17:32:17.342442 140289241822976 genomics_reader.py:213] Reading /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam with NativeSamReader. I1111 17:32:17.361879 140289241822976 make_examples.py:1075] Preparing inputs. 2018-11-11 17:32:18.174362: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1111 17:32:18.184726 140289241822976 genomics_reader.py:213] Reading /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam with NativeSamReader. I1111 17:32:18.227871 140289241822976 make_examples.py:991] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']. [E::hts_open_format] Failed to open file gs://canis/CNR-data/exomes.bed. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1178, in main. make_examples_runner(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1076, in make_examples_runner. regions = processing_regions_from_options(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/deepvariant/make_examples.py",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:826,safety,input,input-gcsfused-,826,"I was able to re-header the BAM file using samtools, now the script fails at a new step: reading the BED file. I am not sure what the problem is here:. ```. Using mount point: /input-gcsfused-0. Opening GCS connection... Opening bucket... Mounting file system... File system has been successfully mounted. 2018-11-11 17:32:17.333150: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1111 17:32:17.342442 140289241822976 genomics_reader.py:213] Reading /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam with NativeSamReader. I1111 17:32:17.361879 140289241822976 make_examples.py:1075] Preparing inputs. 2018-11-11 17:32:18.174362: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1111 17:32:18.184726 140289241822976 genomics_reader.py:213] Reading /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam with NativeSamReader. I1111 17:32:18.227871 140289241822976 make_examples.py:991] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']. [E::hts_open_format] Failed to open file gs://canis/CNR-data/exomes.bed. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1178, in main. make_examples_runner(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1076, in make_examples_runner. regions = processing_regions_from_options(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/deepvariant/make_examples.py",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:1374,safety,modul,module,1374," Unrecognized SAM header type, ignoring: . I1111 17:32:17.342442 140289241822976 genomics_reader.py:213] Reading /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam with NativeSamReader. I1111 17:32:17.361879 140289241822976 make_examples.py:1075] Preparing inputs. 2018-11-11 17:32:18.174362: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1111 17:32:18.184726 140289241822976 genomics_reader.py:213] Reading /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam with NativeSamReader. I1111 17:32:18.227871 140289241822976 make_examples.py:991] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']. [E::hts_open_format] Failed to open file gs://canis/CNR-data/exomes.bed. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1178, in main. make_examples_runner(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1076, in make_examples_runner. regions = processing_regions_from_options(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 993, in processing_regions_from_options. options.exclude_calling_regions). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 587, in build_calling_regions. ranges.RangeSet.from_regions(regions_to_include, contig_dict)). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvar",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:3766,safety,input,input-gcsfused-,3766,"ns_to_include, contig_dict)). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 161, in from_regions. return cls(ranges=from_regions(regions, contig_map=contig_map)). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 113, in __init__. for i, range_ in enumerate(ranges):. File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 464, in from_regions. for elt in reader(region):. File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 429, in bed_parser. with bed.BedReader(filename) as fin:. File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 211, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 127, in _native_reader. return NativeBedReader(input_path, **kwargs). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 104, in __init__. self._reader = bed_reader.BedReader.from_file(bed_path, options). ValueError: Not found: Could not open gs://canis/CNR-data/exomes.bed. parallel: This job failed:. mkdir -p ./input-gcsfused-0 && gcsfuse --implicit-dirs canis /input-gcsfused-0 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/canis/CNR-data/deep_variant_files/examples/0/examples_output.tfrecord@8.gz --reads /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam --ref /mnt/google/.google/input/deepvariant/performance-testdata/hs37d5.fa.gz --task 0 --regions gs://canis/CNR-data/exomes.bed. ```. I've added the BED file to the public bucket:. gs://public-debug/exomes.bed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:3817,safety,input,input-gcsfused-,3817,"ns_to_include, contig_dict)). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 161, in from_regions. return cls(ranges=from_regions(regions, contig_map=contig_map)). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 113, in __init__. for i, range_ in enumerate(ranges):. File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 464, in from_regions. for elt in reader(region):. File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 429, in bed_parser. with bed.BedReader(filename) as fin:. File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 211, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 127, in _native_reader. return NativeBedReader(input_path, **kwargs). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 104, in __init__. self._reader = bed_reader.BedReader.from_file(bed_path, options). ValueError: Not found: Could not open gs://canis/CNR-data/exomes.bed. parallel: This job failed:. mkdir -p ./input-gcsfused-0 && gcsfuse --implicit-dirs canis /input-gcsfused-0 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/canis/CNR-data/deep_variant_files/examples/0/examples_output.tfrecord@8.gz --reads /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam --ref /mnt/google/.google/input/deepvariant/performance-testdata/hs37d5.fa.gz --task 0 --regions gs://canis/CNR-data/exomes.bed. ```. I've added the BED file to the public bucket:. gs://public-debug/exomes.bed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:4009,safety,input,input-gcsfused-,4009,"ns_to_include, contig_dict)). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 161, in from_regions. return cls(ranges=from_regions(regions, contig_map=contig_map)). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 113, in __init__. for i, range_ in enumerate(ranges):. File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 464, in from_regions. for elt in reader(region):. File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 429, in bed_parser. with bed.BedReader(filename) as fin:. File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 211, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 127, in _native_reader. return NativeBedReader(input_path, **kwargs). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 104, in __init__. self._reader = bed_reader.BedReader.from_file(bed_path, options). ValueError: Not found: Could not open gs://canis/CNR-data/exomes.bed. parallel: This job failed:. mkdir -p ./input-gcsfused-0 && gcsfuse --implicit-dirs canis /input-gcsfused-0 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/canis/CNR-data/deep_variant_files/examples/0/examples_output.tfrecord@8.gz --reads /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam --ref /mnt/google/.google/input/deepvariant/performance-testdata/hs37d5.fa.gz --task 0 --regions gs://canis/CNR-data/exomes.bed. ```. I've added the BED file to the public bucket:. gs://public-debug/exomes.bed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:4084,safety,input,input,4084,"ns_to_include, contig_dict)). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 161, in from_regions. return cls(ranges=from_regions(regions, contig_map=contig_map)). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 113, in __init__. for i, range_ in enumerate(ranges):. File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 464, in from_regions. for elt in reader(region):. File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 429, in bed_parser. with bed.BedReader(filename) as fin:. File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 211, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 127, in _native_reader. return NativeBedReader(input_path, **kwargs). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 104, in __init__. self._reader = bed_reader.BedReader.from_file(bed_path, options). ValueError: Not found: Could not open gs://canis/CNR-data/exomes.bed. parallel: This job failed:. mkdir -p ./input-gcsfused-0 && gcsfuse --implicit-dirs canis /input-gcsfused-0 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/canis/CNR-data/deep_variant_files/examples/0/examples_output.tfrecord@8.gz --reads /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam --ref /mnt/google/.google/input/deepvariant/performance-testdata/hs37d5.fa.gz --task 0 --regions gs://canis/CNR-data/exomes.bed. ```. I've added the BED file to the public bucket:. gs://public-debug/exomes.bed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:4114,safety,test,testdata,4114,"ns_to_include, contig_dict)). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 161, in from_regions. return cls(ranges=from_regions(regions, contig_map=contig_map)). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 113, in __init__. for i, range_ in enumerate(ranges):. File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 464, in from_regions. for elt in reader(region):. File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 429, in bed_parser. with bed.BedReader(filename) as fin:. File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 211, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 127, in _native_reader. return NativeBedReader(input_path, **kwargs). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 104, in __init__. self._reader = bed_reader.BedReader.from_file(bed_path, options). ValueError: Not found: Could not open gs://canis/CNR-data/exomes.bed. parallel: This job failed:. mkdir -p ./input-gcsfused-0 && gcsfuse --implicit-dirs canis /input-gcsfused-0 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/canis/CNR-data/deep_variant_files/examples/0/examples_output.tfrecord@8.gz --reads /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam --ref /mnt/google/.google/input/deepvariant/performance-testdata/hs37d5.fa.gz --task 0 --regions gs://canis/CNR-data/exomes.bed. ```. I've added the BED file to the public bucket:. gs://public-debug/exomes.bed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:1208,testability,Trace,Traceback,1208,"tion... Opening bucket... Mounting file system... File system has been successfully mounted. 2018-11-11 17:32:17.333150: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1111 17:32:17.342442 140289241822976 genomics_reader.py:213] Reading /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam with NativeSamReader. I1111 17:32:17.361879 140289241822976 make_examples.py:1075] Preparing inputs. 2018-11-11 17:32:18.174362: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1111 17:32:18.184726 140289241822976 genomics_reader.py:213] Reading /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam with NativeSamReader. I1111 17:32:18.227871 140289241822976 make_examples.py:991] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']. [E::hts_open_format] Failed to open file gs://canis/CNR-data/exomes.bed. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1178, in main. make_examples_runner(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1076, in make_examples_runner. regions = processing_regions_from_options(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 993, in processing_regions_from_options. options.exclude_calling_regions). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 587, in bu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:4114,testability,test,testdata,4114,"ns_to_include, contig_dict)). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 161, in from_regions. return cls(ranges=from_regions(regions, contig_map=contig_map)). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 113, in __init__. for i, range_ in enumerate(ranges):. File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 464, in from_regions. for elt in reader(region):. File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 429, in bed_parser. with bed.BedReader(filename) as fin:. File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 211, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 127, in _native_reader. return NativeBedReader(input_path, **kwargs). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 104, in __init__. self._reader = bed_reader.BedReader.from_file(bed_path, options). ValueError: Not found: Could not open gs://canis/CNR-data/exomes.bed. parallel: This job failed:. mkdir -p ./input-gcsfused-0 && gcsfuse --implicit-dirs canis /input-gcsfused-0 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/canis/CNR-data/deep_variant_files/examples/0/examples_output.tfrecord@8.gz --reads /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam --ref /mnt/google/.google/input/deepvariant/performance-testdata/hs37d5.fa.gz --task 0 --regions gs://canis/CNR-data/exomes.bed. ```. I've added the BED file to the public bucket:. gs://public-debug/exomes.bed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:177,usability,input,input-gcsfused-,177,"I was able to re-header the BAM file using samtools, now the script fails at a new step: reading the BED file. I am not sure what the problem is here:. ```. Using mount point: /input-gcsfused-0. Opening GCS connection... Opening bucket... Mounting file system... File system has been successfully mounted. 2018-11-11 17:32:17.333150: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1111 17:32:17.342442 140289241822976 genomics_reader.py:213] Reading /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam with NativeSamReader. I1111 17:32:17.361879 140289241822976 make_examples.py:1075] Preparing inputs. 2018-11-11 17:32:18.174362: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1111 17:32:18.184726 140289241822976 genomics_reader.py:213] Reading /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam with NativeSamReader. I1111 17:32:18.227871 140289241822976 make_examples.py:991] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']. [E::hts_open_format] Failed to open file gs://canis/CNR-data/exomes.bed. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1178, in main. make_examples_runner(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1076, in make_examples_runner. regions = processing_regions_from_options(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/deepvariant/make_examples.py",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:491,usability,input,input-gcsfused-,491,"I was able to re-header the BAM file using samtools, now the script fails at a new step: reading the BED file. I am not sure what the problem is here:. ```. Using mount point: /input-gcsfused-0. Opening GCS connection... Opening bucket... Mounting file system... File system has been successfully mounted. 2018-11-11 17:32:17.333150: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1111 17:32:17.342442 140289241822976 genomics_reader.py:213] Reading /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam with NativeSamReader. I1111 17:32:17.361879 140289241822976 make_examples.py:1075] Preparing inputs. 2018-11-11 17:32:18.174362: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1111 17:32:18.184726 140289241822976 genomics_reader.py:213] Reading /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam with NativeSamReader. I1111 17:32:18.227871 140289241822976 make_examples.py:991] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']. [E::hts_open_format] Failed to open file gs://canis/CNR-data/exomes.bed. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1178, in main. make_examples_runner(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1076, in make_examples_runner. regions = processing_regions_from_options(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/deepvariant/make_examples.py",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:633,usability,input,inputs,633,"I was able to re-header the BAM file using samtools, now the script fails at a new step: reading the BED file. I am not sure what the problem is here:. ```. Using mount point: /input-gcsfused-0. Opening GCS connection... Opening bucket... Mounting file system... File system has been successfully mounted. 2018-11-11 17:32:17.333150: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1111 17:32:17.342442 140289241822976 genomics_reader.py:213] Reading /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam with NativeSamReader. I1111 17:32:17.361879 140289241822976 make_examples.py:1075] Preparing inputs. 2018-11-11 17:32:18.174362: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1111 17:32:18.184726 140289241822976 genomics_reader.py:213] Reading /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam with NativeSamReader. I1111 17:32:18.227871 140289241822976 make_examples.py:991] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']. [E::hts_open_format] Failed to open file gs://canis/CNR-data/exomes.bed. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1178, in main. make_examples_runner(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1076, in make_examples_runner. regions = processing_regions_from_options(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/deepvariant/make_examples.py",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:826,usability,input,input-gcsfused-,826,"I was able to re-header the BAM file using samtools, now the script fails at a new step: reading the BED file. I am not sure what the problem is here:. ```. Using mount point: /input-gcsfused-0. Opening GCS connection... Opening bucket... Mounting file system... File system has been successfully mounted. 2018-11-11 17:32:17.333150: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1111 17:32:17.342442 140289241822976 genomics_reader.py:213] Reading /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam with NativeSamReader. I1111 17:32:17.361879 140289241822976 make_examples.py:1075] Preparing inputs. 2018-11-11 17:32:18.174362: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1111 17:32:18.184726 140289241822976 genomics_reader.py:213] Reading /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam with NativeSamReader. I1111 17:32:18.227871 140289241822976 make_examples.py:991] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']. [E::hts_open_format] Failed to open file gs://canis/CNR-data/exomes.bed. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1178, in main. make_examples_runner(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1076, in make_examples_runner. regions = processing_regions_from_options(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/deepvariant/make_examples.py",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:3766,usability,input,input-gcsfused-,3766,"ns_to_include, contig_dict)). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 161, in from_regions. return cls(ranges=from_regions(regions, contig_map=contig_map)). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 113, in __init__. for i, range_ in enumerate(ranges):. File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 464, in from_regions. for elt in reader(region):. File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 429, in bed_parser. with bed.BedReader(filename) as fin:. File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 211, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 127, in _native_reader. return NativeBedReader(input_path, **kwargs). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 104, in __init__. self._reader = bed_reader.BedReader.from_file(bed_path, options). ValueError: Not found: Could not open gs://canis/CNR-data/exomes.bed. parallel: This job failed:. mkdir -p ./input-gcsfused-0 && gcsfuse --implicit-dirs canis /input-gcsfused-0 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/canis/CNR-data/deep_variant_files/examples/0/examples_output.tfrecord@8.gz --reads /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam --ref /mnt/google/.google/input/deepvariant/performance-testdata/hs37d5.fa.gz --task 0 --regions gs://canis/CNR-data/exomes.bed. ```. I've added the BED file to the public bucket:. gs://public-debug/exomes.bed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:3817,usability,input,input-gcsfused-,3817,"ns_to_include, contig_dict)). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 161, in from_regions. return cls(ranges=from_regions(regions, contig_map=contig_map)). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 113, in __init__. for i, range_ in enumerate(ranges):. File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 464, in from_regions. for elt in reader(region):. File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 429, in bed_parser. with bed.BedReader(filename) as fin:. File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 211, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 127, in _native_reader. return NativeBedReader(input_path, **kwargs). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 104, in __init__. self._reader = bed_reader.BedReader.from_file(bed_path, options). ValueError: Not found: Could not open gs://canis/CNR-data/exomes.bed. parallel: This job failed:. mkdir -p ./input-gcsfused-0 && gcsfuse --implicit-dirs canis /input-gcsfused-0 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/canis/CNR-data/deep_variant_files/examples/0/examples_output.tfrecord@8.gz --reads /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam --ref /mnt/google/.google/input/deepvariant/performance-testdata/hs37d5.fa.gz --task 0 --regions gs://canis/CNR-data/exomes.bed. ```. I've added the BED file to the public bucket:. gs://public-debug/exomes.bed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:4009,usability,input,input-gcsfused-,4009,"ns_to_include, contig_dict)). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 161, in from_regions. return cls(ranges=from_regions(regions, contig_map=contig_map)). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 113, in __init__. for i, range_ in enumerate(ranges):. File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 464, in from_regions. for elt in reader(region):. File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 429, in bed_parser. with bed.BedReader(filename) as fin:. File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 211, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 127, in _native_reader. return NativeBedReader(input_path, **kwargs). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 104, in __init__. self._reader = bed_reader.BedReader.from_file(bed_path, options). ValueError: Not found: Could not open gs://canis/CNR-data/exomes.bed. parallel: This job failed:. mkdir -p ./input-gcsfused-0 && gcsfuse --implicit-dirs canis /input-gcsfused-0 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/canis/CNR-data/deep_variant_files/examples/0/examples_output.tfrecord@8.gz --reads /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam --ref /mnt/google/.google/input/deepvariant/performance-testdata/hs37d5.fa.gz --task 0 --regions gs://canis/CNR-data/exomes.bed. ```. I've added the BED file to the public bucket:. gs://public-debug/exomes.bed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:4084,usability,input,input,4084,"ns_to_include, contig_dict)). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 161, in from_regions. return cls(ranges=from_regions(regions, contig_map=contig_map)). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 113, in __init__. for i, range_ in enumerate(ranges):. File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 464, in from_regions. for elt in reader(region):. File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 429, in bed_parser. with bed.BedReader(filename) as fin:. File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 211, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 127, in _native_reader. return NativeBedReader(input_path, **kwargs). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 104, in __init__. self._reader = bed_reader.BedReader.from_file(bed_path, options). ValueError: Not found: Could not open gs://canis/CNR-data/exomes.bed. parallel: This job failed:. mkdir -p ./input-gcsfused-0 && gcsfuse --implicit-dirs canis /input-gcsfused-0 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/canis/CNR-data/deep_variant_files/examples/0/examples_output.tfrecord@8.gz --reads /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam --ref /mnt/google/.google/input/deepvariant/performance-testdata/hs37d5.fa.gz --task 0 --regions gs://canis/CNR-data/exomes.bed. ```. I've added the BED file to the public bucket:. gs://public-debug/exomes.bed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:4102,usability,perform,performance-testdata,4102,"ns_to_include, contig_dict)). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 161, in from_regions. return cls(ranges=from_regions(regions, contig_map=contig_map)). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 113, in __init__. for i, range_ in enumerate(ranges):. File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 464, in from_regions. for elt in reader(region):. File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 429, in bed_parser. with bed.BedReader(filename) as fin:. File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 211, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 127, in _native_reader. return NativeBedReader(input_path, **kwargs). File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 104, in __init__. self._reader = bed_reader.BedReader.from_file(bed_path, options). ValueError: Not found: Could not open gs://canis/CNR-data/exomes.bed. parallel: This job failed:. mkdir -p ./input-gcsfused-0 && gcsfuse --implicit-dirs canis /input-gcsfused-0 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/canis/CNR-data/deep_variant_files/examples/0/examples_output.tfrecord@8.gz --reads /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam --ref /mnt/google/.google/input/deepvariant/performance-testdata/hs37d5.fa.gz --task 0 --regions gs://canis/CNR-data/exomes.bed. ```. I've added the BED file to the public bucket:. gs://public-debug/exomes.bed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:427,deployability,log,log,427,"First of all, I assume that you have access to `gs://canis/CNR-data/exomes.bed` file? If you do:. `gsutil cat gs://canis/CNR-data/exomes.bed` from the project that has the same permission, you can first double check that you have access to it. And, I don't fully understand how gcsfuse is implemented in the Google Cloud runner, so I'll defer this to our collaborator at Cloud @nmousavi -- Nima, in the command in the previous log, particularly this one:. `mkdir -p ./input-gcsfused-0 && gcsfuse --implicit-dirs canis /input-gcsfused-0 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/canis/CNR-data/deep_variant_files/examples/0/examples_output.tfrecord@8.gz --reads /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam --ref /mnt/google/.google/input/deepvariant/performance-testdata/hs37d5.fa.gz --task 0 --regions gs://canis/CNR-data/exomes.bed`. It seems weird to me that everything else has been changed to under `/input-gcsfused-0` or `/mnt`, but the BED file still has a `gs://` prefix. This seems to me like something might be wrong in how the runner is using gcsfuse on the BED file. Can you take a look? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:315,energy efficiency,Cloud,Cloud,315,"First of all, I assume that you have access to `gs://canis/CNR-data/exomes.bed` file? If you do:. `gsutil cat gs://canis/CNR-data/exomes.bed` from the project that has the same permission, you can first double check that you have access to it. And, I don't fully understand how gcsfuse is implemented in the Google Cloud runner, so I'll defer this to our collaborator at Cloud @nmousavi -- Nima, in the command in the previous log, particularly this one:. `mkdir -p ./input-gcsfused-0 && gcsfuse --implicit-dirs canis /input-gcsfused-0 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/canis/CNR-data/deep_variant_files/examples/0/examples_output.tfrecord@8.gz --reads /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam --ref /mnt/google/.google/input/deepvariant/performance-testdata/hs37d5.fa.gz --task 0 --regions gs://canis/CNR-data/exomes.bed`. It seems weird to me that everything else has been changed to under `/input-gcsfused-0` or `/mnt`, but the BED file still has a `gs://` prefix. This seems to me like something might be wrong in how the runner is using gcsfuse on the BED file. Can you take a look? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:371,energy efficiency,Cloud,Cloud,371,"First of all, I assume that you have access to `gs://canis/CNR-data/exomes.bed` file? If you do:. `gsutil cat gs://canis/CNR-data/exomes.bed` from the project that has the same permission, you can first double check that you have access to it. And, I don't fully understand how gcsfuse is implemented in the Google Cloud runner, so I'll defer this to our collaborator at Cloud @nmousavi -- Nima, in the command in the previous log, particularly this one:. `mkdir -p ./input-gcsfused-0 && gcsfuse --implicit-dirs canis /input-gcsfused-0 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/canis/CNR-data/deep_variant_files/examples/0/examples_output.tfrecord@8.gz --reads /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam --ref /mnt/google/.google/input/deepvariant/performance-testdata/hs37d5.fa.gz --task 0 --regions gs://canis/CNR-data/exomes.bed`. It seems weird to me that everything else has been changed to under `/input-gcsfused-0` or `/mnt`, but the BED file still has a `gs://` prefix. This seems to me like something might be wrong in how the runner is using gcsfuse on the BED file. Can you take a look? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:804,performance,perform,performance-testdata,804,"First of all, I assume that you have access to `gs://canis/CNR-data/exomes.bed` file? If you do:. `gsutil cat gs://canis/CNR-data/exomes.bed` from the project that has the same permission, you can first double check that you have access to it. And, I don't fully understand how gcsfuse is implemented in the Google Cloud runner, so I'll defer this to our collaborator at Cloud @nmousavi -- Nima, in the command in the previous log, particularly this one:. `mkdir -p ./input-gcsfused-0 && gcsfuse --implicit-dirs canis /input-gcsfused-0 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/canis/CNR-data/deep_variant_files/examples/0/examples_output.tfrecord@8.gz --reads /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam --ref /mnt/google/.google/input/deepvariant/performance-testdata/hs37d5.fa.gz --task 0 --regions gs://canis/CNR-data/exomes.bed`. It seems weird to me that everything else has been changed to under `/input-gcsfused-0` or `/mnt`, but the BED file still has a `gs://` prefix. This seems to me like something might be wrong in how the runner is using gcsfuse on the BED file. Can you take a look? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:177,safety,permiss,permission,177,"First of all, I assume that you have access to `gs://canis/CNR-data/exomes.bed` file? If you do:. `gsutil cat gs://canis/CNR-data/exomes.bed` from the project that has the same permission, you can first double check that you have access to it. And, I don't fully understand how gcsfuse is implemented in the Google Cloud runner, so I'll defer this to our collaborator at Cloud @nmousavi -- Nima, in the command in the previous log, particularly this one:. `mkdir -p ./input-gcsfused-0 && gcsfuse --implicit-dirs canis /input-gcsfused-0 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/canis/CNR-data/deep_variant_files/examples/0/examples_output.tfrecord@8.gz --reads /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam --ref /mnt/google/.google/input/deepvariant/performance-testdata/hs37d5.fa.gz --task 0 --regions gs://canis/CNR-data/exomes.bed`. It seems weird to me that everything else has been changed to under `/input-gcsfused-0` or `/mnt`, but the BED file still has a `gs://` prefix. This seems to me like something might be wrong in how the runner is using gcsfuse on the BED file. Can you take a look? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:427,safety,log,log,427,"First of all, I assume that you have access to `gs://canis/CNR-data/exomes.bed` file? If you do:. `gsutil cat gs://canis/CNR-data/exomes.bed` from the project that has the same permission, you can first double check that you have access to it. And, I don't fully understand how gcsfuse is implemented in the Google Cloud runner, so I'll defer this to our collaborator at Cloud @nmousavi -- Nima, in the command in the previous log, particularly this one:. `mkdir -p ./input-gcsfused-0 && gcsfuse --implicit-dirs canis /input-gcsfused-0 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/canis/CNR-data/deep_variant_files/examples/0/examples_output.tfrecord@8.gz --reads /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam --ref /mnt/google/.google/input/deepvariant/performance-testdata/hs37d5.fa.gz --task 0 --regions gs://canis/CNR-data/exomes.bed`. It seems weird to me that everything else has been changed to under `/input-gcsfused-0` or `/mnt`, but the BED file still has a `gs://` prefix. This seems to me like something might be wrong in how the runner is using gcsfuse on the BED file. Can you take a look? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:468,safety,input,input-gcsfused-,468,"First of all, I assume that you have access to `gs://canis/CNR-data/exomes.bed` file? If you do:. `gsutil cat gs://canis/CNR-data/exomes.bed` from the project that has the same permission, you can first double check that you have access to it. And, I don't fully understand how gcsfuse is implemented in the Google Cloud runner, so I'll defer this to our collaborator at Cloud @nmousavi -- Nima, in the command in the previous log, particularly this one:. `mkdir -p ./input-gcsfused-0 && gcsfuse --implicit-dirs canis /input-gcsfused-0 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/canis/CNR-data/deep_variant_files/examples/0/examples_output.tfrecord@8.gz --reads /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam --ref /mnt/google/.google/input/deepvariant/performance-testdata/hs37d5.fa.gz --task 0 --regions gs://canis/CNR-data/exomes.bed`. It seems weird to me that everything else has been changed to under `/input-gcsfused-0` or `/mnt`, but the BED file still has a `gs://` prefix. This seems to me like something might be wrong in how the runner is using gcsfuse on the BED file. Can you take a look? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:519,safety,input,input-gcsfused-,519,"First of all, I assume that you have access to `gs://canis/CNR-data/exomes.bed` file? If you do:. `gsutil cat gs://canis/CNR-data/exomes.bed` from the project that has the same permission, you can first double check that you have access to it. And, I don't fully understand how gcsfuse is implemented in the Google Cloud runner, so I'll defer this to our collaborator at Cloud @nmousavi -- Nima, in the command in the previous log, particularly this one:. `mkdir -p ./input-gcsfused-0 && gcsfuse --implicit-dirs canis /input-gcsfused-0 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/canis/CNR-data/deep_variant_files/examples/0/examples_output.tfrecord@8.gz --reads /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam --ref /mnt/google/.google/input/deepvariant/performance-testdata/hs37d5.fa.gz --task 0 --regions gs://canis/CNR-data/exomes.bed`. It seems weird to me that everything else has been changed to under `/input-gcsfused-0` or `/mnt`, but the BED file still has a `gs://` prefix. This seems to me like something might be wrong in how the runner is using gcsfuse on the BED file. Can you take a look? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:711,safety,input,input-gcsfused-,711,"First of all, I assume that you have access to `gs://canis/CNR-data/exomes.bed` file? If you do:. `gsutil cat gs://canis/CNR-data/exomes.bed` from the project that has the same permission, you can first double check that you have access to it. And, I don't fully understand how gcsfuse is implemented in the Google Cloud runner, so I'll defer this to our collaborator at Cloud @nmousavi -- Nima, in the command in the previous log, particularly this one:. `mkdir -p ./input-gcsfused-0 && gcsfuse --implicit-dirs canis /input-gcsfused-0 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/canis/CNR-data/deep_variant_files/examples/0/examples_output.tfrecord@8.gz --reads /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam --ref /mnt/google/.google/input/deepvariant/performance-testdata/hs37d5.fa.gz --task 0 --regions gs://canis/CNR-data/exomes.bed`. It seems weird to me that everything else has been changed to under `/input-gcsfused-0` or `/mnt`, but the BED file still has a `gs://` prefix. This seems to me like something might be wrong in how the runner is using gcsfuse on the BED file. Can you take a look? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:786,safety,input,input,786,"First of all, I assume that you have access to `gs://canis/CNR-data/exomes.bed` file? If you do:. `gsutil cat gs://canis/CNR-data/exomes.bed` from the project that has the same permission, you can first double check that you have access to it. And, I don't fully understand how gcsfuse is implemented in the Google Cloud runner, so I'll defer this to our collaborator at Cloud @nmousavi -- Nima, in the command in the previous log, particularly this one:. `mkdir -p ./input-gcsfused-0 && gcsfuse --implicit-dirs canis /input-gcsfused-0 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/canis/CNR-data/deep_variant_files/examples/0/examples_output.tfrecord@8.gz --reads /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam --ref /mnt/google/.google/input/deepvariant/performance-testdata/hs37d5.fa.gz --task 0 --regions gs://canis/CNR-data/exomes.bed`. It seems weird to me that everything else has been changed to under `/input-gcsfused-0` or `/mnt`, but the BED file still has a `gs://` prefix. This seems to me like something might be wrong in how the runner is using gcsfuse on the BED file. Can you take a look? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:816,safety,test,testdata,816,"First of all, I assume that you have access to `gs://canis/CNR-data/exomes.bed` file? If you do:. `gsutil cat gs://canis/CNR-data/exomes.bed` from the project that has the same permission, you can first double check that you have access to it. And, I don't fully understand how gcsfuse is implemented in the Google Cloud runner, so I'll defer this to our collaborator at Cloud @nmousavi -- Nima, in the command in the previous log, particularly this one:. `mkdir -p ./input-gcsfused-0 && gcsfuse --implicit-dirs canis /input-gcsfused-0 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/canis/CNR-data/deep_variant_files/examples/0/examples_output.tfrecord@8.gz --reads /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam --ref /mnt/google/.google/input/deepvariant/performance-testdata/hs37d5.fa.gz --task 0 --regions gs://canis/CNR-data/exomes.bed`. It seems weird to me that everything else has been changed to under `/input-gcsfused-0` or `/mnt`, but the BED file still has a `gs://` prefix. This seems to me like something might be wrong in how the runner is using gcsfuse on the BED file. Can you take a look? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:960,safety,input,input-gcsfused-,960,"First of all, I assume that you have access to `gs://canis/CNR-data/exomes.bed` file? If you do:. `gsutil cat gs://canis/CNR-data/exomes.bed` from the project that has the same permission, you can first double check that you have access to it. And, I don't fully understand how gcsfuse is implemented in the Google Cloud runner, so I'll defer this to our collaborator at Cloud @nmousavi -- Nima, in the command in the previous log, particularly this one:. `mkdir -p ./input-gcsfused-0 && gcsfuse --implicit-dirs canis /input-gcsfused-0 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/canis/CNR-data/deep_variant_files/examples/0/examples_output.tfrecord@8.gz --reads /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam --ref /mnt/google/.google/input/deepvariant/performance-testdata/hs37d5.fa.gz --task 0 --regions gs://canis/CNR-data/exomes.bed`. It seems weird to me that everything else has been changed to under `/input-gcsfused-0` or `/mnt`, but the BED file still has a `gs://` prefix. This seems to me like something might be wrong in how the runner is using gcsfuse on the BED file. Can you take a look? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:37,security,access,access,37,"First of all, I assume that you have access to `gs://canis/CNR-data/exomes.bed` file? If you do:. `gsutil cat gs://canis/CNR-data/exomes.bed` from the project that has the same permission, you can first double check that you have access to it. And, I don't fully understand how gcsfuse is implemented in the Google Cloud runner, so I'll defer this to our collaborator at Cloud @nmousavi -- Nima, in the command in the previous log, particularly this one:. `mkdir -p ./input-gcsfused-0 && gcsfuse --implicit-dirs canis /input-gcsfused-0 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/canis/CNR-data/deep_variant_files/examples/0/examples_output.tfrecord@8.gz --reads /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam --ref /mnt/google/.google/input/deepvariant/performance-testdata/hs37d5.fa.gz --task 0 --regions gs://canis/CNR-data/exomes.bed`. It seems weird to me that everything else has been changed to under `/input-gcsfused-0` or `/mnt`, but the BED file still has a `gs://` prefix. This seems to me like something might be wrong in how the runner is using gcsfuse on the BED file. Can you take a look? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:230,security,access,access,230,"First of all, I assume that you have access to `gs://canis/CNR-data/exomes.bed` file? If you do:. `gsutil cat gs://canis/CNR-data/exomes.bed` from the project that has the same permission, you can first double check that you have access to it. And, I don't fully understand how gcsfuse is implemented in the Google Cloud runner, so I'll defer this to our collaborator at Cloud @nmousavi -- Nima, in the command in the previous log, particularly this one:. `mkdir -p ./input-gcsfused-0 && gcsfuse --implicit-dirs canis /input-gcsfused-0 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/canis/CNR-data/deep_variant_files/examples/0/examples_output.tfrecord@8.gz --reads /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam --ref /mnt/google/.google/input/deepvariant/performance-testdata/hs37d5.fa.gz --task 0 --regions gs://canis/CNR-data/exomes.bed`. It seems weird to me that everything else has been changed to under `/input-gcsfused-0` or `/mnt`, but the BED file still has a `gs://` prefix. This seems to me like something might be wrong in how the runner is using gcsfuse on the BED file. Can you take a look? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:427,security,log,log,427,"First of all, I assume that you have access to `gs://canis/CNR-data/exomes.bed` file? If you do:. `gsutil cat gs://canis/CNR-data/exomes.bed` from the project that has the same permission, you can first double check that you have access to it. And, I don't fully understand how gcsfuse is implemented in the Google Cloud runner, so I'll defer this to our collaborator at Cloud @nmousavi -- Nima, in the command in the previous log, particularly this one:. `mkdir -p ./input-gcsfused-0 && gcsfuse --implicit-dirs canis /input-gcsfused-0 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/canis/CNR-data/deep_variant_files/examples/0/examples_output.tfrecord@8.gz --reads /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam --ref /mnt/google/.google/input/deepvariant/performance-testdata/hs37d5.fa.gz --task 0 --regions gs://canis/CNR-data/exomes.bed`. It seems weird to me that everything else has been changed to under `/input-gcsfused-0` or `/mnt`, but the BED file still has a `gs://` prefix. This seems to me like something might be wrong in how the runner is using gcsfuse on the BED file. Can you take a look? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:263,testability,understand,understand,263,"First of all, I assume that you have access to `gs://canis/CNR-data/exomes.bed` file? If you do:. `gsutil cat gs://canis/CNR-data/exomes.bed` from the project that has the same permission, you can first double check that you have access to it. And, I don't fully understand how gcsfuse is implemented in the Google Cloud runner, so I'll defer this to our collaborator at Cloud @nmousavi -- Nima, in the command in the previous log, particularly this one:. `mkdir -p ./input-gcsfused-0 && gcsfuse --implicit-dirs canis /input-gcsfused-0 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/canis/CNR-data/deep_variant_files/examples/0/examples_output.tfrecord@8.gz --reads /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam --ref /mnt/google/.google/input/deepvariant/performance-testdata/hs37d5.fa.gz --task 0 --regions gs://canis/CNR-data/exomes.bed`. It seems weird to me that everything else has been changed to under `/input-gcsfused-0` or `/mnt`, but the BED file still has a `gs://` prefix. This seems to me like something might be wrong in how the runner is using gcsfuse on the BED file. Can you take a look? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:427,testability,log,log,427,"First of all, I assume that you have access to `gs://canis/CNR-data/exomes.bed` file? If you do:. `gsutil cat gs://canis/CNR-data/exomes.bed` from the project that has the same permission, you can first double check that you have access to it. And, I don't fully understand how gcsfuse is implemented in the Google Cloud runner, so I'll defer this to our collaborator at Cloud @nmousavi -- Nima, in the command in the previous log, particularly this one:. `mkdir -p ./input-gcsfused-0 && gcsfuse --implicit-dirs canis /input-gcsfused-0 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/canis/CNR-data/deep_variant_files/examples/0/examples_output.tfrecord@8.gz --reads /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam --ref /mnt/google/.google/input/deepvariant/performance-testdata/hs37d5.fa.gz --task 0 --regions gs://canis/CNR-data/exomes.bed`. It seems weird to me that everything else has been changed to under `/input-gcsfused-0` or `/mnt`, but the BED file still has a `gs://` prefix. This seems to me like something might be wrong in how the runner is using gcsfuse on the BED file. Can you take a look? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:816,testability,test,testdata,816,"First of all, I assume that you have access to `gs://canis/CNR-data/exomes.bed` file? If you do:. `gsutil cat gs://canis/CNR-data/exomes.bed` from the project that has the same permission, you can first double check that you have access to it. And, I don't fully understand how gcsfuse is implemented in the Google Cloud runner, so I'll defer this to our collaborator at Cloud @nmousavi -- Nima, in the command in the previous log, particularly this one:. `mkdir -p ./input-gcsfused-0 && gcsfuse --implicit-dirs canis /input-gcsfused-0 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/canis/CNR-data/deep_variant_files/examples/0/examples_output.tfrecord@8.gz --reads /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam --ref /mnt/google/.google/input/deepvariant/performance-testdata/hs37d5.fa.gz --task 0 --regions gs://canis/CNR-data/exomes.bed`. It seems weird to me that everything else has been changed to under `/input-gcsfused-0` or `/mnt`, but the BED file still has a `gs://` prefix. This seems to me like something might be wrong in how the runner is using gcsfuse on the BED file. Can you take a look? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:403,usability,command,command,403,"First of all, I assume that you have access to `gs://canis/CNR-data/exomes.bed` file? If you do:. `gsutil cat gs://canis/CNR-data/exomes.bed` from the project that has the same permission, you can first double check that you have access to it. And, I don't fully understand how gcsfuse is implemented in the Google Cloud runner, so I'll defer this to our collaborator at Cloud @nmousavi -- Nima, in the command in the previous log, particularly this one:. `mkdir -p ./input-gcsfused-0 && gcsfuse --implicit-dirs canis /input-gcsfused-0 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/canis/CNR-data/deep_variant_files/examples/0/examples_output.tfrecord@8.gz --reads /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam --ref /mnt/google/.google/input/deepvariant/performance-testdata/hs37d5.fa.gz --task 0 --regions gs://canis/CNR-data/exomes.bed`. It seems weird to me that everything else has been changed to under `/input-gcsfused-0` or `/mnt`, but the BED file still has a `gs://` prefix. This seems to me like something might be wrong in how the runner is using gcsfuse on the BED file. Can you take a look? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:468,usability,input,input-gcsfused-,468,"First of all, I assume that you have access to `gs://canis/CNR-data/exomes.bed` file? If you do:. `gsutil cat gs://canis/CNR-data/exomes.bed` from the project that has the same permission, you can first double check that you have access to it. And, I don't fully understand how gcsfuse is implemented in the Google Cloud runner, so I'll defer this to our collaborator at Cloud @nmousavi -- Nima, in the command in the previous log, particularly this one:. `mkdir -p ./input-gcsfused-0 && gcsfuse --implicit-dirs canis /input-gcsfused-0 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/canis/CNR-data/deep_variant_files/examples/0/examples_output.tfrecord@8.gz --reads /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam --ref /mnt/google/.google/input/deepvariant/performance-testdata/hs37d5.fa.gz --task 0 --regions gs://canis/CNR-data/exomes.bed`. It seems weird to me that everything else has been changed to under `/input-gcsfused-0` or `/mnt`, but the BED file still has a `gs://` prefix. This seems to me like something might be wrong in how the runner is using gcsfuse on the BED file. Can you take a look? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:519,usability,input,input-gcsfused-,519,"First of all, I assume that you have access to `gs://canis/CNR-data/exomes.bed` file? If you do:. `gsutil cat gs://canis/CNR-data/exomes.bed` from the project that has the same permission, you can first double check that you have access to it. And, I don't fully understand how gcsfuse is implemented in the Google Cloud runner, so I'll defer this to our collaborator at Cloud @nmousavi -- Nima, in the command in the previous log, particularly this one:. `mkdir -p ./input-gcsfused-0 && gcsfuse --implicit-dirs canis /input-gcsfused-0 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/canis/CNR-data/deep_variant_files/examples/0/examples_output.tfrecord@8.gz --reads /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam --ref /mnt/google/.google/input/deepvariant/performance-testdata/hs37d5.fa.gz --task 0 --regions gs://canis/CNR-data/exomes.bed`. It seems weird to me that everything else has been changed to under `/input-gcsfused-0` or `/mnt`, but the BED file still has a `gs://` prefix. This seems to me like something might be wrong in how the runner is using gcsfuse on the BED file. Can you take a look? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:711,usability,input,input-gcsfused-,711,"First of all, I assume that you have access to `gs://canis/CNR-data/exomes.bed` file? If you do:. `gsutil cat gs://canis/CNR-data/exomes.bed` from the project that has the same permission, you can first double check that you have access to it. And, I don't fully understand how gcsfuse is implemented in the Google Cloud runner, so I'll defer this to our collaborator at Cloud @nmousavi -- Nima, in the command in the previous log, particularly this one:. `mkdir -p ./input-gcsfused-0 && gcsfuse --implicit-dirs canis /input-gcsfused-0 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/canis/CNR-data/deep_variant_files/examples/0/examples_output.tfrecord@8.gz --reads /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam --ref /mnt/google/.google/input/deepvariant/performance-testdata/hs37d5.fa.gz --task 0 --regions gs://canis/CNR-data/exomes.bed`. It seems weird to me that everything else has been changed to under `/input-gcsfused-0` or `/mnt`, but the BED file still has a `gs://` prefix. This seems to me like something might be wrong in how the runner is using gcsfuse on the BED file. Can you take a look? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:786,usability,input,input,786,"First of all, I assume that you have access to `gs://canis/CNR-data/exomes.bed` file? If you do:. `gsutil cat gs://canis/CNR-data/exomes.bed` from the project that has the same permission, you can first double check that you have access to it. And, I don't fully understand how gcsfuse is implemented in the Google Cloud runner, so I'll defer this to our collaborator at Cloud @nmousavi -- Nima, in the command in the previous log, particularly this one:. `mkdir -p ./input-gcsfused-0 && gcsfuse --implicit-dirs canis /input-gcsfused-0 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/canis/CNR-data/deep_variant_files/examples/0/examples_output.tfrecord@8.gz --reads /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam --ref /mnt/google/.google/input/deepvariant/performance-testdata/hs37d5.fa.gz --task 0 --regions gs://canis/CNR-data/exomes.bed`. It seems weird to me that everything else has been changed to under `/input-gcsfused-0` or `/mnt`, but the BED file still has a `gs://` prefix. This seems to me like something might be wrong in how the runner is using gcsfuse on the BED file. Can you take a look? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:804,usability,perform,performance-testdata,804,"First of all, I assume that you have access to `gs://canis/CNR-data/exomes.bed` file? If you do:. `gsutil cat gs://canis/CNR-data/exomes.bed` from the project that has the same permission, you can first double check that you have access to it. And, I don't fully understand how gcsfuse is implemented in the Google Cloud runner, so I'll defer this to our collaborator at Cloud @nmousavi -- Nima, in the command in the previous log, particularly this one:. `mkdir -p ./input-gcsfused-0 && gcsfuse --implicit-dirs canis /input-gcsfused-0 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/canis/CNR-data/deep_variant_files/examples/0/examples_output.tfrecord@8.gz --reads /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam --ref /mnt/google/.google/input/deepvariant/performance-testdata/hs37d5.fa.gz --task 0 --regions gs://canis/CNR-data/exomes.bed`. It seems weird to me that everything else has been changed to under `/input-gcsfused-0` or `/mnt`, but the BED file still has a `gs://` prefix. This seems to me like something might be wrong in how the runner is using gcsfuse on the BED file. Can you take a look? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:960,usability,input,input-gcsfused-,960,"First of all, I assume that you have access to `gs://canis/CNR-data/exomes.bed` file? If you do:. `gsutil cat gs://canis/CNR-data/exomes.bed` from the project that has the same permission, you can first double check that you have access to it. And, I don't fully understand how gcsfuse is implemented in the Google Cloud runner, so I'll defer this to our collaborator at Cloud @nmousavi -- Nima, in the command in the previous log, particularly this one:. `mkdir -p ./input-gcsfused-0 && gcsfuse --implicit-dirs canis /input-gcsfused-0 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/canis/CNR-data/deep_variant_files/examples/0/examples_output.tfrecord@8.gz --reads /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam --ref /mnt/google/.google/input/deepvariant/performance-testdata/hs37d5.fa.gz --task 0 --regions gs://canis/CNR-data/exomes.bed`. It seems weird to me that everything else has been changed to under `/input-gcsfused-0` or `/mnt`, but the BED file still has a `gs://` prefix. This seems to me like something might be wrong in how the runner is using gcsfuse on the BED file. Can you take a look? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:219,integrability,translat,translated,219,@pichuan Thank you for the followup. I definitely have access to the exomes.bed file and the gsutil cat works fine. . I see what you mean re: the `/input-gcsfused-0/CNR-data/` ...it seems like the parameter did not get translated properly -- I noted in the original launch shell script the bam and bai files are also sent using the gs:// format and assume this is the proper means for the bed file,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:219,interoperability,translat,translated,219,@pichuan Thank you for the followup. I definitely have access to the exomes.bed file and the gsutil cat works fine. . I see what you mean re: the `/input-gcsfused-0/CNR-data/` ...it seems like the parameter did not get translated properly -- I noted in the original launch shell script the bam and bai files are also sent using the gs:// format and assume this is the proper means for the bed file,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:338,interoperability,format,format,338,@pichuan Thank you for the followup. I definitely have access to the exomes.bed file and the gsutil cat works fine. . I see what you mean re: the `/input-gcsfused-0/CNR-data/` ...it seems like the parameter did not get translated properly -- I noted in the original launch shell script the bam and bai files are also sent using the gs:// format and assume this is the proper means for the bed file,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:197,modifiability,paramet,parameter,197,@pichuan Thank you for the followup. I definitely have access to the exomes.bed file and the gsutil cat works fine. . I see what you mean re: the `/input-gcsfused-0/CNR-data/` ...it seems like the parameter did not get translated properly -- I noted in the original launch shell script the bam and bai files are also sent using the gs:// format and assume this is the proper means for the bed file,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:148,safety,input,input-gcsfused-,148,@pichuan Thank you for the followup. I definitely have access to the exomes.bed file and the gsutil cat works fine. . I see what you mean re: the `/input-gcsfused-0/CNR-data/` ...it seems like the parameter did not get translated properly -- I noted in the original launch shell script the bam and bai files are also sent using the gs:// format and assume this is the proper means for the bed file,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:55,security,access,access,55,@pichuan Thank you for the followup. I definitely have access to the exomes.bed file and the gsutil cat works fine. . I see what you mean re: the `/input-gcsfused-0/CNR-data/` ...it seems like the parameter did not get translated properly -- I noted in the original launch shell script the bam and bai files are also sent using the gs:// format and assume this is the proper means for the bed file,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:148,usability,input,input-gcsfused-,148,@pichuan Thank you for the followup. I definitely have access to the exomes.bed file and the gsutil cat works fine. . I see what you mean re: the `/input-gcsfused-0/CNR-data/` ...it seems like the parameter did not get translated properly -- I noted in the original launch shell script the bam and bai files are also sent using the gs:// format and assume this is the proper means for the bed file,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:25,deployability,pipelin,pipeline,25,I was able to run a test pipeline successfully with BED file you provided (`gs://public-debug/exomes.bed`). Could you please run your pipeline with `gs://public-debug/exomes.bed` (not `gs://canis/CNR-data/exomes.bed`) and see if it succeeds. This is what I got in the log. ```. I1112 17:29:38.624562 140623242430208 genomics_reader.py:213] Reading gs://public-debug/exomes.bed with NativeBedReader. I1112 17:30:18.942625 140623242430208 make_examples.py:1086] Writing examples to /mnt/google/.google/output/nmousavi-test/dv-test/2018-11-10/staging/examples/0/examples_output.tfrecord-00000-of-00008.gz. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:134,deployability,pipelin,pipeline,134,I was able to run a test pipeline successfully with BED file you provided (`gs://public-debug/exomes.bed`). Could you please run your pipeline with `gs://public-debug/exomes.bed` (not `gs://canis/CNR-data/exomes.bed`) and see if it succeeds. This is what I got in the log. ```. I1112 17:29:38.624562 140623242430208 genomics_reader.py:213] Reading gs://public-debug/exomes.bed with NativeBedReader. I1112 17:30:18.942625 140623242430208 make_examples.py:1086] Writing examples to /mnt/google/.google/output/nmousavi-test/dv-test/2018-11-10/staging/examples/0/examples_output.tfrecord-00000-of-00008.gz. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:268,deployability,log,log,268,I was able to run a test pipeline successfully with BED file you provided (`gs://public-debug/exomes.bed`). Could you please run your pipeline with `gs://public-debug/exomes.bed` (not `gs://canis/CNR-data/exomes.bed`) and see if it succeeds. This is what I got in the log. ```. I1112 17:29:38.624562 140623242430208 genomics_reader.py:213] Reading gs://public-debug/exomes.bed with NativeBedReader. I1112 17:30:18.942625 140623242430208 make_examples.py:1086] Writing examples to /mnt/google/.google/output/nmousavi-test/dv-test/2018-11-10/staging/examples/0/examples_output.tfrecord-00000-of-00008.gz. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:25,integrability,pipelin,pipeline,25,I was able to run a test pipeline successfully with BED file you provided (`gs://public-debug/exomes.bed`). Could you please run your pipeline with `gs://public-debug/exomes.bed` (not `gs://canis/CNR-data/exomes.bed`) and see if it succeeds. This is what I got in the log. ```. I1112 17:29:38.624562 140623242430208 genomics_reader.py:213] Reading gs://public-debug/exomes.bed with NativeBedReader. I1112 17:30:18.942625 140623242430208 make_examples.py:1086] Writing examples to /mnt/google/.google/output/nmousavi-test/dv-test/2018-11-10/staging/examples/0/examples_output.tfrecord-00000-of-00008.gz. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:81,integrability,pub,public-debug,81,I was able to run a test pipeline successfully with BED file you provided (`gs://public-debug/exomes.bed`). Could you please run your pipeline with `gs://public-debug/exomes.bed` (not `gs://canis/CNR-data/exomes.bed`) and see if it succeeds. This is what I got in the log. ```. I1112 17:29:38.624562 140623242430208 genomics_reader.py:213] Reading gs://public-debug/exomes.bed with NativeBedReader. I1112 17:30:18.942625 140623242430208 make_examples.py:1086] Writing examples to /mnt/google/.google/output/nmousavi-test/dv-test/2018-11-10/staging/examples/0/examples_output.tfrecord-00000-of-00008.gz. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:134,integrability,pipelin,pipeline,134,I was able to run a test pipeline successfully with BED file you provided (`gs://public-debug/exomes.bed`). Could you please run your pipeline with `gs://public-debug/exomes.bed` (not `gs://canis/CNR-data/exomes.bed`) and see if it succeeds. This is what I got in the log. ```. I1112 17:29:38.624562 140623242430208 genomics_reader.py:213] Reading gs://public-debug/exomes.bed with NativeBedReader. I1112 17:30:18.942625 140623242430208 make_examples.py:1086] Writing examples to /mnt/google/.google/output/nmousavi-test/dv-test/2018-11-10/staging/examples/0/examples_output.tfrecord-00000-of-00008.gz. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:154,integrability,pub,public-debug,154,I was able to run a test pipeline successfully with BED file you provided (`gs://public-debug/exomes.bed`). Could you please run your pipeline with `gs://public-debug/exomes.bed` (not `gs://canis/CNR-data/exomes.bed`) and see if it succeeds. This is what I got in the log. ```. I1112 17:29:38.624562 140623242430208 genomics_reader.py:213] Reading gs://public-debug/exomes.bed with NativeBedReader. I1112 17:30:18.942625 140623242430208 make_examples.py:1086] Writing examples to /mnt/google/.google/output/nmousavi-test/dv-test/2018-11-10/staging/examples/0/examples_output.tfrecord-00000-of-00008.gz. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:353,integrability,pub,public-debug,353,I was able to run a test pipeline successfully with BED file you provided (`gs://public-debug/exomes.bed`). Could you please run your pipeline with `gs://public-debug/exomes.bed` (not `gs://canis/CNR-data/exomes.bed`) and see if it succeeds. This is what I got in the log. ```. I1112 17:29:38.624562 140623242430208 genomics_reader.py:213] Reading gs://public-debug/exomes.bed with NativeBedReader. I1112 17:30:18.942625 140623242430208 make_examples.py:1086] Writing examples to /mnt/google/.google/output/nmousavi-test/dv-test/2018-11-10/staging/examples/0/examples_output.tfrecord-00000-of-00008.gz. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:20,safety,test,test,20,I was able to run a test pipeline successfully with BED file you provided (`gs://public-debug/exomes.bed`). Could you please run your pipeline with `gs://public-debug/exomes.bed` (not `gs://canis/CNR-data/exomes.bed`) and see if it succeeds. This is what I got in the log. ```. I1112 17:29:38.624562 140623242430208 genomics_reader.py:213] Reading gs://public-debug/exomes.bed with NativeBedReader. I1112 17:30:18.942625 140623242430208 make_examples.py:1086] Writing examples to /mnt/google/.google/output/nmousavi-test/dv-test/2018-11-10/staging/examples/0/examples_output.tfrecord-00000-of-00008.gz. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:268,safety,log,log,268,I was able to run a test pipeline successfully with BED file you provided (`gs://public-debug/exomes.bed`). Could you please run your pipeline with `gs://public-debug/exomes.bed` (not `gs://canis/CNR-data/exomes.bed`) and see if it succeeds. This is what I got in the log. ```. I1112 17:29:38.624562 140623242430208 genomics_reader.py:213] Reading gs://public-debug/exomes.bed with NativeBedReader. I1112 17:30:18.942625 140623242430208 make_examples.py:1086] Writing examples to /mnt/google/.google/output/nmousavi-test/dv-test/2018-11-10/staging/examples/0/examples_output.tfrecord-00000-of-00008.gz. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:516,safety,test,test,516,I was able to run a test pipeline successfully with BED file you provided (`gs://public-debug/exomes.bed`). Could you please run your pipeline with `gs://public-debug/exomes.bed` (not `gs://canis/CNR-data/exomes.bed`) and see if it succeeds. This is what I got in the log. ```. I1112 17:29:38.624562 140623242430208 genomics_reader.py:213] Reading gs://public-debug/exomes.bed with NativeBedReader. I1112 17:30:18.942625 140623242430208 make_examples.py:1086] Writing examples to /mnt/google/.google/output/nmousavi-test/dv-test/2018-11-10/staging/examples/0/examples_output.tfrecord-00000-of-00008.gz. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:524,safety,test,test,524,I was able to run a test pipeline successfully with BED file you provided (`gs://public-debug/exomes.bed`). Could you please run your pipeline with `gs://public-debug/exomes.bed` (not `gs://canis/CNR-data/exomes.bed`) and see if it succeeds. This is what I got in the log. ```. I1112 17:29:38.624562 140623242430208 genomics_reader.py:213] Reading gs://public-debug/exomes.bed with NativeBedReader. I1112 17:30:18.942625 140623242430208 make_examples.py:1086] Writing examples to /mnt/google/.google/output/nmousavi-test/dv-test/2018-11-10/staging/examples/0/examples_output.tfrecord-00000-of-00008.gz. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:268,security,log,log,268,I was able to run a test pipeline successfully with BED file you provided (`gs://public-debug/exomes.bed`). Could you please run your pipeline with `gs://public-debug/exomes.bed` (not `gs://canis/CNR-data/exomes.bed`) and see if it succeeds. This is what I got in the log. ```. I1112 17:29:38.624562 140623242430208 genomics_reader.py:213] Reading gs://public-debug/exomes.bed with NativeBedReader. I1112 17:30:18.942625 140623242430208 make_examples.py:1086] Writing examples to /mnt/google/.google/output/nmousavi-test/dv-test/2018-11-10/staging/examples/0/examples_output.tfrecord-00000-of-00008.gz. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:20,testability,test,test,20,I was able to run a test pipeline successfully with BED file you provided (`gs://public-debug/exomes.bed`). Could you please run your pipeline with `gs://public-debug/exomes.bed` (not `gs://canis/CNR-data/exomes.bed`) and see if it succeeds. This is what I got in the log. ```. I1112 17:29:38.624562 140623242430208 genomics_reader.py:213] Reading gs://public-debug/exomes.bed with NativeBedReader. I1112 17:30:18.942625 140623242430208 make_examples.py:1086] Writing examples to /mnt/google/.google/output/nmousavi-test/dv-test/2018-11-10/staging/examples/0/examples_output.tfrecord-00000-of-00008.gz. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:268,testability,log,log,268,I was able to run a test pipeline successfully with BED file you provided (`gs://public-debug/exomes.bed`). Could you please run your pipeline with `gs://public-debug/exomes.bed` (not `gs://canis/CNR-data/exomes.bed`) and see if it succeeds. This is what I got in the log. ```. I1112 17:29:38.624562 140623242430208 genomics_reader.py:213] Reading gs://public-debug/exomes.bed with NativeBedReader. I1112 17:30:18.942625 140623242430208 make_examples.py:1086] Writing examples to /mnt/google/.google/output/nmousavi-test/dv-test/2018-11-10/staging/examples/0/examples_output.tfrecord-00000-of-00008.gz. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:516,testability,test,test,516,I was able to run a test pipeline successfully with BED file you provided (`gs://public-debug/exomes.bed`). Could you please run your pipeline with `gs://public-debug/exomes.bed` (not `gs://canis/CNR-data/exomes.bed`) and see if it succeeds. This is what I got in the log. ```. I1112 17:29:38.624562 140623242430208 genomics_reader.py:213] Reading gs://public-debug/exomes.bed with NativeBedReader. I1112 17:30:18.942625 140623242430208 make_examples.py:1086] Writing examples to /mnt/google/.google/output/nmousavi-test/dv-test/2018-11-10/staging/examples/0/examples_output.tfrecord-00000-of-00008.gz. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:524,testability,test,test,524,I was able to run a test pipeline successfully with BED file you provided (`gs://public-debug/exomes.bed`). Could you please run your pipeline with `gs://public-debug/exomes.bed` (not `gs://canis/CNR-data/exomes.bed`) and see if it succeeds. This is what I got in the log. ```. I1112 17:29:38.624562 140623242430208 genomics_reader.py:213] Reading gs://public-debug/exomes.bed with NativeBedReader. I1112 17:30:18.942625 140623242430208 make_examples.py:1086] Writing examples to /mnt/google/.google/output/nmousavi-test/dv-test/2018-11-10/staging/examples/0/examples_output.tfrecord-00000-of-00008.gz. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:90,deployability,pipelin,pipeline,90,"Okay, I will try this with my exomes.bed file. I was already able to successfully run the pipeline with the following params. ```. --regions gs://deepvariant/exome-case-study-testdata/refseq.coding_exons.b37.extended50.bed \. --bam gs://canis/CNR-data/TLE_a_001.reheader.bam \. --bai gs://canis/CNR-data/TLE_a_001.reheader.bam.bai \. --ref gs://deepvariant/performance-testdata/hs37d5.fa.gz \. ```. It is odd that the bed file needs to lie in a public bucket, while the genomic data does not.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:90,integrability,pipelin,pipeline,90,"Okay, I will try this with my exomes.bed file. I was already able to successfully run the pipeline with the following params. ```. --regions gs://deepvariant/exome-case-study-testdata/refseq.coding_exons.b37.extended50.bed \. --bam gs://canis/CNR-data/TLE_a_001.reheader.bam \. --bai gs://canis/CNR-data/TLE_a_001.reheader.bam.bai \. --ref gs://deepvariant/performance-testdata/hs37d5.fa.gz \. ```. It is odd that the bed file needs to lie in a public bucket, while the genomic data does not.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:445,integrability,pub,public,445,"Okay, I will try this with my exomes.bed file. I was already able to successfully run the pipeline with the following params. ```. --regions gs://deepvariant/exome-case-study-testdata/refseq.coding_exons.b37.extended50.bed \. --bam gs://canis/CNR-data/TLE_a_001.reheader.bam \. --bai gs://canis/CNR-data/TLE_a_001.reheader.bam.bai \. --ref gs://deepvariant/performance-testdata/hs37d5.fa.gz \. ```. It is odd that the bed file needs to lie in a public bucket, while the genomic data does not.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:357,performance,perform,performance-testdata,357,"Okay, I will try this with my exomes.bed file. I was already able to successfully run the pipeline with the following params. ```. --regions gs://deepvariant/exome-case-study-testdata/refseq.coding_exons.b37.extended50.bed \. --bam gs://canis/CNR-data/TLE_a_001.reheader.bam \. --bai gs://canis/CNR-data/TLE_a_001.reheader.bam.bai \. --ref gs://deepvariant/performance-testdata/hs37d5.fa.gz \. ```. It is odd that the bed file needs to lie in a public bucket, while the genomic data does not.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:483,reliability,doe,does,483,"Okay, I will try this with my exomes.bed file. I was already able to successfully run the pipeline with the following params. ```. --regions gs://deepvariant/exome-case-study-testdata/refseq.coding_exons.b37.extended50.bed \. --bam gs://canis/CNR-data/TLE_a_001.reheader.bam \. --bai gs://canis/CNR-data/TLE_a_001.reheader.bam.bai \. --ref gs://deepvariant/performance-testdata/hs37d5.fa.gz \. ```. It is odd that the bed file needs to lie in a public bucket, while the genomic data does not.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:175,safety,test,testdata,175,"Okay, I will try this with my exomes.bed file. I was already able to successfully run the pipeline with the following params. ```. --regions gs://deepvariant/exome-case-study-testdata/refseq.coding_exons.b37.extended50.bed \. --bam gs://canis/CNR-data/TLE_a_001.reheader.bam \. --bai gs://canis/CNR-data/TLE_a_001.reheader.bam.bai \. --ref gs://deepvariant/performance-testdata/hs37d5.fa.gz \. ```. It is odd that the bed file needs to lie in a public bucket, while the genomic data does not.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:369,safety,test,testdata,369,"Okay, I will try this with my exomes.bed file. I was already able to successfully run the pipeline with the following params. ```. --regions gs://deepvariant/exome-case-study-testdata/refseq.coding_exons.b37.extended50.bed \. --bam gs://canis/CNR-data/TLE_a_001.reheader.bam \. --bai gs://canis/CNR-data/TLE_a_001.reheader.bam.bai \. --ref gs://deepvariant/performance-testdata/hs37d5.fa.gz \. ```. It is odd that the bed file needs to lie in a public bucket, while the genomic data does not.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:175,testability,test,testdata,175,"Okay, I will try this with my exomes.bed file. I was already able to successfully run the pipeline with the following params. ```. --regions gs://deepvariant/exome-case-study-testdata/refseq.coding_exons.b37.extended50.bed \. --bam gs://canis/CNR-data/TLE_a_001.reheader.bam \. --bai gs://canis/CNR-data/TLE_a_001.reheader.bam.bai \. --ref gs://deepvariant/performance-testdata/hs37d5.fa.gz \. ```. It is odd that the bed file needs to lie in a public bucket, while the genomic data does not.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:369,testability,test,testdata,369,"Okay, I will try this with my exomes.bed file. I was already able to successfully run the pipeline with the following params. ```. --regions gs://deepvariant/exome-case-study-testdata/refseq.coding_exons.b37.extended50.bed \. --bam gs://canis/CNR-data/TLE_a_001.reheader.bam \. --bai gs://canis/CNR-data/TLE_a_001.reheader.bam.bai \. --ref gs://deepvariant/performance-testdata/hs37d5.fa.gz \. ```. It is odd that the bed file needs to lie in a public bucket, while the genomic data does not.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:357,usability,perform,performance-testdata,357,"Okay, I will try this with my exomes.bed file. I was already able to successfully run the pipeline with the following params. ```. --regions gs://deepvariant/exome-case-study-testdata/refseq.coding_exons.b37.extended50.bed \. --bam gs://canis/CNR-data/TLE_a_001.reheader.bam \. --bai gs://canis/CNR-data/TLE_a_001.reheader.bam.bai \. --ref gs://deepvariant/performance-testdata/hs37d5.fa.gz \. ```. It is odd that the bed file needs to lie in a public bucket, while the genomic data does not.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:59,integrability,pub,public,59,"Lets first see if that's the case (i.e. having bed file in public bucket resolves it). If yes, this is a bug and we will fix it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:128,deployability,pipelin,pipeline,128,I can confirm that placing the exomes.bed file in `gs://public-debug` results in successful reading of the BED file (though the pipeline fails at a latter step due to inconsistent contig names within the BED vs. the reference genome).,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:137,deployability,fail,fails,137,I can confirm that placing the exomes.bed file in `gs://public-debug` results in successful reading of the BED file (though the pipeline fails at a latter step due to inconsistent contig names within the BED vs. the reference genome).,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:56,integrability,pub,public-debug,56,I can confirm that placing the exomes.bed file in `gs://public-debug` results in successful reading of the BED file (though the pipeline fails at a latter step due to inconsistent contig names within the BED vs. the reference genome).,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:128,integrability,pipelin,pipeline,128,I can confirm that placing the exomes.bed file in `gs://public-debug` results in successful reading of the BED file (though the pipeline fails at a latter step due to inconsistent contig names within the BED vs. the reference genome).,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:137,reliability,fail,fails,137,I can confirm that placing the exomes.bed file in `gs://public-debug` results in successful reading of the BED file (though the pipeline fails at a latter step due to inconsistent contig names within the BED vs. the reference genome).,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:6,usability,confirm,confirm,6,I can confirm that placing the exomes.bed file in `gs://public-debug` results in successful reading of the BED file (though the pipeline fails at a latter step due to inconsistent contig names within the BED vs. the reference genome).,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:256,integrability,pub,public,256,"Thanks for confirmation. I have done additional testings, and the conclusion is that the underlying htslib used in DeepVariant is the culprit. I have created a issue for my self to fix it (#119). Meanwhile, if it's possible please put your BED file into a public bucket and rerun DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:48,safety,test,testings,48,"Thanks for confirmation. I have done additional testings, and the conclusion is that the underlying htslib used in DeepVariant is the culprit. I have created a issue for my self to fix it (#119). Meanwhile, if it's possible please put your BED file into a public bucket and rerun DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:48,testability,test,testings,48,"Thanks for confirmation. I have done additional testings, and the conclusion is that the underlying htslib used in DeepVariant is the culprit. I have created a issue for my self to fix it (#119). Meanwhile, if it's possible please put your BED file into a public bucket and rerun DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:11,usability,confirm,confirmation,11,"Thanks for confirmation. I have done additional testings, and the conclusion is that the underlying htslib used in DeepVariant is the culprit. I have created a issue for my self to fix it (#119). Meanwhile, if it's possible please put your BED file into a public bucket and rerun DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:59,integrability,pub,public,59,"Thanks for all the help @nmousavi , we can certainly use a public bucket for the region files as there is no PII.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:19,usability,help,help,19,"Thanks for all the help @nmousavi , we can certainly use a public bucket for the region files as there is no PII.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:52,deployability,releas,release,52,"@saliksyed . Hopefully this is resolved with v0.7.2 release, and with the workaround of putting the BED file in a public bucket. If you encounter more issues, let us know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/116:114,integrability,pub,public,114,"@saliksyed . Hopefully this is resolved with v0.7.2 release, and with the workaround of putting the BED file in a public bucket. If you encounter more issues, let us know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/116
https://github.com/google/deepvariant/issues/117:91,availability,checkpoint,checkpoint,91,"From looking at the shapes of the tensors, it seems like you might be using an older model checkpoint? In an older release, we used to have 7 channels instead of 6. Can you confirm whether you're using the 0.7.0 model checkpoint?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/117
https://github.com/google/deepvariant/issues/117:218,availability,checkpoint,checkpoint,218,"From looking at the shapes of the tensors, it seems like you might be using an older model checkpoint? In an older release, we used to have 7 channels instead of 6. Can you confirm whether you're using the 0.7.0 model checkpoint?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/117
https://github.com/google/deepvariant/issues/117:115,deployability,releas,release,115,"From looking at the shapes of the tensors, it seems like you might be using an older model checkpoint? In an older release, we used to have 7 channels instead of 6. Can you confirm whether you're using the 0.7.0 model checkpoint?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/117
https://github.com/google/deepvariant/issues/117:85,energy efficiency,model,model,85,"From looking at the shapes of the tensors, it seems like you might be using an older model checkpoint? In an older release, we used to have 7 channels instead of 6. Can you confirm whether you're using the 0.7.0 model checkpoint?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/117
https://github.com/google/deepvariant/issues/117:212,energy efficiency,model,model,212,"From looking at the shapes of the tensors, it seems like you might be using an older model checkpoint? In an older release, we used to have 7 channels instead of 6. Can you confirm whether you're using the 0.7.0 model checkpoint?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/117
https://github.com/google/deepvariant/issues/117:91,reliability,checkpoint,checkpoint,91,"From looking at the shapes of the tensors, it seems like you might be using an older model checkpoint? In an older release, we used to have 7 channels instead of 6. Can you confirm whether you're using the 0.7.0 model checkpoint?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/117
https://github.com/google/deepvariant/issues/117:218,reliability,checkpoint,checkpoint,218,"From looking at the shapes of the tensors, it seems like you might be using an older model checkpoint? In an older release, we used to have 7 channels instead of 6. Can you confirm whether you're using the 0.7.0 model checkpoint?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/117
https://github.com/google/deepvariant/issues/117:85,security,model,model,85,"From looking at the shapes of the tensors, it seems like you might be using an older model checkpoint? In an older release, we used to have 7 channels instead of 6. Can you confirm whether you're using the 0.7.0 model checkpoint?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/117
https://github.com/google/deepvariant/issues/117:212,security,model,model,212,"From looking at the shapes of the tensors, it seems like you might be using an older model checkpoint? In an older release, we used to have 7 channels instead of 6. Can you confirm whether you're using the 0.7.0 model checkpoint?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/117
https://github.com/google/deepvariant/issues/117:173,usability,confirm,confirm,173,"From looking at the shapes of the tensors, it seems like you might be using an older model checkpoint? In an older release, we used to have 7 channels instead of 6. Can you confirm whether you're using the 0.7.0 model checkpoint?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/117
https://github.com/google/deepvariant/issues/118:80,deployability,Pipelin,Pipeline,80,"No YAML file is needed for running DeepVariant as we migrated to using Genomics Pipeline API v2 since DeepVariant v0.7 . Please see DeepVariant's cloud page for sample scripts. https://cloud.google.com/genomics/docs/tutorials/deepvariant. Note that we keep the cloud page updated with every release, and it is the place to look into in case of major changes to DeepVariant runner. For debugging, we found workers log more helpful in general. In your case, they are under `gs://gbsc-gcp-project-udn-dev-deep-variant/UDN668131_test/deepvariant_staging_folder/logs`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:89,deployability,API,API,89,"No YAML file is needed for running DeepVariant as we migrated to using Genomics Pipeline API v2 since DeepVariant v0.7 . Please see DeepVariant's cloud page for sample scripts. https://cloud.google.com/genomics/docs/tutorials/deepvariant. Note that we keep the cloud page updated with every release, and it is the place to look into in case of major changes to DeepVariant runner. For debugging, we found workers log more helpful in general. In your case, they are under `gs://gbsc-gcp-project-udn-dev-deep-variant/UDN668131_test/deepvariant_staging_folder/logs`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:272,deployability,updat,updated,272,"No YAML file is needed for running DeepVariant as we migrated to using Genomics Pipeline API v2 since DeepVariant v0.7 . Please see DeepVariant's cloud page for sample scripts. https://cloud.google.com/genomics/docs/tutorials/deepvariant. Note that we keep the cloud page updated with every release, and it is the place to look into in case of major changes to DeepVariant runner. For debugging, we found workers log more helpful in general. In your case, they are under `gs://gbsc-gcp-project-udn-dev-deep-variant/UDN668131_test/deepvariant_staging_folder/logs`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:291,deployability,releas,release,291,"No YAML file is needed for running DeepVariant as we migrated to using Genomics Pipeline API v2 since DeepVariant v0.7 . Please see DeepVariant's cloud page for sample scripts. https://cloud.google.com/genomics/docs/tutorials/deepvariant. Note that we keep the cloud page updated with every release, and it is the place to look into in case of major changes to DeepVariant runner. For debugging, we found workers log more helpful in general. In your case, they are under `gs://gbsc-gcp-project-udn-dev-deep-variant/UDN668131_test/deepvariant_staging_folder/logs`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:413,deployability,log,log,413,"No YAML file is needed for running DeepVariant as we migrated to using Genomics Pipeline API v2 since DeepVariant v0.7 . Please see DeepVariant's cloud page for sample scripts. https://cloud.google.com/genomics/docs/tutorials/deepvariant. Note that we keep the cloud page updated with every release, and it is the place to look into in case of major changes to DeepVariant runner. For debugging, we found workers log more helpful in general. In your case, they are under `gs://gbsc-gcp-project-udn-dev-deep-variant/UDN668131_test/deepvariant_staging_folder/logs`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:557,deployability,log,logs,557,"No YAML file is needed for running DeepVariant as we migrated to using Genomics Pipeline API v2 since DeepVariant v0.7 . Please see DeepVariant's cloud page for sample scripts. https://cloud.google.com/genomics/docs/tutorials/deepvariant. Note that we keep the cloud page updated with every release, and it is the place to look into in case of major changes to DeepVariant runner. For debugging, we found workers log more helpful in general. In your case, they are under `gs://gbsc-gcp-project-udn-dev-deep-variant/UDN668131_test/deepvariant_staging_folder/logs`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:146,energy efficiency,cloud,cloud,146,"No YAML file is needed for running DeepVariant as we migrated to using Genomics Pipeline API v2 since DeepVariant v0.7 . Please see DeepVariant's cloud page for sample scripts. https://cloud.google.com/genomics/docs/tutorials/deepvariant. Note that we keep the cloud page updated with every release, and it is the place to look into in case of major changes to DeepVariant runner. For debugging, we found workers log more helpful in general. In your case, they are under `gs://gbsc-gcp-project-udn-dev-deep-variant/UDN668131_test/deepvariant_staging_folder/logs`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:185,energy efficiency,cloud,cloud,185,"No YAML file is needed for running DeepVariant as we migrated to using Genomics Pipeline API v2 since DeepVariant v0.7 . Please see DeepVariant's cloud page for sample scripts. https://cloud.google.com/genomics/docs/tutorials/deepvariant. Note that we keep the cloud page updated with every release, and it is the place to look into in case of major changes to DeepVariant runner. For debugging, we found workers log more helpful in general. In your case, they are under `gs://gbsc-gcp-project-udn-dev-deep-variant/UDN668131_test/deepvariant_staging_folder/logs`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:261,energy efficiency,cloud,cloud,261,"No YAML file is needed for running DeepVariant as we migrated to using Genomics Pipeline API v2 since DeepVariant v0.7 . Please see DeepVariant's cloud page for sample scripts. https://cloud.google.com/genomics/docs/tutorials/deepvariant. Note that we keep the cloud page updated with every release, and it is the place to look into in case of major changes to DeepVariant runner. For debugging, we found workers log more helpful in general. In your case, they are under `gs://gbsc-gcp-project-udn-dev-deep-variant/UDN668131_test/deepvariant_staging_folder/logs`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:80,integrability,Pipelin,Pipeline,80,"No YAML file is needed for running DeepVariant as we migrated to using Genomics Pipeline API v2 since DeepVariant v0.7 . Please see DeepVariant's cloud page for sample scripts. https://cloud.google.com/genomics/docs/tutorials/deepvariant. Note that we keep the cloud page updated with every release, and it is the place to look into in case of major changes to DeepVariant runner. For debugging, we found workers log more helpful in general. In your case, they are under `gs://gbsc-gcp-project-udn-dev-deep-variant/UDN668131_test/deepvariant_staging_folder/logs`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:89,integrability,API,API,89,"No YAML file is needed for running DeepVariant as we migrated to using Genomics Pipeline API v2 since DeepVariant v0.7 . Please see DeepVariant's cloud page for sample scripts. https://cloud.google.com/genomics/docs/tutorials/deepvariant. Note that we keep the cloud page updated with every release, and it is the place to look into in case of major changes to DeepVariant runner. For debugging, we found workers log more helpful in general. In your case, they are under `gs://gbsc-gcp-project-udn-dev-deep-variant/UDN668131_test/deepvariant_staging_folder/logs`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:89,interoperability,API,API,89,"No YAML file is needed for running DeepVariant as we migrated to using Genomics Pipeline API v2 since DeepVariant v0.7 . Please see DeepVariant's cloud page for sample scripts. https://cloud.google.com/genomics/docs/tutorials/deepvariant. Note that we keep the cloud page updated with every release, and it is the place to look into in case of major changes to DeepVariant runner. For debugging, we found workers log more helpful in general. In your case, they are under `gs://gbsc-gcp-project-udn-dev-deep-variant/UDN668131_test/deepvariant_staging_folder/logs`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:272,safety,updat,updated,272,"No YAML file is needed for running DeepVariant as we migrated to using Genomics Pipeline API v2 since DeepVariant v0.7 . Please see DeepVariant's cloud page for sample scripts. https://cloud.google.com/genomics/docs/tutorials/deepvariant. Note that we keep the cloud page updated with every release, and it is the place to look into in case of major changes to DeepVariant runner. For debugging, we found workers log more helpful in general. In your case, they are under `gs://gbsc-gcp-project-udn-dev-deep-variant/UDN668131_test/deepvariant_staging_folder/logs`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:413,safety,log,log,413,"No YAML file is needed for running DeepVariant as we migrated to using Genomics Pipeline API v2 since DeepVariant v0.7 . Please see DeepVariant's cloud page for sample scripts. https://cloud.google.com/genomics/docs/tutorials/deepvariant. Note that we keep the cloud page updated with every release, and it is the place to look into in case of major changes to DeepVariant runner. For debugging, we found workers log more helpful in general. In your case, they are under `gs://gbsc-gcp-project-udn-dev-deep-variant/UDN668131_test/deepvariant_staging_folder/logs`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:557,safety,log,logs,557,"No YAML file is needed for running DeepVariant as we migrated to using Genomics Pipeline API v2 since DeepVariant v0.7 . Please see DeepVariant's cloud page for sample scripts. https://cloud.google.com/genomics/docs/tutorials/deepvariant. Note that we keep the cloud page updated with every release, and it is the place to look into in case of major changes to DeepVariant runner. For debugging, we found workers log more helpful in general. In your case, they are under `gs://gbsc-gcp-project-udn-dev-deep-variant/UDN668131_test/deepvariant_staging_folder/logs`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:272,security,updat,updated,272,"No YAML file is needed for running DeepVariant as we migrated to using Genomics Pipeline API v2 since DeepVariant v0.7 . Please see DeepVariant's cloud page for sample scripts. https://cloud.google.com/genomics/docs/tutorials/deepvariant. Note that we keep the cloud page updated with every release, and it is the place to look into in case of major changes to DeepVariant runner. For debugging, we found workers log more helpful in general. In your case, they are under `gs://gbsc-gcp-project-udn-dev-deep-variant/UDN668131_test/deepvariant_staging_folder/logs`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:413,security,log,log,413,"No YAML file is needed for running DeepVariant as we migrated to using Genomics Pipeline API v2 since DeepVariant v0.7 . Please see DeepVariant's cloud page for sample scripts. https://cloud.google.com/genomics/docs/tutorials/deepvariant. Note that we keep the cloud page updated with every release, and it is the place to look into in case of major changes to DeepVariant runner. For debugging, we found workers log more helpful in general. In your case, they are under `gs://gbsc-gcp-project-udn-dev-deep-variant/UDN668131_test/deepvariant_staging_folder/logs`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:557,security,log,logs,557,"No YAML file is needed for running DeepVariant as we migrated to using Genomics Pipeline API v2 since DeepVariant v0.7 . Please see DeepVariant's cloud page for sample scripts. https://cloud.google.com/genomics/docs/tutorials/deepvariant. Note that we keep the cloud page updated with every release, and it is the place to look into in case of major changes to DeepVariant runner. For debugging, we found workers log more helpful in general. In your case, they are under `gs://gbsc-gcp-project-udn-dev-deep-variant/UDN668131_test/deepvariant_staging_folder/logs`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:413,testability,log,log,413,"No YAML file is needed for running DeepVariant as we migrated to using Genomics Pipeline API v2 since DeepVariant v0.7 . Please see DeepVariant's cloud page for sample scripts. https://cloud.google.com/genomics/docs/tutorials/deepvariant. Note that we keep the cloud page updated with every release, and it is the place to look into in case of major changes to DeepVariant runner. For debugging, we found workers log more helpful in general. In your case, they are under `gs://gbsc-gcp-project-udn-dev-deep-variant/UDN668131_test/deepvariant_staging_folder/logs`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:557,testability,log,logs,557,"No YAML file is needed for running DeepVariant as we migrated to using Genomics Pipeline API v2 since DeepVariant v0.7 . Please see DeepVariant's cloud page for sample scripts. https://cloud.google.com/genomics/docs/tutorials/deepvariant. Note that we keep the cloud page updated with every release, and it is the place to look into in case of major changes to DeepVariant runner. For debugging, we found workers log more helpful in general. In your case, they are under `gs://gbsc-gcp-project-udn-dev-deep-variant/UDN668131_test/deepvariant_staging_folder/logs`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:422,usability,help,helpful,422,"No YAML file is needed for running DeepVariant as we migrated to using Genomics Pipeline API v2 since DeepVariant v0.7 . Please see DeepVariant's cloud page for sample scripts. https://cloud.google.com/genomics/docs/tutorials/deepvariant. Note that we keep the cloud page updated with every release, and it is the place to look into in case of major changes to DeepVariant runner. For debugging, we found workers log more helpful in general. In your case, they are under `gs://gbsc-gcp-project-udn-dev-deep-variant/UDN668131_test/deepvariant_staging_folder/logs`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:119,deployability,log,log,119,"Thanks nmousavi. I have the yaml file only for v0.6.1 and not for v_0.7.0. However, thanks for pointing toward workers log folder. This gives a better idea in debugging. Thanks,. Shruti",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:119,safety,log,log,119,"Thanks nmousavi. I have the yaml file only for v0.6.1 and not for v_0.7.0. However, thanks for pointing toward workers log folder. This gives a better idea in debugging. Thanks,. Shruti",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:119,security,log,log,119,"Thanks nmousavi. I have the yaml file only for v0.6.1 and not for v_0.7.0. However, thanks for pointing toward workers log folder. This gives a better idea in debugging. Thanks,. Shruti",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:119,testability,log,log,119,"Thanks nmousavi. I have the yaml file only for v0.6.1 and not for v_0.7.0. However, thanks for pointing toward workers log folder. This gives a better idea in debugging. Thanks,. Shruti",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:25,deployability,log,log,25,"NP, please share workers log so we can help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:11,interoperability,share,share,11,"NP, please share workers log so we can help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:25,safety,log,log,25,"NP, please share workers log so we can help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:25,security,log,log,25,"NP, please share workers log so we can help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:25,testability,log,log,25,"NP, please share workers log so we can help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:39,usability,help,help,39,"NP, please share workers log so we can help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:44,availability,error,error,44,"Thanks Nima. I ran it again. Looks like the error is because it is unable to open the bed file I have provided. However, the bed file exists on gcp and my v0.6.1 code was able to access it. I am not sure what I am doing wrong. I was able to run the same code successfully if I provide the bed file in the example documentation (gs://deepvariant/exome-case-study-testdata/refseq.coding_exons.b37.extended50.bed). . Error file: gbsc-gcp-project-udn-dev-deep-variant/UDN668131_deepVariant_test4/deepvariant_staging_folder/logs/make_examples/0:. W::hts_idx_load2] The index file is older than the data file: /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bai. 2018-11-09 19:48:51.497793: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1109 19:48:51.498179 140612532332288 genomics_reader.py:213] Reading /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam with NativeSamReader. I1109 19:48:51.518172 140612532332288 make_examples.py:1075] Preparing inputs. [W::hts_idx_load2] The index file is older than the data file: /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bai. 2018-11-09 19:48:52.291229: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1109 19:48:52.291625 140612532332288 genomics_reader.py:213] Reading /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam with NativeSamReader. I1109 19:48:52.335163 140612532332288 make_examples.py:991] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']. [E::hts_open_format] **Failed to open file gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed**. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:414,availability,Error,Error,414,"Thanks Nima. I ran it again. Looks like the error is because it is unable to open the bed file I have provided. However, the bed file exists on gcp and my v0.6.1 code was able to access it. I am not sure what I am doing wrong. I was able to run the same code successfully if I provide the bed file in the example documentation (gs://deepvariant/exome-case-study-testdata/refseq.coding_exons.b37.extended50.bed). . Error file: gbsc-gcp-project-udn-dev-deep-variant/UDN668131_deepVariant_test4/deepvariant_staging_folder/logs/make_examples/0:. W::hts_idx_load2] The index file is older than the data file: /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bai. 2018-11-09 19:48:51.497793: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1109 19:48:51.498179 140612532332288 genomics_reader.py:213] Reading /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam with NativeSamReader. I1109 19:48:51.518172 140612532332288 make_examples.py:1075] Preparing inputs. [W::hts_idx_load2] The index file is older than the data file: /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bai. 2018-11-09 19:48:52.291229: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1109 19:48:52.291625 140612532332288 genomics_reader.py:213] Reading /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam with NativeSamReader. I1109 19:48:52.335163 140612532332288 make_examples.py:991] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']. [E::hts_open_format] **Failed to open file gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed**. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:5123,availability,operat,operation,5123,"unfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 429, in bed_parser. with bed.BedReader(filename) as fin:. File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 211, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 127, in _native_reader. return NativeBedReader(input_path, **kwargs). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 104, in __init__. self._reader = bed_reader.BedReader.from_file(bed_path, options). ValueError: Not found: Could not open gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. parallel: This job failed:. Using mount point: /input-gcsfused-48. Opening GCS connection... Opening bucket... Mounting file system... File system has been successfully mounted. mkdir -p ./input-gcsfused-48 && gcsfuse --implicit-dirs gbsc-gcp-project-udn-dev-deep-variant /input-gcsfused-48 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/gbsc-gcp-project-udn-dev-deep-variant/UDN668131_deepVariant_test4/deepvariant_staging_folder/examples/examples_output.tfrecord@64.gz --reads /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam --ref /mnt/google/.google/input/gbsc-gcp-project-udn-dev-test/hs37d5_ref/hs37d5.fa --task 48 --regions gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. operation id: 3776708258517585322. $ gsutil ls gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. [deepvariant_v0.7.0_UDN668131_test.sh.txt](https://github.com/google/deepvariant/files/2567583/deepvariant_v0.7.0_UDN668131_test.sh.txt). Thanks,. Shruti.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:519,deployability,log,logs,519,"Thanks Nima. I ran it again. Looks like the error is because it is unable to open the bed file I have provided. However, the bed file exists on gcp and my v0.6.1 code was able to access it. I am not sure what I am doing wrong. I was able to run the same code successfully if I provide the bed file in the example documentation (gs://deepvariant/exome-case-study-testdata/refseq.coding_exons.b37.extended50.bed). . Error file: gbsc-gcp-project-udn-dev-deep-variant/UDN668131_deepVariant_test4/deepvariant_staging_folder/logs/make_examples/0:. W::hts_idx_load2] The index file is older than the data file: /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bai. 2018-11-09 19:48:51.497793: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1109 19:48:51.498179 140612532332288 genomics_reader.py:213] Reading /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam with NativeSamReader. I1109 19:48:51.518172 140612532332288 make_examples.py:1075] Preparing inputs. [W::hts_idx_load2] The index file is older than the data file: /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bai. 2018-11-09 19:48:52.291229: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1109 19:48:52.291625 140612532332288 genomics_reader.py:213] Reading /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam with NativeSamReader. I1109 19:48:52.335163 140612532332288 make_examples.py:991] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']. [E::hts_open_format] **Failed to open file gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed**. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:1673,deployability,Fail,Failed,1673,"09 19:48:51.497793: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1109 19:48:51.498179 140612532332288 genomics_reader.py:213] Reading /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam with NativeSamReader. I1109 19:48:51.518172 140612532332288 make_examples.py:1075] Preparing inputs. [W::hts_idx_load2] The index file is older than the data file: /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bai. 2018-11-09 19:48:52.291229: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1109 19:48:52.291625 140612532332288 genomics_reader.py:213] Reading /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam with NativeSamReader. I1109 19:48:52.335163 140612532332288 make_examples.py:991] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']. [E::hts_open_format] **Failed to open file gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed**. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1178, in main. make_examples_runner(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1076, in make_examples_runner. regions = processing_regions_from_options(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 993, in processing_regions_from_options. options.exclude_calling_regions). File ""/mnt/google/.goo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:1945,deployability,modul,module,1945,":51.518172 140612532332288 make_examples.py:1075] Preparing inputs. [W::hts_idx_load2] The index file is older than the data file: /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bai. 2018-11-09 19:48:52.291229: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1109 19:48:52.291625 140612532332288 genomics_reader.py:213] Reading /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam with NativeSamReader. I1109 19:48:52.335163 140612532332288 make_examples.py:991] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']. [E::hts_open_format] **Failed to open file gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed**. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1178, in main. make_examples_runner(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1076, in make_examples_runner. regions = processing_regions_from_options(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 993, in processing_regions_from_options. options.exclude_calling_regions). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 587, in build_calling_regions. ranges.RangeSet.from_regions(regions_to_include, contig_dict)). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvar",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:4369,deployability,fail,failed,4369,"ranges.py"", line 464, in from_regions. for elt in reader(region):. File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 429, in bed_parser. with bed.BedReader(filename) as fin:. File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 211, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 127, in _native_reader. return NativeBedReader(input_path, **kwargs). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 104, in __init__. self._reader = bed_reader.BedReader.from_file(bed_path, options). ValueError: Not found: Could not open gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. parallel: This job failed:. Using mount point: /input-gcsfused-48. Opening GCS connection... Opening bucket... Mounting file system... File system has been successfully mounted. mkdir -p ./input-gcsfused-48 && gcsfuse --implicit-dirs gbsc-gcp-project-udn-dev-deep-variant /input-gcsfused-48 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/gbsc-gcp-project-udn-dev-deep-variant/UDN668131_deepVariant_test4/deepvariant_staging_folder/examples/examples_output.tfrecord@64.gz --reads /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam --ref /mnt/google/.google/input/gbsc-gcp-project-udn-dev-test/hs37d5_ref/hs37d5.fa --task 48 --regions gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. operation id: 3776708258517585322. $ gsutil ls gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. [deepvariant_v0.7.0_UDN668131_test",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:2031,interoperability,platform,platform,2031,"The index file is older than the data file: /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bai. 2018-11-09 19:48:52.291229: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1109 19:48:52.291625 140612532332288 genomics_reader.py:213] Reading /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam with NativeSamReader. I1109 19:48:52.335163 140612532332288 make_examples.py:991] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']. [E::hts_open_format] **Failed to open file gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed**. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1178, in main. make_examples_runner(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1076, in make_examples_runner. regions = processing_regions_from_options(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 993, in processing_regions_from_options. options.exclude_calling_regions). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 587, in build_calling_regions. ranges.RangeSet.from_regions(regions_to_include, contig_dict)). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 161, in from_regions. return cls(ranges=",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:1945,modifiability,modul,module,1945,":51.518172 140612532332288 make_examples.py:1075] Preparing inputs. [W::hts_idx_load2] The index file is older than the data file: /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bai. 2018-11-09 19:48:52.291229: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1109 19:48:52.291625 140612532332288 genomics_reader.py:213] Reading /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam with NativeSamReader. I1109 19:48:52.335163 140612532332288 make_examples.py:991] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']. [E::hts_open_format] **Failed to open file gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed**. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1178, in main. make_examples_runner(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1076, in make_examples_runner. regions = processing_regions_from_options(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 993, in processing_regions_from_options. options.exclude_calling_regions). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 587, in build_calling_regions. ranges.RangeSet.from_regions(regions_to_include, contig_dict)). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvar",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:2004,modifiability,pac,packages,2004,"inputs. [W::hts_idx_load2] The index file is older than the data file: /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bai. 2018-11-09 19:48:52.291229: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1109 19:48:52.291625 140612532332288 genomics_reader.py:213] Reading /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam with NativeSamReader. I1109 19:48:52.335163 140612532332288 make_examples.py:991] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']. [E::hts_open_format] **Failed to open file gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed**. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1178, in main. make_examples_runner(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1076, in make_examples_runner. regions = processing_regions_from_options(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 993, in processing_regions_from_options. options.exclude_calling_regions). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 587, in build_calling_regions. ranges.RangeSet.from_regions(regions_to_include, contig_dict)). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 161, in from_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:44,performance,error,error,44,"Thanks Nima. I ran it again. Looks like the error is because it is unable to open the bed file I have provided. However, the bed file exists on gcp and my v0.6.1 code was able to access it. I am not sure what I am doing wrong. I was able to run the same code successfully if I provide the bed file in the example documentation (gs://deepvariant/exome-case-study-testdata/refseq.coding_exons.b37.extended50.bed). . Error file: gbsc-gcp-project-udn-dev-deep-variant/UDN668131_deepVariant_test4/deepvariant_staging_folder/logs/make_examples/0:. W::hts_idx_load2] The index file is older than the data file: /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bai. 2018-11-09 19:48:51.497793: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1109 19:48:51.498179 140612532332288 genomics_reader.py:213] Reading /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam with NativeSamReader. I1109 19:48:51.518172 140612532332288 make_examples.py:1075] Preparing inputs. [W::hts_idx_load2] The index file is older than the data file: /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bai. 2018-11-09 19:48:52.291229: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1109 19:48:52.291625 140612532332288 genomics_reader.py:213] Reading /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam with NativeSamReader. I1109 19:48:52.335163 140612532332288 make_examples.py:991] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']. [E::hts_open_format] **Failed to open file gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed**. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:414,performance,Error,Error,414,"Thanks Nima. I ran it again. Looks like the error is because it is unable to open the bed file I have provided. However, the bed file exists on gcp and my v0.6.1 code was able to access it. I am not sure what I am doing wrong. I was able to run the same code successfully if I provide the bed file in the example documentation (gs://deepvariant/exome-case-study-testdata/refseq.coding_exons.b37.extended50.bed). . Error file: gbsc-gcp-project-udn-dev-deep-variant/UDN668131_deepVariant_test4/deepvariant_staging_folder/logs/make_examples/0:. W::hts_idx_load2] The index file is older than the data file: /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bai. 2018-11-09 19:48:51.497793: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1109 19:48:51.498179 140612532332288 genomics_reader.py:213] Reading /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam with NativeSamReader. I1109 19:48:51.518172 140612532332288 make_examples.py:1075] Preparing inputs. [W::hts_idx_load2] The index file is older than the data file: /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bai. 2018-11-09 19:48:52.291229: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1109 19:48:52.291625 140612532332288 genomics_reader.py:213] Reading /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam with NativeSamReader. I1109 19:48:52.335163 140612532332288 make_examples.py:991] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']. [E::hts_open_format] **Failed to open file gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed**. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:4350,performance,parallel,parallel,4350,"arty/nucleus/util/ranges.py"", line 464, in from_regions. for elt in reader(region):. File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 429, in bed_parser. with bed.BedReader(filename) as fin:. File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 211, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 127, in _native_reader. return NativeBedReader(input_path, **kwargs). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 104, in __init__. self._reader = bed_reader.BedReader.from_file(bed_path, options). ValueError: Not found: Could not open gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. parallel: This job failed:. Using mount point: /input-gcsfused-48. Opening GCS connection... Opening bucket... Mounting file system... File system has been successfully mounted. mkdir -p ./input-gcsfused-48 && gcsfuse --implicit-dirs gbsc-gcp-project-udn-dev-deep-variant /input-gcsfused-48 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/gbsc-gcp-project-udn-dev-deep-variant/UDN668131_deepVariant_test4/deepvariant_staging_folder/examples/examples_output.tfrecord@64.gz --reads /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam --ref /mnt/google/.google/input/gbsc-gcp-project-udn-dev-test/hs37d5_ref/hs37d5.fa --task 48 --regions gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. operation id: 3776708258517585322. $ gsutil ls gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. [deepvariant_v0.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:1673,reliability,Fail,Failed,1673,"09 19:48:51.497793: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1109 19:48:51.498179 140612532332288 genomics_reader.py:213] Reading /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam with NativeSamReader. I1109 19:48:51.518172 140612532332288 make_examples.py:1075] Preparing inputs. [W::hts_idx_load2] The index file is older than the data file: /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bai. 2018-11-09 19:48:52.291229: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1109 19:48:52.291625 140612532332288 genomics_reader.py:213] Reading /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam with NativeSamReader. I1109 19:48:52.335163 140612532332288 make_examples.py:991] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']. [E::hts_open_format] **Failed to open file gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed**. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1178, in main. make_examples_runner(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1076, in make_examples_runner. regions = processing_regions_from_options(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 993, in processing_regions_from_options. options.exclude_calling_regions). File ""/mnt/google/.goo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:4369,reliability,fail,failed,4369,"ranges.py"", line 464, in from_regions. for elt in reader(region):. File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 429, in bed_parser. with bed.BedReader(filename) as fin:. File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 211, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 127, in _native_reader. return NativeBedReader(input_path, **kwargs). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 104, in __init__. self._reader = bed_reader.BedReader.from_file(bed_path, options). ValueError: Not found: Could not open gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. parallel: This job failed:. Using mount point: /input-gcsfused-48. Opening GCS connection... Opening bucket... Mounting file system... File system has been successfully mounted. mkdir -p ./input-gcsfused-48 && gcsfuse --implicit-dirs gbsc-gcp-project-udn-dev-deep-variant /input-gcsfused-48 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/gbsc-gcp-project-udn-dev-deep-variant/UDN668131_deepVariant_test4/deepvariant_staging_folder/examples/examples_output.tfrecord@64.gz --reads /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam --ref /mnt/google/.google/input/gbsc-gcp-project-udn-dev-test/hs37d5_ref/hs37d5.fa --task 48 --regions gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. operation id: 3776708258517585322. $ gsutil ls gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. [deepvariant_v0.7.0_UDN668131_test",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:44,safety,error,error,44,"Thanks Nima. I ran it again. Looks like the error is because it is unable to open the bed file I have provided. However, the bed file exists on gcp and my v0.6.1 code was able to access it. I am not sure what I am doing wrong. I was able to run the same code successfully if I provide the bed file in the example documentation (gs://deepvariant/exome-case-study-testdata/refseq.coding_exons.b37.extended50.bed). . Error file: gbsc-gcp-project-udn-dev-deep-variant/UDN668131_deepVariant_test4/deepvariant_staging_folder/logs/make_examples/0:. W::hts_idx_load2] The index file is older than the data file: /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bai. 2018-11-09 19:48:51.497793: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1109 19:48:51.498179 140612532332288 genomics_reader.py:213] Reading /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam with NativeSamReader. I1109 19:48:51.518172 140612532332288 make_examples.py:1075] Preparing inputs. [W::hts_idx_load2] The index file is older than the data file: /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bai. 2018-11-09 19:48:52.291229: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1109 19:48:52.291625 140612532332288 genomics_reader.py:213] Reading /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam with NativeSamReader. I1109 19:48:52.335163 140612532332288 make_examples.py:991] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']. [E::hts_open_format] **Failed to open file gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed**. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:362,safety,test,testdata,362,"Thanks Nima. I ran it again. Looks like the error is because it is unable to open the bed file I have provided. However, the bed file exists on gcp and my v0.6.1 code was able to access it. I am not sure what I am doing wrong. I was able to run the same code successfully if I provide the bed file in the example documentation (gs://deepvariant/exome-case-study-testdata/refseq.coding_exons.b37.extended50.bed). . Error file: gbsc-gcp-project-udn-dev-deep-variant/UDN668131_deepVariant_test4/deepvariant_staging_folder/logs/make_examples/0:. W::hts_idx_load2] The index file is older than the data file: /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bai. 2018-11-09 19:48:51.497793: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1109 19:48:51.498179 140612532332288 genomics_reader.py:213] Reading /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam with NativeSamReader. I1109 19:48:51.518172 140612532332288 make_examples.py:1075] Preparing inputs. [W::hts_idx_load2] The index file is older than the data file: /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bai. 2018-11-09 19:48:52.291229: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1109 19:48:52.291625 140612532332288 genomics_reader.py:213] Reading /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam with NativeSamReader. I1109 19:48:52.335163 140612532332288 make_examples.py:991] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']. [E::hts_open_format] **Failed to open file gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed**. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:414,safety,Error,Error,414,"Thanks Nima. I ran it again. Looks like the error is because it is unable to open the bed file I have provided. However, the bed file exists on gcp and my v0.6.1 code was able to access it. I am not sure what I am doing wrong. I was able to run the same code successfully if I provide the bed file in the example documentation (gs://deepvariant/exome-case-study-testdata/refseq.coding_exons.b37.extended50.bed). . Error file: gbsc-gcp-project-udn-dev-deep-variant/UDN668131_deepVariant_test4/deepvariant_staging_folder/logs/make_examples/0:. W::hts_idx_load2] The index file is older than the data file: /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bai. 2018-11-09 19:48:51.497793: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1109 19:48:51.498179 140612532332288 genomics_reader.py:213] Reading /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam with NativeSamReader. I1109 19:48:51.518172 140612532332288 make_examples.py:1075] Preparing inputs. [W::hts_idx_load2] The index file is older than the data file: /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bai. 2018-11-09 19:48:52.291229: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1109 19:48:52.291625 140612532332288 genomics_reader.py:213] Reading /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam with NativeSamReader. I1109 19:48:52.335163 140612532332288 make_examples.py:991] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']. [E::hts_open_format] **Failed to open file gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed**. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:519,safety,log,logs,519,"Thanks Nima. I ran it again. Looks like the error is because it is unable to open the bed file I have provided. However, the bed file exists on gcp and my v0.6.1 code was able to access it. I am not sure what I am doing wrong. I was able to run the same code successfully if I provide the bed file in the example documentation (gs://deepvariant/exome-case-study-testdata/refseq.coding_exons.b37.extended50.bed). . Error file: gbsc-gcp-project-udn-dev-deep-variant/UDN668131_deepVariant_test4/deepvariant_staging_folder/logs/make_examples/0:. W::hts_idx_load2] The index file is older than the data file: /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bai. 2018-11-09 19:48:51.497793: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1109 19:48:51.498179 140612532332288 genomics_reader.py:213] Reading /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam with NativeSamReader. I1109 19:48:51.518172 140612532332288 make_examples.py:1075] Preparing inputs. [W::hts_idx_load2] The index file is older than the data file: /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bai. 2018-11-09 19:48:52.291229: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1109 19:48:52.291625 140612532332288 genomics_reader.py:213] Reading /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam with NativeSamReader. I1109 19:48:52.335163 140612532332288 make_examples.py:991] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']. [E::hts_open_format] **Failed to open file gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed**. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:605,safety,input,input-gcsfused-,605,"Thanks Nima. I ran it again. Looks like the error is because it is unable to open the bed file I have provided. However, the bed file exists on gcp and my v0.6.1 code was able to access it. I am not sure what I am doing wrong. I was able to run the same code successfully if I provide the bed file in the example documentation (gs://deepvariant/exome-case-study-testdata/refseq.coding_exons.b37.extended50.bed). . Error file: gbsc-gcp-project-udn-dev-deep-variant/UDN668131_deepVariant_test4/deepvariant_staging_folder/logs/make_examples/0:. W::hts_idx_load2] The index file is older than the data file: /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bai. 2018-11-09 19:48:51.497793: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1109 19:48:51.498179 140612532332288 genomics_reader.py:213] Reading /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam with NativeSamReader. I1109 19:48:51.518172 140612532332288 make_examples.py:1075] Preparing inputs. [W::hts_idx_load2] The index file is older than the data file: /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bai. 2018-11-09 19:48:52.291229: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1109 19:48:52.291625 140612532332288 genomics_reader.py:213] Reading /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam with NativeSamReader. I1109 19:48:52.335163 140612532332288 make_examples.py:991] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']. [E::hts_open_format] **Failed to open file gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed**. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:853,safety,input,input-gcsfused-,853,"Thanks Nima. I ran it again. Looks like the error is because it is unable to open the bed file I have provided. However, the bed file exists on gcp and my v0.6.1 code was able to access it. I am not sure what I am doing wrong. I was able to run the same code successfully if I provide the bed file in the example documentation (gs://deepvariant/exome-case-study-testdata/refseq.coding_exons.b37.extended50.bed). . Error file: gbsc-gcp-project-udn-dev-deep-variant/UDN668131_deepVariant_test4/deepvariant_staging_folder/logs/make_examples/0:. W::hts_idx_load2] The index file is older than the data file: /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bai. 2018-11-09 19:48:51.497793: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1109 19:48:51.498179 140612532332288 genomics_reader.py:213] Reading /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam with NativeSamReader. I1109 19:48:51.518172 140612532332288 make_examples.py:1075] Preparing inputs. [W::hts_idx_load2] The index file is older than the data file: /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bai. 2018-11-09 19:48:52.291229: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1109 19:48:52.291625 140612532332288 genomics_reader.py:213] Reading /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam with NativeSamReader. I1109 19:48:52.335163 140612532332288 make_examples.py:991] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']. [E::hts_open_format] **Failed to open file gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed**. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:1008,safety,input,inputs,1008,". I ran it again. Looks like the error is because it is unable to open the bed file I have provided. However, the bed file exists on gcp and my v0.6.1 code was able to access it. I am not sure what I am doing wrong. I was able to run the same code successfully if I provide the bed file in the example documentation (gs://deepvariant/exome-case-study-testdata/refseq.coding_exons.b37.extended50.bed). . Error file: gbsc-gcp-project-udn-dev-deep-variant/UDN668131_deepVariant_test4/deepvariant_staging_folder/logs/make_examples/0:. W::hts_idx_load2] The index file is older than the data file: /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bai. 2018-11-09 19:48:51.497793: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1109 19:48:51.498179 140612532332288 genomics_reader.py:213] Reading /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam with NativeSamReader. I1109 19:48:51.518172 140612532332288 make_examples.py:1075] Preparing inputs. [W::hts_idx_load2] The index file is older than the data file: /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bai. 2018-11-09 19:48:52.291229: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1109 19:48:52.291625 140612532332288 genomics_reader.py:213] Reading /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam with NativeSamReader. I1109 19:48:52.335163 140612532332288 make_examples.py:991] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']. [E::hts_open_format] **Failed to open file gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed**. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-package",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:1080,safety,input,input-gcsfused-,1080,"d file I have provided. However, the bed file exists on gcp and my v0.6.1 code was able to access it. I am not sure what I am doing wrong. I was able to run the same code successfully if I provide the bed file in the example documentation (gs://deepvariant/exome-case-study-testdata/refseq.coding_exons.b37.extended50.bed). . Error file: gbsc-gcp-project-udn-dev-deep-variant/UDN668131_deepVariant_test4/deepvariant_staging_folder/logs/make_examples/0:. W::hts_idx_load2] The index file is older than the data file: /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bai. 2018-11-09 19:48:51.497793: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1109 19:48:51.498179 140612532332288 genomics_reader.py:213] Reading /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam with NativeSamReader. I1109 19:48:51.518172 140612532332288 make_examples.py:1075] Preparing inputs. [W::hts_idx_load2] The index file is older than the data file: /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bai. 2018-11-09 19:48:52.291229: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1109 19:48:52.291625 140612532332288 genomics_reader.py:213] Reading /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam with NativeSamReader. I1109 19:48:52.335163 140612532332288 make_examples.py:991] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']. [E::hts_open_format] **Failed to open file gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed**. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv))",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:1328,safety,input,input-gcsfused-,1328,"pvariant/exome-case-study-testdata/refseq.coding_exons.b37.extended50.bed). . Error file: gbsc-gcp-project-udn-dev-deep-variant/UDN668131_deepVariant_test4/deepvariant_staging_folder/logs/make_examples/0:. W::hts_idx_load2] The index file is older than the data file: /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bai. 2018-11-09 19:48:51.497793: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1109 19:48:51.498179 140612532332288 genomics_reader.py:213] Reading /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam with NativeSamReader. I1109 19:48:51.518172 140612532332288 make_examples.py:1075] Preparing inputs. [W::hts_idx_load2] The index file is older than the data file: /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bai. 2018-11-09 19:48:52.291229: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1109 19:48:52.291625 140612532332288 genomics_reader.py:213] Reading /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam with NativeSamReader. I1109 19:48:52.335163 140612532332288 make_examples.py:991] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']. [E::hts_open_format] **Failed to open file gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed**. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1178, in main. make_examples_runner(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvari",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:1723,safety,test,test,1723,"reader.cc:537] Unrecognized SAM header type, ignoring: . I1109 19:48:51.498179 140612532332288 genomics_reader.py:213] Reading /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam with NativeSamReader. I1109 19:48:51.518172 140612532332288 make_examples.py:1075] Preparing inputs. [W::hts_idx_load2] The index file is older than the data file: /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bai. 2018-11-09 19:48:52.291229: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1109 19:48:52.291625 140612532332288 genomics_reader.py:213] Reading /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam with NativeSamReader. I1109 19:48:52.335163 140612532332288 make_examples.py:991] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']. [E::hts_open_format] **Failed to open file gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed**. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1178, in main. make_examples_runner(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1076, in make_examples_runner. regions = processing_regions_from_options(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 993, in processing_regions_from_options. options.exclude_calling_regions). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:1945,safety,modul,module,1945,":51.518172 140612532332288 make_examples.py:1075] Preparing inputs. [W::hts_idx_load2] The index file is older than the data file: /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bai. 2018-11-09 19:48:52.291229: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1109 19:48:52.291625 140612532332288 genomics_reader.py:213] Reading /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam with NativeSamReader. I1109 19:48:52.335163 140612532332288 make_examples.py:991] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']. [E::hts_open_format] **Failed to open file gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed**. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1178, in main. make_examples_runner(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1076, in make_examples_runner. regions = processing_regions_from_options(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 993, in processing_regions_from_options. options.exclude_calling_regions). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 587, in build_calling_regions. ranges.RangeSet.from_regions(regions_to_include, contig_dict)). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvar",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:4296,safety,test,test,4296,".runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 464, in from_regions. for elt in reader(region):. File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 429, in bed_parser. with bed.BedReader(filename) as fin:. File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 211, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 127, in _native_reader. return NativeBedReader(input_path, **kwargs). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 104, in __init__. self._reader = bed_reader.BedReader.from_file(bed_path, options). ValueError: Not found: Could not open gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. parallel: This job failed:. Using mount point: /input-gcsfused-48. Opening GCS connection... Opening bucket... Mounting file system... File system has been successfully mounted. mkdir -p ./input-gcsfused-48 && gcsfuse --implicit-dirs gbsc-gcp-project-udn-dev-deep-variant /input-gcsfused-48 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/gbsc-gcp-project-udn-dev-deep-variant/UDN668131_deepVariant_test4/deepvariant_staging_folder/examples/examples_output.tfrecord@64.gz --reads /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam --ref /mnt/google/.google/input/gbsc-gcp-project-udn-dev-test/hs37d5_ref/hs37d5.fa --task 48 --regions gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. operation id: 3776708258517585322. $ gsutil ls gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. gs://gbsc-gcp-project-udn-dev-test/VCRome_2_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:4398,safety,input,input-gcsfused-,4398,"ons. for elt in reader(region):. File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 429, in bed_parser. with bed.BedReader(filename) as fin:. File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 211, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 127, in _native_reader. return NativeBedReader(input_path, **kwargs). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 104, in __init__. self._reader = bed_reader.BedReader.from_file(bed_path, options). ValueError: Not found: Could not open gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. parallel: This job failed:. Using mount point: /input-gcsfused-48. Opening GCS connection... Opening bucket... Mounting file system... File system has been successfully mounted. mkdir -p ./input-gcsfused-48 && gcsfuse --implicit-dirs gbsc-gcp-project-udn-dev-deep-variant /input-gcsfused-48 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/gbsc-gcp-project-udn-dev-deep-variant/UDN668131_deepVariant_test4/deepvariant_staging_folder/examples/examples_output.tfrecord@64.gz --reads /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam --ref /mnt/google/.google/input/gbsc-gcp-project-udn-dev-test/hs37d5_ref/hs37d5.fa --task 48 --regions gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. operation id: 3776708258517585322. $ gsutil ls gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. [deepvariant_v0.7.0_UDN668131_test.sh.txt](https://github.com/google",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:4539,safety,input,input-gcsfused-,4539,"unfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 429, in bed_parser. with bed.BedReader(filename) as fin:. File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 211, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 127, in _native_reader. return NativeBedReader(input_path, **kwargs). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 104, in __init__. self._reader = bed_reader.BedReader.from_file(bed_path, options). ValueError: Not found: Could not open gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. parallel: This job failed:. Using mount point: /input-gcsfused-48. Opening GCS connection... Opening bucket... Mounting file system... File system has been successfully mounted. mkdir -p ./input-gcsfused-48 && gcsfuse --implicit-dirs gbsc-gcp-project-udn-dev-deep-variant /input-gcsfused-48 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/gbsc-gcp-project-udn-dev-deep-variant/UDN668131_deepVariant_test4/deepvariant_staging_folder/examples/examples_output.tfrecord@64.gz --reads /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam --ref /mnt/google/.google/input/gbsc-gcp-project-udn-dev-test/hs37d5_ref/hs37d5.fa --task 48 --regions gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. operation id: 3776708258517585322. $ gsutil ls gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. [deepvariant_v0.7.0_UDN668131_test.sh.txt](https://github.com/google/deepvariant/files/2567583/deepvariant_v0.7.0_UDN668131_test.sh.txt). Thanks,. Shruti.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:4623,safety,input,input-gcsfused-,4623,"unfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 429, in bed_parser. with bed.BedReader(filename) as fin:. File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 211, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 127, in _native_reader. return NativeBedReader(input_path, **kwargs). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 104, in __init__. self._reader = bed_reader.BedReader.from_file(bed_path, options). ValueError: Not found: Could not open gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. parallel: This job failed:. Using mount point: /input-gcsfused-48. Opening GCS connection... Opening bucket... Mounting file system... File system has been successfully mounted. mkdir -p ./input-gcsfused-48 && gcsfuse --implicit-dirs gbsc-gcp-project-udn-dev-deep-variant /input-gcsfused-48 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/gbsc-gcp-project-udn-dev-deep-variant/UDN668131_deepVariant_test4/deepvariant_staging_folder/examples/examples_output.tfrecord@64.gz --reads /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam --ref /mnt/google/.google/input/gbsc-gcp-project-udn-dev-test/hs37d5_ref/hs37d5.fa --task 48 --regions gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. operation id: 3776708258517585322. $ gsutil ls gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. [deepvariant_v0.7.0_UDN668131_test.sh.txt](https://github.com/google/deepvariant/files/2567583/deepvariant_v0.7.0_UDN668131_test.sh.txt). Thanks,. Shruti.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:4874,safety,input,input-gcsfused-,4874,"unfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 429, in bed_parser. with bed.BedReader(filename) as fin:. File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 211, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 127, in _native_reader. return NativeBedReader(input_path, **kwargs). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 104, in __init__. self._reader = bed_reader.BedReader.from_file(bed_path, options). ValueError: Not found: Could not open gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. parallel: This job failed:. Using mount point: /input-gcsfused-48. Opening GCS connection... Opening bucket... Mounting file system... File system has been successfully mounted. mkdir -p ./input-gcsfused-48 && gcsfuse --implicit-dirs gbsc-gcp-project-udn-dev-deep-variant /input-gcsfused-48 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/gbsc-gcp-project-udn-dev-deep-variant/UDN668131_deepVariant_test4/deepvariant_staging_folder/examples/examples_output.tfrecord@64.gz --reads /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam --ref /mnt/google/.google/input/gbsc-gcp-project-udn-dev-test/hs37d5_ref/hs37d5.fa --task 48 --regions gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. operation id: 3776708258517585322. $ gsutil ls gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. [deepvariant_v0.7.0_UDN668131_test.sh.txt](https://github.com/google/deepvariant/files/2567583/deepvariant_v0.7.0_UDN668131_test.sh.txt). Thanks,. Shruti.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:4962,safety,input,input,4962,"unfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 429, in bed_parser. with bed.BedReader(filename) as fin:. File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 211, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 127, in _native_reader. return NativeBedReader(input_path, **kwargs). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 104, in __init__. self._reader = bed_reader.BedReader.from_file(bed_path, options). ValueError: Not found: Could not open gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. parallel: This job failed:. Using mount point: /input-gcsfused-48. Opening GCS connection... Opening bucket... Mounting file system... File system has been successfully mounted. mkdir -p ./input-gcsfused-48 && gcsfuse --implicit-dirs gbsc-gcp-project-udn-dev-deep-variant /input-gcsfused-48 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/gbsc-gcp-project-udn-dev-deep-variant/UDN668131_deepVariant_test4/deepvariant_staging_folder/examples/examples_output.tfrecord@64.gz --reads /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam --ref /mnt/google/.google/input/gbsc-gcp-project-udn-dev-test/hs37d5_ref/hs37d5.fa --task 48 --regions gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. operation id: 3776708258517585322. $ gsutil ls gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. [deepvariant_v0.7.0_UDN668131_test.sh.txt](https://github.com/google/deepvariant/files/2567583/deepvariant_v0.7.0_UDN668131_test.sh.txt). Thanks,. Shruti.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:4993,safety,test,test,4993,"unfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 429, in bed_parser. with bed.BedReader(filename) as fin:. File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 211, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 127, in _native_reader. return NativeBedReader(input_path, **kwargs). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 104, in __init__. self._reader = bed_reader.BedReader.from_file(bed_path, options). ValueError: Not found: Could not open gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. parallel: This job failed:. Using mount point: /input-gcsfused-48. Opening GCS connection... Opening bucket... Mounting file system... File system has been successfully mounted. mkdir -p ./input-gcsfused-48 && gcsfuse --implicit-dirs gbsc-gcp-project-udn-dev-deep-variant /input-gcsfused-48 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/gbsc-gcp-project-udn-dev-deep-variant/UDN668131_deepVariant_test4/deepvariant_staging_folder/examples/examples_output.tfrecord@64.gz --reads /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam --ref /mnt/google/.google/input/gbsc-gcp-project-udn-dev-test/hs37d5_ref/hs37d5.fa --task 48 --regions gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. operation id: 3776708258517585322. $ gsutil ls gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. [deepvariant_v0.7.0_UDN668131_test.sh.txt](https://github.com/google/deepvariant/files/2567583/deepvariant_v0.7.0_UDN668131_test.sh.txt). Thanks,. Shruti.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:5069,safety,test,test,5069,"unfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 429, in bed_parser. with bed.BedReader(filename) as fin:. File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 211, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 127, in _native_reader. return NativeBedReader(input_path, **kwargs). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 104, in __init__. self._reader = bed_reader.BedReader.from_file(bed_path, options). ValueError: Not found: Could not open gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. parallel: This job failed:. Using mount point: /input-gcsfused-48. Opening GCS connection... Opening bucket... Mounting file system... File system has been successfully mounted. mkdir -p ./input-gcsfused-48 && gcsfuse --implicit-dirs gbsc-gcp-project-udn-dev-deep-variant /input-gcsfused-48 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/gbsc-gcp-project-udn-dev-deep-variant/UDN668131_deepVariant_test4/deepvariant_staging_folder/examples/examples_output.tfrecord@64.gz --reads /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam --ref /mnt/google/.google/input/gbsc-gcp-project-udn-dev-test/hs37d5_ref/hs37d5.fa --task 48 --regions gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. operation id: 3776708258517585322. $ gsutil ls gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. [deepvariant_v0.7.0_UDN668131_test.sh.txt](https://github.com/google/deepvariant/files/2567583/deepvariant_v0.7.0_UDN668131_test.sh.txt). Thanks,. Shruti.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:5200,safety,test,test,5200,"unfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 429, in bed_parser. with bed.BedReader(filename) as fin:. File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 211, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 127, in _native_reader. return NativeBedReader(input_path, **kwargs). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 104, in __init__. self._reader = bed_reader.BedReader.from_file(bed_path, options). ValueError: Not found: Could not open gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. parallel: This job failed:. Using mount point: /input-gcsfused-48. Opening GCS connection... Opening bucket... Mounting file system... File system has been successfully mounted. mkdir -p ./input-gcsfused-48 && gcsfuse --implicit-dirs gbsc-gcp-project-udn-dev-deep-variant /input-gcsfused-48 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/gbsc-gcp-project-udn-dev-deep-variant/UDN668131_deepVariant_test4/deepvariant_staging_folder/examples/examples_output.tfrecord@64.gz --reads /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam --ref /mnt/google/.google/input/gbsc-gcp-project-udn-dev-test/hs37d5_ref/hs37d5.fa --task 48 --regions gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. operation id: 3776708258517585322. $ gsutil ls gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. [deepvariant_v0.7.0_UDN668131_test.sh.txt](https://github.com/google/deepvariant/files/2567583/deepvariant_v0.7.0_UDN668131_test.sh.txt). Thanks,. Shruti.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:5284,safety,test,test,5284,"unfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 429, in bed_parser. with bed.BedReader(filename) as fin:. File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 211, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 127, in _native_reader. return NativeBedReader(input_path, **kwargs). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 104, in __init__. self._reader = bed_reader.BedReader.from_file(bed_path, options). ValueError: Not found: Could not open gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. parallel: This job failed:. Using mount point: /input-gcsfused-48. Opening GCS connection... Opening bucket... Mounting file system... File system has been successfully mounted. mkdir -p ./input-gcsfused-48 && gcsfuse --implicit-dirs gbsc-gcp-project-udn-dev-deep-variant /input-gcsfused-48 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/gbsc-gcp-project-udn-dev-deep-variant/UDN668131_deepVariant_test4/deepvariant_staging_folder/examples/examples_output.tfrecord@64.gz --reads /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam --ref /mnt/google/.google/input/gbsc-gcp-project-udn-dev-test/hs37d5_ref/hs37d5.fa --task 48 --regions gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. operation id: 3776708258517585322. $ gsutil ls gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. [deepvariant_v0.7.0_UDN668131_test.sh.txt](https://github.com/google/deepvariant/files/2567583/deepvariant_v0.7.0_UDN668131_test.sh.txt). Thanks,. Shruti.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:179,security,access,access,179,"Thanks Nima. I ran it again. Looks like the error is because it is unable to open the bed file I have provided. However, the bed file exists on gcp and my v0.6.1 code was able to access it. I am not sure what I am doing wrong. I was able to run the same code successfully if I provide the bed file in the example documentation (gs://deepvariant/exome-case-study-testdata/refseq.coding_exons.b37.extended50.bed). . Error file: gbsc-gcp-project-udn-dev-deep-variant/UDN668131_deepVariant_test4/deepvariant_staging_folder/logs/make_examples/0:. W::hts_idx_load2] The index file is older than the data file: /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bai. 2018-11-09 19:48:51.497793: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1109 19:48:51.498179 140612532332288 genomics_reader.py:213] Reading /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam with NativeSamReader. I1109 19:48:51.518172 140612532332288 make_examples.py:1075] Preparing inputs. [W::hts_idx_load2] The index file is older than the data file: /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bai. 2018-11-09 19:48:52.291229: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1109 19:48:52.291625 140612532332288 genomics_reader.py:213] Reading /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam with NativeSamReader. I1109 19:48:52.335163 140612532332288 make_examples.py:991] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']. [E::hts_open_format] **Failed to open file gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed**. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:519,security,log,logs,519,"Thanks Nima. I ran it again. Looks like the error is because it is unable to open the bed file I have provided. However, the bed file exists on gcp and my v0.6.1 code was able to access it. I am not sure what I am doing wrong. I was able to run the same code successfully if I provide the bed file in the example documentation (gs://deepvariant/exome-case-study-testdata/refseq.coding_exons.b37.extended50.bed). . Error file: gbsc-gcp-project-udn-dev-deep-variant/UDN668131_deepVariant_test4/deepvariant_staging_folder/logs/make_examples/0:. W::hts_idx_load2] The index file is older than the data file: /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bai. 2018-11-09 19:48:51.497793: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1109 19:48:51.498179 140612532332288 genomics_reader.py:213] Reading /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam with NativeSamReader. I1109 19:48:51.518172 140612532332288 make_examples.py:1075] Preparing inputs. [W::hts_idx_load2] The index file is older than the data file: /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bai. 2018-11-09 19:48:52.291229: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1109 19:48:52.291625 140612532332288 genomics_reader.py:213] Reading /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam with NativeSamReader. I1109 19:48:52.335163 140612532332288 make_examples.py:991] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']. [E::hts_open_format] **Failed to open file gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed**. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:362,testability,test,testdata,362,"Thanks Nima. I ran it again. Looks like the error is because it is unable to open the bed file I have provided. However, the bed file exists on gcp and my v0.6.1 code was able to access it. I am not sure what I am doing wrong. I was able to run the same code successfully if I provide the bed file in the example documentation (gs://deepvariant/exome-case-study-testdata/refseq.coding_exons.b37.extended50.bed). . Error file: gbsc-gcp-project-udn-dev-deep-variant/UDN668131_deepVariant_test4/deepvariant_staging_folder/logs/make_examples/0:. W::hts_idx_load2] The index file is older than the data file: /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bai. 2018-11-09 19:48:51.497793: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1109 19:48:51.498179 140612532332288 genomics_reader.py:213] Reading /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam with NativeSamReader. I1109 19:48:51.518172 140612532332288 make_examples.py:1075] Preparing inputs. [W::hts_idx_load2] The index file is older than the data file: /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bai. 2018-11-09 19:48:52.291229: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1109 19:48:52.291625 140612532332288 genomics_reader.py:213] Reading /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam with NativeSamReader. I1109 19:48:52.335163 140612532332288 make_examples.py:991] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']. [E::hts_open_format] **Failed to open file gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed**. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:519,testability,log,logs,519,"Thanks Nima. I ran it again. Looks like the error is because it is unable to open the bed file I have provided. However, the bed file exists on gcp and my v0.6.1 code was able to access it. I am not sure what I am doing wrong. I was able to run the same code successfully if I provide the bed file in the example documentation (gs://deepvariant/exome-case-study-testdata/refseq.coding_exons.b37.extended50.bed). . Error file: gbsc-gcp-project-udn-dev-deep-variant/UDN668131_deepVariant_test4/deepvariant_staging_folder/logs/make_examples/0:. W::hts_idx_load2] The index file is older than the data file: /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bai. 2018-11-09 19:48:51.497793: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1109 19:48:51.498179 140612532332288 genomics_reader.py:213] Reading /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam with NativeSamReader. I1109 19:48:51.518172 140612532332288 make_examples.py:1075] Preparing inputs. [W::hts_idx_load2] The index file is older than the data file: /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bai. 2018-11-09 19:48:52.291229: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1109 19:48:52.291625 140612532332288 genomics_reader.py:213] Reading /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam with NativeSamReader. I1109 19:48:52.335163 140612532332288 make_examples.py:991] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']. [E::hts_open_format] **Failed to open file gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed**. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:1723,testability,test,test,1723,"reader.cc:537] Unrecognized SAM header type, ignoring: . I1109 19:48:51.498179 140612532332288 genomics_reader.py:213] Reading /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam with NativeSamReader. I1109 19:48:51.518172 140612532332288 make_examples.py:1075] Preparing inputs. [W::hts_idx_load2] The index file is older than the data file: /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bai. 2018-11-09 19:48:52.291229: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1109 19:48:52.291625 140612532332288 genomics_reader.py:213] Reading /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam with NativeSamReader. I1109 19:48:52.335163 140612532332288 make_examples.py:991] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']. [E::hts_open_format] **Failed to open file gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed**. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1178, in main. make_examples_runner(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1076, in make_examples_runner. regions = processing_regions_from_options(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 993, in processing_regions_from_options. options.exclude_calling_regions). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:1779,testability,Trace,Traceback,1779,"109 19:48:51.498179 140612532332288 genomics_reader.py:213] Reading /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam with NativeSamReader. I1109 19:48:51.518172 140612532332288 make_examples.py:1075] Preparing inputs. [W::hts_idx_load2] The index file is older than the data file: /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bai. 2018-11-09 19:48:52.291229: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1109 19:48:52.291625 140612532332288 genomics_reader.py:213] Reading /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam with NativeSamReader. I1109 19:48:52.335163 140612532332288 make_examples.py:991] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']. [E::hts_open_format] **Failed to open file gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed**. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1178, in main. make_examples_runner(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1076, in make_examples_runner. regions = processing_regions_from_options(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 993, in processing_regions_from_options. options.exclude_calling_regions). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 587, in bu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:4296,testability,test,test,4296,".runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 464, in from_regions. for elt in reader(region):. File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 429, in bed_parser. with bed.BedReader(filename) as fin:. File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 211, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 127, in _native_reader. return NativeBedReader(input_path, **kwargs). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 104, in __init__. self._reader = bed_reader.BedReader.from_file(bed_path, options). ValueError: Not found: Could not open gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. parallel: This job failed:. Using mount point: /input-gcsfused-48. Opening GCS connection... Opening bucket... Mounting file system... File system has been successfully mounted. mkdir -p ./input-gcsfused-48 && gcsfuse --implicit-dirs gbsc-gcp-project-udn-dev-deep-variant /input-gcsfused-48 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/gbsc-gcp-project-udn-dev-deep-variant/UDN668131_deepVariant_test4/deepvariant_staging_folder/examples/examples_output.tfrecord@64.gz --reads /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam --ref /mnt/google/.google/input/gbsc-gcp-project-udn-dev-test/hs37d5_ref/hs37d5.fa --task 48 --regions gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. operation id: 3776708258517585322. $ gsutil ls gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. gs://gbsc-gcp-project-udn-dev-test/VCRome_2_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:4993,testability,test,test,4993,"unfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 429, in bed_parser. with bed.BedReader(filename) as fin:. File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 211, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 127, in _native_reader. return NativeBedReader(input_path, **kwargs). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 104, in __init__. self._reader = bed_reader.BedReader.from_file(bed_path, options). ValueError: Not found: Could not open gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. parallel: This job failed:. Using mount point: /input-gcsfused-48. Opening GCS connection... Opening bucket... Mounting file system... File system has been successfully mounted. mkdir -p ./input-gcsfused-48 && gcsfuse --implicit-dirs gbsc-gcp-project-udn-dev-deep-variant /input-gcsfused-48 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/gbsc-gcp-project-udn-dev-deep-variant/UDN668131_deepVariant_test4/deepvariant_staging_folder/examples/examples_output.tfrecord@64.gz --reads /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam --ref /mnt/google/.google/input/gbsc-gcp-project-udn-dev-test/hs37d5_ref/hs37d5.fa --task 48 --regions gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. operation id: 3776708258517585322. $ gsutil ls gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. [deepvariant_v0.7.0_UDN668131_test.sh.txt](https://github.com/google/deepvariant/files/2567583/deepvariant_v0.7.0_UDN668131_test.sh.txt). Thanks,. Shruti.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:5069,testability,test,test,5069,"unfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 429, in bed_parser. with bed.BedReader(filename) as fin:. File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 211, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 127, in _native_reader. return NativeBedReader(input_path, **kwargs). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 104, in __init__. self._reader = bed_reader.BedReader.from_file(bed_path, options). ValueError: Not found: Could not open gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. parallel: This job failed:. Using mount point: /input-gcsfused-48. Opening GCS connection... Opening bucket... Mounting file system... File system has been successfully mounted. mkdir -p ./input-gcsfused-48 && gcsfuse --implicit-dirs gbsc-gcp-project-udn-dev-deep-variant /input-gcsfused-48 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/gbsc-gcp-project-udn-dev-deep-variant/UDN668131_deepVariant_test4/deepvariant_staging_folder/examples/examples_output.tfrecord@64.gz --reads /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam --ref /mnt/google/.google/input/gbsc-gcp-project-udn-dev-test/hs37d5_ref/hs37d5.fa --task 48 --regions gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. operation id: 3776708258517585322. $ gsutil ls gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. [deepvariant_v0.7.0_UDN668131_test.sh.txt](https://github.com/google/deepvariant/files/2567583/deepvariant_v0.7.0_UDN668131_test.sh.txt). Thanks,. Shruti.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:5200,testability,test,test,5200,"unfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 429, in bed_parser. with bed.BedReader(filename) as fin:. File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 211, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 127, in _native_reader. return NativeBedReader(input_path, **kwargs). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 104, in __init__. self._reader = bed_reader.BedReader.from_file(bed_path, options). ValueError: Not found: Could not open gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. parallel: This job failed:. Using mount point: /input-gcsfused-48. Opening GCS connection... Opening bucket... Mounting file system... File system has been successfully mounted. mkdir -p ./input-gcsfused-48 && gcsfuse --implicit-dirs gbsc-gcp-project-udn-dev-deep-variant /input-gcsfused-48 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/gbsc-gcp-project-udn-dev-deep-variant/UDN668131_deepVariant_test4/deepvariant_staging_folder/examples/examples_output.tfrecord@64.gz --reads /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam --ref /mnt/google/.google/input/gbsc-gcp-project-udn-dev-test/hs37d5_ref/hs37d5.fa --task 48 --regions gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. operation id: 3776708258517585322. $ gsutil ls gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. [deepvariant_v0.7.0_UDN668131_test.sh.txt](https://github.com/google/deepvariant/files/2567583/deepvariant_v0.7.0_UDN668131_test.sh.txt). Thanks,. Shruti.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:5284,testability,test,test,5284,"unfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 429, in bed_parser. with bed.BedReader(filename) as fin:. File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 211, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 127, in _native_reader. return NativeBedReader(input_path, **kwargs). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 104, in __init__. self._reader = bed_reader.BedReader.from_file(bed_path, options). ValueError: Not found: Could not open gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. parallel: This job failed:. Using mount point: /input-gcsfused-48. Opening GCS connection... Opening bucket... Mounting file system... File system has been successfully mounted. mkdir -p ./input-gcsfused-48 && gcsfuse --implicit-dirs gbsc-gcp-project-udn-dev-deep-variant /input-gcsfused-48 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/gbsc-gcp-project-udn-dev-deep-variant/UDN668131_deepVariant_test4/deepvariant_staging_folder/examples/examples_output.tfrecord@64.gz --reads /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam --ref /mnt/google/.google/input/gbsc-gcp-project-udn-dev-test/hs37d5_ref/hs37d5.fa --task 48 --regions gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. operation id: 3776708258517585322. $ gsutil ls gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. [deepvariant_v0.7.0_UDN668131_test.sh.txt](https://github.com/google/deepvariant/files/2567583/deepvariant_v0.7.0_UDN668131_test.sh.txt). Thanks,. Shruti.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:44,usability,error,error,44,"Thanks Nima. I ran it again. Looks like the error is because it is unable to open the bed file I have provided. However, the bed file exists on gcp and my v0.6.1 code was able to access it. I am not sure what I am doing wrong. I was able to run the same code successfully if I provide the bed file in the example documentation (gs://deepvariant/exome-case-study-testdata/refseq.coding_exons.b37.extended50.bed). . Error file: gbsc-gcp-project-udn-dev-deep-variant/UDN668131_deepVariant_test4/deepvariant_staging_folder/logs/make_examples/0:. W::hts_idx_load2] The index file is older than the data file: /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bai. 2018-11-09 19:48:51.497793: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1109 19:48:51.498179 140612532332288 genomics_reader.py:213] Reading /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam with NativeSamReader. I1109 19:48:51.518172 140612532332288 make_examples.py:1075] Preparing inputs. [W::hts_idx_load2] The index file is older than the data file: /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bai. 2018-11-09 19:48:52.291229: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1109 19:48:52.291625 140612532332288 genomics_reader.py:213] Reading /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam with NativeSamReader. I1109 19:48:52.335163 140612532332288 make_examples.py:991] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']. [E::hts_open_format] **Failed to open file gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed**. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:313,usability,document,documentation,313,"Thanks Nima. I ran it again. Looks like the error is because it is unable to open the bed file I have provided. However, the bed file exists on gcp and my v0.6.1 code was able to access it. I am not sure what I am doing wrong. I was able to run the same code successfully if I provide the bed file in the example documentation (gs://deepvariant/exome-case-study-testdata/refseq.coding_exons.b37.extended50.bed). . Error file: gbsc-gcp-project-udn-dev-deep-variant/UDN668131_deepVariant_test4/deepvariant_staging_folder/logs/make_examples/0:. W::hts_idx_load2] The index file is older than the data file: /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bai. 2018-11-09 19:48:51.497793: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1109 19:48:51.498179 140612532332288 genomics_reader.py:213] Reading /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam with NativeSamReader. I1109 19:48:51.518172 140612532332288 make_examples.py:1075] Preparing inputs. [W::hts_idx_load2] The index file is older than the data file: /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bai. 2018-11-09 19:48:52.291229: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1109 19:48:52.291625 140612532332288 genomics_reader.py:213] Reading /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam with NativeSamReader. I1109 19:48:52.335163 140612532332288 make_examples.py:991] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']. [E::hts_open_format] **Failed to open file gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed**. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:414,usability,Error,Error,414,"Thanks Nima. I ran it again. Looks like the error is because it is unable to open the bed file I have provided. However, the bed file exists on gcp and my v0.6.1 code was able to access it. I am not sure what I am doing wrong. I was able to run the same code successfully if I provide the bed file in the example documentation (gs://deepvariant/exome-case-study-testdata/refseq.coding_exons.b37.extended50.bed). . Error file: gbsc-gcp-project-udn-dev-deep-variant/UDN668131_deepVariant_test4/deepvariant_staging_folder/logs/make_examples/0:. W::hts_idx_load2] The index file is older than the data file: /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bai. 2018-11-09 19:48:51.497793: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1109 19:48:51.498179 140612532332288 genomics_reader.py:213] Reading /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam with NativeSamReader. I1109 19:48:51.518172 140612532332288 make_examples.py:1075] Preparing inputs. [W::hts_idx_load2] The index file is older than the data file: /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bai. 2018-11-09 19:48:52.291229: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1109 19:48:52.291625 140612532332288 genomics_reader.py:213] Reading /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam with NativeSamReader. I1109 19:48:52.335163 140612532332288 make_examples.py:991] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']. [E::hts_open_format] **Failed to open file gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed**. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:605,usability,input,input-gcsfused-,605,"Thanks Nima. I ran it again. Looks like the error is because it is unable to open the bed file I have provided. However, the bed file exists on gcp and my v0.6.1 code was able to access it. I am not sure what I am doing wrong. I was able to run the same code successfully if I provide the bed file in the example documentation (gs://deepvariant/exome-case-study-testdata/refseq.coding_exons.b37.extended50.bed). . Error file: gbsc-gcp-project-udn-dev-deep-variant/UDN668131_deepVariant_test4/deepvariant_staging_folder/logs/make_examples/0:. W::hts_idx_load2] The index file is older than the data file: /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bai. 2018-11-09 19:48:51.497793: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1109 19:48:51.498179 140612532332288 genomics_reader.py:213] Reading /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam with NativeSamReader. I1109 19:48:51.518172 140612532332288 make_examples.py:1075] Preparing inputs. [W::hts_idx_load2] The index file is older than the data file: /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bai. 2018-11-09 19:48:52.291229: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1109 19:48:52.291625 140612532332288 genomics_reader.py:213] Reading /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam with NativeSamReader. I1109 19:48:52.335163 140612532332288 make_examples.py:991] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']. [E::hts_open_format] **Failed to open file gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed**. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:853,usability,input,input-gcsfused-,853,"Thanks Nima. I ran it again. Looks like the error is because it is unable to open the bed file I have provided. However, the bed file exists on gcp and my v0.6.1 code was able to access it. I am not sure what I am doing wrong. I was able to run the same code successfully if I provide the bed file in the example documentation (gs://deepvariant/exome-case-study-testdata/refseq.coding_exons.b37.extended50.bed). . Error file: gbsc-gcp-project-udn-dev-deep-variant/UDN668131_deepVariant_test4/deepvariant_staging_folder/logs/make_examples/0:. W::hts_idx_load2] The index file is older than the data file: /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bai. 2018-11-09 19:48:51.497793: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1109 19:48:51.498179 140612532332288 genomics_reader.py:213] Reading /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam with NativeSamReader. I1109 19:48:51.518172 140612532332288 make_examples.py:1075] Preparing inputs. [W::hts_idx_load2] The index file is older than the data file: /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bai. 2018-11-09 19:48:52.291229: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1109 19:48:52.291625 140612532332288 genomics_reader.py:213] Reading /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam with NativeSamReader. I1109 19:48:52.335163 140612532332288 make_examples.py:991] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']. [E::hts_open_format] **Failed to open file gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed**. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:1008,usability,input,inputs,1008,". I ran it again. Looks like the error is because it is unable to open the bed file I have provided. However, the bed file exists on gcp and my v0.6.1 code was able to access it. I am not sure what I am doing wrong. I was able to run the same code successfully if I provide the bed file in the example documentation (gs://deepvariant/exome-case-study-testdata/refseq.coding_exons.b37.extended50.bed). . Error file: gbsc-gcp-project-udn-dev-deep-variant/UDN668131_deepVariant_test4/deepvariant_staging_folder/logs/make_examples/0:. W::hts_idx_load2] The index file is older than the data file: /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bai. 2018-11-09 19:48:51.497793: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1109 19:48:51.498179 140612532332288 genomics_reader.py:213] Reading /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam with NativeSamReader. I1109 19:48:51.518172 140612532332288 make_examples.py:1075] Preparing inputs. [W::hts_idx_load2] The index file is older than the data file: /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bai. 2018-11-09 19:48:52.291229: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1109 19:48:52.291625 140612532332288 genomics_reader.py:213] Reading /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam with NativeSamReader. I1109 19:48:52.335163 140612532332288 make_examples.py:991] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']. [E::hts_open_format] **Failed to open file gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed**. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-package",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:1080,usability,input,input-gcsfused-,1080,"d file I have provided. However, the bed file exists on gcp and my v0.6.1 code was able to access it. I am not sure what I am doing wrong. I was able to run the same code successfully if I provide the bed file in the example documentation (gs://deepvariant/exome-case-study-testdata/refseq.coding_exons.b37.extended50.bed). . Error file: gbsc-gcp-project-udn-dev-deep-variant/UDN668131_deepVariant_test4/deepvariant_staging_folder/logs/make_examples/0:. W::hts_idx_load2] The index file is older than the data file: /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bai. 2018-11-09 19:48:51.497793: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1109 19:48:51.498179 140612532332288 genomics_reader.py:213] Reading /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam with NativeSamReader. I1109 19:48:51.518172 140612532332288 make_examples.py:1075] Preparing inputs. [W::hts_idx_load2] The index file is older than the data file: /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bai. 2018-11-09 19:48:52.291229: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1109 19:48:52.291625 140612532332288 genomics_reader.py:213] Reading /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam with NativeSamReader. I1109 19:48:52.335163 140612532332288 make_examples.py:991] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']. [E::hts_open_format] **Failed to open file gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed**. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv))",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:1328,usability,input,input-gcsfused-,1328,"pvariant/exome-case-study-testdata/refseq.coding_exons.b37.extended50.bed). . Error file: gbsc-gcp-project-udn-dev-deep-variant/UDN668131_deepVariant_test4/deepvariant_staging_folder/logs/make_examples/0:. W::hts_idx_load2] The index file is older than the data file: /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bai. 2018-11-09 19:48:51.497793: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1109 19:48:51.498179 140612532332288 genomics_reader.py:213] Reading /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam with NativeSamReader. I1109 19:48:51.518172 140612532332288 make_examples.py:1075] Preparing inputs. [W::hts_idx_load2] The index file is older than the data file: /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bai. 2018-11-09 19:48:52.291229: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: . I1109 19:48:52.291625 140612532332288 genomics_reader.py:213] Reading /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam with NativeSamReader. I1109 19:48:52.335163 140612532332288 make_examples.py:991] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']. [E::hts_open_format] **Failed to open file gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed**. Traceback (most recent call last):. File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1178, in main. make_examples_runner(options). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvari",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:4398,usability,input,input-gcsfused-,4398,"ons. for elt in reader(region):. File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 429, in bed_parser. with bed.BedReader(filename) as fin:. File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 211, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 127, in _native_reader. return NativeBedReader(input_path, **kwargs). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 104, in __init__. self._reader = bed_reader.BedReader.from_file(bed_path, options). ValueError: Not found: Could not open gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. parallel: This job failed:. Using mount point: /input-gcsfused-48. Opening GCS connection... Opening bucket... Mounting file system... File system has been successfully mounted. mkdir -p ./input-gcsfused-48 && gcsfuse --implicit-dirs gbsc-gcp-project-udn-dev-deep-variant /input-gcsfused-48 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/gbsc-gcp-project-udn-dev-deep-variant/UDN668131_deepVariant_test4/deepvariant_staging_folder/examples/examples_output.tfrecord@64.gz --reads /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam --ref /mnt/google/.google/input/gbsc-gcp-project-udn-dev-test/hs37d5_ref/hs37d5.fa --task 48 --regions gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. operation id: 3776708258517585322. $ gsutil ls gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. [deepvariant_v0.7.0_UDN668131_test.sh.txt](https://github.com/google",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:4539,usability,input,input-gcsfused-,4539,"unfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 429, in bed_parser. with bed.BedReader(filename) as fin:. File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 211, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 127, in _native_reader. return NativeBedReader(input_path, **kwargs). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 104, in __init__. self._reader = bed_reader.BedReader.from_file(bed_path, options). ValueError: Not found: Could not open gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. parallel: This job failed:. Using mount point: /input-gcsfused-48. Opening GCS connection... Opening bucket... Mounting file system... File system has been successfully mounted. mkdir -p ./input-gcsfused-48 && gcsfuse --implicit-dirs gbsc-gcp-project-udn-dev-deep-variant /input-gcsfused-48 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/gbsc-gcp-project-udn-dev-deep-variant/UDN668131_deepVariant_test4/deepvariant_staging_folder/examples/examples_output.tfrecord@64.gz --reads /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam --ref /mnt/google/.google/input/gbsc-gcp-project-udn-dev-test/hs37d5_ref/hs37d5.fa --task 48 --regions gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. operation id: 3776708258517585322. $ gsutil ls gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. [deepvariant_v0.7.0_UDN668131_test.sh.txt](https://github.com/google/deepvariant/files/2567583/deepvariant_v0.7.0_UDN668131_test.sh.txt). Thanks,. Shruti.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:4623,usability,input,input-gcsfused-,4623,"unfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 429, in bed_parser. with bed.BedReader(filename) as fin:. File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 211, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 127, in _native_reader. return NativeBedReader(input_path, **kwargs). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 104, in __init__. self._reader = bed_reader.BedReader.from_file(bed_path, options). ValueError: Not found: Could not open gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. parallel: This job failed:. Using mount point: /input-gcsfused-48. Opening GCS connection... Opening bucket... Mounting file system... File system has been successfully mounted. mkdir -p ./input-gcsfused-48 && gcsfuse --implicit-dirs gbsc-gcp-project-udn-dev-deep-variant /input-gcsfused-48 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/gbsc-gcp-project-udn-dev-deep-variant/UDN668131_deepVariant_test4/deepvariant_staging_folder/examples/examples_output.tfrecord@64.gz --reads /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam --ref /mnt/google/.google/input/gbsc-gcp-project-udn-dev-test/hs37d5_ref/hs37d5.fa --task 48 --regions gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. operation id: 3776708258517585322. $ gsutil ls gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. [deepvariant_v0.7.0_UDN668131_test.sh.txt](https://github.com/google/deepvariant/files/2567583/deepvariant_v0.7.0_UDN668131_test.sh.txt). Thanks,. Shruti.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:4874,usability,input,input-gcsfused-,4874,"unfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 429, in bed_parser. with bed.BedReader(filename) as fin:. File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 211, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 127, in _native_reader. return NativeBedReader(input_path, **kwargs). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 104, in __init__. self._reader = bed_reader.BedReader.from_file(bed_path, options). ValueError: Not found: Could not open gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. parallel: This job failed:. Using mount point: /input-gcsfused-48. Opening GCS connection... Opening bucket... Mounting file system... File system has been successfully mounted. mkdir -p ./input-gcsfused-48 && gcsfuse --implicit-dirs gbsc-gcp-project-udn-dev-deep-variant /input-gcsfused-48 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/gbsc-gcp-project-udn-dev-deep-variant/UDN668131_deepVariant_test4/deepvariant_staging_folder/examples/examples_output.tfrecord@64.gz --reads /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam --ref /mnt/google/.google/input/gbsc-gcp-project-udn-dev-test/hs37d5_ref/hs37d5.fa --task 48 --regions gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. operation id: 3776708258517585322. $ gsutil ls gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. [deepvariant_v0.7.0_UDN668131_test.sh.txt](https://github.com/google/deepvariant/files/2567583/deepvariant_v0.7.0_UDN668131_test.sh.txt). Thanks,. Shruti.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:4962,usability,input,input,4962,"unfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 429, in bed_parser. with bed.BedReader(filename) as fin:. File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 211, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 127, in _native_reader. return NativeBedReader(input_path, **kwargs). File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 104, in __init__. self._reader = bed_reader.BedReader.from_file(bed_path, options). ValueError: Not found: Could not open gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. parallel: This job failed:. Using mount point: /input-gcsfused-48. Opening GCS connection... Opening bucket... Mounting file system... File system has been successfully mounted. mkdir -p ./input-gcsfused-48 && gcsfuse --implicit-dirs gbsc-gcp-project-udn-dev-deep-variant /input-gcsfused-48 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/gbsc-gcp-project-udn-dev-deep-variant/UDN668131_deepVariant_test4/deepvariant_staging_folder/examples/examples_output.tfrecord@64.gz --reads /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam --ref /mnt/google/.google/input/gbsc-gcp-project-udn-dev-test/hs37d5_ref/hs37d5.fa --task 48 --regions gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. operation id: 3776708258517585322. $ gsutil ls gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. [deepvariant_v0.7.0_UDN668131_test.sh.txt](https://github.com/google/deepvariant/files/2567583/deepvariant_v0.7.0_UDN668131_test.sh.txt). Thanks,. Shruti.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:62,deployability,updat,update,62,"We are working on this, please check related bug #116 for the update.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:62,safety,updat,update,62,"We are working on this, please check related bug #116 for the update.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:62,security,updat,update,62,"We are working on this, please check related bug #116 for the update.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:28,deployability,automat,automated,28,Thanks Nima. Can you (or an automated message) let me know once this will be fixed.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:38,integrability,messag,message,38,Thanks Nima. Can you (or an automated message) let me know once this will be fixed.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:38,interoperability,messag,message,38,Thanks Nima. Can you (or an automated message) let me know once this will be fixed.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:28,testability,automat,automated,28,Thanks Nima. Can you (or an automated message) let me know once this will be fixed.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/118:107,deployability,releas,release,107,"Hi @ShrutiMarwaha . The fix for https://github.com/google/deepvariant/issues/119 is included in the v0.7.2 release. If you still see any issues, let us know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/118
https://github.com/google/deepvariant/issues/119:50,deployability,releas,release,50,"The fix is done, and will be included in the next release. . For the ref: Htslib is not able to read from private gcs bucket and there is a TODO for it. . https://github.com/samtools/htslib/blob/9cd85ab512a74fe42af31fc6321c70203d161ed6/hfile_gcs.c#L75.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/119
https://github.com/google/deepvariant/issues/119:33,deployability,releas,release,33,This fixed is included in v0.7.2 release:. https://github.com/google/deepvariant/releases/tag/v0.7.2,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/119
https://github.com/google/deepvariant/issues/119:81,deployability,releas,releases,81,This fixed is included in v0.7.2 release:. https://github.com/google/deepvariant/releases/tag/v0.7.2,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/119
https://github.com/google/deepvariant/issues/120:31,deployability,log,log,31,"Hi, can you provide the worker log as well? (See this example : https://github.com/google/deepvariant/issues/118#issuecomment-437114999 ).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/120
https://github.com/google/deepvariant/issues/120:31,safety,log,log,31,"Hi, can you provide the worker log as well? (See this example : https://github.com/google/deepvariant/issues/118#issuecomment-437114999 ).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/120
https://github.com/google/deepvariant/issues/120:31,security,log,log,31,"Hi, can you provide the worker log as well? (See this example : https://github.com/google/deepvariant/issues/118#issuecomment-437114999 ).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/120
https://github.com/google/deepvariant/issues/120:31,testability,log,log,31,"Hi, can you provide the worker log as well? (See this example : https://github.com/google/deepvariant/issues/118#issuecomment-437114999 ).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/120
https://github.com/google/deepvariant/issues/120:137,availability,error,error,137,"Dear Pi-Chuan,. Thanks for pointing out the worker log files since I didn't realize the files are log files. . It is really helpful! The error comes from the inappropriate bam index file name I used (*.bai instead of *.bam.bai) as the log file describes: . CommandException: No URLs matched: gs://input_bam/IO_045.sam_sorted_dedup.bam.bai. After changing the bam index file name, the pipeline works now! . Thank you so much for your help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/120
https://github.com/google/deepvariant/issues/120:51,deployability,log,log,51,"Dear Pi-Chuan,. Thanks for pointing out the worker log files since I didn't realize the files are log files. . It is really helpful! The error comes from the inappropriate bam index file name I used (*.bai instead of *.bam.bai) as the log file describes: . CommandException: No URLs matched: gs://input_bam/IO_045.sam_sorted_dedup.bam.bai. After changing the bam index file name, the pipeline works now! . Thank you so much for your help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/120
https://github.com/google/deepvariant/issues/120:98,deployability,log,log,98,"Dear Pi-Chuan,. Thanks for pointing out the worker log files since I didn't realize the files are log files. . It is really helpful! The error comes from the inappropriate bam index file name I used (*.bai instead of *.bam.bai) as the log file describes: . CommandException: No URLs matched: gs://input_bam/IO_045.sam_sorted_dedup.bam.bai. After changing the bam index file name, the pipeline works now! . Thank you so much for your help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/120
https://github.com/google/deepvariant/issues/120:235,deployability,log,log,235,"Dear Pi-Chuan,. Thanks for pointing out the worker log files since I didn't realize the files are log files. . It is really helpful! The error comes from the inappropriate bam index file name I used (*.bai instead of *.bam.bai) as the log file describes: . CommandException: No URLs matched: gs://input_bam/IO_045.sam_sorted_dedup.bam.bai. After changing the bam index file name, the pipeline works now! . Thank you so much for your help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/120
https://github.com/google/deepvariant/issues/120:384,deployability,pipelin,pipeline,384,"Dear Pi-Chuan,. Thanks for pointing out the worker log files since I didn't realize the files are log files. . It is really helpful! The error comes from the inappropriate bam index file name I used (*.bai instead of *.bam.bai) as the log file describes: . CommandException: No URLs matched: gs://input_bam/IO_045.sam_sorted_dedup.bam.bai. After changing the bam index file name, the pipeline works now! . Thank you so much for your help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/120
https://github.com/google/deepvariant/issues/120:384,integrability,pipelin,pipeline,384,"Dear Pi-Chuan,. Thanks for pointing out the worker log files since I didn't realize the files are log files. . It is really helpful! The error comes from the inappropriate bam index file name I used (*.bai instead of *.bam.bai) as the log file describes: . CommandException: No URLs matched: gs://input_bam/IO_045.sam_sorted_dedup.bam.bai. After changing the bam index file name, the pipeline works now! . Thank you so much for your help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/120
https://github.com/google/deepvariant/issues/120:137,performance,error,error,137,"Dear Pi-Chuan,. Thanks for pointing out the worker log files since I didn't realize the files are log files. . It is really helpful! The error comes from the inappropriate bam index file name I used (*.bai instead of *.bam.bai) as the log file describes: . CommandException: No URLs matched: gs://input_bam/IO_045.sam_sorted_dedup.bam.bai. After changing the bam index file name, the pipeline works now! . Thank you so much for your help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/120
https://github.com/google/deepvariant/issues/120:51,safety,log,log,51,"Dear Pi-Chuan,. Thanks for pointing out the worker log files since I didn't realize the files are log files. . It is really helpful! The error comes from the inappropriate bam index file name I used (*.bai instead of *.bam.bai) as the log file describes: . CommandException: No URLs matched: gs://input_bam/IO_045.sam_sorted_dedup.bam.bai. After changing the bam index file name, the pipeline works now! . Thank you so much for your help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/120
https://github.com/google/deepvariant/issues/120:98,safety,log,log,98,"Dear Pi-Chuan,. Thanks for pointing out the worker log files since I didn't realize the files are log files. . It is really helpful! The error comes from the inappropriate bam index file name I used (*.bai instead of *.bam.bai) as the log file describes: . CommandException: No URLs matched: gs://input_bam/IO_045.sam_sorted_dedup.bam.bai. After changing the bam index file name, the pipeline works now! . Thank you so much for your help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/120
https://github.com/google/deepvariant/issues/120:137,safety,error,error,137,"Dear Pi-Chuan,. Thanks for pointing out the worker log files since I didn't realize the files are log files. . It is really helpful! The error comes from the inappropriate bam index file name I used (*.bai instead of *.bam.bai) as the log file describes: . CommandException: No URLs matched: gs://input_bam/IO_045.sam_sorted_dedup.bam.bai. After changing the bam index file name, the pipeline works now! . Thank you so much for your help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/120
https://github.com/google/deepvariant/issues/120:235,safety,log,log,235,"Dear Pi-Chuan,. Thanks for pointing out the worker log files since I didn't realize the files are log files. . It is really helpful! The error comes from the inappropriate bam index file name I used (*.bai instead of *.bam.bai) as the log file describes: . CommandException: No URLs matched: gs://input_bam/IO_045.sam_sorted_dedup.bam.bai. After changing the bam index file name, the pipeline works now! . Thank you so much for your help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/120
https://github.com/google/deepvariant/issues/120:51,security,log,log,51,"Dear Pi-Chuan,. Thanks for pointing out the worker log files since I didn't realize the files are log files. . It is really helpful! The error comes from the inappropriate bam index file name I used (*.bai instead of *.bam.bai) as the log file describes: . CommandException: No URLs matched: gs://input_bam/IO_045.sam_sorted_dedup.bam.bai. After changing the bam index file name, the pipeline works now! . Thank you so much for your help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/120
https://github.com/google/deepvariant/issues/120:98,security,log,log,98,"Dear Pi-Chuan,. Thanks for pointing out the worker log files since I didn't realize the files are log files. . It is really helpful! The error comes from the inappropriate bam index file name I used (*.bai instead of *.bam.bai) as the log file describes: . CommandException: No URLs matched: gs://input_bam/IO_045.sam_sorted_dedup.bam.bai. After changing the bam index file name, the pipeline works now! . Thank you so much for your help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/120
https://github.com/google/deepvariant/issues/120:235,security,log,log,235,"Dear Pi-Chuan,. Thanks for pointing out the worker log files since I didn't realize the files are log files. . It is really helpful! The error comes from the inappropriate bam index file name I used (*.bai instead of *.bam.bai) as the log file describes: . CommandException: No URLs matched: gs://input_bam/IO_045.sam_sorted_dedup.bam.bai. After changing the bam index file name, the pipeline works now! . Thank you so much for your help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/120
https://github.com/google/deepvariant/issues/120:51,testability,log,log,51,"Dear Pi-Chuan,. Thanks for pointing out the worker log files since I didn't realize the files are log files. . It is really helpful! The error comes from the inappropriate bam index file name I used (*.bai instead of *.bam.bai) as the log file describes: . CommandException: No URLs matched: gs://input_bam/IO_045.sam_sorted_dedup.bam.bai. After changing the bam index file name, the pipeline works now! . Thank you so much for your help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/120
https://github.com/google/deepvariant/issues/120:98,testability,log,log,98,"Dear Pi-Chuan,. Thanks for pointing out the worker log files since I didn't realize the files are log files. . It is really helpful! The error comes from the inappropriate bam index file name I used (*.bai instead of *.bam.bai) as the log file describes: . CommandException: No URLs matched: gs://input_bam/IO_045.sam_sorted_dedup.bam.bai. After changing the bam index file name, the pipeline works now! . Thank you so much for your help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/120
https://github.com/google/deepvariant/issues/120:235,testability,log,log,235,"Dear Pi-Chuan,. Thanks for pointing out the worker log files since I didn't realize the files are log files. . It is really helpful! The error comes from the inappropriate bam index file name I used (*.bai instead of *.bam.bai) as the log file describes: . CommandException: No URLs matched: gs://input_bam/IO_045.sam_sorted_dedup.bam.bai. After changing the bam index file name, the pipeline works now! . Thank you so much for your help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/120
https://github.com/google/deepvariant/issues/120:124,usability,help,helpful,124,"Dear Pi-Chuan,. Thanks for pointing out the worker log files since I didn't realize the files are log files. . It is really helpful! The error comes from the inappropriate bam index file name I used (*.bai instead of *.bam.bai) as the log file describes: . CommandException: No URLs matched: gs://input_bam/IO_045.sam_sorted_dedup.bam.bai. After changing the bam index file name, the pipeline works now! . Thank you so much for your help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/120
https://github.com/google/deepvariant/issues/120:137,usability,error,error,137,"Dear Pi-Chuan,. Thanks for pointing out the worker log files since I didn't realize the files are log files. . It is really helpful! The error comes from the inappropriate bam index file name I used (*.bai instead of *.bam.bai) as the log file describes: . CommandException: No URLs matched: gs://input_bam/IO_045.sam_sorted_dedup.bam.bai. After changing the bam index file name, the pipeline works now! . Thank you so much for your help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/120
https://github.com/google/deepvariant/issues/120:257,usability,Command,CommandException,257,"Dear Pi-Chuan,. Thanks for pointing out the worker log files since I didn't realize the files are log files. . It is really helpful! The error comes from the inappropriate bam index file name I used (*.bai instead of *.bam.bai) as the log file describes: . CommandException: No URLs matched: gs://input_bam/IO_045.sam_sorted_dedup.bam.bai. After changing the bam index file name, the pipeline works now! . Thank you so much for your help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/120
https://github.com/google/deepvariant/issues/120:433,usability,help,help,433,"Dear Pi-Chuan,. Thanks for pointing out the worker log files since I didn't realize the files are log files. . It is really helpful! The error comes from the inappropriate bam index file name I used (*.bai instead of *.bam.bai) as the log file describes: . CommandException: No URLs matched: gs://input_bam/IO_045.sam_sorted_dedup.bam.bai. After changing the bam index file name, the pipeline works now! . Thank you so much for your help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/120
https://github.com/google/deepvariant/issues/121:211,interoperability,specif,specific,211,"Hi,. Our team has not used or trained DeepVariant on non-diploid data. I think a similar question might have been asked before, so it's worth doing a quick search to see if there are relevant ideas. But nothing specific from the DeepVariant team. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/121
https://github.com/google/deepvariant/issues/121:9,security,team,team,9,"Hi,. Our team has not used or trained DeepVariant on non-diploid data. I think a similar question might have been asked before, so it's worth doing a quick search to see if there are relevant ideas. But nothing specific from the DeepVariant team. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/121
https://github.com/google/deepvariant/issues/121:241,security,team,team,241,"Hi,. Our team has not used or trained DeepVariant on non-diploid data. I think a similar question might have been asked before, so it's worth doing a quick search to see if there are relevant ideas. But nothing specific from the DeepVariant team. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/121
https://github.com/google/deepvariant/issues/122:24,deployability,version,versions,24,We built CLIF for a few versions of Linux. By hard coding DV_PLATFORM it would mean you're using that version. Unfortunately we don't support all versions of Unix. You can see if you can build clif yourself. This is unfortunately outside our support scope right now.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/122
https://github.com/google/deepvariant/issues/122:102,deployability,version,version,102,We built CLIF for a few versions of Linux. By hard coding DV_PLATFORM it would mean you're using that version. Unfortunately we don't support all versions of Unix. You can see if you can build clif yourself. This is unfortunately outside our support scope right now.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/122
https://github.com/google/deepvariant/issues/122:146,deployability,version,versions,146,We built CLIF for a few versions of Linux. By hard coding DV_PLATFORM it would mean you're using that version. Unfortunately we don't support all versions of Unix. You can see if you can build clif yourself. This is unfortunately outside our support scope right now.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/122
https://github.com/google/deepvariant/issues/122:187,deployability,build,build,187,We built CLIF for a few versions of Linux. By hard coding DV_PLATFORM it would mean you're using that version. Unfortunately we don't support all versions of Unix. You can see if you can build clif yourself. This is unfortunately outside our support scope right now.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/122
https://github.com/google/deepvariant/issues/122:24,integrability,version,versions,24,We built CLIF for a few versions of Linux. By hard coding DV_PLATFORM it would mean you're using that version. Unfortunately we don't support all versions of Unix. You can see if you can build clif yourself. This is unfortunately outside our support scope right now.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/122
https://github.com/google/deepvariant/issues/122:102,integrability,version,version,102,We built CLIF for a few versions of Linux. By hard coding DV_PLATFORM it would mean you're using that version. Unfortunately we don't support all versions of Unix. You can see if you can build clif yourself. This is unfortunately outside our support scope right now.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/122
https://github.com/google/deepvariant/issues/122:146,integrability,version,versions,146,We built CLIF for a few versions of Linux. By hard coding DV_PLATFORM it would mean you're using that version. Unfortunately we don't support all versions of Unix. You can see if you can build clif yourself. This is unfortunately outside our support scope right now.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/122
https://github.com/google/deepvariant/issues/122:24,modifiability,version,versions,24,We built CLIF for a few versions of Linux. By hard coding DV_PLATFORM it would mean you're using that version. Unfortunately we don't support all versions of Unix. You can see if you can build clif yourself. This is unfortunately outside our support scope right now.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/122
https://github.com/google/deepvariant/issues/122:102,modifiability,version,version,102,We built CLIF for a few versions of Linux. By hard coding DV_PLATFORM it would mean you're using that version. Unfortunately we don't support all versions of Unix. You can see if you can build clif yourself. This is unfortunately outside our support scope right now.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/122
https://github.com/google/deepvariant/issues/122:146,modifiability,version,versions,146,We built CLIF for a few versions of Linux. By hard coding DV_PLATFORM it would mean you're using that version. Unfortunately we don't support all versions of Unix. You can see if you can build clif yourself. This is unfortunately outside our support scope right now.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/122
https://github.com/google/deepvariant/issues/122:134,usability,support,support,134,We built CLIF for a few versions of Linux. By hard coding DV_PLATFORM it would mean you're using that version. Unfortunately we don't support all versions of Unix. You can see if you can build clif yourself. This is unfortunately outside our support scope right now.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/122
https://github.com/google/deepvariant/issues/122:242,usability,support,support,242,We built CLIF for a few versions of Linux. By hard coding DV_PLATFORM it would mean you're using that version. Unfortunately we don't support all versions of Unix. You can see if you can build clif yourself. This is unfortunately outside our support scope right now.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/122
https://github.com/google/deepvariant/issues/123:580,availability,down,down,580,"Hi @DiableJambe, . We use libssw (https://github.com/mengyao/Complete-Striped-Smith-Waterman-Library) for our local alignment. This code directly uses emmintrin.h in ssw.c so there's no easy way to remove it for the powerpc architecture. That said, there appears to be a pull request to add PPC64 support (https://github.com/mengyao/Complete-Striped-Smith-Waterman-Library/pull/35) but this has been pending for more than a year. We'd be more than happy to update libssw if you can get an implementation in there that doesn't require SSE intrinsics. Let us know if you want to go down that route.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:457,deployability,updat,update,457,"Hi @DiableJambe, . We use libssw (https://github.com/mengyao/Complete-Striped-Smith-Waterman-Library) for our local alignment. This code directly uses emmintrin.h in ssw.c so there's no easy way to remove it for the powerpc architecture. That said, there appears to be a pull request to add PPC64 support (https://github.com/mengyao/Complete-Striped-Smith-Waterman-Library/pull/35) but this has been pending for more than a year. We'd be more than happy to update libssw if you can get an implementation in there that doesn't require SSE intrinsics. Let us know if you want to go down that route.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:216,energy efficiency,power,powerpc,216,"Hi @DiableJambe, . We use libssw (https://github.com/mengyao/Complete-Striped-Smith-Waterman-Library) for our local alignment. This code directly uses emmintrin.h in ssw.c so there's no easy way to remove it for the powerpc architecture. That said, there appears to be a pull request to add PPC64 support (https://github.com/mengyao/Complete-Striped-Smith-Waterman-Library/pull/35) but this has been pending for more than a year. We'd be more than happy to update libssw if you can get an implementation in there that doesn't require SSE intrinsics. Let us know if you want to go down that route.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:590,integrability,rout,route,590,"Hi @DiableJambe, . We use libssw (https://github.com/mengyao/Complete-Striped-Smith-Waterman-Library) for our local alignment. This code directly uses emmintrin.h in ssw.c so there's no easy way to remove it for the powerpc architecture. That said, there appears to be a pull request to add PPC64 support (https://github.com/mengyao/Complete-Striped-Smith-Waterman-Library/pull/35) but this has been pending for more than a year. We'd be more than happy to update libssw if you can get an implementation in there that doesn't require SSE intrinsics. Let us know if you want to go down that route.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:224,interoperability,architectur,architecture,224,"Hi @DiableJambe, . We use libssw (https://github.com/mengyao/Complete-Striped-Smith-Waterman-Library) for our local alignment. This code directly uses emmintrin.h in ssw.c so there's no easy way to remove it for the powerpc architecture. That said, there appears to be a pull request to add PPC64 support (https://github.com/mengyao/Complete-Striped-Smith-Waterman-Library/pull/35) but this has been pending for more than a year. We'd be more than happy to update libssw if you can get an implementation in there that doesn't require SSE intrinsics. Let us know if you want to go down that route.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:518,reliability,doe,doesn,518,"Hi @DiableJambe, . We use libssw (https://github.com/mengyao/Complete-Striped-Smith-Waterman-Library) for our local alignment. This code directly uses emmintrin.h in ssw.c so there's no easy way to remove it for the powerpc architecture. That said, there appears to be a pull request to add PPC64 support (https://github.com/mengyao/Complete-Striped-Smith-Waterman-Library/pull/35) but this has been pending for more than a year. We'd be more than happy to update libssw if you can get an implementation in there that doesn't require SSE intrinsics. Let us know if you want to go down that route.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:61,safety,Compl,Complete-Striped-Smith-Waterman-Library,61,"Hi @DiableJambe, . We use libssw (https://github.com/mengyao/Complete-Striped-Smith-Waterman-Library) for our local alignment. This code directly uses emmintrin.h in ssw.c so there's no easy way to remove it for the powerpc architecture. That said, there appears to be a pull request to add PPC64 support (https://github.com/mengyao/Complete-Striped-Smith-Waterman-Library/pull/35) but this has been pending for more than a year. We'd be more than happy to update libssw if you can get an implementation in there that doesn't require SSE intrinsics. Let us know if you want to go down that route.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:333,safety,Compl,Complete-Striped-Smith-Waterman-Library,333,"Hi @DiableJambe, . We use libssw (https://github.com/mengyao/Complete-Striped-Smith-Waterman-Library) for our local alignment. This code directly uses emmintrin.h in ssw.c so there's no easy way to remove it for the powerpc architecture. That said, there appears to be a pull request to add PPC64 support (https://github.com/mengyao/Complete-Striped-Smith-Waterman-Library/pull/35) but this has been pending for more than a year. We'd be more than happy to update libssw if you can get an implementation in there that doesn't require SSE intrinsics. Let us know if you want to go down that route.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:457,safety,updat,update,457,"Hi @DiableJambe, . We use libssw (https://github.com/mengyao/Complete-Striped-Smith-Waterman-Library) for our local alignment. This code directly uses emmintrin.h in ssw.c so there's no easy way to remove it for the powerpc architecture. That said, there appears to be a pull request to add PPC64 support (https://github.com/mengyao/Complete-Striped-Smith-Waterman-Library/pull/35) but this has been pending for more than a year. We'd be more than happy to update libssw if you can get an implementation in there that doesn't require SSE intrinsics. Let us know if you want to go down that route.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:61,security,Compl,Complete-Striped-Smith-Waterman-Library,61,"Hi @DiableJambe, . We use libssw (https://github.com/mengyao/Complete-Striped-Smith-Waterman-Library) for our local alignment. This code directly uses emmintrin.h in ssw.c so there's no easy way to remove it for the powerpc architecture. That said, there appears to be a pull request to add PPC64 support (https://github.com/mengyao/Complete-Striped-Smith-Waterman-Library/pull/35) but this has been pending for more than a year. We'd be more than happy to update libssw if you can get an implementation in there that doesn't require SSE intrinsics. Let us know if you want to go down that route.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:333,security,Compl,Complete-Striped-Smith-Waterman-Library,333,"Hi @DiableJambe, . We use libssw (https://github.com/mengyao/Complete-Striped-Smith-Waterman-Library) for our local alignment. This code directly uses emmintrin.h in ssw.c so there's no easy way to remove it for the powerpc architecture. That said, there appears to be a pull request to add PPC64 support (https://github.com/mengyao/Complete-Striped-Smith-Waterman-Library/pull/35) but this has been pending for more than a year. We'd be more than happy to update libssw if you can get an implementation in there that doesn't require SSE intrinsics. Let us know if you want to go down that route.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:457,security,updat,update,457,"Hi @DiableJambe, . We use libssw (https://github.com/mengyao/Complete-Striped-Smith-Waterman-Library) for our local alignment. This code directly uses emmintrin.h in ssw.c so there's no easy way to remove it for the powerpc architecture. That said, there appears to be a pull request to add PPC64 support (https://github.com/mengyao/Complete-Striped-Smith-Waterman-Library/pull/35) but this has been pending for more than a year. We'd be more than happy to update libssw if you can get an implementation in there that doesn't require SSE intrinsics. Let us know if you want to go down that route.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:297,usability,support,support,297,"Hi @DiableJambe, . We use libssw (https://github.com/mengyao/Complete-Striped-Smith-Waterman-Library) for our local alignment. This code directly uses emmintrin.h in ssw.c so there's no easy way to remove it for the powerpc architecture. That said, there appears to be a pull request to add PPC64 support (https://github.com/mengyao/Complete-Striped-Smith-Waterman-Library/pull/35) but this has been pending for more than a year. We'd be more than happy to update libssw if you can get an implementation in there that doesn't require SSE intrinsics. Let us know if you want to go down that route.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:52,deployability,resourc,resource,52,"Hi @depristo . Thanks for the clarification and the resource! Right now, my goal is to get the flow working, so I may need to use an Intel machine for the time being. I will need to revisit this later. I wonder whether this affects only make_examples. If so, it may be good to separate the build into two parts, one for make_examples, and one for the rest. I am not sure whether this is worth supporting from your side. If I am constrained to use Power machines for my subsequent runs, I will be able to look into this aspect.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:290,deployability,build,build,290,"Hi @depristo . Thanks for the clarification and the resource! Right now, my goal is to get the flow working, so I may need to use an Intel machine for the time being. I will need to revisit this later. I wonder whether this affects only make_examples. If so, it may be good to separate the build into two parts, one for make_examples, and one for the rest. I am not sure whether this is worth supporting from your side. If I am constrained to use Power machines for my subsequent runs, I will be able to look into this aspect.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:52,energy efficiency,resourc,resource,52,"Hi @depristo . Thanks for the clarification and the resource! Right now, my goal is to get the flow working, so I may need to use an Intel machine for the time being. I will need to revisit this later. I wonder whether this affects only make_examples. If so, it may be good to separate the build into two parts, one for make_examples, and one for the rest. I am not sure whether this is worth supporting from your side. If I am constrained to use Power machines for my subsequent runs, I will be able to look into this aspect.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:447,energy efficiency,Power,Power,447,"Hi @depristo . Thanks for the clarification and the resource! Right now, my goal is to get the flow working, so I may need to use an Intel machine for the time being. I will need to revisit this later. I wonder whether this affects only make_examples. If so, it may be good to separate the build into two parts, one for make_examples, and one for the rest. I am not sure whether this is worth supporting from your side. If I am constrained to use Power machines for my subsequent runs, I will be able to look into this aspect.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:469,integrability,sub,subsequent,469,"Hi @depristo . Thanks for the clarification and the resource! Right now, my goal is to get the flow working, so I may need to use an Intel machine for the time being. I will need to revisit this later. I wonder whether this affects only make_examples. If so, it may be good to separate the build into two parts, one for make_examples, and one for the rest. I am not sure whether this is worth supporting from your side. If I am constrained to use Power machines for my subsequent runs, I will be able to look into this aspect.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:52,performance,resourc,resource,52,"Hi @depristo . Thanks for the clarification and the resource! Right now, my goal is to get the flow working, so I may need to use an Intel machine for the time being. I will need to revisit this later. I wonder whether this affects only make_examples. If so, it may be good to separate the build into two parts, one for make_examples, and one for the rest. I am not sure whether this is worth supporting from your side. If I am constrained to use Power machines for my subsequent runs, I will be able to look into this aspect.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:155,performance,time,time,155,"Hi @depristo . Thanks for the clarification and the resource! Right now, my goal is to get the flow working, so I may need to use an Intel machine for the time being. I will need to revisit this later. I wonder whether this affects only make_examples. If so, it may be good to separate the build into two parts, one for make_examples, and one for the rest. I am not sure whether this is worth supporting from your side. If I am constrained to use Power machines for my subsequent runs, I will be able to look into this aspect.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:52,safety,resourc,resource,52,"Hi @depristo . Thanks for the clarification and the resource! Right now, my goal is to get the flow working, so I may need to use an Intel machine for the time being. I will need to revisit this later. I wonder whether this affects only make_examples. If so, it may be good to separate the build into two parts, one for make_examples, and one for the rest. I am not sure whether this is worth supporting from your side. If I am constrained to use Power machines for my subsequent runs, I will be able to look into this aspect.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:52,testability,resourc,resource,52,"Hi @depristo . Thanks for the clarification and the resource! Right now, my goal is to get the flow working, so I may need to use an Intel machine for the time being. I will need to revisit this later. I wonder whether this affects only make_examples. If so, it may be good to separate the build into two parts, one for make_examples, and one for the rest. I am not sure whether this is worth supporting from your side. If I am constrained to use Power machines for my subsequent runs, I will be able to look into this aspect.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:393,usability,support,supporting,393,"Hi @depristo . Thanks for the clarification and the resource! Right now, my goal is to get the flow working, so I may need to use an Intel machine for the time being. I will need to revisit this later. I wonder whether this affects only make_examples. If so, it may be good to separate the build into two parts, one for make_examples, and one for the rest. I am not sure whether this is worth supporting from your side. If I am constrained to use Power machines for my subsequent runs, I will be able to look into this aspect.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:117,deployability,updat,updates,117,"Hi @DiableJambe ,. thanks for reporting the issue. I can keep this bug open for now in case you want to give us more updates. Just note that our team is not actively looking into this right now. (I also receive notification emails on updated GitHub issues even after they're closed. So keeping this open or closed doesn't really matter much in terms of communication).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:234,deployability,updat,updated,234,"Hi @DiableJambe ,. thanks for reporting the issue. I can keep this bug open for now in case you want to give us more updates. Just note that our team is not actively looking into this right now. (I also receive notification emails on updated GitHub issues even after they're closed. So keeping this open or closed doesn't really matter much in terms of communication).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:314,reliability,doe,doesn,314,"Hi @DiableJambe ,. thanks for reporting the issue. I can keep this bug open for now in case you want to give us more updates. Just note that our team is not actively looking into this right now. (I also receive notification emails on updated GitHub issues even after they're closed. So keeping this open or closed doesn't really matter much in terms of communication).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:117,safety,updat,updates,117,"Hi @DiableJambe ,. thanks for reporting the issue. I can keep this bug open for now in case you want to give us more updates. Just note that our team is not actively looking into this right now. (I also receive notification emails on updated GitHub issues even after they're closed. So keeping this open or closed doesn't really matter much in terms of communication).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:234,safety,updat,updated,234,"Hi @DiableJambe ,. thanks for reporting the issue. I can keep this bug open for now in case you want to give us more updates. Just note that our team is not actively looking into this right now. (I also receive notification emails on updated GitHub issues even after they're closed. So keeping this open or closed doesn't really matter much in terms of communication).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:117,security,updat,updates,117,"Hi @DiableJambe ,. thanks for reporting the issue. I can keep this bug open for now in case you want to give us more updates. Just note that our team is not actively looking into this right now. (I also receive notification emails on updated GitHub issues even after they're closed. So keeping this open or closed doesn't really matter much in terms of communication).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:145,security,team,team,145,"Hi @DiableJambe ,. thanks for reporting the issue. I can keep this bug open for now in case you want to give us more updates. Just note that our team is not actively looking into this right now. (I also receive notification emails on updated GitHub issues even after they're closed. So keeping this open or closed doesn't really matter much in terms of communication).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:234,security,updat,updated,234,"Hi @DiableJambe ,. thanks for reporting the issue. I can keep this bug open for now in case you want to give us more updates. Just note that our team is not actively looking into this right now. (I also receive notification emails on updated GitHub issues even after they're closed. So keeping this open or closed doesn't really matter much in terms of communication).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:275,usability,close,closed,275,"Hi @DiableJambe ,. thanks for reporting the issue. I can keep this bug open for now in case you want to give us more updates. Just note that our team is not actively looking into this right now. (I also receive notification emails on updated GitHub issues even after they're closed. So keeping this open or closed doesn't really matter much in terms of communication).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:307,usability,close,closed,307,"Hi @DiableJambe ,. thanks for reporting the issue. I can keep this bug open for now in case you want to give us more updates. Just note that our team is not actively looking into this right now. (I also receive notification emails on updated GitHub issues even after they're closed. So keeping this open or closed doesn't really matter much in terms of communication).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:615,availability,error,error,615,"Hi @DiableJambe and @pichuan ,. This issue can be resolved by IBM Advance Toolchain 11.0 :). Some brief steps for your references of how to build DeepVariant on Power8 & Redhat 7.5. 1. Build the following packages with ""/usr/bin/gcc"". cmake 3.13.3. Protobuf 3.6.1 C++ (static build with --enable-static for bazel). bazel 0.15.0. 2. Install Advance Toolchain 11.0 and build the following packages with /opt/at11.0/bin/gcc. Python 2 and Pip 19.0.2. Protobuf 3.6.1 C++ (uninstall static and build shared). Protobuf 3.6.1 Python (should build and install from source or CLIF will fail). TensorFlow 1.12.0 (fix floatn.h error with the link Floatn.h error: https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f). CLIF. Opencv-python 3.4.5.20 (for tensor2tensor install). Then build DeepVariant will pass, and results here (excepted //deepvariant/labeler:haplotype_labeler_test tracked in issue 154). ```. ================================================================================. (05:42:50) INFO: Elapsed time: 715.015s, Critical Path: 689.68s. (05:42:50) INFO: 1835 processes: 1835 local. (05:42:50) INFO: Build completed, 1 test FAILED, 2433 total actions. //deepvariant:allelecounter_test PASSED in 0.1s. //deepvariant:call_variants_test PASSED in 59.8s. //deepvariant:data_providers_test PASSED in 11.8s. //deepvariant:dv_vcf_constants_test PASSED in 0.5s. //deepvariant:exclude_contigs_test PASSED in 1.6s. //deepvariant:haplotypes_test PASSED in 1.7s. . //deepvariant:modeling_test PASSED in 48.2s. //deepvariant:pileup_image_test PASSED in 1.8s. //deepvariant:postprocess_variants_lib_test PASSED in 0.1s. //deepvariant:postprocess_variants_test PASSED in 4.8s. //deepvariant:resources_test PASSED in 1.8s. //deepvariant:tf_utils_test PASSED in 3.8s. //deepvariant:utils_test PASSED in 0.1s. //deepvariant:variant_caller_test PASSED in 2.4s. //deepvariant:variant_calling_test PASSED in 0.1s. //deepvariant/environment_tests:env_smoke_test PASSED in 0.4s. //deepvariant/environmen",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:644,availability,error,error,644,"Hi @DiableJambe and @pichuan ,. This issue can be resolved by IBM Advance Toolchain 11.0 :). Some brief steps for your references of how to build DeepVariant on Power8 & Redhat 7.5. 1. Build the following packages with ""/usr/bin/gcc"". cmake 3.13.3. Protobuf 3.6.1 C++ (static build with --enable-static for bazel). bazel 0.15.0. 2. Install Advance Toolchain 11.0 and build the following packages with /opt/at11.0/bin/gcc. Python 2 and Pip 19.0.2. Protobuf 3.6.1 C++ (uninstall static and build shared). Protobuf 3.6.1 Python (should build and install from source or CLIF will fail). TensorFlow 1.12.0 (fix floatn.h error with the link Floatn.h error: https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f). CLIF. Opencv-python 3.4.5.20 (for tensor2tensor install). Then build DeepVariant will pass, and results here (excepted //deepvariant/labeler:haplotype_labeler_test tracked in issue 154). ```. ================================================================================. (05:42:50) INFO: Elapsed time: 715.015s, Critical Path: 689.68s. (05:42:50) INFO: 1835 processes: 1835 local. (05:42:50) INFO: Build completed, 1 test FAILED, 2433 total actions. //deepvariant:allelecounter_test PASSED in 0.1s. //deepvariant:call_variants_test PASSED in 59.8s. //deepvariant:data_providers_test PASSED in 11.8s. //deepvariant:dv_vcf_constants_test PASSED in 0.5s. //deepvariant:exclude_contigs_test PASSED in 1.6s. //deepvariant:haplotypes_test PASSED in 1.7s. . //deepvariant:modeling_test PASSED in 48.2s. //deepvariant:pileup_image_test PASSED in 1.8s. //deepvariant:postprocess_variants_lib_test PASSED in 0.1s. //deepvariant:postprocess_variants_test PASSED in 4.8s. //deepvariant:resources_test PASSED in 1.8s. //deepvariant:tf_utils_test PASSED in 3.8s. //deepvariant:utils_test PASSED in 0.1s. //deepvariant:variant_caller_test PASSED in 2.4s. //deepvariant:variant_calling_test PASSED in 0.1s. //deepvariant/environment_tests:env_smoke_test PASSED in 0.4s. //deepvariant/environmen",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:74,deployability,Toolchain,Toolchain,74,"Hi @DiableJambe and @pichuan ,. This issue can be resolved by IBM Advance Toolchain 11.0 :). Some brief steps for your references of how to build DeepVariant on Power8 & Redhat 7.5. 1. Build the following packages with ""/usr/bin/gcc"". cmake 3.13.3. Protobuf 3.6.1 C++ (static build with --enable-static for bazel). bazel 0.15.0. 2. Install Advance Toolchain 11.0 and build the following packages with /opt/at11.0/bin/gcc. Python 2 and Pip 19.0.2. Protobuf 3.6.1 C++ (uninstall static and build shared). Protobuf 3.6.1 Python (should build and install from source or CLIF will fail). TensorFlow 1.12.0 (fix floatn.h error with the link Floatn.h error: https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f). CLIF. Opencv-python 3.4.5.20 (for tensor2tensor install). Then build DeepVariant will pass, and results here (excepted //deepvariant/labeler:haplotype_labeler_test tracked in issue 154). ```. ================================================================================. (05:42:50) INFO: Elapsed time: 715.015s, Critical Path: 689.68s. (05:42:50) INFO: 1835 processes: 1835 local. (05:42:50) INFO: Build completed, 1 test FAILED, 2433 total actions. //deepvariant:allelecounter_test PASSED in 0.1s. //deepvariant:call_variants_test PASSED in 59.8s. //deepvariant:data_providers_test PASSED in 11.8s. //deepvariant:dv_vcf_constants_test PASSED in 0.5s. //deepvariant:exclude_contigs_test PASSED in 1.6s. //deepvariant:haplotypes_test PASSED in 1.7s. . //deepvariant:modeling_test PASSED in 48.2s. //deepvariant:pileup_image_test PASSED in 1.8s. //deepvariant:postprocess_variants_lib_test PASSED in 0.1s. //deepvariant:postprocess_variants_test PASSED in 4.8s. //deepvariant:resources_test PASSED in 1.8s. //deepvariant:tf_utils_test PASSED in 3.8s. //deepvariant:utils_test PASSED in 0.1s. //deepvariant:variant_caller_test PASSED in 2.4s. //deepvariant:variant_calling_test PASSED in 0.1s. //deepvariant/environment_tests:env_smoke_test PASSED in 0.4s. //deepvariant/environmen",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:140,deployability,build,build,140,"Hi @DiableJambe and @pichuan ,. This issue can be resolved by IBM Advance Toolchain 11.0 :). Some brief steps for your references of how to build DeepVariant on Power8 & Redhat 7.5. 1. Build the following packages with ""/usr/bin/gcc"". cmake 3.13.3. Protobuf 3.6.1 C++ (static build with --enable-static for bazel). bazel 0.15.0. 2. Install Advance Toolchain 11.0 and build the following packages with /opt/at11.0/bin/gcc. Python 2 and Pip 19.0.2. Protobuf 3.6.1 C++ (uninstall static and build shared). Protobuf 3.6.1 Python (should build and install from source or CLIF will fail). TensorFlow 1.12.0 (fix floatn.h error with the link Floatn.h error: https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f). CLIF. Opencv-python 3.4.5.20 (for tensor2tensor install). Then build DeepVariant will pass, and results here (excepted //deepvariant/labeler:haplotype_labeler_test tracked in issue 154). ```. ================================================================================. (05:42:50) INFO: Elapsed time: 715.015s, Critical Path: 689.68s. (05:42:50) INFO: 1835 processes: 1835 local. (05:42:50) INFO: Build completed, 1 test FAILED, 2433 total actions. //deepvariant:allelecounter_test PASSED in 0.1s. //deepvariant:call_variants_test PASSED in 59.8s. //deepvariant:data_providers_test PASSED in 11.8s. //deepvariant:dv_vcf_constants_test PASSED in 0.5s. //deepvariant:exclude_contigs_test PASSED in 1.6s. //deepvariant:haplotypes_test PASSED in 1.7s. . //deepvariant:modeling_test PASSED in 48.2s. //deepvariant:pileup_image_test PASSED in 1.8s. //deepvariant:postprocess_variants_lib_test PASSED in 0.1s. //deepvariant:postprocess_variants_test PASSED in 4.8s. //deepvariant:resources_test PASSED in 1.8s. //deepvariant:tf_utils_test PASSED in 3.8s. //deepvariant:utils_test PASSED in 0.1s. //deepvariant:variant_caller_test PASSED in 2.4s. //deepvariant:variant_calling_test PASSED in 0.1s. //deepvariant/environment_tests:env_smoke_test PASSED in 0.4s. //deepvariant/environmen",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:185,deployability,Build,Build,185,"Hi @DiableJambe and @pichuan ,. This issue can be resolved by IBM Advance Toolchain 11.0 :). Some brief steps for your references of how to build DeepVariant on Power8 & Redhat 7.5. 1. Build the following packages with ""/usr/bin/gcc"". cmake 3.13.3. Protobuf 3.6.1 C++ (static build with --enable-static for bazel). bazel 0.15.0. 2. Install Advance Toolchain 11.0 and build the following packages with /opt/at11.0/bin/gcc. Python 2 and Pip 19.0.2. Protobuf 3.6.1 C++ (uninstall static and build shared). Protobuf 3.6.1 Python (should build and install from source or CLIF will fail). TensorFlow 1.12.0 (fix floatn.h error with the link Floatn.h error: https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f). CLIF. Opencv-python 3.4.5.20 (for tensor2tensor install). Then build DeepVariant will pass, and results here (excepted //deepvariant/labeler:haplotype_labeler_test tracked in issue 154). ```. ================================================================================. (05:42:50) INFO: Elapsed time: 715.015s, Critical Path: 689.68s. (05:42:50) INFO: 1835 processes: 1835 local. (05:42:50) INFO: Build completed, 1 test FAILED, 2433 total actions. //deepvariant:allelecounter_test PASSED in 0.1s. //deepvariant:call_variants_test PASSED in 59.8s. //deepvariant:data_providers_test PASSED in 11.8s. //deepvariant:dv_vcf_constants_test PASSED in 0.5s. //deepvariant:exclude_contigs_test PASSED in 1.6s. //deepvariant:haplotypes_test PASSED in 1.7s. . //deepvariant:modeling_test PASSED in 48.2s. //deepvariant:pileup_image_test PASSED in 1.8s. //deepvariant:postprocess_variants_lib_test PASSED in 0.1s. //deepvariant:postprocess_variants_test PASSED in 4.8s. //deepvariant:resources_test PASSED in 1.8s. //deepvariant:tf_utils_test PASSED in 3.8s. //deepvariant:utils_test PASSED in 0.1s. //deepvariant:variant_caller_test PASSED in 2.4s. //deepvariant:variant_calling_test PASSED in 0.1s. //deepvariant/environment_tests:env_smoke_test PASSED in 0.4s. //deepvariant/environmen",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:276,deployability,build,build,276,"Hi @DiableJambe and @pichuan ,. This issue can be resolved by IBM Advance Toolchain 11.0 :). Some brief steps for your references of how to build DeepVariant on Power8 & Redhat 7.5. 1. Build the following packages with ""/usr/bin/gcc"". cmake 3.13.3. Protobuf 3.6.1 C++ (static build with --enable-static for bazel). bazel 0.15.0. 2. Install Advance Toolchain 11.0 and build the following packages with /opt/at11.0/bin/gcc. Python 2 and Pip 19.0.2. Protobuf 3.6.1 C++ (uninstall static and build shared). Protobuf 3.6.1 Python (should build and install from source or CLIF will fail). TensorFlow 1.12.0 (fix floatn.h error with the link Floatn.h error: https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f). CLIF. Opencv-python 3.4.5.20 (for tensor2tensor install). Then build DeepVariant will pass, and results here (excepted //deepvariant/labeler:haplotype_labeler_test tracked in issue 154). ```. ================================================================================. (05:42:50) INFO: Elapsed time: 715.015s, Critical Path: 689.68s. (05:42:50) INFO: 1835 processes: 1835 local. (05:42:50) INFO: Build completed, 1 test FAILED, 2433 total actions. //deepvariant:allelecounter_test PASSED in 0.1s. //deepvariant:call_variants_test PASSED in 59.8s. //deepvariant:data_providers_test PASSED in 11.8s. //deepvariant:dv_vcf_constants_test PASSED in 0.5s. //deepvariant:exclude_contigs_test PASSED in 1.6s. //deepvariant:haplotypes_test PASSED in 1.7s. . //deepvariant:modeling_test PASSED in 48.2s. //deepvariant:pileup_image_test PASSED in 1.8s. //deepvariant:postprocess_variants_lib_test PASSED in 0.1s. //deepvariant:postprocess_variants_test PASSED in 4.8s. //deepvariant:resources_test PASSED in 1.8s. //deepvariant:tf_utils_test PASSED in 3.8s. //deepvariant:utils_test PASSED in 0.1s. //deepvariant:variant_caller_test PASSED in 2.4s. //deepvariant:variant_calling_test PASSED in 0.1s. //deepvariant/environment_tests:env_smoke_test PASSED in 0.4s. //deepvariant/environmen",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:332,deployability,Instal,Install,332,"Hi @DiableJambe and @pichuan ,. This issue can be resolved by IBM Advance Toolchain 11.0 :). Some brief steps for your references of how to build DeepVariant on Power8 & Redhat 7.5. 1. Build the following packages with ""/usr/bin/gcc"". cmake 3.13.3. Protobuf 3.6.1 C++ (static build with --enable-static for bazel). bazel 0.15.0. 2. Install Advance Toolchain 11.0 and build the following packages with /opt/at11.0/bin/gcc. Python 2 and Pip 19.0.2. Protobuf 3.6.1 C++ (uninstall static and build shared). Protobuf 3.6.1 Python (should build and install from source or CLIF will fail). TensorFlow 1.12.0 (fix floatn.h error with the link Floatn.h error: https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f). CLIF. Opencv-python 3.4.5.20 (for tensor2tensor install). Then build DeepVariant will pass, and results here (excepted //deepvariant/labeler:haplotype_labeler_test tracked in issue 154). ```. ================================================================================. (05:42:50) INFO: Elapsed time: 715.015s, Critical Path: 689.68s. (05:42:50) INFO: 1835 processes: 1835 local. (05:42:50) INFO: Build completed, 1 test FAILED, 2433 total actions. //deepvariant:allelecounter_test PASSED in 0.1s. //deepvariant:call_variants_test PASSED in 59.8s. //deepvariant:data_providers_test PASSED in 11.8s. //deepvariant:dv_vcf_constants_test PASSED in 0.5s. //deepvariant:exclude_contigs_test PASSED in 1.6s. //deepvariant:haplotypes_test PASSED in 1.7s. . //deepvariant:modeling_test PASSED in 48.2s. //deepvariant:pileup_image_test PASSED in 1.8s. //deepvariant:postprocess_variants_lib_test PASSED in 0.1s. //deepvariant:postprocess_variants_test PASSED in 4.8s. //deepvariant:resources_test PASSED in 1.8s. //deepvariant:tf_utils_test PASSED in 3.8s. //deepvariant:utils_test PASSED in 0.1s. //deepvariant:variant_caller_test PASSED in 2.4s. //deepvariant:variant_calling_test PASSED in 0.1s. //deepvariant/environment_tests:env_smoke_test PASSED in 0.4s. //deepvariant/environmen",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:348,deployability,Toolchain,Toolchain,348,"Hi @DiableJambe and @pichuan ,. This issue can be resolved by IBM Advance Toolchain 11.0 :). Some brief steps for your references of how to build DeepVariant on Power8 & Redhat 7.5. 1. Build the following packages with ""/usr/bin/gcc"". cmake 3.13.3. Protobuf 3.6.1 C++ (static build with --enable-static for bazel). bazel 0.15.0. 2. Install Advance Toolchain 11.0 and build the following packages with /opt/at11.0/bin/gcc. Python 2 and Pip 19.0.2. Protobuf 3.6.1 C++ (uninstall static and build shared). Protobuf 3.6.1 Python (should build and install from source or CLIF will fail). TensorFlow 1.12.0 (fix floatn.h error with the link Floatn.h error: https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f). CLIF. Opencv-python 3.4.5.20 (for tensor2tensor install). Then build DeepVariant will pass, and results here (excepted //deepvariant/labeler:haplotype_labeler_test tracked in issue 154). ```. ================================================================================. (05:42:50) INFO: Elapsed time: 715.015s, Critical Path: 689.68s. (05:42:50) INFO: 1835 processes: 1835 local. (05:42:50) INFO: Build completed, 1 test FAILED, 2433 total actions. //deepvariant:allelecounter_test PASSED in 0.1s. //deepvariant:call_variants_test PASSED in 59.8s. //deepvariant:data_providers_test PASSED in 11.8s. //deepvariant:dv_vcf_constants_test PASSED in 0.5s. //deepvariant:exclude_contigs_test PASSED in 1.6s. //deepvariant:haplotypes_test PASSED in 1.7s. . //deepvariant:modeling_test PASSED in 48.2s. //deepvariant:pileup_image_test PASSED in 1.8s. //deepvariant:postprocess_variants_lib_test PASSED in 0.1s. //deepvariant:postprocess_variants_test PASSED in 4.8s. //deepvariant:resources_test PASSED in 1.8s. //deepvariant:tf_utils_test PASSED in 3.8s. //deepvariant:utils_test PASSED in 0.1s. //deepvariant:variant_caller_test PASSED in 2.4s. //deepvariant:variant_calling_test PASSED in 0.1s. //deepvariant/environment_tests:env_smoke_test PASSED in 0.4s. //deepvariant/environmen",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:367,deployability,build,build,367,"Hi @DiableJambe and @pichuan ,. This issue can be resolved by IBM Advance Toolchain 11.0 :). Some brief steps for your references of how to build DeepVariant on Power8 & Redhat 7.5. 1. Build the following packages with ""/usr/bin/gcc"". cmake 3.13.3. Protobuf 3.6.1 C++ (static build with --enable-static for bazel). bazel 0.15.0. 2. Install Advance Toolchain 11.0 and build the following packages with /opt/at11.0/bin/gcc. Python 2 and Pip 19.0.2. Protobuf 3.6.1 C++ (uninstall static and build shared). Protobuf 3.6.1 Python (should build and install from source or CLIF will fail). TensorFlow 1.12.0 (fix floatn.h error with the link Floatn.h error: https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f). CLIF. Opencv-python 3.4.5.20 (for tensor2tensor install). Then build DeepVariant will pass, and results here (excepted //deepvariant/labeler:haplotype_labeler_test tracked in issue 154). ```. ================================================================================. (05:42:50) INFO: Elapsed time: 715.015s, Critical Path: 689.68s. (05:42:50) INFO: 1835 processes: 1835 local. (05:42:50) INFO: Build completed, 1 test FAILED, 2433 total actions. //deepvariant:allelecounter_test PASSED in 0.1s. //deepvariant:call_variants_test PASSED in 59.8s. //deepvariant:data_providers_test PASSED in 11.8s. //deepvariant:dv_vcf_constants_test PASSED in 0.5s. //deepvariant:exclude_contigs_test PASSED in 1.6s. //deepvariant:haplotypes_test PASSED in 1.7s. . //deepvariant:modeling_test PASSED in 48.2s. //deepvariant:pileup_image_test PASSED in 1.8s. //deepvariant:postprocess_variants_lib_test PASSED in 0.1s. //deepvariant:postprocess_variants_test PASSED in 4.8s. //deepvariant:resources_test PASSED in 1.8s. //deepvariant:tf_utils_test PASSED in 3.8s. //deepvariant:utils_test PASSED in 0.1s. //deepvariant:variant_caller_test PASSED in 2.4s. //deepvariant:variant_calling_test PASSED in 0.1s. //deepvariant/environment_tests:env_smoke_test PASSED in 0.4s. //deepvariant/environmen",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:488,deployability,build,build,488,"Hi @DiableJambe and @pichuan ,. This issue can be resolved by IBM Advance Toolchain 11.0 :). Some brief steps for your references of how to build DeepVariant on Power8 & Redhat 7.5. 1. Build the following packages with ""/usr/bin/gcc"". cmake 3.13.3. Protobuf 3.6.1 C++ (static build with --enable-static for bazel). bazel 0.15.0. 2. Install Advance Toolchain 11.0 and build the following packages with /opt/at11.0/bin/gcc. Python 2 and Pip 19.0.2. Protobuf 3.6.1 C++ (uninstall static and build shared). Protobuf 3.6.1 Python (should build and install from source or CLIF will fail). TensorFlow 1.12.0 (fix floatn.h error with the link Floatn.h error: https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f). CLIF. Opencv-python 3.4.5.20 (for tensor2tensor install). Then build DeepVariant will pass, and results here (excepted //deepvariant/labeler:haplotype_labeler_test tracked in issue 154). ```. ================================================================================. (05:42:50) INFO: Elapsed time: 715.015s, Critical Path: 689.68s. (05:42:50) INFO: 1835 processes: 1835 local. (05:42:50) INFO: Build completed, 1 test FAILED, 2433 total actions. //deepvariant:allelecounter_test PASSED in 0.1s. //deepvariant:call_variants_test PASSED in 59.8s. //deepvariant:data_providers_test PASSED in 11.8s. //deepvariant:dv_vcf_constants_test PASSED in 0.5s. //deepvariant:exclude_contigs_test PASSED in 1.6s. //deepvariant:haplotypes_test PASSED in 1.7s. . //deepvariant:modeling_test PASSED in 48.2s. //deepvariant:pileup_image_test PASSED in 1.8s. //deepvariant:postprocess_variants_lib_test PASSED in 0.1s. //deepvariant:postprocess_variants_test PASSED in 4.8s. //deepvariant:resources_test PASSED in 1.8s. //deepvariant:tf_utils_test PASSED in 3.8s. //deepvariant:utils_test PASSED in 0.1s. //deepvariant:variant_caller_test PASSED in 2.4s. //deepvariant:variant_calling_test PASSED in 0.1s. //deepvariant/environment_tests:env_smoke_test PASSED in 0.4s. //deepvariant/environmen",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:533,deployability,build,build,533,"Hi @DiableJambe and @pichuan ,. This issue can be resolved by IBM Advance Toolchain 11.0 :). Some brief steps for your references of how to build DeepVariant on Power8 & Redhat 7.5. 1. Build the following packages with ""/usr/bin/gcc"". cmake 3.13.3. Protobuf 3.6.1 C++ (static build with --enable-static for bazel). bazel 0.15.0. 2. Install Advance Toolchain 11.0 and build the following packages with /opt/at11.0/bin/gcc. Python 2 and Pip 19.0.2. Protobuf 3.6.1 C++ (uninstall static and build shared). Protobuf 3.6.1 Python (should build and install from source or CLIF will fail). TensorFlow 1.12.0 (fix floatn.h error with the link Floatn.h error: https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f). CLIF. Opencv-python 3.4.5.20 (for tensor2tensor install). Then build DeepVariant will pass, and results here (excepted //deepvariant/labeler:haplotype_labeler_test tracked in issue 154). ```. ================================================================================. (05:42:50) INFO: Elapsed time: 715.015s, Critical Path: 689.68s. (05:42:50) INFO: 1835 processes: 1835 local. (05:42:50) INFO: Build completed, 1 test FAILED, 2433 total actions. //deepvariant:allelecounter_test PASSED in 0.1s. //deepvariant:call_variants_test PASSED in 59.8s. //deepvariant:data_providers_test PASSED in 11.8s. //deepvariant:dv_vcf_constants_test PASSED in 0.5s. //deepvariant:exclude_contigs_test PASSED in 1.6s. //deepvariant:haplotypes_test PASSED in 1.7s. . //deepvariant:modeling_test PASSED in 48.2s. //deepvariant:pileup_image_test PASSED in 1.8s. //deepvariant:postprocess_variants_lib_test PASSED in 0.1s. //deepvariant:postprocess_variants_test PASSED in 4.8s. //deepvariant:resources_test PASSED in 1.8s. //deepvariant:tf_utils_test PASSED in 3.8s. //deepvariant:utils_test PASSED in 0.1s. //deepvariant:variant_caller_test PASSED in 2.4s. //deepvariant:variant_calling_test PASSED in 0.1s. //deepvariant/environment_tests:env_smoke_test PASSED in 0.4s. //deepvariant/environmen",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:543,deployability,instal,install,543,"Hi @DiableJambe and @pichuan ,. This issue can be resolved by IBM Advance Toolchain 11.0 :). Some brief steps for your references of how to build DeepVariant on Power8 & Redhat 7.5. 1. Build the following packages with ""/usr/bin/gcc"". cmake 3.13.3. Protobuf 3.6.1 C++ (static build with --enable-static for bazel). bazel 0.15.0. 2. Install Advance Toolchain 11.0 and build the following packages with /opt/at11.0/bin/gcc. Python 2 and Pip 19.0.2. Protobuf 3.6.1 C++ (uninstall static and build shared). Protobuf 3.6.1 Python (should build and install from source or CLIF will fail). TensorFlow 1.12.0 (fix floatn.h error with the link Floatn.h error: https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f). CLIF. Opencv-python 3.4.5.20 (for tensor2tensor install). Then build DeepVariant will pass, and results here (excepted //deepvariant/labeler:haplotype_labeler_test tracked in issue 154). ```. ================================================================================. (05:42:50) INFO: Elapsed time: 715.015s, Critical Path: 689.68s. (05:42:50) INFO: 1835 processes: 1835 local. (05:42:50) INFO: Build completed, 1 test FAILED, 2433 total actions. //deepvariant:allelecounter_test PASSED in 0.1s. //deepvariant:call_variants_test PASSED in 59.8s. //deepvariant:data_providers_test PASSED in 11.8s. //deepvariant:dv_vcf_constants_test PASSED in 0.5s. //deepvariant:exclude_contigs_test PASSED in 1.6s. //deepvariant:haplotypes_test PASSED in 1.7s. . //deepvariant:modeling_test PASSED in 48.2s. //deepvariant:pileup_image_test PASSED in 1.8s. //deepvariant:postprocess_variants_lib_test PASSED in 0.1s. //deepvariant:postprocess_variants_test PASSED in 4.8s. //deepvariant:resources_test PASSED in 1.8s. //deepvariant:tf_utils_test PASSED in 3.8s. //deepvariant:utils_test PASSED in 0.1s. //deepvariant:variant_caller_test PASSED in 2.4s. //deepvariant:variant_calling_test PASSED in 0.1s. //deepvariant/environment_tests:env_smoke_test PASSED in 0.4s. //deepvariant/environmen",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:576,deployability,fail,fail,576,"Hi @DiableJambe and @pichuan ,. This issue can be resolved by IBM Advance Toolchain 11.0 :). Some brief steps for your references of how to build DeepVariant on Power8 & Redhat 7.5. 1. Build the following packages with ""/usr/bin/gcc"". cmake 3.13.3. Protobuf 3.6.1 C++ (static build with --enable-static for bazel). bazel 0.15.0. 2. Install Advance Toolchain 11.0 and build the following packages with /opt/at11.0/bin/gcc. Python 2 and Pip 19.0.2. Protobuf 3.6.1 C++ (uninstall static and build shared). Protobuf 3.6.1 Python (should build and install from source or CLIF will fail). TensorFlow 1.12.0 (fix floatn.h error with the link Floatn.h error: https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f). CLIF. Opencv-python 3.4.5.20 (for tensor2tensor install). Then build DeepVariant will pass, and results here (excepted //deepvariant/labeler:haplotype_labeler_test tracked in issue 154). ```. ================================================================================. (05:42:50) INFO: Elapsed time: 715.015s, Critical Path: 689.68s. (05:42:50) INFO: 1835 processes: 1835 local. (05:42:50) INFO: Build completed, 1 test FAILED, 2433 total actions. //deepvariant:allelecounter_test PASSED in 0.1s. //deepvariant:call_variants_test PASSED in 59.8s. //deepvariant:data_providers_test PASSED in 11.8s. //deepvariant:dv_vcf_constants_test PASSED in 0.5s. //deepvariant:exclude_contigs_test PASSED in 1.6s. //deepvariant:haplotypes_test PASSED in 1.7s. . //deepvariant:modeling_test PASSED in 48.2s. //deepvariant:pileup_image_test PASSED in 1.8s. //deepvariant:postprocess_variants_lib_test PASSED in 0.1s. //deepvariant:postprocess_variants_test PASSED in 4.8s. //deepvariant:resources_test PASSED in 1.8s. //deepvariant:tf_utils_test PASSED in 3.8s. //deepvariant:utils_test PASSED in 0.1s. //deepvariant:variant_caller_test PASSED in 2.4s. //deepvariant:variant_calling_test PASSED in 0.1s. //deepvariant/environment_tests:env_smoke_test PASSED in 0.4s. //deepvariant/environmen",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:766,deployability,instal,install,766,"Hi @DiableJambe and @pichuan ,. This issue can be resolved by IBM Advance Toolchain 11.0 :). Some brief steps for your references of how to build DeepVariant on Power8 & Redhat 7.5. 1. Build the following packages with ""/usr/bin/gcc"". cmake 3.13.3. Protobuf 3.6.1 C++ (static build with --enable-static for bazel). bazel 0.15.0. 2. Install Advance Toolchain 11.0 and build the following packages with /opt/at11.0/bin/gcc. Python 2 and Pip 19.0.2. Protobuf 3.6.1 C++ (uninstall static and build shared). Protobuf 3.6.1 Python (should build and install from source or CLIF will fail). TensorFlow 1.12.0 (fix floatn.h error with the link Floatn.h error: https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f). CLIF. Opencv-python 3.4.5.20 (for tensor2tensor install). Then build DeepVariant will pass, and results here (excepted //deepvariant/labeler:haplotype_labeler_test tracked in issue 154). ```. ================================================================================. (05:42:50) INFO: Elapsed time: 715.015s, Critical Path: 689.68s. (05:42:50) INFO: 1835 processes: 1835 local. (05:42:50) INFO: Build completed, 1 test FAILED, 2433 total actions. //deepvariant:allelecounter_test PASSED in 0.1s. //deepvariant:call_variants_test PASSED in 59.8s. //deepvariant:data_providers_test PASSED in 11.8s. //deepvariant:dv_vcf_constants_test PASSED in 0.5s. //deepvariant:exclude_contigs_test PASSED in 1.6s. //deepvariant:haplotypes_test PASSED in 1.7s. . //deepvariant:modeling_test PASSED in 48.2s. //deepvariant:pileup_image_test PASSED in 1.8s. //deepvariant:postprocess_variants_lib_test PASSED in 0.1s. //deepvariant:postprocess_variants_test PASSED in 4.8s. //deepvariant:resources_test PASSED in 1.8s. //deepvariant:tf_utils_test PASSED in 3.8s. //deepvariant:utils_test PASSED in 0.1s. //deepvariant:variant_caller_test PASSED in 2.4s. //deepvariant:variant_calling_test PASSED in 0.1s. //deepvariant/environment_tests:env_smoke_test PASSED in 0.4s. //deepvariant/environmen",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:781,deployability,build,build,781,"Hi @DiableJambe and @pichuan ,. This issue can be resolved by IBM Advance Toolchain 11.0 :). Some brief steps for your references of how to build DeepVariant on Power8 & Redhat 7.5. 1. Build the following packages with ""/usr/bin/gcc"". cmake 3.13.3. Protobuf 3.6.1 C++ (static build with --enable-static for bazel). bazel 0.15.0. 2. Install Advance Toolchain 11.0 and build the following packages with /opt/at11.0/bin/gcc. Python 2 and Pip 19.0.2. Protobuf 3.6.1 C++ (uninstall static and build shared). Protobuf 3.6.1 Python (should build and install from source or CLIF will fail). TensorFlow 1.12.0 (fix floatn.h error with the link Floatn.h error: https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f). CLIF. Opencv-python 3.4.5.20 (for tensor2tensor install). Then build DeepVariant will pass, and results here (excepted //deepvariant/labeler:haplotype_labeler_test tracked in issue 154). ```. ================================================================================. (05:42:50) INFO: Elapsed time: 715.015s, Critical Path: 689.68s. (05:42:50) INFO: 1835 processes: 1835 local. (05:42:50) INFO: Build completed, 1 test FAILED, 2433 total actions. //deepvariant:allelecounter_test PASSED in 0.1s. //deepvariant:call_variants_test PASSED in 59.8s. //deepvariant:data_providers_test PASSED in 11.8s. //deepvariant:dv_vcf_constants_test PASSED in 0.5s. //deepvariant:exclude_contigs_test PASSED in 1.6s. //deepvariant:haplotypes_test PASSED in 1.7s. . //deepvariant:modeling_test PASSED in 48.2s. //deepvariant:pileup_image_test PASSED in 1.8s. //deepvariant:postprocess_variants_lib_test PASSED in 0.1s. //deepvariant:postprocess_variants_test PASSED in 4.8s. //deepvariant:resources_test PASSED in 1.8s. //deepvariant:tf_utils_test PASSED in 3.8s. //deepvariant:utils_test PASSED in 0.1s. //deepvariant:variant_caller_test PASSED in 2.4s. //deepvariant:variant_calling_test PASSED in 0.1s. //deepvariant/environment_tests:env_smoke_test PASSED in 0.4s. //deepvariant/environmen",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:1119,deployability,Build,Build,1119,"erences of how to build DeepVariant on Power8 & Redhat 7.5. 1. Build the following packages with ""/usr/bin/gcc"". cmake 3.13.3. Protobuf 3.6.1 C++ (static build with --enable-static for bazel). bazel 0.15.0. 2. Install Advance Toolchain 11.0 and build the following packages with /opt/at11.0/bin/gcc. Python 2 and Pip 19.0.2. Protobuf 3.6.1 C++ (uninstall static and build shared). Protobuf 3.6.1 Python (should build and install from source or CLIF will fail). TensorFlow 1.12.0 (fix floatn.h error with the link Floatn.h error: https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f). CLIF. Opencv-python 3.4.5.20 (for tensor2tensor install). Then build DeepVariant will pass, and results here (excepted //deepvariant/labeler:haplotype_labeler_test tracked in issue 154). ```. ================================================================================. (05:42:50) INFO: Elapsed time: 715.015s, Critical Path: 689.68s. (05:42:50) INFO: 1835 processes: 1835 local. (05:42:50) INFO: Build completed, 1 test FAILED, 2433 total actions. //deepvariant:allelecounter_test PASSED in 0.1s. //deepvariant:call_variants_test PASSED in 59.8s. //deepvariant:data_providers_test PASSED in 11.8s. //deepvariant:dv_vcf_constants_test PASSED in 0.5s. //deepvariant:exclude_contigs_test PASSED in 1.6s. //deepvariant:haplotypes_test PASSED in 1.7s. . //deepvariant:modeling_test PASSED in 48.2s. //deepvariant:pileup_image_test PASSED in 1.8s. //deepvariant:postprocess_variants_lib_test PASSED in 0.1s. //deepvariant:postprocess_variants_test PASSED in 4.8s. //deepvariant:resources_test PASSED in 1.8s. //deepvariant:tf_utils_test PASSED in 3.8s. //deepvariant:utils_test PASSED in 0.1s. //deepvariant:variant_caller_test PASSED in 2.4s. //deepvariant:variant_calling_test PASSED in 0.1s. //deepvariant/environment_tests:env_smoke_test PASSED in 0.4s. //deepvariant/environment_tests:protobuf_implementation_test PASSED in 0.8s. //deepvariant/labeler:customized_classes_labeler_test PASSED in 1.8s.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:1143,deployability,FAIL,FAILED,1143,"DeepVariant on Power8 & Redhat 7.5. 1. Build the following packages with ""/usr/bin/gcc"". cmake 3.13.3. Protobuf 3.6.1 C++ (static build with --enable-static for bazel). bazel 0.15.0. 2. Install Advance Toolchain 11.0 and build the following packages with /opt/at11.0/bin/gcc. Python 2 and Pip 19.0.2. Protobuf 3.6.1 C++ (uninstall static and build shared). Protobuf 3.6.1 Python (should build and install from source or CLIF will fail). TensorFlow 1.12.0 (fix floatn.h error with the link Floatn.h error: https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f). CLIF. Opencv-python 3.4.5.20 (for tensor2tensor install). Then build DeepVariant will pass, and results here (excepted //deepvariant/labeler:haplotype_labeler_test tracked in issue 154). ```. ================================================================================. (05:42:50) INFO: Elapsed time: 715.015s, Critical Path: 689.68s. (05:42:50) INFO: 1835 processes: 1835 local. (05:42:50) INFO: Build completed, 1 test FAILED, 2433 total actions. //deepvariant:allelecounter_test PASSED in 0.1s. //deepvariant:call_variants_test PASSED in 59.8s. //deepvariant:data_providers_test PASSED in 11.8s. //deepvariant:dv_vcf_constants_test PASSED in 0.5s. //deepvariant:exclude_contigs_test PASSED in 1.6s. //deepvariant:haplotypes_test PASSED in 1.7s. . //deepvariant:modeling_test PASSED in 48.2s. //deepvariant:pileup_image_test PASSED in 1.8s. //deepvariant:postprocess_variants_lib_test PASSED in 0.1s. //deepvariant:postprocess_variants_test PASSED in 4.8s. //deepvariant:resources_test PASSED in 1.8s. //deepvariant:tf_utils_test PASSED in 3.8s. //deepvariant:utils_test PASSED in 0.1s. //deepvariant:variant_caller_test PASSED in 2.4s. //deepvariant:variant_calling_test PASSED in 0.1s. //deepvariant/environment_tests:env_smoke_test PASSED in 0.4s. //deepvariant/environment_tests:protobuf_implementation_test PASSED in 0.8s. //deepvariant/labeler:customized_classes_labeler_test PASSED in 1.8s. //deepvariant/labeler:l",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:3173,deployability,FAIL,FAILED,3173," //deepvariant:tf_utils_test PASSED in 3.8s. //deepvariant:utils_test PASSED in 0.1s. //deepvariant:variant_caller_test PASSED in 2.4s. //deepvariant:variant_calling_test PASSED in 0.1s. //deepvariant/environment_tests:env_smoke_test PASSED in 0.4s. //deepvariant/environment_tests:protobuf_implementation_test PASSED in 0.8s. //deepvariant/labeler:customized_classes_labeler_test PASSED in 1.8s. //deepvariant/labeler:labeled_examples_to_vcf_test PASSED in 1.9s. //deepvariant/labeler:positional_labeler_test PASSED in 1.8s. //deepvariant/labeler:variant_labeler_test PASSED in 1.8s. //deepvariant/python:allelecounter_wrap_test PASSED in 1.7s. //deepvariant/python:variant_calling_wrap_test PASSED in 1.8s. //deepvariant/realigner:aligner_test PASSED in 1.7s. //deepvariant/realigner:fast_pass_aligner_test PASSED in 0.1s. //deepvariant/realigner:realigner_test PASSED in 3.1s. //deepvariant/realigner:ssw_test PASSED in 0.1s. //deepvariant/realigner:window_selector_test PASSED in 1.8s. //deepvariant/realigner/allele_count_linear:generate_trained_model_test PASSED in 2.7s. //deepvariant/realigner/allele_count_linear:model_evaluation_test PASSED in 4.3s. //deepvariant/realigner/python:debruijn_graph_wrap_test PASSED in 1.8s. //deepvariant/realigner/python:ssw_misc_test PASSED in 0.3s. //deepvariant/realigner/python:ssw_wrap_test PASSED in 0.3s. //deepvariant/vendor:timer_test PASSED in 0.5s. //deepvariant/labeler:haplotype_labeler_test FAILED in 2.6s. /root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/testlogs/deepvariant/labeler/haplotype_labeler_test/test.log. //deepvariant:make_examples_test PASSED in 13.4s. Stats over 2 runs: max = 13.4s, min = 12.8s, avg = 13.1s, dev = 0.3s. //deepvariant:model_eval_test PASSED in 40.9s. Stats over 10 runs: max = 40.9s, min = 2.8s, avg = 7.3s, dev = 11.3s. //deepvariant:model_train_test PASSED in 120.0s. Stats over 10 runs: max = 120.0s, min = 3.0s, avg = 38.6s, dev = 44.3s. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:3360,deployability,log,log,3360," //deepvariant:tf_utils_test PASSED in 3.8s. //deepvariant:utils_test PASSED in 0.1s. //deepvariant:variant_caller_test PASSED in 2.4s. //deepvariant:variant_calling_test PASSED in 0.1s. //deepvariant/environment_tests:env_smoke_test PASSED in 0.4s. //deepvariant/environment_tests:protobuf_implementation_test PASSED in 0.8s. //deepvariant/labeler:customized_classes_labeler_test PASSED in 1.8s. //deepvariant/labeler:labeled_examples_to_vcf_test PASSED in 1.9s. //deepvariant/labeler:positional_labeler_test PASSED in 1.8s. //deepvariant/labeler:variant_labeler_test PASSED in 1.8s. //deepvariant/python:allelecounter_wrap_test PASSED in 1.7s. //deepvariant/python:variant_calling_wrap_test PASSED in 1.8s. //deepvariant/realigner:aligner_test PASSED in 1.7s. //deepvariant/realigner:fast_pass_aligner_test PASSED in 0.1s. //deepvariant/realigner:realigner_test PASSED in 3.1s. //deepvariant/realigner:ssw_test PASSED in 0.1s. //deepvariant/realigner:window_selector_test PASSED in 1.8s. //deepvariant/realigner/allele_count_linear:generate_trained_model_test PASSED in 2.7s. //deepvariant/realigner/allele_count_linear:model_evaluation_test PASSED in 4.3s. //deepvariant/realigner/python:debruijn_graph_wrap_test PASSED in 1.8s. //deepvariant/realigner/python:ssw_misc_test PASSED in 0.3s. //deepvariant/realigner/python:ssw_wrap_test PASSED in 0.3s. //deepvariant/vendor:timer_test PASSED in 0.5s. //deepvariant/labeler:haplotype_labeler_test FAILED in 2.6s. /root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/testlogs/deepvariant/labeler/haplotype_labeler_test/test.log. //deepvariant:make_examples_test PASSED in 13.4s. Stats over 2 runs: max = 13.4s, min = 12.8s, avg = 13.1s, dev = 0.3s. //deepvariant:model_eval_test PASSED in 40.9s. Stats over 10 runs: max = 40.9s, min = 2.8s, avg = 7.3s, dev = 11.3s. //deepvariant:model_train_test PASSED in 120.0s. Stats over 10 runs: max = 120.0s, min = 3.0s, avg = 38.6s, dev = 44.3s. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:494,interoperability,share,shared,494,"Hi @DiableJambe and @pichuan ,. This issue can be resolved by IBM Advance Toolchain 11.0 :). Some brief steps for your references of how to build DeepVariant on Power8 & Redhat 7.5. 1. Build the following packages with ""/usr/bin/gcc"". cmake 3.13.3. Protobuf 3.6.1 C++ (static build with --enable-static for bazel). bazel 0.15.0. 2. Install Advance Toolchain 11.0 and build the following packages with /opt/at11.0/bin/gcc. Python 2 and Pip 19.0.2. Protobuf 3.6.1 C++ (uninstall static and build shared). Protobuf 3.6.1 Python (should build and install from source or CLIF will fail). TensorFlow 1.12.0 (fix floatn.h error with the link Floatn.h error: https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f). CLIF. Opencv-python 3.4.5.20 (for tensor2tensor install). Then build DeepVariant will pass, and results here (excepted //deepvariant/labeler:haplotype_labeler_test tracked in issue 154). ```. ================================================================================. (05:42:50) INFO: Elapsed time: 715.015s, Critical Path: 689.68s. (05:42:50) INFO: 1835 processes: 1835 local. (05:42:50) INFO: Build completed, 1 test FAILED, 2433 total actions. //deepvariant:allelecounter_test PASSED in 0.1s. //deepvariant:call_variants_test PASSED in 59.8s. //deepvariant:data_providers_test PASSED in 11.8s. //deepvariant:dv_vcf_constants_test PASSED in 0.5s. //deepvariant:exclude_contigs_test PASSED in 1.6s. //deepvariant:haplotypes_test PASSED in 1.7s. . //deepvariant:modeling_test PASSED in 48.2s. //deepvariant:pileup_image_test PASSED in 1.8s. //deepvariant:postprocess_variants_lib_test PASSED in 0.1s. //deepvariant:postprocess_variants_test PASSED in 4.8s. //deepvariant:resources_test PASSED in 1.8s. //deepvariant:tf_utils_test PASSED in 3.8s. //deepvariant:utils_test PASSED in 0.1s. //deepvariant:variant_caller_test PASSED in 2.4s. //deepvariant:variant_calling_test PASSED in 0.1s. //deepvariant/environment_tests:env_smoke_test PASSED in 0.4s. //deepvariant/environmen",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:205,modifiability,pac,packages,205,"Hi @DiableJambe and @pichuan ,. This issue can be resolved by IBM Advance Toolchain 11.0 :). Some brief steps for your references of how to build DeepVariant on Power8 & Redhat 7.5. 1. Build the following packages with ""/usr/bin/gcc"". cmake 3.13.3. Protobuf 3.6.1 C++ (static build with --enable-static for bazel). bazel 0.15.0. 2. Install Advance Toolchain 11.0 and build the following packages with /opt/at11.0/bin/gcc. Python 2 and Pip 19.0.2. Protobuf 3.6.1 C++ (uninstall static and build shared). Protobuf 3.6.1 Python (should build and install from source or CLIF will fail). TensorFlow 1.12.0 (fix floatn.h error with the link Floatn.h error: https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f). CLIF. Opencv-python 3.4.5.20 (for tensor2tensor install). Then build DeepVariant will pass, and results here (excepted //deepvariant/labeler:haplotype_labeler_test tracked in issue 154). ```. ================================================================================. (05:42:50) INFO: Elapsed time: 715.015s, Critical Path: 689.68s. (05:42:50) INFO: 1835 processes: 1835 local. (05:42:50) INFO: Build completed, 1 test FAILED, 2433 total actions. //deepvariant:allelecounter_test PASSED in 0.1s. //deepvariant:call_variants_test PASSED in 59.8s. //deepvariant:data_providers_test PASSED in 11.8s. //deepvariant:dv_vcf_constants_test PASSED in 0.5s. //deepvariant:exclude_contigs_test PASSED in 1.6s. //deepvariant:haplotypes_test PASSED in 1.7s. . //deepvariant:modeling_test PASSED in 48.2s. //deepvariant:pileup_image_test PASSED in 1.8s. //deepvariant:postprocess_variants_lib_test PASSED in 0.1s. //deepvariant:postprocess_variants_test PASSED in 4.8s. //deepvariant:resources_test PASSED in 1.8s. //deepvariant:tf_utils_test PASSED in 3.8s. //deepvariant:utils_test PASSED in 0.1s. //deepvariant:variant_caller_test PASSED in 2.4s. //deepvariant:variant_calling_test PASSED in 0.1s. //deepvariant/environment_tests:env_smoke_test PASSED in 0.4s. //deepvariant/environmen",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:387,modifiability,pac,packages,387,"Hi @DiableJambe and @pichuan ,. This issue can be resolved by IBM Advance Toolchain 11.0 :). Some brief steps for your references of how to build DeepVariant on Power8 & Redhat 7.5. 1. Build the following packages with ""/usr/bin/gcc"". cmake 3.13.3. Protobuf 3.6.1 C++ (static build with --enable-static for bazel). bazel 0.15.0. 2. Install Advance Toolchain 11.0 and build the following packages with /opt/at11.0/bin/gcc. Python 2 and Pip 19.0.2. Protobuf 3.6.1 C++ (uninstall static and build shared). Protobuf 3.6.1 Python (should build and install from source or CLIF will fail). TensorFlow 1.12.0 (fix floatn.h error with the link Floatn.h error: https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f). CLIF. Opencv-python 3.4.5.20 (for tensor2tensor install). Then build DeepVariant will pass, and results here (excepted //deepvariant/labeler:haplotype_labeler_test tracked in issue 154). ```. ================================================================================. (05:42:50) INFO: Elapsed time: 715.015s, Critical Path: 689.68s. (05:42:50) INFO: 1835 processes: 1835 local. (05:42:50) INFO: Build completed, 1 test FAILED, 2433 total actions. //deepvariant:allelecounter_test PASSED in 0.1s. //deepvariant:call_variants_test PASSED in 59.8s. //deepvariant:data_providers_test PASSED in 11.8s. //deepvariant:dv_vcf_constants_test PASSED in 0.5s. //deepvariant:exclude_contigs_test PASSED in 1.6s. //deepvariant:haplotypes_test PASSED in 1.7s. . //deepvariant:modeling_test PASSED in 48.2s. //deepvariant:pileup_image_test PASSED in 1.8s. //deepvariant:postprocess_variants_lib_test PASSED in 0.1s. //deepvariant:postprocess_variants_test PASSED in 4.8s. //deepvariant:resources_test PASSED in 1.8s. //deepvariant:tf_utils_test PASSED in 3.8s. //deepvariant:utils_test PASSED in 0.1s. //deepvariant:variant_caller_test PASSED in 2.4s. //deepvariant:variant_calling_test PASSED in 0.1s. //deepvariant/environment_tests:env_smoke_test PASSED in 0.4s. //deepvariant/environmen",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:615,performance,error,error,615,"Hi @DiableJambe and @pichuan ,. This issue can be resolved by IBM Advance Toolchain 11.0 :). Some brief steps for your references of how to build DeepVariant on Power8 & Redhat 7.5. 1. Build the following packages with ""/usr/bin/gcc"". cmake 3.13.3. Protobuf 3.6.1 C++ (static build with --enable-static for bazel). bazel 0.15.0. 2. Install Advance Toolchain 11.0 and build the following packages with /opt/at11.0/bin/gcc. Python 2 and Pip 19.0.2. Protobuf 3.6.1 C++ (uninstall static and build shared). Protobuf 3.6.1 Python (should build and install from source or CLIF will fail). TensorFlow 1.12.0 (fix floatn.h error with the link Floatn.h error: https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f). CLIF. Opencv-python 3.4.5.20 (for tensor2tensor install). Then build DeepVariant will pass, and results here (excepted //deepvariant/labeler:haplotype_labeler_test tracked in issue 154). ```. ================================================================================. (05:42:50) INFO: Elapsed time: 715.015s, Critical Path: 689.68s. (05:42:50) INFO: 1835 processes: 1835 local. (05:42:50) INFO: Build completed, 1 test FAILED, 2433 total actions. //deepvariant:allelecounter_test PASSED in 0.1s. //deepvariant:call_variants_test PASSED in 59.8s. //deepvariant:data_providers_test PASSED in 11.8s. //deepvariant:dv_vcf_constants_test PASSED in 0.5s. //deepvariant:exclude_contigs_test PASSED in 1.6s. //deepvariant:haplotypes_test PASSED in 1.7s. . //deepvariant:modeling_test PASSED in 48.2s. //deepvariant:pileup_image_test PASSED in 1.8s. //deepvariant:postprocess_variants_lib_test PASSED in 0.1s. //deepvariant:postprocess_variants_test PASSED in 4.8s. //deepvariant:resources_test PASSED in 1.8s. //deepvariant:tf_utils_test PASSED in 3.8s. //deepvariant:utils_test PASSED in 0.1s. //deepvariant:variant_caller_test PASSED in 2.4s. //deepvariant:variant_calling_test PASSED in 0.1s. //deepvariant/environment_tests:env_smoke_test PASSED in 0.4s. //deepvariant/environmen",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:644,performance,error,error,644,"Hi @DiableJambe and @pichuan ,. This issue can be resolved by IBM Advance Toolchain 11.0 :). Some brief steps for your references of how to build DeepVariant on Power8 & Redhat 7.5. 1. Build the following packages with ""/usr/bin/gcc"". cmake 3.13.3. Protobuf 3.6.1 C++ (static build with --enable-static for bazel). bazel 0.15.0. 2. Install Advance Toolchain 11.0 and build the following packages with /opt/at11.0/bin/gcc. Python 2 and Pip 19.0.2. Protobuf 3.6.1 C++ (uninstall static and build shared). Protobuf 3.6.1 Python (should build and install from source or CLIF will fail). TensorFlow 1.12.0 (fix floatn.h error with the link Floatn.h error: https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f). CLIF. Opencv-python 3.4.5.20 (for tensor2tensor install). Then build DeepVariant will pass, and results here (excepted //deepvariant/labeler:haplotype_labeler_test tracked in issue 154). ```. ================================================================================. (05:42:50) INFO: Elapsed time: 715.015s, Critical Path: 689.68s. (05:42:50) INFO: 1835 processes: 1835 local. (05:42:50) INFO: Build completed, 1 test FAILED, 2433 total actions. //deepvariant:allelecounter_test PASSED in 0.1s. //deepvariant:call_variants_test PASSED in 59.8s. //deepvariant:data_providers_test PASSED in 11.8s. //deepvariant:dv_vcf_constants_test PASSED in 0.5s. //deepvariant:exclude_contigs_test PASSED in 1.6s. //deepvariant:haplotypes_test PASSED in 1.7s. . //deepvariant:modeling_test PASSED in 48.2s. //deepvariant:pileup_image_test PASSED in 1.8s. //deepvariant:postprocess_variants_lib_test PASSED in 0.1s. //deepvariant:postprocess_variants_test PASSED in 4.8s. //deepvariant:resources_test PASSED in 1.8s. //deepvariant:tf_utils_test PASSED in 3.8s. //deepvariant:utils_test PASSED in 0.1s. //deepvariant:variant_caller_test PASSED in 2.4s. //deepvariant:variant_calling_test PASSED in 0.1s. //deepvariant/environment_tests:env_smoke_test PASSED in 0.4s. //deepvariant/environmen",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:1017,performance,time,time,1017," @pichuan ,. This issue can be resolved by IBM Advance Toolchain 11.0 :). Some brief steps for your references of how to build DeepVariant on Power8 & Redhat 7.5. 1. Build the following packages with ""/usr/bin/gcc"". cmake 3.13.3. Protobuf 3.6.1 C++ (static build with --enable-static for bazel). bazel 0.15.0. 2. Install Advance Toolchain 11.0 and build the following packages with /opt/at11.0/bin/gcc. Python 2 and Pip 19.0.2. Protobuf 3.6.1 C++ (uninstall static and build shared). Protobuf 3.6.1 Python (should build and install from source or CLIF will fail). TensorFlow 1.12.0 (fix floatn.h error with the link Floatn.h error: https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f). CLIF. Opencv-python 3.4.5.20 (for tensor2tensor install). Then build DeepVariant will pass, and results here (excepted //deepvariant/labeler:haplotype_labeler_test tracked in issue 154). ```. ================================================================================. (05:42:50) INFO: Elapsed time: 715.015s, Critical Path: 689.68s. (05:42:50) INFO: 1835 processes: 1835 local. (05:42:50) INFO: Build completed, 1 test FAILED, 2433 total actions. //deepvariant:allelecounter_test PASSED in 0.1s. //deepvariant:call_variants_test PASSED in 59.8s. //deepvariant:data_providers_test PASSED in 11.8s. //deepvariant:dv_vcf_constants_test PASSED in 0.5s. //deepvariant:exclude_contigs_test PASSED in 1.6s. //deepvariant:haplotypes_test PASSED in 1.7s. . //deepvariant:modeling_test PASSED in 48.2s. //deepvariant:pileup_image_test PASSED in 1.8s. //deepvariant:postprocess_variants_lib_test PASSED in 0.1s. //deepvariant:postprocess_variants_test PASSED in 4.8s. //deepvariant:resources_test PASSED in 1.8s. //deepvariant:tf_utils_test PASSED in 3.8s. //deepvariant:utils_test PASSED in 0.1s. //deepvariant:variant_caller_test PASSED in 2.4s. //deepvariant:variant_calling_test PASSED in 0.1s. //deepvariant/environment_tests:env_smoke_test PASSED in 0.4s. //deepvariant/environment_tests:protobuf_im",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:3196,performance,cach,cache,3196," //deepvariant:tf_utils_test PASSED in 3.8s. //deepvariant:utils_test PASSED in 0.1s. //deepvariant:variant_caller_test PASSED in 2.4s. //deepvariant:variant_calling_test PASSED in 0.1s. //deepvariant/environment_tests:env_smoke_test PASSED in 0.4s. //deepvariant/environment_tests:protobuf_implementation_test PASSED in 0.8s. //deepvariant/labeler:customized_classes_labeler_test PASSED in 1.8s. //deepvariant/labeler:labeled_examples_to_vcf_test PASSED in 1.9s. //deepvariant/labeler:positional_labeler_test PASSED in 1.8s. //deepvariant/labeler:variant_labeler_test PASSED in 1.8s. //deepvariant/python:allelecounter_wrap_test PASSED in 1.7s. //deepvariant/python:variant_calling_wrap_test PASSED in 1.8s. //deepvariant/realigner:aligner_test PASSED in 1.7s. //deepvariant/realigner:fast_pass_aligner_test PASSED in 0.1s. //deepvariant/realigner:realigner_test PASSED in 3.1s. //deepvariant/realigner:ssw_test PASSED in 0.1s. //deepvariant/realigner:window_selector_test PASSED in 1.8s. //deepvariant/realigner/allele_count_linear:generate_trained_model_test PASSED in 2.7s. //deepvariant/realigner/allele_count_linear:model_evaluation_test PASSED in 4.3s. //deepvariant/realigner/python:debruijn_graph_wrap_test PASSED in 1.8s. //deepvariant/realigner/python:ssw_misc_test PASSED in 0.3s. //deepvariant/realigner/python:ssw_wrap_test PASSED in 0.3s. //deepvariant/vendor:timer_test PASSED in 0.5s. //deepvariant/labeler:haplotype_labeler_test FAILED in 2.6s. /root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/testlogs/deepvariant/labeler/haplotype_labeler_test/test.log. //deepvariant:make_examples_test PASSED in 13.4s. Stats over 2 runs: max = 13.4s, min = 12.8s, avg = 13.1s, dev = 0.3s. //deepvariant:model_eval_test PASSED in 40.9s. Stats over 10 runs: max = 40.9s, min = 2.8s, avg = 7.3s, dev = 11.3s. //deepvariant:model_train_test PASSED in 120.0s. Stats over 10 runs: max = 120.0s, min = 3.0s, avg = 38.6s, dev = 44.3s. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:576,reliability,fail,fail,576,"Hi @DiableJambe and @pichuan ,. This issue can be resolved by IBM Advance Toolchain 11.0 :). Some brief steps for your references of how to build DeepVariant on Power8 & Redhat 7.5. 1. Build the following packages with ""/usr/bin/gcc"". cmake 3.13.3. Protobuf 3.6.1 C++ (static build with --enable-static for bazel). bazel 0.15.0. 2. Install Advance Toolchain 11.0 and build the following packages with /opt/at11.0/bin/gcc. Python 2 and Pip 19.0.2. Protobuf 3.6.1 C++ (uninstall static and build shared). Protobuf 3.6.1 Python (should build and install from source or CLIF will fail). TensorFlow 1.12.0 (fix floatn.h error with the link Floatn.h error: https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f). CLIF. Opencv-python 3.4.5.20 (for tensor2tensor install). Then build DeepVariant will pass, and results here (excepted //deepvariant/labeler:haplotype_labeler_test tracked in issue 154). ```. ================================================================================. (05:42:50) INFO: Elapsed time: 715.015s, Critical Path: 689.68s. (05:42:50) INFO: 1835 processes: 1835 local. (05:42:50) INFO: Build completed, 1 test FAILED, 2433 total actions. //deepvariant:allelecounter_test PASSED in 0.1s. //deepvariant:call_variants_test PASSED in 59.8s. //deepvariant:data_providers_test PASSED in 11.8s. //deepvariant:dv_vcf_constants_test PASSED in 0.5s. //deepvariant:exclude_contigs_test PASSED in 1.6s. //deepvariant:haplotypes_test PASSED in 1.7s. . //deepvariant:modeling_test PASSED in 48.2s. //deepvariant:pileup_image_test PASSED in 1.8s. //deepvariant:postprocess_variants_lib_test PASSED in 0.1s. //deepvariant:postprocess_variants_test PASSED in 4.8s. //deepvariant:resources_test PASSED in 1.8s. //deepvariant:tf_utils_test PASSED in 3.8s. //deepvariant:utils_test PASSED in 0.1s. //deepvariant:variant_caller_test PASSED in 2.4s. //deepvariant:variant_calling_test PASSED in 0.1s. //deepvariant/environment_tests:env_smoke_test PASSED in 0.4s. //deepvariant/environmen",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:1143,reliability,FAIL,FAILED,1143,"DeepVariant on Power8 & Redhat 7.5. 1. Build the following packages with ""/usr/bin/gcc"". cmake 3.13.3. Protobuf 3.6.1 C++ (static build with --enable-static for bazel). bazel 0.15.0. 2. Install Advance Toolchain 11.0 and build the following packages with /opt/at11.0/bin/gcc. Python 2 and Pip 19.0.2. Protobuf 3.6.1 C++ (uninstall static and build shared). Protobuf 3.6.1 Python (should build and install from source or CLIF will fail). TensorFlow 1.12.0 (fix floatn.h error with the link Floatn.h error: https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f). CLIF. Opencv-python 3.4.5.20 (for tensor2tensor install). Then build DeepVariant will pass, and results here (excepted //deepvariant/labeler:haplotype_labeler_test tracked in issue 154). ```. ================================================================================. (05:42:50) INFO: Elapsed time: 715.015s, Critical Path: 689.68s. (05:42:50) INFO: 1835 processes: 1835 local. (05:42:50) INFO: Build completed, 1 test FAILED, 2433 total actions. //deepvariant:allelecounter_test PASSED in 0.1s. //deepvariant:call_variants_test PASSED in 59.8s. //deepvariant:data_providers_test PASSED in 11.8s. //deepvariant:dv_vcf_constants_test PASSED in 0.5s. //deepvariant:exclude_contigs_test PASSED in 1.6s. //deepvariant:haplotypes_test PASSED in 1.7s. . //deepvariant:modeling_test PASSED in 48.2s. //deepvariant:pileup_image_test PASSED in 1.8s. //deepvariant:postprocess_variants_lib_test PASSED in 0.1s. //deepvariant:postprocess_variants_test PASSED in 4.8s. //deepvariant:resources_test PASSED in 1.8s. //deepvariant:tf_utils_test PASSED in 3.8s. //deepvariant:utils_test PASSED in 0.1s. //deepvariant:variant_caller_test PASSED in 2.4s. //deepvariant:variant_calling_test PASSED in 0.1s. //deepvariant/environment_tests:env_smoke_test PASSED in 0.4s. //deepvariant/environment_tests:protobuf_implementation_test PASSED in 0.8s. //deepvariant/labeler:customized_classes_labeler_test PASSED in 1.8s. //deepvariant/labeler:l",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:3173,reliability,FAIL,FAILED,3173," //deepvariant:tf_utils_test PASSED in 3.8s. //deepvariant:utils_test PASSED in 0.1s. //deepvariant:variant_caller_test PASSED in 2.4s. //deepvariant:variant_calling_test PASSED in 0.1s. //deepvariant/environment_tests:env_smoke_test PASSED in 0.4s. //deepvariant/environment_tests:protobuf_implementation_test PASSED in 0.8s. //deepvariant/labeler:customized_classes_labeler_test PASSED in 1.8s. //deepvariant/labeler:labeled_examples_to_vcf_test PASSED in 1.9s. //deepvariant/labeler:positional_labeler_test PASSED in 1.8s. //deepvariant/labeler:variant_labeler_test PASSED in 1.8s. //deepvariant/python:allelecounter_wrap_test PASSED in 1.7s. //deepvariant/python:variant_calling_wrap_test PASSED in 1.8s. //deepvariant/realigner:aligner_test PASSED in 1.7s. //deepvariant/realigner:fast_pass_aligner_test PASSED in 0.1s. //deepvariant/realigner:realigner_test PASSED in 3.1s. //deepvariant/realigner:ssw_test PASSED in 0.1s. //deepvariant/realigner:window_selector_test PASSED in 1.8s. //deepvariant/realigner/allele_count_linear:generate_trained_model_test PASSED in 2.7s. //deepvariant/realigner/allele_count_linear:model_evaluation_test PASSED in 4.3s. //deepvariant/realigner/python:debruijn_graph_wrap_test PASSED in 1.8s. //deepvariant/realigner/python:ssw_misc_test PASSED in 0.3s. //deepvariant/realigner/python:ssw_wrap_test PASSED in 0.3s. //deepvariant/vendor:timer_test PASSED in 0.5s. //deepvariant/labeler:haplotype_labeler_test FAILED in 2.6s. /root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/testlogs/deepvariant/labeler/haplotype_labeler_test/test.log. //deepvariant:make_examples_test PASSED in 13.4s. Stats over 2 runs: max = 13.4s, min = 12.8s, avg = 13.1s, dev = 0.3s. //deepvariant:model_eval_test PASSED in 40.9s. Stats over 10 runs: max = 40.9s, min = 2.8s, avg = 7.3s, dev = 11.3s. //deepvariant:model_train_test PASSED in 120.0s. Stats over 10 runs: max = 120.0s, min = 3.0s, avg = 38.6s, dev = 44.3s. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:615,safety,error,error,615,"Hi @DiableJambe and @pichuan ,. This issue can be resolved by IBM Advance Toolchain 11.0 :). Some brief steps for your references of how to build DeepVariant on Power8 & Redhat 7.5. 1. Build the following packages with ""/usr/bin/gcc"". cmake 3.13.3. Protobuf 3.6.1 C++ (static build with --enable-static for bazel). bazel 0.15.0. 2. Install Advance Toolchain 11.0 and build the following packages with /opt/at11.0/bin/gcc. Python 2 and Pip 19.0.2. Protobuf 3.6.1 C++ (uninstall static and build shared). Protobuf 3.6.1 Python (should build and install from source or CLIF will fail). TensorFlow 1.12.0 (fix floatn.h error with the link Floatn.h error: https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f). CLIF. Opencv-python 3.4.5.20 (for tensor2tensor install). Then build DeepVariant will pass, and results here (excepted //deepvariant/labeler:haplotype_labeler_test tracked in issue 154). ```. ================================================================================. (05:42:50) INFO: Elapsed time: 715.015s, Critical Path: 689.68s. (05:42:50) INFO: 1835 processes: 1835 local. (05:42:50) INFO: Build completed, 1 test FAILED, 2433 total actions. //deepvariant:allelecounter_test PASSED in 0.1s. //deepvariant:call_variants_test PASSED in 59.8s. //deepvariant:data_providers_test PASSED in 11.8s. //deepvariant:dv_vcf_constants_test PASSED in 0.5s. //deepvariant:exclude_contigs_test PASSED in 1.6s. //deepvariant:haplotypes_test PASSED in 1.7s. . //deepvariant:modeling_test PASSED in 48.2s. //deepvariant:pileup_image_test PASSED in 1.8s. //deepvariant:postprocess_variants_lib_test PASSED in 0.1s. //deepvariant:postprocess_variants_test PASSED in 4.8s. //deepvariant:resources_test PASSED in 1.8s. //deepvariant:tf_utils_test PASSED in 3.8s. //deepvariant:utils_test PASSED in 0.1s. //deepvariant:variant_caller_test PASSED in 2.4s. //deepvariant:variant_calling_test PASSED in 0.1s. //deepvariant/environment_tests:env_smoke_test PASSED in 0.4s. //deepvariant/environmen",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:644,safety,error,error,644,"Hi @DiableJambe and @pichuan ,. This issue can be resolved by IBM Advance Toolchain 11.0 :). Some brief steps for your references of how to build DeepVariant on Power8 & Redhat 7.5. 1. Build the following packages with ""/usr/bin/gcc"". cmake 3.13.3. Protobuf 3.6.1 C++ (static build with --enable-static for bazel). bazel 0.15.0. 2. Install Advance Toolchain 11.0 and build the following packages with /opt/at11.0/bin/gcc. Python 2 and Pip 19.0.2. Protobuf 3.6.1 C++ (uninstall static and build shared). Protobuf 3.6.1 Python (should build and install from source or CLIF will fail). TensorFlow 1.12.0 (fix floatn.h error with the link Floatn.h error: https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f). CLIF. Opencv-python 3.4.5.20 (for tensor2tensor install). Then build DeepVariant will pass, and results here (excepted //deepvariant/labeler:haplotype_labeler_test tracked in issue 154). ```. ================================================================================. (05:42:50) INFO: Elapsed time: 715.015s, Critical Path: 689.68s. (05:42:50) INFO: 1835 processes: 1835 local. (05:42:50) INFO: Build completed, 1 test FAILED, 2433 total actions. //deepvariant:allelecounter_test PASSED in 0.1s. //deepvariant:call_variants_test PASSED in 59.8s. //deepvariant:data_providers_test PASSED in 11.8s. //deepvariant:dv_vcf_constants_test PASSED in 0.5s. //deepvariant:exclude_contigs_test PASSED in 1.6s. //deepvariant:haplotypes_test PASSED in 1.7s. . //deepvariant:modeling_test PASSED in 48.2s. //deepvariant:pileup_image_test PASSED in 1.8s. //deepvariant:postprocess_variants_lib_test PASSED in 0.1s. //deepvariant:postprocess_variants_test PASSED in 4.8s. //deepvariant:resources_test PASSED in 1.8s. //deepvariant:tf_utils_test PASSED in 3.8s. //deepvariant:utils_test PASSED in 0.1s. //deepvariant:variant_caller_test PASSED in 2.4s. //deepvariant:variant_calling_test PASSED in 0.1s. //deepvariant/environment_tests:env_smoke_test PASSED in 0.4s. //deepvariant/environmen",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:828,safety,except,excepted,828,"Hi @DiableJambe and @pichuan ,. This issue can be resolved by IBM Advance Toolchain 11.0 :). Some brief steps for your references of how to build DeepVariant on Power8 & Redhat 7.5. 1. Build the following packages with ""/usr/bin/gcc"". cmake 3.13.3. Protobuf 3.6.1 C++ (static build with --enable-static for bazel). bazel 0.15.0. 2. Install Advance Toolchain 11.0 and build the following packages with /opt/at11.0/bin/gcc. Python 2 and Pip 19.0.2. Protobuf 3.6.1 C++ (uninstall static and build shared). Protobuf 3.6.1 Python (should build and install from source or CLIF will fail). TensorFlow 1.12.0 (fix floatn.h error with the link Floatn.h error: https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f). CLIF. Opencv-python 3.4.5.20 (for tensor2tensor install). Then build DeepVariant will pass, and results here (excepted //deepvariant/labeler:haplotype_labeler_test tracked in issue 154). ```. ================================================================================. (05:42:50) INFO: Elapsed time: 715.015s, Critical Path: 689.68s. (05:42:50) INFO: 1835 processes: 1835 local. (05:42:50) INFO: Build completed, 1 test FAILED, 2433 total actions. //deepvariant:allelecounter_test PASSED in 0.1s. //deepvariant:call_variants_test PASSED in 59.8s. //deepvariant:data_providers_test PASSED in 11.8s. //deepvariant:dv_vcf_constants_test PASSED in 0.5s. //deepvariant:exclude_contigs_test PASSED in 1.6s. //deepvariant:haplotypes_test PASSED in 1.7s. . //deepvariant:modeling_test PASSED in 48.2s. //deepvariant:pileup_image_test PASSED in 1.8s. //deepvariant:postprocess_variants_lib_test PASSED in 0.1s. //deepvariant:postprocess_variants_test PASSED in 4.8s. //deepvariant:resources_test PASSED in 1.8s. //deepvariant:tf_utils_test PASSED in 3.8s. //deepvariant:utils_test PASSED in 0.1s. //deepvariant:variant_caller_test PASSED in 2.4s. //deepvariant:variant_calling_test PASSED in 0.1s. //deepvariant/environment_tests:env_smoke_test PASSED in 0.4s. //deepvariant/environmen",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:1125,safety,compl,completed,1125,"of how to build DeepVariant on Power8 & Redhat 7.5. 1. Build the following packages with ""/usr/bin/gcc"". cmake 3.13.3. Protobuf 3.6.1 C++ (static build with --enable-static for bazel). bazel 0.15.0. 2. Install Advance Toolchain 11.0 and build the following packages with /opt/at11.0/bin/gcc. Python 2 and Pip 19.0.2. Protobuf 3.6.1 C++ (uninstall static and build shared). Protobuf 3.6.1 Python (should build and install from source or CLIF will fail). TensorFlow 1.12.0 (fix floatn.h error with the link Floatn.h error: https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f). CLIF. Opencv-python 3.4.5.20 (for tensor2tensor install). Then build DeepVariant will pass, and results here (excepted //deepvariant/labeler:haplotype_labeler_test tracked in issue 154). ```. ================================================================================. (05:42:50) INFO: Elapsed time: 715.015s, Critical Path: 689.68s. (05:42:50) INFO: 1835 processes: 1835 local. (05:42:50) INFO: Build completed, 1 test FAILED, 2433 total actions. //deepvariant:allelecounter_test PASSED in 0.1s. //deepvariant:call_variants_test PASSED in 59.8s. //deepvariant:data_providers_test PASSED in 11.8s. //deepvariant:dv_vcf_constants_test PASSED in 0.5s. //deepvariant:exclude_contigs_test PASSED in 1.6s. //deepvariant:haplotypes_test PASSED in 1.7s. . //deepvariant:modeling_test PASSED in 48.2s. //deepvariant:pileup_image_test PASSED in 1.8s. //deepvariant:postprocess_variants_lib_test PASSED in 0.1s. //deepvariant:postprocess_variants_test PASSED in 4.8s. //deepvariant:resources_test PASSED in 1.8s. //deepvariant:tf_utils_test PASSED in 3.8s. //deepvariant:utils_test PASSED in 0.1s. //deepvariant:variant_caller_test PASSED in 2.4s. //deepvariant:variant_calling_test PASSED in 0.1s. //deepvariant/environment_tests:env_smoke_test PASSED in 0.4s. //deepvariant/environment_tests:protobuf_implementation_test PASSED in 0.8s. //deepvariant/labeler:customized_classes_labeler_test PASSED in 1.8s. //deepv",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:1138,safety,test,test,1138,"build DeepVariant on Power8 & Redhat 7.5. 1. Build the following packages with ""/usr/bin/gcc"". cmake 3.13.3. Protobuf 3.6.1 C++ (static build with --enable-static for bazel). bazel 0.15.0. 2. Install Advance Toolchain 11.0 and build the following packages with /opt/at11.0/bin/gcc. Python 2 and Pip 19.0.2. Protobuf 3.6.1 C++ (uninstall static and build shared). Protobuf 3.6.1 Python (should build and install from source or CLIF will fail). TensorFlow 1.12.0 (fix floatn.h error with the link Floatn.h error: https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f). CLIF. Opencv-python 3.4.5.20 (for tensor2tensor install). Then build DeepVariant will pass, and results here (excepted //deepvariant/labeler:haplotype_labeler_test tracked in issue 154). ```. ================================================================================. (05:42:50) INFO: Elapsed time: 715.015s, Critical Path: 689.68s. (05:42:50) INFO: 1835 processes: 1835 local. (05:42:50) INFO: Build completed, 1 test FAILED, 2433 total actions. //deepvariant:allelecounter_test PASSED in 0.1s. //deepvariant:call_variants_test PASSED in 59.8s. //deepvariant:data_providers_test PASSED in 11.8s. //deepvariant:dv_vcf_constants_test PASSED in 0.5s. //deepvariant:exclude_contigs_test PASSED in 1.6s. //deepvariant:haplotypes_test PASSED in 1.7s. . //deepvariant:modeling_test PASSED in 48.2s. //deepvariant:pileup_image_test PASSED in 1.8s. //deepvariant:postprocess_variants_lib_test PASSED in 0.1s. //deepvariant:postprocess_variants_test PASSED in 4.8s. //deepvariant:resources_test PASSED in 1.8s. //deepvariant:tf_utils_test PASSED in 3.8s. //deepvariant:utils_test PASSED in 0.1s. //deepvariant:variant_caller_test PASSED in 2.4s. //deepvariant:variant_calling_test PASSED in 0.1s. //deepvariant/environment_tests:env_smoke_test PASSED in 0.4s. //deepvariant/environment_tests:protobuf_implementation_test PASSED in 0.8s. //deepvariant/labeler:customized_classes_labeler_test PASSED in 1.8s. //deepvariant/lab",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:3303,safety,test,testlogs,3303," //deepvariant:tf_utils_test PASSED in 3.8s. //deepvariant:utils_test PASSED in 0.1s. //deepvariant:variant_caller_test PASSED in 2.4s. //deepvariant:variant_calling_test PASSED in 0.1s. //deepvariant/environment_tests:env_smoke_test PASSED in 0.4s. //deepvariant/environment_tests:protobuf_implementation_test PASSED in 0.8s. //deepvariant/labeler:customized_classes_labeler_test PASSED in 1.8s. //deepvariant/labeler:labeled_examples_to_vcf_test PASSED in 1.9s. //deepvariant/labeler:positional_labeler_test PASSED in 1.8s. //deepvariant/labeler:variant_labeler_test PASSED in 1.8s. //deepvariant/python:allelecounter_wrap_test PASSED in 1.7s. //deepvariant/python:variant_calling_wrap_test PASSED in 1.8s. //deepvariant/realigner:aligner_test PASSED in 1.7s. //deepvariant/realigner:fast_pass_aligner_test PASSED in 0.1s. //deepvariant/realigner:realigner_test PASSED in 3.1s. //deepvariant/realigner:ssw_test PASSED in 0.1s. //deepvariant/realigner:window_selector_test PASSED in 1.8s. //deepvariant/realigner/allele_count_linear:generate_trained_model_test PASSED in 2.7s. //deepvariant/realigner/allele_count_linear:model_evaluation_test PASSED in 4.3s. //deepvariant/realigner/python:debruijn_graph_wrap_test PASSED in 1.8s. //deepvariant/realigner/python:ssw_misc_test PASSED in 0.3s. //deepvariant/realigner/python:ssw_wrap_test PASSED in 0.3s. //deepvariant/vendor:timer_test PASSED in 0.5s. //deepvariant/labeler:haplotype_labeler_test FAILED in 2.6s. /root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/testlogs/deepvariant/labeler/haplotype_labeler_test/test.log. //deepvariant:make_examples_test PASSED in 13.4s. Stats over 2 runs: max = 13.4s, min = 12.8s, avg = 13.1s, dev = 0.3s. //deepvariant:model_eval_test PASSED in 40.9s. Stats over 10 runs: max = 40.9s, min = 2.8s, avg = 7.3s, dev = 11.3s. //deepvariant:model_train_test PASSED in 120.0s. Stats over 10 runs: max = 120.0s, min = 3.0s, avg = 38.6s, dev = 44.3s. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:3355,safety,test,test,3355," //deepvariant:tf_utils_test PASSED in 3.8s. //deepvariant:utils_test PASSED in 0.1s. //deepvariant:variant_caller_test PASSED in 2.4s. //deepvariant:variant_calling_test PASSED in 0.1s. //deepvariant/environment_tests:env_smoke_test PASSED in 0.4s. //deepvariant/environment_tests:protobuf_implementation_test PASSED in 0.8s. //deepvariant/labeler:customized_classes_labeler_test PASSED in 1.8s. //deepvariant/labeler:labeled_examples_to_vcf_test PASSED in 1.9s. //deepvariant/labeler:positional_labeler_test PASSED in 1.8s. //deepvariant/labeler:variant_labeler_test PASSED in 1.8s. //deepvariant/python:allelecounter_wrap_test PASSED in 1.7s. //deepvariant/python:variant_calling_wrap_test PASSED in 1.8s. //deepvariant/realigner:aligner_test PASSED in 1.7s. //deepvariant/realigner:fast_pass_aligner_test PASSED in 0.1s. //deepvariant/realigner:realigner_test PASSED in 3.1s. //deepvariant/realigner:ssw_test PASSED in 0.1s. //deepvariant/realigner:window_selector_test PASSED in 1.8s. //deepvariant/realigner/allele_count_linear:generate_trained_model_test PASSED in 2.7s. //deepvariant/realigner/allele_count_linear:model_evaluation_test PASSED in 4.3s. //deepvariant/realigner/python:debruijn_graph_wrap_test PASSED in 1.8s. //deepvariant/realigner/python:ssw_misc_test PASSED in 0.3s. //deepvariant/realigner/python:ssw_wrap_test PASSED in 0.3s. //deepvariant/vendor:timer_test PASSED in 0.5s. //deepvariant/labeler:haplotype_labeler_test FAILED in 2.6s. /root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/testlogs/deepvariant/labeler/haplotype_labeler_test/test.log. //deepvariant:make_examples_test PASSED in 13.4s. Stats over 2 runs: max = 13.4s, min = 12.8s, avg = 13.1s, dev = 0.3s. //deepvariant:model_eval_test PASSED in 40.9s. Stats over 10 runs: max = 40.9s, min = 2.8s, avg = 7.3s, dev = 11.3s. //deepvariant:model_train_test PASSED in 120.0s. Stats over 10 runs: max = 120.0s, min = 3.0s, avg = 38.6s, dev = 44.3s. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:3360,safety,log,log,3360," //deepvariant:tf_utils_test PASSED in 3.8s. //deepvariant:utils_test PASSED in 0.1s. //deepvariant:variant_caller_test PASSED in 2.4s. //deepvariant:variant_calling_test PASSED in 0.1s. //deepvariant/environment_tests:env_smoke_test PASSED in 0.4s. //deepvariant/environment_tests:protobuf_implementation_test PASSED in 0.8s. //deepvariant/labeler:customized_classes_labeler_test PASSED in 1.8s. //deepvariant/labeler:labeled_examples_to_vcf_test PASSED in 1.9s. //deepvariant/labeler:positional_labeler_test PASSED in 1.8s. //deepvariant/labeler:variant_labeler_test PASSED in 1.8s. //deepvariant/python:allelecounter_wrap_test PASSED in 1.7s. //deepvariant/python:variant_calling_wrap_test PASSED in 1.8s. //deepvariant/realigner:aligner_test PASSED in 1.7s. //deepvariant/realigner:fast_pass_aligner_test PASSED in 0.1s. //deepvariant/realigner:realigner_test PASSED in 3.1s. //deepvariant/realigner:ssw_test PASSED in 0.1s. //deepvariant/realigner:window_selector_test PASSED in 1.8s. //deepvariant/realigner/allele_count_linear:generate_trained_model_test PASSED in 2.7s. //deepvariant/realigner/allele_count_linear:model_evaluation_test PASSED in 4.3s. //deepvariant/realigner/python:debruijn_graph_wrap_test PASSED in 1.8s. //deepvariant/realigner/python:ssw_misc_test PASSED in 0.3s. //deepvariant/realigner/python:ssw_wrap_test PASSED in 0.3s. //deepvariant/vendor:timer_test PASSED in 0.5s. //deepvariant/labeler:haplotype_labeler_test FAILED in 2.6s. /root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/testlogs/deepvariant/labeler/haplotype_labeler_test/test.log. //deepvariant:make_examples_test PASSED in 13.4s. Stats over 2 runs: max = 13.4s, min = 12.8s, avg = 13.1s, dev = 0.3s. //deepvariant:model_eval_test PASSED in 40.9s. Stats over 10 runs: max = 40.9s, min = 2.8s, avg = 7.3s, dev = 11.3s. //deepvariant:model_train_test PASSED in 120.0s. Stats over 10 runs: max = 120.0s, min = 3.0s, avg = 38.6s, dev = 44.3s. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:1125,security,compl,completed,1125,"of how to build DeepVariant on Power8 & Redhat 7.5. 1. Build the following packages with ""/usr/bin/gcc"". cmake 3.13.3. Protobuf 3.6.1 C++ (static build with --enable-static for bazel). bazel 0.15.0. 2. Install Advance Toolchain 11.0 and build the following packages with /opt/at11.0/bin/gcc. Python 2 and Pip 19.0.2. Protobuf 3.6.1 C++ (uninstall static and build shared). Protobuf 3.6.1 Python (should build and install from source or CLIF will fail). TensorFlow 1.12.0 (fix floatn.h error with the link Floatn.h error: https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f). CLIF. Opencv-python 3.4.5.20 (for tensor2tensor install). Then build DeepVariant will pass, and results here (excepted //deepvariant/labeler:haplotype_labeler_test tracked in issue 154). ```. ================================================================================. (05:42:50) INFO: Elapsed time: 715.015s, Critical Path: 689.68s. (05:42:50) INFO: 1835 processes: 1835 local. (05:42:50) INFO: Build completed, 1 test FAILED, 2433 total actions. //deepvariant:allelecounter_test PASSED in 0.1s. //deepvariant:call_variants_test PASSED in 59.8s. //deepvariant:data_providers_test PASSED in 11.8s. //deepvariant:dv_vcf_constants_test PASSED in 0.5s. //deepvariant:exclude_contigs_test PASSED in 1.6s. //deepvariant:haplotypes_test PASSED in 1.7s. . //deepvariant:modeling_test PASSED in 48.2s. //deepvariant:pileup_image_test PASSED in 1.8s. //deepvariant:postprocess_variants_lib_test PASSED in 0.1s. //deepvariant:postprocess_variants_test PASSED in 4.8s. //deepvariant:resources_test PASSED in 1.8s. //deepvariant:tf_utils_test PASSED in 3.8s. //deepvariant:utils_test PASSED in 0.1s. //deepvariant:variant_caller_test PASSED in 2.4s. //deepvariant:variant_calling_test PASSED in 0.1s. //deepvariant/environment_tests:env_smoke_test PASSED in 0.4s. //deepvariant/environment_tests:protobuf_implementation_test PASSED in 0.8s. //deepvariant/labeler:customized_classes_labeler_test PASSED in 1.8s. //deepv",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:3360,security,log,log,3360," //deepvariant:tf_utils_test PASSED in 3.8s. //deepvariant:utils_test PASSED in 0.1s. //deepvariant:variant_caller_test PASSED in 2.4s. //deepvariant:variant_calling_test PASSED in 0.1s. //deepvariant/environment_tests:env_smoke_test PASSED in 0.4s. //deepvariant/environment_tests:protobuf_implementation_test PASSED in 0.8s. //deepvariant/labeler:customized_classes_labeler_test PASSED in 1.8s. //deepvariant/labeler:labeled_examples_to_vcf_test PASSED in 1.9s. //deepvariant/labeler:positional_labeler_test PASSED in 1.8s. //deepvariant/labeler:variant_labeler_test PASSED in 1.8s. //deepvariant/python:allelecounter_wrap_test PASSED in 1.7s. //deepvariant/python:variant_calling_wrap_test PASSED in 1.8s. //deepvariant/realigner:aligner_test PASSED in 1.7s. //deepvariant/realigner:fast_pass_aligner_test PASSED in 0.1s. //deepvariant/realigner:realigner_test PASSED in 3.1s. //deepvariant/realigner:ssw_test PASSED in 0.1s. //deepvariant/realigner:window_selector_test PASSED in 1.8s. //deepvariant/realigner/allele_count_linear:generate_trained_model_test PASSED in 2.7s. //deepvariant/realigner/allele_count_linear:model_evaluation_test PASSED in 4.3s. //deepvariant/realigner/python:debruijn_graph_wrap_test PASSED in 1.8s. //deepvariant/realigner/python:ssw_misc_test PASSED in 0.3s. //deepvariant/realigner/python:ssw_wrap_test PASSED in 0.3s. //deepvariant/vendor:timer_test PASSED in 0.5s. //deepvariant/labeler:haplotype_labeler_test FAILED in 2.6s. /root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/testlogs/deepvariant/labeler/haplotype_labeler_test/test.log. //deepvariant:make_examples_test PASSED in 13.4s. Stats over 2 runs: max = 13.4s, min = 12.8s, avg = 13.1s, dev = 0.3s. //deepvariant:model_eval_test PASSED in 40.9s. Stats over 10 runs: max = 40.9s, min = 2.8s, avg = 7.3s, dev = 11.3s. //deepvariant:model_train_test PASSED in 120.0s. Stats over 10 runs: max = 120.0s, min = 3.0s, avg = 38.6s, dev = 44.3s. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:1138,testability,test,test,1138,"build DeepVariant on Power8 & Redhat 7.5. 1. Build the following packages with ""/usr/bin/gcc"". cmake 3.13.3. Protobuf 3.6.1 C++ (static build with --enable-static for bazel). bazel 0.15.0. 2. Install Advance Toolchain 11.0 and build the following packages with /opt/at11.0/bin/gcc. Python 2 and Pip 19.0.2. Protobuf 3.6.1 C++ (uninstall static and build shared). Protobuf 3.6.1 Python (should build and install from source or CLIF will fail). TensorFlow 1.12.0 (fix floatn.h error with the link Floatn.h error: https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f). CLIF. Opencv-python 3.4.5.20 (for tensor2tensor install). Then build DeepVariant will pass, and results here (excepted //deepvariant/labeler:haplotype_labeler_test tracked in issue 154). ```. ================================================================================. (05:42:50) INFO: Elapsed time: 715.015s, Critical Path: 689.68s. (05:42:50) INFO: 1835 processes: 1835 local. (05:42:50) INFO: Build completed, 1 test FAILED, 2433 total actions. //deepvariant:allelecounter_test PASSED in 0.1s. //deepvariant:call_variants_test PASSED in 59.8s. //deepvariant:data_providers_test PASSED in 11.8s. //deepvariant:dv_vcf_constants_test PASSED in 0.5s. //deepvariant:exclude_contigs_test PASSED in 1.6s. //deepvariant:haplotypes_test PASSED in 1.7s. . //deepvariant:modeling_test PASSED in 48.2s. //deepvariant:pileup_image_test PASSED in 1.8s. //deepvariant:postprocess_variants_lib_test PASSED in 0.1s. //deepvariant:postprocess_variants_test PASSED in 4.8s. //deepvariant:resources_test PASSED in 1.8s. //deepvariant:tf_utils_test PASSED in 3.8s. //deepvariant:utils_test PASSED in 0.1s. //deepvariant:variant_caller_test PASSED in 2.4s. //deepvariant:variant_calling_test PASSED in 0.1s. //deepvariant/environment_tests:env_smoke_test PASSED in 0.4s. //deepvariant/environment_tests:protobuf_implementation_test PASSED in 0.8s. //deepvariant/labeler:customized_classes_labeler_test PASSED in 1.8s. //deepvariant/lab",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:3303,testability,test,testlogs,3303," //deepvariant:tf_utils_test PASSED in 3.8s. //deepvariant:utils_test PASSED in 0.1s. //deepvariant:variant_caller_test PASSED in 2.4s. //deepvariant:variant_calling_test PASSED in 0.1s. //deepvariant/environment_tests:env_smoke_test PASSED in 0.4s. //deepvariant/environment_tests:protobuf_implementation_test PASSED in 0.8s. //deepvariant/labeler:customized_classes_labeler_test PASSED in 1.8s. //deepvariant/labeler:labeled_examples_to_vcf_test PASSED in 1.9s. //deepvariant/labeler:positional_labeler_test PASSED in 1.8s. //deepvariant/labeler:variant_labeler_test PASSED in 1.8s. //deepvariant/python:allelecounter_wrap_test PASSED in 1.7s. //deepvariant/python:variant_calling_wrap_test PASSED in 1.8s. //deepvariant/realigner:aligner_test PASSED in 1.7s. //deepvariant/realigner:fast_pass_aligner_test PASSED in 0.1s. //deepvariant/realigner:realigner_test PASSED in 3.1s. //deepvariant/realigner:ssw_test PASSED in 0.1s. //deepvariant/realigner:window_selector_test PASSED in 1.8s. //deepvariant/realigner/allele_count_linear:generate_trained_model_test PASSED in 2.7s. //deepvariant/realigner/allele_count_linear:model_evaluation_test PASSED in 4.3s. //deepvariant/realigner/python:debruijn_graph_wrap_test PASSED in 1.8s. //deepvariant/realigner/python:ssw_misc_test PASSED in 0.3s. //deepvariant/realigner/python:ssw_wrap_test PASSED in 0.3s. //deepvariant/vendor:timer_test PASSED in 0.5s. //deepvariant/labeler:haplotype_labeler_test FAILED in 2.6s. /root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/testlogs/deepvariant/labeler/haplotype_labeler_test/test.log. //deepvariant:make_examples_test PASSED in 13.4s. Stats over 2 runs: max = 13.4s, min = 12.8s, avg = 13.1s, dev = 0.3s. //deepvariant:model_eval_test PASSED in 40.9s. Stats over 10 runs: max = 40.9s, min = 2.8s, avg = 7.3s, dev = 11.3s. //deepvariant:model_train_test PASSED in 120.0s. Stats over 10 runs: max = 120.0s, min = 3.0s, avg = 38.6s, dev = 44.3s. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:3355,testability,test,test,3355," //deepvariant:tf_utils_test PASSED in 3.8s. //deepvariant:utils_test PASSED in 0.1s. //deepvariant:variant_caller_test PASSED in 2.4s. //deepvariant:variant_calling_test PASSED in 0.1s. //deepvariant/environment_tests:env_smoke_test PASSED in 0.4s. //deepvariant/environment_tests:protobuf_implementation_test PASSED in 0.8s. //deepvariant/labeler:customized_classes_labeler_test PASSED in 1.8s. //deepvariant/labeler:labeled_examples_to_vcf_test PASSED in 1.9s. //deepvariant/labeler:positional_labeler_test PASSED in 1.8s. //deepvariant/labeler:variant_labeler_test PASSED in 1.8s. //deepvariant/python:allelecounter_wrap_test PASSED in 1.7s. //deepvariant/python:variant_calling_wrap_test PASSED in 1.8s. //deepvariant/realigner:aligner_test PASSED in 1.7s. //deepvariant/realigner:fast_pass_aligner_test PASSED in 0.1s. //deepvariant/realigner:realigner_test PASSED in 3.1s. //deepvariant/realigner:ssw_test PASSED in 0.1s. //deepvariant/realigner:window_selector_test PASSED in 1.8s. //deepvariant/realigner/allele_count_linear:generate_trained_model_test PASSED in 2.7s. //deepvariant/realigner/allele_count_linear:model_evaluation_test PASSED in 4.3s. //deepvariant/realigner/python:debruijn_graph_wrap_test PASSED in 1.8s. //deepvariant/realigner/python:ssw_misc_test PASSED in 0.3s. //deepvariant/realigner/python:ssw_wrap_test PASSED in 0.3s. //deepvariant/vendor:timer_test PASSED in 0.5s. //deepvariant/labeler:haplotype_labeler_test FAILED in 2.6s. /root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/testlogs/deepvariant/labeler/haplotype_labeler_test/test.log. //deepvariant:make_examples_test PASSED in 13.4s. Stats over 2 runs: max = 13.4s, min = 12.8s, avg = 13.1s, dev = 0.3s. //deepvariant:model_eval_test PASSED in 40.9s. Stats over 10 runs: max = 40.9s, min = 2.8s, avg = 7.3s, dev = 11.3s. //deepvariant:model_train_test PASSED in 120.0s. Stats over 10 runs: max = 120.0s, min = 3.0s, avg = 38.6s, dev = 44.3s. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:3360,testability,log,log,3360," //deepvariant:tf_utils_test PASSED in 3.8s. //deepvariant:utils_test PASSED in 0.1s. //deepvariant:variant_caller_test PASSED in 2.4s. //deepvariant:variant_calling_test PASSED in 0.1s. //deepvariant/environment_tests:env_smoke_test PASSED in 0.4s. //deepvariant/environment_tests:protobuf_implementation_test PASSED in 0.8s. //deepvariant/labeler:customized_classes_labeler_test PASSED in 1.8s. //deepvariant/labeler:labeled_examples_to_vcf_test PASSED in 1.9s. //deepvariant/labeler:positional_labeler_test PASSED in 1.8s. //deepvariant/labeler:variant_labeler_test PASSED in 1.8s. //deepvariant/python:allelecounter_wrap_test PASSED in 1.7s. //deepvariant/python:variant_calling_wrap_test PASSED in 1.8s. //deepvariant/realigner:aligner_test PASSED in 1.7s. //deepvariant/realigner:fast_pass_aligner_test PASSED in 0.1s. //deepvariant/realigner:realigner_test PASSED in 3.1s. //deepvariant/realigner:ssw_test PASSED in 0.1s. //deepvariant/realigner:window_selector_test PASSED in 1.8s. //deepvariant/realigner/allele_count_linear:generate_trained_model_test PASSED in 2.7s. //deepvariant/realigner/allele_count_linear:model_evaluation_test PASSED in 4.3s. //deepvariant/realigner/python:debruijn_graph_wrap_test PASSED in 1.8s. //deepvariant/realigner/python:ssw_misc_test PASSED in 0.3s. //deepvariant/realigner/python:ssw_wrap_test PASSED in 0.3s. //deepvariant/vendor:timer_test PASSED in 0.5s. //deepvariant/labeler:haplotype_labeler_test FAILED in 2.6s. /root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/testlogs/deepvariant/labeler/haplotype_labeler_test/test.log. //deepvariant:make_examples_test PASSED in 13.4s. Stats over 2 runs: max = 13.4s, min = 12.8s, avg = 13.1s, dev = 0.3s. //deepvariant:model_eval_test PASSED in 40.9s. Stats over 10 runs: max = 40.9s, min = 2.8s, avg = 7.3s, dev = 11.3s. //deepvariant:model_train_test PASSED in 120.0s. Stats over 10 runs: max = 120.0s, min = 3.0s, avg = 38.6s, dev = 44.3s. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:74,usability,Tool,Toolchain,74,"Hi @DiableJambe and @pichuan ,. This issue can be resolved by IBM Advance Toolchain 11.0 :). Some brief steps for your references of how to build DeepVariant on Power8 & Redhat 7.5. 1. Build the following packages with ""/usr/bin/gcc"". cmake 3.13.3. Protobuf 3.6.1 C++ (static build with --enable-static for bazel). bazel 0.15.0. 2. Install Advance Toolchain 11.0 and build the following packages with /opt/at11.0/bin/gcc. Python 2 and Pip 19.0.2. Protobuf 3.6.1 C++ (uninstall static and build shared). Protobuf 3.6.1 Python (should build and install from source or CLIF will fail). TensorFlow 1.12.0 (fix floatn.h error with the link Floatn.h error: https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f). CLIF. Opencv-python 3.4.5.20 (for tensor2tensor install). Then build DeepVariant will pass, and results here (excepted //deepvariant/labeler:haplotype_labeler_test tracked in issue 154). ```. ================================================================================. (05:42:50) INFO: Elapsed time: 715.015s, Critical Path: 689.68s. (05:42:50) INFO: 1835 processes: 1835 local. (05:42:50) INFO: Build completed, 1 test FAILED, 2433 total actions. //deepvariant:allelecounter_test PASSED in 0.1s. //deepvariant:call_variants_test PASSED in 59.8s. //deepvariant:data_providers_test PASSED in 11.8s. //deepvariant:dv_vcf_constants_test PASSED in 0.5s. //deepvariant:exclude_contigs_test PASSED in 1.6s. //deepvariant:haplotypes_test PASSED in 1.7s. . //deepvariant:modeling_test PASSED in 48.2s. //deepvariant:pileup_image_test PASSED in 1.8s. //deepvariant:postprocess_variants_lib_test PASSED in 0.1s. //deepvariant:postprocess_variants_test PASSED in 4.8s. //deepvariant:resources_test PASSED in 1.8s. //deepvariant:tf_utils_test PASSED in 3.8s. //deepvariant:utils_test PASSED in 0.1s. //deepvariant:variant_caller_test PASSED in 2.4s. //deepvariant:variant_calling_test PASSED in 0.1s. //deepvariant/environment_tests:env_smoke_test PASSED in 0.4s. //deepvariant/environmen",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:348,usability,Tool,Toolchain,348,"Hi @DiableJambe and @pichuan ,. This issue can be resolved by IBM Advance Toolchain 11.0 :). Some brief steps for your references of how to build DeepVariant on Power8 & Redhat 7.5. 1. Build the following packages with ""/usr/bin/gcc"". cmake 3.13.3. Protobuf 3.6.1 C++ (static build with --enable-static for bazel). bazel 0.15.0. 2. Install Advance Toolchain 11.0 and build the following packages with /opt/at11.0/bin/gcc. Python 2 and Pip 19.0.2. Protobuf 3.6.1 C++ (uninstall static and build shared). Protobuf 3.6.1 Python (should build and install from source or CLIF will fail). TensorFlow 1.12.0 (fix floatn.h error with the link Floatn.h error: https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f). CLIF. Opencv-python 3.4.5.20 (for tensor2tensor install). Then build DeepVariant will pass, and results here (excepted //deepvariant/labeler:haplotype_labeler_test tracked in issue 154). ```. ================================================================================. (05:42:50) INFO: Elapsed time: 715.015s, Critical Path: 689.68s. (05:42:50) INFO: 1835 processes: 1835 local. (05:42:50) INFO: Build completed, 1 test FAILED, 2433 total actions. //deepvariant:allelecounter_test PASSED in 0.1s. //deepvariant:call_variants_test PASSED in 59.8s. //deepvariant:data_providers_test PASSED in 11.8s. //deepvariant:dv_vcf_constants_test PASSED in 0.5s. //deepvariant:exclude_contigs_test PASSED in 1.6s. //deepvariant:haplotypes_test PASSED in 1.7s. . //deepvariant:modeling_test PASSED in 48.2s. //deepvariant:pileup_image_test PASSED in 1.8s. //deepvariant:postprocess_variants_lib_test PASSED in 0.1s. //deepvariant:postprocess_variants_test PASSED in 4.8s. //deepvariant:resources_test PASSED in 1.8s. //deepvariant:tf_utils_test PASSED in 3.8s. //deepvariant:utils_test PASSED in 0.1s. //deepvariant:variant_caller_test PASSED in 2.4s. //deepvariant:variant_calling_test PASSED in 0.1s. //deepvariant/environment_tests:env_smoke_test PASSED in 0.4s. //deepvariant/environmen",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:615,usability,error,error,615,"Hi @DiableJambe and @pichuan ,. This issue can be resolved by IBM Advance Toolchain 11.0 :). Some brief steps for your references of how to build DeepVariant on Power8 & Redhat 7.5. 1. Build the following packages with ""/usr/bin/gcc"". cmake 3.13.3. Protobuf 3.6.1 C++ (static build with --enable-static for bazel). bazel 0.15.0. 2. Install Advance Toolchain 11.0 and build the following packages with /opt/at11.0/bin/gcc. Python 2 and Pip 19.0.2. Protobuf 3.6.1 C++ (uninstall static and build shared). Protobuf 3.6.1 Python (should build and install from source or CLIF will fail). TensorFlow 1.12.0 (fix floatn.h error with the link Floatn.h error: https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f). CLIF. Opencv-python 3.4.5.20 (for tensor2tensor install). Then build DeepVariant will pass, and results here (excepted //deepvariant/labeler:haplotype_labeler_test tracked in issue 154). ```. ================================================================================. (05:42:50) INFO: Elapsed time: 715.015s, Critical Path: 689.68s. (05:42:50) INFO: 1835 processes: 1835 local. (05:42:50) INFO: Build completed, 1 test FAILED, 2433 total actions. //deepvariant:allelecounter_test PASSED in 0.1s. //deepvariant:call_variants_test PASSED in 59.8s. //deepvariant:data_providers_test PASSED in 11.8s. //deepvariant:dv_vcf_constants_test PASSED in 0.5s. //deepvariant:exclude_contigs_test PASSED in 1.6s. //deepvariant:haplotypes_test PASSED in 1.7s. . //deepvariant:modeling_test PASSED in 48.2s. //deepvariant:pileup_image_test PASSED in 1.8s. //deepvariant:postprocess_variants_lib_test PASSED in 0.1s. //deepvariant:postprocess_variants_test PASSED in 4.8s. //deepvariant:resources_test PASSED in 1.8s. //deepvariant:tf_utils_test PASSED in 3.8s. //deepvariant:utils_test PASSED in 0.1s. //deepvariant:variant_caller_test PASSED in 2.4s. //deepvariant:variant_calling_test PASSED in 0.1s. //deepvariant/environment_tests:env_smoke_test PASSED in 0.4s. //deepvariant/environmen",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:644,usability,error,error,644,"Hi @DiableJambe and @pichuan ,. This issue can be resolved by IBM Advance Toolchain 11.0 :). Some brief steps for your references of how to build DeepVariant on Power8 & Redhat 7.5. 1. Build the following packages with ""/usr/bin/gcc"". cmake 3.13.3. Protobuf 3.6.1 C++ (static build with --enable-static for bazel). bazel 0.15.0. 2. Install Advance Toolchain 11.0 and build the following packages with /opt/at11.0/bin/gcc. Python 2 and Pip 19.0.2. Protobuf 3.6.1 C++ (uninstall static and build shared). Protobuf 3.6.1 Python (should build and install from source or CLIF will fail). TensorFlow 1.12.0 (fix floatn.h error with the link Floatn.h error: https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f). CLIF. Opencv-python 3.4.5.20 (for tensor2tensor install). Then build DeepVariant will pass, and results here (excepted //deepvariant/labeler:haplotype_labeler_test tracked in issue 154). ```. ================================================================================. (05:42:50) INFO: Elapsed time: 715.015s, Critical Path: 689.68s. (05:42:50) INFO: 1835 processes: 1835 local. (05:42:50) INFO: Build completed, 1 test FAILED, 2433 total actions. //deepvariant:allelecounter_test PASSED in 0.1s. //deepvariant:call_variants_test PASSED in 59.8s. //deepvariant:data_providers_test PASSED in 11.8s. //deepvariant:dv_vcf_constants_test PASSED in 0.5s. //deepvariant:exclude_contigs_test PASSED in 1.6s. //deepvariant:haplotypes_test PASSED in 1.7s. . //deepvariant:modeling_test PASSED in 48.2s. //deepvariant:pileup_image_test PASSED in 1.8s. //deepvariant:postprocess_variants_lib_test PASSED in 0.1s. //deepvariant:postprocess_variants_test PASSED in 4.8s. //deepvariant:resources_test PASSED in 1.8s. //deepvariant:tf_utils_test PASSED in 3.8s. //deepvariant:utils_test PASSED in 0.1s. //deepvariant:variant_caller_test PASSED in 2.4s. //deepvariant:variant_calling_test PASSED in 0.1s. //deepvariant/environment_tests:env_smoke_test PASSED in 0.4s. //deepvariant/environmen",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:66,availability,avail,available,66,"Hey @qili93 . Thanks for letting me know! Is there a docker image available for this build (DeepVariant or prereqs)? If not, I will try to do it myself. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:85,deployability,build,build,85,"Hey @qili93 . Thanks for letting me know! Is there a docker image available for this build (DeepVariant or prereqs)? If not, I will try to do it myself. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:66,reliability,availab,available,66,"Hey @qili93 . Thanks for letting me know! Is there a docker image available for this build (DeepVariant or prereqs)? If not, I will try to do it myself. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:66,safety,avail,available,66,"Hey @qili93 . Thanks for letting me know! Is there a docker image available for this build (DeepVariant or prereqs)? If not, I will try to do it myself. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:66,security,availab,available,66,"Hey @qili93 . Thanks for letting me know! Is there a docker image available for this build (DeepVariant or prereqs)? If not, I will try to do it myself. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:955,availability,failur,failure,955,"Hi @DiableJambe ,. Sorry for my late response, I do not have a docker image yet. And I attached the detailed steps here for your reference. It's a bit long :). # DeepVariant. ## Environment. ```bash. # Power8 environment. export CPU=power8. export HOMEPATH=/home/qilibj. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # AT 11.0 environment. source /etc/profile. module load at11.0. export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # python2 and pip environment. export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH. export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. export PYTHON_BIN_PATH=/home/qilibj/inst/bin/python. export PYTHON_LIB_PATH=/home/qilibj/inst/lib/python2.7/site-packages. ```. ## Cmake 3.13.3. Build and install doc: [https://cmake.org/install/](https://cmake.org/install/). >Note: only can be built with system gcc, AT11.0 will cause build failure. ```bash. # download source code. wget https://github.com/Kitware/CMake/releases/download/v3.13.3/cmake-3.13.3.tar.gz. tar -zxvf cmake-3.13.3.tar.gz. cd cmake-3.13.3. # build scirpt. ./bootstrap. make -j20. make -j20 install. export PATH=/usr/local/bin:$PATH. ```. ## Protobuf 3.6.1 C++ static. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from /usr/bin/gcc, static for bazel. ```bash. # download source code. wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:975,availability,down,download,975,"Hi @DiableJambe ,. Sorry for my late response, I do not have a docker image yet. And I attached the detailed steps here for your reference. It's a bit long :). # DeepVariant. ## Environment. ```bash. # Power8 environment. export CPU=power8. export HOMEPATH=/home/qilibj. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # AT 11.0 environment. source /etc/profile. module load at11.0. export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # python2 and pip environment. export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH. export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. export PYTHON_BIN_PATH=/home/qilibj/inst/bin/python. export PYTHON_LIB_PATH=/home/qilibj/inst/lib/python2.7/site-packages. ```. ## Cmake 3.13.3. Build and install doc: [https://cmake.org/install/](https://cmake.org/install/). >Note: only can be built with system gcc, AT11.0 will cause build failure. ```bash. # download source code. wget https://github.com/Kitware/CMake/releases/download/v3.13.3/cmake-3.13.3.tar.gz. tar -zxvf cmake-3.13.3.tar.gz. cd cmake-3.13.3. # build scirpt. ./bootstrap. make -j20. make -j20 install. export PATH=/usr/local/bin:$PATH. ```. ## Protobuf 3.6.1 C++ static. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from /usr/bin/gcc, static for bazel. ```bash. # download source code. wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:1044,availability,down,download,1044," do not have a docker image yet. And I attached the detailed steps here for your reference. It's a bit long :). # DeepVariant. ## Environment. ```bash. # Power8 environment. export CPU=power8. export HOMEPATH=/home/qilibj. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # AT 11.0 environment. source /etc/profile. module load at11.0. export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # python2 and pip environment. export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH. export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. export PYTHON_BIN_PATH=/home/qilibj/inst/bin/python. export PYTHON_LIB_PATH=/home/qilibj/inst/lib/python2.7/site-packages. ```. ## Cmake 3.13.3. Build and install doc: [https://cmake.org/install/](https://cmake.org/install/). >Note: only can be built with system gcc, AT11.0 will cause build failure. ```bash. # download source code. wget https://github.com/Kitware/CMake/releases/download/v3.13.3/cmake-3.13.3.tar.gz. tar -zxvf cmake-3.13.3.tar.gz. cd cmake-3.13.3. # build scirpt. ./bootstrap. make -j20. make -j20 install. export PATH=/usr/local/bin:$PATH. ```. ## Protobuf 3.6.1 C++ static. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from /usr/bin/gcc, static for bazel. ```bash. # download source code. wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:1494,availability,down,download,1494,"PATH. # python2 and pip environment. export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH. export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. export PYTHON_BIN_PATH=/home/qilibj/inst/bin/python. export PYTHON_LIB_PATH=/home/qilibj/inst/lib/python2.7/site-packages. ```. ## Cmake 3.13.3. Build and install doc: [https://cmake.org/install/](https://cmake.org/install/). >Note: only can be built with system gcc, AT11.0 will cause build failure. ```bash. # download source code. wget https://github.com/Kitware/CMake/releases/download/v3.13.3/cmake-3.13.3.tar.gz. tar -zxvf cmake-3.13.3.tar.gz. cd cmake-3.13.3. # build scirpt. ./bootstrap. make -j20. make -j20 install. export PATH=/usr/local/bin:$PATH. ```. ## Protobuf 3.6.1 C++ static. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from /usr/bin/gcc, static for bazel. ```bash. # download source code. wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --disable-shared --enable-static. make clean. make -j20. make -j20 check # check if protobuf build is good. # install. make install. sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig. #cd .. # verify. echo ""$(which protoc)"". echo ""$(protoc --version)"". ```. ## Bazel 0.15.0. Build instructions: [https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:1574,availability,down,download,1574,"b:$LD_LIBRARY_PATH. export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. export PYTHON_BIN_PATH=/home/qilibj/inst/bin/python. export PYTHON_LIB_PATH=/home/qilibj/inst/lib/python2.7/site-packages. ```. ## Cmake 3.13.3. Build and install doc: [https://cmake.org/install/](https://cmake.org/install/). >Note: only can be built with system gcc, AT11.0 will cause build failure. ```bash. # download source code. wget https://github.com/Kitware/CMake/releases/download/v3.13.3/cmake-3.13.3.tar.gz. tar -zxvf cmake-3.13.3.tar.gz. cd cmake-3.13.3. # build scirpt. ./bootstrap. make -j20. make -j20 install. export PATH=/usr/local/bin:$PATH. ```. ## Protobuf 3.6.1 C++ static. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from /usr/bin/gcc, static for bazel. ```bash. # download source code. wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --disable-shared --enable-static. make clean. make -j20. make -j20 check # check if protobuf build is good. # install. make install. sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig. #cd .. # verify. echo ""$(which protoc)"". echo ""$(protoc --version)"". ```. ## Bazel 0.15.0. Build instructions: [https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel](https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:2319,availability,echo,echo,2319,"rc/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from /usr/bin/gcc, static for bazel. ```bash. # download source code. wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --disable-shared --enable-static. make clean. make -j20. make -j20 check # check if protobuf build is good. # install. make install. sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig. #cd .. # verify. echo ""$(which protoc)"". echo ""$(protoc --version)"". ```. ## Bazel 0.15.0. Build instructions: [https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel](https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel). Edit crosstool: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). > Note: Bazel 0.15.0 should be built from /usr/bin/gcc. ```bash. # download source code. wget https://github.com/bazelbuild/bazel/releases/download/0.15.0/bazel-0.15.0-dist.zip. mkdir bazel-0.15.0. unzip -n bazel-0.15.0-dist.zip -d ./bazel-0.15.0. cd bazel-0.15.0. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export JAVA_HOME=/usr/lib/jvm/java-1.8.0. export PATH=$HOMEPATH/inst/bin:$JAVA_HOME/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # build from scratch. PROTOC=$HOMEPATH/inst/bin/protoc ./compile.sh. rsync -avP output/bazel $HOMEP",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:2343,availability,echo,echo,2343,"thub.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from /usr/bin/gcc, static for bazel. ```bash. # download source code. wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --disable-shared --enable-static. make clean. make -j20. make -j20 check # check if protobuf build is good. # install. make install. sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig. #cd .. # verify. echo ""$(which protoc)"". echo ""$(protoc --version)"". ```. ## Bazel 0.15.0. Build instructions: [https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel](https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel). Edit crosstool: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). > Note: Bazel 0.15.0 should be built from /usr/bin/gcc. ```bash. # download source code. wget https://github.com/bazelbuild/bazel/releases/download/0.15.0/bazel-0.15.0-dist.zip. mkdir bazel-0.15.0. unzip -n bazel-0.15.0-dist.zip -d ./bazel-0.15.0. cd bazel-0.15.0. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export JAVA_HOME=/usr/lib/jvm/java-1.8.0. export PATH=$HOMEPATH/inst/bin:$JAVA_HOME/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # build from scratch. PROTOC=$HOMEPATH/inst/bin/protoc ./compile.sh. rsync -avP output/bazel $HOMEPATH/inst/bin/. # verific",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:2786,availability,down,download,2786,"me/qilibj. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --disable-shared --enable-static. make clean. make -j20. make -j20 check # check if protobuf build is good. # install. make install. sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig. #cd .. # verify. echo ""$(which protoc)"". echo ""$(protoc --version)"". ```. ## Bazel 0.15.0. Build instructions: [https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel](https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel). Edit crosstool: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). > Note: Bazel 0.15.0 should be built from /usr/bin/gcc. ```bash. # download source code. wget https://github.com/bazelbuild/bazel/releases/download/0.15.0/bazel-0.15.0-dist.zip. mkdir bazel-0.15.0. unzip -n bazel-0.15.0-dist.zip -d ./bazel-0.15.0. cd bazel-0.15.0. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export JAVA_HOME=/usr/lib/jvm/java-1.8.0. export PATH=$HOMEPATH/inst/bin:$JAVA_HOME/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # build from scratch. PROTOC=$HOMEPATH/inst/bin/protoc ./compile.sh. rsync -avP output/bazel $HOMEPATH/inst/bin/. # verification. bazel info. ```. ## Advance Toolchain 11.0. Doc: [https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/](https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/). ```bash. # gpg public key. wget ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. rpm --import gpg-pubkey-6976a827-5164221b. # Configure the Advance Toolchain repositories. [root@cit107",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:2858,availability,down,download,2858,"local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --disable-shared --enable-static. make clean. make -j20. make -j20 check # check if protobuf build is good. # install. make install. sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig. #cd .. # verify. echo ""$(which protoc)"". echo ""$(protoc --version)"". ```. ## Bazel 0.15.0. Build instructions: [https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel](https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel). Edit crosstool: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). > Note: Bazel 0.15.0 should be built from /usr/bin/gcc. ```bash. # download source code. wget https://github.com/bazelbuild/bazel/releases/download/0.15.0/bazel-0.15.0-dist.zip. mkdir bazel-0.15.0. unzip -n bazel-0.15.0-dist.zip -d ./bazel-0.15.0. cd bazel-0.15.0. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export JAVA_HOME=/usr/lib/jvm/java-1.8.0. export PATH=$HOMEPATH/inst/bin:$JAVA_HOME/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # build from scratch. PROTOC=$HOMEPATH/inst/bin/protoc ./compile.sh. rsync -avP output/bazel $HOMEPATH/inst/bin/. # verification. bazel info. ```. ## Advance Toolchain 11.0. Doc: [https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/](https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/). ```bash. # gpg public key. wget ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. rpm --import gpg-pubkey-6976a827-5164221b. # Configure the Advance Toolchain repositories. [root@cit1074 deepvariant]# cat /etc/yum.repos.d/advance-toolchain.repo. #Begin of c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:4009,availability,failov,failovermethod,4009,. export HOMEPATH=/home/qilibj. export JAVA_HOME=/usr/lib/jvm/java-1.8.0. export PATH=$HOMEPATH/inst/bin:$JAVA_HOME/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # build from scratch. PROTOC=$HOMEPATH/inst/bin/protoc ./compile.sh. rsync -avP output/bazel $HOMEPATH/inst/bin/. # verification. bazel info. ```. ## Advance Toolchain 11.0. Doc: [https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/](https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/). ```bash. # gpg public key. wget ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. rpm --import gpg-pubkey-6976a827-5164221b. # Configure the Advance Toolchain repositories. [root@cit1074 deepvariant]# cat /etc/yum.repos.d/advance-toolchain.repo. #Begin of configuration file. [advance-toolchain]. name=Advance Toolchain Unicamp FTP. baseurl=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7. failovermethod=priority. enabled=1. gpgcheck=1. gpgkey=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. # End of configuration file. # Install the Advance Toolchain. yum install advance-toolchain-at11.0-runtime. yum install advance-toolchain-at11.0-devel. yum install advance-toolchain-at11.0-perf. yum install advance-toolchain-at11.0-mcore-libs. # Load enviroment. export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # Do not need to export. # export LD_LIBRARY_PATH=/opt/at11.0/lib64:/lib64:$LD_LIBRARY_PATH . # Load environment. sudo yum install environment-modules. source /etc/profile. module load at11.0. module unload at11.0. ```. ## Python 2 and Pip 19.0.1. Build instructions: [https://docs.python.org/2/using/unix.html](https://docs.python.org/2/using/unix.html). > Note: Python 2 should be built from AT 11.0. ```bash. # download source code . wget https://www.python.org/ftp/python/2.7.15/Python-2.7.15.tgz. tar -zxvf Python-2.7.15.tgz. cd Python-2.7.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:4885,availability,down,download,4885,"-toolchain]. name=Advance Toolchain Unicamp FTP. baseurl=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7. failovermethod=priority. enabled=1. gpgcheck=1. gpgkey=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. # End of configuration file. # Install the Advance Toolchain. yum install advance-toolchain-at11.0-runtime. yum install advance-toolchain-at11.0-devel. yum install advance-toolchain-at11.0-perf. yum install advance-toolchain-at11.0-mcore-libs. # Load enviroment. export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # Do not need to export. # export LD_LIBRARY_PATH=/opt/at11.0/lib64:/lib64:$LD_LIBRARY_PATH . # Load environment. sudo yum install environment-modules. source /etc/profile. module load at11.0. module unload at11.0. ```. ## Python 2 and Pip 19.0.1. Build instructions: [https://docs.python.org/2/using/unix.html](https://docs.python.org/2/using/unix.html). > Note: Python 2 should be built from AT 11.0. ```bash. # download source code . wget https://www.python.org/ftp/python/2.7.15/Python-2.7.15.tgz. tar -zxvf Python-2.7.15.tgz. cd Python-2.7.15. # environment. export HOMEPATH=/home/qilibj. export CPU=power8. # check gcc before build, should be AT11.0. which gcc. # build. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CPPFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static. make -j20. make install. # set environment. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH. export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. # verify 2.7.15. echo ""$(python --version)"". # Pip 19.0.2. wget -qc https://bootstrap.pypa.io/get-pip.py --no-check-certificate. $HOMEPATH/inst/bin/python ./get-pip.py --prefix $HOMEPATH/inst. #pip install --upgrade --force-reinstall pip. echo ""$(pip --version)"". pip install setuptools nose asv cython futur",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:5598,availability,echo,echo,5598,"l environment-modules. source /etc/profile. module load at11.0. module unload at11.0. ```. ## Python 2 and Pip 19.0.1. Build instructions: [https://docs.python.org/2/using/unix.html](https://docs.python.org/2/using/unix.html). > Note: Python 2 should be built from AT 11.0. ```bash. # download source code . wget https://www.python.org/ftp/python/2.7.15/Python-2.7.15.tgz. tar -zxvf Python-2.7.15.tgz. cd Python-2.7.15. # environment. export HOMEPATH=/home/qilibj. export CPU=power8. # check gcc before build, should be AT11.0. which gcc. # build. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CPPFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static. make -j20. make install. # set environment. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH. export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. # verify 2.7.15. echo ""$(python --version)"". # Pip 19.0.2. wget -qc https://bootstrap.pypa.io/get-pip.py --no-check-certificate. $HOMEPATH/inst/bin/python ./get-pip.py --prefix $HOMEPATH/inst. #pip install --upgrade --force-reinstall pip. echo ""$(pip --version)"". pip install setuptools nose asv cython future protobuf==3.6.1 six mock. pip install --upgrade setuptools. ```. ## Protobuf 3.6.1 C++ shared. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from AT 11.0, C++ shared for TF 1.12. ```bash. # download source code. wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CP",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:5820,availability,echo,echo,5820,"ml). > Note: Python 2 should be built from AT 11.0. ```bash. # download source code . wget https://www.python.org/ftp/python/2.7.15/Python-2.7.15.tgz. tar -zxvf Python-2.7.15.tgz. cd Python-2.7.15. # environment. export HOMEPATH=/home/qilibj. export CPU=power8. # check gcc before build, should be AT11.0. which gcc. # build. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CPPFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static. make -j20. make install. # set environment. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH. export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. # verify 2.7.15. echo ""$(python --version)"". # Pip 19.0.2. wget -qc https://bootstrap.pypa.io/get-pip.py --no-check-certificate. $HOMEPATH/inst/bin/python ./get-pip.py --prefix $HOMEPATH/inst. #pip install --upgrade --force-reinstall pip. echo ""$(pip --version)"". pip install setuptools nose asv cython future protobuf==3.6.1 six mock. pip install --upgrade setuptools. ```. ## Protobuf 3.6.1 C++ shared. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from AT 11.0, C++ shared for TF 1.12. ```bash. # download source code. wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static. make clean. make -j20. # optional. make -j20 check # check if protobuf build is good. # install. make install. sudo ldconfig # refresh shared lib",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:6223,availability,down,download,6223,"figure --prefix=$HOMEPATH/inst --enable-shared --disable-static. make -j20. make install. # set environment. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH. export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. # verify 2.7.15. echo ""$(python --version)"". # Pip 19.0.2. wget -qc https://bootstrap.pypa.io/get-pip.py --no-check-certificate. $HOMEPATH/inst/bin/python ./get-pip.py --prefix $HOMEPATH/inst. #pip install --upgrade --force-reinstall pip. echo ""$(pip --version)"". pip install setuptools nose asv cython future protobuf==3.6.1 six mock. pip install --upgrade setuptools. ```. ## Protobuf 3.6.1 C++ shared. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from AT 11.0, C++ shared for TF 1.12. ```bash. # download source code. wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static. make clean. make -j20. # optional. make -j20 check # check if protobuf build is good. # install. make install. sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig. #cd .. # verify. echo ""$(which protoc)"". echo ""$(protoc --version)"". ```. ## Protobuf 3.6.1 Python from source code. Python: [https://github.com/protocolbuffers/protobuf/blob/master/python/README.md](https://github.com/protocolbuffers/protobuf/blob/master/python/README.md). > Note: Protobuf 3.6.1 should be built from source code or CLIF cannot find protobuf. ```b",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:6303,availability,down,download,6303," install. # set environment. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH. export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. # verify 2.7.15. echo ""$(python --version)"". # Pip 19.0.2. wget -qc https://bootstrap.pypa.io/get-pip.py --no-check-certificate. $HOMEPATH/inst/bin/python ./get-pip.py --prefix $HOMEPATH/inst. #pip install --upgrade --force-reinstall pip. echo ""$(pip --version)"". pip install setuptools nose asv cython future protobuf==3.6.1 six mock. pip install --upgrade setuptools. ```. ## Protobuf 3.6.1 C++ shared. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from AT 11.0, C++ shared for TF 1.12. ```bash. # download source code. wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static. make clean. make -j20. # optional. make -j20 check # check if protobuf build is good. # install. make install. sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig. #cd .. # verify. echo ""$(which protoc)"". echo ""$(protoc --version)"". ```. ## Protobuf 3.6.1 Python from source code. Python: [https://github.com/protocolbuffers/protobuf/blob/master/python/README.md](https://github.com/protocolbuffers/protobuf/blob/master/python/README.md). > Note: Protobuf 3.6.1 should be built from source code or CLIF cannot find protobuf. ```bash. # share build for Python. python --version # python 2.7 or newer. protoc --",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:6879,availability,echo,echo,6879,"on future protobuf==3.6.1 six mock. pip install --upgrade setuptools. ```. ## Protobuf 3.6.1 C++ shared. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from AT 11.0, C++ shared for TF 1.12. ```bash. # download source code. wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static. make clean. make -j20. # optional. make -j20 check # check if protobuf build is good. # install. make install. sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig. #cd .. # verify. echo ""$(which protoc)"". echo ""$(protoc --version)"". ```. ## Protobuf 3.6.1 Python from source code. Python: [https://github.com/protocolbuffers/protobuf/blob/master/python/README.md](https://github.com/protocolbuffers/protobuf/blob/master/python/README.md). > Note: Protobuf 3.6.1 should be built from source code or CLIF cannot find protobuf. ```bash. # share build for Python. python --version # python 2.7 or newer. protoc --version. # build. cd protobuf-3.6.1/python/. python setup.py build. python setup.py test. # install from source as deepvariant needed. python setup.py install. # install from wheel. python setup.py bdist_wheel. pip install protobuf-3.6.1-py2.py3-none-any.whl --force-reinstall. # verify. python -c ""import google.protobuf"". ```. ## OpenBLAS 0.3.5. ```bash. git clone -b v0.3.5 https://github.com/xianyi/OpenBLAS.git OpenBLAS-0.3.5. cd OpenBLAS-0.3.5. make TARGET=power8. make TARGET=power8 PREFIX=$HOMEPATH/inst install. ```. ## Boost 1.66.0. ```bash. wget -qc https://dl.bi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:6903,availability,echo,echo,6903,"1 six mock. pip install --upgrade setuptools. ```. ## Protobuf 3.6.1 C++ shared. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from AT 11.0, C++ shared for TF 1.12. ```bash. # download source code. wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static. make clean. make -j20. # optional. make -j20 check # check if protobuf build is good. # install. make install. sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig. #cd .. # verify. echo ""$(which protoc)"". echo ""$(protoc --version)"". ```. ## Protobuf 3.6.1 Python from source code. Python: [https://github.com/protocolbuffers/protobuf/blob/master/python/README.md](https://github.com/protocolbuffers/protobuf/blob/master/python/README.md). > Note: Protobuf 3.6.1 should be built from source code or CLIF cannot find protobuf. ```bash. # share build for Python. python --version # python 2.7 or newer. protoc --version. # build. cd protobuf-3.6.1/python/. python setup.py build. python setup.py test. # install from source as deepvariant needed. python setup.py install. # install from wheel. python setup.py bdist_wheel. pip install protobuf-3.6.1-py2.py3-none-any.whl --force-reinstall. # verify. python -c ""import google.protobuf"". ```. ## OpenBLAS 0.3.5. ```bash. git clone -b v0.3.5 https://github.com/xianyi/OpenBLAS.git OpenBLAS-0.3.5. cd OpenBLAS-0.3.5. make TARGET=power8. make TARGET=power8 PREFIX=$HOMEPATH/inst install. ```. ## Boost 1.66.0. ```bash. wget -qc https://dl.bintray.com/boostorg/relea",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:8424,availability,error,error,8424,"ariant needed. python setup.py install. # install from wheel. python setup.py bdist_wheel. pip install protobuf-3.6.1-py2.py3-none-any.whl --force-reinstall. # verify. python -c ""import google.protobuf"". ```. ## OpenBLAS 0.3.5. ```bash. git clone -b v0.3.5 https://github.com/xianyi/OpenBLAS.git OpenBLAS-0.3.5. cd OpenBLAS-0.3.5. make TARGET=power8. make TARGET=power8 PREFIX=$HOMEPATH/inst install. ```. ## Boost 1.66.0. ```bash. wget -qc https://dl.bintray.com/boostorg/release/1.66.0/source/boost_1_66_0.tar.gz. tar xzf boost_1_66_0.tar.gz. cd boost_1_66_0. ./bootstrap.sh --with-toolset=gcc --prefix=$HOMEPATH/inst. ./b2 dll-path=""$HOMEPATH/inst/lib"" install. ```. ## TensorFlow 1.12.0. > Note: Tensorflow 1.12.0 can be built with AT 11.0. Build instructions: [https://www.tensorflow.org/install/source](https://www.tensorflow.org/install/source). Edit instructions: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). Floatn.h error: [https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f](https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f). ```bash. # development packages. yum install python-devel python-pip -y. # dependency of numpy 1.14.6. OPT=""-D_XLOCALE_H=1 -O3 -mcpu=$CPU -mtune=$CPU -maltivec -mvsx -ffast-math -fpeel-loops -funroll-loops -ftree-vectorize -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free"" BLAS=$HOMEPATH/inst/lib/libopenblas.so LAPACK=$HOMEPATH/inst/lib/libopenblas.so ATLAS=$HOMEPATH/inst/lib/libopenblas.so pip install numpy==1.14.6. # verify. python -c ""import numpy"". # dependecy of scipy 1.2.0. OPT=""-O3 -mcpu=$CPU -mtune=$CPU -maltivec -mvsx -ffast-math -fpeel-loops -funroll-loops -ftree-vectorize -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free"" BLAS=$HOMEPATH/inst/lib/libopenblas.so LAPACK=$HOMEPATH/inst/lib/libopenblas.so ATLAS=$HOMEPATH/inst/lib/libopenblas.so pip install scipy==1.2.0. # verify. python ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:9623,availability,down,download,9623,"on-pip -y. # dependency of numpy 1.14.6. OPT=""-D_XLOCALE_H=1 -O3 -mcpu=$CPU -mtune=$CPU -maltivec -mvsx -ffast-math -fpeel-loops -funroll-loops -ftree-vectorize -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free"" BLAS=$HOMEPATH/inst/lib/libopenblas.so LAPACK=$HOMEPATH/inst/lib/libopenblas.so ATLAS=$HOMEPATH/inst/lib/libopenblas.so pip install numpy==1.14.6. # verify. python -c ""import numpy"". # dependecy of scipy 1.2.0. OPT=""-O3 -mcpu=$CPU -mtune=$CPU -maltivec -mvsx -ffast-math -fpeel-loops -funroll-loops -ftree-vectorize -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free"" BLAS=$HOMEPATH/inst/lib/libopenblas.so LAPACK=$HOMEPATH/inst/lib/libopenblas.so ATLAS=$HOMEPATH/inst/lib/libopenblas.so pip install scipy==1.2.0. # verify. python -c ""import scipy"". # pip package dependencies. # pip install pip six wheel mock. pip install wheel autograd h5py==2.9.0 enum34. pip install keras_applications==1.0.6 keras_preprocessing==1.0.5. # download source code. git clone -b r1.12 https://github.com/tensorflow/tensorflow.git tensorflow-1.12. cd tensorflow-1.12. # configure in tensorflow/.tf_configure.bazelrc. PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python"" \. PYTHON_LIB_PATH=""$HOMEPATH/inst/lib/python2.7/site-packages"" \. TF_NEED_IGNITE=""0"" \. TF_ENABLE_XLA=""0"" \. TF_NEED_OPENCL_SYCL=""0"" \. TF_NEED_ROCM=""0"" \. TF_NEED_MPI=""0"" \. TF_NEED_TENSORRT=""0"" \. TF_NEED_CUDA=""1"" \. TF_CUDA_VERSION=""10.0"" \. CUDA_TOOLKIT_PATH=""/usr/local/cuda"" \. TF_CUDNN_VERSION=""7"" \. CUDNN_INSTALL_PATH=""/usr/local/cuda-10.0"" \. TF_NCCL_VERSION=""2"" \. NCCL_INSTALL_PATH=""/usr/local/cuda-10.0/targets/ppc64le-linux/lib"" \. NCCL_HDR_PATH=""/usr/local/cuda-10.0/targets/ppc64le-linux/lib/../include"" \. TF_CUDA_COMPUTE_CAPABILITIES=""3.7"" \. TF_CUDA_CLANG=""0"" \. GCC_HOST_COMPILER_PATH=""/opt/at11.0/bin/gcc"" \. CC_OPT_FLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" \. TF_SET_ANDROID_WORKSPACE=0 \. ./configure. # fix build error. vim /opt/at11.0/include/bits/floatn.h. -----------",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:10570,availability,error,error,10570,"applications==1.0.6 keras_preprocessing==1.0.5. # download source code. git clone -b r1.12 https://github.com/tensorflow/tensorflow.git tensorflow-1.12. cd tensorflow-1.12. # configure in tensorflow/.tf_configure.bazelrc. PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python"" \. PYTHON_LIB_PATH=""$HOMEPATH/inst/lib/python2.7/site-packages"" \. TF_NEED_IGNITE=""0"" \. TF_ENABLE_XLA=""0"" \. TF_NEED_OPENCL_SYCL=""0"" \. TF_NEED_ROCM=""0"" \. TF_NEED_MPI=""0"" \. TF_NEED_TENSORRT=""0"" \. TF_NEED_CUDA=""1"" \. TF_CUDA_VERSION=""10.0"" \. CUDA_TOOLKIT_PATH=""/usr/local/cuda"" \. TF_CUDNN_VERSION=""7"" \. CUDNN_INSTALL_PATH=""/usr/local/cuda-10.0"" \. TF_NCCL_VERSION=""2"" \. NCCL_INSTALL_PATH=""/usr/local/cuda-10.0/targets/ppc64le-linux/lib"" \. NCCL_HDR_PATH=""/usr/local/cuda-10.0/targets/ppc64le-linux/lib/../include"" \. TF_CUDA_COMPUTE_CAPABILITIES=""3.7"" \. TF_CUDA_CLANG=""0"" \. GCC_HOST_COMPILER_PATH=""/opt/at11.0/bin/gcc"" \. CC_OPT_FLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" \. TF_SET_ANDROID_WORKSPACE=0 \. ./configure. # fix build error. vim /opt/at11.0/include/bits/floatn.h. -------------------------------------. #include <features.h>. /* Defined to 1 if the current compiler invocation provides a. floating-point type with the IEEE 754 binary128 format, and this glibc. includes corresponding *f128 interfaces for it. */. #if defined _ARCH_PWR8 && defined __LITTLE_ENDIAN__ && (_CALL_ELF == 2) \. && defined __FLOAT128__. # define __HAVE_FLOAT128 1. #else. # define __HAVE_FLOAT128 0. #endif. /* add the following block of fix tensorflow build error */. #if CUDART_VERSION. #undef __HAVE_FLOAT128. #define __HAVE_FLOAT128 0. #endif. /* Defined to 1 if __HAVE_FLOAT128 is 1 and the type is ABI-distinct. from the default float, double and long double types in this glibc. */. #if __HAVE_FLOAT128. -------------------------------------. # build. bazel clean. export PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python"". bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --cxxopt=""-D_GLIBCXX_USE_CXX11_AB",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:11087,availability,error,error,11087,"TOOLKIT_PATH=""/usr/local/cuda"" \. TF_CUDNN_VERSION=""7"" \. CUDNN_INSTALL_PATH=""/usr/local/cuda-10.0"" \. TF_NCCL_VERSION=""2"" \. NCCL_INSTALL_PATH=""/usr/local/cuda-10.0/targets/ppc64le-linux/lib"" \. NCCL_HDR_PATH=""/usr/local/cuda-10.0/targets/ppc64le-linux/lib/../include"" \. TF_CUDA_COMPUTE_CAPABILITIES=""3.7"" \. TF_CUDA_CLANG=""0"" \. GCC_HOST_COMPILER_PATH=""/opt/at11.0/bin/gcc"" \. CC_OPT_FLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" \. TF_SET_ANDROID_WORKSPACE=0 \. ./configure. # fix build error. vim /opt/at11.0/include/bits/floatn.h. -------------------------------------. #include <features.h>. /* Defined to 1 if the current compiler invocation provides a. floating-point type with the IEEE 754 binary128 format, and this glibc. includes corresponding *f128 interfaces for it. */. #if defined _ARCH_PWR8 && defined __LITTLE_ENDIAN__ && (_CALL_ELF == 2) \. && defined __FLOAT128__. # define __HAVE_FLOAT128 1. #else. # define __HAVE_FLOAT128 0. #endif. /* add the following block of fix tensorflow build error */. #if CUDART_VERSION. #undef __HAVE_FLOAT128. #define __HAVE_FLOAT128 0. #endif. /* Defined to 1 if __HAVE_FLOAT128 is 1 and the type is ABI-distinct. from the default float, double and long double types in this glibc. */. #if __HAVE_FLOAT128. -------------------------------------. # build. bazel clean. export PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python"". bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" --action_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH --test_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH --distinct_host_configuration=false. # generate package. bazel-bin/tensorflow/tools/pip_package/build_pip_package $HOMEPATH/tensorflow_package. # install. pip install $HOMEPATH/tensorflow_package/tensorflow-1.12.0-*.whl. # verification. python -c ""import tensorflow as tf; print(tf.__version__)"". ```. ## CLIF. > Note: CLIF can be built with AT 11.0. Git Repository: [https://github.com/google/clif](https://github.com/goo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:12385,availability,down,download,12385,"zel clean. export PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python"". bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" --action_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH --test_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH --distinct_host_configuration=false. # generate package. bazel-bin/tensorflow/tools/pip_package/build_pip_package $HOMEPATH/tensorflow_package. # install. pip install $HOMEPATH/tensorflow_package/tensorflow-1.12.0-*.whl. # verification. python -c ""import tensorflow as tf; print(tf.__version__)"". ```. ## CLIF. > Note: CLIF can be built with AT 11.0. Git Repository: [https://github.com/google/clif](https://github.com/google/clif). ```bash. # Prerequisites. cmake --version #3.5+. protoc --version # 3.2.0+ build from source code for both C++ and Python. pip install virtualenv. pip install pyparsing. yum install subversion. yum install ocaml. pip install 'pyparsing>=2.2.0'. pkg-config --libs python # workable. # download source code. cd $HOMEPATH. git clone https://github.com/google/clif.git. cd clif. # set environment. export INSTALL_DIR=""$HOMEPATH/inst"". export CLIFSRC_DIR=""$HOMEPATH/clif"". export LLVM_DIR=""$CLIFSRC_DIR/../clif_backend"". export BUILD_DIR=""$LLVM_DIR/build_matcher"". export PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python"". export PYTHON_LIB_PATH=""$HOMEPATH/inst/lib/python2.7/site-packages"". export PROTOC_PREFIX_PATH=""$(dirname ""$(dirname ""$(which protoc)"")"")"". export CLIF_VIRTUALENV=""$INSTALL_DIR""/clif. export CLIF_PIP=""$CLIF_VIRTUALENV/bin/pip"". virtualenv -p ""$PYTHON_BIN_PATH"" ""$CLIF_VIRTUALENV"". $CLIF_PIP install --upgrade pip. $CLIF_PIP install --upgrade setuptools. # Checkout LLVM and Clang source trees. mkdir -p $LLVM_DIR. cd $LLVM_DIR. svn co https://llvm.org/svn/llvm-project/llvm/trunk@307315 llvm. cd llvm/tools. svn co https://llvm.org/svn/llvm-project/cfe/trunk@307315 clang. ln -s -f -n $CLIFSRC_DIR/clif clif. # Builds must be done outside of the LLVM tree. mkdir -p $BUILD_DIR",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:14667,availability,echo,echo,14667,"LLVM_BUILD_DOCS=false \. -DLLVM_TARGETS_TO_BUILD=PowerPC \. -DPYTHON_INCLUDE_DIR=""$HOMEPATH/inst/include/python2.7"" \. -DPYTHON_LIBRARY=""$HOMEPATH/inst/lib/libpython2.7.so"" \. -DPYTHON_EXECUTABLE=""$HOMEPATH/inst/bin/python"" \. ""$LLVM_DIR/llvm"". make -j20 clif-matcher. # export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH. make -j20 clif_python_utils_proto_util. make -j20 install. ## Get back to the CLIF Python directory and have pip run setup.py. cd ""$CLIFSRC_DIR"". # Grab the python compiled .proto. cp ""$BUILD_DIR/tools/clif/protos/ast_pb2.py"" clif/protos/. # Grab CLIF generated wrapper implementation for proto_util. cp ""$BUILD_DIR/tools/clif/python/utils/proto_util.cc"" clif/python/utils/. cp ""$BUILD_DIR/tools/clif/python/utils/proto_util.h"" clif/python/utils/. cp ""$BUILD_DIR/tools/clif/python/utils/proto_util.init.cc"" clif/python/utils/. # install. export C_INCLUDE_PATH=/home/qilibj/inst/include. export CPLUS_INCLUDE_PATH=/home/qilibj/inst/include. ""$CLIF_PIP"" install . # echo ""SUCCESS - To use pyclif, run $CLIF_VIRTUALENV/bin/pyclif."". python setup.py bdist_wheel. # Note: pyclif should be installed into virtualenv. ""$CLIF_PIP"" install pyclif-0.3-cp27-none-linux_ppc64le.whl. pip install dist/pyclif-0.3-cp27-cp27m-linux_ppc64le.whl. # verify. python -c ""from clif.python.proto import start"". # link for deepvariant. ln -s /home/qilibj/inst/clif /usr/local/. ```. ## Opencv-python 3.4.5.20. Git repository: [https://github.com/skvark/opencv-python](https://github.com/skvark/opencv-python). ```bash. # Checkout repository and submodules. git clone https://github.com/skvark/opencv-python.git. cd opencv-python/. # fetch the tags to your local repository. git fetch --all --tags --prune. # check out tag 3.4.5.20. git checkout tags/20. # load submoduel. git submodule update --init --recursive. # Dependency. pip install pyparsing. yum install qt-devel. # Build. python setup.py bdist_wheel. # Insatll. pip install dist/opencv_python-3.4.5.20-cp27-cp27mu-linux_ppc64le.whl",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:16246,availability,echo,echo,16246,"tps://github.com/skvark/opencv-python.git. cd opencv-python/. # fetch the tags to your local repository. git fetch --all --tags --prune. # check out tag 3.4.5.20. git checkout tags/20. # load submoduel. git submodule update --init --recursive. # Dependency. pip install pyparsing. yum install qt-devel. # Build. python setup.py bdist_wheel. # Insatll. pip install dist/opencv_python-3.4.5.20-cp27-cp27mu-linux_ppc64le.whl. # Verify in a new session. python -c ""import cv2"". ```. ## DV Prerequisite. ```bash. ####################################################################. # misc setup. ####################################################################. # development packages. yum install python2-pkgconfig zip zlib-devel unzip curl -y. # python packages. yum install python-devel python-pip python-wheel -y. ####################################################################. # python packages. ####################################################################. # python 2 required. echo ""$(python --version)"". echo ""$(pip --version)"". # Install python packages. pip install contextlib2. pip install enum34. pip install intervaltree. pip install 'mock>=2.0.0'. # pip install 'numpy==1.14' => skip as installed in TF. pip install 'requests>=2.18'. # pip install 'scipy==1.0' => skip as installed in TF. pip install 'oauth2client>=4.0.0'. pip install 'crcmod>=1.7'. pip install six. pip install sklearn. pip install pandas. pip install psutil. pip install --upgrade google-api-python-client. ####################################################################. # depend on opencv-python wheel - build from source. ####################################################################. pip install 'tensor2tensor>=1.9.0'. ####################################################################. # depend on - TensorFlow - 1.12 build from source. ####################################################################. pip install tensorflow-1.12.0-cp27-cp27mu-linux_ppc64le.whl. ##############",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:16274,availability,echo,echo,16274,"cv-python.git. cd opencv-python/. # fetch the tags to your local repository. git fetch --all --tags --prune. # check out tag 3.4.5.20. git checkout tags/20. # load submoduel. git submodule update --init --recursive. # Dependency. pip install pyparsing. yum install qt-devel. # Build. python setup.py bdist_wheel. # Insatll. pip install dist/opencv_python-3.4.5.20-cp27-cp27mu-linux_ppc64le.whl. # Verify in a new session. python -c ""import cv2"". ```. ## DV Prerequisite. ```bash. ####################################################################. # misc setup. ####################################################################. # development packages. yum install python2-pkgconfig zip zlib-devel unzip curl -y. # python packages. yum install python-devel python-pip python-wheel -y. ####################################################################. # python packages. ####################################################################. # python 2 required. echo ""$(python --version)"". echo ""$(pip --version)"". # Install python packages. pip install contextlib2. pip install enum34. pip install intervaltree. pip install 'mock>=2.0.0'. # pip install 'numpy==1.14' => skip as installed in TF. pip install 'requests>=2.18'. # pip install 'scipy==1.0' => skip as installed in TF. pip install 'oauth2client>=4.0.0'. pip install 'crcmod>=1.7'. pip install six. pip install sklearn. pip install pandas. pip install psutil. pip install --upgrade google-api-python-client. ####################################################################. # depend on opencv-python wheel - build from source. ####################################################################. pip install 'tensor2tensor>=1.9.0'. ####################################################################. # depend on - TensorFlow - 1.12 build from source. ####################################################################. pip install tensorflow-1.12.0-cp27-cp27mu-linux_ppc64le.whl. ##########################################",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:19603,availability,echo,echo,19603,"ImportError: No module named google.protobuf"" by install protobuf from source. bazel clean. bazel shutdown. bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" deepvariant/... \. --action_env=LD_LIBRARY_PATH --test_env=LD_LIBRARY_PATH \. --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH \. --action_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH --test_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH \. --action_env=PYTHON_LIB_PATH=$PYTHON_LIB_PATH --test_env=PYTHON_LIB_PATH=$PYTHON_LIB_PATH \. --action_env=BAZEL_PYTHON=$BAZEL_PYTHON --test_env=BAZEL_PYTHON=$BAZEL_PYTHON >& output.log 2>&1 &. bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" deepvariant:gpu_tests --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH. # for CPU only. bazel test -c opt ${DV_COPT_FLAGS} ""$@"" deepvariant/... # build binary. bazel build -c opt ${DV_COPT_FLAGS} ""$@"" deepvariant:binaries --build_python_zip --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH. echo 'Expect a usage message:'. (bazel-bin/deepvariant/call_variants --help || : ) | grep '/call_variants.py:'. bazel build :licenses_zip --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH. ```. ## Fix DV Error. ```bash. ################################################################################. # bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@""//deepvariant:resources_test. # use lscpu to show the actual CPU number. ################################################################################. python -c ""import multiprocessing; print(multiprocessing.cpu_count())"" #160. python -c ""import psutil;print(p/sutil.cpu_count. ())"" #160. vim deepvariant/resources.py. --------------------------------. def _get_cpu_count():. """"""Gets the number of physical cores in this machine. Returns:. int >= 1 if the call to get the cpu_count succeeded, or 0 if not. """""". # return psutil.cpu_count(logical=False) or 0 ==> comment. return 20. --------------------------------. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:19827,availability,Error,Error,19827,"v=LD_LIBRARY_PATH \. --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH \. --action_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH --test_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH \. --action_env=PYTHON_LIB_PATH=$PYTHON_LIB_PATH --test_env=PYTHON_LIB_PATH=$PYTHON_LIB_PATH \. --action_env=BAZEL_PYTHON=$BAZEL_PYTHON --test_env=BAZEL_PYTHON=$BAZEL_PYTHON >& output.log 2>&1 &. bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" deepvariant:gpu_tests --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH. # for CPU only. bazel test -c opt ${DV_COPT_FLAGS} ""$@"" deepvariant/... # build binary. bazel build -c opt ${DV_COPT_FLAGS} ""$@"" deepvariant:binaries --build_python_zip --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH. echo 'Expect a usage message:'. (bazel-bin/deepvariant/call_variants --help || : ) | grep '/call_variants.py:'. bazel build :licenses_zip --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH. ```. ## Fix DV Error. ```bash. ################################################################################. # bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@""//deepvariant:resources_test. # use lscpu to show the actual CPU number. ################################################################################. python -c ""import multiprocessing; print(multiprocessing.cpu_count())"" #160. python -c ""import psutil;print(p/sutil.cpu_count. ())"" #160. vim deepvariant/resources.py. --------------------------------. def _get_cpu_count():. """"""Gets the number of physical cores in this machine. Returns:. int >= 1 if the call to get the cpu_count succeeded, or 0 if not. """""". # return psutil.cpu_count(logical=False) or 0 ==> comment. return 20. --------------------------------. vim deepvariant/resources_test.py. --------------------------------. def test_metrics_is_ok_when_cpu_count_returns_none(self):. # Some psutil functions, such as cpu_freq(), can return None depending on. # the environment; mak",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:20981,availability,monitor,monitor,20981,"""//deepvariant:resources_test. # use lscpu to show the actual CPU number. ################################################################################. python -c ""import multiprocessing; print(multiprocessing.cpu_count())"" #160. python -c ""import psutil;print(p/sutil.cpu_count. ())"" #160. vim deepvariant/resources.py. --------------------------------. def _get_cpu_count():. """"""Gets the number of physical cores in this machine. Returns:. int >= 1 if the call to get the cpu_count succeeded, or 0 if not. """""". # return psutil.cpu_count(logical=False) or 0 ==> comment. return 20. --------------------------------. vim deepvariant/resources_test.py. --------------------------------. def test_metrics_is_ok_when_cpu_count_returns_none(self):. # Some psutil functions, such as cpu_freq(), can return None depending on. # the environment; make sure we don't crash when that occurs. with mock.patch.object(resources.psutil, 'cpu_count', return_value=None):. with resources.ResourceMonitor() as monitor:. #self.assertEqual(monitor.metrics().physical_core_count, 0) ==> comment. self.assertEqual(monitor.metrics().physical_core_count, 20). --------------------------------. ##########################################################################. # //deepvariant/realigner/allele_count_linear:generate_trained_model_test. # ImportError: /root/.local/lib/python2.7/site-packages/sklearn/__check_build/_check_build.so: undefined symbol: PyUnicodeUCS4_DecodeUTF8. ##########################################################################. # reinstall numpy, scipy, Cpython, scikit-learn to fix with AT built Python. python -c ""import numpy"" # prequests of TF 1.12.0. python -c ""import scipy"" # prequests of TF 1.12.0. pip install Cython --force-reinstall --no-deos. pip install scikit-learn --force-reinstall --no-deos. # build from source. wget https://github.com/scikit-learn/scikit-learn/archive/0.20.2.tar.gz. tar zxvf 0.20.2.tar.gz. cd scikit-learn-0.20.2. python setup.py bdist_wheel. # verify",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:21009,availability,monitor,monitor,21009,"t. # use lscpu to show the actual CPU number. ################################################################################. python -c ""import multiprocessing; print(multiprocessing.cpu_count())"" #160. python -c ""import psutil;print(p/sutil.cpu_count. ())"" #160. vim deepvariant/resources.py. --------------------------------. def _get_cpu_count():. """"""Gets the number of physical cores in this machine. Returns:. int >= 1 if the call to get the cpu_count succeeded, or 0 if not. """""". # return psutil.cpu_count(logical=False) or 0 ==> comment. return 20. --------------------------------. vim deepvariant/resources_test.py. --------------------------------. def test_metrics_is_ok_when_cpu_count_returns_none(self):. # Some psutil functions, such as cpu_freq(), can return None depending on. # the environment; make sure we don't crash when that occurs. with mock.patch.object(resources.psutil, 'cpu_count', return_value=None):. with resources.ResourceMonitor() as monitor:. #self.assertEqual(monitor.metrics().physical_core_count, 0) ==> comment. self.assertEqual(monitor.metrics().physical_core_count, 20). --------------------------------. ##########################################################################. # //deepvariant/realigner/allele_count_linear:generate_trained_model_test. # ImportError: /root/.local/lib/python2.7/site-packages/sklearn/__check_build/_check_build.so: undefined symbol: PyUnicodeUCS4_DecodeUTF8. ##########################################################################. # reinstall numpy, scipy, Cpython, scikit-learn to fix with AT built Python. python -c ""import numpy"" # prequests of TF 1.12.0. python -c ""import scipy"" # prequests of TF 1.12.0. pip install Cython --force-reinstall --no-deos. pip install scikit-learn --force-reinstall --no-deos. # build from source. wget https://github.com/scikit-learn/scikit-learn/archive/0.20.2.tar.gz. tar zxvf 0.20.2.tar.gz. cd scikit-learn-0.20.2. python setup.py bdist_wheel. # verify. python -c ""from sklearn.ex",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:21081,availability,monitor,monitor,21081,"######################################################. python -c ""import multiprocessing; print(multiprocessing.cpu_count())"" #160. python -c ""import psutil;print(p/sutil.cpu_count. ())"" #160. vim deepvariant/resources.py. --------------------------------. def _get_cpu_count():. """"""Gets the number of physical cores in this machine. Returns:. int >= 1 if the call to get the cpu_count succeeded, or 0 if not. """""". # return psutil.cpu_count(logical=False) or 0 ==> comment. return 20. --------------------------------. vim deepvariant/resources_test.py. --------------------------------. def test_metrics_is_ok_when_cpu_count_returns_none(self):. # Some psutil functions, such as cpu_freq(), can return None depending on. # the environment; make sure we don't crash when that occurs. with mock.patch.object(resources.psutil, 'cpu_count', return_value=None):. with resources.ResourceMonitor() as monitor:. #self.assertEqual(monitor.metrics().physical_core_count, 0) ==> comment. self.assertEqual(monitor.metrics().physical_core_count, 20). --------------------------------. ##########################################################################. # //deepvariant/realigner/allele_count_linear:generate_trained_model_test. # ImportError: /root/.local/lib/python2.7/site-packages/sklearn/__check_build/_check_build.so: undefined symbol: PyUnicodeUCS4_DecodeUTF8. ##########################################################################. # reinstall numpy, scipy, Cpython, scikit-learn to fix with AT built Python. python -c ""import numpy"" # prequests of TF 1.12.0. python -c ""import scipy"" # prequests of TF 1.12.0. pip install Cython --force-reinstall --no-deos. pip install scikit-learn --force-reinstall --no-deos. # build from source. wget https://github.com/scikit-learn/scikit-learn/archive/0.20.2.tar.gz. tar zxvf 0.20.2.tar.gz. cd scikit-learn-0.20.2. python setup.py bdist_wheel. # verify. python -c ""from sklearn.externals import joblib"". ################################################",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:432,deployability,modul,module,432,"Hi @DiableJambe ,. Sorry for my late response, I do not have a docker image yet. And I attached the detailed steps here for your reference. It's a bit long :). # DeepVariant. ## Environment. ```bash. # Power8 environment. export CPU=power8. export HOMEPATH=/home/qilibj. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # AT 11.0 environment. source /etc/profile. module load at11.0. export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # python2 and pip environment. export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH. export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. export PYTHON_BIN_PATH=/home/qilibj/inst/bin/python. export PYTHON_LIB_PATH=/home/qilibj/inst/lib/python2.7/site-packages. ```. ## Cmake 3.13.3. Build and install doc: [https://cmake.org/install/](https://cmake.org/install/). >Note: only can be built with system gcc, AT11.0 will cause build failure. ```bash. # download source code. wget https://github.com/Kitware/CMake/releases/download/v3.13.3/cmake-3.13.3.tar.gz. tar -zxvf cmake-3.13.3.tar.gz. cd cmake-3.13.3. # build scirpt. ./bootstrap. make -j20. make -j20 install. export PATH=/usr/local/bin:$PATH. ```. ## Protobuf 3.6.1 C++ static. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from /usr/bin/gcc, static for bazel. ```bash. # download source code. wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:808,deployability,Build,Build,808,"Hi @DiableJambe ,. Sorry for my late response, I do not have a docker image yet. And I attached the detailed steps here for your reference. It's a bit long :). # DeepVariant. ## Environment. ```bash. # Power8 environment. export CPU=power8. export HOMEPATH=/home/qilibj. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # AT 11.0 environment. source /etc/profile. module load at11.0. export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # python2 and pip environment. export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH. export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. export PYTHON_BIN_PATH=/home/qilibj/inst/bin/python. export PYTHON_LIB_PATH=/home/qilibj/inst/lib/python2.7/site-packages. ```. ## Cmake 3.13.3. Build and install doc: [https://cmake.org/install/](https://cmake.org/install/). >Note: only can be built with system gcc, AT11.0 will cause build failure. ```bash. # download source code. wget https://github.com/Kitware/CMake/releases/download/v3.13.3/cmake-3.13.3.tar.gz. tar -zxvf cmake-3.13.3.tar.gz. cd cmake-3.13.3. # build scirpt. ./bootstrap. make -j20. make -j20 install. export PATH=/usr/local/bin:$PATH. ```. ## Protobuf 3.6.1 C++ static. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from /usr/bin/gcc, static for bazel. ```bash. # download source code. wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:818,deployability,instal,install,818,"Hi @DiableJambe ,. Sorry for my late response, I do not have a docker image yet. And I attached the detailed steps here for your reference. It's a bit long :). # DeepVariant. ## Environment. ```bash. # Power8 environment. export CPU=power8. export HOMEPATH=/home/qilibj. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # AT 11.0 environment. source /etc/profile. module load at11.0. export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # python2 and pip environment. export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH. export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. export PYTHON_BIN_PATH=/home/qilibj/inst/bin/python. export PYTHON_LIB_PATH=/home/qilibj/inst/lib/python2.7/site-packages. ```. ## Cmake 3.13.3. Build and install doc: [https://cmake.org/install/](https://cmake.org/install/). >Note: only can be built with system gcc, AT11.0 will cause build failure. ```bash. # download source code. wget https://github.com/Kitware/CMake/releases/download/v3.13.3/cmake-3.13.3.tar.gz. tar -zxvf cmake-3.13.3.tar.gz. cd cmake-3.13.3. # build scirpt. ./bootstrap. make -j20. make -j20 install. export PATH=/usr/local/bin:$PATH. ```. ## Protobuf 3.6.1 C++ static. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from /usr/bin/gcc, static for bazel. ```bash. # download source code. wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:850,deployability,instal,install,850,"Hi @DiableJambe ,. Sorry for my late response, I do not have a docker image yet. And I attached the detailed steps here for your reference. It's a bit long :). # DeepVariant. ## Environment. ```bash. # Power8 environment. export CPU=power8. export HOMEPATH=/home/qilibj. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # AT 11.0 environment. source /etc/profile. module load at11.0. export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # python2 and pip environment. export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH. export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. export PYTHON_BIN_PATH=/home/qilibj/inst/bin/python. export PYTHON_LIB_PATH=/home/qilibj/inst/lib/python2.7/site-packages. ```. ## Cmake 3.13.3. Build and install doc: [https://cmake.org/install/](https://cmake.org/install/). >Note: only can be built with system gcc, AT11.0 will cause build failure. ```bash. # download source code. wget https://github.com/Kitware/CMake/releases/download/v3.13.3/cmake-3.13.3.tar.gz. tar -zxvf cmake-3.13.3.tar.gz. cd cmake-3.13.3. # build scirpt. ./bootstrap. make -j20. make -j20 install. export PATH=/usr/local/bin:$PATH. ```. ## Protobuf 3.6.1 C++ static. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from /usr/bin/gcc, static for bazel. ```bash. # download source code. wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:878,deployability,instal,install,878,"Hi @DiableJambe ,. Sorry for my late response, I do not have a docker image yet. And I attached the detailed steps here for your reference. It's a bit long :). # DeepVariant. ## Environment. ```bash. # Power8 environment. export CPU=power8. export HOMEPATH=/home/qilibj. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # AT 11.0 environment. source /etc/profile. module load at11.0. export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # python2 and pip environment. export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH. export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. export PYTHON_BIN_PATH=/home/qilibj/inst/bin/python. export PYTHON_LIB_PATH=/home/qilibj/inst/lib/python2.7/site-packages. ```. ## Cmake 3.13.3. Build and install doc: [https://cmake.org/install/](https://cmake.org/install/). >Note: only can be built with system gcc, AT11.0 will cause build failure. ```bash. # download source code. wget https://github.com/Kitware/CMake/releases/download/v3.13.3/cmake-3.13.3.tar.gz. tar -zxvf cmake-3.13.3.tar.gz. cd cmake-3.13.3. # build scirpt. ./bootstrap. make -j20. make -j20 install. export PATH=/usr/local/bin:$PATH. ```. ## Protobuf 3.6.1 C++ static. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from /usr/bin/gcc, static for bazel. ```bash. # download source code. wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:949,deployability,build,build,949,"Hi @DiableJambe ,. Sorry for my late response, I do not have a docker image yet. And I attached the detailed steps here for your reference. It's a bit long :). # DeepVariant. ## Environment. ```bash. # Power8 environment. export CPU=power8. export HOMEPATH=/home/qilibj. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # AT 11.0 environment. source /etc/profile. module load at11.0. export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # python2 and pip environment. export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH. export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. export PYTHON_BIN_PATH=/home/qilibj/inst/bin/python. export PYTHON_LIB_PATH=/home/qilibj/inst/lib/python2.7/site-packages. ```. ## Cmake 3.13.3. Build and install doc: [https://cmake.org/install/](https://cmake.org/install/). >Note: only can be built with system gcc, AT11.0 will cause build failure. ```bash. # download source code. wget https://github.com/Kitware/CMake/releases/download/v3.13.3/cmake-3.13.3.tar.gz. tar -zxvf cmake-3.13.3.tar.gz. cd cmake-3.13.3. # build scirpt. ./bootstrap. make -j20. make -j20 install. export PATH=/usr/local/bin:$PATH. ```. ## Protobuf 3.6.1 C++ static. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from /usr/bin/gcc, static for bazel. ```bash. # download source code. wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:955,deployability,fail,failure,955,"Hi @DiableJambe ,. Sorry for my late response, I do not have a docker image yet. And I attached the detailed steps here for your reference. It's a bit long :). # DeepVariant. ## Environment. ```bash. # Power8 environment. export CPU=power8. export HOMEPATH=/home/qilibj. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # AT 11.0 environment. source /etc/profile. module load at11.0. export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # python2 and pip environment. export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH. export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. export PYTHON_BIN_PATH=/home/qilibj/inst/bin/python. export PYTHON_LIB_PATH=/home/qilibj/inst/lib/python2.7/site-packages. ```. ## Cmake 3.13.3. Build and install doc: [https://cmake.org/install/](https://cmake.org/install/). >Note: only can be built with system gcc, AT11.0 will cause build failure. ```bash. # download source code. wget https://github.com/Kitware/CMake/releases/download/v3.13.3/cmake-3.13.3.tar.gz. tar -zxvf cmake-3.13.3.tar.gz. cd cmake-3.13.3. # build scirpt. ./bootstrap. make -j20. make -j20 install. export PATH=/usr/local/bin:$PATH. ```. ## Protobuf 3.6.1 C++ static. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from /usr/bin/gcc, static for bazel. ```bash. # download source code. wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:1035,deployability,releas,releases,1035,"sponse, I do not have a docker image yet. And I attached the detailed steps here for your reference. It's a bit long :). # DeepVariant. ## Environment. ```bash. # Power8 environment. export CPU=power8. export HOMEPATH=/home/qilibj. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # AT 11.0 environment. source /etc/profile. module load at11.0. export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # python2 and pip environment. export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH. export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. export PYTHON_BIN_PATH=/home/qilibj/inst/bin/python. export PYTHON_LIB_PATH=/home/qilibj/inst/lib/python2.7/site-packages. ```. ## Cmake 3.13.3. Build and install doc: [https://cmake.org/install/](https://cmake.org/install/). >Note: only can be built with system gcc, AT11.0 will cause build failure. ```bash. # download source code. wget https://github.com/Kitware/CMake/releases/download/v3.13.3/cmake-3.13.3.tar.gz. tar -zxvf cmake-3.13.3.tar.gz. cd cmake-3.13.3. # build scirpt. ./bootstrap. make -j20. make -j20 install. export PATH=/usr/local/bin:$PATH. ```. ## Protobuf 3.6.1 C++ static. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from /usr/bin/gcc, static for bazel. ```bash. # download source code. wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$C",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:1132,deployability,build,build,1132,"nce. It's a bit long :). # DeepVariant. ## Environment. ```bash. # Power8 environment. export CPU=power8. export HOMEPATH=/home/qilibj. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # AT 11.0 environment. source /etc/profile. module load at11.0. export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # python2 and pip environment. export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH. export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. export PYTHON_BIN_PATH=/home/qilibj/inst/bin/python. export PYTHON_LIB_PATH=/home/qilibj/inst/lib/python2.7/site-packages. ```. ## Cmake 3.13.3. Build and install doc: [https://cmake.org/install/](https://cmake.org/install/). >Note: only can be built with system gcc, AT11.0 will cause build failure. ```bash. # download source code. wget https://github.com/Kitware/CMake/releases/download/v3.13.3/cmake-3.13.3.tar.gz. tar -zxvf cmake-3.13.3.tar.gz. cd cmake-3.13.3. # build scirpt. ./bootstrap. make -j20. make -j20 install. export PATH=/usr/local/bin:$PATH. ```. ## Protobuf 3.6.1 C++ static. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from /usr/bin/gcc, static for bazel. ```bash. # download source code. wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --disable-shared --enable-static. make c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:1180,deployability,instal,install,1180,"nment. ```bash. # Power8 environment. export CPU=power8. export HOMEPATH=/home/qilibj. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # AT 11.0 environment. source /etc/profile. module load at11.0. export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # python2 and pip environment. export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH. export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. export PYTHON_BIN_PATH=/home/qilibj/inst/bin/python. export PYTHON_LIB_PATH=/home/qilibj/inst/lib/python2.7/site-packages. ```. ## Cmake 3.13.3. Build and install doc: [https://cmake.org/install/](https://cmake.org/install/). >Note: only can be built with system gcc, AT11.0 will cause build failure. ```bash. # download source code. wget https://github.com/Kitware/CMake/releases/download/v3.13.3/cmake-3.13.3.tar.gz. tar -zxvf cmake-3.13.3.tar.gz. cd cmake-3.13.3. # build scirpt. ./bootstrap. make -j20. make -j20 install. export PATH=/usr/local/bin:$PATH. ```. ## Protobuf 3.6.1 C++ static. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from /usr/bin/gcc, static for bazel. ```bash. # download source code. wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --disable-shared --enable-static. make clean. make -j20. make -j20 check # check if proto",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:1565,deployability,releas,releases,1565,"j/inst/lib:$LD_LIBRARY_PATH. export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. export PYTHON_BIN_PATH=/home/qilibj/inst/bin/python. export PYTHON_LIB_PATH=/home/qilibj/inst/lib/python2.7/site-packages. ```. ## Cmake 3.13.3. Build and install doc: [https://cmake.org/install/](https://cmake.org/install/). >Note: only can be built with system gcc, AT11.0 will cause build failure. ```bash. # download source code. wget https://github.com/Kitware/CMake/releases/download/v3.13.3/cmake-3.13.3.tar.gz. tar -zxvf cmake-3.13.3.tar.gz. cd cmake-3.13.3. # build scirpt. ./bootstrap. make -j20. make -j20 install. export PATH=/usr/local/bin:$PATH. ```. ## Protobuf 3.6.1 C++ static. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from /usr/bin/gcc, static for bazel. ```bash. # download source code. wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --disable-shared --enable-static. make clean. make -j20. make -j20 check # check if protobuf build is good. # install. make install. sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig. #cd .. # verify. echo ""$(which protoc)"". echo ""$(protoc --version)"". ```. ## Bazel 0.15.0. Build instructions: [https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel](https://docs.bazel.build/versions/master/install-compile-source.html#",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:1698,deployability,build,build,1698,"st/bin/python. export PYTHON_LIB_PATH=/home/qilibj/inst/lib/python2.7/site-packages. ```. ## Cmake 3.13.3. Build and install doc: [https://cmake.org/install/](https://cmake.org/install/). >Note: only can be built with system gcc, AT11.0 will cause build failure. ```bash. # download source code. wget https://github.com/Kitware/CMake/releases/download/v3.13.3/cmake-3.13.3.tar.gz. tar -zxvf cmake-3.13.3.tar.gz. cd cmake-3.13.3. # build scirpt. ./bootstrap. make -j20. make -j20 install. export PATH=/usr/local/bin:$PATH. ```. ## Protobuf 3.6.1 C++ static. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from /usr/bin/gcc, static for bazel. ```bash. # download source code. wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --disable-shared --enable-static. make clean. make -j20. make -j20 check # check if protobuf build is good. # install. make install. sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig. #cd .. # verify. echo ""$(which protoc)"". echo ""$(protoc --version)"". ```. ## Bazel 0.15.0. Build instructions: [https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel](https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel). Edit crosstool: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:1926,deployability,build,build,1926,", AT11.0 will cause build failure. ```bash. # download source code. wget https://github.com/Kitware/CMake/releases/download/v3.13.3/cmake-3.13.3.tar.gz. tar -zxvf cmake-3.13.3.tar.gz. cd cmake-3.13.3. # build scirpt. ./bootstrap. make -j20. make -j20 install. export PATH=/usr/local/bin:$PATH. ```. ## Protobuf 3.6.1 C++ static. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from /usr/bin/gcc, static for bazel. ```bash. # download source code. wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --disable-shared --enable-static. make clean. make -j20. make -j20 check # check if protobuf build is good. # install. make install. sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig. #cd .. # verify. echo ""$(which protoc)"". echo ""$(protoc --version)"". ```. ## Bazel 0.15.0. Build instructions: [https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel](https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel). Edit crosstool: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). > Note: Bazel 0.15.0 should be built from /usr/bin/gcc. ```bash. # download source code. wget https://github.com/bazelbuild/bazel/releases/download/0.15.0/bazel-0.15.0-dist.zip. mkdir bazel-0.15.0. unzip -n baz",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:1943,deployability,instal,install,1943,"e build failure. ```bash. # download source code. wget https://github.com/Kitware/CMake/releases/download/v3.13.3/cmake-3.13.3.tar.gz. tar -zxvf cmake-3.13.3.tar.gz. cd cmake-3.13.3. # build scirpt. ./bootstrap. make -j20. make -j20 install. export PATH=/usr/local/bin:$PATH. ```. ## Protobuf 3.6.1 C++ static. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from /usr/bin/gcc, static for bazel. ```bash. # download source code. wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --disable-shared --enable-static. make clean. make -j20. make -j20 check # check if protobuf build is good. # install. make install. sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig. #cd .. # verify. echo ""$(which protoc)"". echo ""$(protoc --version)"". ```. ## Bazel 0.15.0. Build instructions: [https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel](https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel). Edit crosstool: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). > Note: Bazel 0.15.0 should be built from /usr/bin/gcc. ```bash. # download source code. wget https://github.com/bazelbuild/bazel/releases/download/0.15.0/bazel-0.15.0-dist.zip. mkdir bazel-0.15.0. unzip -n bazel-0.15.0-dist.zip",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:2188,deployability,build,build,2188,"port PATH=/usr/local/bin:$PATH. ```. ## Protobuf 3.6.1 C++ static. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from /usr/bin/gcc, static for bazel. ```bash. # download source code. wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --disable-shared --enable-static. make clean. make -j20. make -j20 check # check if protobuf build is good. # install. make install. sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig. #cd .. # verify. echo ""$(which protoc)"". echo ""$(protoc --version)"". ```. ## Bazel 0.15.0. Build instructions: [https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel](https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel). Edit crosstool: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). > Note: Bazel 0.15.0 should be built from /usr/bin/gcc. ```bash. # download source code. wget https://github.com/bazelbuild/bazel/releases/download/0.15.0/bazel-0.15.0-dist.zip. mkdir bazel-0.15.0. unzip -n bazel-0.15.0-dist.zip -d ./bazel-0.15.0. cd bazel-0.15.0. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export JAVA_HOME=/usr/lib/jvm/java-1.8.0. export PATH=$HOMEPATH/inst/bin:$JAVA_HOME/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:2205,deployability,instal,install,2205,"al/bin:$PATH. ```. ## Protobuf 3.6.1 C++ static. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from /usr/bin/gcc, static for bazel. ```bash. # download source code. wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --disable-shared --enable-static. make clean. make -j20. make -j20 check # check if protobuf build is good. # install. make install. sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig. #cd .. # verify. echo ""$(which protoc)"". echo ""$(protoc --version)"". ```. ## Bazel 0.15.0. Build instructions: [https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel](https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel). Edit crosstool: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). > Note: Bazel 0.15.0 should be built from /usr/bin/gcc. ```bash. # download source code. wget https://github.com/bazelbuild/bazel/releases/download/0.15.0/bazel-0.15.0-dist.zip. mkdir bazel-0.15.0. unzip -n bazel-0.15.0-dist.zip -d ./bazel-0.15.0. cd bazel-0.15.0. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export JAVA_HOME=/usr/lib/jvm/java-1.8.0. export PATH=$HOMEPATH/inst/bin:$JAVA_HOME/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:2219,deployability,instal,install,2219,"```. ## Protobuf 3.6.1 C++ static. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from /usr/bin/gcc, static for bazel. ```bash. # download source code. wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --disable-shared --enable-static. make clean. make -j20. make -j20 check # check if protobuf build is good. # install. make install. sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig. #cd .. # verify. echo ""$(which protoc)"". echo ""$(protoc --version)"". ```. ## Bazel 0.15.0. Build instructions: [https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel](https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel). Edit crosstool: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). > Note: Bazel 0.15.0 should be built from /usr/bin/gcc. ```bash. # download source code. wget https://github.com/bazelbuild/bazel/releases/download/0.15.0/bazel-0.15.0-dist.zip. mkdir bazel-0.15.0. unzip -n bazel-0.15.0-dist.zip -d ./bazel-0.15.0. cd bazel-0.15.0. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export JAVA_HOME=/usr/lib/jvm/java-1.8.0. export PATH=$HOMEPATH/inst/bin:$JAVA_HOME/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. #",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:2360,deployability,version,version,2360,"ffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from /usr/bin/gcc, static for bazel. ```bash. # download source code. wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --disable-shared --enable-static. make clean. make -j20. make -j20 check # check if protobuf build is good. # install. make install. sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig. #cd .. # verify. echo ""$(which protoc)"". echo ""$(protoc --version)"". ```. ## Bazel 0.15.0. Build instructions: [https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel](https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel). Edit crosstool: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). > Note: Bazel 0.15.0 should be built from /usr/bin/gcc. ```bash. # download source code. wget https://github.com/bazelbuild/bazel/releases/download/0.15.0/bazel-0.15.0-dist.zip. mkdir bazel-0.15.0. unzip -n bazel-0.15.0-dist.zip -d ./bazel-0.15.0. cd bazel-0.15.0. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export JAVA_HOME=/usr/lib/jvm/java-1.8.0. export PATH=$HOMEPATH/inst/bin:$JAVA_HOME/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # build from scratch. PROTOC=$HOMEPATH/inst/bin/protoc ./compile.sh. rsync -avP output/bazel $HOMEPATH/inst/bin/. # verification. bazel info. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:2393,deployability,Build,Build,2393,"EADME.md). > Note: Protobuf 3.6.1 should be built from /usr/bin/gcc, static for bazel. ```bash. # download source code. wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --disable-shared --enable-static. make clean. make -j20. make -j20 check # check if protobuf build is good. # install. make install. sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig. #cd .. # verify. echo ""$(which protoc)"". echo ""$(protoc --version)"". ```. ## Bazel 0.15.0. Build instructions: [https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel](https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel). Edit crosstool: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). > Note: Bazel 0.15.0 should be built from /usr/bin/gcc. ```bash. # download source code. wget https://github.com/bazelbuild/bazel/releases/download/0.15.0/bazel-0.15.0-dist.zip. mkdir bazel-0.15.0. unzip -n bazel-0.15.0-dist.zip -d ./bazel-0.15.0. cd bazel-0.15.0. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export JAVA_HOME=/usr/lib/jvm/java-1.8.0. export PATH=$HOMEPATH/inst/bin:$JAVA_HOME/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # build from scratch. PROTOC=$HOMEPATH/inst/bin/protoc ./compile.sh. rsync -avP output/bazel $HOMEPATH/inst/bin/. # verification. bazel info. ```. ## Advance Toolchain 11.0. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:2433,deployability,build,build,2433," be built from /usr/bin/gcc, static for bazel. ```bash. # download source code. wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --disable-shared --enable-static. make clean. make -j20. make -j20 check # check if protobuf build is good. # install. make install. sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig. #cd .. # verify. echo ""$(which protoc)"". echo ""$(protoc --version)"". ```. ## Bazel 0.15.0. Build instructions: [https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel](https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel). Edit crosstool: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). > Note: Bazel 0.15.0 should be built from /usr/bin/gcc. ```bash. # download source code. wget https://github.com/bazelbuild/bazel/releases/download/0.15.0/bazel-0.15.0-dist.zip. mkdir bazel-0.15.0. unzip -n bazel-0.15.0-dist.zip -d ./bazel-0.15.0. cd bazel-0.15.0. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export JAVA_HOME=/usr/lib/jvm/java-1.8.0. export PATH=$HOMEPATH/inst/bin:$JAVA_HOME/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # build from scratch. PROTOC=$HOMEPATH/inst/bin/protoc ./compile.sh. rsync -avP output/bazel $HOMEPATH/inst/bin/. # verification. bazel info. ```. ## Advance Toolchain 11.0. Doc: [https://developer.ibm.com/linuxonp",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:2439,deployability,version,versions,2439,"lt from /usr/bin/gcc, static for bazel. ```bash. # download source code. wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --disable-shared --enable-static. make clean. make -j20. make -j20 check # check if protobuf build is good. # install. make install. sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig. #cd .. # verify. echo ""$(which protoc)"". echo ""$(protoc --version)"". ```. ## Bazel 0.15.0. Build instructions: [https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel](https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel). Edit crosstool: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). > Note: Bazel 0.15.0 should be built from /usr/bin/gcc. ```bash. # download source code. wget https://github.com/bazelbuild/bazel/releases/download/0.15.0/bazel-0.15.0-dist.zip. mkdir bazel-0.15.0. unzip -n bazel-0.15.0-dist.zip -d ./bazel-0.15.0. cd bazel-0.15.0. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export JAVA_HOME=/usr/lib/jvm/java-1.8.0. export PATH=$HOMEPATH/inst/bin:$JAVA_HOME/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # build from scratch. PROTOC=$HOMEPATH/inst/bin/protoc ./compile.sh. rsync -avP output/bazel $HOMEPATH/inst/bin/. # verification. bazel info. ```. ## Advance Toolchain 11.0. Doc: [https://developer.ibm.com/linuxonpower/ad",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:2455,deployability,instal,install-compile-source,2455,"tatic for bazel. ```bash. # download source code. wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --disable-shared --enable-static. make clean. make -j20. make -j20 check # check if protobuf build is good. # install. make install. sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig. #cd .. # verify. echo ""$(which protoc)"". echo ""$(protoc --version)"". ```. ## Bazel 0.15.0. Build instructions: [https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel](https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel). Edit crosstool: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). > Note: Bazel 0.15.0 should be built from /usr/bin/gcc. ```bash. # download source code. wget https://github.com/bazelbuild/bazel/releases/download/0.15.0/bazel-0.15.0-dist.zip. mkdir bazel-0.15.0. unzip -n bazel-0.15.0-dist.zip -d ./bazel-0.15.0. cd bazel-0.15.0. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export JAVA_HOME=/usr/lib/jvm/java-1.8.0. export PATH=$HOMEPATH/inst/bin:$JAVA_HOME/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # build from scratch. PROTOC=$HOMEPATH/inst/bin/protoc ./compile.sh. rsync -avP output/bazel $HOMEPATH/inst/bin/. # verification. bazel info. ```. ## Advance Toolchain 11.0. Doc: [https://developer.ibm.com/linuxonpower/advance-toolchain/advtool",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:2519,deployability,build,build,2519,"ttps://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --disable-shared --enable-static. make clean. make -j20. make -j20 check # check if protobuf build is good. # install. make install. sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig. #cd .. # verify. echo ""$(which protoc)"". echo ""$(protoc --version)"". ```. ## Bazel 0.15.0. Build instructions: [https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel](https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel). Edit crosstool: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). > Note: Bazel 0.15.0 should be built from /usr/bin/gcc. ```bash. # download source code. wget https://github.com/bazelbuild/bazel/releases/download/0.15.0/bazel-0.15.0-dist.zip. mkdir bazel-0.15.0. unzip -n bazel-0.15.0-dist.zip -d ./bazel-0.15.0. cd bazel-0.15.0. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export JAVA_HOME=/usr/lib/jvm/java-1.8.0. export PATH=$HOMEPATH/inst/bin:$JAVA_HOME/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # build from scratch. PROTOC=$HOMEPATH/inst/bin/protoc ./compile.sh. rsync -avP output/bazel $HOMEPATH/inst/bin/. # verification. bazel info. ```. ## Advance Toolchain 11.0. Doc: [https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/](https://developer.ibm.com/linuxonpower/a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:2525,deployability,version,versions,2525,"github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --disable-shared --enable-static. make clean. make -j20. make -j20 check # check if protobuf build is good. # install. make install. sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig. #cd .. # verify. echo ""$(which protoc)"". echo ""$(protoc --version)"". ```. ## Bazel 0.15.0. Build instructions: [https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel](https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel). Edit crosstool: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). > Note: Bazel 0.15.0 should be built from /usr/bin/gcc. ```bash. # download source code. wget https://github.com/bazelbuild/bazel/releases/download/0.15.0/bazel-0.15.0-dist.zip. mkdir bazel-0.15.0. unzip -n bazel-0.15.0-dist.zip -d ./bazel-0.15.0. cd bazel-0.15.0. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export JAVA_HOME=/usr/lib/jvm/java-1.8.0. export PATH=$HOMEPATH/inst/bin:$JAVA_HOME/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # build from scratch. PROTOC=$HOMEPATH/inst/bin/protoc ./compile.sh. rsync -avP output/bazel $HOMEPATH/inst/bin/. # verification. bazel info. ```. ## Advance Toolchain 11.0. Doc: [https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/](https://developer.ibm.com/linuxonpower/advance-",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:2541,deployability,instal,install-compile-source,2541,"ers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --disable-shared --enable-static. make clean. make -j20. make -j20 check # check if protobuf build is good. # install. make install. sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig. #cd .. # verify. echo ""$(which protoc)"". echo ""$(protoc --version)"". ```. ## Bazel 0.15.0. Build instructions: [https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel](https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel). Edit crosstool: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). > Note: Bazel 0.15.0 should be built from /usr/bin/gcc. ```bash. # download source code. wget https://github.com/bazelbuild/bazel/releases/download/0.15.0/bazel-0.15.0-dist.zip. mkdir bazel-0.15.0. unzip -n bazel-0.15.0-dist.zip -d ./bazel-0.15.0. cd bazel-0.15.0. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export JAVA_HOME=/usr/lib/jvm/java-1.8.0. export PATH=$HOMEPATH/inst/bin:$JAVA_HOME/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # build from scratch. PROTOC=$HOMEPATH/inst/bin/protoc ./compile.sh. rsync -avP output/bazel $HOMEPATH/inst/bin/. # verification. bazel info. ```. ## Advance Toolchain 11.0. Doc: [https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/](https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-insta",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:2849,deployability,releas,releases,2849,"bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --disable-shared --enable-static. make clean. make -j20. make -j20 check # check if protobuf build is good. # install. make install. sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig. #cd .. # verify. echo ""$(which protoc)"". echo ""$(protoc --version)"". ```. ## Bazel 0.15.0. Build instructions: [https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel](https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel). Edit crosstool: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). > Note: Bazel 0.15.0 should be built from /usr/bin/gcc. ```bash. # download source code. wget https://github.com/bazelbuild/bazel/releases/download/0.15.0/bazel-0.15.0-dist.zip. mkdir bazel-0.15.0. unzip -n bazel-0.15.0-dist.zip -d ./bazel-0.15.0. cd bazel-0.15.0. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export JAVA_HOME=/usr/lib/jvm/java-1.8.0. export PATH=$HOMEPATH/inst/bin:$JAVA_HOME/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # build from scratch. PROTOC=$HOMEPATH/inst/bin/protoc ./compile.sh. rsync -avP output/bazel $HOMEPATH/inst/bin/. # verification. bazel info. ```. ## Advance Toolchain 11.0. Doc: [https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/](https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/). ```bash. # gpg public key. wget ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. rpm --import gpg-pubkey-6976a827-5164221b. # Configure the Advance Toolchain repositories. [root@cit1074 deepvariant]# cat /etc/yum.repos.d/advance-toolchain.repo. #B",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:3224,deployability,build,build,3224," sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig. #cd .. # verify. echo ""$(which protoc)"". echo ""$(protoc --version)"". ```. ## Bazel 0.15.0. Build instructions: [https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel](https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel). Edit crosstool: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). > Note: Bazel 0.15.0 should be built from /usr/bin/gcc. ```bash. # download source code. wget https://github.com/bazelbuild/bazel/releases/download/0.15.0/bazel-0.15.0-dist.zip. mkdir bazel-0.15.0. unzip -n bazel-0.15.0-dist.zip -d ./bazel-0.15.0. cd bazel-0.15.0. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export JAVA_HOME=/usr/lib/jvm/java-1.8.0. export PATH=$HOMEPATH/inst/bin:$JAVA_HOME/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # build from scratch. PROTOC=$HOMEPATH/inst/bin/protoc ./compile.sh. rsync -avP output/bazel $HOMEPATH/inst/bin/. # verification. bazel info. ```. ## Advance Toolchain 11.0. Doc: [https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/](https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/). ```bash. # gpg public key. wget ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. rpm --import gpg-pubkey-6976a827-5164221b. # Configure the Advance Toolchain repositories. [root@cit1074 deepvariant]# cat /etc/yum.repos.d/advance-toolchain.repo. #Begin of configuration file. [advance-toolchain]. name=Advance Toolchain Unicamp FTP. baseurl=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7. failovermethod=priority. enabled=1. gpgcheck=1. gpgkey=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. # End of configuration file. # Install the Advance Toolchain. yum insta",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:3380,deployability,Toolchain,Toolchain,3380,0.15.0. Build instructions: [https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel](https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel). Edit crosstool: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). > Note: Bazel 0.15.0 should be built from /usr/bin/gcc. ```bash. # download source code. wget https://github.com/bazelbuild/bazel/releases/download/0.15.0/bazel-0.15.0-dist.zip. mkdir bazel-0.15.0. unzip -n bazel-0.15.0-dist.zip -d ./bazel-0.15.0. cd bazel-0.15.0. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export JAVA_HOME=/usr/lib/jvm/java-1.8.0. export PATH=$HOMEPATH/inst/bin:$JAVA_HOME/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # build from scratch. PROTOC=$HOMEPATH/inst/bin/protoc ./compile.sh. rsync -avP output/bazel $HOMEPATH/inst/bin/. # verification. bazel info. ```. ## Advance Toolchain 11.0. Doc: [https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/](https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/). ```bash. # gpg public key. wget ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. rpm --import gpg-pubkey-6976a827-5164221b. # Configure the Advance Toolchain repositories. [root@cit1074 deepvariant]# cat /etc/yum.repos.d/advance-toolchain.repo. #Begin of configuration file. [advance-toolchain]. name=Advance Toolchain Unicamp FTP. baseurl=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7. failovermethod=priority. enabled=1. gpgcheck=1. gpgkey=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. # End of configuration file. # Install the Advance Toolchain. yum install advance-toolchain-at11.0-runtime. yum install advance-toolchain-at11.0-devel. yum install advance-toolchain-at11.0-perf. yum install advance-toolchain-at11,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:3449,deployability,toolchain,toolchain,3449,/install-compile-source.html#bootstrap-bazel](https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel). Edit crosstool: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). > Note: Bazel 0.15.0 should be built from /usr/bin/gcc. ```bash. # download source code. wget https://github.com/bazelbuild/bazel/releases/download/0.15.0/bazel-0.15.0-dist.zip. mkdir bazel-0.15.0. unzip -n bazel-0.15.0-dist.zip -d ./bazel-0.15.0. cd bazel-0.15.0. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export JAVA_HOME=/usr/lib/jvm/java-1.8.0. export PATH=$HOMEPATH/inst/bin:$JAVA_HOME/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # build from scratch. PROTOC=$HOMEPATH/inst/bin/protoc ./compile.sh. rsync -avP output/bazel $HOMEPATH/inst/bin/. # verification. bazel info. ```. ## Advance Toolchain 11.0. Doc: [https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/](https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/). ```bash. # gpg public key. wget ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. rpm --import gpg-pubkey-6976a827-5164221b. # Configure the Advance Toolchain repositories. [root@cit1074 deepvariant]# cat /etc/yum.repos.d/advance-toolchain.repo. #Begin of configuration file. [advance-toolchain]. name=Advance Toolchain Unicamp FTP. baseurl=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7. failovermethod=priority. enabled=1. gpgcheck=1. gpgkey=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. # End of configuration file. # Install the Advance Toolchain. yum install advance-toolchain-at11.0-runtime. yum install advance-toolchain-at11.0-devel. yum install advance-toolchain-at11.0-perf. yum install advance-toolchain-at11.0-mcore-libs. # Load enviroment. export PATH=/opt/at11.0/bin:/opt/at,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:3467,deployability,instal,installation,3467,urce.html#bootstrap-bazel](https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel). Edit crosstool: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). > Note: Bazel 0.15.0 should be built from /usr/bin/gcc. ```bash. # download source code. wget https://github.com/bazelbuild/bazel/releases/download/0.15.0/bazel-0.15.0-dist.zip. mkdir bazel-0.15.0. unzip -n bazel-0.15.0-dist.zip -d ./bazel-0.15.0. cd bazel-0.15.0. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export JAVA_HOME=/usr/lib/jvm/java-1.8.0. export PATH=$HOMEPATH/inst/bin:$JAVA_HOME/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # build from scratch. PROTOC=$HOMEPATH/inst/bin/protoc ./compile.sh. rsync -avP output/bazel $HOMEPATH/inst/bin/. # verification. bazel info. ```. ## Advance Toolchain 11.0. Doc: [https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/](https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/). ```bash. # gpg public key. wget ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. rpm --import gpg-pubkey-6976a827-5164221b. # Configure the Advance Toolchain repositories. [root@cit1074 deepvariant]# cat /etc/yum.repos.d/advance-toolchain.repo. #Begin of configuration file. [advance-toolchain]. name=Advance Toolchain Unicamp FTP. baseurl=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7. failovermethod=priority. enabled=1. gpgcheck=1. gpgkey=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. # End of configuration file. # Install the Advance Toolchain. yum install advance-toolchain-at11.0-runtime. yum install advance-toolchain-at11.0-devel. yum install advance-toolchain-at11.0-perf. yum install advance-toolchain-at11.0-mcore-libs. # Load enviroment. export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # ,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:3529,deployability,toolchain,toolchain,3529,master/install-compile-source.html#bootstrap-bazel). Edit crosstool: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). > Note: Bazel 0.15.0 should be built from /usr/bin/gcc. ```bash. # download source code. wget https://github.com/bazelbuild/bazel/releases/download/0.15.0/bazel-0.15.0-dist.zip. mkdir bazel-0.15.0. unzip -n bazel-0.15.0-dist.zip -d ./bazel-0.15.0. cd bazel-0.15.0. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export JAVA_HOME=/usr/lib/jvm/java-1.8.0. export PATH=$HOMEPATH/inst/bin:$JAVA_HOME/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # build from scratch. PROTOC=$HOMEPATH/inst/bin/protoc ./compile.sh. rsync -avP output/bazel $HOMEPATH/inst/bin/. # verification. bazel info. ```. ## Advance Toolchain 11.0. Doc: [https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/](https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/). ```bash. # gpg public key. wget ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. rpm --import gpg-pubkey-6976a827-5164221b. # Configure the Advance Toolchain repositories. [root@cit1074 deepvariant]# cat /etc/yum.repos.d/advance-toolchain.repo. #Begin of configuration file. [advance-toolchain]. name=Advance Toolchain Unicamp FTP. baseurl=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7. failovermethod=priority. enabled=1. gpgcheck=1. gpgkey=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. # End of configuration file. # Install the Advance Toolchain. yum install advance-toolchain-at11.0-runtime. yum install advance-toolchain-at11.0-devel. yum install advance-toolchain-at11.0-perf. yum install advance-toolchain-at11.0-mcore-libs. # Load enviroment. export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # Do not need to export. # export LD_LIBRARY_PATH=/opt/at11.0/l,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:3547,deployability,instal,installation,3547,ile-source.html#bootstrap-bazel). Edit crosstool: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). > Note: Bazel 0.15.0 should be built from /usr/bin/gcc. ```bash. # download source code. wget https://github.com/bazelbuild/bazel/releases/download/0.15.0/bazel-0.15.0-dist.zip. mkdir bazel-0.15.0. unzip -n bazel-0.15.0-dist.zip -d ./bazel-0.15.0. cd bazel-0.15.0. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export JAVA_HOME=/usr/lib/jvm/java-1.8.0. export PATH=$HOMEPATH/inst/bin:$JAVA_HOME/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # build from scratch. PROTOC=$HOMEPATH/inst/bin/protoc ./compile.sh. rsync -avP output/bazel $HOMEPATH/inst/bin/. # verification. bazel info. ```. ## Advance Toolchain 11.0. Doc: [https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/](https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/). ```bash. # gpg public key. wget ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. rpm --import gpg-pubkey-6976a827-5164221b. # Configure the Advance Toolchain repositories. [root@cit1074 deepvariant]# cat /etc/yum.repos.d/advance-toolchain.repo. #Begin of configuration file. [advance-toolchain]. name=Advance Toolchain Unicamp FTP. baseurl=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7. failovermethod=priority. enabled=1. gpgcheck=1. gpgkey=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. # End of configuration file. # Install the Advance Toolchain. yum install advance-toolchain-at11.0-runtime. yum install advance-toolchain-at11.0-devel. yum install advance-toolchain-at11.0-perf. yum install advance-toolchain-at11.0-mcore-libs. # Load enviroment. export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # Do not need to export. # export LD_LIBRARY_PATH=/opt/at11.0/lib64:/lib64:$LD_LIB,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:3631,deployability,toolchain,toolchain,3631,yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). > Note: Bazel 0.15.0 should be built from /usr/bin/gcc. ```bash. # download source code. wget https://github.com/bazelbuild/bazel/releases/download/0.15.0/bazel-0.15.0-dist.zip. mkdir bazel-0.15.0. unzip -n bazel-0.15.0-dist.zip -d ./bazel-0.15.0. cd bazel-0.15.0. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export JAVA_HOME=/usr/lib/jvm/java-1.8.0. export PATH=$HOMEPATH/inst/bin:$JAVA_HOME/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # build from scratch. PROTOC=$HOMEPATH/inst/bin/protoc ./compile.sh. rsync -avP output/bazel $HOMEPATH/inst/bin/. # verification. bazel info. ```. ## Advance Toolchain 11.0. Doc: [https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/](https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/). ```bash. # gpg public key. wget ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. rpm --import gpg-pubkey-6976a827-5164221b. # Configure the Advance Toolchain repositories. [root@cit1074 deepvariant]# cat /etc/yum.repos.d/advance-toolchain.repo. #Begin of configuration file. [advance-toolchain]. name=Advance Toolchain Unicamp FTP. baseurl=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7. failovermethod=priority. enabled=1. gpgcheck=1. gpgkey=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. # End of configuration file. # Install the Advance Toolchain. yum install advance-toolchain-at11.0-runtime. yum install advance-toolchain-at11.0-devel. yum install advance-toolchain-at11.0-perf. yum install advance-toolchain-at11.0-mcore-libs. # Load enviroment. export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # Do not need to export. # export LD_LIBRARY_PATH=/opt/at11.0/lib64:/lib64:$LD_LIBRARY_PATH . # Load environment. sudo yum install environment-modules. source /etc/p,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:3754,deployability,Toolchain,Toolchain,3754,m /usr/bin/gcc. ```bash. # download source code. wget https://github.com/bazelbuild/bazel/releases/download/0.15.0/bazel-0.15.0-dist.zip. mkdir bazel-0.15.0. unzip -n bazel-0.15.0-dist.zip -d ./bazel-0.15.0. cd bazel-0.15.0. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export JAVA_HOME=/usr/lib/jvm/java-1.8.0. export PATH=$HOMEPATH/inst/bin:$JAVA_HOME/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # build from scratch. PROTOC=$HOMEPATH/inst/bin/protoc ./compile.sh. rsync -avP output/bazel $HOMEPATH/inst/bin/. # verification. bazel info. ```. ## Advance Toolchain 11.0. Doc: [https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/](https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/). ```bash. # gpg public key. wget ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. rpm --import gpg-pubkey-6976a827-5164221b. # Configure the Advance Toolchain repositories. [root@cit1074 deepvariant]# cat /etc/yum.repos.d/advance-toolchain.repo. #Begin of configuration file. [advance-toolchain]. name=Advance Toolchain Unicamp FTP. baseurl=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7. failovermethod=priority. enabled=1. gpgcheck=1. gpgkey=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. # End of configuration file. # Install the Advance Toolchain. yum install advance-toolchain-at11.0-runtime. yum install advance-toolchain-at11.0-devel. yum install advance-toolchain-at11.0-perf. yum install advance-toolchain-at11.0-mcore-libs. # Load enviroment. export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # Do not need to export. # export LD_LIBRARY_PATH=/opt/at11.0/lib64:/lib64:$LD_LIBRARY_PATH . # Load environment. sudo yum install environment-modules. source /etc/profile. module load at11.0. module unload at11.0. ```. ## Python 2 and Pip 19.0.1. Build instructions: [https://docs.python,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:3835,deployability,toolchain,toolchain,3835,ld/bazel/releases/download/0.15.0/bazel-0.15.0-dist.zip. mkdir bazel-0.15.0. unzip -n bazel-0.15.0-dist.zip -d ./bazel-0.15.0. cd bazel-0.15.0. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export JAVA_HOME=/usr/lib/jvm/java-1.8.0. export PATH=$HOMEPATH/inst/bin:$JAVA_HOME/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # build from scratch. PROTOC=$HOMEPATH/inst/bin/protoc ./compile.sh. rsync -avP output/bazel $HOMEPATH/inst/bin/. # verification. bazel info. ```. ## Advance Toolchain 11.0. Doc: [https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/](https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/). ```bash. # gpg public key. wget ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. rpm --import gpg-pubkey-6976a827-5164221b. # Configure the Advance Toolchain repositories. [root@cit1074 deepvariant]# cat /etc/yum.repos.d/advance-toolchain.repo. #Begin of configuration file. [advance-toolchain]. name=Advance Toolchain Unicamp FTP. baseurl=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7. failovermethod=priority. enabled=1. gpgcheck=1. gpgkey=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. # End of configuration file. # Install the Advance Toolchain. yum install advance-toolchain-at11.0-runtime. yum install advance-toolchain-at11.0-devel. yum install advance-toolchain-at11.0-perf. yum install advance-toolchain-at11.0-mcore-libs. # Load enviroment. export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # Do not need to export. # export LD_LIBRARY_PATH=/opt/at11.0/lib64:/lib64:$LD_LIBRARY_PATH . # Load environment. sudo yum install environment-modules. source /etc/profile. module load at11.0. module unload at11.0. ```. ## Python 2 and Pip 19.0.1. Build instructions: [https://docs.python.org/2/using/unix.html](https://docs.python.org/2/using/unix.html). > Note: Pytho,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:3861,deployability,configurat,configuration,3861,.15.0/bazel-0.15.0-dist.zip. mkdir bazel-0.15.0. unzip -n bazel-0.15.0-dist.zip -d ./bazel-0.15.0. cd bazel-0.15.0. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export JAVA_HOME=/usr/lib/jvm/java-1.8.0. export PATH=$HOMEPATH/inst/bin:$JAVA_HOME/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # build from scratch. PROTOC=$HOMEPATH/inst/bin/protoc ./compile.sh. rsync -avP output/bazel $HOMEPATH/inst/bin/. # verification. bazel info. ```. ## Advance Toolchain 11.0. Doc: [https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/](https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/). ```bash. # gpg public key. wget ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. rpm --import gpg-pubkey-6976a827-5164221b. # Configure the Advance Toolchain repositories. [root@cit1074 deepvariant]# cat /etc/yum.repos.d/advance-toolchain.repo. #Begin of configuration file. [advance-toolchain]. name=Advance Toolchain Unicamp FTP. baseurl=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7. failovermethod=priority. enabled=1. gpgcheck=1. gpgkey=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. # End of configuration file. # Install the Advance Toolchain. yum install advance-toolchain-at11.0-runtime. yum install advance-toolchain-at11.0-devel. yum install advance-toolchain-at11.0-perf. yum install advance-toolchain-at11.0-mcore-libs. # Load enviroment. export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # Do not need to export. # export LD_LIBRARY_PATH=/opt/at11.0/lib64:/lib64:$LD_LIBRARY_PATH . # Load environment. sudo yum install environment-modules. source /etc/profile. module load at11.0. module unload at11.0. ```. ## Python 2 and Pip 19.0.1. Build instructions: [https://docs.python.org/2/using/unix.html](https://docs.python.org/2/using/unix.html). > Note: Python 2 should be built from AT ,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:3890,deployability,toolchain,toolchain,3890,. mkdir bazel-0.15.0. unzip -n bazel-0.15.0-dist.zip -d ./bazel-0.15.0. cd bazel-0.15.0. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export JAVA_HOME=/usr/lib/jvm/java-1.8.0. export PATH=$HOMEPATH/inst/bin:$JAVA_HOME/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # build from scratch. PROTOC=$HOMEPATH/inst/bin/protoc ./compile.sh. rsync -avP output/bazel $HOMEPATH/inst/bin/. # verification. bazel info. ```. ## Advance Toolchain 11.0. Doc: [https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/](https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/). ```bash. # gpg public key. wget ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. rpm --import gpg-pubkey-6976a827-5164221b. # Configure the Advance Toolchain repositories. [root@cit1074 deepvariant]# cat /etc/yum.repos.d/advance-toolchain.repo. #Begin of configuration file. [advance-toolchain]. name=Advance Toolchain Unicamp FTP. baseurl=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7. failovermethod=priority. enabled=1. gpgcheck=1. gpgkey=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. # End of configuration file. # Install the Advance Toolchain. yum install advance-toolchain-at11.0-runtime. yum install advance-toolchain-at11.0-devel. yum install advance-toolchain-at11.0-perf. yum install advance-toolchain-at11.0-mcore-libs. # Load enviroment. export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # Do not need to export. # export LD_LIBRARY_PATH=/opt/at11.0/lib64:/lib64:$LD_LIBRARY_PATH . # Load environment. sudo yum install environment-modules. source /etc/profile. module load at11.0. module unload at11.0. ```. ## Python 2 and Pip 19.0.1. Build instructions: [https://docs.python.org/2/using/unix.html](https://docs.python.org/2/using/unix.html). > Note: Python 2 should be built from AT 11.0. ```bash. # download s,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:3915,deployability,Toolchain,Toolchain,3915,ip -n bazel-0.15.0-dist.zip -d ./bazel-0.15.0. cd bazel-0.15.0. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export JAVA_HOME=/usr/lib/jvm/java-1.8.0. export PATH=$HOMEPATH/inst/bin:$JAVA_HOME/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # build from scratch. PROTOC=$HOMEPATH/inst/bin/protoc ./compile.sh. rsync -avP output/bazel $HOMEPATH/inst/bin/. # verification. bazel info. ```. ## Advance Toolchain 11.0. Doc: [https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/](https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/). ```bash. # gpg public key. wget ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. rpm --import gpg-pubkey-6976a827-5164221b. # Configure the Advance Toolchain repositories. [root@cit1074 deepvariant]# cat /etc/yum.repos.d/advance-toolchain.repo. #Begin of configuration file. [advance-toolchain]. name=Advance Toolchain Unicamp FTP. baseurl=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7. failovermethod=priority. enabled=1. gpgcheck=1. gpgkey=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. # End of configuration file. # Install the Advance Toolchain. yum install advance-toolchain-at11.0-runtime. yum install advance-toolchain-at11.0-devel. yum install advance-toolchain-at11.0-perf. yum install advance-toolchain-at11.0-mcore-libs. # Load enviroment. export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # Do not need to export. # export LD_LIBRARY_PATH=/opt/at11.0/lib64:/lib64:$LD_LIBRARY_PATH . # Load environment. sudo yum install environment-modules. source /etc/profile. module load at11.0. module unload at11.0. ```. ## Python 2 and Pip 19.0.1. Build instructions: [https://docs.python.org/2/using/unix.html](https://docs.python.org/2/using/unix.html). > Note: Python 2 should be built from AT 11.0. ```bash. # download source code . wget https:/,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:3982,deployability,toolchain,toolchain,3982,nvironment. export CPU=power8. export HOMEPATH=/home/qilibj. export JAVA_HOME=/usr/lib/jvm/java-1.8.0. export PATH=$HOMEPATH/inst/bin:$JAVA_HOME/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # build from scratch. PROTOC=$HOMEPATH/inst/bin/protoc ./compile.sh. rsync -avP output/bazel $HOMEPATH/inst/bin/. # verification. bazel info. ```. ## Advance Toolchain 11.0. Doc: [https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/](https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/). ```bash. # gpg public key. wget ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. rpm --import gpg-pubkey-6976a827-5164221b. # Configure the Advance Toolchain repositories. [root@cit1074 deepvariant]# cat /etc/yum.repos.d/advance-toolchain.repo. #Begin of configuration file. [advance-toolchain]. name=Advance Toolchain Unicamp FTP. baseurl=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7. failovermethod=priority. enabled=1. gpgcheck=1. gpgkey=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. # End of configuration file. # Install the Advance Toolchain. yum install advance-toolchain-at11.0-runtime. yum install advance-toolchain-at11.0-devel. yum install advance-toolchain-at11.0-perf. yum install advance-toolchain-at11.0-mcore-libs. # Load enviroment. export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # Do not need to export. # export LD_LIBRARY_PATH=/opt/at11.0/lib64:/lib64:$LD_LIBRARY_PATH . # Load environment. sudo yum install environment-modules. source /etc/profile. module load at11.0. module unload at11.0. ```. ## Python 2 and Pip 19.0.1. Build instructions: [https://docs.python.org/2/using/unix.html](https://docs.python.org/2/using/unix.html). > Note: Python 2 should be built from AT 11.0. ```bash. # download source code . wget https://www.python.org/ftp/python/2.7.15/Python-2.7.15.tgz. tar -zxvf Pyth,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:4009,deployability,fail,failovermethod,4009,. export HOMEPATH=/home/qilibj. export JAVA_HOME=/usr/lib/jvm/java-1.8.0. export PATH=$HOMEPATH/inst/bin:$JAVA_HOME/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # build from scratch. PROTOC=$HOMEPATH/inst/bin/protoc ./compile.sh. rsync -avP output/bazel $HOMEPATH/inst/bin/. # verification. bazel info. ```. ## Advance Toolchain 11.0. Doc: [https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/](https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/). ```bash. # gpg public key. wget ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. rpm --import gpg-pubkey-6976a827-5164221b. # Configure the Advance Toolchain repositories. [root@cit1074 deepvariant]# cat /etc/yum.repos.d/advance-toolchain.repo. #Begin of configuration file. [advance-toolchain]. name=Advance Toolchain Unicamp FTP. baseurl=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7. failovermethod=priority. enabled=1. gpgcheck=1. gpgkey=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. # End of configuration file. # Install the Advance Toolchain. yum install advance-toolchain-at11.0-runtime. yum install advance-toolchain-at11.0-devel. yum install advance-toolchain-at11.0-perf. yum install advance-toolchain-at11.0-mcore-libs. # Load enviroment. export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # Do not need to export. # export LD_LIBRARY_PATH=/opt/at11.0/lib64:/lib64:$LD_LIBRARY_PATH . # Load environment. sudo yum install environment-modules. source /etc/profile. module load at11.0. module unload at11.0. ```. ## Python 2 and Pip 19.0.1. Build instructions: [https://docs.python.org/2/using/unix.html](https://docs.python.org/2/using/unix.html). > Note: Python 2 should be built from AT 11.0. ```bash. # download source code . wget https://www.python.org/ftp/python/2.7.15/Python-2.7.15.tgz. tar -zxvf Python-2.7.15.tgz. cd Python-2.7.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:4100,deployability,toolchain,toolchain,4100,MEPATH/inst/bin:$JAVA_HOME/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # build from scratch. PROTOC=$HOMEPATH/inst/bin/protoc ./compile.sh. rsync -avP output/bazel $HOMEPATH/inst/bin/. # verification. bazel info. ```. ## Advance Toolchain 11.0. Doc: [https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/](https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/). ```bash. # gpg public key. wget ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. rpm --import gpg-pubkey-6976a827-5164221b. # Configure the Advance Toolchain repositories. [root@cit1074 deepvariant]# cat /etc/yum.repos.d/advance-toolchain.repo. #Begin of configuration file. [advance-toolchain]. name=Advance Toolchain Unicamp FTP. baseurl=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7. failovermethod=priority. enabled=1. gpgcheck=1. gpgkey=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. # End of configuration file. # Install the Advance Toolchain. yum install advance-toolchain-at11.0-runtime. yum install advance-toolchain-at11.0-devel. yum install advance-toolchain-at11.0-perf. yum install advance-toolchain-at11.0-mcore-libs. # Load enviroment. export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # Do not need to export. # export LD_LIBRARY_PATH=/opt/at11.0/lib64:/lib64:$LD_LIBRARY_PATH . # Load environment. sudo yum install environment-modules. source /etc/profile. module load at11.0. module unload at11.0. ```. ## Python 2 and Pip 19.0.1. Build instructions: [https://docs.python.org/2/using/unix.html](https://docs.python.org/2/using/unix.html). > Note: Python 2 should be built from AT 11.0. ```bash. # download source code . wget https://www.python.org/ftp/python/2.7.15/Python-2.7.15.tgz. tar -zxvf Python-2.7.15.tgz. cd Python-2.7.15. # environment. export HOMEPATH=/home/qilibj. export CPU=power8. # check gcc before bu,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:4165,deployability,configurat,configuration,4165,"sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # build from scratch. PROTOC=$HOMEPATH/inst/bin/protoc ./compile.sh. rsync -avP output/bazel $HOMEPATH/inst/bin/. # verification. bazel info. ```. ## Advance Toolchain 11.0. Doc: [https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/](https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/). ```bash. # gpg public key. wget ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. rpm --import gpg-pubkey-6976a827-5164221b. # Configure the Advance Toolchain repositories. [root@cit1074 deepvariant]# cat /etc/yum.repos.d/advance-toolchain.repo. #Begin of configuration file. [advance-toolchain]. name=Advance Toolchain Unicamp FTP. baseurl=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7. failovermethod=priority. enabled=1. gpgcheck=1. gpgkey=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. # End of configuration file. # Install the Advance Toolchain. yum install advance-toolchain-at11.0-runtime. yum install advance-toolchain-at11.0-devel. yum install advance-toolchain-at11.0-perf. yum install advance-toolchain-at11.0-mcore-libs. # Load enviroment. export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # Do not need to export. # export LD_LIBRARY_PATH=/opt/at11.0/lib64:/lib64:$LD_LIBRARY_PATH . # Load environment. sudo yum install environment-modules. source /etc/profile. module load at11.0. module unload at11.0. ```. ## Python 2 and Pip 19.0.1. Build instructions: [https://docs.python.org/2/using/unix.html](https://docs.python.org/2/using/unix.html). > Note: Python 2 should be built from AT 11.0. ```bash. # download source code . wget https://www.python.org/ftp/python/2.7.15/Python-2.7.15.tgz. tar -zxvf Python-2.7.15.tgz. cd Python-2.7.15. # environment. export HOMEPATH=/home/qilibj. export CPU=power8. # check gcc before build, should be AT11.0. which gcc. # build. CFLAGS=""-mcpu=$CPU -mtun",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:4187,deployability,Instal,Install,4187,":/usr/sbin:/usr/bin:/root/bin. # build from scratch. PROTOC=$HOMEPATH/inst/bin/protoc ./compile.sh. rsync -avP output/bazel $HOMEPATH/inst/bin/. # verification. bazel info. ```. ## Advance Toolchain 11.0. Doc: [https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/](https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/). ```bash. # gpg public key. wget ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. rpm --import gpg-pubkey-6976a827-5164221b. # Configure the Advance Toolchain repositories. [root@cit1074 deepvariant]# cat /etc/yum.repos.d/advance-toolchain.repo. #Begin of configuration file. [advance-toolchain]. name=Advance Toolchain Unicamp FTP. baseurl=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7. failovermethod=priority. enabled=1. gpgcheck=1. gpgkey=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. # End of configuration file. # Install the Advance Toolchain. yum install advance-toolchain-at11.0-runtime. yum install advance-toolchain-at11.0-devel. yum install advance-toolchain-at11.0-perf. yum install advance-toolchain-at11.0-mcore-libs. # Load enviroment. export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # Do not need to export. # export LD_LIBRARY_PATH=/opt/at11.0/lib64:/lib64:$LD_LIBRARY_PATH . # Load environment. sudo yum install environment-modules. source /etc/profile. module load at11.0. module unload at11.0. ```. ## Python 2 and Pip 19.0.1. Build instructions: [https://docs.python.org/2/using/unix.html](https://docs.python.org/2/using/unix.html). > Note: Python 2 should be built from AT 11.0. ```bash. # download source code . wget https://www.python.org/ftp/python/2.7.15/Python-2.7.15.tgz. tar -zxvf Python-2.7.15.tgz. cd Python-2.7.15. # environment. export HOMEPATH=/home/qilibj. export CPU=power8. # check gcc before build, should be AT11.0. which gcc. # build. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CPPFLAG",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:4207,deployability,Toolchain,Toolchain,4207,"root/bin. # build from scratch. PROTOC=$HOMEPATH/inst/bin/protoc ./compile.sh. rsync -avP output/bazel $HOMEPATH/inst/bin/. # verification. bazel info. ```. ## Advance Toolchain 11.0. Doc: [https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/](https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/). ```bash. # gpg public key. wget ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. rpm --import gpg-pubkey-6976a827-5164221b. # Configure the Advance Toolchain repositories. [root@cit1074 deepvariant]# cat /etc/yum.repos.d/advance-toolchain.repo. #Begin of configuration file. [advance-toolchain]. name=Advance Toolchain Unicamp FTP. baseurl=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7. failovermethod=priority. enabled=1. gpgcheck=1. gpgkey=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. # End of configuration file. # Install the Advance Toolchain. yum install advance-toolchain-at11.0-runtime. yum install advance-toolchain-at11.0-devel. yum install advance-toolchain-at11.0-perf. yum install advance-toolchain-at11.0-mcore-libs. # Load enviroment. export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # Do not need to export. # export LD_LIBRARY_PATH=/opt/at11.0/lib64:/lib64:$LD_LIBRARY_PATH . # Load environment. sudo yum install environment-modules. source /etc/profile. module load at11.0. module unload at11.0. ```. ## Python 2 and Pip 19.0.1. Build instructions: [https://docs.python.org/2/using/unix.html](https://docs.python.org/2/using/unix.html). > Note: Python 2 should be built from AT 11.0. ```bash. # download source code . wget https://www.python.org/ftp/python/2.7.15/Python-2.7.15.tgz. tar -zxvf Python-2.7.15.tgz. cd Python-2.7.15. # environment. export HOMEPATH=/home/qilibj. export CPU=power8. # check gcc before build, should be AT11.0. which gcc. # build. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CPPFLAGS=""-mcpu=$CPU -mtune=",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:4222,deployability,instal,install,4222,"ild from scratch. PROTOC=$HOMEPATH/inst/bin/protoc ./compile.sh. rsync -avP output/bazel $HOMEPATH/inst/bin/. # verification. bazel info. ```. ## Advance Toolchain 11.0. Doc: [https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/](https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/). ```bash. # gpg public key. wget ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. rpm --import gpg-pubkey-6976a827-5164221b. # Configure the Advance Toolchain repositories. [root@cit1074 deepvariant]# cat /etc/yum.repos.d/advance-toolchain.repo. #Begin of configuration file. [advance-toolchain]. name=Advance Toolchain Unicamp FTP. baseurl=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7. failovermethod=priority. enabled=1. gpgcheck=1. gpgkey=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. # End of configuration file. # Install the Advance Toolchain. yum install advance-toolchain-at11.0-runtime. yum install advance-toolchain-at11.0-devel. yum install advance-toolchain-at11.0-perf. yum install advance-toolchain-at11.0-mcore-libs. # Load enviroment. export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # Do not need to export. # export LD_LIBRARY_PATH=/opt/at11.0/lib64:/lib64:$LD_LIBRARY_PATH . # Load environment. sudo yum install environment-modules. source /etc/profile. module load at11.0. module unload at11.0. ```. ## Python 2 and Pip 19.0.1. Build instructions: [https://docs.python.org/2/using/unix.html](https://docs.python.org/2/using/unix.html). > Note: Python 2 should be built from AT 11.0. ```bash. # download source code . wget https://www.python.org/ftp/python/2.7.15/Python-2.7.15.tgz. tar -zxvf Python-2.7.15.tgz. cd Python-2.7.15. # environment. export HOMEPATH=/home/qilibj. export CPU=power8. # check gcc before build, should be AT11.0. which gcc. # build. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CPPFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./co",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:4238,deployability,toolchain,toolchain-,4238," PROTOC=$HOMEPATH/inst/bin/protoc ./compile.sh. rsync -avP output/bazel $HOMEPATH/inst/bin/. # verification. bazel info. ```. ## Advance Toolchain 11.0. Doc: [https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/](https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/). ```bash. # gpg public key. wget ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. rpm --import gpg-pubkey-6976a827-5164221b. # Configure the Advance Toolchain repositories. [root@cit1074 deepvariant]# cat /etc/yum.repos.d/advance-toolchain.repo. #Begin of configuration file. [advance-toolchain]. name=Advance Toolchain Unicamp FTP. baseurl=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7. failovermethod=priority. enabled=1. gpgcheck=1. gpgkey=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. # End of configuration file. # Install the Advance Toolchain. yum install advance-toolchain-at11.0-runtime. yum install advance-toolchain-at11.0-devel. yum install advance-toolchain-at11.0-perf. yum install advance-toolchain-at11.0-mcore-libs. # Load enviroment. export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # Do not need to export. # export LD_LIBRARY_PATH=/opt/at11.0/lib64:/lib64:$LD_LIBRARY_PATH . # Load environment. sudo yum install environment-modules. source /etc/profile. module load at11.0. module unload at11.0. ```. ## Python 2 and Pip 19.0.1. Build instructions: [https://docs.python.org/2/using/unix.html](https://docs.python.org/2/using/unix.html). > Note: Python 2 should be built from AT 11.0. ```bash. # download source code . wget https://www.python.org/ftp/python/2.7.15/Python-2.7.15.tgz. tar -zxvf Python-2.7.15.tgz. cd Python-2.7.15. # environment. export HOMEPATH=/home/qilibj. export CPU=power8. # check gcc before build, should be AT11.0. which gcc. # build. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CPPFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:4268,deployability,instal,install,4268,"otoc ./compile.sh. rsync -avP output/bazel $HOMEPATH/inst/bin/. # verification. bazel info. ```. ## Advance Toolchain 11.0. Doc: [https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/](https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/). ```bash. # gpg public key. wget ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. rpm --import gpg-pubkey-6976a827-5164221b. # Configure the Advance Toolchain repositories. [root@cit1074 deepvariant]# cat /etc/yum.repos.d/advance-toolchain.repo. #Begin of configuration file. [advance-toolchain]. name=Advance Toolchain Unicamp FTP. baseurl=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7. failovermethod=priority. enabled=1. gpgcheck=1. gpgkey=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. # End of configuration file. # Install the Advance Toolchain. yum install advance-toolchain-at11.0-runtime. yum install advance-toolchain-at11.0-devel. yum install advance-toolchain-at11.0-perf. yum install advance-toolchain-at11.0-mcore-libs. # Load enviroment. export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # Do not need to export. # export LD_LIBRARY_PATH=/opt/at11.0/lib64:/lib64:$LD_LIBRARY_PATH . # Load environment. sudo yum install environment-modules. source /etc/profile. module load at11.0. module unload at11.0. ```. ## Python 2 and Pip 19.0.1. Build instructions: [https://docs.python.org/2/using/unix.html](https://docs.python.org/2/using/unix.html). > Note: Python 2 should be built from AT 11.0. ```bash. # download source code . wget https://www.python.org/ftp/python/2.7.15/Python-2.7.15.tgz. tar -zxvf Python-2.7.15.tgz. cd Python-2.7.15. # environment. export HOMEPATH=/home/qilibj. export CPU=power8. # check gcc before build, should be AT11.0. which gcc. # build. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CPPFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-share",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:4284,deployability,toolchain,toolchain-,4284,". rsync -avP output/bazel $HOMEPATH/inst/bin/. # verification. bazel info. ```. ## Advance Toolchain 11.0. Doc: [https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/](https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/). ```bash. # gpg public key. wget ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. rpm --import gpg-pubkey-6976a827-5164221b. # Configure the Advance Toolchain repositories. [root@cit1074 deepvariant]# cat /etc/yum.repos.d/advance-toolchain.repo. #Begin of configuration file. [advance-toolchain]. name=Advance Toolchain Unicamp FTP. baseurl=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7. failovermethod=priority. enabled=1. gpgcheck=1. gpgkey=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. # End of configuration file. # Install the Advance Toolchain. yum install advance-toolchain-at11.0-runtime. yum install advance-toolchain-at11.0-devel. yum install advance-toolchain-at11.0-perf. yum install advance-toolchain-at11.0-mcore-libs. # Load enviroment. export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # Do not need to export. # export LD_LIBRARY_PATH=/opt/at11.0/lib64:/lib64:$LD_LIBRARY_PATH . # Load environment. sudo yum install environment-modules. source /etc/profile. module load at11.0. module unload at11.0. ```. ## Python 2 and Pip 19.0.1. Build instructions: [https://docs.python.org/2/using/unix.html](https://docs.python.org/2/using/unix.html). > Note: Python 2 should be built from AT 11.0. ```bash. # download source code . wget https://www.python.org/ftp/python/2.7.15/Python-2.7.15.tgz. tar -zxvf Python-2.7.15.tgz. cd Python-2.7.15. # environment. export HOMEPATH=/home/qilibj. export CPU=power8. # check gcc before build, should be AT11.0. which gcc. # build. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CPPFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-stati",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:4312,deployability,instal,install,4312,"HOMEPATH/inst/bin/. # verification. bazel info. ```. ## Advance Toolchain 11.0. Doc: [https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/](https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/). ```bash. # gpg public key. wget ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. rpm --import gpg-pubkey-6976a827-5164221b. # Configure the Advance Toolchain repositories. [root@cit1074 deepvariant]# cat /etc/yum.repos.d/advance-toolchain.repo. #Begin of configuration file. [advance-toolchain]. name=Advance Toolchain Unicamp FTP. baseurl=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7. failovermethod=priority. enabled=1. gpgcheck=1. gpgkey=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. # End of configuration file. # Install the Advance Toolchain. yum install advance-toolchain-at11.0-runtime. yum install advance-toolchain-at11.0-devel. yum install advance-toolchain-at11.0-perf. yum install advance-toolchain-at11.0-mcore-libs. # Load enviroment. export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # Do not need to export. # export LD_LIBRARY_PATH=/opt/at11.0/lib64:/lib64:$LD_LIBRARY_PATH . # Load environment. sudo yum install environment-modules. source /etc/profile. module load at11.0. module unload at11.0. ```. ## Python 2 and Pip 19.0.1. Build instructions: [https://docs.python.org/2/using/unix.html](https://docs.python.org/2/using/unix.html). > Note: Python 2 should be built from AT 11.0. ```bash. # download source code . wget https://www.python.org/ftp/python/2.7.15/Python-2.7.15.tgz. tar -zxvf Python-2.7.15.tgz. cd Python-2.7.15. # environment. export HOMEPATH=/home/qilibj. export CPU=power8. # check gcc before build, should be AT11.0. which gcc. # build. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CPPFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static. make -j20. make install.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:4328,deployability,toolchain,toolchain-,4328,"/. # verification. bazel info. ```. ## Advance Toolchain 11.0. Doc: [https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/](https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/). ```bash. # gpg public key. wget ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. rpm --import gpg-pubkey-6976a827-5164221b. # Configure the Advance Toolchain repositories. [root@cit1074 deepvariant]# cat /etc/yum.repos.d/advance-toolchain.repo. #Begin of configuration file. [advance-toolchain]. name=Advance Toolchain Unicamp FTP. baseurl=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7. failovermethod=priority. enabled=1. gpgcheck=1. gpgkey=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. # End of configuration file. # Install the Advance Toolchain. yum install advance-toolchain-at11.0-runtime. yum install advance-toolchain-at11.0-devel. yum install advance-toolchain-at11.0-perf. yum install advance-toolchain-at11.0-mcore-libs. # Load enviroment. export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # Do not need to export. # export LD_LIBRARY_PATH=/opt/at11.0/lib64:/lib64:$LD_LIBRARY_PATH . # Load environment. sudo yum install environment-modules. source /etc/profile. module load at11.0. module unload at11.0. ```. ## Python 2 and Pip 19.0.1. Build instructions: [https://docs.python.org/2/using/unix.html](https://docs.python.org/2/using/unix.html). > Note: Python 2 should be built from AT 11.0. ```bash. # download source code . wget https://www.python.org/ftp/python/2.7.15/Python-2.7.15.tgz. tar -zxvf Python-2.7.15.tgz. cd Python-2.7.15. # environment. export HOMEPATH=/home/qilibj. export CPU=power8. # check gcc before build, should be AT11.0. which gcc. # build. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CPPFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static. make -j20. make install. # set environmen",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:4355,deployability,instal,install,4355,"nfo. ```. ## Advance Toolchain 11.0. Doc: [https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/](https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/). ```bash. # gpg public key. wget ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. rpm --import gpg-pubkey-6976a827-5164221b. # Configure the Advance Toolchain repositories. [root@cit1074 deepvariant]# cat /etc/yum.repos.d/advance-toolchain.repo. #Begin of configuration file. [advance-toolchain]. name=Advance Toolchain Unicamp FTP. baseurl=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7. failovermethod=priority. enabled=1. gpgcheck=1. gpgkey=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. # End of configuration file. # Install the Advance Toolchain. yum install advance-toolchain-at11.0-runtime. yum install advance-toolchain-at11.0-devel. yum install advance-toolchain-at11.0-perf. yum install advance-toolchain-at11.0-mcore-libs. # Load enviroment. export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # Do not need to export. # export LD_LIBRARY_PATH=/opt/at11.0/lib64:/lib64:$LD_LIBRARY_PATH . # Load environment. sudo yum install environment-modules. source /etc/profile. module load at11.0. module unload at11.0. ```. ## Python 2 and Pip 19.0.1. Build instructions: [https://docs.python.org/2/using/unix.html](https://docs.python.org/2/using/unix.html). > Note: Python 2 should be built from AT 11.0. ```bash. # download source code . wget https://www.python.org/ftp/python/2.7.15/Python-2.7.15.tgz. tar -zxvf Python-2.7.15.tgz. cd Python-2.7.15. # environment. export HOMEPATH=/home/qilibj. export CPU=power8. # check gcc before build, should be AT11.0. which gcc. # build. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CPPFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static. make -j20. make install. # set environment. export PATH=$HOMEPATH/i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:4371,deployability,toolchain,toolchain-,4371,"nce Toolchain 11.0. Doc: [https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/](https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/). ```bash. # gpg public key. wget ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. rpm --import gpg-pubkey-6976a827-5164221b. # Configure the Advance Toolchain repositories. [root@cit1074 deepvariant]# cat /etc/yum.repos.d/advance-toolchain.repo. #Begin of configuration file. [advance-toolchain]. name=Advance Toolchain Unicamp FTP. baseurl=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7. failovermethod=priority. enabled=1. gpgcheck=1. gpgkey=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. # End of configuration file. # Install the Advance Toolchain. yum install advance-toolchain-at11.0-runtime. yum install advance-toolchain-at11.0-devel. yum install advance-toolchain-at11.0-perf. yum install advance-toolchain-at11.0-mcore-libs. # Load enviroment. export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # Do not need to export. # export LD_LIBRARY_PATH=/opt/at11.0/lib64:/lib64:$LD_LIBRARY_PATH . # Load environment. sudo yum install environment-modules. source /etc/profile. module load at11.0. module unload at11.0. ```. ## Python 2 and Pip 19.0.1. Build instructions: [https://docs.python.org/2/using/unix.html](https://docs.python.org/2/using/unix.html). > Note: Python 2 should be built from AT 11.0. ```bash. # download source code . wget https://www.python.org/ftp/python/2.7.15/Python-2.7.15.tgz. tar -zxvf Python-2.7.15.tgz. cd Python-2.7.15. # environment. export HOMEPATH=/home/qilibj. export CPU=power8. # check gcc before build, should be AT11.0. which gcc. # build. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CPPFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static. make -j20. make install. # set environment. export PATH=$HOMEPATH/inst/bin:/usr/loca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:4594,deployability,instal,install,4594,"://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. rpm --import gpg-pubkey-6976a827-5164221b. # Configure the Advance Toolchain repositories. [root@cit1074 deepvariant]# cat /etc/yum.repos.d/advance-toolchain.repo. #Begin of configuration file. [advance-toolchain]. name=Advance Toolchain Unicamp FTP. baseurl=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7. failovermethod=priority. enabled=1. gpgcheck=1. gpgkey=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. # End of configuration file. # Install the Advance Toolchain. yum install advance-toolchain-at11.0-runtime. yum install advance-toolchain-at11.0-devel. yum install advance-toolchain-at11.0-perf. yum install advance-toolchain-at11.0-mcore-libs. # Load enviroment. export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # Do not need to export. # export LD_LIBRARY_PATH=/opt/at11.0/lib64:/lib64:$LD_LIBRARY_PATH . # Load environment. sudo yum install environment-modules. source /etc/profile. module load at11.0. module unload at11.0. ```. ## Python 2 and Pip 19.0.1. Build instructions: [https://docs.python.org/2/using/unix.html](https://docs.python.org/2/using/unix.html). > Note: Python 2 should be built from AT 11.0. ```bash. # download source code . wget https://www.python.org/ftp/python/2.7.15/Python-2.7.15.tgz. tar -zxvf Python-2.7.15.tgz. cd Python-2.7.15. # environment. export HOMEPATH=/home/qilibj. export CPU=power8. # check gcc before build, should be AT11.0. which gcc. # build. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CPPFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static. make -j20. make install. # set environment. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH. export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. # verify 2.7.15. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:4614,deployability,modul,modules,4614,"b/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. rpm --import gpg-pubkey-6976a827-5164221b. # Configure the Advance Toolchain repositories. [root@cit1074 deepvariant]# cat /etc/yum.repos.d/advance-toolchain.repo. #Begin of configuration file. [advance-toolchain]. name=Advance Toolchain Unicamp FTP. baseurl=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7. failovermethod=priority. enabled=1. gpgcheck=1. gpgkey=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. # End of configuration file. # Install the Advance Toolchain. yum install advance-toolchain-at11.0-runtime. yum install advance-toolchain-at11.0-devel. yum install advance-toolchain-at11.0-perf. yum install advance-toolchain-at11.0-mcore-libs. # Load enviroment. export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # Do not need to export. # export LD_LIBRARY_PATH=/opt/at11.0/lib64:/lib64:$LD_LIBRARY_PATH . # Load environment. sudo yum install environment-modules. source /etc/profile. module load at11.0. module unload at11.0. ```. ## Python 2 and Pip 19.0.1. Build instructions: [https://docs.python.org/2/using/unix.html](https://docs.python.org/2/using/unix.html). > Note: Python 2 should be built from AT 11.0. ```bash. # download source code . wget https://www.python.org/ftp/python/2.7.15/Python-2.7.15.tgz. tar -zxvf Python-2.7.15.tgz. cd Python-2.7.15. # environment. export HOMEPATH=/home/qilibj. export CPU=power8. # check gcc before build, should be AT11.0. which gcc. # build. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CPPFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static. make -j20. make install. # set environment. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH. export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. # verify 2.7.15. echo ""$(python --ver",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:4644,deployability,modul,module,4644,"hat/RHEL7/gpg-pubkey-6976a827-5164221b. rpm --import gpg-pubkey-6976a827-5164221b. # Configure the Advance Toolchain repositories. [root@cit1074 deepvariant]# cat /etc/yum.repos.d/advance-toolchain.repo. #Begin of configuration file. [advance-toolchain]. name=Advance Toolchain Unicamp FTP. baseurl=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7. failovermethod=priority. enabled=1. gpgcheck=1. gpgkey=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. # End of configuration file. # Install the Advance Toolchain. yum install advance-toolchain-at11.0-runtime. yum install advance-toolchain-at11.0-devel. yum install advance-toolchain-at11.0-perf. yum install advance-toolchain-at11.0-mcore-libs. # Load enviroment. export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # Do not need to export. # export LD_LIBRARY_PATH=/opt/at11.0/lib64:/lib64:$LD_LIBRARY_PATH . # Load environment. sudo yum install environment-modules. source /etc/profile. module load at11.0. module unload at11.0. ```. ## Python 2 and Pip 19.0.1. Build instructions: [https://docs.python.org/2/using/unix.html](https://docs.python.org/2/using/unix.html). > Note: Python 2 should be built from AT 11.0. ```bash. # download source code . wget https://www.python.org/ftp/python/2.7.15/Python-2.7.15.tgz. tar -zxvf Python-2.7.15.tgz. cd Python-2.7.15. # environment. export HOMEPATH=/home/qilibj. export CPU=power8. # check gcc before build, should be AT11.0. which gcc. # build. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CPPFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static. make -j20. make install. # set environment. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH. export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. # verify 2.7.15. echo ""$(python --version)"". # Pip 19.0.2. wget -q",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:4664,deployability,modul,module,4664,"-6976a827-5164221b. rpm --import gpg-pubkey-6976a827-5164221b. # Configure the Advance Toolchain repositories. [root@cit1074 deepvariant]# cat /etc/yum.repos.d/advance-toolchain.repo. #Begin of configuration file. [advance-toolchain]. name=Advance Toolchain Unicamp FTP. baseurl=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7. failovermethod=priority. enabled=1. gpgcheck=1. gpgkey=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. # End of configuration file. # Install the Advance Toolchain. yum install advance-toolchain-at11.0-runtime. yum install advance-toolchain-at11.0-devel. yum install advance-toolchain-at11.0-perf. yum install advance-toolchain-at11.0-mcore-libs. # Load enviroment. export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # Do not need to export. # export LD_LIBRARY_PATH=/opt/at11.0/lib64:/lib64:$LD_LIBRARY_PATH . # Load environment. sudo yum install environment-modules. source /etc/profile. module load at11.0. module unload at11.0. ```. ## Python 2 and Pip 19.0.1. Build instructions: [https://docs.python.org/2/using/unix.html](https://docs.python.org/2/using/unix.html). > Note: Python 2 should be built from AT 11.0. ```bash. # download source code . wget https://www.python.org/ftp/python/2.7.15/Python-2.7.15.tgz. tar -zxvf Python-2.7.15.tgz. cd Python-2.7.15. # environment. export HOMEPATH=/home/qilibj. export CPU=power8. # check gcc before build, should be AT11.0. which gcc. # build. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CPPFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static. make -j20. make install. # set environment. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH. export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. # verify 2.7.15. echo ""$(python --version)"". # Pip 19.0.2. wget -qc https://bootstrap.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:4671,deployability,unload,unload,4671,"27-5164221b. rpm --import gpg-pubkey-6976a827-5164221b. # Configure the Advance Toolchain repositories. [root@cit1074 deepvariant]# cat /etc/yum.repos.d/advance-toolchain.repo. #Begin of configuration file. [advance-toolchain]. name=Advance Toolchain Unicamp FTP. baseurl=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7. failovermethod=priority. enabled=1. gpgcheck=1. gpgkey=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. # End of configuration file. # Install the Advance Toolchain. yum install advance-toolchain-at11.0-runtime. yum install advance-toolchain-at11.0-devel. yum install advance-toolchain-at11.0-perf. yum install advance-toolchain-at11.0-mcore-libs. # Load enviroment. export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # Do not need to export. # export LD_LIBRARY_PATH=/opt/at11.0/lib64:/lib64:$LD_LIBRARY_PATH . # Load environment. sudo yum install environment-modules. source /etc/profile. module load at11.0. module unload at11.0. ```. ## Python 2 and Pip 19.0.1. Build instructions: [https://docs.python.org/2/using/unix.html](https://docs.python.org/2/using/unix.html). > Note: Python 2 should be built from AT 11.0. ```bash. # download source code . wget https://www.python.org/ftp/python/2.7.15/Python-2.7.15.tgz. tar -zxvf Python-2.7.15.tgz. cd Python-2.7.15. # environment. export HOMEPATH=/home/qilibj. export CPU=power8. # check gcc before build, should be AT11.0. which gcc. # build. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CPPFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static. make -j20. make install. # set environment. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH. export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. # verify 2.7.15. echo ""$(python --version)"". # Pip 19.0.2. wget -qc https://bootstrap.pypa.io",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:4719,deployability,Build,Build,4719,"64221b. # Configure the Advance Toolchain repositories. [root@cit1074 deepvariant]# cat /etc/yum.repos.d/advance-toolchain.repo. #Begin of configuration file. [advance-toolchain]. name=Advance Toolchain Unicamp FTP. baseurl=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7. failovermethod=priority. enabled=1. gpgcheck=1. gpgkey=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. # End of configuration file. # Install the Advance Toolchain. yum install advance-toolchain-at11.0-runtime. yum install advance-toolchain-at11.0-devel. yum install advance-toolchain-at11.0-perf. yum install advance-toolchain-at11.0-mcore-libs. # Load enviroment. export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # Do not need to export. # export LD_LIBRARY_PATH=/opt/at11.0/lib64:/lib64:$LD_LIBRARY_PATH . # Load environment. sudo yum install environment-modules. source /etc/profile. module load at11.0. module unload at11.0. ```. ## Python 2 and Pip 19.0.1. Build instructions: [https://docs.python.org/2/using/unix.html](https://docs.python.org/2/using/unix.html). > Note: Python 2 should be built from AT 11.0. ```bash. # download source code . wget https://www.python.org/ftp/python/2.7.15/Python-2.7.15.tgz. tar -zxvf Python-2.7.15.tgz. cd Python-2.7.15. # environment. export HOMEPATH=/home/qilibj. export CPU=power8. # check gcc before build, should be AT11.0. which gcc. # build. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CPPFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static. make -j20. make install. # set environment. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH. export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. # verify 2.7.15. echo ""$(python --version)"". # Pip 19.0.2. wget -qc https://bootstrap.pypa.io/get-pip.py --no-check-certificate. $HOMEPATH/in",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:5103,deployability,build,build,5103,"ain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. # End of configuration file. # Install the Advance Toolchain. yum install advance-toolchain-at11.0-runtime. yum install advance-toolchain-at11.0-devel. yum install advance-toolchain-at11.0-perf. yum install advance-toolchain-at11.0-mcore-libs. # Load enviroment. export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # Do not need to export. # export LD_LIBRARY_PATH=/opt/at11.0/lib64:/lib64:$LD_LIBRARY_PATH . # Load environment. sudo yum install environment-modules. source /etc/profile. module load at11.0. module unload at11.0. ```. ## Python 2 and Pip 19.0.1. Build instructions: [https://docs.python.org/2/using/unix.html](https://docs.python.org/2/using/unix.html). > Note: Python 2 should be built from AT 11.0. ```bash. # download source code . wget https://www.python.org/ftp/python/2.7.15/Python-2.7.15.tgz. tar -zxvf Python-2.7.15.tgz. cd Python-2.7.15. # environment. export HOMEPATH=/home/qilibj. export CPU=power8. # check gcc before build, should be AT11.0. which gcc. # build. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CPPFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static. make -j20. make install. # set environment. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH. export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. # verify 2.7.15. echo ""$(python --version)"". # Pip 19.0.2. wget -qc https://bootstrap.pypa.io/get-pip.py --no-check-certificate. $HOMEPATH/inst/bin/python ./get-pip.py --prefix $HOMEPATH/inst. #pip install --upgrade --force-reinstall pip. echo ""$(pip --version)"". pip install setuptools nose asv cython future protobuf==3.6.1 six mock. pip install --upgrade setuptools. ```. ## Protobuf 3.6.1 C++ shared. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:5141,deployability,build,build,5141,"7-5164221b. # End of configuration file. # Install the Advance Toolchain. yum install advance-toolchain-at11.0-runtime. yum install advance-toolchain-at11.0-devel. yum install advance-toolchain-at11.0-perf. yum install advance-toolchain-at11.0-mcore-libs. # Load enviroment. export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # Do not need to export. # export LD_LIBRARY_PATH=/opt/at11.0/lib64:/lib64:$LD_LIBRARY_PATH . # Load environment. sudo yum install environment-modules. source /etc/profile. module load at11.0. module unload at11.0. ```. ## Python 2 and Pip 19.0.1. Build instructions: [https://docs.python.org/2/using/unix.html](https://docs.python.org/2/using/unix.html). > Note: Python 2 should be built from AT 11.0. ```bash. # download source code . wget https://www.python.org/ftp/python/2.7.15/Python-2.7.15.tgz. tar -zxvf Python-2.7.15.tgz. cd Python-2.7.15. # environment. export HOMEPATH=/home/qilibj. export CPU=power8. # check gcc before build, should be AT11.0. which gcc. # build. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CPPFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static. make -j20. make install. # set environment. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH. export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. # verify 2.7.15. echo ""$(python --version)"". # Pip 19.0.2. wget -qc https://bootstrap.pypa.io/get-pip.py --no-check-certificate. $HOMEPATH/inst/bin/python ./get-pip.py --prefix $HOMEPATH/inst. #pip install --upgrade --force-reinstall pip. echo ""$(pip --version)"". pip install setuptools nose asv cython future protobuf==3.6.1 six mock. pip install --upgrade setuptools. ```. ## Protobuf 3.6.1 C++ shared. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: P",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:5308,deployability,instal,install,5308,"install advance-toolchain-at11.0-perf. yum install advance-toolchain-at11.0-mcore-libs. # Load enviroment. export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # Do not need to export. # export LD_LIBRARY_PATH=/opt/at11.0/lib64:/lib64:$LD_LIBRARY_PATH . # Load environment. sudo yum install environment-modules. source /etc/profile. module load at11.0. module unload at11.0. ```. ## Python 2 and Pip 19.0.1. Build instructions: [https://docs.python.org/2/using/unix.html](https://docs.python.org/2/using/unix.html). > Note: Python 2 should be built from AT 11.0. ```bash. # download source code . wget https://www.python.org/ftp/python/2.7.15/Python-2.7.15.tgz. tar -zxvf Python-2.7.15.tgz. cd Python-2.7.15. # environment. export HOMEPATH=/home/qilibj. export CPU=power8. # check gcc before build, should be AT11.0. which gcc. # build. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CPPFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static. make -j20. make install. # set environment. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH. export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. # verify 2.7.15. echo ""$(python --version)"". # Pip 19.0.2. wget -qc https://bootstrap.pypa.io/get-pip.py --no-check-certificate. $HOMEPATH/inst/bin/python ./get-pip.py --prefix $HOMEPATH/inst. #pip install --upgrade --force-reinstall pip. echo ""$(pip --version)"". pip install setuptools nose asv cython future protobuf==3.6.1 six mock. pip install --upgrade setuptools. ```. ## Protobuf 3.6.1 C++ shared. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from AT 11.0, C++ shared for TF 1.12. ```bash. # download source code. wget https://github.com/protocolbuffers/protobuf/releases/download/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:5615,deployability,version,version,5615,"es. source /etc/profile. module load at11.0. module unload at11.0. ```. ## Python 2 and Pip 19.0.1. Build instructions: [https://docs.python.org/2/using/unix.html](https://docs.python.org/2/using/unix.html). > Note: Python 2 should be built from AT 11.0. ```bash. # download source code . wget https://www.python.org/ftp/python/2.7.15/Python-2.7.15.tgz. tar -zxvf Python-2.7.15.tgz. cd Python-2.7.15. # environment. export HOMEPATH=/home/qilibj. export CPU=power8. # check gcc before build, should be AT11.0. which gcc. # build. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CPPFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static. make -j20. make install. # set environment. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH. export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. # verify 2.7.15. echo ""$(python --version)"". # Pip 19.0.2. wget -qc https://bootstrap.pypa.io/get-pip.py --no-check-certificate. $HOMEPATH/inst/bin/python ./get-pip.py --prefix $HOMEPATH/inst. #pip install --upgrade --force-reinstall pip. echo ""$(pip --version)"". pip install setuptools nose asv cython future protobuf==3.6.1 six mock. pip install --upgrade setuptools. ```. ## Protobuf 3.6.1 C++ shared. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from AT 11.0, C++ shared for TF 1.12. ```bash. # download source code. wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:5779,deployability,instal,install,5779,"https://docs.python.org/2/using/unix.html). > Note: Python 2 should be built from AT 11.0. ```bash. # download source code . wget https://www.python.org/ftp/python/2.7.15/Python-2.7.15.tgz. tar -zxvf Python-2.7.15.tgz. cd Python-2.7.15. # environment. export HOMEPATH=/home/qilibj. export CPU=power8. # check gcc before build, should be AT11.0. which gcc. # build. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CPPFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static. make -j20. make install. # set environment. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH. export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. # verify 2.7.15. echo ""$(python --version)"". # Pip 19.0.2. wget -qc https://bootstrap.pypa.io/get-pip.py --no-check-certificate. $HOMEPATH/inst/bin/python ./get-pip.py --prefix $HOMEPATH/inst. #pip install --upgrade --force-reinstall pip. echo ""$(pip --version)"". pip install setuptools nose asv cython future protobuf==3.6.1 six mock. pip install --upgrade setuptools. ```. ## Protobuf 3.6.1 C++ shared. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from AT 11.0, C++ shared for TF 1.12. ```bash. # download source code. wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static. make clean. make -j20. # optional. make -j20 check # check if protobuf build is good. # install. make inst",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:5789,deployability,upgrad,upgrade,5789,"cs.python.org/2/using/unix.html). > Note: Python 2 should be built from AT 11.0. ```bash. # download source code . wget https://www.python.org/ftp/python/2.7.15/Python-2.7.15.tgz. tar -zxvf Python-2.7.15.tgz. cd Python-2.7.15. # environment. export HOMEPATH=/home/qilibj. export CPU=power8. # check gcc before build, should be AT11.0. which gcc. # build. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CPPFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static. make -j20. make install. # set environment. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH. export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. # verify 2.7.15. echo ""$(python --version)"". # Pip 19.0.2. wget -qc https://bootstrap.pypa.io/get-pip.py --no-check-certificate. $HOMEPATH/inst/bin/python ./get-pip.py --prefix $HOMEPATH/inst. #pip install --upgrade --force-reinstall pip. echo ""$(pip --version)"". pip install setuptools nose asv cython future protobuf==3.6.1 six mock. pip install --upgrade setuptools. ```. ## Protobuf 3.6.1 C++ shared. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from AT 11.0, C++ shared for TF 1.12. ```bash. # download source code. wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static. make clean. make -j20. # optional. make -j20 check # check if protobuf build is good. # install. make install. sudo ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:5834,deployability,version,version,5834,"hon 2 should be built from AT 11.0. ```bash. # download source code . wget https://www.python.org/ftp/python/2.7.15/Python-2.7.15.tgz. tar -zxvf Python-2.7.15.tgz. cd Python-2.7.15. # environment. export HOMEPATH=/home/qilibj. export CPU=power8. # check gcc before build, should be AT11.0. which gcc. # build. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CPPFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static. make -j20. make install. # set environment. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH. export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. # verify 2.7.15. echo ""$(python --version)"". # Pip 19.0.2. wget -qc https://bootstrap.pypa.io/get-pip.py --no-check-certificate. $HOMEPATH/inst/bin/python ./get-pip.py --prefix $HOMEPATH/inst. #pip install --upgrade --force-reinstall pip. echo ""$(pip --version)"". pip install setuptools nose asv cython future protobuf==3.6.1 six mock. pip install --upgrade setuptools. ```. ## Protobuf 3.6.1 C++ shared. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from AT 11.0, C++ shared for TF 1.12. ```bash. # download source code. wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static. make clean. make -j20. # optional. make -j20 check # check if protobuf build is good. # install. make install. sudo ldconfig # refresh shared library cache - /op",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:5849,deployability,instal,install,5849," built from AT 11.0. ```bash. # download source code . wget https://www.python.org/ftp/python/2.7.15/Python-2.7.15.tgz. tar -zxvf Python-2.7.15.tgz. cd Python-2.7.15. # environment. export HOMEPATH=/home/qilibj. export CPU=power8. # check gcc before build, should be AT11.0. which gcc. # build. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CPPFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static. make -j20. make install. # set environment. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH. export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. # verify 2.7.15. echo ""$(python --version)"". # Pip 19.0.2. wget -qc https://bootstrap.pypa.io/get-pip.py --no-check-certificate. $HOMEPATH/inst/bin/python ./get-pip.py --prefix $HOMEPATH/inst. #pip install --upgrade --force-reinstall pip. echo ""$(pip --version)"". pip install setuptools nose asv cython future protobuf==3.6.1 six mock. pip install --upgrade setuptools. ```. ## Protobuf 3.6.1 C++ shared. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from AT 11.0, C++ shared for TF 1.12. ```bash. # download source code. wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static. make clean. make -j20. # optional. make -j20 check # check if protobuf build is good. # install. make install. sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/l",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:5921,deployability,instal,install,5921,"python.org/ftp/python/2.7.15/Python-2.7.15.tgz. tar -zxvf Python-2.7.15.tgz. cd Python-2.7.15. # environment. export HOMEPATH=/home/qilibj. export CPU=power8. # check gcc before build, should be AT11.0. which gcc. # build. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CPPFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static. make -j20. make install. # set environment. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH. export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. # verify 2.7.15. echo ""$(python --version)"". # Pip 19.0.2. wget -qc https://bootstrap.pypa.io/get-pip.py --no-check-certificate. $HOMEPATH/inst/bin/python ./get-pip.py --prefix $HOMEPATH/inst. #pip install --upgrade --force-reinstall pip. echo ""$(pip --version)"". pip install setuptools nose asv cython future protobuf==3.6.1 six mock. pip install --upgrade setuptools. ```. ## Protobuf 3.6.1 C++ shared. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from AT 11.0, C++ shared for TF 1.12. ```bash. # download source code. wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static. make clean. make -j20. # optional. make -j20 check # check if protobuf build is good. # install. make install. sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig. #cd .. # verify. echo ""$(which protoc)"". echo ""$(protoc --versi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:5931,deployability,upgrad,upgrade,5931,"/ftp/python/2.7.15/Python-2.7.15.tgz. tar -zxvf Python-2.7.15.tgz. cd Python-2.7.15. # environment. export HOMEPATH=/home/qilibj. export CPU=power8. # check gcc before build, should be AT11.0. which gcc. # build. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CPPFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static. make -j20. make install. # set environment. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH. export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. # verify 2.7.15. echo ""$(python --version)"". # Pip 19.0.2. wget -qc https://bootstrap.pypa.io/get-pip.py --no-check-certificate. $HOMEPATH/inst/bin/python ./get-pip.py --prefix $HOMEPATH/inst. #pip install --upgrade --force-reinstall pip. echo ""$(pip --version)"". pip install setuptools nose asv cython future protobuf==3.6.1 six mock. pip install --upgrade setuptools. ```. ## Protobuf 3.6.1 C++ shared. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from AT 11.0, C++ shared for TF 1.12. ```bash. # download source code. wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static. make clean. make -j20. # optional. make -j20 check # check if protobuf build is good. # install. make install. sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig. #cd .. # verify. echo ""$(which protoc)"". echo ""$(protoc --version)"". ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:6294,deployability,releas,releases,6294,"j20. make install. # set environment. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH. export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. # verify 2.7.15. echo ""$(python --version)"". # Pip 19.0.2. wget -qc https://bootstrap.pypa.io/get-pip.py --no-check-certificate. $HOMEPATH/inst/bin/python ./get-pip.py --prefix $HOMEPATH/inst. #pip install --upgrade --force-reinstall pip. echo ""$(pip --version)"". pip install setuptools nose asv cython future protobuf==3.6.1 six mock. pip install --upgrade setuptools. ```. ## Protobuf 3.6.1 C++ shared. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from AT 11.0, C++ shared for TF 1.12. ```bash. # download source code. wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static. make clean. make -j20. # optional. make -j20 check # check if protobuf build is good. # install. make install. sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig. #cd .. # verify. echo ""$(which protoc)"". echo ""$(protoc --version)"". ```. ## Protobuf 3.6.1 Python from source code. Python: [https://github.com/protocolbuffers/protobuf/blob/master/python/README.md](https://github.com/protocolbuffers/protobuf/blob/master/python/README.md). > Note: Protobuf 3.6.1 should be built from source code or CLIF cannot find protobuf. ```bash. # share build for Python. python --version # python 2.7 or newer. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:6427,deployability,build,build,6427,"in:/usr/bin:/root/bin. export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH. export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. # verify 2.7.15. echo ""$(python --version)"". # Pip 19.0.2. wget -qc https://bootstrap.pypa.io/get-pip.py --no-check-certificate. $HOMEPATH/inst/bin/python ./get-pip.py --prefix $HOMEPATH/inst. #pip install --upgrade --force-reinstall pip. echo ""$(pip --version)"". pip install setuptools nose asv cython future protobuf==3.6.1 six mock. pip install --upgrade setuptools. ```. ## Protobuf 3.6.1 C++ shared. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from AT 11.0, C++ shared for TF 1.12. ```bash. # download source code. wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static. make clean. make -j20. # optional. make -j20 check # check if protobuf build is good. # install. make install. sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig. #cd .. # verify. echo ""$(which protoc)"". echo ""$(protoc --version)"". ```. ## Protobuf 3.6.1 Python from source code. Python: [https://github.com/protocolbuffers/protobuf/blob/master/python/README.md](https://github.com/protocolbuffers/protobuf/blob/master/python/README.md). > Note: Protobuf 3.6.1 should be built from source code or CLIF cannot find protobuf. ```bash. # share build for Python. python --version # python 2.7 or newer. protoc --version. # build. cd protobuf-3.6.1/python/. python setup.py build. python setup.py test. # install from source as deepvari",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:6474,deployability,build,build,6474,"home/qilibj/inst/lib:$LD_LIBRARY_PATH. export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. # verify 2.7.15. echo ""$(python --version)"". # Pip 19.0.2. wget -qc https://bootstrap.pypa.io/get-pip.py --no-check-certificate. $HOMEPATH/inst/bin/python ./get-pip.py --prefix $HOMEPATH/inst. #pip install --upgrade --force-reinstall pip. echo ""$(pip --version)"". pip install setuptools nose asv cython future protobuf==3.6.1 six mock. pip install --upgrade setuptools. ```. ## Protobuf 3.6.1 C++ shared. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from AT 11.0, C++ shared for TF 1.12. ```bash. # download source code. wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static. make clean. make -j20. # optional. make -j20 check # check if protobuf build is good. # install. make install. sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig. #cd .. # verify. echo ""$(which protoc)"". echo ""$(protoc --version)"". ```. ## Protobuf 3.6.1 Python from source code. Python: [https://github.com/protocolbuffers/protobuf/blob/master/python/README.md](https://github.com/protocolbuffers/protobuf/blob/master/python/README.md). > Note: Protobuf 3.6.1 should be built from source code or CLIF cannot find protobuf. ```bash. # share build for Python. python --version # python 2.7 or newer. protoc --version. # build. cd protobuf-3.6.1/python/. python setup.py build. python setup.py test. # install from source as deepvariant needed. python setup.py install. # install ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:6491,deployability,instal,install,6491,"ib:$LD_LIBRARY_PATH. export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. # verify 2.7.15. echo ""$(python --version)"". # Pip 19.0.2. wget -qc https://bootstrap.pypa.io/get-pip.py --no-check-certificate. $HOMEPATH/inst/bin/python ./get-pip.py --prefix $HOMEPATH/inst. #pip install --upgrade --force-reinstall pip. echo ""$(pip --version)"". pip install setuptools nose asv cython future protobuf==3.6.1 six mock. pip install --upgrade setuptools. ```. ## Protobuf 3.6.1 C++ shared. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from AT 11.0, C++ shared for TF 1.12. ```bash. # download source code. wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static. make clean. make -j20. # optional. make -j20 check # check if protobuf build is good. # install. make install. sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig. #cd .. # verify. echo ""$(which protoc)"". echo ""$(protoc --version)"". ```. ## Protobuf 3.6.1 Python from source code. Python: [https://github.com/protocolbuffers/protobuf/blob/master/python/README.md](https://github.com/protocolbuffers/protobuf/blob/master/python/README.md). > Note: Protobuf 3.6.1 should be built from source code or CLIF cannot find protobuf. ```bash. # share build for Python. python --version # python 2.7 or newer. protoc --version. # build. cd protobuf-3.6.1/python/. python setup.py build. python setup.py test. # install from source as deepvariant needed. python setup.py install. # install from wheel. python",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:6748,deployability,build,build,6748,"prefix $HOMEPATH/inst. #pip install --upgrade --force-reinstall pip. echo ""$(pip --version)"". pip install setuptools nose asv cython future protobuf==3.6.1 six mock. pip install --upgrade setuptools. ```. ## Protobuf 3.6.1 C++ shared. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from AT 11.0, C++ shared for TF 1.12. ```bash. # download source code. wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static. make clean. make -j20. # optional. make -j20 check # check if protobuf build is good. # install. make install. sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig. #cd .. # verify. echo ""$(which protoc)"". echo ""$(protoc --version)"". ```. ## Protobuf 3.6.1 Python from source code. Python: [https://github.com/protocolbuffers/protobuf/blob/master/python/README.md](https://github.com/protocolbuffers/protobuf/blob/master/python/README.md). > Note: Protobuf 3.6.1 should be built from source code or CLIF cannot find protobuf. ```bash. # share build for Python. python --version # python 2.7 or newer. protoc --version. # build. cd protobuf-3.6.1/python/. python setup.py build. python setup.py test. # install from source as deepvariant needed. python setup.py install. # install from wheel. python setup.py bdist_wheel. pip install protobuf-3.6.1-py2.py3-none-any.whl --force-reinstall. # verify. python -c ""import google.protobuf"". ```. ## OpenBLAS 0.3.5. ```bash. git clone -b v0.3.5 https://github.com/xianyi/OpenBLAS.git OpenBLAS-0.3.5. cd OpenBLAS-",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:6765,deployability,instal,install,6765,"nst. #pip install --upgrade --force-reinstall pip. echo ""$(pip --version)"". pip install setuptools nose asv cython future protobuf==3.6.1 six mock. pip install --upgrade setuptools. ```. ## Protobuf 3.6.1 C++ shared. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from AT 11.0, C++ shared for TF 1.12. ```bash. # download source code. wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static. make clean. make -j20. # optional. make -j20 check # check if protobuf build is good. # install. make install. sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig. #cd .. # verify. echo ""$(which protoc)"". echo ""$(protoc --version)"". ```. ## Protobuf 3.6.1 Python from source code. Python: [https://github.com/protocolbuffers/protobuf/blob/master/python/README.md](https://github.com/protocolbuffers/protobuf/blob/master/python/README.md). > Note: Protobuf 3.6.1 should be built from source code or CLIF cannot find protobuf. ```bash. # share build for Python. python --version # python 2.7 or newer. protoc --version. # build. cd protobuf-3.6.1/python/. python setup.py build. python setup.py test. # install from source as deepvariant needed. python setup.py install. # install from wheel. python setup.py bdist_wheel. pip install protobuf-3.6.1-py2.py3-none-any.whl --force-reinstall. # verify. python -c ""import google.protobuf"". ```. ## OpenBLAS 0.3.5. ```bash. git clone -b v0.3.5 https://github.com/xianyi/OpenBLAS.git OpenBLAS-0.3.5. cd OpenBLAS-0.3.5. make TARGET",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:6779,deployability,instal,install,6779,"all --upgrade --force-reinstall pip. echo ""$(pip --version)"". pip install setuptools nose asv cython future protobuf==3.6.1 six mock. pip install --upgrade setuptools. ```. ## Protobuf 3.6.1 C++ shared. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from AT 11.0, C++ shared for TF 1.12. ```bash. # download source code. wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static. make clean. make -j20. # optional. make -j20 check # check if protobuf build is good. # install. make install. sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig. #cd .. # verify. echo ""$(which protoc)"". echo ""$(protoc --version)"". ```. ## Protobuf 3.6.1 Python from source code. Python: [https://github.com/protocolbuffers/protobuf/blob/master/python/README.md](https://github.com/protocolbuffers/protobuf/blob/master/python/README.md). > Note: Protobuf 3.6.1 should be built from source code or CLIF cannot find protobuf. ```bash. # share build for Python. python --version # python 2.7 or newer. protoc --version. # build. cd protobuf-3.6.1/python/. python setup.py build. python setup.py test. # install from source as deepvariant needed. python setup.py install. # install from wheel. python setup.py bdist_wheel. pip install protobuf-3.6.1-py2.py3-none-any.whl --force-reinstall. # verify. python -c ""import google.protobuf"". ```. ## OpenBLAS 0.3.5. ```bash. git clone -b v0.3.5 https://github.com/xianyi/OpenBLAS.git OpenBLAS-0.3.5. cd OpenBLAS-0.3.5. make TARGET=power8. make ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:6920,deployability,version,version,6920,"tall --upgrade setuptools. ```. ## Protobuf 3.6.1 C++ shared. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from AT 11.0, C++ shared for TF 1.12. ```bash. # download source code. wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static. make clean. make -j20. # optional. make -j20 check # check if protobuf build is good. # install. make install. sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig. #cd .. # verify. echo ""$(which protoc)"". echo ""$(protoc --version)"". ```. ## Protobuf 3.6.1 Python from source code. Python: [https://github.com/protocolbuffers/protobuf/blob/master/python/README.md](https://github.com/protocolbuffers/protobuf/blob/master/python/README.md). > Note: Protobuf 3.6.1 should be built from source code or CLIF cannot find protobuf. ```bash. # share build for Python. python --version # python 2.7 or newer. protoc --version. # build. cd protobuf-3.6.1/python/. python setup.py build. python setup.py test. # install from source as deepvariant needed. python setup.py install. # install from wheel. python setup.py bdist_wheel. pip install protobuf-3.6.1-py2.py3-none-any.whl --force-reinstall. # verify. python -c ""import google.protobuf"". ```. ## OpenBLAS 0.3.5. ```bash. git clone -b v0.3.5 https://github.com/xianyi/OpenBLAS.git OpenBLAS-0.3.5. cd OpenBLAS-0.3.5. make TARGET=power8. make TARGET=power8 PREFIX=$HOMEPATH/inst install. ```. ## Boost 1.66.0. ```bash. wget -qc https://dl.bintray.com/boostorg/release/1.66.0/source/bo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:7240,deployability,build,build,7240,". wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static. make clean. make -j20. # optional. make -j20 check # check if protobuf build is good. # install. make install. sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig. #cd .. # verify. echo ""$(which protoc)"". echo ""$(protoc --version)"". ```. ## Protobuf 3.6.1 Python from source code. Python: [https://github.com/protocolbuffers/protobuf/blob/master/python/README.md](https://github.com/protocolbuffers/protobuf/blob/master/python/README.md). > Note: Protobuf 3.6.1 should be built from source code or CLIF cannot find protobuf. ```bash. # share build for Python. python --version # python 2.7 or newer. protoc --version. # build. cd protobuf-3.6.1/python/. python setup.py build. python setup.py test. # install from source as deepvariant needed. python setup.py install. # install from wheel. python setup.py bdist_wheel. pip install protobuf-3.6.1-py2.py3-none-any.whl --force-reinstall. # verify. python -c ""import google.protobuf"". ```. ## OpenBLAS 0.3.5. ```bash. git clone -b v0.3.5 https://github.com/xianyi/OpenBLAS.git OpenBLAS-0.3.5. cd OpenBLAS-0.3.5. make TARGET=power8. make TARGET=power8 PREFIX=$HOMEPATH/inst install. ```. ## Boost 1.66.0. ```bash. wget -qc https://dl.bintray.com/boostorg/release/1.66.0/source/boost_1_66_0.tar.gz. tar xzf boost_1_66_0.tar.gz. cd boost_1_66_0. ./bootstrap.sh --with-toolset=gcc --prefix=$HOMEPATH/inst. ./b2 dll-path=""$HOMEPATH/inst/lib"" install. ```. ## TensorFlow 1.12.0. > Note: Tensorflow 1.12.0 can be built with AT 11.0. Build instructions: [https://www.tensorflow.org/install/source](https:/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:7267,deployability,version,version,7267,"otocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static. make clean. make -j20. # optional. make -j20 check # check if protobuf build is good. # install. make install. sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig. #cd .. # verify. echo ""$(which protoc)"". echo ""$(protoc --version)"". ```. ## Protobuf 3.6.1 Python from source code. Python: [https://github.com/protocolbuffers/protobuf/blob/master/python/README.md](https://github.com/protocolbuffers/protobuf/blob/master/python/README.md). > Note: Protobuf 3.6.1 should be built from source code or CLIF cannot find protobuf. ```bash. # share build for Python. python --version # python 2.7 or newer. protoc --version. # build. cd protobuf-3.6.1/python/. python setup.py build. python setup.py test. # install from source as deepvariant needed. python setup.py install. # install from wheel. python setup.py bdist_wheel. pip install protobuf-3.6.1-py2.py3-none-any.whl --force-reinstall. # verify. python -c ""import google.protobuf"". ```. ## OpenBLAS 0.3.5. ```bash. git clone -b v0.3.5 https://github.com/xianyi/OpenBLAS.git OpenBLAS-0.3.5. cd OpenBLAS-0.3.5. make TARGET=power8. make TARGET=power8 PREFIX=$HOMEPATH/inst install. ```. ## Boost 1.66.0. ```bash. wget -qc https://dl.bintray.com/boostorg/release/1.66.0/source/boost_1_66_0.tar.gz. tar xzf boost_1_66_0.tar.gz. cd boost_1_66_0. ./bootstrap.sh --with-toolset=gcc --prefix=$HOMEPATH/inst. ./b2 dll-path=""$HOMEPATH/inst/lib"" install. ```. ## TensorFlow 1.12.0. > Note: Tensorflow 1.12.0 can be built with AT 11.0. Build instructions: [https://www.tensorflow.org/install/source](https://www.tensorflow.org/install/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:7307,deployability,version,version,7307,"/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static. make clean. make -j20. # optional. make -j20 check # check if protobuf build is good. # install. make install. sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig. #cd .. # verify. echo ""$(which protoc)"". echo ""$(protoc --version)"". ```. ## Protobuf 3.6.1 Python from source code. Python: [https://github.com/protocolbuffers/protobuf/blob/master/python/README.md](https://github.com/protocolbuffers/protobuf/blob/master/python/README.md). > Note: Protobuf 3.6.1 should be built from source code or CLIF cannot find protobuf. ```bash. # share build for Python. python --version # python 2.7 or newer. protoc --version. # build. cd protobuf-3.6.1/python/. python setup.py build. python setup.py test. # install from source as deepvariant needed. python setup.py install. # install from wheel. python setup.py bdist_wheel. pip install protobuf-3.6.1-py2.py3-none-any.whl --force-reinstall. # verify. python -c ""import google.protobuf"". ```. ## OpenBLAS 0.3.5. ```bash. git clone -b v0.3.5 https://github.com/xianyi/OpenBLAS.git OpenBLAS-0.3.5. cd OpenBLAS-0.3.5. make TARGET=power8. make TARGET=power8 PREFIX=$HOMEPATH/inst install. ```. ## Boost 1.66.0. ```bash. wget -qc https://dl.bintray.com/boostorg/release/1.66.0/source/boost_1_66_0.tar.gz. tar xzf boost_1_66_0.tar.gz. cd boost_1_66_0. ./bootstrap.sh --with-toolset=gcc --prefix=$HOMEPATH/inst. ./b2 dll-path=""$HOMEPATH/inst/lib"" install. ```. ## TensorFlow 1.12.0. > Note: Tensorflow 1.12.0 can be built with AT 11.0. Build instructions: [https://www.tensorflow.org/install/source](https://www.tensorflow.org/install/source). Edit instructions: [http://biop",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:7318,deployability,build,build,7318,"otobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static. make clean. make -j20. # optional. make -j20 check # check if protobuf build is good. # install. make install. sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig. #cd .. # verify. echo ""$(which protoc)"". echo ""$(protoc --version)"". ```. ## Protobuf 3.6.1 Python from source code. Python: [https://github.com/protocolbuffers/protobuf/blob/master/python/README.md](https://github.com/protocolbuffers/protobuf/blob/master/python/README.md). > Note: Protobuf 3.6.1 should be built from source code or CLIF cannot find protobuf. ```bash. # share build for Python. python --version # python 2.7 or newer. protoc --version. # build. cd protobuf-3.6.1/python/. python setup.py build. python setup.py test. # install from source as deepvariant needed. python setup.py install. # install from wheel. python setup.py bdist_wheel. pip install protobuf-3.6.1-py2.py3-none-any.whl --force-reinstall. # verify. python -c ""import google.protobuf"". ```. ## OpenBLAS 0.3.5. ```bash. git clone -b v0.3.5 https://github.com/xianyi/OpenBLAS.git OpenBLAS-0.3.5. cd OpenBLAS-0.3.5. make TARGET=power8. make TARGET=power8 PREFIX=$HOMEPATH/inst install. ```. ## Boost 1.66.0. ```bash. wget -qc https://dl.bintray.com/boostorg/release/1.66.0/source/boost_1_66_0.tar.gz. tar xzf boost_1_66_0.tar.gz. cd boost_1_66_0. ./bootstrap.sh --with-toolset=gcc --prefix=$HOMEPATH/inst. ./b2 dll-path=""$HOMEPATH/inst/lib"" install. ```. ## TensorFlow 1.12.0. > Note: Tensorflow 1.12.0 can be built with AT 11.0. Build instructions: [https://www.tensorflow.org/install/source](https://www.tensorflow.org/install/source). Edit instructions: [http://biophysics.med",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:7368,deployability,build,build,7368,"6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static. make clean. make -j20. # optional. make -j20 check # check if protobuf build is good. # install. make install. sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig. #cd .. # verify. echo ""$(which protoc)"". echo ""$(protoc --version)"". ```. ## Protobuf 3.6.1 Python from source code. Python: [https://github.com/protocolbuffers/protobuf/blob/master/python/README.md](https://github.com/protocolbuffers/protobuf/blob/master/python/README.md). > Note: Protobuf 3.6.1 should be built from source code or CLIF cannot find protobuf. ```bash. # share build for Python. python --version # python 2.7 or newer. protoc --version. # build. cd protobuf-3.6.1/python/. python setup.py build. python setup.py test. # install from source as deepvariant needed. python setup.py install. # install from wheel. python setup.py bdist_wheel. pip install protobuf-3.6.1-py2.py3-none-any.whl --force-reinstall. # verify. python -c ""import google.protobuf"". ```. ## OpenBLAS 0.3.5. ```bash. git clone -b v0.3.5 https://github.com/xianyi/OpenBLAS.git OpenBLAS-0.3.5. cd OpenBLAS-0.3.5. make TARGET=power8. make TARGET=power8 PREFIX=$HOMEPATH/inst install. ```. ## Boost 1.66.0. ```bash. wget -qc https://dl.bintray.com/boostorg/release/1.66.0/source/boost_1_66_0.tar.gz. tar xzf boost_1_66_0.tar.gz. cd boost_1_66_0. ./bootstrap.sh --with-toolset=gcc --prefix=$HOMEPATH/inst. ./b2 dll-path=""$HOMEPATH/inst/lib"" install. ```. ## TensorFlow 1.12.0. > Note: Tensorflow 1.12.0 can be built with AT 11.0. Build instructions: [https://www.tensorflow.org/install/source](https://www.tensorflow.org/install/source). Edit instructions: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophys",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:7399,deployability,instal,install,7399,"# clean static protobuf build. make uninstall. make distclean. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static. make clean. make -j20. # optional. make -j20 check # check if protobuf build is good. # install. make install. sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig. #cd .. # verify. echo ""$(which protoc)"". echo ""$(protoc --version)"". ```. ## Protobuf 3.6.1 Python from source code. Python: [https://github.com/protocolbuffers/protobuf/blob/master/python/README.md](https://github.com/protocolbuffers/protobuf/blob/master/python/README.md). > Note: Protobuf 3.6.1 should be built from source code or CLIF cannot find protobuf. ```bash. # share build for Python. python --version # python 2.7 or newer. protoc --version. # build. cd protobuf-3.6.1/python/. python setup.py build. python setup.py test. # install from source as deepvariant needed. python setup.py install. # install from wheel. python setup.py bdist_wheel. pip install protobuf-3.6.1-py2.py3-none-any.whl --force-reinstall. # verify. python -c ""import google.protobuf"". ```. ## OpenBLAS 0.3.5. ```bash. git clone -b v0.3.5 https://github.com/xianyi/OpenBLAS.git OpenBLAS-0.3.5. cd OpenBLAS-0.3.5. make TARGET=power8. make TARGET=power8 PREFIX=$HOMEPATH/inst install. ```. ## Boost 1.66.0. ```bash. wget -qc https://dl.bintray.com/boostorg/release/1.66.0/source/boost_1_66_0.tar.gz. tar xzf boost_1_66_0.tar.gz. cd boost_1_66_0. ./bootstrap.sh --with-toolset=gcc --prefix=$HOMEPATH/inst. ./b2 dll-path=""$HOMEPATH/inst/lib"" install. ```. ## TensorFlow 1.12.0. > Note: Tensorflow 1.12.0 can be built with AT 11.0. Build instructions: [https://www.tensorflow.org/install/source](https://www.tensorflow.org/install/source). Edit instructions: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensor",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:7458,deployability,instal,install,7458,"an. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static. make clean. make -j20. # optional. make -j20 check # check if protobuf build is good. # install. make install. sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig. #cd .. # verify. echo ""$(which protoc)"". echo ""$(protoc --version)"". ```. ## Protobuf 3.6.1 Python from source code. Python: [https://github.com/protocolbuffers/protobuf/blob/master/python/README.md](https://github.com/protocolbuffers/protobuf/blob/master/python/README.md). > Note: Protobuf 3.6.1 should be built from source code or CLIF cannot find protobuf. ```bash. # share build for Python. python --version # python 2.7 or newer. protoc --version. # build. cd protobuf-3.6.1/python/. python setup.py build. python setup.py test. # install from source as deepvariant needed. python setup.py install. # install from wheel. python setup.py bdist_wheel. pip install protobuf-3.6.1-py2.py3-none-any.whl --force-reinstall. # verify. python -c ""import google.protobuf"". ```. ## OpenBLAS 0.3.5. ```bash. git clone -b v0.3.5 https://github.com/xianyi/OpenBLAS.git OpenBLAS-0.3.5. cd OpenBLAS-0.3.5. make TARGET=power8. make TARGET=power8 PREFIX=$HOMEPATH/inst install. ```. ## Boost 1.66.0. ```bash. wget -qc https://dl.bintray.com/boostorg/release/1.66.0/source/boost_1_66_0.tar.gz. tar xzf boost_1_66_0.tar.gz. cd boost_1_66_0. ./bootstrap.sh --with-toolset=gcc --prefix=$HOMEPATH/inst. ./b2 dll-path=""$HOMEPATH/inst/lib"" install. ```. ## TensorFlow 1.12.0. > Note: Tensorflow 1.12.0 can be built with AT 11.0. Build instructions: [https://www.tensorflow.org/install/source](https://www.tensorflow.org/install/source). Edit instructions: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). Floatn.h error: [https://gist.github.com/nonchi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:7469,deployability,instal,install,7469," build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static. make clean. make -j20. # optional. make -j20 check # check if protobuf build is good. # install. make install. sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig. #cd .. # verify. echo ""$(which protoc)"". echo ""$(protoc --version)"". ```. ## Protobuf 3.6.1 Python from source code. Python: [https://github.com/protocolbuffers/protobuf/blob/master/python/README.md](https://github.com/protocolbuffers/protobuf/blob/master/python/README.md). > Note: Protobuf 3.6.1 should be built from source code or CLIF cannot find protobuf. ```bash. # share build for Python. python --version # python 2.7 or newer. protoc --version. # build. cd protobuf-3.6.1/python/. python setup.py build. python setup.py test. # install from source as deepvariant needed. python setup.py install. # install from wheel. python setup.py bdist_wheel. pip install protobuf-3.6.1-py2.py3-none-any.whl --force-reinstall. # verify. python -c ""import google.protobuf"". ```. ## OpenBLAS 0.3.5. ```bash. git clone -b v0.3.5 https://github.com/xianyi/OpenBLAS.git OpenBLAS-0.3.5. cd OpenBLAS-0.3.5. make TARGET=power8. make TARGET=power8 PREFIX=$HOMEPATH/inst install. ```. ## Boost 1.66.0. ```bash. wget -qc https://dl.bintray.com/boostorg/release/1.66.0/source/boost_1_66_0.tar.gz. tar xzf boost_1_66_0.tar.gz. cd boost_1_66_0. ./bootstrap.sh --with-toolset=gcc --prefix=$HOMEPATH/inst. ./b2 dll-path=""$HOMEPATH/inst/lib"" install. ```. ## TensorFlow 1.12.0. > Note: Tensorflow 1.12.0 can be built with AT 11.0. Build instructions: [https://www.tensorflow.org/install/source](https://www.tensorflow.org/install/source). Edit instructions: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). Floatn.h error: [https://gist.github.com/nonchip/2c93ff2d9",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:7522,deployability,instal,install,7522,"local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static. make clean. make -j20. # optional. make -j20 check # check if protobuf build is good. # install. make install. sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig. #cd .. # verify. echo ""$(which protoc)"". echo ""$(protoc --version)"". ```. ## Protobuf 3.6.1 Python from source code. Python: [https://github.com/protocolbuffers/protobuf/blob/master/python/README.md](https://github.com/protocolbuffers/protobuf/blob/master/python/README.md). > Note: Protobuf 3.6.1 should be built from source code or CLIF cannot find protobuf. ```bash. # share build for Python. python --version # python 2.7 or newer. protoc --version. # build. cd protobuf-3.6.1/python/. python setup.py build. python setup.py test. # install from source as deepvariant needed. python setup.py install. # install from wheel. python setup.py bdist_wheel. pip install protobuf-3.6.1-py2.py3-none-any.whl --force-reinstall. # verify. python -c ""import google.protobuf"". ```. ## OpenBLAS 0.3.5. ```bash. git clone -b v0.3.5 https://github.com/xianyi/OpenBLAS.git OpenBLAS-0.3.5. cd OpenBLAS-0.3.5. make TARGET=power8. make TARGET=power8 PREFIX=$HOMEPATH/inst install. ```. ## Boost 1.66.0. ```bash. wget -qc https://dl.bintray.com/boostorg/release/1.66.0/source/boost_1_66_0.tar.gz. tar xzf boost_1_66_0.tar.gz. cd boost_1_66_0. ./bootstrap.sh --with-toolset=gcc --prefix=$HOMEPATH/inst. ./b2 dll-path=""$HOMEPATH/inst/lib"" install. ```. ## TensorFlow 1.12.0. > Note: Tensorflow 1.12.0 can be built with AT 11.0. Build instructions: [https://www.tensorflow.org/install/source](https://www.tensorflow.org/install/source). Edit instructions: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). Floatn.h error: [https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f](https://gist.github.com/nonc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:7819,deployability,instal,install,7819,"ary cache - /opt/at11.0/sbin/ldconfig. #cd .. # verify. echo ""$(which protoc)"". echo ""$(protoc --version)"". ```. ## Protobuf 3.6.1 Python from source code. Python: [https://github.com/protocolbuffers/protobuf/blob/master/python/README.md](https://github.com/protocolbuffers/protobuf/blob/master/python/README.md). > Note: Protobuf 3.6.1 should be built from source code or CLIF cannot find protobuf. ```bash. # share build for Python. python --version # python 2.7 or newer. protoc --version. # build. cd protobuf-3.6.1/python/. python setup.py build. python setup.py test. # install from source as deepvariant needed. python setup.py install. # install from wheel. python setup.py bdist_wheel. pip install protobuf-3.6.1-py2.py3-none-any.whl --force-reinstall. # verify. python -c ""import google.protobuf"". ```. ## OpenBLAS 0.3.5. ```bash. git clone -b v0.3.5 https://github.com/xianyi/OpenBLAS.git OpenBLAS-0.3.5. cd OpenBLAS-0.3.5. make TARGET=power8. make TARGET=power8 PREFIX=$HOMEPATH/inst install. ```. ## Boost 1.66.0. ```bash. wget -qc https://dl.bintray.com/boostorg/release/1.66.0/source/boost_1_66_0.tar.gz. tar xzf boost_1_66_0.tar.gz. cd boost_1_66_0. ./bootstrap.sh --with-toolset=gcc --prefix=$HOMEPATH/inst. ./b2 dll-path=""$HOMEPATH/inst/lib"" install. ```. ## TensorFlow 1.12.0. > Note: Tensorflow 1.12.0 can be built with AT 11.0. Build instructions: [https://www.tensorflow.org/install/source](https://www.tensorflow.org/install/source). Edit instructions: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). Floatn.h error: [https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f](https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f). ```bash. # development packages. yum install python-devel python-pip -y. # dependency of numpy 1.14.6. OPT=""-D_XLOCALE_H=1 -O3 -mcpu=$CPU -mtune=$CPU -maltivec -mvsx -ffast-math -fpeel-loops -funroll-loops -ftree-vectorize -fno-builtin-malloc -fno-builtin-ca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:7900,deployability,releas,release,7900,"cho ""$(protoc --version)"". ```. ## Protobuf 3.6.1 Python from source code. Python: [https://github.com/protocolbuffers/protobuf/blob/master/python/README.md](https://github.com/protocolbuffers/protobuf/blob/master/python/README.md). > Note: Protobuf 3.6.1 should be built from source code or CLIF cannot find protobuf. ```bash. # share build for Python. python --version # python 2.7 or newer. protoc --version. # build. cd protobuf-3.6.1/python/. python setup.py build. python setup.py test. # install from source as deepvariant needed. python setup.py install. # install from wheel. python setup.py bdist_wheel. pip install protobuf-3.6.1-py2.py3-none-any.whl --force-reinstall. # verify. python -c ""import google.protobuf"". ```. ## OpenBLAS 0.3.5. ```bash. git clone -b v0.3.5 https://github.com/xianyi/OpenBLAS.git OpenBLAS-0.3.5. cd OpenBLAS-0.3.5. make TARGET=power8. make TARGET=power8 PREFIX=$HOMEPATH/inst install. ```. ## Boost 1.66.0. ```bash. wget -qc https://dl.bintray.com/boostorg/release/1.66.0/source/boost_1_66_0.tar.gz. tar xzf boost_1_66_0.tar.gz. cd boost_1_66_0. ./bootstrap.sh --with-toolset=gcc --prefix=$HOMEPATH/inst. ./b2 dll-path=""$HOMEPATH/inst/lib"" install. ```. ## TensorFlow 1.12.0. > Note: Tensorflow 1.12.0 can be built with AT 11.0. Build instructions: [https://www.tensorflow.org/install/source](https://www.tensorflow.org/install/source). Edit instructions: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). Floatn.h error: [https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f](https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f). ```bash. # development packages. yum install python-devel python-pip -y. # dependency of numpy 1.14.6. OPT=""-D_XLOCALE_H=1 -O3 -mcpu=$CPU -mtune=$CPU -maltivec -mvsx -ffast-math -fpeel-loops -funroll-loops -ftree-vectorize -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free"" BLAS=$HOMEPATH/inst/lib/libopenblas.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:8083,deployability,instal,install,8083,"olbuffers/protobuf/blob/master/python/README.md). > Note: Protobuf 3.6.1 should be built from source code or CLIF cannot find protobuf. ```bash. # share build for Python. python --version # python 2.7 or newer. protoc --version. # build. cd protobuf-3.6.1/python/. python setup.py build. python setup.py test. # install from source as deepvariant needed. python setup.py install. # install from wheel. python setup.py bdist_wheel. pip install protobuf-3.6.1-py2.py3-none-any.whl --force-reinstall. # verify. python -c ""import google.protobuf"". ```. ## OpenBLAS 0.3.5. ```bash. git clone -b v0.3.5 https://github.com/xianyi/OpenBLAS.git OpenBLAS-0.3.5. cd OpenBLAS-0.3.5. make TARGET=power8. make TARGET=power8 PREFIX=$HOMEPATH/inst install. ```. ## Boost 1.66.0. ```bash. wget -qc https://dl.bintray.com/boostorg/release/1.66.0/source/boost_1_66_0.tar.gz. tar xzf boost_1_66_0.tar.gz. cd boost_1_66_0. ./bootstrap.sh --with-toolset=gcc --prefix=$HOMEPATH/inst. ./b2 dll-path=""$HOMEPATH/inst/lib"" install. ```. ## TensorFlow 1.12.0. > Note: Tensorflow 1.12.0 can be built with AT 11.0. Build instructions: [https://www.tensorflow.org/install/source](https://www.tensorflow.org/install/source). Edit instructions: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). Floatn.h error: [https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f](https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f). ```bash. # development packages. yum install python-devel python-pip -y. # dependency of numpy 1.14.6. OPT=""-D_XLOCALE_H=1 -O3 -mcpu=$CPU -mtune=$CPU -maltivec -mvsx -ffast-math -fpeel-loops -funroll-loops -ftree-vectorize -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free"" BLAS=$HOMEPATH/inst/lib/libopenblas.so LAPACK=$HOMEPATH/inst/lib/libopenblas.so ATLAS=$HOMEPATH/inst/lib/libopenblas.so pip install numpy==1.14.6. # verify. python -c ""import numpy"". # dependecy of scipy 1.2.0. OPT=""-O3",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:8172,deployability,Build,Build,8172," from source code or CLIF cannot find protobuf. ```bash. # share build for Python. python --version # python 2.7 or newer. protoc --version. # build. cd protobuf-3.6.1/python/. python setup.py build. python setup.py test. # install from source as deepvariant needed. python setup.py install. # install from wheel. python setup.py bdist_wheel. pip install protobuf-3.6.1-py2.py3-none-any.whl --force-reinstall. # verify. python -c ""import google.protobuf"". ```. ## OpenBLAS 0.3.5. ```bash. git clone -b v0.3.5 https://github.com/xianyi/OpenBLAS.git OpenBLAS-0.3.5. cd OpenBLAS-0.3.5. make TARGET=power8. make TARGET=power8 PREFIX=$HOMEPATH/inst install. ```. ## Boost 1.66.0. ```bash. wget -qc https://dl.bintray.com/boostorg/release/1.66.0/source/boost_1_66_0.tar.gz. tar xzf boost_1_66_0.tar.gz. cd boost_1_66_0. ./bootstrap.sh --with-toolset=gcc --prefix=$HOMEPATH/inst. ./b2 dll-path=""$HOMEPATH/inst/lib"" install. ```. ## TensorFlow 1.12.0. > Note: Tensorflow 1.12.0 can be built with AT 11.0. Build instructions: [https://www.tensorflow.org/install/source](https://www.tensorflow.org/install/source). Edit instructions: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). Floatn.h error: [https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f](https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f). ```bash. # development packages. yum install python-devel python-pip -y. # dependency of numpy 1.14.6. OPT=""-D_XLOCALE_H=1 -O3 -mcpu=$CPU -mtune=$CPU -maltivec -mvsx -ffast-math -fpeel-loops -funroll-loops -ftree-vectorize -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free"" BLAS=$HOMEPATH/inst/lib/libopenblas.so LAPACK=$HOMEPATH/inst/lib/libopenblas.so ATLAS=$HOMEPATH/inst/lib/libopenblas.so pip install numpy==1.14.6. # verify. python -c ""import numpy"". # dependecy of scipy 1.2.0. OPT=""-O3 -mcpu=$CPU -mtune=$CPU -maltivec -mvsx -ffast-math -fpeel-loops -funroll-loops -ftree-v",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:8220,deployability,instal,install,8220,"``bash. # share build for Python. python --version # python 2.7 or newer. protoc --version. # build. cd protobuf-3.6.1/python/. python setup.py build. python setup.py test. # install from source as deepvariant needed. python setup.py install. # install from wheel. python setup.py bdist_wheel. pip install protobuf-3.6.1-py2.py3-none-any.whl --force-reinstall. # verify. python -c ""import google.protobuf"". ```. ## OpenBLAS 0.3.5. ```bash. git clone -b v0.3.5 https://github.com/xianyi/OpenBLAS.git OpenBLAS-0.3.5. cd OpenBLAS-0.3.5. make TARGET=power8. make TARGET=power8 PREFIX=$HOMEPATH/inst install. ```. ## Boost 1.66.0. ```bash. wget -qc https://dl.bintray.com/boostorg/release/1.66.0/source/boost_1_66_0.tar.gz. tar xzf boost_1_66_0.tar.gz. cd boost_1_66_0. ./bootstrap.sh --with-toolset=gcc --prefix=$HOMEPATH/inst. ./b2 dll-path=""$HOMEPATH/inst/lib"" install. ```. ## TensorFlow 1.12.0. > Note: Tensorflow 1.12.0 can be built with AT 11.0. Build instructions: [https://www.tensorflow.org/install/source](https://www.tensorflow.org/install/source). Edit instructions: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). Floatn.h error: [https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f](https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f). ```bash. # development packages. yum install python-devel python-pip -y. # dependency of numpy 1.14.6. OPT=""-D_XLOCALE_H=1 -O3 -mcpu=$CPU -mtune=$CPU -maltivec -mvsx -ffast-math -fpeel-loops -funroll-loops -ftree-vectorize -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free"" BLAS=$HOMEPATH/inst/lib/libopenblas.so LAPACK=$HOMEPATH/inst/lib/libopenblas.so ATLAS=$HOMEPATH/inst/lib/libopenblas.so pip install numpy==1.14.6. # verify. python -c ""import numpy"". # dependecy of scipy 1.2.0. OPT=""-O3 -mcpu=$CPU -mtune=$CPU -maltivec -mvsx -ffast-math -fpeel-loops -funroll-loops -ftree-vectorize -fno-builtin-malloc -fno-builtin-calloc ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:8263,deployability,instal,install,8263,"version # python 2.7 or newer. protoc --version. # build. cd protobuf-3.6.1/python/. python setup.py build. python setup.py test. # install from source as deepvariant needed. python setup.py install. # install from wheel. python setup.py bdist_wheel. pip install protobuf-3.6.1-py2.py3-none-any.whl --force-reinstall. # verify. python -c ""import google.protobuf"". ```. ## OpenBLAS 0.3.5. ```bash. git clone -b v0.3.5 https://github.com/xianyi/OpenBLAS.git OpenBLAS-0.3.5. cd OpenBLAS-0.3.5. make TARGET=power8. make TARGET=power8 PREFIX=$HOMEPATH/inst install. ```. ## Boost 1.66.0. ```bash. wget -qc https://dl.bintray.com/boostorg/release/1.66.0/source/boost_1_66_0.tar.gz. tar xzf boost_1_66_0.tar.gz. cd boost_1_66_0. ./bootstrap.sh --with-toolset=gcc --prefix=$HOMEPATH/inst. ./b2 dll-path=""$HOMEPATH/inst/lib"" install. ```. ## TensorFlow 1.12.0. > Note: Tensorflow 1.12.0 can be built with AT 11.0. Build instructions: [https://www.tensorflow.org/install/source](https://www.tensorflow.org/install/source). Edit instructions: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). Floatn.h error: [https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f](https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f). ```bash. # development packages. yum install python-devel python-pip -y. # dependency of numpy 1.14.6. OPT=""-D_XLOCALE_H=1 -O3 -mcpu=$CPU -mtune=$CPU -maltivec -mvsx -ffast-math -fpeel-loops -funroll-loops -ftree-vectorize -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free"" BLAS=$HOMEPATH/inst/lib/libopenblas.so LAPACK=$HOMEPATH/inst/lib/libopenblas.so ATLAS=$HOMEPATH/inst/lib/libopenblas.so pip install numpy==1.14.6. # verify. python -c ""import numpy"". # dependecy of scipy 1.2.0. OPT=""-O3 -mcpu=$CPU -mtune=$CPU -maltivec -mvsx -ffast-math -fpeel-loops -funroll-loops -ftree-vectorize -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free"" BLA",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:8602,deployability,instal,install,8602,"import google.protobuf"". ```. ## OpenBLAS 0.3.5. ```bash. git clone -b v0.3.5 https://github.com/xianyi/OpenBLAS.git OpenBLAS-0.3.5. cd OpenBLAS-0.3.5. make TARGET=power8. make TARGET=power8 PREFIX=$HOMEPATH/inst install. ```. ## Boost 1.66.0. ```bash. wget -qc https://dl.bintray.com/boostorg/release/1.66.0/source/boost_1_66_0.tar.gz. tar xzf boost_1_66_0.tar.gz. cd boost_1_66_0. ./bootstrap.sh --with-toolset=gcc --prefix=$HOMEPATH/inst. ./b2 dll-path=""$HOMEPATH/inst/lib"" install. ```. ## TensorFlow 1.12.0. > Note: Tensorflow 1.12.0 can be built with AT 11.0. Build instructions: [https://www.tensorflow.org/install/source](https://www.tensorflow.org/install/source). Edit instructions: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). Floatn.h error: [https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f](https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f). ```bash. # development packages. yum install python-devel python-pip -y. # dependency of numpy 1.14.6. OPT=""-D_XLOCALE_H=1 -O3 -mcpu=$CPU -mtune=$CPU -maltivec -mvsx -ffast-math -fpeel-loops -funroll-loops -ftree-vectorize -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free"" BLAS=$HOMEPATH/inst/lib/libopenblas.so LAPACK=$HOMEPATH/inst/lib/libopenblas.so ATLAS=$HOMEPATH/inst/lib/libopenblas.so pip install numpy==1.14.6. # verify. python -c ""import numpy"". # dependecy of scipy 1.2.0. OPT=""-O3 -mcpu=$CPU -mtune=$CPU -maltivec -mvsx -ffast-math -fpeel-loops -funroll-loops -ftree-vectorize -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free"" BLAS=$HOMEPATH/inst/lib/libopenblas.so LAPACK=$HOMEPATH/inst/lib/libopenblas.so ATLAS=$HOMEPATH/inst/lib/libopenblas.so pip install scipy==1.2.0. # verify. python -c ""import scipy"". # pip package dependencies. # pip install pip six wheel mock. pip install wheel autograd h5py==2.9.0 enum34. pip install keras_applications==1.0.6 keras_preproc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:8640,deployability,depend,dependency,8640,"AS 0.3.5. ```bash. git clone -b v0.3.5 https://github.com/xianyi/OpenBLAS.git OpenBLAS-0.3.5. cd OpenBLAS-0.3.5. make TARGET=power8. make TARGET=power8 PREFIX=$HOMEPATH/inst install. ```. ## Boost 1.66.0. ```bash. wget -qc https://dl.bintray.com/boostorg/release/1.66.0/source/boost_1_66_0.tar.gz. tar xzf boost_1_66_0.tar.gz. cd boost_1_66_0. ./bootstrap.sh --with-toolset=gcc --prefix=$HOMEPATH/inst. ./b2 dll-path=""$HOMEPATH/inst/lib"" install. ```. ## TensorFlow 1.12.0. > Note: Tensorflow 1.12.0 can be built with AT 11.0. Build instructions: [https://www.tensorflow.org/install/source](https://www.tensorflow.org/install/source). Edit instructions: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). Floatn.h error: [https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f](https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f). ```bash. # development packages. yum install python-devel python-pip -y. # dependency of numpy 1.14.6. OPT=""-D_XLOCALE_H=1 -O3 -mcpu=$CPU -mtune=$CPU -maltivec -mvsx -ffast-math -fpeel-loops -funroll-loops -ftree-vectorize -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free"" BLAS=$HOMEPATH/inst/lib/libopenblas.so LAPACK=$HOMEPATH/inst/lib/libopenblas.so ATLAS=$HOMEPATH/inst/lib/libopenblas.so pip install numpy==1.14.6. # verify. python -c ""import numpy"". # dependecy of scipy 1.2.0. OPT=""-O3 -mcpu=$CPU -mtune=$CPU -maltivec -mvsx -ffast-math -fpeel-loops -funroll-loops -ftree-vectorize -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free"" BLAS=$HOMEPATH/inst/lib/libopenblas.so LAPACK=$HOMEPATH/inst/lib/libopenblas.so ATLAS=$HOMEPATH/inst/lib/libopenblas.so pip install scipy==1.2.0. # verify. python -c ""import scipy"". # pip package dependencies. # pip install pip six wheel mock. pip install wheel autograd h5py==2.9.0 enum34. pip install keras_applications==1.0.6 keras_preprocessing==1.0.5. # download source code. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:8992,deployability,instal,install,8992,"trap.sh --with-toolset=gcc --prefix=$HOMEPATH/inst. ./b2 dll-path=""$HOMEPATH/inst/lib"" install. ```. ## TensorFlow 1.12.0. > Note: Tensorflow 1.12.0 can be built with AT 11.0. Build instructions: [https://www.tensorflow.org/install/source](https://www.tensorflow.org/install/source). Edit instructions: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). Floatn.h error: [https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f](https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f). ```bash. # development packages. yum install python-devel python-pip -y. # dependency of numpy 1.14.6. OPT=""-D_XLOCALE_H=1 -O3 -mcpu=$CPU -mtune=$CPU -maltivec -mvsx -ffast-math -fpeel-loops -funroll-loops -ftree-vectorize -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free"" BLAS=$HOMEPATH/inst/lib/libopenblas.so LAPACK=$HOMEPATH/inst/lib/libopenblas.so ATLAS=$HOMEPATH/inst/lib/libopenblas.so pip install numpy==1.14.6. # verify. python -c ""import numpy"". # dependecy of scipy 1.2.0. OPT=""-O3 -mcpu=$CPU -mtune=$CPU -maltivec -mvsx -ffast-math -fpeel-loops -funroll-loops -ftree-vectorize -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free"" BLAS=$HOMEPATH/inst/lib/libopenblas.so LAPACK=$HOMEPATH/inst/lib/libopenblas.so ATLAS=$HOMEPATH/inst/lib/libopenblas.so pip install scipy==1.2.0. # verify. python -c ""import scipy"". # pip package dependencies. # pip install pip six wheel mock. pip install wheel autograd h5py==2.9.0 enum34. pip install keras_applications==1.0.6 keras_preprocessing==1.0.5. # download source code. git clone -b r1.12 https://github.com/tensorflow/tensorflow.git tensorflow-1.12. cd tensorflow-1.12. # configure in tensorflow/.tf_configure.bazelrc. PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python"" \. PYTHON_LIB_PATH=""$HOMEPATH/inst/lib/python2.7/site-packages"" \. TF_NEED_IGNITE=""0"" \. TF_ENABLE_XLA=""0"" \. TF_NEED_OPENCL_SYCL=""0"" \. TF_NEED_ROCM=""0"" \. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:9053,deployability,depend,dependecy,9053,"ath=""$HOMEPATH/inst/lib"" install. ```. ## TensorFlow 1.12.0. > Note: Tensorflow 1.12.0 can be built with AT 11.0. Build instructions: [https://www.tensorflow.org/install/source](https://www.tensorflow.org/install/source). Edit instructions: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). Floatn.h error: [https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f](https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f). ```bash. # development packages. yum install python-devel python-pip -y. # dependency of numpy 1.14.6. OPT=""-D_XLOCALE_H=1 -O3 -mcpu=$CPU -mtune=$CPU -maltivec -mvsx -ffast-math -fpeel-loops -funroll-loops -ftree-vectorize -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free"" BLAS=$HOMEPATH/inst/lib/libopenblas.so LAPACK=$HOMEPATH/inst/lib/libopenblas.so ATLAS=$HOMEPATH/inst/lib/libopenblas.so pip install numpy==1.14.6. # verify. python -c ""import numpy"". # dependecy of scipy 1.2.0. OPT=""-O3 -mcpu=$CPU -mtune=$CPU -maltivec -mvsx -ffast-math -fpeel-loops -funroll-loops -ftree-vectorize -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free"" BLAS=$HOMEPATH/inst/lib/libopenblas.so LAPACK=$HOMEPATH/inst/lib/libopenblas.so ATLAS=$HOMEPATH/inst/lib/libopenblas.so pip install scipy==1.2.0. # verify. python -c ""import scipy"". # pip package dependencies. # pip install pip six wheel mock. pip install wheel autograd h5py==2.9.0 enum34. pip install keras_applications==1.0.6 keras_preprocessing==1.0.5. # download source code. git clone -b r1.12 https://github.com/tensorflow/tensorflow.git tensorflow-1.12. cd tensorflow-1.12. # configure in tensorflow/.tf_configure.bazelrc. PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python"" \. PYTHON_LIB_PATH=""$HOMEPATH/inst/lib/python2.7/site-packages"" \. TF_NEED_IGNITE=""0"" \. TF_ENABLE_XLA=""0"" \. TF_NEED_OPENCL_SYCL=""0"" \. TF_NEED_ROCM=""0"" \. TF_NEED_MPI=""0"" \. TF_NEED_TENSORRT=""0"" \. TF_NEED_CUDA=""1"" \.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:9388,deployability,instal,install,9388,"u120/tensorflow.html). Floatn.h error: [https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f](https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f). ```bash. # development packages. yum install python-devel python-pip -y. # dependency of numpy 1.14.6. OPT=""-D_XLOCALE_H=1 -O3 -mcpu=$CPU -mtune=$CPU -maltivec -mvsx -ffast-math -fpeel-loops -funroll-loops -ftree-vectorize -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free"" BLAS=$HOMEPATH/inst/lib/libopenblas.so LAPACK=$HOMEPATH/inst/lib/libopenblas.so ATLAS=$HOMEPATH/inst/lib/libopenblas.so pip install numpy==1.14.6. # verify. python -c ""import numpy"". # dependecy of scipy 1.2.0. OPT=""-O3 -mcpu=$CPU -mtune=$CPU -maltivec -mvsx -ffast-math -fpeel-loops -funroll-loops -ftree-vectorize -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free"" BLAS=$HOMEPATH/inst/lib/libopenblas.so LAPACK=$HOMEPATH/inst/lib/libopenblas.so ATLAS=$HOMEPATH/inst/lib/libopenblas.so pip install scipy==1.2.0. # verify. python -c ""import scipy"". # pip package dependencies. # pip install pip six wheel mock. pip install wheel autograd h5py==2.9.0 enum34. pip install keras_applications==1.0.6 keras_preprocessing==1.0.5. # download source code. git clone -b r1.12 https://github.com/tensorflow/tensorflow.git tensorflow-1.12. cd tensorflow-1.12. # configure in tensorflow/.tf_configure.bazelrc. PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python"" \. PYTHON_LIB_PATH=""$HOMEPATH/inst/lib/python2.7/site-packages"" \. TF_NEED_IGNITE=""0"" \. TF_ENABLE_XLA=""0"" \. TF_NEED_OPENCL_SYCL=""0"" \. TF_NEED_ROCM=""0"" \. TF_NEED_MPI=""0"" \. TF_NEED_TENSORRT=""0"" \. TF_NEED_CUDA=""1"" \. TF_CUDA_VERSION=""10.0"" \. CUDA_TOOLKIT_PATH=""/usr/local/cuda"" \. TF_CUDNN_VERSION=""7"" \. CUDNN_INSTALL_PATH=""/usr/local/cuda-10.0"" \. TF_NCCL_VERSION=""2"" \. NCCL_INSTALL_PATH=""/usr/local/cuda-10.0/targets/ppc64le-linux/lib"" \. NCCL_HDR_PATH=""/usr/local/cuda-10.0/targets/ppc64le-linux/lib/../include"" \. TF_CUDA_COMPUTE_CAPABILITIES=",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:9460,deployability,depend,dependencies,9460,"93ff2d9bc1bf2cd12bc6e76010da0f](https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f). ```bash. # development packages. yum install python-devel python-pip -y. # dependency of numpy 1.14.6. OPT=""-D_XLOCALE_H=1 -O3 -mcpu=$CPU -mtune=$CPU -maltivec -mvsx -ffast-math -fpeel-loops -funroll-loops -ftree-vectorize -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free"" BLAS=$HOMEPATH/inst/lib/libopenblas.so LAPACK=$HOMEPATH/inst/lib/libopenblas.so ATLAS=$HOMEPATH/inst/lib/libopenblas.so pip install numpy==1.14.6. # verify. python -c ""import numpy"". # dependecy of scipy 1.2.0. OPT=""-O3 -mcpu=$CPU -mtune=$CPU -maltivec -mvsx -ffast-math -fpeel-loops -funroll-loops -ftree-vectorize -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free"" BLAS=$HOMEPATH/inst/lib/libopenblas.so LAPACK=$HOMEPATH/inst/lib/libopenblas.so ATLAS=$HOMEPATH/inst/lib/libopenblas.so pip install scipy==1.2.0. # verify. python -c ""import scipy"". # pip package dependencies. # pip install pip six wheel mock. pip install wheel autograd h5py==2.9.0 enum34. pip install keras_applications==1.0.6 keras_preprocessing==1.0.5. # download source code. git clone -b r1.12 https://github.com/tensorflow/tensorflow.git tensorflow-1.12. cd tensorflow-1.12. # configure in tensorflow/.tf_configure.bazelrc. PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python"" \. PYTHON_LIB_PATH=""$HOMEPATH/inst/lib/python2.7/site-packages"" \. TF_NEED_IGNITE=""0"" \. TF_ENABLE_XLA=""0"" \. TF_NEED_OPENCL_SYCL=""0"" \. TF_NEED_ROCM=""0"" \. TF_NEED_MPI=""0"" \. TF_NEED_TENSORRT=""0"" \. TF_NEED_CUDA=""1"" \. TF_CUDA_VERSION=""10.0"" \. CUDA_TOOLKIT_PATH=""/usr/local/cuda"" \. TF_CUDNN_VERSION=""7"" \. CUDNN_INSTALL_PATH=""/usr/local/cuda-10.0"" \. TF_NCCL_VERSION=""2"" \. NCCL_INSTALL_PATH=""/usr/local/cuda-10.0/targets/ppc64le-linux/lib"" \. NCCL_HDR_PATH=""/usr/local/cuda-10.0/targets/ppc64le-linux/lib/../include"" \. TF_CUDA_COMPUTE_CAPABILITIES=""3.7"" \. TF_CUDA_CLANG=""0"" \. GCC_HOST_COMPILER_PATH=""/opt/at11.0/bin/gcc""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:9480,deployability,instal,install,9480,"c6e76010da0f](https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f). ```bash. # development packages. yum install python-devel python-pip -y. # dependency of numpy 1.14.6. OPT=""-D_XLOCALE_H=1 -O3 -mcpu=$CPU -mtune=$CPU -maltivec -mvsx -ffast-math -fpeel-loops -funroll-loops -ftree-vectorize -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free"" BLAS=$HOMEPATH/inst/lib/libopenblas.so LAPACK=$HOMEPATH/inst/lib/libopenblas.so ATLAS=$HOMEPATH/inst/lib/libopenblas.so pip install numpy==1.14.6. # verify. python -c ""import numpy"". # dependecy of scipy 1.2.0. OPT=""-O3 -mcpu=$CPU -mtune=$CPU -maltivec -mvsx -ffast-math -fpeel-loops -funroll-loops -ftree-vectorize -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free"" BLAS=$HOMEPATH/inst/lib/libopenblas.so LAPACK=$HOMEPATH/inst/lib/libopenblas.so ATLAS=$HOMEPATH/inst/lib/libopenblas.so pip install scipy==1.2.0. # verify. python -c ""import scipy"". # pip package dependencies. # pip install pip six wheel mock. pip install wheel autograd h5py==2.9.0 enum34. pip install keras_applications==1.0.6 keras_preprocessing==1.0.5. # download source code. git clone -b r1.12 https://github.com/tensorflow/tensorflow.git tensorflow-1.12. cd tensorflow-1.12. # configure in tensorflow/.tf_configure.bazelrc. PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python"" \. PYTHON_LIB_PATH=""$HOMEPATH/inst/lib/python2.7/site-packages"" \. TF_NEED_IGNITE=""0"" \. TF_ENABLE_XLA=""0"" \. TF_NEED_OPENCL_SYCL=""0"" \. TF_NEED_ROCM=""0"" \. TF_NEED_MPI=""0"" \. TF_NEED_TENSORRT=""0"" \. TF_NEED_CUDA=""1"" \. TF_CUDA_VERSION=""10.0"" \. CUDA_TOOLKIT_PATH=""/usr/local/cuda"" \. TF_CUDNN_VERSION=""7"" \. CUDNN_INSTALL_PATH=""/usr/local/cuda-10.0"" \. TF_NCCL_VERSION=""2"" \. NCCL_INSTALL_PATH=""/usr/local/cuda-10.0/targets/ppc64le-linux/lib"" \. NCCL_HDR_PATH=""/usr/local/cuda-10.0/targets/ppc64le-linux/lib/../include"" \. TF_CUDA_COMPUTE_CAPABILITIES=""3.7"" \. TF_CUDA_CLANG=""0"" \. GCC_HOST_COMPILER_PATH=""/opt/at11.0/bin/gcc"" \. CC_OPT_FLAGS=""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:9512,deployability,instal,install,9512,"b.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f). ```bash. # development packages. yum install python-devel python-pip -y. # dependency of numpy 1.14.6. OPT=""-D_XLOCALE_H=1 -O3 -mcpu=$CPU -mtune=$CPU -maltivec -mvsx -ffast-math -fpeel-loops -funroll-loops -ftree-vectorize -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free"" BLAS=$HOMEPATH/inst/lib/libopenblas.so LAPACK=$HOMEPATH/inst/lib/libopenblas.so ATLAS=$HOMEPATH/inst/lib/libopenblas.so pip install numpy==1.14.6. # verify. python -c ""import numpy"". # dependecy of scipy 1.2.0. OPT=""-O3 -mcpu=$CPU -mtune=$CPU -maltivec -mvsx -ffast-math -fpeel-loops -funroll-loops -ftree-vectorize -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free"" BLAS=$HOMEPATH/inst/lib/libopenblas.so LAPACK=$HOMEPATH/inst/lib/libopenblas.so ATLAS=$HOMEPATH/inst/lib/libopenblas.so pip install scipy==1.2.0. # verify. python -c ""import scipy"". # pip package dependencies. # pip install pip six wheel mock. pip install wheel autograd h5py==2.9.0 enum34. pip install keras_applications==1.0.6 keras_preprocessing==1.0.5. # download source code. git clone -b r1.12 https://github.com/tensorflow/tensorflow.git tensorflow-1.12. cd tensorflow-1.12. # configure in tensorflow/.tf_configure.bazelrc. PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python"" \. PYTHON_LIB_PATH=""$HOMEPATH/inst/lib/python2.7/site-packages"" \. TF_NEED_IGNITE=""0"" \. TF_ENABLE_XLA=""0"" \. TF_NEED_OPENCL_SYCL=""0"" \. TF_NEED_ROCM=""0"" \. TF_NEED_MPI=""0"" \. TF_NEED_TENSORRT=""0"" \. TF_NEED_CUDA=""1"" \. TF_CUDA_VERSION=""10.0"" \. CUDA_TOOLKIT_PATH=""/usr/local/cuda"" \. TF_CUDNN_VERSION=""7"" \. CUDNN_INSTALL_PATH=""/usr/local/cuda-10.0"" \. TF_NCCL_VERSION=""2"" \. NCCL_INSTALL_PATH=""/usr/local/cuda-10.0/targets/ppc64le-linux/lib"" \. NCCL_HDR_PATH=""/usr/local/cuda-10.0/targets/ppc64le-linux/lib/../include"" \. TF_CUDA_COMPUTE_CAPABILITIES=""3.7"" \. TF_CUDA_CLANG=""0"" \. GCC_HOST_COMPILER_PATH=""/opt/at11.0/bin/gcc"" \. CC_OPT_FLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" \. T",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:9559,deployability,instal,install,9559,". ```bash. # development packages. yum install python-devel python-pip -y. # dependency of numpy 1.14.6. OPT=""-D_XLOCALE_H=1 -O3 -mcpu=$CPU -mtune=$CPU -maltivec -mvsx -ffast-math -fpeel-loops -funroll-loops -ftree-vectorize -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free"" BLAS=$HOMEPATH/inst/lib/libopenblas.so LAPACK=$HOMEPATH/inst/lib/libopenblas.so ATLAS=$HOMEPATH/inst/lib/libopenblas.so pip install numpy==1.14.6. # verify. python -c ""import numpy"". # dependecy of scipy 1.2.0. OPT=""-O3 -mcpu=$CPU -mtune=$CPU -maltivec -mvsx -ffast-math -fpeel-loops -funroll-loops -ftree-vectorize -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free"" BLAS=$HOMEPATH/inst/lib/libopenblas.so LAPACK=$HOMEPATH/inst/lib/libopenblas.so ATLAS=$HOMEPATH/inst/lib/libopenblas.so pip install scipy==1.2.0. # verify. python -c ""import scipy"". # pip package dependencies. # pip install pip six wheel mock. pip install wheel autograd h5py==2.9.0 enum34. pip install keras_applications==1.0.6 keras_preprocessing==1.0.5. # download source code. git clone -b r1.12 https://github.com/tensorflow/tensorflow.git tensorflow-1.12. cd tensorflow-1.12. # configure in tensorflow/.tf_configure.bazelrc. PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python"" \. PYTHON_LIB_PATH=""$HOMEPATH/inst/lib/python2.7/site-packages"" \. TF_NEED_IGNITE=""0"" \. TF_ENABLE_XLA=""0"" \. TF_NEED_OPENCL_SYCL=""0"" \. TF_NEED_ROCM=""0"" \. TF_NEED_MPI=""0"" \. TF_NEED_TENSORRT=""0"" \. TF_NEED_CUDA=""1"" \. TF_CUDA_VERSION=""10.0"" \. CUDA_TOOLKIT_PATH=""/usr/local/cuda"" \. TF_CUDNN_VERSION=""7"" \. CUDNN_INSTALL_PATH=""/usr/local/cuda-10.0"" \. TF_NCCL_VERSION=""2"" \. NCCL_INSTALL_PATH=""/usr/local/cuda-10.0/targets/ppc64le-linux/lib"" \. NCCL_HDR_PATH=""/usr/local/cuda-10.0/targets/ppc64le-linux/lib/../include"" \. TF_CUDA_COMPUTE_CAPABILITIES=""3.7"" \. TF_CUDA_CLANG=""0"" \. GCC_HOST_COMPILER_PATH=""/opt/at11.0/bin/gcc"" \. CC_OPT_FLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" \. TF_SET_ANDROID_WORKSPACE=0 \. ./configure. # fix",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:10564,deployability,build,build,10564,"keras_applications==1.0.6 keras_preprocessing==1.0.5. # download source code. git clone -b r1.12 https://github.com/tensorflow/tensorflow.git tensorflow-1.12. cd tensorflow-1.12. # configure in tensorflow/.tf_configure.bazelrc. PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python"" \. PYTHON_LIB_PATH=""$HOMEPATH/inst/lib/python2.7/site-packages"" \. TF_NEED_IGNITE=""0"" \. TF_ENABLE_XLA=""0"" \. TF_NEED_OPENCL_SYCL=""0"" \. TF_NEED_ROCM=""0"" \. TF_NEED_MPI=""0"" \. TF_NEED_TENSORRT=""0"" \. TF_NEED_CUDA=""1"" \. TF_CUDA_VERSION=""10.0"" \. CUDA_TOOLKIT_PATH=""/usr/local/cuda"" \. TF_CUDNN_VERSION=""7"" \. CUDNN_INSTALL_PATH=""/usr/local/cuda-10.0"" \. TF_NCCL_VERSION=""2"" \. NCCL_INSTALL_PATH=""/usr/local/cuda-10.0/targets/ppc64le-linux/lib"" \. NCCL_HDR_PATH=""/usr/local/cuda-10.0/targets/ppc64le-linux/lib/../include"" \. TF_CUDA_COMPUTE_CAPABILITIES=""3.7"" \. TF_CUDA_CLANG=""0"" \. GCC_HOST_COMPILER_PATH=""/opt/at11.0/bin/gcc"" \. CC_OPT_FLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" \. TF_SET_ANDROID_WORKSPACE=0 \. ./configure. # fix build error. vim /opt/at11.0/include/bits/floatn.h. -------------------------------------. #include <features.h>. /* Defined to 1 if the current compiler invocation provides a. floating-point type with the IEEE 754 binary128 format, and this glibc. includes corresponding *f128 interfaces for it. */. #if defined _ARCH_PWR8 && defined __LITTLE_ENDIAN__ && (_CALL_ELF == 2) \. && defined __FLOAT128__. # define __HAVE_FLOAT128 1. #else. # define __HAVE_FLOAT128 0. #endif. /* add the following block of fix tensorflow build error */. #if CUDART_VERSION. #undef __HAVE_FLOAT128. #define __HAVE_FLOAT128 0. #endif. /* Defined to 1 if __HAVE_FLOAT128 is 1 and the type is ABI-distinct. from the default float, double and long double types in this glibc. */. #if __HAVE_FLOAT128. -------------------------------------. # build. bazel clean. export PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python"". bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --cxxopt=""-D_GLIBCXX_USE_CX",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:11081,deployability,build,build,11081," CUDA_TOOLKIT_PATH=""/usr/local/cuda"" \. TF_CUDNN_VERSION=""7"" \. CUDNN_INSTALL_PATH=""/usr/local/cuda-10.0"" \. TF_NCCL_VERSION=""2"" \. NCCL_INSTALL_PATH=""/usr/local/cuda-10.0/targets/ppc64le-linux/lib"" \. NCCL_HDR_PATH=""/usr/local/cuda-10.0/targets/ppc64le-linux/lib/../include"" \. TF_CUDA_COMPUTE_CAPABILITIES=""3.7"" \. TF_CUDA_CLANG=""0"" \. GCC_HOST_COMPILER_PATH=""/opt/at11.0/bin/gcc"" \. CC_OPT_FLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" \. TF_SET_ANDROID_WORKSPACE=0 \. ./configure. # fix build error. vim /opt/at11.0/include/bits/floatn.h. -------------------------------------. #include <features.h>. /* Defined to 1 if the current compiler invocation provides a. floating-point type with the IEEE 754 binary128 format, and this glibc. includes corresponding *f128 interfaces for it. */. #if defined _ARCH_PWR8 && defined __LITTLE_ENDIAN__ && (_CALL_ELF == 2) \. && defined __FLOAT128__. # define __HAVE_FLOAT128 1. #else. # define __HAVE_FLOAT128 0. #endif. /* add the following block of fix tensorflow build error */. #if CUDART_VERSION. #undef __HAVE_FLOAT128. #define __HAVE_FLOAT128 0. #endif. /* Defined to 1 if __HAVE_FLOAT128 is 1 and the type is ABI-distinct. from the default float, double and long double types in this glibc. */. #if __HAVE_FLOAT128. -------------------------------------. # build. bazel clean. export PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python"". bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" --action_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH --test_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH --distinct_host_configuration=false. # generate package. bazel-bin/tensorflow/tools/pip_package/build_pip_package $HOMEPATH/tensorflow_package. # install. pip install $HOMEPATH/tensorflow_package/tensorflow-1.12.0-*.whl. # verification. python -c ""import tensorflow as tf; print(tf.__version__)"". ```. ## CLIF. > Note: CLIF can be built with AT 11.0. Git Repository: [https://github.com/google/clif](https://github.c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:11380,deployability,build,build,11380,"BILITIES=""3.7"" \. TF_CUDA_CLANG=""0"" \. GCC_HOST_COMPILER_PATH=""/opt/at11.0/bin/gcc"" \. CC_OPT_FLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" \. TF_SET_ANDROID_WORKSPACE=0 \. ./configure. # fix build error. vim /opt/at11.0/include/bits/floatn.h. -------------------------------------. #include <features.h>. /* Defined to 1 if the current compiler invocation provides a. floating-point type with the IEEE 754 binary128 format, and this glibc. includes corresponding *f128 interfaces for it. */. #if defined _ARCH_PWR8 && defined __LITTLE_ENDIAN__ && (_CALL_ELF == 2) \. && defined __FLOAT128__. # define __HAVE_FLOAT128 1. #else. # define __HAVE_FLOAT128 0. #endif. /* add the following block of fix tensorflow build error */. #if CUDART_VERSION. #undef __HAVE_FLOAT128. #define __HAVE_FLOAT128 0. #endif. /* Defined to 1 if __HAVE_FLOAT128 is 1 and the type is ABI-distinct. from the default float, double and long double types in this glibc. */. #if __HAVE_FLOAT128. -------------------------------------. # build. bazel clean. export PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python"". bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" --action_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH --test_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH --distinct_host_configuration=false. # generate package. bazel-bin/tensorflow/tools/pip_package/build_pip_package $HOMEPATH/tensorflow_package. # install. pip install $HOMEPATH/tensorflow_package/tensorflow-1.12.0-*.whl. # verification. python -c ""import tensorflow as tf; print(tf.__version__)"". ```. ## CLIF. > Note: CLIF can be built with AT 11.0. Git Repository: [https://github.com/google/clif](https://github.com/google/clif). ```bash. # Prerequisites. cmake --version #3.5+. protoc --version # 3.2.0+ build from source code for both C++ and Python. pip install virtualenv. pip install pyparsing. yum install subversion. yum install ocaml. pip install 'pyparsing>=2.2.0'. pkg-config --libs python # workable. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:11458,deployability,build,build,11458,"/gcc"" \. CC_OPT_FLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" \. TF_SET_ANDROID_WORKSPACE=0 \. ./configure. # fix build error. vim /opt/at11.0/include/bits/floatn.h. -------------------------------------. #include <features.h>. /* Defined to 1 if the current compiler invocation provides a. floating-point type with the IEEE 754 binary128 format, and this glibc. includes corresponding *f128 interfaces for it. */. #if defined _ARCH_PWR8 && defined __LITTLE_ENDIAN__ && (_CALL_ELF == 2) \. && defined __FLOAT128__. # define __HAVE_FLOAT128 1. #else. # define __HAVE_FLOAT128 0. #endif. /* add the following block of fix tensorflow build error */. #if CUDART_VERSION. #undef __HAVE_FLOAT128. #define __HAVE_FLOAT128 0. #endif. /* Defined to 1 if __HAVE_FLOAT128 is 1 and the type is ABI-distinct. from the default float, double and long double types in this glibc. */. #if __HAVE_FLOAT128. -------------------------------------. # build. bazel clean. export PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python"". bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" --action_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH --test_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH --distinct_host_configuration=false. # generate package. bazel-bin/tensorflow/tools/pip_package/build_pip_package $HOMEPATH/tensorflow_package. # install. pip install $HOMEPATH/tensorflow_package/tensorflow-1.12.0-*.whl. # verification. python -c ""import tensorflow as tf; print(tf.__version__)"". ```. ## CLIF. > Note: CLIF can be built with AT 11.0. Git Repository: [https://github.com/google/clif](https://github.com/google/clif). ```bash. # Prerequisites. cmake --version #3.5+. protoc --version # 3.2.0+ build from source code for both C++ and Python. pip install virtualenv. pip install pyparsing. yum install subversion. yum install ocaml. pip install 'pyparsing>=2.2.0'. pkg-config --libs python # workable. # download source code. cd $HOMEPATH. git clone https://github.com/google/clif",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:11814,deployability,instal,install,11814,"des corresponding *f128 interfaces for it. */. #if defined _ARCH_PWR8 && defined __LITTLE_ENDIAN__ && (_CALL_ELF == 2) \. && defined __FLOAT128__. # define __HAVE_FLOAT128 1. #else. # define __HAVE_FLOAT128 0. #endif. /* add the following block of fix tensorflow build error */. #if CUDART_VERSION. #undef __HAVE_FLOAT128. #define __HAVE_FLOAT128 0. #endif. /* Defined to 1 if __HAVE_FLOAT128 is 1 and the type is ABI-distinct. from the default float, double and long double types in this glibc. */. #if __HAVE_FLOAT128. -------------------------------------. # build. bazel clean. export PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python"". bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" --action_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH --test_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH --distinct_host_configuration=false. # generate package. bazel-bin/tensorflow/tools/pip_package/build_pip_package $HOMEPATH/tensorflow_package. # install. pip install $HOMEPATH/tensorflow_package/tensorflow-1.12.0-*.whl. # verification. python -c ""import tensorflow as tf; print(tf.__version__)"". ```. ## CLIF. > Note: CLIF can be built with AT 11.0. Git Repository: [https://github.com/google/clif](https://github.com/google/clif). ```bash. # Prerequisites. cmake --version #3.5+. protoc --version # 3.2.0+ build from source code for both C++ and Python. pip install virtualenv. pip install pyparsing. yum install subversion. yum install ocaml. pip install 'pyparsing>=2.2.0'. pkg-config --libs python # workable. # download source code. cd $HOMEPATH. git clone https://github.com/google/clif.git. cd clif. # set environment. export INSTALL_DIR=""$HOMEPATH/inst"". export CLIFSRC_DIR=""$HOMEPATH/clif"". export LLVM_DIR=""$CLIFSRC_DIR/../clif_backend"". export BUILD_DIR=""$LLVM_DIR/build_matcher"". export PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python"". export PYTHON_LIB_PATH=""$HOMEPATH/inst/lib/python2.7/site-packages"". export PROTOC_PREFIX_PATH=""$(dirname",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:11827,deployability,instal,install,11827,"ding *f128 interfaces for it. */. #if defined _ARCH_PWR8 && defined __LITTLE_ENDIAN__ && (_CALL_ELF == 2) \. && defined __FLOAT128__. # define __HAVE_FLOAT128 1. #else. # define __HAVE_FLOAT128 0. #endif. /* add the following block of fix tensorflow build error */. #if CUDART_VERSION. #undef __HAVE_FLOAT128. #define __HAVE_FLOAT128 0. #endif. /* Defined to 1 if __HAVE_FLOAT128 is 1 and the type is ABI-distinct. from the default float, double and long double types in this glibc. */. #if __HAVE_FLOAT128. -------------------------------------. # build. bazel clean. export PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python"". bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" --action_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH --test_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH --distinct_host_configuration=false. # generate package. bazel-bin/tensorflow/tools/pip_package/build_pip_package $HOMEPATH/tensorflow_package. # install. pip install $HOMEPATH/tensorflow_package/tensorflow-1.12.0-*.whl. # verification. python -c ""import tensorflow as tf; print(tf.__version__)"". ```. ## CLIF. > Note: CLIF can be built with AT 11.0. Git Repository: [https://github.com/google/clif](https://github.com/google/clif). ```bash. # Prerequisites. cmake --version #3.5+. protoc --version # 3.2.0+ build from source code for both C++ and Python. pip install virtualenv. pip install pyparsing. yum install subversion. yum install ocaml. pip install 'pyparsing>=2.2.0'. pkg-config --libs python # workable. # download source code. cd $HOMEPATH. git clone https://github.com/google/clif.git. cd clif. # set environment. export INSTALL_DIR=""$HOMEPATH/inst"". export CLIFSRC_DIR=""$HOMEPATH/clif"". export LLVM_DIR=""$CLIFSRC_DIR/../clif_backend"". export BUILD_DIR=""$LLVM_DIR/build_matcher"". export PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python"". export PYTHON_LIB_PATH=""$HOMEPATH/inst/lib/python2.7/site-packages"". export PROTOC_PREFIX_PATH=""$(dirname ""$(dirname """,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:12135,deployability,version,version,12135,". #define __HAVE_FLOAT128 0. #endif. /* Defined to 1 if __HAVE_FLOAT128 is 1 and the type is ABI-distinct. from the default float, double and long double types in this glibc. */. #if __HAVE_FLOAT128. -------------------------------------. # build. bazel clean. export PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python"". bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" --action_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH --test_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH --distinct_host_configuration=false. # generate package. bazel-bin/tensorflow/tools/pip_package/build_pip_package $HOMEPATH/tensorflow_package. # install. pip install $HOMEPATH/tensorflow_package/tensorflow-1.12.0-*.whl. # verification. python -c ""import tensorflow as tf; print(tf.__version__)"". ```. ## CLIF. > Note: CLIF can be built with AT 11.0. Git Repository: [https://github.com/google/clif](https://github.com/google/clif). ```bash. # Prerequisites. cmake --version #3.5+. protoc --version # 3.2.0+ build from source code for both C++ and Python. pip install virtualenv. pip install pyparsing. yum install subversion. yum install ocaml. pip install 'pyparsing>=2.2.0'. pkg-config --libs python # workable. # download source code. cd $HOMEPATH. git clone https://github.com/google/clif.git. cd clif. # set environment. export INSTALL_DIR=""$HOMEPATH/inst"". export CLIFSRC_DIR=""$HOMEPATH/clif"". export LLVM_DIR=""$CLIFSRC_DIR/../clif_backend"". export BUILD_DIR=""$LLVM_DIR/build_matcher"". export PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python"". export PYTHON_LIB_PATH=""$HOMEPATH/inst/lib/python2.7/site-packages"". export PROTOC_PREFIX_PATH=""$(dirname ""$(dirname ""$(which protoc)"")"")"". export CLIF_VIRTUALENV=""$INSTALL_DIR""/clif. export CLIF_PIP=""$CLIF_VIRTUALENV/bin/pip"". virtualenv -p ""$PYTHON_BIN_PATH"" ""$CLIF_VIRTUALENV"". $CLIF_PIP install --upgrade pip. $CLIF_PIP install --upgrade setuptools. # Checkout LLVM and Clang source trees. mkdir -p $LLVM_DIR. cd $LLVM_DIR",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:12159,deployability,version,version,12159,"8 0. #endif. /* Defined to 1 if __HAVE_FLOAT128 is 1 and the type is ABI-distinct. from the default float, double and long double types in this glibc. */. #if __HAVE_FLOAT128. -------------------------------------. # build. bazel clean. export PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python"". bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" --action_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH --test_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH --distinct_host_configuration=false. # generate package. bazel-bin/tensorflow/tools/pip_package/build_pip_package $HOMEPATH/tensorflow_package. # install. pip install $HOMEPATH/tensorflow_package/tensorflow-1.12.0-*.whl. # verification. python -c ""import tensorflow as tf; print(tf.__version__)"". ```. ## CLIF. > Note: CLIF can be built with AT 11.0. Git Repository: [https://github.com/google/clif](https://github.com/google/clif). ```bash. # Prerequisites. cmake --version #3.5+. protoc --version # 3.2.0+ build from source code for both C++ and Python. pip install virtualenv. pip install pyparsing. yum install subversion. yum install ocaml. pip install 'pyparsing>=2.2.0'. pkg-config --libs python # workable. # download source code. cd $HOMEPATH. git clone https://github.com/google/clif.git. cd clif. # set environment. export INSTALL_DIR=""$HOMEPATH/inst"". export CLIFSRC_DIR=""$HOMEPATH/clif"". export LLVM_DIR=""$CLIFSRC_DIR/../clif_backend"". export BUILD_DIR=""$LLVM_DIR/build_matcher"". export PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python"". export PYTHON_LIB_PATH=""$HOMEPATH/inst/lib/python2.7/site-packages"". export PROTOC_PREFIX_PATH=""$(dirname ""$(dirname ""$(which protoc)"")"")"". export CLIF_VIRTUALENV=""$INSTALL_DIR""/clif. export CLIF_PIP=""$CLIF_VIRTUALENV/bin/pip"". virtualenv -p ""$PYTHON_BIN_PATH"" ""$CLIF_VIRTUALENV"". $CLIF_PIP install --upgrade pip. $CLIF_PIP install --upgrade setuptools. # Checkout LLVM and Clang source trees. mkdir -p $LLVM_DIR. cd $LLVM_DIR. svn co https://llvm.or",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:12176,deployability,build,build,12176,"Defined to 1 if __HAVE_FLOAT128 is 1 and the type is ABI-distinct. from the default float, double and long double types in this glibc. */. #if __HAVE_FLOAT128. -------------------------------------. # build. bazel clean. export PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python"". bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" --action_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH --test_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH --distinct_host_configuration=false. # generate package. bazel-bin/tensorflow/tools/pip_package/build_pip_package $HOMEPATH/tensorflow_package. # install. pip install $HOMEPATH/tensorflow_package/tensorflow-1.12.0-*.whl. # verification. python -c ""import tensorflow as tf; print(tf.__version__)"". ```. ## CLIF. > Note: CLIF can be built with AT 11.0. Git Repository: [https://github.com/google/clif](https://github.com/google/clif). ```bash. # Prerequisites. cmake --version #3.5+. protoc --version # 3.2.0+ build from source code for both C++ and Python. pip install virtualenv. pip install pyparsing. yum install subversion. yum install ocaml. pip install 'pyparsing>=2.2.0'. pkg-config --libs python # workable. # download source code. cd $HOMEPATH. git clone https://github.com/google/clif.git. cd clif. # set environment. export INSTALL_DIR=""$HOMEPATH/inst"". export CLIFSRC_DIR=""$HOMEPATH/clif"". export LLVM_DIR=""$CLIFSRC_DIR/../clif_backend"". export BUILD_DIR=""$LLVM_DIR/build_matcher"". export PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python"". export PYTHON_LIB_PATH=""$HOMEPATH/inst/lib/python2.7/site-packages"". export PROTOC_PREFIX_PATH=""$(dirname ""$(dirname ""$(which protoc)"")"")"". export CLIF_VIRTUALENV=""$INSTALL_DIR""/clif. export CLIF_PIP=""$CLIF_VIRTUALENV/bin/pip"". virtualenv -p ""$PYTHON_BIN_PATH"" ""$CLIF_VIRTUALENV"". $CLIF_PIP install --upgrade pip. $CLIF_PIP install --upgrade setuptools. # Checkout LLVM and Clang source trees. mkdir -p $LLVM_DIR. cd $LLVM_DIR. svn co https://llvm.org/svn/llvm-proje",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:12228,deployability,instal,install,12228,"ABI-distinct. from the default float, double and long double types in this glibc. */. #if __HAVE_FLOAT128. -------------------------------------. # build. bazel clean. export PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python"". bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" --action_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH --test_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH --distinct_host_configuration=false. # generate package. bazel-bin/tensorflow/tools/pip_package/build_pip_package $HOMEPATH/tensorflow_package. # install. pip install $HOMEPATH/tensorflow_package/tensorflow-1.12.0-*.whl. # verification. python -c ""import tensorflow as tf; print(tf.__version__)"". ```. ## CLIF. > Note: CLIF can be built with AT 11.0. Git Repository: [https://github.com/google/clif](https://github.com/google/clif). ```bash. # Prerequisites. cmake --version #3.5+. protoc --version # 3.2.0+ build from source code for both C++ and Python. pip install virtualenv. pip install pyparsing. yum install subversion. yum install ocaml. pip install 'pyparsing>=2.2.0'. pkg-config --libs python # workable. # download source code. cd $HOMEPATH. git clone https://github.com/google/clif.git. cd clif. # set environment. export INSTALL_DIR=""$HOMEPATH/inst"". export CLIFSRC_DIR=""$HOMEPATH/clif"". export LLVM_DIR=""$CLIFSRC_DIR/../clif_backend"". export BUILD_DIR=""$LLVM_DIR/build_matcher"". export PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python"". export PYTHON_LIB_PATH=""$HOMEPATH/inst/lib/python2.7/site-packages"". export PROTOC_PREFIX_PATH=""$(dirname ""$(dirname ""$(which protoc)"")"")"". export CLIF_VIRTUALENV=""$INSTALL_DIR""/clif. export CLIF_PIP=""$CLIF_VIRTUALENV/bin/pip"". virtualenv -p ""$PYTHON_BIN_PATH"" ""$CLIF_VIRTUALENV"". $CLIF_PIP install --upgrade pip. $CLIF_PIP install --upgrade setuptools. # Checkout LLVM and Clang source trees. mkdir -p $LLVM_DIR. cd $LLVM_DIR. svn co https://llvm.org/svn/llvm-project/llvm/trunk@307315 llvm. cd llvm/tools. svn co http",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:12252,deployability,instal,install,12252,"efault float, double and long double types in this glibc. */. #if __HAVE_FLOAT128. -------------------------------------. # build. bazel clean. export PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python"". bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" --action_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH --test_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH --distinct_host_configuration=false. # generate package. bazel-bin/tensorflow/tools/pip_package/build_pip_package $HOMEPATH/tensorflow_package. # install. pip install $HOMEPATH/tensorflow_package/tensorflow-1.12.0-*.whl. # verification. python -c ""import tensorflow as tf; print(tf.__version__)"". ```. ## CLIF. > Note: CLIF can be built with AT 11.0. Git Repository: [https://github.com/google/clif](https://github.com/google/clif). ```bash. # Prerequisites. cmake --version #3.5+. protoc --version # 3.2.0+ build from source code for both C++ and Python. pip install virtualenv. pip install pyparsing. yum install subversion. yum install ocaml. pip install 'pyparsing>=2.2.0'. pkg-config --libs python # workable. # download source code. cd $HOMEPATH. git clone https://github.com/google/clif.git. cd clif. # set environment. export INSTALL_DIR=""$HOMEPATH/inst"". export CLIFSRC_DIR=""$HOMEPATH/clif"". export LLVM_DIR=""$CLIFSRC_DIR/../clif_backend"". export BUILD_DIR=""$LLVM_DIR/build_matcher"". export PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python"". export PYTHON_LIB_PATH=""$HOMEPATH/inst/lib/python2.7/site-packages"". export PROTOC_PREFIX_PATH=""$(dirname ""$(dirname ""$(which protoc)"")"")"". export CLIF_VIRTUALENV=""$INSTALL_DIR""/clif. export CLIF_PIP=""$CLIF_VIRTUALENV/bin/pip"". virtualenv -p ""$PYTHON_BIN_PATH"" ""$CLIF_VIRTUALENV"". $CLIF_PIP install --upgrade pip. $CLIF_PIP install --upgrade setuptools. # Checkout LLVM and Clang source trees. mkdir -p $LLVM_DIR. cd $LLVM_DIR. svn co https://llvm.org/svn/llvm-project/llvm/trunk@307315 llvm. cd llvm/tools. svn co https://llvm.org/svn/llvm-pr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:12275,deployability,instal,install,12275,"d long double types in this glibc. */. #if __HAVE_FLOAT128. -------------------------------------. # build. bazel clean. export PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python"". bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" --action_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH --test_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH --distinct_host_configuration=false. # generate package. bazel-bin/tensorflow/tools/pip_package/build_pip_package $HOMEPATH/tensorflow_package. # install. pip install $HOMEPATH/tensorflow_package/tensorflow-1.12.0-*.whl. # verification. python -c ""import tensorflow as tf; print(tf.__version__)"". ```. ## CLIF. > Note: CLIF can be built with AT 11.0. Git Repository: [https://github.com/google/clif](https://github.com/google/clif). ```bash. # Prerequisites. cmake --version #3.5+. protoc --version # 3.2.0+ build from source code for both C++ and Python. pip install virtualenv. pip install pyparsing. yum install subversion. yum install ocaml. pip install 'pyparsing>=2.2.0'. pkg-config --libs python # workable. # download source code. cd $HOMEPATH. git clone https://github.com/google/clif.git. cd clif. # set environment. export INSTALL_DIR=""$HOMEPATH/inst"". export CLIFSRC_DIR=""$HOMEPATH/clif"". export LLVM_DIR=""$CLIFSRC_DIR/../clif_backend"". export BUILD_DIR=""$LLVM_DIR/build_matcher"". export PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python"". export PYTHON_LIB_PATH=""$HOMEPATH/inst/lib/python2.7/site-packages"". export PROTOC_PREFIX_PATH=""$(dirname ""$(dirname ""$(which protoc)"")"")"". export CLIF_VIRTUALENV=""$INSTALL_DIR""/clif. export CLIF_PIP=""$CLIF_VIRTUALENV/bin/pip"". virtualenv -p ""$PYTHON_BIN_PATH"" ""$CLIF_VIRTUALENV"". $CLIF_PIP install --upgrade pip. $CLIF_PIP install --upgrade setuptools. # Checkout LLVM and Clang source trees. mkdir -p $LLVM_DIR. cd $LLVM_DIR. svn co https://llvm.org/svn/llvm-project/llvm/trunk@307315 llvm. cd llvm/tools. svn co https://llvm.org/svn/llvm-project/cfe/trunk@307315 ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:12299,deployability,instal,install,12299,"his glibc. */. #if __HAVE_FLOAT128. -------------------------------------. # build. bazel clean. export PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python"". bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" --action_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH --test_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH --distinct_host_configuration=false. # generate package. bazel-bin/tensorflow/tools/pip_package/build_pip_package $HOMEPATH/tensorflow_package. # install. pip install $HOMEPATH/tensorflow_package/tensorflow-1.12.0-*.whl. # verification. python -c ""import tensorflow as tf; print(tf.__version__)"". ```. ## CLIF. > Note: CLIF can be built with AT 11.0. Git Repository: [https://github.com/google/clif](https://github.com/google/clif). ```bash. # Prerequisites. cmake --version #3.5+. protoc --version # 3.2.0+ build from source code for both C++ and Python. pip install virtualenv. pip install pyparsing. yum install subversion. yum install ocaml. pip install 'pyparsing>=2.2.0'. pkg-config --libs python # workable. # download source code. cd $HOMEPATH. git clone https://github.com/google/clif.git. cd clif. # set environment. export INSTALL_DIR=""$HOMEPATH/inst"". export CLIFSRC_DIR=""$HOMEPATH/clif"". export LLVM_DIR=""$CLIFSRC_DIR/../clif_backend"". export BUILD_DIR=""$LLVM_DIR/build_matcher"". export PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python"". export PYTHON_LIB_PATH=""$HOMEPATH/inst/lib/python2.7/site-packages"". export PROTOC_PREFIX_PATH=""$(dirname ""$(dirname ""$(which protoc)"")"")"". export CLIF_VIRTUALENV=""$INSTALL_DIR""/clif. export CLIF_PIP=""$CLIF_VIRTUALENV/bin/pip"". virtualenv -p ""$PYTHON_BIN_PATH"" ""$CLIF_VIRTUALENV"". $CLIF_PIP install --upgrade pip. $CLIF_PIP install --upgrade setuptools. # Checkout LLVM and Clang source trees. mkdir -p $LLVM_DIR. cd $LLVM_DIR. svn co https://llvm.org/svn/llvm-project/llvm/trunk@307315 llvm. cd llvm/tools. svn co https://llvm.org/svn/llvm-project/cfe/trunk@307315 clang. ln -s -f -n $CLIF",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:12318,deployability,instal,install,12318,"__HAVE_FLOAT128. -------------------------------------. # build. bazel clean. export PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python"". bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" --action_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH --test_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH --distinct_host_configuration=false. # generate package. bazel-bin/tensorflow/tools/pip_package/build_pip_package $HOMEPATH/tensorflow_package. # install. pip install $HOMEPATH/tensorflow_package/tensorflow-1.12.0-*.whl. # verification. python -c ""import tensorflow as tf; print(tf.__version__)"". ```. ## CLIF. > Note: CLIF can be built with AT 11.0. Git Repository: [https://github.com/google/clif](https://github.com/google/clif). ```bash. # Prerequisites. cmake --version #3.5+. protoc --version # 3.2.0+ build from source code for both C++ and Python. pip install virtualenv. pip install pyparsing. yum install subversion. yum install ocaml. pip install 'pyparsing>=2.2.0'. pkg-config --libs python # workable. # download source code. cd $HOMEPATH. git clone https://github.com/google/clif.git. cd clif. # set environment. export INSTALL_DIR=""$HOMEPATH/inst"". export CLIFSRC_DIR=""$HOMEPATH/clif"". export LLVM_DIR=""$CLIFSRC_DIR/../clif_backend"". export BUILD_DIR=""$LLVM_DIR/build_matcher"". export PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python"". export PYTHON_LIB_PATH=""$HOMEPATH/inst/lib/python2.7/site-packages"". export PROTOC_PREFIX_PATH=""$(dirname ""$(dirname ""$(which protoc)"")"")"". export CLIF_VIRTUALENV=""$INSTALL_DIR""/clif. export CLIF_PIP=""$CLIF_VIRTUALENV/bin/pip"". virtualenv -p ""$PYTHON_BIN_PATH"" ""$CLIF_VIRTUALENV"". $CLIF_PIP install --upgrade pip. $CLIF_PIP install --upgrade setuptools. # Checkout LLVM and Clang source trees. mkdir -p $LLVM_DIR. cd $LLVM_DIR. svn co https://llvm.org/svn/llvm-project/llvm/trunk@307315 llvm. cd llvm/tools. svn co https://llvm.org/svn/llvm-project/cfe/trunk@307315 clang. ln -s -f -n $CLIFSRC_DIR/clif clif. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:13004,deployability,instal,install,13004,"h AT 11.0. Git Repository: [https://github.com/google/clif](https://github.com/google/clif). ```bash. # Prerequisites. cmake --version #3.5+. protoc --version # 3.2.0+ build from source code for both C++ and Python. pip install virtualenv. pip install pyparsing. yum install subversion. yum install ocaml. pip install 'pyparsing>=2.2.0'. pkg-config --libs python # workable. # download source code. cd $HOMEPATH. git clone https://github.com/google/clif.git. cd clif. # set environment. export INSTALL_DIR=""$HOMEPATH/inst"". export CLIFSRC_DIR=""$HOMEPATH/clif"". export LLVM_DIR=""$CLIFSRC_DIR/../clif_backend"". export BUILD_DIR=""$LLVM_DIR/build_matcher"". export PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python"". export PYTHON_LIB_PATH=""$HOMEPATH/inst/lib/python2.7/site-packages"". export PROTOC_PREFIX_PATH=""$(dirname ""$(dirname ""$(which protoc)"")"")"". export CLIF_VIRTUALENV=""$INSTALL_DIR""/clif. export CLIF_PIP=""$CLIF_VIRTUALENV/bin/pip"". virtualenv -p ""$PYTHON_BIN_PATH"" ""$CLIF_VIRTUALENV"". $CLIF_PIP install --upgrade pip. $CLIF_PIP install --upgrade setuptools. # Checkout LLVM and Clang source trees. mkdir -p $LLVM_DIR. cd $LLVM_DIR. svn co https://llvm.org/svn/llvm-project/llvm/trunk@307315 llvm. cd llvm/tools. svn co https://llvm.org/svn/llvm-project/cfe/trunk@307315 clang. ln -s -f -n $CLIFSRC_DIR/clif clif. # Builds must be done outside of the LLVM tree. mkdir -p $BUILD_DIR. cd $BUILD_DIR. # Note to remove -DLLVM_TARGETS_TO_BUILD=X86. # ""rm CMakeCache.txt"" to remove cmake cache. cmake -DCMAKE_INSTALL_PREFIX=""$CLIF_VIRTUALENV/clang"" \. -DCMAKE_PREFIX_PATH=""$PROTOC_PREFIX_PATH"" \. -DLLVM_INSTALL_TOOLCHAIN_ONLY=true \. -DCMAKE_BUILD_TYPE=Release \. -DLLVM_BUILD_DOCS=false \. -DLLVM_TARGETS_TO_BUILD=PowerPC \. -DPYTHON_INCLUDE_DIR=""$HOMEPATH/inst/include/python2.7"" \. -DPYTHON_LIBRARY=""$HOMEPATH/inst/lib/libpython2.7.so"" \. -DPYTHON_EXECUTABLE=""$HOMEPATH/inst/bin/python"" \. ""$LLVM_DIR/llvm"". make -j20 clif-matcher. # export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH. make ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:13014,deployability,upgrad,upgrade,13014," Git Repository: [https://github.com/google/clif](https://github.com/google/clif). ```bash. # Prerequisites. cmake --version #3.5+. protoc --version # 3.2.0+ build from source code for both C++ and Python. pip install virtualenv. pip install pyparsing. yum install subversion. yum install ocaml. pip install 'pyparsing>=2.2.0'. pkg-config --libs python # workable. # download source code. cd $HOMEPATH. git clone https://github.com/google/clif.git. cd clif. # set environment. export INSTALL_DIR=""$HOMEPATH/inst"". export CLIFSRC_DIR=""$HOMEPATH/clif"". export LLVM_DIR=""$CLIFSRC_DIR/../clif_backend"". export BUILD_DIR=""$LLVM_DIR/build_matcher"". export PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python"". export PYTHON_LIB_PATH=""$HOMEPATH/inst/lib/python2.7/site-packages"". export PROTOC_PREFIX_PATH=""$(dirname ""$(dirname ""$(which protoc)"")"")"". export CLIF_VIRTUALENV=""$INSTALL_DIR""/clif. export CLIF_PIP=""$CLIF_VIRTUALENV/bin/pip"". virtualenv -p ""$PYTHON_BIN_PATH"" ""$CLIF_VIRTUALENV"". $CLIF_PIP install --upgrade pip. $CLIF_PIP install --upgrade setuptools. # Checkout LLVM and Clang source trees. mkdir -p $LLVM_DIR. cd $LLVM_DIR. svn co https://llvm.org/svn/llvm-project/llvm/trunk@307315 llvm. cd llvm/tools. svn co https://llvm.org/svn/llvm-project/cfe/trunk@307315 clang. ln -s -f -n $CLIFSRC_DIR/clif clif. # Builds must be done outside of the LLVM tree. mkdir -p $BUILD_DIR. cd $BUILD_DIR. # Note to remove -DLLVM_TARGETS_TO_BUILD=X86. # ""rm CMakeCache.txt"" to remove cmake cache. cmake -DCMAKE_INSTALL_PREFIX=""$CLIF_VIRTUALENV/clang"" \. -DCMAKE_PREFIX_PATH=""$PROTOC_PREFIX_PATH"" \. -DLLVM_INSTALL_TOOLCHAIN_ONLY=true \. -DCMAKE_BUILD_TYPE=Release \. -DLLVM_BUILD_DOCS=false \. -DLLVM_TARGETS_TO_BUILD=PowerPC \. -DPYTHON_INCLUDE_DIR=""$HOMEPATH/inst/include/python2.7"" \. -DPYTHON_LIBRARY=""$HOMEPATH/inst/lib/libpython2.7.so"" \. -DPYTHON_EXECUTABLE=""$HOMEPATH/inst/bin/python"" \. ""$LLVM_DIR/llvm"". make -j20 clif-matcher. # export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH. make -j20 clif_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:13037,deployability,instal,install,13037,"://github.com/google/clif](https://github.com/google/clif). ```bash. # Prerequisites. cmake --version #3.5+. protoc --version # 3.2.0+ build from source code for both C++ and Python. pip install virtualenv. pip install pyparsing. yum install subversion. yum install ocaml. pip install 'pyparsing>=2.2.0'. pkg-config --libs python # workable. # download source code. cd $HOMEPATH. git clone https://github.com/google/clif.git. cd clif. # set environment. export INSTALL_DIR=""$HOMEPATH/inst"". export CLIFSRC_DIR=""$HOMEPATH/clif"". export LLVM_DIR=""$CLIFSRC_DIR/../clif_backend"". export BUILD_DIR=""$LLVM_DIR/build_matcher"". export PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python"". export PYTHON_LIB_PATH=""$HOMEPATH/inst/lib/python2.7/site-packages"". export PROTOC_PREFIX_PATH=""$(dirname ""$(dirname ""$(which protoc)"")"")"". export CLIF_VIRTUALENV=""$INSTALL_DIR""/clif. export CLIF_PIP=""$CLIF_VIRTUALENV/bin/pip"". virtualenv -p ""$PYTHON_BIN_PATH"" ""$CLIF_VIRTUALENV"". $CLIF_PIP install --upgrade pip. $CLIF_PIP install --upgrade setuptools. # Checkout LLVM and Clang source trees. mkdir -p $LLVM_DIR. cd $LLVM_DIR. svn co https://llvm.org/svn/llvm-project/llvm/trunk@307315 llvm. cd llvm/tools. svn co https://llvm.org/svn/llvm-project/cfe/trunk@307315 clang. ln -s -f -n $CLIFSRC_DIR/clif clif. # Builds must be done outside of the LLVM tree. mkdir -p $BUILD_DIR. cd $BUILD_DIR. # Note to remove -DLLVM_TARGETS_TO_BUILD=X86. # ""rm CMakeCache.txt"" to remove cmake cache. cmake -DCMAKE_INSTALL_PREFIX=""$CLIF_VIRTUALENV/clang"" \. -DCMAKE_PREFIX_PATH=""$PROTOC_PREFIX_PATH"" \. -DLLVM_INSTALL_TOOLCHAIN_ONLY=true \. -DCMAKE_BUILD_TYPE=Release \. -DLLVM_BUILD_DOCS=false \. -DLLVM_TARGETS_TO_BUILD=PowerPC \. -DPYTHON_INCLUDE_DIR=""$HOMEPATH/inst/include/python2.7"" \. -DPYTHON_LIBRARY=""$HOMEPATH/inst/lib/libpython2.7.so"" \. -DPYTHON_EXECUTABLE=""$HOMEPATH/inst/bin/python"" \. ""$LLVM_DIR/llvm"". make -j20 clif-matcher. # export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH. make -j20 clif_python_utils_proto_util",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:13047,deployability,upgrad,upgrade,13047,"com/google/clif](https://github.com/google/clif). ```bash. # Prerequisites. cmake --version #3.5+. protoc --version # 3.2.0+ build from source code for both C++ and Python. pip install virtualenv. pip install pyparsing. yum install subversion. yum install ocaml. pip install 'pyparsing>=2.2.0'. pkg-config --libs python # workable. # download source code. cd $HOMEPATH. git clone https://github.com/google/clif.git. cd clif. # set environment. export INSTALL_DIR=""$HOMEPATH/inst"". export CLIFSRC_DIR=""$HOMEPATH/clif"". export LLVM_DIR=""$CLIFSRC_DIR/../clif_backend"". export BUILD_DIR=""$LLVM_DIR/build_matcher"". export PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python"". export PYTHON_LIB_PATH=""$HOMEPATH/inst/lib/python2.7/site-packages"". export PROTOC_PREFIX_PATH=""$(dirname ""$(dirname ""$(which protoc)"")"")"". export CLIF_VIRTUALENV=""$INSTALL_DIR""/clif. export CLIF_PIP=""$CLIF_VIRTUALENV/bin/pip"". virtualenv -p ""$PYTHON_BIN_PATH"" ""$CLIF_VIRTUALENV"". $CLIF_PIP install --upgrade pip. $CLIF_PIP install --upgrade setuptools. # Checkout LLVM and Clang source trees. mkdir -p $LLVM_DIR. cd $LLVM_DIR. svn co https://llvm.org/svn/llvm-project/llvm/trunk@307315 llvm. cd llvm/tools. svn co https://llvm.org/svn/llvm-project/cfe/trunk@307315 clang. ln -s -f -n $CLIFSRC_DIR/clif clif. # Builds must be done outside of the LLVM tree. mkdir -p $BUILD_DIR. cd $BUILD_DIR. # Note to remove -DLLVM_TARGETS_TO_BUILD=X86. # ""rm CMakeCache.txt"" to remove cmake cache. cmake -DCMAKE_INSTALL_PREFIX=""$CLIF_VIRTUALENV/clang"" \. -DCMAKE_PREFIX_PATH=""$PROTOC_PREFIX_PATH"" \. -DLLVM_INSTALL_TOOLCHAIN_ONLY=true \. -DCMAKE_BUILD_TYPE=Release \. -DLLVM_BUILD_DOCS=false \. -DLLVM_TARGETS_TO_BUILD=PowerPC \. -DPYTHON_INCLUDE_DIR=""$HOMEPATH/inst/include/python2.7"" \. -DPYTHON_LIBRARY=""$HOMEPATH/inst/lib/libpython2.7.so"" \. -DPYTHON_EXECUTABLE=""$HOMEPATH/inst/bin/python"" \. ""$LLVM_DIR/llvm"". make -j20 clif-matcher. # export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH. make -j20 clif_python_utils_proto_util. make -j2",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:13324,deployability,Build,Builds,13324,"pyparsing>=2.2.0'. pkg-config --libs python # workable. # download source code. cd $HOMEPATH. git clone https://github.com/google/clif.git. cd clif. # set environment. export INSTALL_DIR=""$HOMEPATH/inst"". export CLIFSRC_DIR=""$HOMEPATH/clif"". export LLVM_DIR=""$CLIFSRC_DIR/../clif_backend"". export BUILD_DIR=""$LLVM_DIR/build_matcher"". export PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python"". export PYTHON_LIB_PATH=""$HOMEPATH/inst/lib/python2.7/site-packages"". export PROTOC_PREFIX_PATH=""$(dirname ""$(dirname ""$(which protoc)"")"")"". export CLIF_VIRTUALENV=""$INSTALL_DIR""/clif. export CLIF_PIP=""$CLIF_VIRTUALENV/bin/pip"". virtualenv -p ""$PYTHON_BIN_PATH"" ""$CLIF_VIRTUALENV"". $CLIF_PIP install --upgrade pip. $CLIF_PIP install --upgrade setuptools. # Checkout LLVM and Clang source trees. mkdir -p $LLVM_DIR. cd $LLVM_DIR. svn co https://llvm.org/svn/llvm-project/llvm/trunk@307315 llvm. cd llvm/tools. svn co https://llvm.org/svn/llvm-project/cfe/trunk@307315 clang. ln -s -f -n $CLIFSRC_DIR/clif clif. # Builds must be done outside of the LLVM tree. mkdir -p $BUILD_DIR. cd $BUILD_DIR. # Note to remove -DLLVM_TARGETS_TO_BUILD=X86. # ""rm CMakeCache.txt"" to remove cmake cache. cmake -DCMAKE_INSTALL_PREFIX=""$CLIF_VIRTUALENV/clang"" \. -DCMAKE_PREFIX_PATH=""$PROTOC_PREFIX_PATH"" \. -DLLVM_INSTALL_TOOLCHAIN_ONLY=true \. -DCMAKE_BUILD_TYPE=Release \. -DLLVM_BUILD_DOCS=false \. -DLLVM_TARGETS_TO_BUILD=PowerPC \. -DPYTHON_INCLUDE_DIR=""$HOMEPATH/inst/include/python2.7"" \. -DPYTHON_LIBRARY=""$HOMEPATH/inst/lib/libpython2.7.so"" \. -DPYTHON_EXECUTABLE=""$HOMEPATH/inst/bin/python"" \. ""$LLVM_DIR/llvm"". make -j20 clif-matcher. # export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH. make -j20 clif_python_utils_proto_util. make -j20 install. ## Get back to the CLIF Python directory and have pip run setup.py. cd ""$CLIFSRC_DIR"". # Grab the python compiled .proto. cp ""$BUILD_DIR/tools/clif/protos/ast_pb2.py"" clif/protos/. # Grab CLIF generated wrapper implementation for proto_util. cp ""$BUILD_DIR/tools/cl",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:13656,deployability,Releas,Release,13656," export PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python"". export PYTHON_LIB_PATH=""$HOMEPATH/inst/lib/python2.7/site-packages"". export PROTOC_PREFIX_PATH=""$(dirname ""$(dirname ""$(which protoc)"")"")"". export CLIF_VIRTUALENV=""$INSTALL_DIR""/clif. export CLIF_PIP=""$CLIF_VIRTUALENV/bin/pip"". virtualenv -p ""$PYTHON_BIN_PATH"" ""$CLIF_VIRTUALENV"". $CLIF_PIP install --upgrade pip. $CLIF_PIP install --upgrade setuptools. # Checkout LLVM and Clang source trees. mkdir -p $LLVM_DIR. cd $LLVM_DIR. svn co https://llvm.org/svn/llvm-project/llvm/trunk@307315 llvm. cd llvm/tools. svn co https://llvm.org/svn/llvm-project/cfe/trunk@307315 clang. ln -s -f -n $CLIFSRC_DIR/clif clif. # Builds must be done outside of the LLVM tree. mkdir -p $BUILD_DIR. cd $BUILD_DIR. # Note to remove -DLLVM_TARGETS_TO_BUILD=X86. # ""rm CMakeCache.txt"" to remove cmake cache. cmake -DCMAKE_INSTALL_PREFIX=""$CLIF_VIRTUALENV/clang"" \. -DCMAKE_PREFIX_PATH=""$PROTOC_PREFIX_PATH"" \. -DLLVM_INSTALL_TOOLCHAIN_ONLY=true \. -DCMAKE_BUILD_TYPE=Release \. -DLLVM_BUILD_DOCS=false \. -DLLVM_TARGETS_TO_BUILD=PowerPC \. -DPYTHON_INCLUDE_DIR=""$HOMEPATH/inst/include/python2.7"" \. -DPYTHON_LIBRARY=""$HOMEPATH/inst/lib/libpython2.7.so"" \. -DPYTHON_EXECUTABLE=""$HOMEPATH/inst/bin/python"" \. ""$LLVM_DIR/llvm"". make -j20 clif-matcher. # export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH. make -j20 clif_python_utils_proto_util. make -j20 install. ## Get back to the CLIF Python directory and have pip run setup.py. cd ""$CLIFSRC_DIR"". # Grab the python compiled .proto. cp ""$BUILD_DIR/tools/clif/protos/ast_pb2.py"" clif/protos/. # Grab CLIF generated wrapper implementation for proto_util. cp ""$BUILD_DIR/tools/clif/python/utils/proto_util.cc"" clif/python/utils/. cp ""$BUILD_DIR/tools/clif/python/utils/proto_util.h"" clif/python/utils/. cp ""$BUILD_DIR/tools/clif/python/utils/proto_util.init.cc"" clif/python/utils/. # install. export C_INCLUDE_PATH=/home/qilibj/inst/include. export CPLUS_INCLUDE_PATH=/home/qilibj/inst/include. ""$CLIF_PIP"" insta",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:14053,deployability,instal,install,14053,"tuptools. # Checkout LLVM and Clang source trees. mkdir -p $LLVM_DIR. cd $LLVM_DIR. svn co https://llvm.org/svn/llvm-project/llvm/trunk@307315 llvm. cd llvm/tools. svn co https://llvm.org/svn/llvm-project/cfe/trunk@307315 clang. ln -s -f -n $CLIFSRC_DIR/clif clif. # Builds must be done outside of the LLVM tree. mkdir -p $BUILD_DIR. cd $BUILD_DIR. # Note to remove -DLLVM_TARGETS_TO_BUILD=X86. # ""rm CMakeCache.txt"" to remove cmake cache. cmake -DCMAKE_INSTALL_PREFIX=""$CLIF_VIRTUALENV/clang"" \. -DCMAKE_PREFIX_PATH=""$PROTOC_PREFIX_PATH"" \. -DLLVM_INSTALL_TOOLCHAIN_ONLY=true \. -DCMAKE_BUILD_TYPE=Release \. -DLLVM_BUILD_DOCS=false \. -DLLVM_TARGETS_TO_BUILD=PowerPC \. -DPYTHON_INCLUDE_DIR=""$HOMEPATH/inst/include/python2.7"" \. -DPYTHON_LIBRARY=""$HOMEPATH/inst/lib/libpython2.7.so"" \. -DPYTHON_EXECUTABLE=""$HOMEPATH/inst/bin/python"" \. ""$LLVM_DIR/llvm"". make -j20 clif-matcher. # export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH. make -j20 clif_python_utils_proto_util. make -j20 install. ## Get back to the CLIF Python directory and have pip run setup.py. cd ""$CLIFSRC_DIR"". # Grab the python compiled .proto. cp ""$BUILD_DIR/tools/clif/protos/ast_pb2.py"" clif/protos/. # Grab CLIF generated wrapper implementation for proto_util. cp ""$BUILD_DIR/tools/clif/python/utils/proto_util.cc"" clif/python/utils/. cp ""$BUILD_DIR/tools/clif/python/utils/proto_util.h"" clif/python/utils/. cp ""$BUILD_DIR/tools/clif/python/utils/proto_util.init.cc"" clif/python/utils/. # install. export C_INCLUDE_PATH=/home/qilibj/inst/include. export CPLUS_INCLUDE_PATH=/home/qilibj/inst/include. ""$CLIF_PIP"" install . # echo ""SUCCESS - To use pyclif, run $CLIF_VIRTUALENV/bin/pyclif."". python setup.py bdist_wheel. # Note: pyclif should be installed into virtualenv. ""$CLIF_PIP"" install pyclif-0.3-cp27-none-linux_ppc64le.whl. pip install dist/pyclif-0.3-cp27-cp27m-linux_ppc64le.whl. # verify. python -c ""from clif.python.proto import start"". # link for deepvariant. ln -s /home/qilibj/inst/clif /usr/local/. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:14532,deployability,instal,install,14532,"TUALENV/clang"" \. -DCMAKE_PREFIX_PATH=""$PROTOC_PREFIX_PATH"" \. -DLLVM_INSTALL_TOOLCHAIN_ONLY=true \. -DCMAKE_BUILD_TYPE=Release \. -DLLVM_BUILD_DOCS=false \. -DLLVM_TARGETS_TO_BUILD=PowerPC \. -DPYTHON_INCLUDE_DIR=""$HOMEPATH/inst/include/python2.7"" \. -DPYTHON_LIBRARY=""$HOMEPATH/inst/lib/libpython2.7.so"" \. -DPYTHON_EXECUTABLE=""$HOMEPATH/inst/bin/python"" \. ""$LLVM_DIR/llvm"". make -j20 clif-matcher. # export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH. make -j20 clif_python_utils_proto_util. make -j20 install. ## Get back to the CLIF Python directory and have pip run setup.py. cd ""$CLIFSRC_DIR"". # Grab the python compiled .proto. cp ""$BUILD_DIR/tools/clif/protos/ast_pb2.py"" clif/protos/. # Grab CLIF generated wrapper implementation for proto_util. cp ""$BUILD_DIR/tools/clif/python/utils/proto_util.cc"" clif/python/utils/. cp ""$BUILD_DIR/tools/clif/python/utils/proto_util.h"" clif/python/utils/. cp ""$BUILD_DIR/tools/clif/python/utils/proto_util.init.cc"" clif/python/utils/. # install. export C_INCLUDE_PATH=/home/qilibj/inst/include. export CPLUS_INCLUDE_PATH=/home/qilibj/inst/include. ""$CLIF_PIP"" install . # echo ""SUCCESS - To use pyclif, run $CLIF_VIRTUALENV/bin/pyclif."". python setup.py bdist_wheel. # Note: pyclif should be installed into virtualenv. ""$CLIF_PIP"" install pyclif-0.3-cp27-none-linux_ppc64le.whl. pip install dist/pyclif-0.3-cp27-cp27m-linux_ppc64le.whl. # verify. python -c ""from clif.python.proto import start"". # link for deepvariant. ln -s /home/qilibj/inst/clif /usr/local/. ```. ## Opencv-python 3.4.5.20. Git repository: [https://github.com/skvark/opencv-python](https://github.com/skvark/opencv-python). ```bash. # Checkout repository and submodules. git clone https://github.com/skvark/opencv-python.git. cd opencv-python/. # fetch the tags to your local repository. git fetch --all --tags --prune. # check out tag 3.4.5.20. git checkout tags/20. # load submoduel. git submodule update --init --recursive. # Dependency. pip install pyparsing. yum ins",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:14655,deployability,instal,install,14655,"ease \. -DLLVM_BUILD_DOCS=false \. -DLLVM_TARGETS_TO_BUILD=PowerPC \. -DPYTHON_INCLUDE_DIR=""$HOMEPATH/inst/include/python2.7"" \. -DPYTHON_LIBRARY=""$HOMEPATH/inst/lib/libpython2.7.so"" \. -DPYTHON_EXECUTABLE=""$HOMEPATH/inst/bin/python"" \. ""$LLVM_DIR/llvm"". make -j20 clif-matcher. # export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH. make -j20 clif_python_utils_proto_util. make -j20 install. ## Get back to the CLIF Python directory and have pip run setup.py. cd ""$CLIFSRC_DIR"". # Grab the python compiled .proto. cp ""$BUILD_DIR/tools/clif/protos/ast_pb2.py"" clif/protos/. # Grab CLIF generated wrapper implementation for proto_util. cp ""$BUILD_DIR/tools/clif/python/utils/proto_util.cc"" clif/python/utils/. cp ""$BUILD_DIR/tools/clif/python/utils/proto_util.h"" clif/python/utils/. cp ""$BUILD_DIR/tools/clif/python/utils/proto_util.init.cc"" clif/python/utils/. # install. export C_INCLUDE_PATH=/home/qilibj/inst/include. export CPLUS_INCLUDE_PATH=/home/qilibj/inst/include. ""$CLIF_PIP"" install . # echo ""SUCCESS - To use pyclif, run $CLIF_VIRTUALENV/bin/pyclif."". python setup.py bdist_wheel. # Note: pyclif should be installed into virtualenv. ""$CLIF_PIP"" install pyclif-0.3-cp27-none-linux_ppc64le.whl. pip install dist/pyclif-0.3-cp27-cp27m-linux_ppc64le.whl. # verify. python -c ""from clif.python.proto import start"". # link for deepvariant. ln -s /home/qilibj/inst/clif /usr/local/. ```. ## Opencv-python 3.4.5.20. Git repository: [https://github.com/skvark/opencv-python](https://github.com/skvark/opencv-python). ```bash. # Checkout repository and submodules. git clone https://github.com/skvark/opencv-python.git. cd opencv-python/. # fetch the tags to your local repository. git fetch --all --tags --prune. # check out tag 3.4.5.20. git checkout tags/20. # load submoduel. git submodule update --init --recursive. # Dependency. pip install pyparsing. yum install qt-devel. # Build. python setup.py bdist_wheel. # Insatll. pip install dist/opencv_python-3.4.5.20-cp27-cp27mu-linux_p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:14787,deployability,instal,installed,14787,"THON_LIBRARY=""$HOMEPATH/inst/lib/libpython2.7.so"" \. -DPYTHON_EXECUTABLE=""$HOMEPATH/inst/bin/python"" \. ""$LLVM_DIR/llvm"". make -j20 clif-matcher. # export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH. make -j20 clif_python_utils_proto_util. make -j20 install. ## Get back to the CLIF Python directory and have pip run setup.py. cd ""$CLIFSRC_DIR"". # Grab the python compiled .proto. cp ""$BUILD_DIR/tools/clif/protos/ast_pb2.py"" clif/protos/. # Grab CLIF generated wrapper implementation for proto_util. cp ""$BUILD_DIR/tools/clif/python/utils/proto_util.cc"" clif/python/utils/. cp ""$BUILD_DIR/tools/clif/python/utils/proto_util.h"" clif/python/utils/. cp ""$BUILD_DIR/tools/clif/python/utils/proto_util.init.cc"" clif/python/utils/. # install. export C_INCLUDE_PATH=/home/qilibj/inst/include. export CPLUS_INCLUDE_PATH=/home/qilibj/inst/include. ""$CLIF_PIP"" install . # echo ""SUCCESS - To use pyclif, run $CLIF_VIRTUALENV/bin/pyclif."". python setup.py bdist_wheel. # Note: pyclif should be installed into virtualenv. ""$CLIF_PIP"" install pyclif-0.3-cp27-none-linux_ppc64le.whl. pip install dist/pyclif-0.3-cp27-cp27m-linux_ppc64le.whl. # verify. python -c ""from clif.python.proto import start"". # link for deepvariant. ln -s /home/qilibj/inst/clif /usr/local/. ```. ## Opencv-python 3.4.5.20. Git repository: [https://github.com/skvark/opencv-python](https://github.com/skvark/opencv-python). ```bash. # Checkout repository and submodules. git clone https://github.com/skvark/opencv-python.git. cd opencv-python/. # fetch the tags to your local repository. git fetch --all --tags --prune. # check out tag 3.4.5.20. git checkout tags/20. # load submoduel. git submodule update --init --recursive. # Dependency. pip install pyparsing. yum install qt-devel. # Build. python setup.py bdist_wheel. # Insatll. pip install dist/opencv_python-3.4.5.20-cp27-cp27mu-linux_ppc64le.whl. # Verify in a new session. python -c ""import cv2"". ```. ## DV Prerequisite. ```bash. ####################################",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:14826,deployability,instal,install,14826,"thon2.7.so"" \. -DPYTHON_EXECUTABLE=""$HOMEPATH/inst/bin/python"" \. ""$LLVM_DIR/llvm"". make -j20 clif-matcher. # export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH. make -j20 clif_python_utils_proto_util. make -j20 install. ## Get back to the CLIF Python directory and have pip run setup.py. cd ""$CLIFSRC_DIR"". # Grab the python compiled .proto. cp ""$BUILD_DIR/tools/clif/protos/ast_pb2.py"" clif/protos/. # Grab CLIF generated wrapper implementation for proto_util. cp ""$BUILD_DIR/tools/clif/python/utils/proto_util.cc"" clif/python/utils/. cp ""$BUILD_DIR/tools/clif/python/utils/proto_util.h"" clif/python/utils/. cp ""$BUILD_DIR/tools/clif/python/utils/proto_util.init.cc"" clif/python/utils/. # install. export C_INCLUDE_PATH=/home/qilibj/inst/include. export CPLUS_INCLUDE_PATH=/home/qilibj/inst/include. ""$CLIF_PIP"" install . # echo ""SUCCESS - To use pyclif, run $CLIF_VIRTUALENV/bin/pyclif."". python setup.py bdist_wheel. # Note: pyclif should be installed into virtualenv. ""$CLIF_PIP"" install pyclif-0.3-cp27-none-linux_ppc64le.whl. pip install dist/pyclif-0.3-cp27-cp27m-linux_ppc64le.whl. # verify. python -c ""from clif.python.proto import start"". # link for deepvariant. ln -s /home/qilibj/inst/clif /usr/local/. ```. ## Opencv-python 3.4.5.20. Git repository: [https://github.com/skvark/opencv-python](https://github.com/skvark/opencv-python). ```bash. # Checkout repository and submodules. git clone https://github.com/skvark/opencv-python.git. cd opencv-python/. # fetch the tags to your local repository. git fetch --all --tags --prune. # check out tag 3.4.5.20. git checkout tags/20. # load submoduel. git submodule update --init --recursive. # Dependency. pip install pyparsing. yum install qt-devel. # Build. python setup.py bdist_wheel. # Insatll. pip install dist/opencv_python-3.4.5.20-cp27-cp27mu-linux_ppc64le.whl. # Verify in a new session. python -c ""import cv2"". ```. ## DV Prerequisite. ```bash. ####################################################################. # mi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:14878,deployability,instal,install,14878,"in/python"" \. ""$LLVM_DIR/llvm"". make -j20 clif-matcher. # export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH. make -j20 clif_python_utils_proto_util. make -j20 install. ## Get back to the CLIF Python directory and have pip run setup.py. cd ""$CLIFSRC_DIR"". # Grab the python compiled .proto. cp ""$BUILD_DIR/tools/clif/protos/ast_pb2.py"" clif/protos/. # Grab CLIF generated wrapper implementation for proto_util. cp ""$BUILD_DIR/tools/clif/python/utils/proto_util.cc"" clif/python/utils/. cp ""$BUILD_DIR/tools/clif/python/utils/proto_util.h"" clif/python/utils/. cp ""$BUILD_DIR/tools/clif/python/utils/proto_util.init.cc"" clif/python/utils/. # install. export C_INCLUDE_PATH=/home/qilibj/inst/include. export CPLUS_INCLUDE_PATH=/home/qilibj/inst/include. ""$CLIF_PIP"" install . # echo ""SUCCESS - To use pyclif, run $CLIF_VIRTUALENV/bin/pyclif."". python setup.py bdist_wheel. # Note: pyclif should be installed into virtualenv. ""$CLIF_PIP"" install pyclif-0.3-cp27-none-linux_ppc64le.whl. pip install dist/pyclif-0.3-cp27-cp27m-linux_ppc64le.whl. # verify. python -c ""from clif.python.proto import start"". # link for deepvariant. ln -s /home/qilibj/inst/clif /usr/local/. ```. ## Opencv-python 3.4.5.20. Git repository: [https://github.com/skvark/opencv-python](https://github.com/skvark/opencv-python). ```bash. # Checkout repository and submodules. git clone https://github.com/skvark/opencv-python.git. cd opencv-python/. # fetch the tags to your local repository. git fetch --all --tags --prune. # check out tag 3.4.5.20. git checkout tags/20. # load submoduel. git submodule update --init --recursive. # Dependency. pip install pyparsing. yum install qt-devel. # Build. python setup.py bdist_wheel. # Insatll. pip install dist/opencv_python-3.4.5.20-cp27-cp27mu-linux_ppc64le.whl. # Verify in a new session. python -c ""import cv2"". ```. ## DV Prerequisite. ```bash. ####################################################################. # misc setup. ##########################################",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:15465,deployability,updat,update,15465,"ols/clif/python/utils/proto_util.init.cc"" clif/python/utils/. # install. export C_INCLUDE_PATH=/home/qilibj/inst/include. export CPLUS_INCLUDE_PATH=/home/qilibj/inst/include. ""$CLIF_PIP"" install . # echo ""SUCCESS - To use pyclif, run $CLIF_VIRTUALENV/bin/pyclif."". python setup.py bdist_wheel. # Note: pyclif should be installed into virtualenv. ""$CLIF_PIP"" install pyclif-0.3-cp27-none-linux_ppc64le.whl. pip install dist/pyclif-0.3-cp27-cp27m-linux_ppc64le.whl. # verify. python -c ""from clif.python.proto import start"". # link for deepvariant. ln -s /home/qilibj/inst/clif /usr/local/. ```. ## Opencv-python 3.4.5.20. Git repository: [https://github.com/skvark/opencv-python](https://github.com/skvark/opencv-python). ```bash. # Checkout repository and submodules. git clone https://github.com/skvark/opencv-python.git. cd opencv-python/. # fetch the tags to your local repository. git fetch --all --tags --prune. # check out tag 3.4.5.20. git checkout tags/20. # load submoduel. git submodule update --init --recursive. # Dependency. pip install pyparsing. yum install qt-devel. # Build. python setup.py bdist_wheel. # Insatll. pip install dist/opencv_python-3.4.5.20-cp27-cp27mu-linux_ppc64le.whl. # Verify in a new session. python -c ""import cv2"". ```. ## DV Prerequisite. ```bash. ####################################################################. # misc setup. ####################################################################. # development packages. yum install python2-pkgconfig zip zlib-devel unzip curl -y. # python packages. yum install python-devel python-pip python-wheel -y. ####################################################################. # python packages. ####################################################################. # python 2 required. echo ""$(python --version)"". echo ""$(pip --version)"". # Install python packages. pip install contextlib2. pip install enum34. pip install intervaltree. pip install 'mock>=2.0.0'. # pip install 'numpy==1.14' => skip as insta",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:15494,deployability,Depend,Dependency,15494,"l.init.cc"" clif/python/utils/. # install. export C_INCLUDE_PATH=/home/qilibj/inst/include. export CPLUS_INCLUDE_PATH=/home/qilibj/inst/include. ""$CLIF_PIP"" install . # echo ""SUCCESS - To use pyclif, run $CLIF_VIRTUALENV/bin/pyclif."". python setup.py bdist_wheel. # Note: pyclif should be installed into virtualenv. ""$CLIF_PIP"" install pyclif-0.3-cp27-none-linux_ppc64le.whl. pip install dist/pyclif-0.3-cp27-cp27m-linux_ppc64le.whl. # verify. python -c ""from clif.python.proto import start"". # link for deepvariant. ln -s /home/qilibj/inst/clif /usr/local/. ```. ## Opencv-python 3.4.5.20. Git repository: [https://github.com/skvark/opencv-python](https://github.com/skvark/opencv-python). ```bash. # Checkout repository and submodules. git clone https://github.com/skvark/opencv-python.git. cd opencv-python/. # fetch the tags to your local repository. git fetch --all --tags --prune. # check out tag 3.4.5.20. git checkout tags/20. # load submoduel. git submodule update --init --recursive. # Dependency. pip install pyparsing. yum install qt-devel. # Build. python setup.py bdist_wheel. # Insatll. pip install dist/opencv_python-3.4.5.20-cp27-cp27mu-linux_ppc64le.whl. # Verify in a new session. python -c ""import cv2"". ```. ## DV Prerequisite. ```bash. ####################################################################. # misc setup. ####################################################################. # development packages. yum install python2-pkgconfig zip zlib-devel unzip curl -y. # python packages. yum install python-devel python-pip python-wheel -y. ####################################################################. # python packages. ####################################################################. # python 2 required. echo ""$(python --version)"". echo ""$(pip --version)"". # Install python packages. pip install contextlib2. pip install enum34. pip install intervaltree. pip install 'mock>=2.0.0'. # pip install 'numpy==1.14' => skip as installed in TF. pip install 'reques",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:15510,deployability,instal,install,15510,"/python/utils/. # install. export C_INCLUDE_PATH=/home/qilibj/inst/include. export CPLUS_INCLUDE_PATH=/home/qilibj/inst/include. ""$CLIF_PIP"" install . # echo ""SUCCESS - To use pyclif, run $CLIF_VIRTUALENV/bin/pyclif."". python setup.py bdist_wheel. # Note: pyclif should be installed into virtualenv. ""$CLIF_PIP"" install pyclif-0.3-cp27-none-linux_ppc64le.whl. pip install dist/pyclif-0.3-cp27-cp27m-linux_ppc64le.whl. # verify. python -c ""from clif.python.proto import start"". # link for deepvariant. ln -s /home/qilibj/inst/clif /usr/local/. ```. ## Opencv-python 3.4.5.20. Git repository: [https://github.com/skvark/opencv-python](https://github.com/skvark/opencv-python). ```bash. # Checkout repository and submodules. git clone https://github.com/skvark/opencv-python.git. cd opencv-python/. # fetch the tags to your local repository. git fetch --all --tags --prune. # check out tag 3.4.5.20. git checkout tags/20. # load submoduel. git submodule update --init --recursive. # Dependency. pip install pyparsing. yum install qt-devel. # Build. python setup.py bdist_wheel. # Insatll. pip install dist/opencv_python-3.4.5.20-cp27-cp27mu-linux_ppc64le.whl. # Verify in a new session. python -c ""import cv2"". ```. ## DV Prerequisite. ```bash. ####################################################################. # misc setup. ####################################################################. # development packages. yum install python2-pkgconfig zip zlib-devel unzip curl -y. # python packages. yum install python-devel python-pip python-wheel -y. ####################################################################. # python packages. ####################################################################. # python 2 required. echo ""$(python --version)"". echo ""$(pip --version)"". # Install python packages. pip install contextlib2. pip install enum34. pip install intervaltree. pip install 'mock>=2.0.0'. # pip install 'numpy==1.14' => skip as installed in TF. pip install 'requests>=2.18'. # pi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:15533,deployability,instal,install,15533,"ll. export C_INCLUDE_PATH=/home/qilibj/inst/include. export CPLUS_INCLUDE_PATH=/home/qilibj/inst/include. ""$CLIF_PIP"" install . # echo ""SUCCESS - To use pyclif, run $CLIF_VIRTUALENV/bin/pyclif."". python setup.py bdist_wheel. # Note: pyclif should be installed into virtualenv. ""$CLIF_PIP"" install pyclif-0.3-cp27-none-linux_ppc64le.whl. pip install dist/pyclif-0.3-cp27-cp27m-linux_ppc64le.whl. # verify. python -c ""from clif.python.proto import start"". # link for deepvariant. ln -s /home/qilibj/inst/clif /usr/local/. ```. ## Opencv-python 3.4.5.20. Git repository: [https://github.com/skvark/opencv-python](https://github.com/skvark/opencv-python). ```bash. # Checkout repository and submodules. git clone https://github.com/skvark/opencv-python.git. cd opencv-python/. # fetch the tags to your local repository. git fetch --all --tags --prune. # check out tag 3.4.5.20. git checkout tags/20. # load submoduel. git submodule update --init --recursive. # Dependency. pip install pyparsing. yum install qt-devel. # Build. python setup.py bdist_wheel. # Insatll. pip install dist/opencv_python-3.4.5.20-cp27-cp27mu-linux_ppc64le.whl. # Verify in a new session. python -c ""import cv2"". ```. ## DV Prerequisite. ```bash. ####################################################################. # misc setup. ####################################################################. # development packages. yum install python2-pkgconfig zip zlib-devel unzip curl -y. # python packages. yum install python-devel python-pip python-wheel -y. ####################################################################. # python packages. ####################################################################. # python 2 required. echo ""$(python --version)"". echo ""$(pip --version)"". # Install python packages. pip install contextlib2. pip install enum34. pip install intervaltree. pip install 'mock>=2.0.0'. # pip install 'numpy==1.14' => skip as installed in TF. pip install 'requests>=2.18'. # pip install 'scipy==1.0' ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:15553,deployability,Build,Build,15553,"E_PATH=/home/qilibj/inst/include. export CPLUS_INCLUDE_PATH=/home/qilibj/inst/include. ""$CLIF_PIP"" install . # echo ""SUCCESS - To use pyclif, run $CLIF_VIRTUALENV/bin/pyclif."". python setup.py bdist_wheel. # Note: pyclif should be installed into virtualenv. ""$CLIF_PIP"" install pyclif-0.3-cp27-none-linux_ppc64le.whl. pip install dist/pyclif-0.3-cp27-cp27m-linux_ppc64le.whl. # verify. python -c ""from clif.python.proto import start"". # link for deepvariant. ln -s /home/qilibj/inst/clif /usr/local/. ```. ## Opencv-python 3.4.5.20. Git repository: [https://github.com/skvark/opencv-python](https://github.com/skvark/opencv-python). ```bash. # Checkout repository and submodules. git clone https://github.com/skvark/opencv-python.git. cd opencv-python/. # fetch the tags to your local repository. git fetch --all --tags --prune. # check out tag 3.4.5.20. git checkout tags/20. # load submoduel. git submodule update --init --recursive. # Dependency. pip install pyparsing. yum install qt-devel. # Build. python setup.py bdist_wheel. # Insatll. pip install dist/opencv_python-3.4.5.20-cp27-cp27mu-linux_ppc64le.whl. # Verify in a new session. python -c ""import cv2"". ```. ## DV Prerequisite. ```bash. ####################################################################. # misc setup. ####################################################################. # development packages. yum install python2-pkgconfig zip zlib-devel unzip curl -y. # python packages. yum install python-devel python-pip python-wheel -y. ####################################################################. # python packages. ####################################################################. # python 2 required. echo ""$(python --version)"". echo ""$(pip --version)"". # Install python packages. pip install contextlib2. pip install enum34. pip install intervaltree. pip install 'mock>=2.0.0'. # pip install 'numpy==1.14' => skip as installed in TF. pip install 'requests>=2.18'. # pip install 'scipy==1.0' => skip as installe",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:15604,deployability,instal,install,15604,"DE_PATH=/home/qilibj/inst/include. ""$CLIF_PIP"" install . # echo ""SUCCESS - To use pyclif, run $CLIF_VIRTUALENV/bin/pyclif."". python setup.py bdist_wheel. # Note: pyclif should be installed into virtualenv. ""$CLIF_PIP"" install pyclif-0.3-cp27-none-linux_ppc64le.whl. pip install dist/pyclif-0.3-cp27-cp27m-linux_ppc64le.whl. # verify. python -c ""from clif.python.proto import start"". # link for deepvariant. ln -s /home/qilibj/inst/clif /usr/local/. ```. ## Opencv-python 3.4.5.20. Git repository: [https://github.com/skvark/opencv-python](https://github.com/skvark/opencv-python). ```bash. # Checkout repository and submodules. git clone https://github.com/skvark/opencv-python.git. cd opencv-python/. # fetch the tags to your local repository. git fetch --all --tags --prune. # check out tag 3.4.5.20. git checkout tags/20. # load submoduel. git submodule update --init --recursive. # Dependency. pip install pyparsing. yum install qt-devel. # Build. python setup.py bdist_wheel. # Insatll. pip install dist/opencv_python-3.4.5.20-cp27-cp27mu-linux_ppc64le.whl. # Verify in a new session. python -c ""import cv2"". ```. ## DV Prerequisite. ```bash. ####################################################################. # misc setup. ####################################################################. # development packages. yum install python2-pkgconfig zip zlib-devel unzip curl -y. # python packages. yum install python-devel python-pip python-wheel -y. ####################################################################. # python packages. ####################################################################. # python 2 required. echo ""$(python --version)"". echo ""$(pip --version)"". # Install python packages. pip install contextlib2. pip install enum34. pip install intervaltree. pip install 'mock>=2.0.0'. # pip install 'numpy==1.14' => skip as installed in TF. pip install 'requests>=2.18'. # pip install 'scipy==1.0' => skip as installed in TF. pip install 'oauth2client>=4.0.0'. pip inst",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:15938,deployability,instal,install,15938,"python -c ""from clif.python.proto import start"". # link for deepvariant. ln -s /home/qilibj/inst/clif /usr/local/. ```. ## Opencv-python 3.4.5.20. Git repository: [https://github.com/skvark/opencv-python](https://github.com/skvark/opencv-python). ```bash. # Checkout repository and submodules. git clone https://github.com/skvark/opencv-python.git. cd opencv-python/. # fetch the tags to your local repository. git fetch --all --tags --prune. # check out tag 3.4.5.20. git checkout tags/20. # load submoduel. git submodule update --init --recursive. # Dependency. pip install pyparsing. yum install qt-devel. # Build. python setup.py bdist_wheel. # Insatll. pip install dist/opencv_python-3.4.5.20-cp27-cp27mu-linux_ppc64le.whl. # Verify in a new session. python -c ""import cv2"". ```. ## DV Prerequisite. ```bash. ####################################################################. # misc setup. ####################################################################. # development packages. yum install python2-pkgconfig zip zlib-devel unzip curl -y. # python packages. yum install python-devel python-pip python-wheel -y. ####################################################################. # python packages. ####################################################################. # python 2 required. echo ""$(python --version)"". echo ""$(pip --version)"". # Install python packages. pip install contextlib2. pip install enum34. pip install intervaltree. pip install 'mock>=2.0.0'. # pip install 'numpy==1.14' => skip as installed in TF. pip install 'requests>=2.18'. # pip install 'scipy==1.0' => skip as installed in TF. pip install 'oauth2client>=4.0.0'. pip install 'crcmod>=1.7'. pip install six. pip install sklearn. pip install pandas. pip install psutil. pip install --upgrade google-api-python-client. ####################################################################. # depend on opencv-python wheel - build from source. ##################################################################",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:16017,deployability,instal,install,16017,"/home/qilibj/inst/clif /usr/local/. ```. ## Opencv-python 3.4.5.20. Git repository: [https://github.com/skvark/opencv-python](https://github.com/skvark/opencv-python). ```bash. # Checkout repository and submodules. git clone https://github.com/skvark/opencv-python.git. cd opencv-python/. # fetch the tags to your local repository. git fetch --all --tags --prune. # check out tag 3.4.5.20. git checkout tags/20. # load submoduel. git submodule update --init --recursive. # Dependency. pip install pyparsing. yum install qt-devel. # Build. python setup.py bdist_wheel. # Insatll. pip install dist/opencv_python-3.4.5.20-cp27-cp27mu-linux_ppc64le.whl. # Verify in a new session. python -c ""import cv2"". ```. ## DV Prerequisite. ```bash. ####################################################################. # misc setup. ####################################################################. # development packages. yum install python2-pkgconfig zip zlib-devel unzip curl -y. # python packages. yum install python-devel python-pip python-wheel -y. ####################################################################. # python packages. ####################################################################. # python 2 required. echo ""$(python --version)"". echo ""$(pip --version)"". # Install python packages. pip install contextlib2. pip install enum34. pip install intervaltree. pip install 'mock>=2.0.0'. # pip install 'numpy==1.14' => skip as installed in TF. pip install 'requests>=2.18'. # pip install 'scipy==1.0' => skip as installed in TF. pip install 'oauth2client>=4.0.0'. pip install 'crcmod>=1.7'. pip install six. pip install sklearn. pip install pandas. pip install psutil. pip install --upgrade google-api-python-client. ####################################################################. # depend on opencv-python wheel - build from source. ####################################################################. pip install 'tensor2tensor>=1.9.0'. #######################################",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:16263,deployability,version,version,16263,"vark/opencv-python.git. cd opencv-python/. # fetch the tags to your local repository. git fetch --all --tags --prune. # check out tag 3.4.5.20. git checkout tags/20. # load submoduel. git submodule update --init --recursive. # Dependency. pip install pyparsing. yum install qt-devel. # Build. python setup.py bdist_wheel. # Insatll. pip install dist/opencv_python-3.4.5.20-cp27-cp27mu-linux_ppc64le.whl. # Verify in a new session. python -c ""import cv2"". ```. ## DV Prerequisite. ```bash. ####################################################################. # misc setup. ####################################################################. # development packages. yum install python2-pkgconfig zip zlib-devel unzip curl -y. # python packages. yum install python-devel python-pip python-wheel -y. ####################################################################. # python packages. ####################################################################. # python 2 required. echo ""$(python --version)"". echo ""$(pip --version)"". # Install python packages. pip install contextlib2. pip install enum34. pip install intervaltree. pip install 'mock>=2.0.0'. # pip install 'numpy==1.14' => skip as installed in TF. pip install 'requests>=2.18'. # pip install 'scipy==1.0' => skip as installed in TF. pip install 'oauth2client>=4.0.0'. pip install 'crcmod>=1.7'. pip install six. pip install sklearn. pip install pandas. pip install psutil. pip install --upgrade google-api-python-client. ####################################################################. # depend on opencv-python wheel - build from source. ####################################################################. pip install 'tensor2tensor>=1.9.0'. ####################################################################. # depend on - TensorFlow - 1.12 build from source. ####################################################################. pip install tensorflow-1.12.0-cp27-cp27mu-linux_ppc64le.whl. #################################",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:16288,deployability,version,version,16288,"d opencv-python/. # fetch the tags to your local repository. git fetch --all --tags --prune. # check out tag 3.4.5.20. git checkout tags/20. # load submoduel. git submodule update --init --recursive. # Dependency. pip install pyparsing. yum install qt-devel. # Build. python setup.py bdist_wheel. # Insatll. pip install dist/opencv_python-3.4.5.20-cp27-cp27mu-linux_ppc64le.whl. # Verify in a new session. python -c ""import cv2"". ```. ## DV Prerequisite. ```bash. ####################################################################. # misc setup. ####################################################################. # development packages. yum install python2-pkgconfig zip zlib-devel unzip curl -y. # python packages. yum install python-devel python-pip python-wheel -y. ####################################################################. # python packages. ####################################################################. # python 2 required. echo ""$(python --version)"". echo ""$(pip --version)"". # Install python packages. pip install contextlib2. pip install enum34. pip install intervaltree. pip install 'mock>=2.0.0'. # pip install 'numpy==1.14' => skip as installed in TF. pip install 'requests>=2.18'. # pip install 'scipy==1.0' => skip as installed in TF. pip install 'oauth2client>=4.0.0'. pip install 'crcmod>=1.7'. pip install six. pip install sklearn. pip install pandas. pip install psutil. pip install --upgrade google-api-python-client. ####################################################################. # depend on opencv-python wheel - build from source. ####################################################################. pip install 'tensor2tensor>=1.9.0'. ####################################################################. # depend on - TensorFlow - 1.12 build from source. ####################################################################. pip install tensorflow-1.12.0-cp27-cp27mu-linux_ppc64le.whl. ##########################################################",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:16301,deployability,Instal,Install,16301,"on/. # fetch the tags to your local repository. git fetch --all --tags --prune. # check out tag 3.4.5.20. git checkout tags/20. # load submoduel. git submodule update --init --recursive. # Dependency. pip install pyparsing. yum install qt-devel. # Build. python setup.py bdist_wheel. # Insatll. pip install dist/opencv_python-3.4.5.20-cp27-cp27mu-linux_ppc64le.whl. # Verify in a new session. python -c ""import cv2"". ```. ## DV Prerequisite. ```bash. ####################################################################. # misc setup. ####################################################################. # development packages. yum install python2-pkgconfig zip zlib-devel unzip curl -y. # python packages. yum install python-devel python-pip python-wheel -y. ####################################################################. # python packages. ####################################################################. # python 2 required. echo ""$(python --version)"". echo ""$(pip --version)"". # Install python packages. pip install contextlib2. pip install enum34. pip install intervaltree. pip install 'mock>=2.0.0'. # pip install 'numpy==1.14' => skip as installed in TF. pip install 'requests>=2.18'. # pip install 'scipy==1.0' => skip as installed in TF. pip install 'oauth2client>=4.0.0'. pip install 'crcmod>=1.7'. pip install six. pip install sklearn. pip install pandas. pip install psutil. pip install --upgrade google-api-python-client. ####################################################################. # depend on opencv-python wheel - build from source. ####################################################################. pip install 'tensor2tensor>=1.9.0'. ####################################################################. # depend on - TensorFlow - 1.12 build from source. ####################################################################. pip install tensorflow-1.12.0-cp27-cp27mu-linux_ppc64le.whl. ####################################################################. #",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:16330,deployability,instal,install,16330," local repository. git fetch --all --tags --prune. # check out tag 3.4.5.20. git checkout tags/20. # load submoduel. git submodule update --init --recursive. # Dependency. pip install pyparsing. yum install qt-devel. # Build. python setup.py bdist_wheel. # Insatll. pip install dist/opencv_python-3.4.5.20-cp27-cp27mu-linux_ppc64le.whl. # Verify in a new session. python -c ""import cv2"". ```. ## DV Prerequisite. ```bash. ####################################################################. # misc setup. ####################################################################. # development packages. yum install python2-pkgconfig zip zlib-devel unzip curl -y. # python packages. yum install python-devel python-pip python-wheel -y. ####################################################################. # python packages. ####################################################################. # python 2 required. echo ""$(python --version)"". echo ""$(pip --version)"". # Install python packages. pip install contextlib2. pip install enum34. pip install intervaltree. pip install 'mock>=2.0.0'. # pip install 'numpy==1.14' => skip as installed in TF. pip install 'requests>=2.18'. # pip install 'scipy==1.0' => skip as installed in TF. pip install 'oauth2client>=4.0.0'. pip install 'crcmod>=1.7'. pip install six. pip install sklearn. pip install pandas. pip install psutil. pip install --upgrade google-api-python-client. ####################################################################. # depend on opencv-python wheel - build from source. ####################################################################. pip install 'tensor2tensor>=1.9.0'. ####################################################################. # depend on - TensorFlow - 1.12 build from source. ####################################################################. pip install tensorflow-1.12.0-cp27-cp27mu-linux_ppc64le.whl. ####################################################################. # Misc dependencies. #########",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:16355,deployability,instal,install,16355,"tch --all --tags --prune. # check out tag 3.4.5.20. git checkout tags/20. # load submoduel. git submodule update --init --recursive. # Dependency. pip install pyparsing. yum install qt-devel. # Build. python setup.py bdist_wheel. # Insatll. pip install dist/opencv_python-3.4.5.20-cp27-cp27mu-linux_ppc64le.whl. # Verify in a new session. python -c ""import cv2"". ```. ## DV Prerequisite. ```bash. ####################################################################. # misc setup. ####################################################################. # development packages. yum install python2-pkgconfig zip zlib-devel unzip curl -y. # python packages. yum install python-devel python-pip python-wheel -y. ####################################################################. # python packages. ####################################################################. # python 2 required. echo ""$(python --version)"". echo ""$(pip --version)"". # Install python packages. pip install contextlib2. pip install enum34. pip install intervaltree. pip install 'mock>=2.0.0'. # pip install 'numpy==1.14' => skip as installed in TF. pip install 'requests>=2.18'. # pip install 'scipy==1.0' => skip as installed in TF. pip install 'oauth2client>=4.0.0'. pip install 'crcmod>=1.7'. pip install six. pip install sklearn. pip install pandas. pip install psutil. pip install --upgrade google-api-python-client. ####################################################################. # depend on opencv-python wheel - build from source. ####################################################################. pip install 'tensor2tensor>=1.9.0'. ####################################################################. # depend on - TensorFlow - 1.12 build from source. ####################################################################. pip install tensorflow-1.12.0-cp27-cp27mu-linux_ppc64le.whl. ####################################################################. # Misc dependencies. ##################################",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:16375,deployability,instal,install,16375,"rune. # check out tag 3.4.5.20. git checkout tags/20. # load submoduel. git submodule update --init --recursive. # Dependency. pip install pyparsing. yum install qt-devel. # Build. python setup.py bdist_wheel. # Insatll. pip install dist/opencv_python-3.4.5.20-cp27-cp27mu-linux_ppc64le.whl. # Verify in a new session. python -c ""import cv2"". ```. ## DV Prerequisite. ```bash. ####################################################################. # misc setup. ####################################################################. # development packages. yum install python2-pkgconfig zip zlib-devel unzip curl -y. # python packages. yum install python-devel python-pip python-wheel -y. ####################################################################. # python packages. ####################################################################. # python 2 required. echo ""$(python --version)"". echo ""$(pip --version)"". # Install python packages. pip install contextlib2. pip install enum34. pip install intervaltree. pip install 'mock>=2.0.0'. # pip install 'numpy==1.14' => skip as installed in TF. pip install 'requests>=2.18'. # pip install 'scipy==1.0' => skip as installed in TF. pip install 'oauth2client>=4.0.0'. pip install 'crcmod>=1.7'. pip install six. pip install sklearn. pip install pandas. pip install psutil. pip install --upgrade google-api-python-client. ####################################################################. # depend on opencv-python wheel - build from source. ####################################################################. pip install 'tensor2tensor>=1.9.0'. ####################################################################. # depend on - TensorFlow - 1.12 build from source. ####################################################################. pip install tensorflow-1.12.0-cp27-cp27mu-linux_ppc64le.whl. ####################################################################. # Misc dependencies. ######################################################",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:16401,deployability,instal,install,16401,"5.20. git checkout tags/20. # load submoduel. git submodule update --init --recursive. # Dependency. pip install pyparsing. yum install qt-devel. # Build. python setup.py bdist_wheel. # Insatll. pip install dist/opencv_python-3.4.5.20-cp27-cp27mu-linux_ppc64le.whl. # Verify in a new session. python -c ""import cv2"". ```. ## DV Prerequisite. ```bash. ####################################################################. # misc setup. ####################################################################. # development packages. yum install python2-pkgconfig zip zlib-devel unzip curl -y. # python packages. yum install python-devel python-pip python-wheel -y. ####################################################################. # python packages. ####################################################################. # python 2 required. echo ""$(python --version)"". echo ""$(pip --version)"". # Install python packages. pip install contextlib2. pip install enum34. pip install intervaltree. pip install 'mock>=2.0.0'. # pip install 'numpy==1.14' => skip as installed in TF. pip install 'requests>=2.18'. # pip install 'scipy==1.0' => skip as installed in TF. pip install 'oauth2client>=4.0.0'. pip install 'crcmod>=1.7'. pip install six. pip install sklearn. pip install pandas. pip install psutil. pip install --upgrade google-api-python-client. ####################################################################. # depend on opencv-python wheel - build from source. ####################################################################. pip install 'tensor2tensor>=1.9.0'. ####################################################################. # depend on - TensorFlow - 1.12 build from source. ####################################################################. pip install tensorflow-1.12.0-cp27-cp27mu-linux_ppc64le.whl. ####################################################################. # Misc dependencies. ####################################################################. yum instal",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:16430,deployability,instal,install,16430," load submoduel. git submodule update --init --recursive. # Dependency. pip install pyparsing. yum install qt-devel. # Build. python setup.py bdist_wheel. # Insatll. pip install dist/opencv_python-3.4.5.20-cp27-cp27mu-linux_ppc64le.whl. # Verify in a new session. python -c ""import cv2"". ```. ## DV Prerequisite. ```bash. ####################################################################. # misc setup. ####################################################################. # development packages. yum install python2-pkgconfig zip zlib-devel unzip curl -y. # python packages. yum install python-devel python-pip python-wheel -y. ####################################################################. # python packages. ####################################################################. # python 2 required. echo ""$(python --version)"". echo ""$(pip --version)"". # Install python packages. pip install contextlib2. pip install enum34. pip install intervaltree. pip install 'mock>=2.0.0'. # pip install 'numpy==1.14' => skip as installed in TF. pip install 'requests>=2.18'. # pip install 'scipy==1.0' => skip as installed in TF. pip install 'oauth2client>=4.0.0'. pip install 'crcmod>=1.7'. pip install six. pip install sklearn. pip install pandas. pip install psutil. pip install --upgrade google-api-python-client. ####################################################################. # depend on opencv-python wheel - build from source. ####################################################################. pip install 'tensor2tensor>=1.9.0'. ####################################################################. # depend on - TensorFlow - 1.12 build from source. ####################################################################. pip install tensorflow-1.12.0-cp27-cp27mu-linux_ppc64le.whl. ####################################################################. # Misc dependencies. ####################################################################. yum install openssl-devel curl-devel zl",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:16463,deployability,instal,installed,16463,"ate --init --recursive. # Dependency. pip install pyparsing. yum install qt-devel. # Build. python setup.py bdist_wheel. # Insatll. pip install dist/opencv_python-3.4.5.20-cp27-cp27mu-linux_ppc64le.whl. # Verify in a new session. python -c ""import cv2"". ```. ## DV Prerequisite. ```bash. ####################################################################. # misc setup. ####################################################################. # development packages. yum install python2-pkgconfig zip zlib-devel unzip curl -y. # python packages. yum install python-devel python-pip python-wheel -y. ####################################################################. # python packages. ####################################################################. # python 2 required. echo ""$(python --version)"". echo ""$(pip --version)"". # Install python packages. pip install contextlib2. pip install enum34. pip install intervaltree. pip install 'mock>=2.0.0'. # pip install 'numpy==1.14' => skip as installed in TF. pip install 'requests>=2.18'. # pip install 'scipy==1.0' => skip as installed in TF. pip install 'oauth2client>=4.0.0'. pip install 'crcmod>=1.7'. pip install six. pip install sklearn. pip install pandas. pip install psutil. pip install --upgrade google-api-python-client. ####################################################################. # depend on opencv-python wheel - build from source. ####################################################################. pip install 'tensor2tensor>=1.9.0'. ####################################################################. # depend on - TensorFlow - 1.12 build from source. ####################################################################. pip install tensorflow-1.12.0-cp27-cp27mu-linux_ppc64le.whl. ####################################################################. # Misc dependencies. ####################################################################. yum install openssl-devel curl-devel zlib-devel bzip2-devel xz-devel. yum",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:16484,deployability,instal,install,16484,"ve. # Dependency. pip install pyparsing. yum install qt-devel. # Build. python setup.py bdist_wheel. # Insatll. pip install dist/opencv_python-3.4.5.20-cp27-cp27mu-linux_ppc64le.whl. # Verify in a new session. python -c ""import cv2"". ```. ## DV Prerequisite. ```bash. ####################################################################. # misc setup. ####################################################################. # development packages. yum install python2-pkgconfig zip zlib-devel unzip curl -y. # python packages. yum install python-devel python-pip python-wheel -y. ####################################################################. # python packages. ####################################################################. # python 2 required. echo ""$(python --version)"". echo ""$(pip --version)"". # Install python packages. pip install contextlib2. pip install enum34. pip install intervaltree. pip install 'mock>=2.0.0'. # pip install 'numpy==1.14' => skip as installed in TF. pip install 'requests>=2.18'. # pip install 'scipy==1.0' => skip as installed in TF. pip install 'oauth2client>=4.0.0'. pip install 'crcmod>=1.7'. pip install six. pip install sklearn. pip install pandas. pip install psutil. pip install --upgrade google-api-python-client. ####################################################################. # depend on opencv-python wheel - build from source. ####################################################################. pip install 'tensor2tensor>=1.9.0'. ####################################################################. # depend on - TensorFlow - 1.12 build from source. ####################################################################. pip install tensorflow-1.12.0-cp27-cp27mu-linux_ppc64le.whl. ####################################################################. # Misc dependencies. ####################################################################. yum install openssl-devel curl-devel zlib-devel bzip2-devel xz-devel. yum install boost-devel",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:16516,deployability,instal,install,16516,"parsing. yum install qt-devel. # Build. python setup.py bdist_wheel. # Insatll. pip install dist/opencv_python-3.4.5.20-cp27-cp27mu-linux_ppc64le.whl. # Verify in a new session. python -c ""import cv2"". ```. ## DV Prerequisite. ```bash. ####################################################################. # misc setup. ####################################################################. # development packages. yum install python2-pkgconfig zip zlib-devel unzip curl -y. # python packages. yum install python-devel python-pip python-wheel -y. ####################################################################. # python packages. ####################################################################. # python 2 required. echo ""$(python --version)"". echo ""$(pip --version)"". # Install python packages. pip install contextlib2. pip install enum34. pip install intervaltree. pip install 'mock>=2.0.0'. # pip install 'numpy==1.14' => skip as installed in TF. pip install 'requests>=2.18'. # pip install 'scipy==1.0' => skip as installed in TF. pip install 'oauth2client>=4.0.0'. pip install 'crcmod>=1.7'. pip install six. pip install sklearn. pip install pandas. pip install psutil. pip install --upgrade google-api-python-client. ####################################################################. # depend on opencv-python wheel - build from source. ####################################################################. pip install 'tensor2tensor>=1.9.0'. ####################################################################. # depend on - TensorFlow - 1.12 build from source. ####################################################################. pip install tensorflow-1.12.0-cp27-cp27mu-linux_ppc64le.whl. ####################################################################. # Misc dependencies. ####################################################################. yum install openssl-devel curl-devel zlib-devel bzip2-devel xz-devel. yum install boost-devel. ```. ## DeepVariant. Git Repos",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:16548,deployability,instal,installed,16548,"Build. python setup.py bdist_wheel. # Insatll. pip install dist/opencv_python-3.4.5.20-cp27-cp27mu-linux_ppc64le.whl. # Verify in a new session. python -c ""import cv2"". ```. ## DV Prerequisite. ```bash. ####################################################################. # misc setup. ####################################################################. # development packages. yum install python2-pkgconfig zip zlib-devel unzip curl -y. # python packages. yum install python-devel python-pip python-wheel -y. ####################################################################. # python packages. ####################################################################. # python 2 required. echo ""$(python --version)"". echo ""$(pip --version)"". # Install python packages. pip install contextlib2. pip install enum34. pip install intervaltree. pip install 'mock>=2.0.0'. # pip install 'numpy==1.14' => skip as installed in TF. pip install 'requests>=2.18'. # pip install 'scipy==1.0' => skip as installed in TF. pip install 'oauth2client>=4.0.0'. pip install 'crcmod>=1.7'. pip install six. pip install sklearn. pip install pandas. pip install psutil. pip install --upgrade google-api-python-client. ####################################################################. # depend on opencv-python wheel - build from source. ####################################################################. pip install 'tensor2tensor>=1.9.0'. ####################################################################. # depend on - TensorFlow - 1.12 build from source. ####################################################################. pip install tensorflow-1.12.0-cp27-cp27mu-linux_ppc64le.whl. ####################################################################. # Misc dependencies. ####################################################################. yum install openssl-devel curl-devel zlib-devel bzip2-devel xz-devel. yum install boost-devel. ```. ## DeepVariant. Git Repository [https://github.com/google/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:16569,deployability,instal,install,16569,"py bdist_wheel. # Insatll. pip install dist/opencv_python-3.4.5.20-cp27-cp27mu-linux_ppc64le.whl. # Verify in a new session. python -c ""import cv2"". ```. ## DV Prerequisite. ```bash. ####################################################################. # misc setup. ####################################################################. # development packages. yum install python2-pkgconfig zip zlib-devel unzip curl -y. # python packages. yum install python-devel python-pip python-wheel -y. ####################################################################. # python packages. ####################################################################. # python 2 required. echo ""$(python --version)"". echo ""$(pip --version)"". # Install python packages. pip install contextlib2. pip install enum34. pip install intervaltree. pip install 'mock>=2.0.0'. # pip install 'numpy==1.14' => skip as installed in TF. pip install 'requests>=2.18'. # pip install 'scipy==1.0' => skip as installed in TF. pip install 'oauth2client>=4.0.0'. pip install 'crcmod>=1.7'. pip install six. pip install sklearn. pip install pandas. pip install psutil. pip install --upgrade google-api-python-client. ####################################################################. # depend on opencv-python wheel - build from source. ####################################################################. pip install 'tensor2tensor>=1.9.0'. ####################################################################. # depend on - TensorFlow - 1.12 build from source. ####################################################################. pip install tensorflow-1.12.0-cp27-cp27mu-linux_ppc64le.whl. ####################################################################. # Misc dependencies. ####################################################################. yum install openssl-devel curl-devel zlib-devel bzip2-devel xz-devel. yum install boost-devel. ```. ## DeepVariant. Git Repository [https://github.com/google/deepvariant](https:/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:16604,deployability,instal,install,16604,"all dist/opencv_python-3.4.5.20-cp27-cp27mu-linux_ppc64le.whl. # Verify in a new session. python -c ""import cv2"". ```. ## DV Prerequisite. ```bash. ####################################################################. # misc setup. ####################################################################. # development packages. yum install python2-pkgconfig zip zlib-devel unzip curl -y. # python packages. yum install python-devel python-pip python-wheel -y. ####################################################################. # python packages. ####################################################################. # python 2 required. echo ""$(python --version)"". echo ""$(pip --version)"". # Install python packages. pip install contextlib2. pip install enum34. pip install intervaltree. pip install 'mock>=2.0.0'. # pip install 'numpy==1.14' => skip as installed in TF. pip install 'requests>=2.18'. # pip install 'scipy==1.0' => skip as installed in TF. pip install 'oauth2client>=4.0.0'. pip install 'crcmod>=1.7'. pip install six. pip install sklearn. pip install pandas. pip install psutil. pip install --upgrade google-api-python-client. ####################################################################. # depend on opencv-python wheel - build from source. ####################################################################. pip install 'tensor2tensor>=1.9.0'. ####################################################################. # depend on - TensorFlow - 1.12 build from source. ####################################################################. pip install tensorflow-1.12.0-cp27-cp27mu-linux_ppc64le.whl. ####################################################################. # Misc dependencies. ####################################################################. yum install openssl-devel curl-devel zlib-devel bzip2-devel xz-devel. yum install boost-devel. ```. ## DeepVariant. Git Repository [https://github.com/google/deepvariant](https://github.com/google/deepvariant). ``",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:16631,deployability,instal,install,16631,"5.20-cp27-cp27mu-linux_ppc64le.whl. # Verify in a new session. python -c ""import cv2"". ```. ## DV Prerequisite. ```bash. ####################################################################. # misc setup. ####################################################################. # development packages. yum install python2-pkgconfig zip zlib-devel unzip curl -y. # python packages. yum install python-devel python-pip python-wheel -y. ####################################################################. # python packages. ####################################################################. # python 2 required. echo ""$(python --version)"". echo ""$(pip --version)"". # Install python packages. pip install contextlib2. pip install enum34. pip install intervaltree. pip install 'mock>=2.0.0'. # pip install 'numpy==1.14' => skip as installed in TF. pip install 'requests>=2.18'. # pip install 'scipy==1.0' => skip as installed in TF. pip install 'oauth2client>=4.0.0'. pip install 'crcmod>=1.7'. pip install six. pip install sklearn. pip install pandas. pip install psutil. pip install --upgrade google-api-python-client. ####################################################################. # depend on opencv-python wheel - build from source. ####################################################################. pip install 'tensor2tensor>=1.9.0'. ####################################################################. # depend on - TensorFlow - 1.12 build from source. ####################################################################. pip install tensorflow-1.12.0-cp27-cp27mu-linux_ppc64le.whl. ####################################################################. # Misc dependencies. ####################################################################. yum install openssl-devel curl-devel zlib-devel bzip2-devel xz-devel. yum install boost-devel. ```. ## DeepVariant. Git Repository [https://github.com/google/deepvariant](https://github.com/google/deepvariant). ```bash. # check out source c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:16648,deployability,instal,install,16648,"linux_ppc64le.whl. # Verify in a new session. python -c ""import cv2"". ```. ## DV Prerequisite. ```bash. ####################################################################. # misc setup. ####################################################################. # development packages. yum install python2-pkgconfig zip zlib-devel unzip curl -y. # python packages. yum install python-devel python-pip python-wheel -y. ####################################################################. # python packages. ####################################################################. # python 2 required. echo ""$(python --version)"". echo ""$(pip --version)"". # Install python packages. pip install contextlib2. pip install enum34. pip install intervaltree. pip install 'mock>=2.0.0'. # pip install 'numpy==1.14' => skip as installed in TF. pip install 'requests>=2.18'. # pip install 'scipy==1.0' => skip as installed in TF. pip install 'oauth2client>=4.0.0'. pip install 'crcmod>=1.7'. pip install six. pip install sklearn. pip install pandas. pip install psutil. pip install --upgrade google-api-python-client. ####################################################################. # depend on opencv-python wheel - build from source. ####################################################################. pip install 'tensor2tensor>=1.9.0'. ####################################################################. # depend on - TensorFlow - 1.12 build from source. ####################################################################. pip install tensorflow-1.12.0-cp27-cp27mu-linux_ppc64le.whl. ####################################################################. # Misc dependencies. ####################################################################. yum install openssl-devel curl-devel zlib-devel bzip2-devel xz-devel. yum install boost-devel. ```. ## DeepVariant. Git Repository [https://github.com/google/deepvariant](https://github.com/google/deepvariant). ```bash. # check out source code. git clone ht",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:16669,deployability,instal,install,16669,"Verify in a new session. python -c ""import cv2"". ```. ## DV Prerequisite. ```bash. ####################################################################. # misc setup. ####################################################################. # development packages. yum install python2-pkgconfig zip zlib-devel unzip curl -y. # python packages. yum install python-devel python-pip python-wheel -y. ####################################################################. # python packages. ####################################################################. # python 2 required. echo ""$(python --version)"". echo ""$(pip --version)"". # Install python packages. pip install contextlib2. pip install enum34. pip install intervaltree. pip install 'mock>=2.0.0'. # pip install 'numpy==1.14' => skip as installed in TF. pip install 'requests>=2.18'. # pip install 'scipy==1.0' => skip as installed in TF. pip install 'oauth2client>=4.0.0'. pip install 'crcmod>=1.7'. pip install six. pip install sklearn. pip install pandas. pip install psutil. pip install --upgrade google-api-python-client. ####################################################################. # depend on opencv-python wheel - build from source. ####################################################################. pip install 'tensor2tensor>=1.9.0'. ####################################################################. # depend on - TensorFlow - 1.12 build from source. ####################################################################. pip install tensorflow-1.12.0-cp27-cp27mu-linux_ppc64le.whl. ####################################################################. # Misc dependencies. ####################################################################. yum install openssl-devel curl-devel zlib-devel bzip2-devel xz-devel. yum install boost-devel. ```. ## DeepVariant. Git Repository [https://github.com/google/deepvariant](https://github.com/google/deepvariant). ```bash. # check out source code. git clone https://github.com/goog",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:16689,deployability,instal,install,16689,"ion. python -c ""import cv2"". ```. ## DV Prerequisite. ```bash. ####################################################################. # misc setup. ####################################################################. # development packages. yum install python2-pkgconfig zip zlib-devel unzip curl -y. # python packages. yum install python-devel python-pip python-wheel -y. ####################################################################. # python packages. ####################################################################. # python 2 required. echo ""$(python --version)"". echo ""$(pip --version)"". # Install python packages. pip install contextlib2. pip install enum34. pip install intervaltree. pip install 'mock>=2.0.0'. # pip install 'numpy==1.14' => skip as installed in TF. pip install 'requests>=2.18'. # pip install 'scipy==1.0' => skip as installed in TF. pip install 'oauth2client>=4.0.0'. pip install 'crcmod>=1.7'. pip install six. pip install sklearn. pip install pandas. pip install psutil. pip install --upgrade google-api-python-client. ####################################################################. # depend on opencv-python wheel - build from source. ####################################################################. pip install 'tensor2tensor>=1.9.0'. ####################################################################. # depend on - TensorFlow - 1.12 build from source. ####################################################################. pip install tensorflow-1.12.0-cp27-cp27mu-linux_ppc64le.whl. ####################################################################. # Misc dependencies. ####################################################################. yum install openssl-devel curl-devel zlib-devel bzip2-devel xz-devel. yum install boost-devel. ```. ## DeepVariant. Git Repository [https://github.com/google/deepvariant](https://github.com/google/deepvariant). ```bash. # check out source code. git clone https://github.com/google/deepvariant.git. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:16709,deployability,instal,install,16709,"rt cv2"". ```. ## DV Prerequisite. ```bash. ####################################################################. # misc setup. ####################################################################. # development packages. yum install python2-pkgconfig zip zlib-devel unzip curl -y. # python packages. yum install python-devel python-pip python-wheel -y. ####################################################################. # python packages. ####################################################################. # python 2 required. echo ""$(python --version)"". echo ""$(pip --version)"". # Install python packages. pip install contextlib2. pip install enum34. pip install intervaltree. pip install 'mock>=2.0.0'. # pip install 'numpy==1.14' => skip as installed in TF. pip install 'requests>=2.18'. # pip install 'scipy==1.0' => skip as installed in TF. pip install 'oauth2client>=4.0.0'. pip install 'crcmod>=1.7'. pip install six. pip install sklearn. pip install pandas. pip install psutil. pip install --upgrade google-api-python-client. ####################################################################. # depend on opencv-python wheel - build from source. ####################################################################. pip install 'tensor2tensor>=1.9.0'. ####################################################################. # depend on - TensorFlow - 1.12 build from source. ####################################################################. pip install tensorflow-1.12.0-cp27-cp27mu-linux_ppc64le.whl. ####################################################################. # Misc dependencies. ####################################################################. yum install openssl-devel curl-devel zlib-devel bzip2-devel xz-devel. yum install boost-devel. ```. ## DeepVariant. Git Repository [https://github.com/google/deepvariant](https://github.com/google/deepvariant). ```bash. # check out source code. git clone https://github.com/google/deepvariant.git. cd deepvariant. # fe",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:16719,deployability,upgrad,upgrade,16719,"``. ## DV Prerequisite. ```bash. ####################################################################. # misc setup. ####################################################################. # development packages. yum install python2-pkgconfig zip zlib-devel unzip curl -y. # python packages. yum install python-devel python-pip python-wheel -y. ####################################################################. # python packages. ####################################################################. # python 2 required. echo ""$(python --version)"". echo ""$(pip --version)"". # Install python packages. pip install contextlib2. pip install enum34. pip install intervaltree. pip install 'mock>=2.0.0'. # pip install 'numpy==1.14' => skip as installed in TF. pip install 'requests>=2.18'. # pip install 'scipy==1.0' => skip as installed in TF. pip install 'oauth2client>=4.0.0'. pip install 'crcmod>=1.7'. pip install six. pip install sklearn. pip install pandas. pip install psutil. pip install --upgrade google-api-python-client. ####################################################################. # depend on opencv-python wheel - build from source. ####################################################################. pip install 'tensor2tensor>=1.9.0'. ####################################################################. # depend on - TensorFlow - 1.12 build from source. ####################################################################. pip install tensorflow-1.12.0-cp27-cp27mu-linux_ppc64le.whl. ####################################################################. # Misc dependencies. ####################################################################. yum install openssl-devel curl-devel zlib-devel bzip2-devel xz-devel. yum install boost-devel. ```. ## DeepVariant. Git Repository [https://github.com/google/deepvariant](https://github.com/google/deepvariant). ```bash. # check out source code. git clone https://github.com/google/deepvariant.git. cd deepvariant. # fetch all ta",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:16734,deployability,api,api-python-client,16734,"te. ```bash. ####################################################################. # misc setup. ####################################################################. # development packages. yum install python2-pkgconfig zip zlib-devel unzip curl -y. # python packages. yum install python-devel python-pip python-wheel -y. ####################################################################. # python packages. ####################################################################. # python 2 required. echo ""$(python --version)"". echo ""$(pip --version)"". # Install python packages. pip install contextlib2. pip install enum34. pip install intervaltree. pip install 'mock>=2.0.0'. # pip install 'numpy==1.14' => skip as installed in TF. pip install 'requests>=2.18'. # pip install 'scipy==1.0' => skip as installed in TF. pip install 'oauth2client>=4.0.0'. pip install 'crcmod>=1.7'. pip install six. pip install sklearn. pip install pandas. pip install psutil. pip install --upgrade google-api-python-client. ####################################################################. # depend on opencv-python wheel - build from source. ####################################################################. pip install 'tensor2tensor>=1.9.0'. ####################################################################. # depend on - TensorFlow - 1.12 build from source. ####################################################################. pip install tensorflow-1.12.0-cp27-cp27mu-linux_ppc64le.whl. ####################################################################. # Misc dependencies. ####################################################################. yum install openssl-devel curl-devel zlib-devel bzip2-devel xz-devel. yum install boost-devel. ```. ## DeepVariant. Git Repository [https://github.com/google/deepvariant](https://github.com/google/deepvariant). ```bash. # check out source code. git clone https://github.com/google/deepvariant.git. cd deepvariant. # fetch all tags. git fetch --all ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:16825,deployability,depend,depend,16825,"misc setup. ####################################################################. # development packages. yum install python2-pkgconfig zip zlib-devel unzip curl -y. # python packages. yum install python-devel python-pip python-wheel -y. ####################################################################. # python packages. ####################################################################. # python 2 required. echo ""$(python --version)"". echo ""$(pip --version)"". # Install python packages. pip install contextlib2. pip install enum34. pip install intervaltree. pip install 'mock>=2.0.0'. # pip install 'numpy==1.14' => skip as installed in TF. pip install 'requests>=2.18'. # pip install 'scipy==1.0' => skip as installed in TF. pip install 'oauth2client>=4.0.0'. pip install 'crcmod>=1.7'. pip install six. pip install sklearn. pip install pandas. pip install psutil. pip install --upgrade google-api-python-client. ####################################################################. # depend on opencv-python wheel - build from source. ####################################################################. pip install 'tensor2tensor>=1.9.0'. ####################################################################. # depend on - TensorFlow - 1.12 build from source. ####################################################################. pip install tensorflow-1.12.0-cp27-cp27mu-linux_ppc64le.whl. ####################################################################. # Misc dependencies. ####################################################################. yum install openssl-devel curl-devel zlib-devel bzip2-devel xz-devel. yum install boost-devel. ```. ## DeepVariant. Git Repository [https://github.com/google/deepvariant](https://github.com/google/deepvariant). ```bash. # check out source code. git clone https://github.com/google/deepvariant.git. cd deepvariant. # fetch all tags. git fetch --all --tags --prune. # check out tag. git checkout tags/v0.7.2. # Edit ctx.action with use",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:16857,deployability,build,build,16857,"################################################. # development packages. yum install python2-pkgconfig zip zlib-devel unzip curl -y. # python packages. yum install python-devel python-pip python-wheel -y. ####################################################################. # python packages. ####################################################################. # python 2 required. echo ""$(python --version)"". echo ""$(pip --version)"". # Install python packages. pip install contextlib2. pip install enum34. pip install intervaltree. pip install 'mock>=2.0.0'. # pip install 'numpy==1.14' => skip as installed in TF. pip install 'requests>=2.18'. # pip install 'scipy==1.0' => skip as installed in TF. pip install 'oauth2client>=4.0.0'. pip install 'crcmod>=1.7'. pip install six. pip install sklearn. pip install pandas. pip install psutil. pip install --upgrade google-api-python-client. ####################################################################. # depend on opencv-python wheel - build from source. ####################################################################. pip install 'tensor2tensor>=1.9.0'. ####################################################################. # depend on - TensorFlow - 1.12 build from source. ####################################################################. pip install tensorflow-1.12.0-cp27-cp27mu-linux_ppc64le.whl. ####################################################################. # Misc dependencies. ####################################################################. yum install openssl-devel curl-devel zlib-devel bzip2-devel xz-devel. yum install boost-devel. ```. ## DeepVariant. Git Repository [https://github.com/google/deepvariant](https://github.com/google/deepvariant). ```bash. # check out source code. git clone https://github.com/google/deepvariant.git. cd deepvariant. # fetch all tags. git fetch --all --tags --prune. # check out tag. git checkout tags/v0.7.2. # Edit ctx.action with use_default_shell_env=True. vim ./t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:16950,deployability,instal,install,16950,"pkgconfig zip zlib-devel unzip curl -y. # python packages. yum install python-devel python-pip python-wheel -y. ####################################################################. # python packages. ####################################################################. # python 2 required. echo ""$(python --version)"". echo ""$(pip --version)"". # Install python packages. pip install contextlib2. pip install enum34. pip install intervaltree. pip install 'mock>=2.0.0'. # pip install 'numpy==1.14' => skip as installed in TF. pip install 'requests>=2.18'. # pip install 'scipy==1.0' => skip as installed in TF. pip install 'oauth2client>=4.0.0'. pip install 'crcmod>=1.7'. pip install six. pip install sklearn. pip install pandas. pip install psutil. pip install --upgrade google-api-python-client. ####################################################################. # depend on opencv-python wheel - build from source. ####################################################################. pip install 'tensor2tensor>=1.9.0'. ####################################################################. # depend on - TensorFlow - 1.12 build from source. ####################################################################. pip install tensorflow-1.12.0-cp27-cp27mu-linux_ppc64le.whl. ####################################################################. # Misc dependencies. ####################################################################. yum install openssl-devel curl-devel zlib-devel bzip2-devel xz-devel. yum install boost-devel. ```. ## DeepVariant. Git Repository [https://github.com/google/deepvariant](https://github.com/google/deepvariant). ```bash. # check out source code. git clone https://github.com/google/deepvariant.git. cd deepvariant. # fetch all tags. git fetch --all --tags --prune. # check out tag. git checkout tags/v0.7.2. # Edit ctx.action with use_default_shell_env=True. vim ./third_party/clif.bzl. # Build and test. export LD_LIBRARY_PATH=/home/qilibj/inst/lib:/qilibj/in",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:17054,deployability,depend,depend,17054,"heel -y. ####################################################################. # python packages. ####################################################################. # python 2 required. echo ""$(python --version)"". echo ""$(pip --version)"". # Install python packages. pip install contextlib2. pip install enum34. pip install intervaltree. pip install 'mock>=2.0.0'. # pip install 'numpy==1.14' => skip as installed in TF. pip install 'requests>=2.18'. # pip install 'scipy==1.0' => skip as installed in TF. pip install 'oauth2client>=4.0.0'. pip install 'crcmod>=1.7'. pip install six. pip install sklearn. pip install pandas. pip install psutil. pip install --upgrade google-api-python-client. ####################################################################. # depend on opencv-python wheel - build from source. ####################################################################. pip install 'tensor2tensor>=1.9.0'. ####################################################################. # depend on - TensorFlow - 1.12 build from source. ####################################################################. pip install tensorflow-1.12.0-cp27-cp27mu-linux_ppc64le.whl. ####################################################################. # Misc dependencies. ####################################################################. yum install openssl-devel curl-devel zlib-devel bzip2-devel xz-devel. yum install boost-devel. ```. ## DeepVariant. Git Repository [https://github.com/google/deepvariant](https://github.com/google/deepvariant). ```bash. # check out source code. git clone https://github.com/google/deepvariant.git. cd deepvariant. # fetch all tags. git fetch --all --tags --prune. # check out tag. git checkout tags/v0.7.2. # Edit ctx.action with use_default_shell_env=True. vim ./third_party/clif.bzl. # Build and test. export LD_LIBRARY_PATH=/home/qilibj/inst/lib:/qilibj/inst/lib/python2.7/site-packages:$LD_LIBRARY_PATH. export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:17084,deployability,build,build,17084,"###############################################. # python packages. ####################################################################. # python 2 required. echo ""$(python --version)"". echo ""$(pip --version)"". # Install python packages. pip install contextlib2. pip install enum34. pip install intervaltree. pip install 'mock>=2.0.0'. # pip install 'numpy==1.14' => skip as installed in TF. pip install 'requests>=2.18'. # pip install 'scipy==1.0' => skip as installed in TF. pip install 'oauth2client>=4.0.0'. pip install 'crcmod>=1.7'. pip install six. pip install sklearn. pip install pandas. pip install psutil. pip install --upgrade google-api-python-client. ####################################################################. # depend on opencv-python wheel - build from source. ####################################################################. pip install 'tensor2tensor>=1.9.0'. ####################################################################. # depend on - TensorFlow - 1.12 build from source. ####################################################################. pip install tensorflow-1.12.0-cp27-cp27mu-linux_ppc64le.whl. ####################################################################. # Misc dependencies. ####################################################################. yum install openssl-devel curl-devel zlib-devel bzip2-devel xz-devel. yum install boost-devel. ```. ## DeepVariant. Git Repository [https://github.com/google/deepvariant](https://github.com/google/deepvariant). ```bash. # check out source code. git clone https://github.com/google/deepvariant.git. cd deepvariant. # fetch all tags. git fetch --all --tags --prune. # check out tag. git checkout tags/v0.7.2. # Edit ctx.action with use_default_shell_env=True. vim ./third_party/clif.bzl. # Build and test. export LD_LIBRARY_PATH=/home/qilibj/inst/lib:/qilibj/inst/lib/python2.7/site-packages:$LD_LIBRARY_PATH. export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. export PYTHON_BIN_P",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:17177,deployability,instal,install,17177,"##########################################. # python 2 required. echo ""$(python --version)"". echo ""$(pip --version)"". # Install python packages. pip install contextlib2. pip install enum34. pip install intervaltree. pip install 'mock>=2.0.0'. # pip install 'numpy==1.14' => skip as installed in TF. pip install 'requests>=2.18'. # pip install 'scipy==1.0' => skip as installed in TF. pip install 'oauth2client>=4.0.0'. pip install 'crcmod>=1.7'. pip install six. pip install sklearn. pip install pandas. pip install psutil. pip install --upgrade google-api-python-client. ####################################################################. # depend on opencv-python wheel - build from source. ####################################################################. pip install 'tensor2tensor>=1.9.0'. ####################################################################. # depend on - TensorFlow - 1.12 build from source. ####################################################################. pip install tensorflow-1.12.0-cp27-cp27mu-linux_ppc64le.whl. ####################################################################. # Misc dependencies. ####################################################################. yum install openssl-devel curl-devel zlib-devel bzip2-devel xz-devel. yum install boost-devel. ```. ## DeepVariant. Git Repository [https://github.com/google/deepvariant](https://github.com/google/deepvariant). ```bash. # check out source code. git clone https://github.com/google/deepvariant.git. cd deepvariant. # fetch all tags. git fetch --all --tags --prune. # check out tag. git checkout tags/v0.7.2. # Edit ctx.action with use_default_shell_env=True. vim ./third_party/clif.bzl. # Build and test. export LD_LIBRARY_PATH=/home/qilibj/inst/lib:/qilibj/inst/lib/python2.7/site-packages:$LD_LIBRARY_PATH. export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. export PYTHON_BIN_PATH=/home/qilibj/inst/bin/python. export PYTHON_LIB_PATH=/home/qilibj/inst/lib/python2.7/site-",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:17311,deployability,depend,dependencies,17311,"ackages. pip install contextlib2. pip install enum34. pip install intervaltree. pip install 'mock>=2.0.0'. # pip install 'numpy==1.14' => skip as installed in TF. pip install 'requests>=2.18'. # pip install 'scipy==1.0' => skip as installed in TF. pip install 'oauth2client>=4.0.0'. pip install 'crcmod>=1.7'. pip install six. pip install sklearn. pip install pandas. pip install psutil. pip install --upgrade google-api-python-client. ####################################################################. # depend on opencv-python wheel - build from source. ####################################################################. pip install 'tensor2tensor>=1.9.0'. ####################################################################. # depend on - TensorFlow - 1.12 build from source. ####################################################################. pip install tensorflow-1.12.0-cp27-cp27mu-linux_ppc64le.whl. ####################################################################. # Misc dependencies. ####################################################################. yum install openssl-devel curl-devel zlib-devel bzip2-devel xz-devel. yum install boost-devel. ```. ## DeepVariant. Git Repository [https://github.com/google/deepvariant](https://github.com/google/deepvariant). ```bash. # check out source code. git clone https://github.com/google/deepvariant.git. cd deepvariant. # fetch all tags. git fetch --all --tags --prune. # check out tag. git checkout tags/v0.7.2. # Edit ctx.action with use_default_shell_env=True. vim ./third_party/clif.bzl. # Build and test. export LD_LIBRARY_PATH=/home/qilibj/inst/lib:/qilibj/inst/lib/python2.7/site-packages:$LD_LIBRARY_PATH. export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. export PYTHON_BIN_PATH=/home/qilibj/inst/bin/python. export PYTHON_LIB_PATH=/home/qilibj/inst/lib/python2.7/site-packages. export BAZEL_PYTHON=/home/qilibj/inst/bin/python. export DV_COPT_FLAGS=""--copt=-maltivec --copt=-Wno-sign-compare --copt=-Wno-",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:17399,deployability,instal,install,17399,"stall 'mock>=2.0.0'. # pip install 'numpy==1.14' => skip as installed in TF. pip install 'requests>=2.18'. # pip install 'scipy==1.0' => skip as installed in TF. pip install 'oauth2client>=4.0.0'. pip install 'crcmod>=1.7'. pip install six. pip install sklearn. pip install pandas. pip install psutil. pip install --upgrade google-api-python-client. ####################################################################. # depend on opencv-python wheel - build from source. ####################################################################. pip install 'tensor2tensor>=1.9.0'. ####################################################################. # depend on - TensorFlow - 1.12 build from source. ####################################################################. pip install tensorflow-1.12.0-cp27-cp27mu-linux_ppc64le.whl. ####################################################################. # Misc dependencies. ####################################################################. yum install openssl-devel curl-devel zlib-devel bzip2-devel xz-devel. yum install boost-devel. ```. ## DeepVariant. Git Repository [https://github.com/google/deepvariant](https://github.com/google/deepvariant). ```bash. # check out source code. git clone https://github.com/google/deepvariant.git. cd deepvariant. # fetch all tags. git fetch --all --tags --prune. # check out tag. git checkout tags/v0.7.2. # Edit ctx.action with use_default_shell_env=True. vim ./third_party/clif.bzl. # Build and test. export LD_LIBRARY_PATH=/home/qilibj/inst/lib:/qilibj/inst/lib/python2.7/site-packages:$LD_LIBRARY_PATH. export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. export PYTHON_BIN_PATH=/home/qilibj/inst/bin/python. export PYTHON_LIB_PATH=/home/qilibj/inst/lib/python2.7/site-packages. export BAZEL_PYTHON=/home/qilibj/inst/bin/python. export DV_COPT_FLAGS=""--copt=-maltivec --copt=-Wno-sign-compare --copt=-Wno-write-strings --copt=-DNO_WARN_X86_INTRINSICS --cxxopt=-std=gnu++11"". # export DV_COPT",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:17469,deployability,instal,install,17469,"in TF. pip install 'requests>=2.18'. # pip install 'scipy==1.0' => skip as installed in TF. pip install 'oauth2client>=4.0.0'. pip install 'crcmod>=1.7'. pip install six. pip install sklearn. pip install pandas. pip install psutil. pip install --upgrade google-api-python-client. ####################################################################. # depend on opencv-python wheel - build from source. ####################################################################. pip install 'tensor2tensor>=1.9.0'. ####################################################################. # depend on - TensorFlow - 1.12 build from source. ####################################################################. pip install tensorflow-1.12.0-cp27-cp27mu-linux_ppc64le.whl. ####################################################################. # Misc dependencies. ####################################################################. yum install openssl-devel curl-devel zlib-devel bzip2-devel xz-devel. yum install boost-devel. ```. ## DeepVariant. Git Repository [https://github.com/google/deepvariant](https://github.com/google/deepvariant). ```bash. # check out source code. git clone https://github.com/google/deepvariant.git. cd deepvariant. # fetch all tags. git fetch --all --tags --prune. # check out tag. git checkout tags/v0.7.2. # Edit ctx.action with use_default_shell_env=True. vim ./third_party/clif.bzl. # Build and test. export LD_LIBRARY_PATH=/home/qilibj/inst/lib:/qilibj/inst/lib/python2.7/site-packages:$LD_LIBRARY_PATH. export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. export PYTHON_BIN_PATH=/home/qilibj/inst/bin/python. export PYTHON_LIB_PATH=/home/qilibj/inst/lib/python2.7/site-packages. export BAZEL_PYTHON=/home/qilibj/inst/bin/python. export DV_COPT_FLAGS=""--copt=-maltivec --copt=-Wno-sign-compare --copt=-Wno-write-strings --copt=-DNO_WARN_X86_INTRINSICS --cxxopt=-std=gnu++11"". # export DV_COPT_FLAGS=""--copt=-maltivec --copt=-Wno-sign-compare --copt=-Wno-write-st",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:17883,deployability,Build,Build,17883,"##########################################################. pip install 'tensor2tensor>=1.9.0'. ####################################################################. # depend on - TensorFlow - 1.12 build from source. ####################################################################. pip install tensorflow-1.12.0-cp27-cp27mu-linux_ppc64le.whl. ####################################################################. # Misc dependencies. ####################################################################. yum install openssl-devel curl-devel zlib-devel bzip2-devel xz-devel. yum install boost-devel. ```. ## DeepVariant. Git Repository [https://github.com/google/deepvariant](https://github.com/google/deepvariant). ```bash. # check out source code. git clone https://github.com/google/deepvariant.git. cd deepvariant. # fetch all tags. git fetch --all --tags --prune. # check out tag. git checkout tags/v0.7.2. # Edit ctx.action with use_default_shell_env=True. vim ./third_party/clif.bzl. # Build and test. export LD_LIBRARY_PATH=/home/qilibj/inst/lib:/qilibj/inst/lib/python2.7/site-packages:$LD_LIBRARY_PATH. export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. export PYTHON_BIN_PATH=/home/qilibj/inst/bin/python. export PYTHON_LIB_PATH=/home/qilibj/inst/lib/python2.7/site-packages. export BAZEL_PYTHON=/home/qilibj/inst/bin/python. export DV_COPT_FLAGS=""--copt=-maltivec --copt=-Wno-sign-compare --copt=-Wno-write-strings --copt=-DNO_WARN_X86_INTRINSICS --cxxopt=-std=gnu++11"". # export DV_COPT_FLAGS=""--copt=-maltivec --copt=-Wno-sign-compare --copt=-Wno-write-strings --copt=-DNO_WARN_X86_INTRINSICS --cxxopt=-std=gnu++11 --copt=-fsigned-char --cxxopt=-fsigned-char"". # for GPU enabled. # fix ""ImportError: No module named google.protobuf"" by install protobuf from source. bazel clean. bazel shutdown. bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" deepvariant/... \. --action_env=LD_LIBRARY_PATH --test_env=LD_LIBRARY_PATH \. --action_env=PYTHONPATH=$PYTHONPATH",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:18621,deployability,modul,module,18621,"ut source code. git clone https://github.com/google/deepvariant.git. cd deepvariant. # fetch all tags. git fetch --all --tags --prune. # check out tag. git checkout tags/v0.7.2. # Edit ctx.action with use_default_shell_env=True. vim ./third_party/clif.bzl. # Build and test. export LD_LIBRARY_PATH=/home/qilibj/inst/lib:/qilibj/inst/lib/python2.7/site-packages:$LD_LIBRARY_PATH. export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. export PYTHON_BIN_PATH=/home/qilibj/inst/bin/python. export PYTHON_LIB_PATH=/home/qilibj/inst/lib/python2.7/site-packages. export BAZEL_PYTHON=/home/qilibj/inst/bin/python. export DV_COPT_FLAGS=""--copt=-maltivec --copt=-Wno-sign-compare --copt=-Wno-write-strings --copt=-DNO_WARN_X86_INTRINSICS --cxxopt=-std=gnu++11"". # export DV_COPT_FLAGS=""--copt=-maltivec --copt=-Wno-sign-compare --copt=-Wno-write-strings --copt=-DNO_WARN_X86_INTRINSICS --cxxopt=-std=gnu++11 --copt=-fsigned-char --cxxopt=-fsigned-char"". # for GPU enabled. # fix ""ImportError: No module named google.protobuf"" by install protobuf from source. bazel clean. bazel shutdown. bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" deepvariant/... \. --action_env=LD_LIBRARY_PATH --test_env=LD_LIBRARY_PATH \. --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH \. --action_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH --test_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH \. --action_env=PYTHON_LIB_PATH=$PYTHON_LIB_PATH --test_env=PYTHON_LIB_PATH=$PYTHON_LIB_PATH \. --action_env=BAZEL_PYTHON=$BAZEL_PYTHON --test_env=BAZEL_PYTHON=$BAZEL_PYTHON >& output.log 2>&1 &. bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" deepvariant:gpu_tests --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH. # for CPU only. bazel test -c opt ${DV_COPT_FLAGS} ""$@"" deepvariant/... # build binary. bazel build -c opt ${DV_COPT_FLAGS} ""$@"" deepvariant:binaries --build_python_zip --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH. echo 'Expect a usage ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:18654,deployability,instal,install,18654,"github.com/google/deepvariant.git. cd deepvariant. # fetch all tags. git fetch --all --tags --prune. # check out tag. git checkout tags/v0.7.2. # Edit ctx.action with use_default_shell_env=True. vim ./third_party/clif.bzl. # Build and test. export LD_LIBRARY_PATH=/home/qilibj/inst/lib:/qilibj/inst/lib/python2.7/site-packages:$LD_LIBRARY_PATH. export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. export PYTHON_BIN_PATH=/home/qilibj/inst/bin/python. export PYTHON_LIB_PATH=/home/qilibj/inst/lib/python2.7/site-packages. export BAZEL_PYTHON=/home/qilibj/inst/bin/python. export DV_COPT_FLAGS=""--copt=-maltivec --copt=-Wno-sign-compare --copt=-Wno-write-strings --copt=-DNO_WARN_X86_INTRINSICS --cxxopt=-std=gnu++11"". # export DV_COPT_FLAGS=""--copt=-maltivec --copt=-Wno-sign-compare --copt=-Wno-write-strings --copt=-DNO_WARN_X86_INTRINSICS --cxxopt=-std=gnu++11 --copt=-fsigned-char --cxxopt=-fsigned-char"". # for GPU enabled. # fix ""ImportError: No module named google.protobuf"" by install protobuf from source. bazel clean. bazel shutdown. bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" deepvariant/... \. --action_env=LD_LIBRARY_PATH --test_env=LD_LIBRARY_PATH \. --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH \. --action_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH --test_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH \. --action_env=PYTHON_LIB_PATH=$PYTHON_LIB_PATH --test_env=PYTHON_LIB_PATH=$PYTHON_LIB_PATH \. --action_env=BAZEL_PYTHON=$BAZEL_PYTHON --test_env=BAZEL_PYTHON=$BAZEL_PYTHON >& output.log 2>&1 &. bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" deepvariant:gpu_tests --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH. # for CPU only. bazel test -c opt ${DV_COPT_FLAGS} ""$@"" deepvariant/... # build binary. bazel build -c opt ${DV_COPT_FLAGS} ""$@"" deepvariant:binaries --build_python_zip --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH. echo 'Expect a usage message:'. (bazel-bin/deepvariant/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:19198,deployability,log,log,19198,"ZEL_PYTHON=/home/qilibj/inst/bin/python. export DV_COPT_FLAGS=""--copt=-maltivec --copt=-Wno-sign-compare --copt=-Wno-write-strings --copt=-DNO_WARN_X86_INTRINSICS --cxxopt=-std=gnu++11"". # export DV_COPT_FLAGS=""--copt=-maltivec --copt=-Wno-sign-compare --copt=-Wno-write-strings --copt=-DNO_WARN_X86_INTRINSICS --cxxopt=-std=gnu++11 --copt=-fsigned-char --cxxopt=-fsigned-char"". # for GPU enabled. # fix ""ImportError: No module named google.protobuf"" by install protobuf from source. bazel clean. bazel shutdown. bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" deepvariant/... \. --action_env=LD_LIBRARY_PATH --test_env=LD_LIBRARY_PATH \. --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH \. --action_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH --test_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH \. --action_env=PYTHON_LIB_PATH=$PYTHON_LIB_PATH --test_env=PYTHON_LIB_PATH=$PYTHON_LIB_PATH \. --action_env=BAZEL_PYTHON=$BAZEL_PYTHON --test_env=BAZEL_PYTHON=$BAZEL_PYTHON >& output.log 2>&1 &. bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" deepvariant:gpu_tests --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH. # for CPU only. bazel test -c opt ${DV_COPT_FLAGS} ""$@"" deepvariant/... # build binary. bazel build -c opt ${DV_COPT_FLAGS} ""$@"" deepvariant:binaries --build_python_zip --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH. echo 'Expect a usage message:'. (bazel-bin/deepvariant/call_variants --help || : ) | grep '/call_variants.py:'. bazel build :licenses_zip --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH. ```. ## Fix DV Error. ```bash. ################################################################################. # bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@""//deepvariant:resources_test. # use lscpu to show the actual CPU number. ################################################################################. python -c ""import multiprocessing; print(multiprocessing.cp",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:19437,deployability,build,build,19437,"sign-compare --copt=-Wno-write-strings --copt=-DNO_WARN_X86_INTRINSICS --cxxopt=-std=gnu++11 --copt=-fsigned-char --cxxopt=-fsigned-char"". # for GPU enabled. # fix ""ImportError: No module named google.protobuf"" by install protobuf from source. bazel clean. bazel shutdown. bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" deepvariant/... \. --action_env=LD_LIBRARY_PATH --test_env=LD_LIBRARY_PATH \. --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH \. --action_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH --test_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH \. --action_env=PYTHON_LIB_PATH=$PYTHON_LIB_PATH --test_env=PYTHON_LIB_PATH=$PYTHON_LIB_PATH \. --action_env=BAZEL_PYTHON=$BAZEL_PYTHON --test_env=BAZEL_PYTHON=$BAZEL_PYTHON >& output.log 2>&1 &. bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" deepvariant:gpu_tests --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH. # for CPU only. bazel test -c opt ${DV_COPT_FLAGS} ""$@"" deepvariant/... # build binary. bazel build -c opt ${DV_COPT_FLAGS} ""$@"" deepvariant:binaries --build_python_zip --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH. echo 'Expect a usage message:'. (bazel-bin/deepvariant/call_variants --help || : ) | grep '/call_variants.py:'. bazel build :licenses_zip --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH. ```. ## Fix DV Error. ```bash. ################################################################################. # bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@""//deepvariant:resources_test. # use lscpu to show the actual CPU number. ################################################################################. python -c ""import multiprocessing; print(multiprocessing.cpu_count())"" #160. python -c ""import psutil;print(p/sutil.cpu_count. ())"" #160. vim deepvariant/resources.py. --------------------------------. def _get_cpu_count():. """"""Gets the number of physical cores in this machine. Returns:. int >= 1 i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:19457,deployability,build,build,19457,"-Wno-write-strings --copt=-DNO_WARN_X86_INTRINSICS --cxxopt=-std=gnu++11 --copt=-fsigned-char --cxxopt=-fsigned-char"". # for GPU enabled. # fix ""ImportError: No module named google.protobuf"" by install protobuf from source. bazel clean. bazel shutdown. bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" deepvariant/... \. --action_env=LD_LIBRARY_PATH --test_env=LD_LIBRARY_PATH \. --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH \. --action_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH --test_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH \. --action_env=PYTHON_LIB_PATH=$PYTHON_LIB_PATH --test_env=PYTHON_LIB_PATH=$PYTHON_LIB_PATH \. --action_env=BAZEL_PYTHON=$BAZEL_PYTHON --test_env=BAZEL_PYTHON=$BAZEL_PYTHON >& output.log 2>&1 &. bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" deepvariant:gpu_tests --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH. # for CPU only. bazel test -c opt ${DV_COPT_FLAGS} ""$@"" deepvariant/... # build binary. bazel build -c opt ${DV_COPT_FLAGS} ""$@"" deepvariant:binaries --build_python_zip --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH. echo 'Expect a usage message:'. (bazel-bin/deepvariant/call_variants --help || : ) | grep '/call_variants.py:'. bazel build :licenses_zip --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH. ```. ## Fix DV Error. ```bash. ################################################################################. # bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@""//deepvariant:resources_test. # use lscpu to show the actual CPU number. ################################################################################. python -c ""import multiprocessing; print(multiprocessing.cpu_count())"" #160. python -c ""import psutil;print(p/sutil.cpu_count. ())"" #160. vim deepvariant/resources.py. --------------------------------. def _get_cpu_count():. """"""Gets the number of physical cores in this machine. Returns:. int >= 1 if the call to get th",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:19721,deployability,build,build,19721,"-c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" deepvariant/... \. --action_env=LD_LIBRARY_PATH --test_env=LD_LIBRARY_PATH \. --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH \. --action_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH --test_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH \. --action_env=PYTHON_LIB_PATH=$PYTHON_LIB_PATH --test_env=PYTHON_LIB_PATH=$PYTHON_LIB_PATH \. --action_env=BAZEL_PYTHON=$BAZEL_PYTHON --test_env=BAZEL_PYTHON=$BAZEL_PYTHON >& output.log 2>&1 &. bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" deepvariant:gpu_tests --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH. # for CPU only. bazel test -c opt ${DV_COPT_FLAGS} ""$@"" deepvariant/... # build binary. bazel build -c opt ${DV_COPT_FLAGS} ""$@"" deepvariant:binaries --build_python_zip --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH. echo 'Expect a usage message:'. (bazel-bin/deepvariant/call_variants --help || : ) | grep '/call_variants.py:'. bazel build :licenses_zip --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH. ```. ## Fix DV Error. ```bash. ################################################################################. # bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@""//deepvariant:resources_test. # use lscpu to show the actual CPU number. ################################################################################. python -c ""import multiprocessing; print(multiprocessing.cpu_count())"" #160. python -c ""import psutil;print(p/sutil.cpu_count. ())"" #160. vim deepvariant/resources.py. --------------------------------. def _get_cpu_count():. """"""Gets the number of physical cores in this machine. Returns:. int >= 1 if the call to get the cpu_count succeeded, or 0 if not. """""". # return psutil.cpu_count(logical=False) or 0 ==> comment. return 20. --------------------------------. vim deepvariant/resources_test.py. --------------------------------. def test_metrics_is_ok_when_cpu_count_returns_none",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:20295,deployability,resourc,resources,20295,"_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH. # for CPU only. bazel test -c opt ${DV_COPT_FLAGS} ""$@"" deepvariant/... # build binary. bazel build -c opt ${DV_COPT_FLAGS} ""$@"" deepvariant:binaries --build_python_zip --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH. echo 'Expect a usage message:'. (bazel-bin/deepvariant/call_variants --help || : ) | grep '/call_variants.py:'. bazel build :licenses_zip --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH. ```. ## Fix DV Error. ```bash. ################################################################################. # bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@""//deepvariant:resources_test. # use lscpu to show the actual CPU number. ################################################################################. python -c ""import multiprocessing; print(multiprocessing.cpu_count())"" #160. python -c ""import psutil;print(p/sutil.cpu_count. ())"" #160. vim deepvariant/resources.py. --------------------------------. def _get_cpu_count():. """"""Gets the number of physical cores in this machine. Returns:. int >= 1 if the call to get the cpu_count succeeded, or 0 if not. """""". # return psutil.cpu_count(logical=False) or 0 ==> comment. return 20. --------------------------------. vim deepvariant/resources_test.py. --------------------------------. def test_metrics_is_ok_when_cpu_count_returns_none(self):. # Some psutil functions, such as cpu_freq(), can return None depending on. # the environment; make sure we don't crash when that occurs. with mock.patch.object(resources.psutil, 'cpu_count', return_value=None):. with resources.ResourceMonitor() as monitor:. #self.assertEqual(monitor.metrics().physical_core_count, 0) ==> comment. self.assertEqual(monitor.metrics().physical_core_count, 20). --------------------------------. ##########################################################################. # //deepvariant/realigner/allele_count_linear:generate_trained_mo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:20527,deployability,log,logical,20527," --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH. echo 'Expect a usage message:'. (bazel-bin/deepvariant/call_variants --help || : ) | grep '/call_variants.py:'. bazel build :licenses_zip --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH. ```. ## Fix DV Error. ```bash. ################################################################################. # bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@""//deepvariant:resources_test. # use lscpu to show the actual CPU number. ################################################################################. python -c ""import multiprocessing; print(multiprocessing.cpu_count())"" #160. python -c ""import psutil;print(p/sutil.cpu_count. ())"" #160. vim deepvariant/resources.py. --------------------------------. def _get_cpu_count():. """"""Gets the number of physical cores in this machine. Returns:. int >= 1 if the call to get the cpu_count succeeded, or 0 if not. """""". # return psutil.cpu_count(logical=False) or 0 ==> comment. return 20. --------------------------------. vim deepvariant/resources_test.py. --------------------------------. def test_metrics_is_ok_when_cpu_count_returns_none(self):. # Some psutil functions, such as cpu_freq(), can return None depending on. # the environment; make sure we don't crash when that occurs. with mock.patch.object(resources.psutil, 'cpu_count', return_value=None):. with resources.ResourceMonitor() as monitor:. #self.assertEqual(monitor.metrics().physical_core_count, 0) ==> comment. self.assertEqual(monitor.metrics().physical_core_count, 20). --------------------------------. ##########################################################################. # //deepvariant/realigner/allele_count_linear:generate_trained_model_test. # ImportError: /root/.local/lib/python2.7/site-packages/sklearn/__check_build/_check_build.so: undefined symbol: PyUnicodeUCS4_DecodeUTF8. ##########################################################################. # rein",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:20794,deployability,depend,depending,20794,"$PYTHONPATH. ```. ## Fix DV Error. ```bash. ################################################################################. # bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@""//deepvariant:resources_test. # use lscpu to show the actual CPU number. ################################################################################. python -c ""import multiprocessing; print(multiprocessing.cpu_count())"" #160. python -c ""import psutil;print(p/sutil.cpu_count. ())"" #160. vim deepvariant/resources.py. --------------------------------. def _get_cpu_count():. """"""Gets the number of physical cores in this machine. Returns:. int >= 1 if the call to get the cpu_count succeeded, or 0 if not. """""". # return psutil.cpu_count(logical=False) or 0 ==> comment. return 20. --------------------------------. vim deepvariant/resources_test.py. --------------------------------. def test_metrics_is_ok_when_cpu_count_returns_none(self):. # Some psutil functions, such as cpu_freq(), can return None depending on. # the environment; make sure we don't crash when that occurs. with mock.patch.object(resources.psutil, 'cpu_count', return_value=None):. with resources.ResourceMonitor() as monitor:. #self.assertEqual(monitor.metrics().physical_core_count, 0) ==> comment. self.assertEqual(monitor.metrics().physical_core_count, 20). --------------------------------. ##########################################################################. # //deepvariant/realigner/allele_count_linear:generate_trained_model_test. # ImportError: /root/.local/lib/python2.7/site-packages/sklearn/__check_build/_check_build.so: undefined symbol: PyUnicodeUCS4_DecodeUTF8. ##########################################################################. # reinstall numpy, scipy, Cpython, scikit-learn to fix with AT built Python. python -c ""import numpy"" # prequests of TF 1.12.0. python -c ""import scipy"" # prequests of TF 1.12.0. pip install Cython --force-reinstall --no-deos. pip install scikit-learn --force-reinstall --no",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:20880,deployability,patch,patch,20880,"########################################. # bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@""//deepvariant:resources_test. # use lscpu to show the actual CPU number. ################################################################################. python -c ""import multiprocessing; print(multiprocessing.cpu_count())"" #160. python -c ""import psutil;print(p/sutil.cpu_count. ())"" #160. vim deepvariant/resources.py. --------------------------------. def _get_cpu_count():. """"""Gets the number of physical cores in this machine. Returns:. int >= 1 if the call to get the cpu_count succeeded, or 0 if not. """""". # return psutil.cpu_count(logical=False) or 0 ==> comment. return 20. --------------------------------. vim deepvariant/resources_test.py. --------------------------------. def test_metrics_is_ok_when_cpu_count_returns_none(self):. # Some psutil functions, such as cpu_freq(), can return None depending on. # the environment; make sure we don't crash when that occurs. with mock.patch.object(resources.psutil, 'cpu_count', return_value=None):. with resources.ResourceMonitor() as monitor:. #self.assertEqual(monitor.metrics().physical_core_count, 0) ==> comment. self.assertEqual(monitor.metrics().physical_core_count, 20). --------------------------------. ##########################################################################. # //deepvariant/realigner/allele_count_linear:generate_trained_model_test. # ImportError: /root/.local/lib/python2.7/site-packages/sklearn/__check_build/_check_build.so: undefined symbol: PyUnicodeUCS4_DecodeUTF8. ##########################################################################. # reinstall numpy, scipy, Cpython, scikit-learn to fix with AT built Python. python -c ""import numpy"" # prequests of TF 1.12.0. python -c ""import scipy"" # prequests of TF 1.12.0. pip install Cython --force-reinstall --no-deos. pip install scikit-learn --force-reinstall --no-deos. # build from source. wget https://github.com/scikit-learn/scikit-learn/archiv",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:20893,deployability,resourc,resources,20893,"#########################. # bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@""//deepvariant:resources_test. # use lscpu to show the actual CPU number. ################################################################################. python -c ""import multiprocessing; print(multiprocessing.cpu_count())"" #160. python -c ""import psutil;print(p/sutil.cpu_count. ())"" #160. vim deepvariant/resources.py. --------------------------------. def _get_cpu_count():. """"""Gets the number of physical cores in this machine. Returns:. int >= 1 if the call to get the cpu_count succeeded, or 0 if not. """""". # return psutil.cpu_count(logical=False) or 0 ==> comment. return 20. --------------------------------. vim deepvariant/resources_test.py. --------------------------------. def test_metrics_is_ok_when_cpu_count_returns_none(self):. # Some psutil functions, such as cpu_freq(), can return None depending on. # the environment; make sure we don't crash when that occurs. with mock.patch.object(resources.psutil, 'cpu_count', return_value=None):. with resources.ResourceMonitor() as monitor:. #self.assertEqual(monitor.metrics().physical_core_count, 0) ==> comment. self.assertEqual(monitor.metrics().physical_core_count, 20). --------------------------------. ##########################################################################. # //deepvariant/realigner/allele_count_linear:generate_trained_model_test. # ImportError: /root/.local/lib/python2.7/site-packages/sklearn/__check_build/_check_build.so: undefined symbol: PyUnicodeUCS4_DecodeUTF8. ##########################################################################. # reinstall numpy, scipy, Cpython, scikit-learn to fix with AT built Python. python -c ""import numpy"" # prequests of TF 1.12.0. python -c ""import scipy"" # prequests of TF 1.12.0. pip install Cython --force-reinstall --no-deos. pip install scikit-learn --force-reinstall --no-deos. # build from source. wget https://github.com/scikit-learn/scikit-learn/archive/0.20.2.tar.gz",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:20950,deployability,resourc,resources,20950,"st_jobs=1 ${DV_COPT_FLAGS} ""$@""//deepvariant:resources_test. # use lscpu to show the actual CPU number. ################################################################################. python -c ""import multiprocessing; print(multiprocessing.cpu_count())"" #160. python -c ""import psutil;print(p/sutil.cpu_count. ())"" #160. vim deepvariant/resources.py. --------------------------------. def _get_cpu_count():. """"""Gets the number of physical cores in this machine. Returns:. int >= 1 if the call to get the cpu_count succeeded, or 0 if not. """""". # return psutil.cpu_count(logical=False) or 0 ==> comment. return 20. --------------------------------. vim deepvariant/resources_test.py. --------------------------------. def test_metrics_is_ok_when_cpu_count_returns_none(self):. # Some psutil functions, such as cpu_freq(), can return None depending on. # the environment; make sure we don't crash when that occurs. with mock.patch.object(resources.psutil, 'cpu_count', return_value=None):. with resources.ResourceMonitor() as monitor:. #self.assertEqual(monitor.metrics().physical_core_count, 0) ==> comment. self.assertEqual(monitor.metrics().physical_core_count, 20). --------------------------------. ##########################################################################. # //deepvariant/realigner/allele_count_linear:generate_trained_model_test. # ImportError: /root/.local/lib/python2.7/site-packages/sklearn/__check_build/_check_build.so: undefined symbol: PyUnicodeUCS4_DecodeUTF8. ##########################################################################. # reinstall numpy, scipy, Cpython, scikit-learn to fix with AT built Python. python -c ""import numpy"" # prequests of TF 1.12.0. python -c ""import scipy"" # prequests of TF 1.12.0. pip install Cython --force-reinstall --no-deos. pip install scikit-learn --force-reinstall --no-deos. # build from source. wget https://github.com/scikit-learn/scikit-learn/archive/0.20.2.tar.gz. tar zxvf 0.20.2.tar.gz. cd scikit-learn-0.20.2. python ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:20960,deployability,Resourc,ResourceMonitor,20960,"V_COPT_FLAGS} ""$@""//deepvariant:resources_test. # use lscpu to show the actual CPU number. ################################################################################. python -c ""import multiprocessing; print(multiprocessing.cpu_count())"" #160. python -c ""import psutil;print(p/sutil.cpu_count. ())"" #160. vim deepvariant/resources.py. --------------------------------. def _get_cpu_count():. """"""Gets the number of physical cores in this machine. Returns:. int >= 1 if the call to get the cpu_count succeeded, or 0 if not. """""". # return psutil.cpu_count(logical=False) or 0 ==> comment. return 20. --------------------------------. vim deepvariant/resources_test.py. --------------------------------. def test_metrics_is_ok_when_cpu_count_returns_none(self):. # Some psutil functions, such as cpu_freq(), can return None depending on. # the environment; make sure we don't crash when that occurs. with mock.patch.object(resources.psutil, 'cpu_count', return_value=None):. with resources.ResourceMonitor() as monitor:. #self.assertEqual(monitor.metrics().physical_core_count, 0) ==> comment. self.assertEqual(monitor.metrics().physical_core_count, 20). --------------------------------. ##########################################################################. # //deepvariant/realigner/allele_count_linear:generate_trained_model_test. # ImportError: /root/.local/lib/python2.7/site-packages/sklearn/__check_build/_check_build.so: undefined symbol: PyUnicodeUCS4_DecodeUTF8. ##########################################################################. # reinstall numpy, scipy, Cpython, scikit-learn to fix with AT built Python. python -c ""import numpy"" # prequests of TF 1.12.0. python -c ""import scipy"" # prequests of TF 1.12.0. pip install Cython --force-reinstall --no-deos. pip install scikit-learn --force-reinstall --no-deos. # build from source. wget https://github.com/scikit-learn/scikit-learn/archive/0.20.2.tar.gz. tar zxvf 0.20.2.tar.gz. cd scikit-learn-0.20.2. python setup.py bdis",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:20981,deployability,monitor,monitor,20981,"""//deepvariant:resources_test. # use lscpu to show the actual CPU number. ################################################################################. python -c ""import multiprocessing; print(multiprocessing.cpu_count())"" #160. python -c ""import psutil;print(p/sutil.cpu_count. ())"" #160. vim deepvariant/resources.py. --------------------------------. def _get_cpu_count():. """"""Gets the number of physical cores in this machine. Returns:. int >= 1 if the call to get the cpu_count succeeded, or 0 if not. """""". # return psutil.cpu_count(logical=False) or 0 ==> comment. return 20. --------------------------------. vim deepvariant/resources_test.py. --------------------------------. def test_metrics_is_ok_when_cpu_count_returns_none(self):. # Some psutil functions, such as cpu_freq(), can return None depending on. # the environment; make sure we don't crash when that occurs. with mock.patch.object(resources.psutil, 'cpu_count', return_value=None):. with resources.ResourceMonitor() as monitor:. #self.assertEqual(monitor.metrics().physical_core_count, 0) ==> comment. self.assertEqual(monitor.metrics().physical_core_count, 20). --------------------------------. ##########################################################################. # //deepvariant/realigner/allele_count_linear:generate_trained_model_test. # ImportError: /root/.local/lib/python2.7/site-packages/sklearn/__check_build/_check_build.so: undefined symbol: PyUnicodeUCS4_DecodeUTF8. ##########################################################################. # reinstall numpy, scipy, Cpython, scikit-learn to fix with AT built Python. python -c ""import numpy"" # prequests of TF 1.12.0. python -c ""import scipy"" # prequests of TF 1.12.0. pip install Cython --force-reinstall --no-deos. pip install scikit-learn --force-reinstall --no-deos. # build from source. wget https://github.com/scikit-learn/scikit-learn/archive/0.20.2.tar.gz. tar zxvf 0.20.2.tar.gz. cd scikit-learn-0.20.2. python setup.py bdist_wheel. # verify",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:21009,deployability,monitor,monitor,21009,"t. # use lscpu to show the actual CPU number. ################################################################################. python -c ""import multiprocessing; print(multiprocessing.cpu_count())"" #160. python -c ""import psutil;print(p/sutil.cpu_count. ())"" #160. vim deepvariant/resources.py. --------------------------------. def _get_cpu_count():. """"""Gets the number of physical cores in this machine. Returns:. int >= 1 if the call to get the cpu_count succeeded, or 0 if not. """""". # return psutil.cpu_count(logical=False) or 0 ==> comment. return 20. --------------------------------. vim deepvariant/resources_test.py. --------------------------------. def test_metrics_is_ok_when_cpu_count_returns_none(self):. # Some psutil functions, such as cpu_freq(), can return None depending on. # the environment; make sure we don't crash when that occurs. with mock.patch.object(resources.psutil, 'cpu_count', return_value=None):. with resources.ResourceMonitor() as monitor:. #self.assertEqual(monitor.metrics().physical_core_count, 0) ==> comment. self.assertEqual(monitor.metrics().physical_core_count, 20). --------------------------------. ##########################################################################. # //deepvariant/realigner/allele_count_linear:generate_trained_model_test. # ImportError: /root/.local/lib/python2.7/site-packages/sklearn/__check_build/_check_build.so: undefined symbol: PyUnicodeUCS4_DecodeUTF8. ##########################################################################. # reinstall numpy, scipy, Cpython, scikit-learn to fix with AT built Python. python -c ""import numpy"" # prequests of TF 1.12.0. python -c ""import scipy"" # prequests of TF 1.12.0. pip install Cython --force-reinstall --no-deos. pip install scikit-learn --force-reinstall --no-deos. # build from source. wget https://github.com/scikit-learn/scikit-learn/archive/0.20.2.tar.gz. tar zxvf 0.20.2.tar.gz. cd scikit-learn-0.20.2. python setup.py bdist_wheel. # verify. python -c ""from sklearn.ex",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:21081,deployability,monitor,monitor,21081,"######################################################. python -c ""import multiprocessing; print(multiprocessing.cpu_count())"" #160. python -c ""import psutil;print(p/sutil.cpu_count. ())"" #160. vim deepvariant/resources.py. --------------------------------. def _get_cpu_count():. """"""Gets the number of physical cores in this machine. Returns:. int >= 1 if the call to get the cpu_count succeeded, or 0 if not. """""". # return psutil.cpu_count(logical=False) or 0 ==> comment. return 20. --------------------------------. vim deepvariant/resources_test.py. --------------------------------. def test_metrics_is_ok_when_cpu_count_returns_none(self):. # Some psutil functions, such as cpu_freq(), can return None depending on. # the environment; make sure we don't crash when that occurs. with mock.patch.object(resources.psutil, 'cpu_count', return_value=None):. with resources.ResourceMonitor() as monitor:. #self.assertEqual(monitor.metrics().physical_core_count, 0) ==> comment. self.assertEqual(monitor.metrics().physical_core_count, 20). --------------------------------. ##########################################################################. # //deepvariant/realigner/allele_count_linear:generate_trained_model_test. # ImportError: /root/.local/lib/python2.7/site-packages/sklearn/__check_build/_check_build.so: undefined symbol: PyUnicodeUCS4_DecodeUTF8. ##########################################################################. # reinstall numpy, scipy, Cpython, scikit-learn to fix with AT built Python. python -c ""import numpy"" # prequests of TF 1.12.0. python -c ""import scipy"" # prequests of TF 1.12.0. pip install Cython --force-reinstall --no-deos. pip install scikit-learn --force-reinstall --no-deos. # build from source. wget https://github.com/scikit-learn/scikit-learn/archive/0.20.2.tar.gz. tar zxvf 0.20.2.tar.gz. cd scikit-learn-0.20.2. python setup.py bdist_wheel. # verify. python -c ""from sklearn.externals import joblib"". ################################################",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:21708,deployability,instal,install,21708,"returns_none(self):. # Some psutil functions, such as cpu_freq(), can return None depending on. # the environment; make sure we don't crash when that occurs. with mock.patch.object(resources.psutil, 'cpu_count', return_value=None):. with resources.ResourceMonitor() as monitor:. #self.assertEqual(monitor.metrics().physical_core_count, 0) ==> comment. self.assertEqual(monitor.metrics().physical_core_count, 20). --------------------------------. ##########################################################################. # //deepvariant/realigner/allele_count_linear:generate_trained_model_test. # ImportError: /root/.local/lib/python2.7/site-packages/sklearn/__check_build/_check_build.so: undefined symbol: PyUnicodeUCS4_DecodeUTF8. ##########################################################################. # reinstall numpy, scipy, Cpython, scikit-learn to fix with AT built Python. python -c ""import numpy"" # prequests of TF 1.12.0. python -c ""import scipy"" # prequests of TF 1.12.0. pip install Cython --force-reinstall --no-deos. pip install scikit-learn --force-reinstall --no-deos. # build from source. wget https://github.com/scikit-learn/scikit-learn/archive/0.20.2.tar.gz. tar zxvf 0.20.2.tar.gz. cd scikit-learn-0.20.2. python setup.py bdist_wheel. # verify. python -c ""from sklearn.externals import joblib"". ##########################################################################. # //deepvariant/labeler:haplotype_labeler_test. # bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@""//deepvariant/labeler:haplotype_labeler_test --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH. ##########################################################################. # fail due to mock data, open an issue in github. https://github.com/google/deepvariant/issues/154. ##########################################################################. # //deepvariant:make_examples_test. # bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@""//deepvariant:make_exampl",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:21756,deployability,instal,install,21756,"ch as cpu_freq(), can return None depending on. # the environment; make sure we don't crash when that occurs. with mock.patch.object(resources.psutil, 'cpu_count', return_value=None):. with resources.ResourceMonitor() as monitor:. #self.assertEqual(monitor.metrics().physical_core_count, 0) ==> comment. self.assertEqual(monitor.metrics().physical_core_count, 20). --------------------------------. ##########################################################################. # //deepvariant/realigner/allele_count_linear:generate_trained_model_test. # ImportError: /root/.local/lib/python2.7/site-packages/sklearn/__check_build/_check_build.so: undefined symbol: PyUnicodeUCS4_DecodeUTF8. ##########################################################################. # reinstall numpy, scipy, Cpython, scikit-learn to fix with AT built Python. python -c ""import numpy"" # prequests of TF 1.12.0. python -c ""import scipy"" # prequests of TF 1.12.0. pip install Cython --force-reinstall --no-deos. pip install scikit-learn --force-reinstall --no-deos. # build from source. wget https://github.com/scikit-learn/scikit-learn/archive/0.20.2.tar.gz. tar zxvf 0.20.2.tar.gz. cd scikit-learn-0.20.2. python setup.py bdist_wheel. # verify. python -c ""from sklearn.externals import joblib"". ##########################################################################. # //deepvariant/labeler:haplotype_labeler_test. # bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@""//deepvariant/labeler:haplotype_labeler_test --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH. ##########################################################################. # fail due to mock data, open an issue in github. https://github.com/google/deepvariant/issues/154. ##########################################################################. # //deepvariant:make_examples_test. # bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@""//deepvariant:make_examples_test --action_env=PYTHONPATH=$PYTHONPATH --te",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:21808,deployability,build,build,21808,"he environment; make sure we don't crash when that occurs. with mock.patch.object(resources.psutil, 'cpu_count', return_value=None):. with resources.ResourceMonitor() as monitor:. #self.assertEqual(monitor.metrics().physical_core_count, 0) ==> comment. self.assertEqual(monitor.metrics().physical_core_count, 20). --------------------------------. ##########################################################################. # //deepvariant/realigner/allele_count_linear:generate_trained_model_test. # ImportError: /root/.local/lib/python2.7/site-packages/sklearn/__check_build/_check_build.so: undefined symbol: PyUnicodeUCS4_DecodeUTF8. ##########################################################################. # reinstall numpy, scipy, Cpython, scikit-learn to fix with AT built Python. python -c ""import numpy"" # prequests of TF 1.12.0. python -c ""import scipy"" # prequests of TF 1.12.0. pip install Cython --force-reinstall --no-deos. pip install scikit-learn --force-reinstall --no-deos. # build from source. wget https://github.com/scikit-learn/scikit-learn/archive/0.20.2.tar.gz. tar zxvf 0.20.2.tar.gz. cd scikit-learn-0.20.2. python setup.py bdist_wheel. # verify. python -c ""from sklearn.externals import joblib"". ##########################################################################. # //deepvariant/labeler:haplotype_labeler_test. # bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@""//deepvariant/labeler:haplotype_labeler_test --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH. ##########################################################################. # fail due to mock data, open an issue in github. https://github.com/google/deepvariant/issues/154. ##########################################################################. # //deepvariant:make_examples_test. # bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@""//deepvariant:make_examples_test --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH. # internvaltree v3 h",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:22416,deployability,fail,fail,22416,"urces.ResourceMonitor() as monitor:. #self.assertEqual(monitor.metrics().physical_core_count, 0) ==> comment. self.assertEqual(monitor.metrics().physical_core_count, 20). --------------------------------. ##########################################################################. # //deepvariant/realigner/allele_count_linear:generate_trained_model_test. # ImportError: /root/.local/lib/python2.7/site-packages/sklearn/__check_build/_check_build.so: undefined symbol: PyUnicodeUCS4_DecodeUTF8. ##########################################################################. # reinstall numpy, scipy, Cpython, scikit-learn to fix with AT built Python. python -c ""import numpy"" # prequests of TF 1.12.0. python -c ""import scipy"" # prequests of TF 1.12.0. pip install Cython --force-reinstall --no-deos. pip install scikit-learn --force-reinstall --no-deos. # build from source. wget https://github.com/scikit-learn/scikit-learn/archive/0.20.2.tar.gz. tar zxvf 0.20.2.tar.gz. cd scikit-learn-0.20.2. python setup.py bdist_wheel. # verify. python -c ""from sklearn.externals import joblib"". ##########################################################################. # //deepvariant/labeler:haplotype_labeler_test. # bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@""//deepvariant/labeler:haplotype_labeler_test --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH. ##########################################################################. # fail due to mock data, open an issue in github. https://github.com/google/deepvariant/issues/154. ##########################################################################. # //deepvariant:make_examples_test. # bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@""//deepvariant:make_examples_test --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH. # internvaltree v3 has some API changes with v2. ##########################################################################. pip install 'intervaltree==2.1.0'. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:22819,deployability,API,API,22819,"urces.ResourceMonitor() as monitor:. #self.assertEqual(monitor.metrics().physical_core_count, 0) ==> comment. self.assertEqual(monitor.metrics().physical_core_count, 20). --------------------------------. ##########################################################################. # //deepvariant/realigner/allele_count_linear:generate_trained_model_test. # ImportError: /root/.local/lib/python2.7/site-packages/sklearn/__check_build/_check_build.so: undefined symbol: PyUnicodeUCS4_DecodeUTF8. ##########################################################################. # reinstall numpy, scipy, Cpython, scikit-learn to fix with AT built Python. python -c ""import numpy"" # prequests of TF 1.12.0. python -c ""import scipy"" # prequests of TF 1.12.0. pip install Cython --force-reinstall --no-deos. pip install scikit-learn --force-reinstall --no-deos. # build from source. wget https://github.com/scikit-learn/scikit-learn/archive/0.20.2.tar.gz. tar zxvf 0.20.2.tar.gz. cd scikit-learn-0.20.2. python setup.py bdist_wheel. # verify. python -c ""from sklearn.externals import joblib"". ##########################################################################. # //deepvariant/labeler:haplotype_labeler_test. # bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@""//deepvariant/labeler:haplotype_labeler_test --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH. ##########################################################################. # fail due to mock data, open an issue in github. https://github.com/google/deepvariant/issues/154. ##########################################################################. # //deepvariant:make_examples_test. # bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@""//deepvariant:make_examples_test --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH. # internvaltree v3 has some API changes with v2. ##########################################################################. pip install 'intervaltree==2.1.0'. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:22920,deployability,instal,install,22920,"urces.ResourceMonitor() as monitor:. #self.assertEqual(monitor.metrics().physical_core_count, 0) ==> comment. self.assertEqual(monitor.metrics().physical_core_count, 20). --------------------------------. ##########################################################################. # //deepvariant/realigner/allele_count_linear:generate_trained_model_test. # ImportError: /root/.local/lib/python2.7/site-packages/sklearn/__check_build/_check_build.so: undefined symbol: PyUnicodeUCS4_DecodeUTF8. ##########################################################################. # reinstall numpy, scipy, Cpython, scikit-learn to fix with AT built Python. python -c ""import numpy"" # prequests of TF 1.12.0. python -c ""import scipy"" # prequests of TF 1.12.0. pip install Cython --force-reinstall --no-deos. pip install scikit-learn --force-reinstall --no-deos. # build from source. wget https://github.com/scikit-learn/scikit-learn/archive/0.20.2.tar.gz. tar zxvf 0.20.2.tar.gz. cd scikit-learn-0.20.2. python setup.py bdist_wheel. # verify. python -c ""from sklearn.externals import joblib"". ##########################################################################. # //deepvariant/labeler:haplotype_labeler_test. # bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@""//deepvariant/labeler:haplotype_labeler_test --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH. ##########################################################################. # fail due to mock data, open an issue in github. https://github.com/google/deepvariant/issues/154. ##########################################################################. # //deepvariant:make_examples_test. # bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@""//deepvariant:make_examples_test --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH. # internvaltree v3 has some API changes with v2. ##########################################################################. pip install 'intervaltree==2.1.0'. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:229,energy efficiency,CPU,CPU,229,"Hi @DiableJambe ,. Sorry for my late response, I do not have a docker image yet. And I attached the detailed steps here for your reference. It's a bit long :). # DeepVariant. ## Environment. ```bash. # Power8 environment. export CPU=power8. export HOMEPATH=/home/qilibj. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # AT 11.0 environment. source /etc/profile. module load at11.0. export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # python2 and pip environment. export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH. export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. export PYTHON_BIN_PATH=/home/qilibj/inst/bin/python. export PYTHON_LIB_PATH=/home/qilibj/inst/lib/python2.7/site-packages. ```. ## Cmake 3.13.3. Build and install doc: [https://cmake.org/install/](https://cmake.org/install/). >Note: only can be built with system gcc, AT11.0 will cause build failure. ```bash. # download source code. wget https://github.com/Kitware/CMake/releases/download/v3.13.3/cmake-3.13.3.tar.gz. tar -zxvf cmake-3.13.3.tar.gz. cd cmake-3.13.3. # build scirpt. ./bootstrap. make -j20. make -j20 install. export PATH=/usr/local/bin:$PATH. ```. ## Protobuf 3.6.1 C++ static. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from /usr/bin/gcc, static for bazel. ```bash. # download source code. wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:423,energy efficiency,profil,profile,423,"Hi @DiableJambe ,. Sorry for my late response, I do not have a docker image yet. And I attached the detailed steps here for your reference. It's a bit long :). # DeepVariant. ## Environment. ```bash. # Power8 environment. export CPU=power8. export HOMEPATH=/home/qilibj. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # AT 11.0 environment. source /etc/profile. module load at11.0. export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # python2 and pip environment. export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH. export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. export PYTHON_BIN_PATH=/home/qilibj/inst/bin/python. export PYTHON_LIB_PATH=/home/qilibj/inst/lib/python2.7/site-packages. ```. ## Cmake 3.13.3. Build and install doc: [https://cmake.org/install/](https://cmake.org/install/). >Note: only can be built with system gcc, AT11.0 will cause build failure. ```bash. # download source code. wget https://github.com/Kitware/CMake/releases/download/v3.13.3/cmake-3.13.3.tar.gz. tar -zxvf cmake-3.13.3.tar.gz. cd cmake-3.13.3. # build scirpt. ./bootstrap. make -j20. make -j20 install. export PATH=/usr/local/bin:$PATH. ```. ## Protobuf 3.6.1 C++ static. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from /usr/bin/gcc, static for bazel. ```bash. # download source code. wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:439,energy efficiency,load,load,439,"Hi @DiableJambe ,. Sorry for my late response, I do not have a docker image yet. And I attached the detailed steps here for your reference. It's a bit long :). # DeepVariant. ## Environment. ```bash. # Power8 environment. export CPU=power8. export HOMEPATH=/home/qilibj. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # AT 11.0 environment. source /etc/profile. module load at11.0. export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # python2 and pip environment. export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH. export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. export PYTHON_BIN_PATH=/home/qilibj/inst/bin/python. export PYTHON_LIB_PATH=/home/qilibj/inst/lib/python2.7/site-packages. ```. ## Cmake 3.13.3. Build and install doc: [https://cmake.org/install/](https://cmake.org/install/). >Note: only can be built with system gcc, AT11.0 will cause build failure. ```bash. # download source code. wget https://github.com/Kitware/CMake/releases/download/v3.13.3/cmake-3.13.3.tar.gz. tar -zxvf cmake-3.13.3.tar.gz. cd cmake-3.13.3. # build scirpt. ./bootstrap. make -j20. make -j20 install. export PATH=/usr/local/bin:$PATH. ```. ## Protobuf 3.6.1 C++ static. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from /usr/bin/gcc, static for bazel. ```bash. # download source code. wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:1759,energy efficiency,CPU,CPU,1759,"python2.7/site-packages. ```. ## Cmake 3.13.3. Build and install doc: [https://cmake.org/install/](https://cmake.org/install/). >Note: only can be built with system gcc, AT11.0 will cause build failure. ```bash. # download source code. wget https://github.com/Kitware/CMake/releases/download/v3.13.3/cmake-3.13.3.tar.gz. tar -zxvf cmake-3.13.3.tar.gz. cd cmake-3.13.3. # build scirpt. ./bootstrap. make -j20. make -j20 install. export PATH=/usr/local/bin:$PATH. ```. ## Protobuf 3.6.1 C++ static. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from /usr/bin/gcc, static for bazel. ```bash. # download source code. wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --disable-shared --enable-static. make clean. make -j20. make -j20 check # check if protobuf build is good. # install. make install. sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig. #cd .. # verify. echo ""$(which protoc)"". echo ""$(protoc --version)"". ```. ## Bazel 0.15.0. Build instructions: [https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel](https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel). Edit crosstool: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). > Note: Bazel 0.15.0 should be built from ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:2000,energy efficiency,CPU,CPU,2000,"https://github.com/Kitware/CMake/releases/download/v3.13.3/cmake-3.13.3.tar.gz. tar -zxvf cmake-3.13.3.tar.gz. cd cmake-3.13.3. # build scirpt. ./bootstrap. make -j20. make -j20 install. export PATH=/usr/local/bin:$PATH. ```. ## Protobuf 3.6.1 C++ static. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from /usr/bin/gcc, static for bazel. ```bash. # download source code. wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --disable-shared --enable-static. make clean. make -j20. make -j20 check # check if protobuf build is good. # install. make install. sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig. #cd .. # verify. echo ""$(which protoc)"". echo ""$(protoc --version)"". ```. ## Bazel 0.15.0. Build instructions: [https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel](https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel). Edit crosstool: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). > Note: Bazel 0.15.0 should be built from /usr/bin/gcc. ```bash. # download source code. wget https://github.com/bazelbuild/bazel/releases/download/0.15.0/bazel-0.15.0-dist.zip. mkdir bazel-0.15.0. unzip -n bazel-0.15.0-dist.zip -d ./bazel-0.15.0. cd bazel-0.15.0. # environment. exp",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:2012,energy efficiency,CPU,CPU,2012,"ub.com/Kitware/CMake/releases/download/v3.13.3/cmake-3.13.3.tar.gz. tar -zxvf cmake-3.13.3.tar.gz. cd cmake-3.13.3. # build scirpt. ./bootstrap. make -j20. make -j20 install. export PATH=/usr/local/bin:$PATH. ```. ## Protobuf 3.6.1 C++ static. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from /usr/bin/gcc, static for bazel. ```bash. # download source code. wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --disable-shared --enable-static. make clean. make -j20. make -j20 check # check if protobuf build is good. # install. make install. sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig. #cd .. # verify. echo ""$(which protoc)"". echo ""$(protoc --version)"". ```. ## Bazel 0.15.0. Build instructions: [https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel](https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel). Edit crosstool: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). > Note: Bazel 0.15.0 should be built from /usr/bin/gcc. ```bash. # download source code. wget https://github.com/bazelbuild/bazel/releases/download/0.15.0/bazel-0.15.0-dist.zip. mkdir bazel-0.15.0. unzip -n bazel-0.15.0-dist.zip -d ./bazel-0.15.0. cd bazel-0.15.0. # environment. export CPU=powe",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:2038,energy efficiency,CPU,CPU,2038,"ses/download/v3.13.3/cmake-3.13.3.tar.gz. tar -zxvf cmake-3.13.3.tar.gz. cd cmake-3.13.3. # build scirpt. ./bootstrap. make -j20. make -j20 install. export PATH=/usr/local/bin:$PATH. ```. ## Protobuf 3.6.1 C++ static. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from /usr/bin/gcc, static for bazel. ```bash. # download source code. wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --disable-shared --enable-static. make clean. make -j20. make -j20 check # check if protobuf build is good. # install. make install. sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig. #cd .. # verify. echo ""$(which protoc)"". echo ""$(protoc --version)"". ```. ## Bazel 0.15.0. Build instructions: [https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel](https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel). Edit crosstool: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). > Note: Bazel 0.15.0 should be built from /usr/bin/gcc. ```bash. # download source code. wget https://github.com/bazelbuild/bazel/releases/download/0.15.0/bazel-0.15.0-dist.zip. mkdir bazel-0.15.0. unzip -n bazel-0.15.0-dist.zip -d ./bazel-0.15.0. cd bazel-0.15.0. # environment. export CPU=power8. export HOMEPATH=/home/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:2050,energy efficiency,CPU,CPU,2050,"/v3.13.3/cmake-3.13.3.tar.gz. tar -zxvf cmake-3.13.3.tar.gz. cd cmake-3.13.3. # build scirpt. ./bootstrap. make -j20. make -j20 install. export PATH=/usr/local/bin:$PATH. ```. ## Protobuf 3.6.1 C++ static. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from /usr/bin/gcc, static for bazel. ```bash. # download source code. wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --disable-shared --enable-static. make clean. make -j20. make -j20 check # check if protobuf build is good. # install. make install. sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig. #cd .. # verify. echo ""$(which protoc)"". echo ""$(protoc --version)"". ```. ## Bazel 0.15.0. Build instructions: [https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel](https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel). Edit crosstool: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). > Note: Bazel 0.15.0 should be built from /usr/bin/gcc. ```bash. # download source code. wget https://github.com/bazelbuild/bazel/releases/download/0.15.0/bazel-0.15.0-dist.zip. mkdir bazel-0.15.0. unzip -n bazel-0.15.0-dist.zip -d ./bazel-0.15.0. cd bazel-0.15.0. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. expo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:3006,energy efficiency,CPU,CPU,3006,"ne=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --disable-shared --enable-static. make clean. make -j20. make -j20 check # check if protobuf build is good. # install. make install. sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig. #cd .. # verify. echo ""$(which protoc)"". echo ""$(protoc --version)"". ```. ## Bazel 0.15.0. Build instructions: [https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel](https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel). Edit crosstool: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). > Note: Bazel 0.15.0 should be built from /usr/bin/gcc. ```bash. # download source code. wget https://github.com/bazelbuild/bazel/releases/download/0.15.0/bazel-0.15.0-dist.zip. mkdir bazel-0.15.0. unzip -n bazel-0.15.0-dist.zip -d ./bazel-0.15.0. cd bazel-0.15.0. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export JAVA_HOME=/usr/lib/jvm/java-1.8.0. export PATH=$HOMEPATH/inst/bin:$JAVA_HOME/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # build from scratch. PROTOC=$HOMEPATH/inst/bin/protoc ./compile.sh. rsync -avP output/bazel $HOMEPATH/inst/bin/. # verification. bazel info. ```. ## Advance Toolchain 11.0. Doc: [https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/](https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/). ```bash. # gpg public key. wget ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. rpm --import gpg-pubkey-6976a827-5164221b. # Configure the Advance Toolchain repositories. [root@cit1074 deepvariant]# cat /etc/yum.repos.d/advance-toolchain.repo. #Begin of configuration file. [advance-toolchain]. name=Advance Toolchain Unicamp FTP. baseurl=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:4402,energy efficiency,Load,Load,4402,"tps://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/](https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/). ```bash. # gpg public key. wget ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. rpm --import gpg-pubkey-6976a827-5164221b. # Configure the Advance Toolchain repositories. [root@cit1074 deepvariant]# cat /etc/yum.repos.d/advance-toolchain.repo. #Begin of configuration file. [advance-toolchain]. name=Advance Toolchain Unicamp FTP. baseurl=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7. failovermethod=priority. enabled=1. gpgcheck=1. gpgkey=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. # End of configuration file. # Install the Advance Toolchain. yum install advance-toolchain-at11.0-runtime. yum install advance-toolchain-at11.0-devel. yum install advance-toolchain-at11.0-perf. yum install advance-toolchain-at11.0-mcore-libs. # Load enviroment. export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # Do not need to export. # export LD_LIBRARY_PATH=/opt/at11.0/lib64:/lib64:$LD_LIBRARY_PATH . # Load environment. sudo yum install environment-modules. source /etc/profile. module load at11.0. module unload at11.0. ```. ## Python 2 and Pip 19.0.1. Build instructions: [https://docs.python.org/2/using/unix.html](https://docs.python.org/2/using/unix.html). > Note: Python 2 should be built from AT 11.0. ```bash. # download source code . wget https://www.python.org/ftp/python/2.7.15/Python-2.7.15.tgz. tar -zxvf Python-2.7.15.tgz. cd Python-2.7.15. # environment. export HOMEPATH=/home/qilibj. export CPU=power8. # check gcc before build, should be AT11.0. which gcc. # build. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CPPFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static. make -j20. make install. # set environment. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:4567,energy efficiency,Load,Load,4567,"h. # gpg public key. wget ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. rpm --import gpg-pubkey-6976a827-5164221b. # Configure the Advance Toolchain repositories. [root@cit1074 deepvariant]# cat /etc/yum.repos.d/advance-toolchain.repo. #Begin of configuration file. [advance-toolchain]. name=Advance Toolchain Unicamp FTP. baseurl=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7. failovermethod=priority. enabled=1. gpgcheck=1. gpgkey=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. # End of configuration file. # Install the Advance Toolchain. yum install advance-toolchain-at11.0-runtime. yum install advance-toolchain-at11.0-devel. yum install advance-toolchain-at11.0-perf. yum install advance-toolchain-at11.0-mcore-libs. # Load enviroment. export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # Do not need to export. # export LD_LIBRARY_PATH=/opt/at11.0/lib64:/lib64:$LD_LIBRARY_PATH . # Load environment. sudo yum install environment-modules. source /etc/profile. module load at11.0. module unload at11.0. ```. ## Python 2 and Pip 19.0.1. Build instructions: [https://docs.python.org/2/using/unix.html](https://docs.python.org/2/using/unix.html). > Note: Python 2 should be built from AT 11.0. ```bash. # download source code . wget https://www.python.org/ftp/python/2.7.15/Python-2.7.15.tgz. tar -zxvf Python-2.7.15.tgz. cd Python-2.7.15. # environment. export HOMEPATH=/home/qilibj. export CPU=power8. # check gcc before build, should be AT11.0. which gcc. # build. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CPPFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static. make -j20. make install. # set environment. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH. export PYTHONPATH=/home/qilibj/inst/lib/python2.7/sit",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:4635,energy efficiency,profil,profile,4635,"n/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. rpm --import gpg-pubkey-6976a827-5164221b. # Configure the Advance Toolchain repositories. [root@cit1074 deepvariant]# cat /etc/yum.repos.d/advance-toolchain.repo. #Begin of configuration file. [advance-toolchain]. name=Advance Toolchain Unicamp FTP. baseurl=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7. failovermethod=priority. enabled=1. gpgcheck=1. gpgkey=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. # End of configuration file. # Install the Advance Toolchain. yum install advance-toolchain-at11.0-runtime. yum install advance-toolchain-at11.0-devel. yum install advance-toolchain-at11.0-perf. yum install advance-toolchain-at11.0-mcore-libs. # Load enviroment. export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # Do not need to export. # export LD_LIBRARY_PATH=/opt/at11.0/lib64:/lib64:$LD_LIBRARY_PATH . # Load environment. sudo yum install environment-modules. source /etc/profile. module load at11.0. module unload at11.0. ```. ## Python 2 and Pip 19.0.1. Build instructions: [https://docs.python.org/2/using/unix.html](https://docs.python.org/2/using/unix.html). > Note: Python 2 should be built from AT 11.0. ```bash. # download source code . wget https://www.python.org/ftp/python/2.7.15/Python-2.7.15.tgz. tar -zxvf Python-2.7.15.tgz. cd Python-2.7.15. # environment. export HOMEPATH=/home/qilibj. export CPU=power8. # check gcc before build, should be AT11.0. which gcc. # build. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CPPFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static. make -j20. make install. # set environment. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH. export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. # verify 2.7.15. echo ""$(python --version)"". # Pip 19.0.2.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:4651,energy efficiency,load,load,4651,"EL7/gpg-pubkey-6976a827-5164221b. rpm --import gpg-pubkey-6976a827-5164221b. # Configure the Advance Toolchain repositories. [root@cit1074 deepvariant]# cat /etc/yum.repos.d/advance-toolchain.repo. #Begin of configuration file. [advance-toolchain]. name=Advance Toolchain Unicamp FTP. baseurl=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7. failovermethod=priority. enabled=1. gpgcheck=1. gpgkey=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. # End of configuration file. # Install the Advance Toolchain. yum install advance-toolchain-at11.0-runtime. yum install advance-toolchain-at11.0-devel. yum install advance-toolchain-at11.0-perf. yum install advance-toolchain-at11.0-mcore-libs. # Load enviroment. export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # Do not need to export. # export LD_LIBRARY_PATH=/opt/at11.0/lib64:/lib64:$LD_LIBRARY_PATH . # Load environment. sudo yum install environment-modules. source /etc/profile. module load at11.0. module unload at11.0. ```. ## Python 2 and Pip 19.0.1. Build instructions: [https://docs.python.org/2/using/unix.html](https://docs.python.org/2/using/unix.html). > Note: Python 2 should be built from AT 11.0. ```bash. # download source code . wget https://www.python.org/ftp/python/2.7.15/Python-2.7.15.tgz. tar -zxvf Python-2.7.15.tgz. cd Python-2.7.15. # environment. export HOMEPATH=/home/qilibj. export CPU=power8. # check gcc before build, should be AT11.0. which gcc. # build. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CPPFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static. make -j20. make install. # set environment. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH. export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. # verify 2.7.15. echo ""$(python --version)"". # Pip 19.0.2. wget -qc http",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:5072,energy efficiency,CPU,CPU,5072,"unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. # End of configuration file. # Install the Advance Toolchain. yum install advance-toolchain-at11.0-runtime. yum install advance-toolchain-at11.0-devel. yum install advance-toolchain-at11.0-perf. yum install advance-toolchain-at11.0-mcore-libs. # Load enviroment. export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # Do not need to export. # export LD_LIBRARY_PATH=/opt/at11.0/lib64:/lib64:$LD_LIBRARY_PATH . # Load environment. sudo yum install environment-modules. source /etc/profile. module load at11.0. module unload at11.0. ```. ## Python 2 and Pip 19.0.1. Build instructions: [https://docs.python.org/2/using/unix.html](https://docs.python.org/2/using/unix.html). > Note: Python 2 should be built from AT 11.0. ```bash. # download source code . wget https://www.python.org/ftp/python/2.7.15/Python-2.7.15.tgz. tar -zxvf Python-2.7.15.tgz. cd Python-2.7.15. # environment. export HOMEPATH=/home/qilibj. export CPU=power8. # check gcc before build, should be AT11.0. which gcc. # build. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CPPFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static. make -j20. make install. # set environment. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH. export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. # verify 2.7.15. echo ""$(python --version)"". # Pip 19.0.2. wget -qc https://bootstrap.pypa.io/get-pip.py --no-check-certificate. $HOMEPATH/inst/bin/python ./get-pip.py --prefix $HOMEPATH/inst. #pip install --upgrade --force-reinstall pip. echo ""$(pip --version)"". pip install setuptools nose asv cython future protobuf==3.6.1 six mock. pip install --upgrade setuptools. ```. ## Protobuf 3.6.1 C++ shared. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://git",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:5163,energy efficiency,CPU,CPU,5163,"configuration file. # Install the Advance Toolchain. yum install advance-toolchain-at11.0-runtime. yum install advance-toolchain-at11.0-devel. yum install advance-toolchain-at11.0-perf. yum install advance-toolchain-at11.0-mcore-libs. # Load enviroment. export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # Do not need to export. # export LD_LIBRARY_PATH=/opt/at11.0/lib64:/lib64:$LD_LIBRARY_PATH . # Load environment. sudo yum install environment-modules. source /etc/profile. module load at11.0. module unload at11.0. ```. ## Python 2 and Pip 19.0.1. Build instructions: [https://docs.python.org/2/using/unix.html](https://docs.python.org/2/using/unix.html). > Note: Python 2 should be built from AT 11.0. ```bash. # download source code . wget https://www.python.org/ftp/python/2.7.15/Python-2.7.15.tgz. tar -zxvf Python-2.7.15.tgz. cd Python-2.7.15. # environment. export HOMEPATH=/home/qilibj. export CPU=power8. # check gcc before build, should be AT11.0. which gcc. # build. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CPPFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static. make -j20. make install. # set environment. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH. export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. # verify 2.7.15. echo ""$(python --version)"". # Pip 19.0.2. wget -qc https://bootstrap.pypa.io/get-pip.py --no-check-certificate. $HOMEPATH/inst/bin/python ./get-pip.py --prefix $HOMEPATH/inst. #pip install --upgrade --force-reinstall pip. echo ""$(pip --version)"". pip install setuptools nose asv cython future protobuf==3.6.1 six mock. pip install --upgrade setuptools. ```. ## Protobuf 3.6.1 C++ shared. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:5175,energy efficiency,CPU,CPU,5175,"n file. # Install the Advance Toolchain. yum install advance-toolchain-at11.0-runtime. yum install advance-toolchain-at11.0-devel. yum install advance-toolchain-at11.0-perf. yum install advance-toolchain-at11.0-mcore-libs. # Load enviroment. export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # Do not need to export. # export LD_LIBRARY_PATH=/opt/at11.0/lib64:/lib64:$LD_LIBRARY_PATH . # Load environment. sudo yum install environment-modules. source /etc/profile. module load at11.0. module unload at11.0. ```. ## Python 2 and Pip 19.0.1. Build instructions: [https://docs.python.org/2/using/unix.html](https://docs.python.org/2/using/unix.html). > Note: Python 2 should be built from AT 11.0. ```bash. # download source code . wget https://www.python.org/ftp/python/2.7.15/Python-2.7.15.tgz. tar -zxvf Python-2.7.15.tgz. cd Python-2.7.15. # environment. export HOMEPATH=/home/qilibj. export CPU=power8. # check gcc before build, should be AT11.0. which gcc. # build. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CPPFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static. make -j20. make install. # set environment. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH. export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. # verify 2.7.15. echo ""$(python --version)"". # Pip 19.0.2. wget -qc https://bootstrap.pypa.io/get-pip.py --no-check-certificate. $HOMEPATH/inst/bin/python ./get-pip.py --prefix $HOMEPATH/inst. #pip install --upgrade --force-reinstall pip. echo ""$(pip --version)"". pip install setuptools nose asv cython future protobuf==3.6.1 six mock. pip install --upgrade setuptools. ```. ## Protobuf 3.6.1 C++ shared. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built fro",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:5201,energy efficiency,CPU,CPU,5201,"nce Toolchain. yum install advance-toolchain-at11.0-runtime. yum install advance-toolchain-at11.0-devel. yum install advance-toolchain-at11.0-perf. yum install advance-toolchain-at11.0-mcore-libs. # Load enviroment. export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # Do not need to export. # export LD_LIBRARY_PATH=/opt/at11.0/lib64:/lib64:$LD_LIBRARY_PATH . # Load environment. sudo yum install environment-modules. source /etc/profile. module load at11.0. module unload at11.0. ```. ## Python 2 and Pip 19.0.1. Build instructions: [https://docs.python.org/2/using/unix.html](https://docs.python.org/2/using/unix.html). > Note: Python 2 should be built from AT 11.0. ```bash. # download source code . wget https://www.python.org/ftp/python/2.7.15/Python-2.7.15.tgz. tar -zxvf Python-2.7.15.tgz. cd Python-2.7.15. # environment. export HOMEPATH=/home/qilibj. export CPU=power8. # check gcc before build, should be AT11.0. which gcc. # build. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CPPFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static. make -j20. make install. # set environment. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH. export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. # verify 2.7.15. echo ""$(python --version)"". # Pip 19.0.2. wget -qc https://bootstrap.pypa.io/get-pip.py --no-check-certificate. $HOMEPATH/inst/bin/python ./get-pip.py --prefix $HOMEPATH/inst. #pip install --upgrade --force-reinstall pip. echo ""$(pip --version)"". pip install setuptools nose asv cython future protobuf==3.6.1 six mock. pip install --upgrade setuptools. ```. ## Protobuf 3.6.1 C++ shared. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from AT 11.0, C++ shared for ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:5213,energy efficiency,CPU,CPU,5213,"n. yum install advance-toolchain-at11.0-runtime. yum install advance-toolchain-at11.0-devel. yum install advance-toolchain-at11.0-perf. yum install advance-toolchain-at11.0-mcore-libs. # Load enviroment. export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # Do not need to export. # export LD_LIBRARY_PATH=/opt/at11.0/lib64:/lib64:$LD_LIBRARY_PATH . # Load environment. sudo yum install environment-modules. source /etc/profile. module load at11.0. module unload at11.0. ```. ## Python 2 and Pip 19.0.1. Build instructions: [https://docs.python.org/2/using/unix.html](https://docs.python.org/2/using/unix.html). > Note: Python 2 should be built from AT 11.0. ```bash. # download source code . wget https://www.python.org/ftp/python/2.7.15/Python-2.7.15.tgz. tar -zxvf Python-2.7.15.tgz. cd Python-2.7.15. # environment. export HOMEPATH=/home/qilibj. export CPU=power8. # check gcc before build, should be AT11.0. which gcc. # build. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CPPFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static. make -j20. make install. # set environment. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH. export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. # verify 2.7.15. echo ""$(python --version)"". # Pip 19.0.2. wget -qc https://bootstrap.pypa.io/get-pip.py --no-check-certificate. $HOMEPATH/inst/bin/python ./get-pip.py --prefix $HOMEPATH/inst. #pip install --upgrade --force-reinstall pip. echo ""$(pip --version)"". pip install setuptools nose asv cython future protobuf==3.6.1 six mock. pip install --upgrade setuptools. ```. ## Protobuf 3.6.1 C++ shared. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from AT 11.0, C++ shared for TF 1.12. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:6548,energy efficiency,CPU,CPU,6548,"t/lib/python2.7/site-packages. # verify 2.7.15. echo ""$(python --version)"". # Pip 19.0.2. wget -qc https://bootstrap.pypa.io/get-pip.py --no-check-certificate. $HOMEPATH/inst/bin/python ./get-pip.py --prefix $HOMEPATH/inst. #pip install --upgrade --force-reinstall pip. echo ""$(pip --version)"". pip install setuptools nose asv cython future protobuf==3.6.1 six mock. pip install --upgrade setuptools. ```. ## Protobuf 3.6.1 C++ shared. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from AT 11.0, C++ shared for TF 1.12. ```bash. # download source code. wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static. make clean. make -j20. # optional. make -j20 check # check if protobuf build is good. # install. make install. sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig. #cd .. # verify. echo ""$(which protoc)"". echo ""$(protoc --version)"". ```. ## Protobuf 3.6.1 Python from source code. Python: [https://github.com/protocolbuffers/protobuf/blob/master/python/README.md](https://github.com/protocolbuffers/protobuf/blob/master/python/README.md). > Note: Protobuf 3.6.1 should be built from source code or CLIF cannot find protobuf. ```bash. # share build for Python. python --version # python 2.7 or newer. protoc --version. # build. cd protobuf-3.6.1/python/. python setup.py build. python setup.py test. # install from source as deepvariant needed. python setup.py install. # install from wheel. python setup.py bdist_wheel. pip install protobuf-3.6.1-py2.p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:6560,energy efficiency,CPU,CPU,6560,"2.7/site-packages. # verify 2.7.15. echo ""$(python --version)"". # Pip 19.0.2. wget -qc https://bootstrap.pypa.io/get-pip.py --no-check-certificate. $HOMEPATH/inst/bin/python ./get-pip.py --prefix $HOMEPATH/inst. #pip install --upgrade --force-reinstall pip. echo ""$(pip --version)"". pip install setuptools nose asv cython future protobuf==3.6.1 six mock. pip install --upgrade setuptools. ```. ## Protobuf 3.6.1 C++ shared. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from AT 11.0, C++ shared for TF 1.12. ```bash. # download source code. wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static. make clean. make -j20. # optional. make -j20 check # check if protobuf build is good. # install. make install. sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig. #cd .. # verify. echo ""$(which protoc)"". echo ""$(protoc --version)"". ```. ## Protobuf 3.6.1 Python from source code. Python: [https://github.com/protocolbuffers/protobuf/blob/master/python/README.md](https://github.com/protocolbuffers/protobuf/blob/master/python/README.md). > Note: Protobuf 3.6.1 should be built from source code or CLIF cannot find protobuf. ```bash. # share build for Python. python --version # python 2.7 or newer. protoc --version. # build. cd protobuf-3.6.1/python/. python setup.py build. python setup.py test. # install from source as deepvariant needed. python setup.py install. # install from wheel. python setup.py bdist_wheel. pip install protobuf-3.6.1-py2.py3-none-any.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:6586,energy efficiency,CPU,CPU,6586,"y 2.7.15. echo ""$(python --version)"". # Pip 19.0.2. wget -qc https://bootstrap.pypa.io/get-pip.py --no-check-certificate. $HOMEPATH/inst/bin/python ./get-pip.py --prefix $HOMEPATH/inst. #pip install --upgrade --force-reinstall pip. echo ""$(pip --version)"". pip install setuptools nose asv cython future protobuf==3.6.1 six mock. pip install --upgrade setuptools. ```. ## Protobuf 3.6.1 C++ shared. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from AT 11.0, C++ shared for TF 1.12. ```bash. # download source code. wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static. make clean. make -j20. # optional. make -j20 check # check if protobuf build is good. # install. make install. sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig. #cd .. # verify. echo ""$(which protoc)"". echo ""$(protoc --version)"". ```. ## Protobuf 3.6.1 Python from source code. Python: [https://github.com/protocolbuffers/protobuf/blob/master/python/README.md](https://github.com/protocolbuffers/protobuf/blob/master/python/README.md). > Note: Protobuf 3.6.1 should be built from source code or CLIF cannot find protobuf. ```bash. # share build for Python. python --version # python 2.7 or newer. protoc --version. # build. cd protobuf-3.6.1/python/. python setup.py build. python setup.py test. # install from source as deepvariant needed. python setup.py install. # install from wheel. python setup.py bdist_wheel. pip install protobuf-3.6.1-py2.py3-none-any.whl --force-reinstall. # v",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:6598,energy efficiency,CPU,CPU,6598,"ho ""$(python --version)"". # Pip 19.0.2. wget -qc https://bootstrap.pypa.io/get-pip.py --no-check-certificate. $HOMEPATH/inst/bin/python ./get-pip.py --prefix $HOMEPATH/inst. #pip install --upgrade --force-reinstall pip. echo ""$(pip --version)"". pip install setuptools nose asv cython future protobuf==3.6.1 six mock. pip install --upgrade setuptools. ```. ## Protobuf 3.6.1 C++ shared. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from AT 11.0, C++ shared for TF 1.12. ```bash. # download source code. wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static. make clean. make -j20. # optional. make -j20 check # check if protobuf build is good. # install. make install. sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig. #cd .. # verify. echo ""$(which protoc)"". echo ""$(protoc --version)"". ```. ## Protobuf 3.6.1 Python from source code. Python: [https://github.com/protocolbuffers/protobuf/blob/master/python/README.md](https://github.com/protocolbuffers/protobuf/blob/master/python/README.md). > Note: Protobuf 3.6.1 should be built from source code or CLIF cannot find protobuf. ```bash. # share build for Python. python --version # python 2.7 or newer. protoc --version. # build. cd protobuf-3.6.1/python/. python setup.py build. python setup.py test. # install from source as deepvariant needed. python setup.py install. # install from wheel. python setup.py bdist_wheel. pip install protobuf-3.6.1-py2.py3-none-any.whl --force-reinstall. # verify. pytho",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:8699,energy efficiency,CPU,CPU,8699,"m/xianyi/OpenBLAS.git OpenBLAS-0.3.5. cd OpenBLAS-0.3.5. make TARGET=power8. make TARGET=power8 PREFIX=$HOMEPATH/inst install. ```. ## Boost 1.66.0. ```bash. wget -qc https://dl.bintray.com/boostorg/release/1.66.0/source/boost_1_66_0.tar.gz. tar xzf boost_1_66_0.tar.gz. cd boost_1_66_0. ./bootstrap.sh --with-toolset=gcc --prefix=$HOMEPATH/inst. ./b2 dll-path=""$HOMEPATH/inst/lib"" install. ```. ## TensorFlow 1.12.0. > Note: Tensorflow 1.12.0 can be built with AT 11.0. Build instructions: [https://www.tensorflow.org/install/source](https://www.tensorflow.org/install/source). Edit instructions: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). Floatn.h error: [https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f](https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f). ```bash. # development packages. yum install python-devel python-pip -y. # dependency of numpy 1.14.6. OPT=""-D_XLOCALE_H=1 -O3 -mcpu=$CPU -mtune=$CPU -maltivec -mvsx -ffast-math -fpeel-loops -funroll-loops -ftree-vectorize -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free"" BLAS=$HOMEPATH/inst/lib/libopenblas.so LAPACK=$HOMEPATH/inst/lib/libopenblas.so ATLAS=$HOMEPATH/inst/lib/libopenblas.so pip install numpy==1.14.6. # verify. python -c ""import numpy"". # dependecy of scipy 1.2.0. OPT=""-O3 -mcpu=$CPU -mtune=$CPU -maltivec -mvsx -ffast-math -fpeel-loops -funroll-loops -ftree-vectorize -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free"" BLAS=$HOMEPATH/inst/lib/libopenblas.so LAPACK=$HOMEPATH/inst/lib/libopenblas.so ATLAS=$HOMEPATH/inst/lib/libopenblas.so pip install scipy==1.2.0. # verify. python -c ""import scipy"". # pip package dependencies. # pip install pip six wheel mock. pip install wheel autograd h5py==2.9.0 enum34. pip install keras_applications==1.0.6 keras_preprocessing==1.0.5. # download source code. git clone -b r1.12 https://github.com/tensorflow/tensorf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:8711,energy efficiency,CPU,CPU,8711,"nBLAS.git OpenBLAS-0.3.5. cd OpenBLAS-0.3.5. make TARGET=power8. make TARGET=power8 PREFIX=$HOMEPATH/inst install. ```. ## Boost 1.66.0. ```bash. wget -qc https://dl.bintray.com/boostorg/release/1.66.0/source/boost_1_66_0.tar.gz. tar xzf boost_1_66_0.tar.gz. cd boost_1_66_0. ./bootstrap.sh --with-toolset=gcc --prefix=$HOMEPATH/inst. ./b2 dll-path=""$HOMEPATH/inst/lib"" install. ```. ## TensorFlow 1.12.0. > Note: Tensorflow 1.12.0 can be built with AT 11.0. Build instructions: [https://www.tensorflow.org/install/source](https://www.tensorflow.org/install/source). Edit instructions: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). Floatn.h error: [https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f](https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f). ```bash. # development packages. yum install python-devel python-pip -y. # dependency of numpy 1.14.6. OPT=""-D_XLOCALE_H=1 -O3 -mcpu=$CPU -mtune=$CPU -maltivec -mvsx -ffast-math -fpeel-loops -funroll-loops -ftree-vectorize -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free"" BLAS=$HOMEPATH/inst/lib/libopenblas.so LAPACK=$HOMEPATH/inst/lib/libopenblas.so ATLAS=$HOMEPATH/inst/lib/libopenblas.so pip install numpy==1.14.6. # verify. python -c ""import numpy"". # dependecy of scipy 1.2.0. OPT=""-O3 -mcpu=$CPU -mtune=$CPU -maltivec -mvsx -ffast-math -fpeel-loops -funroll-loops -ftree-vectorize -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free"" BLAS=$HOMEPATH/inst/lib/libopenblas.so LAPACK=$HOMEPATH/inst/lib/libopenblas.so ATLAS=$HOMEPATH/inst/lib/libopenblas.so pip install scipy==1.2.0. # verify. python -c ""import scipy"". # pip package dependencies. # pip install pip six wheel mock. pip install wheel autograd h5py==2.9.0 enum34. pip install keras_applications==1.0.6 keras_preprocessing==1.0.5. # download source code. git clone -b r1.12 https://github.com/tensorflow/tensorflow.git tens",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:9095,energy efficiency,CPU,CPU,9095,"## TensorFlow 1.12.0. > Note: Tensorflow 1.12.0 can be built with AT 11.0. Build instructions: [https://www.tensorflow.org/install/source](https://www.tensorflow.org/install/source). Edit instructions: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). Floatn.h error: [https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f](https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f). ```bash. # development packages. yum install python-devel python-pip -y. # dependency of numpy 1.14.6. OPT=""-D_XLOCALE_H=1 -O3 -mcpu=$CPU -mtune=$CPU -maltivec -mvsx -ffast-math -fpeel-loops -funroll-loops -ftree-vectorize -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free"" BLAS=$HOMEPATH/inst/lib/libopenblas.so LAPACK=$HOMEPATH/inst/lib/libopenblas.so ATLAS=$HOMEPATH/inst/lib/libopenblas.so pip install numpy==1.14.6. # verify. python -c ""import numpy"". # dependecy of scipy 1.2.0. OPT=""-O3 -mcpu=$CPU -mtune=$CPU -maltivec -mvsx -ffast-math -fpeel-loops -funroll-loops -ftree-vectorize -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free"" BLAS=$HOMEPATH/inst/lib/libopenblas.so LAPACK=$HOMEPATH/inst/lib/libopenblas.so ATLAS=$HOMEPATH/inst/lib/libopenblas.so pip install scipy==1.2.0. # verify. python -c ""import scipy"". # pip package dependencies. # pip install pip six wheel mock. pip install wheel autograd h5py==2.9.0 enum34. pip install keras_applications==1.0.6 keras_preprocessing==1.0.5. # download source code. git clone -b r1.12 https://github.com/tensorflow/tensorflow.git tensorflow-1.12. cd tensorflow-1.12. # configure in tensorflow/.tf_configure.bazelrc. PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python"" \. PYTHON_LIB_PATH=""$HOMEPATH/inst/lib/python2.7/site-packages"" \. TF_NEED_IGNITE=""0"" \. TF_ENABLE_XLA=""0"" \. TF_NEED_OPENCL_SYCL=""0"" \. TF_NEED_ROCM=""0"" \. TF_NEED_MPI=""0"" \. TF_NEED_TENSORRT=""0"" \. TF_NEED_CUDA=""1"" \. TF_CUDA_VERSION=""10.0"" \. CUDA_TOOLKIT",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:9107,energy efficiency,CPU,CPU,9107,"w 1.12.0. > Note: Tensorflow 1.12.0 can be built with AT 11.0. Build instructions: [https://www.tensorflow.org/install/source](https://www.tensorflow.org/install/source). Edit instructions: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). Floatn.h error: [https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f](https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f). ```bash. # development packages. yum install python-devel python-pip -y. # dependency of numpy 1.14.6. OPT=""-D_XLOCALE_H=1 -O3 -mcpu=$CPU -mtune=$CPU -maltivec -mvsx -ffast-math -fpeel-loops -funroll-loops -ftree-vectorize -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free"" BLAS=$HOMEPATH/inst/lib/libopenblas.so LAPACK=$HOMEPATH/inst/lib/libopenblas.so ATLAS=$HOMEPATH/inst/lib/libopenblas.so pip install numpy==1.14.6. # verify. python -c ""import numpy"". # dependecy of scipy 1.2.0. OPT=""-O3 -mcpu=$CPU -mtune=$CPU -maltivec -mvsx -ffast-math -fpeel-loops -funroll-loops -ftree-vectorize -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free"" BLAS=$HOMEPATH/inst/lib/libopenblas.so LAPACK=$HOMEPATH/inst/lib/libopenblas.so ATLAS=$HOMEPATH/inst/lib/libopenblas.so pip install scipy==1.2.0. # verify. python -c ""import scipy"". # pip package dependencies. # pip install pip six wheel mock. pip install wheel autograd h5py==2.9.0 enum34. pip install keras_applications==1.0.6 keras_preprocessing==1.0.5. # download source code. git clone -b r1.12 https://github.com/tensorflow/tensorflow.git tensorflow-1.12. cd tensorflow-1.12. # configure in tensorflow/.tf_configure.bazelrc. PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python"" \. PYTHON_LIB_PATH=""$HOMEPATH/inst/lib/python2.7/site-packages"" \. TF_NEED_IGNITE=""0"" \. TF_ENABLE_XLA=""0"" \. TF_NEED_OPENCL_SYCL=""0"" \. TF_NEED_ROCM=""0"" \. TF_NEED_MPI=""0"" \. TF_NEED_TENSORRT=""0"" \. TF_NEED_CUDA=""1"" \. TF_CUDA_VERSION=""10.0"" \. CUDA_TOOLKIT_PATH=""/usr/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:10491,energy efficiency,CPU,CPU,10491,"ix wheel mock. pip install wheel autograd h5py==2.9.0 enum34. pip install keras_applications==1.0.6 keras_preprocessing==1.0.5. # download source code. git clone -b r1.12 https://github.com/tensorflow/tensorflow.git tensorflow-1.12. cd tensorflow-1.12. # configure in tensorflow/.tf_configure.bazelrc. PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python"" \. PYTHON_LIB_PATH=""$HOMEPATH/inst/lib/python2.7/site-packages"" \. TF_NEED_IGNITE=""0"" \. TF_ENABLE_XLA=""0"" \. TF_NEED_OPENCL_SYCL=""0"" \. TF_NEED_ROCM=""0"" \. TF_NEED_MPI=""0"" \. TF_NEED_TENSORRT=""0"" \. TF_NEED_CUDA=""1"" \. TF_CUDA_VERSION=""10.0"" \. CUDA_TOOLKIT_PATH=""/usr/local/cuda"" \. TF_CUDNN_VERSION=""7"" \. CUDNN_INSTALL_PATH=""/usr/local/cuda-10.0"" \. TF_NCCL_VERSION=""2"" \. NCCL_INSTALL_PATH=""/usr/local/cuda-10.0/targets/ppc64le-linux/lib"" \. NCCL_HDR_PATH=""/usr/local/cuda-10.0/targets/ppc64le-linux/lib/../include"" \. TF_CUDA_COMPUTE_CAPABILITIES=""3.7"" \. TF_CUDA_CLANG=""0"" \. GCC_HOST_COMPILER_PATH=""/opt/at11.0/bin/gcc"" \. CC_OPT_FLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" \. TF_SET_ANDROID_WORKSPACE=0 \. ./configure. # fix build error. vim /opt/at11.0/include/bits/floatn.h. -------------------------------------. #include <features.h>. /* Defined to 1 if the current compiler invocation provides a. floating-point type with the IEEE 754 binary128 format, and this glibc. includes corresponding *f128 interfaces for it. */. #if defined _ARCH_PWR8 && defined __LITTLE_ENDIAN__ && (_CALL_ELF == 2) \. && defined __FLOAT128__. # define __HAVE_FLOAT128 1. #else. # define __HAVE_FLOAT128 0. #endif. /* add the following block of fix tensorflow build error */. #if CUDART_VERSION. #undef __HAVE_FLOAT128. #define __HAVE_FLOAT128 0. #endif. /* Defined to 1 if __HAVE_FLOAT128 is 1 and the type is ABI-distinct. from the default float, double and long double types in this glibc. */. #if __HAVE_FLOAT128. -------------------------------------. # build. bazel clean. export PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python"". bazel build --config=opt --config=cuda //",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:10503,energy efficiency,CPU,CPU,10503,"k. pip install wheel autograd h5py==2.9.0 enum34. pip install keras_applications==1.0.6 keras_preprocessing==1.0.5. # download source code. git clone -b r1.12 https://github.com/tensorflow/tensorflow.git tensorflow-1.12. cd tensorflow-1.12. # configure in tensorflow/.tf_configure.bazelrc. PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python"" \. PYTHON_LIB_PATH=""$HOMEPATH/inst/lib/python2.7/site-packages"" \. TF_NEED_IGNITE=""0"" \. TF_ENABLE_XLA=""0"" \. TF_NEED_OPENCL_SYCL=""0"" \. TF_NEED_ROCM=""0"" \. TF_NEED_MPI=""0"" \. TF_NEED_TENSORRT=""0"" \. TF_NEED_CUDA=""1"" \. TF_CUDA_VERSION=""10.0"" \. CUDA_TOOLKIT_PATH=""/usr/local/cuda"" \. TF_CUDNN_VERSION=""7"" \. CUDNN_INSTALL_PATH=""/usr/local/cuda-10.0"" \. TF_NCCL_VERSION=""2"" \. NCCL_INSTALL_PATH=""/usr/local/cuda-10.0/targets/ppc64le-linux/lib"" \. NCCL_HDR_PATH=""/usr/local/cuda-10.0/targets/ppc64le-linux/lib/../include"" \. TF_CUDA_COMPUTE_CAPABILITIES=""3.7"" \. TF_CUDA_CLANG=""0"" \. GCC_HOST_COMPILER_PATH=""/opt/at11.0/bin/gcc"" \. CC_OPT_FLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" \. TF_SET_ANDROID_WORKSPACE=0 \. ./configure. # fix build error. vim /opt/at11.0/include/bits/floatn.h. -------------------------------------. #include <features.h>. /* Defined to 1 if the current compiler invocation provides a. floating-point type with the IEEE 754 binary128 format, and this glibc. includes corresponding *f128 interfaces for it. */. #if defined _ARCH_PWR8 && defined __LITTLE_ENDIAN__ && (_CALL_ELF == 2) \. && defined __FLOAT128__. # define __HAVE_FLOAT128 1. #else. # define __HAVE_FLOAT128 0. #endif. /* add the following block of fix tensorflow build error */. #if CUDART_VERSION. #undef __HAVE_FLOAT128. #define __HAVE_FLOAT128 0. #endif. /* Defined to 1 if __HAVE_FLOAT128 is 1 and the type is ABI-distinct. from the default float, double and long double types in this glibc. */. #if __HAVE_FLOAT128. -------------------------------------. # build. bazel clean. export PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python"". bazel build --config=opt --config=cuda //tensorflow/t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:10701,energy efficiency,current,current,10701,"git tensorflow-1.12. cd tensorflow-1.12. # configure in tensorflow/.tf_configure.bazelrc. PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python"" \. PYTHON_LIB_PATH=""$HOMEPATH/inst/lib/python2.7/site-packages"" \. TF_NEED_IGNITE=""0"" \. TF_ENABLE_XLA=""0"" \. TF_NEED_OPENCL_SYCL=""0"" \. TF_NEED_ROCM=""0"" \. TF_NEED_MPI=""0"" \. TF_NEED_TENSORRT=""0"" \. TF_NEED_CUDA=""1"" \. TF_CUDA_VERSION=""10.0"" \. CUDA_TOOLKIT_PATH=""/usr/local/cuda"" \. TF_CUDNN_VERSION=""7"" \. CUDNN_INSTALL_PATH=""/usr/local/cuda-10.0"" \. TF_NCCL_VERSION=""2"" \. NCCL_INSTALL_PATH=""/usr/local/cuda-10.0/targets/ppc64le-linux/lib"" \. NCCL_HDR_PATH=""/usr/local/cuda-10.0/targets/ppc64le-linux/lib/../include"" \. TF_CUDA_COMPUTE_CAPABILITIES=""3.7"" \. TF_CUDA_CLANG=""0"" \. GCC_HOST_COMPILER_PATH=""/opt/at11.0/bin/gcc"" \. CC_OPT_FLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" \. TF_SET_ANDROID_WORKSPACE=0 \. ./configure. # fix build error. vim /opt/at11.0/include/bits/floatn.h. -------------------------------------. #include <features.h>. /* Defined to 1 if the current compiler invocation provides a. floating-point type with the IEEE 754 binary128 format, and this glibc. includes corresponding *f128 interfaces for it. */. #if defined _ARCH_PWR8 && defined __LITTLE_ENDIAN__ && (_CALL_ELF == 2) \. && defined __FLOAT128__. # define __HAVE_FLOAT128 1. #else. # define __HAVE_FLOAT128 0. #endif. /* add the following block of fix tensorflow build error */. #if CUDART_VERSION. #undef __HAVE_FLOAT128. #define __HAVE_FLOAT128 0. #endif. /* Defined to 1 if __HAVE_FLOAT128 is 1 and the type is ABI-distinct. from the default float, double and long double types in this glibc. */. #if __HAVE_FLOAT128. -------------------------------------. # build. bazel clean. export PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python"". bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" --action_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH --test_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH --distinct_host_configuration=false. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:13718,energy efficiency,Power,PowerPC,13718,"THON_LIB_PATH=""$HOMEPATH/inst/lib/python2.7/site-packages"". export PROTOC_PREFIX_PATH=""$(dirname ""$(dirname ""$(which protoc)"")"")"". export CLIF_VIRTUALENV=""$INSTALL_DIR""/clif. export CLIF_PIP=""$CLIF_VIRTUALENV/bin/pip"". virtualenv -p ""$PYTHON_BIN_PATH"" ""$CLIF_VIRTUALENV"". $CLIF_PIP install --upgrade pip. $CLIF_PIP install --upgrade setuptools. # Checkout LLVM and Clang source trees. mkdir -p $LLVM_DIR. cd $LLVM_DIR. svn co https://llvm.org/svn/llvm-project/llvm/trunk@307315 llvm. cd llvm/tools. svn co https://llvm.org/svn/llvm-project/cfe/trunk@307315 clang. ln -s -f -n $CLIFSRC_DIR/clif clif. # Builds must be done outside of the LLVM tree. mkdir -p $BUILD_DIR. cd $BUILD_DIR. # Note to remove -DLLVM_TARGETS_TO_BUILD=X86. # ""rm CMakeCache.txt"" to remove cmake cache. cmake -DCMAKE_INSTALL_PREFIX=""$CLIF_VIRTUALENV/clang"" \. -DCMAKE_PREFIX_PATH=""$PROTOC_PREFIX_PATH"" \. -DLLVM_INSTALL_TOOLCHAIN_ONLY=true \. -DCMAKE_BUILD_TYPE=Release \. -DLLVM_BUILD_DOCS=false \. -DLLVM_TARGETS_TO_BUILD=PowerPC \. -DPYTHON_INCLUDE_DIR=""$HOMEPATH/inst/include/python2.7"" \. -DPYTHON_LIBRARY=""$HOMEPATH/inst/lib/libpython2.7.so"" \. -DPYTHON_EXECUTABLE=""$HOMEPATH/inst/bin/python"" \. ""$LLVM_DIR/llvm"". make -j20 clif-matcher. # export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH. make -j20 clif_python_utils_proto_util. make -j20 install. ## Get back to the CLIF Python directory and have pip run setup.py. cd ""$CLIFSRC_DIR"". # Grab the python compiled .proto. cp ""$BUILD_DIR/tools/clif/protos/ast_pb2.py"" clif/protos/. # Grab CLIF generated wrapper implementation for proto_util. cp ""$BUILD_DIR/tools/clif/python/utils/proto_util.cc"" clif/python/utils/. cp ""$BUILD_DIR/tools/clif/python/utils/proto_util.h"" clif/python/utils/. cp ""$BUILD_DIR/tools/clif/python/utils/proto_util.init.cc"" clif/python/utils/. # install. export C_INCLUDE_PATH=/home/qilibj/inst/include. export CPLUS_INCLUDE_PATH=/home/qilibj/inst/include. ""$CLIF_PIP"" install . # echo ""SUCCESS - To use pyclif, run $CLIF_VIRTUALENV/bin",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:15435,energy efficiency,load,load,15435,"ython/utils/. cp ""$BUILD_DIR/tools/clif/python/utils/proto_util.init.cc"" clif/python/utils/. # install. export C_INCLUDE_PATH=/home/qilibj/inst/include. export CPLUS_INCLUDE_PATH=/home/qilibj/inst/include. ""$CLIF_PIP"" install . # echo ""SUCCESS - To use pyclif, run $CLIF_VIRTUALENV/bin/pyclif."". python setup.py bdist_wheel. # Note: pyclif should be installed into virtualenv. ""$CLIF_PIP"" install pyclif-0.3-cp27-none-linux_ppc64le.whl. pip install dist/pyclif-0.3-cp27-cp27m-linux_ppc64le.whl. # verify. python -c ""from clif.python.proto import start"". # link for deepvariant. ln -s /home/qilibj/inst/clif /usr/local/. ```. ## Opencv-python 3.4.5.20. Git repository: [https://github.com/skvark/opencv-python](https://github.com/skvark/opencv-python). ```bash. # Checkout repository and submodules. git clone https://github.com/skvark/opencv-python.git. cd opencv-python/. # fetch the tags to your local repository. git fetch --all --tags --prune. # check out tag 3.4.5.20. git checkout tags/20. # load submoduel. git submodule update --init --recursive. # Dependency. pip install pyparsing. yum install qt-devel. # Build. python setup.py bdist_wheel. # Insatll. pip install dist/opencv_python-3.4.5.20-cp27-cp27mu-linux_ppc64le.whl. # Verify in a new session. python -c ""import cv2"". ```. ## DV Prerequisite. ```bash. ####################################################################. # misc setup. ####################################################################. # development packages. yum install python2-pkgconfig zip zlib-devel unzip curl -y. # python packages. yum install python-devel python-pip python-wheel -y. ####################################################################. # python packages. ####################################################################. # python 2 required. echo ""$(python --version)"". echo ""$(pip --version)"". # Install python packages. pip install contextlib2. pip install enum34. pip install intervaltree. pip install 'mock>=2.0.0'. # pip install",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:18585,energy efficiency,GPU,GPU,18585,"ogle/deepvariant). ```bash. # check out source code. git clone https://github.com/google/deepvariant.git. cd deepvariant. # fetch all tags. git fetch --all --tags --prune. # check out tag. git checkout tags/v0.7.2. # Edit ctx.action with use_default_shell_env=True. vim ./third_party/clif.bzl. # Build and test. export LD_LIBRARY_PATH=/home/qilibj/inst/lib:/qilibj/inst/lib/python2.7/site-packages:$LD_LIBRARY_PATH. export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. export PYTHON_BIN_PATH=/home/qilibj/inst/bin/python. export PYTHON_LIB_PATH=/home/qilibj/inst/lib/python2.7/site-packages. export BAZEL_PYTHON=/home/qilibj/inst/bin/python. export DV_COPT_FLAGS=""--copt=-maltivec --copt=-Wno-sign-compare --copt=-Wno-write-strings --copt=-DNO_WARN_X86_INTRINSICS --cxxopt=-std=gnu++11"". # export DV_COPT_FLAGS=""--copt=-maltivec --copt=-Wno-sign-compare --copt=-Wno-write-strings --copt=-DNO_WARN_X86_INTRINSICS --cxxopt=-std=gnu++11 --copt=-fsigned-char --cxxopt=-fsigned-char"". # for GPU enabled. # fix ""ImportError: No module named google.protobuf"" by install protobuf from source. bazel clean. bazel shutdown. bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" deepvariant/... \. --action_env=LD_LIBRARY_PATH --test_env=LD_LIBRARY_PATH \. --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH \. --action_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH --test_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH \. --action_env=PYTHON_LIB_PATH=$PYTHON_LIB_PATH --test_env=PYTHON_LIB_PATH=$PYTHON_LIB_PATH \. --action_env=BAZEL_PYTHON=$BAZEL_PYTHON --test_env=BAZEL_PYTHON=$BAZEL_PYTHON >& output.log 2>&1 &. bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" deepvariant:gpu_tests --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH. # for CPU only. bazel test -c opt ${DV_COPT_FLAGS} ""$@"" deepvariant/... # build binary. bazel build -c opt ${DV_COPT_FLAGS} ""$@"" deepvariant:binaries --build_python_zip --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPA",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:19369,energy efficiency,CPU,CPU,19369,"=-std=gnu++11"". # export DV_COPT_FLAGS=""--copt=-maltivec --copt=-Wno-sign-compare --copt=-Wno-write-strings --copt=-DNO_WARN_X86_INTRINSICS --cxxopt=-std=gnu++11 --copt=-fsigned-char --cxxopt=-fsigned-char"". # for GPU enabled. # fix ""ImportError: No module named google.protobuf"" by install protobuf from source. bazel clean. bazel shutdown. bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" deepvariant/... \. --action_env=LD_LIBRARY_PATH --test_env=LD_LIBRARY_PATH \. --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH \. --action_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH --test_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH \. --action_env=PYTHON_LIB_PATH=$PYTHON_LIB_PATH --test_env=PYTHON_LIB_PATH=$PYTHON_LIB_PATH \. --action_env=BAZEL_PYTHON=$BAZEL_PYTHON --test_env=BAZEL_PYTHON=$BAZEL_PYTHON >& output.log 2>&1 &. bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" deepvariant:gpu_tests --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH. # for CPU only. bazel test -c opt ${DV_COPT_FLAGS} ""$@"" deepvariant/... # build binary. bazel build -c opt ${DV_COPT_FLAGS} ""$@"" deepvariant:binaries --build_python_zip --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH. echo 'Expect a usage message:'. (bazel-bin/deepvariant/call_variants --help || : ) | grep '/call_variants.py:'. bazel build :licenses_zip --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH. ```. ## Fix DV Error. ```bash. ################################################################################. # bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@""//deepvariant:resources_test. # use lscpu to show the actual CPU number. ################################################################################. python -c ""import multiprocessing; print(multiprocessing.cpu_count())"" #160. python -c ""import psutil;print(p/sutil.cpu_count. ())"" #160. vim deepvariant/resources.py. --------------------------------. def _get_cpu_count():. """"""Ge",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:20047,energy efficiency,CPU,CPU,20047,"THON_LIB_PATH --test_env=PYTHON_LIB_PATH=$PYTHON_LIB_PATH \. --action_env=BAZEL_PYTHON=$BAZEL_PYTHON --test_env=BAZEL_PYTHON=$BAZEL_PYTHON >& output.log 2>&1 &. bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" deepvariant:gpu_tests --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH. # for CPU only. bazel test -c opt ${DV_COPT_FLAGS} ""$@"" deepvariant/... # build binary. bazel build -c opt ${DV_COPT_FLAGS} ""$@"" deepvariant:binaries --build_python_zip --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH. echo 'Expect a usage message:'. (bazel-bin/deepvariant/call_variants --help || : ) | grep '/call_variants.py:'. bazel build :licenses_zip --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH. ```. ## Fix DV Error. ```bash. ################################################################################. # bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@""//deepvariant:resources_test. # use lscpu to show the actual CPU number. ################################################################################. python -c ""import multiprocessing; print(multiprocessing.cpu_count())"" #160. python -c ""import psutil;print(p/sutil.cpu_count. ())"" #160. vim deepvariant/resources.py. --------------------------------. def _get_cpu_count():. """"""Gets the number of physical cores in this machine. Returns:. int >= 1 if the call to get the cpu_count succeeded, or 0 if not. """""". # return psutil.cpu_count(logical=False) or 0 ==> comment. return 20. --------------------------------. vim deepvariant/resources_test.py. --------------------------------. def test_metrics_is_ok_when_cpu_count_returns_none(self):. # Some psutil functions, such as cpu_freq(), can return None depending on. # the environment; make sure we don't crash when that occurs. with mock.patch.object(resources.psutil, 'cpu_count', return_value=None):. with resources.ResourceMonitor() as monitor:. #self.assertEqual(monitor.metrics().physical_core_count, 0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:20295,energy efficiency,resourc,resources,20295,"_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH. # for CPU only. bazel test -c opt ${DV_COPT_FLAGS} ""$@"" deepvariant/... # build binary. bazel build -c opt ${DV_COPT_FLAGS} ""$@"" deepvariant:binaries --build_python_zip --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH. echo 'Expect a usage message:'. (bazel-bin/deepvariant/call_variants --help || : ) | grep '/call_variants.py:'. bazel build :licenses_zip --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH. ```. ## Fix DV Error. ```bash. ################################################################################. # bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@""//deepvariant:resources_test. # use lscpu to show the actual CPU number. ################################################################################. python -c ""import multiprocessing; print(multiprocessing.cpu_count())"" #160. python -c ""import psutil;print(p/sutil.cpu_count. ())"" #160. vim deepvariant/resources.py. --------------------------------. def _get_cpu_count():. """"""Gets the number of physical cores in this machine. Returns:. int >= 1 if the call to get the cpu_count succeeded, or 0 if not. """""". # return psutil.cpu_count(logical=False) or 0 ==> comment. return 20. --------------------------------. vim deepvariant/resources_test.py. --------------------------------. def test_metrics_is_ok_when_cpu_count_returns_none(self):. # Some psutil functions, such as cpu_freq(), can return None depending on. # the environment; make sure we don't crash when that occurs. with mock.patch.object(resources.psutil, 'cpu_count', return_value=None):. with resources.ResourceMonitor() as monitor:. #self.assertEqual(monitor.metrics().physical_core_count, 0) ==> comment. self.assertEqual(monitor.metrics().physical_core_count, 20). --------------------------------. ##########################################################################. # //deepvariant/realigner/allele_count_linear:generate_trained_mo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:20397,energy efficiency,core,cores,20397,"V_COPT_FLAGS} ""$@"" deepvariant/... # build binary. bazel build -c opt ${DV_COPT_FLAGS} ""$@"" deepvariant:binaries --build_python_zip --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH. echo 'Expect a usage message:'. (bazel-bin/deepvariant/call_variants --help || : ) | grep '/call_variants.py:'. bazel build :licenses_zip --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH. ```. ## Fix DV Error. ```bash. ################################################################################. # bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@""//deepvariant:resources_test. # use lscpu to show the actual CPU number. ################################################################################. python -c ""import multiprocessing; print(multiprocessing.cpu_count())"" #160. python -c ""import psutil;print(p/sutil.cpu_count. ())"" #160. vim deepvariant/resources.py. --------------------------------. def _get_cpu_count():. """"""Gets the number of physical cores in this machine. Returns:. int >= 1 if the call to get the cpu_count succeeded, or 0 if not. """""". # return psutil.cpu_count(logical=False) or 0 ==> comment. return 20. --------------------------------. vim deepvariant/resources_test.py. --------------------------------. def test_metrics_is_ok_when_cpu_count_returns_none(self):. # Some psutil functions, such as cpu_freq(), can return None depending on. # the environment; make sure we don't crash when that occurs. with mock.patch.object(resources.psutil, 'cpu_count', return_value=None):. with resources.ResourceMonitor() as monitor:. #self.assertEqual(monitor.metrics().physical_core_count, 0) ==> comment. self.assertEqual(monitor.metrics().physical_core_count, 20). --------------------------------. ##########################################################################. # //deepvariant/realigner/allele_count_linear:generate_trained_model_test. # ImportError: /root/.local/lib/python2.7/site-packages/sklearn/__check_build/_check_build",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:20893,energy efficiency,resourc,resources,20893,"#########################. # bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@""//deepvariant:resources_test. # use lscpu to show the actual CPU number. ################################################################################. python -c ""import multiprocessing; print(multiprocessing.cpu_count())"" #160. python -c ""import psutil;print(p/sutil.cpu_count. ())"" #160. vim deepvariant/resources.py. --------------------------------. def _get_cpu_count():. """"""Gets the number of physical cores in this machine. Returns:. int >= 1 if the call to get the cpu_count succeeded, or 0 if not. """""". # return psutil.cpu_count(logical=False) or 0 ==> comment. return 20. --------------------------------. vim deepvariant/resources_test.py. --------------------------------. def test_metrics_is_ok_when_cpu_count_returns_none(self):. # Some psutil functions, such as cpu_freq(), can return None depending on. # the environment; make sure we don't crash when that occurs. with mock.patch.object(resources.psutil, 'cpu_count', return_value=None):. with resources.ResourceMonitor() as monitor:. #self.assertEqual(monitor.metrics().physical_core_count, 0) ==> comment. self.assertEqual(monitor.metrics().physical_core_count, 20). --------------------------------. ##########################################################################. # //deepvariant/realigner/allele_count_linear:generate_trained_model_test. # ImportError: /root/.local/lib/python2.7/site-packages/sklearn/__check_build/_check_build.so: undefined symbol: PyUnicodeUCS4_DecodeUTF8. ##########################################################################. # reinstall numpy, scipy, Cpython, scikit-learn to fix with AT built Python. python -c ""import numpy"" # prequests of TF 1.12.0. python -c ""import scipy"" # prequests of TF 1.12.0. pip install Cython --force-reinstall --no-deos. pip install scikit-learn --force-reinstall --no-deos. # build from source. wget https://github.com/scikit-learn/scikit-learn/archive/0.20.2.tar.gz",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:20950,energy efficiency,resourc,resources,20950,"st_jobs=1 ${DV_COPT_FLAGS} ""$@""//deepvariant:resources_test. # use lscpu to show the actual CPU number. ################################################################################. python -c ""import multiprocessing; print(multiprocessing.cpu_count())"" #160. python -c ""import psutil;print(p/sutil.cpu_count. ())"" #160. vim deepvariant/resources.py. --------------------------------. def _get_cpu_count():. """"""Gets the number of physical cores in this machine. Returns:. int >= 1 if the call to get the cpu_count succeeded, or 0 if not. """""". # return psutil.cpu_count(logical=False) or 0 ==> comment. return 20. --------------------------------. vim deepvariant/resources_test.py. --------------------------------. def test_metrics_is_ok_when_cpu_count_returns_none(self):. # Some psutil functions, such as cpu_freq(), can return None depending on. # the environment; make sure we don't crash when that occurs. with mock.patch.object(resources.psutil, 'cpu_count', return_value=None):. with resources.ResourceMonitor() as monitor:. #self.assertEqual(monitor.metrics().physical_core_count, 0) ==> comment. self.assertEqual(monitor.metrics().physical_core_count, 20). --------------------------------. ##########################################################################. # //deepvariant/realigner/allele_count_linear:generate_trained_model_test. # ImportError: /root/.local/lib/python2.7/site-packages/sklearn/__check_build/_check_build.so: undefined symbol: PyUnicodeUCS4_DecodeUTF8. ##########################################################################. # reinstall numpy, scipy, Cpython, scikit-learn to fix with AT built Python. python -c ""import numpy"" # prequests of TF 1.12.0. python -c ""import scipy"" # prequests of TF 1.12.0. pip install Cython --force-reinstall --no-deos. pip install scikit-learn --force-reinstall --no-deos. # build from source. wget https://github.com/scikit-learn/scikit-learn/archive/0.20.2.tar.gz. tar zxvf 0.20.2.tar.gz. cd scikit-learn-0.20.2. python ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:20960,energy efficiency,Resourc,ResourceMonitor,20960,"V_COPT_FLAGS} ""$@""//deepvariant:resources_test. # use lscpu to show the actual CPU number. ################################################################################. python -c ""import multiprocessing; print(multiprocessing.cpu_count())"" #160. python -c ""import psutil;print(p/sutil.cpu_count. ())"" #160. vim deepvariant/resources.py. --------------------------------. def _get_cpu_count():. """"""Gets the number of physical cores in this machine. Returns:. int >= 1 if the call to get the cpu_count succeeded, or 0 if not. """""". # return psutil.cpu_count(logical=False) or 0 ==> comment. return 20. --------------------------------. vim deepvariant/resources_test.py. --------------------------------. def test_metrics_is_ok_when_cpu_count_returns_none(self):. # Some psutil functions, such as cpu_freq(), can return None depending on. # the environment; make sure we don't crash when that occurs. with mock.patch.object(resources.psutil, 'cpu_count', return_value=None):. with resources.ResourceMonitor() as monitor:. #self.assertEqual(monitor.metrics().physical_core_count, 0) ==> comment. self.assertEqual(monitor.metrics().physical_core_count, 20). --------------------------------. ##########################################################################. # //deepvariant/realigner/allele_count_linear:generate_trained_model_test. # ImportError: /root/.local/lib/python2.7/site-packages/sklearn/__check_build/_check_build.so: undefined symbol: PyUnicodeUCS4_DecodeUTF8. ##########################################################################. # reinstall numpy, scipy, Cpython, scikit-learn to fix with AT built Python. python -c ""import numpy"" # prequests of TF 1.12.0. python -c ""import scipy"" # prequests of TF 1.12.0. pip install Cython --force-reinstall --no-deos. pip install scikit-learn --force-reinstall --no-deos. # build from source. wget https://github.com/scikit-learn/scikit-learn/archive/0.20.2.tar.gz. tar zxvf 0.20.2.tar.gz. cd scikit-learn-0.20.2. python setup.py bdis",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:20981,energy efficiency,monitor,monitor,20981,"""//deepvariant:resources_test. # use lscpu to show the actual CPU number. ################################################################################. python -c ""import multiprocessing; print(multiprocessing.cpu_count())"" #160. python -c ""import psutil;print(p/sutil.cpu_count. ())"" #160. vim deepvariant/resources.py. --------------------------------. def _get_cpu_count():. """"""Gets the number of physical cores in this machine. Returns:. int >= 1 if the call to get the cpu_count succeeded, or 0 if not. """""". # return psutil.cpu_count(logical=False) or 0 ==> comment. return 20. --------------------------------. vim deepvariant/resources_test.py. --------------------------------. def test_metrics_is_ok_when_cpu_count_returns_none(self):. # Some psutil functions, such as cpu_freq(), can return None depending on. # the environment; make sure we don't crash when that occurs. with mock.patch.object(resources.psutil, 'cpu_count', return_value=None):. with resources.ResourceMonitor() as monitor:. #self.assertEqual(monitor.metrics().physical_core_count, 0) ==> comment. self.assertEqual(monitor.metrics().physical_core_count, 20). --------------------------------. ##########################################################################. # //deepvariant/realigner/allele_count_linear:generate_trained_model_test. # ImportError: /root/.local/lib/python2.7/site-packages/sklearn/__check_build/_check_build.so: undefined symbol: PyUnicodeUCS4_DecodeUTF8. ##########################################################################. # reinstall numpy, scipy, Cpython, scikit-learn to fix with AT built Python. python -c ""import numpy"" # prequests of TF 1.12.0. python -c ""import scipy"" # prequests of TF 1.12.0. pip install Cython --force-reinstall --no-deos. pip install scikit-learn --force-reinstall --no-deos. # build from source. wget https://github.com/scikit-learn/scikit-learn/archive/0.20.2.tar.gz. tar zxvf 0.20.2.tar.gz. cd scikit-learn-0.20.2. python setup.py bdist_wheel. # verify",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:21009,energy efficiency,monitor,monitor,21009,"t. # use lscpu to show the actual CPU number. ################################################################################. python -c ""import multiprocessing; print(multiprocessing.cpu_count())"" #160. python -c ""import psutil;print(p/sutil.cpu_count. ())"" #160. vim deepvariant/resources.py. --------------------------------. def _get_cpu_count():. """"""Gets the number of physical cores in this machine. Returns:. int >= 1 if the call to get the cpu_count succeeded, or 0 if not. """""". # return psutil.cpu_count(logical=False) or 0 ==> comment. return 20. --------------------------------. vim deepvariant/resources_test.py. --------------------------------. def test_metrics_is_ok_when_cpu_count_returns_none(self):. # Some psutil functions, such as cpu_freq(), can return None depending on. # the environment; make sure we don't crash when that occurs. with mock.patch.object(resources.psutil, 'cpu_count', return_value=None):. with resources.ResourceMonitor() as monitor:. #self.assertEqual(monitor.metrics().physical_core_count, 0) ==> comment. self.assertEqual(monitor.metrics().physical_core_count, 20). --------------------------------. ##########################################################################. # //deepvariant/realigner/allele_count_linear:generate_trained_model_test. # ImportError: /root/.local/lib/python2.7/site-packages/sklearn/__check_build/_check_build.so: undefined symbol: PyUnicodeUCS4_DecodeUTF8. ##########################################################################. # reinstall numpy, scipy, Cpython, scikit-learn to fix with AT built Python. python -c ""import numpy"" # prequests of TF 1.12.0. python -c ""import scipy"" # prequests of TF 1.12.0. pip install Cython --force-reinstall --no-deos. pip install scikit-learn --force-reinstall --no-deos. # build from source. wget https://github.com/scikit-learn/scikit-learn/archive/0.20.2.tar.gz. tar zxvf 0.20.2.tar.gz. cd scikit-learn-0.20.2. python setup.py bdist_wheel. # verify. python -c ""from sklearn.ex",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:21081,energy efficiency,monitor,monitor,21081,"######################################################. python -c ""import multiprocessing; print(multiprocessing.cpu_count())"" #160. python -c ""import psutil;print(p/sutil.cpu_count. ())"" #160. vim deepvariant/resources.py. --------------------------------. def _get_cpu_count():. """"""Gets the number of physical cores in this machine. Returns:. int >= 1 if the call to get the cpu_count succeeded, or 0 if not. """""". # return psutil.cpu_count(logical=False) or 0 ==> comment. return 20. --------------------------------. vim deepvariant/resources_test.py. --------------------------------. def test_metrics_is_ok_when_cpu_count_returns_none(self):. # Some psutil functions, such as cpu_freq(), can return None depending on. # the environment; make sure we don't crash when that occurs. with mock.patch.object(resources.psutil, 'cpu_count', return_value=None):. with resources.ResourceMonitor() as monitor:. #self.assertEqual(monitor.metrics().physical_core_count, 0) ==> comment. self.assertEqual(monitor.metrics().physical_core_count, 20). --------------------------------. ##########################################################################. # //deepvariant/realigner/allele_count_linear:generate_trained_model_test. # ImportError: /root/.local/lib/python2.7/site-packages/sklearn/__check_build/_check_build.so: undefined symbol: PyUnicodeUCS4_DecodeUTF8. ##########################################################################. # reinstall numpy, scipy, Cpython, scikit-learn to fix with AT built Python. python -c ""import numpy"" # prequests of TF 1.12.0. python -c ""import scipy"" # prequests of TF 1.12.0. pip install Cython --force-reinstall --no-deos. pip install scikit-learn --force-reinstall --no-deos. # build from source. wget https://github.com/scikit-learn/scikit-learn/archive/0.20.2.tar.gz. tar zxvf 0.20.2.tar.gz. cd scikit-learn-0.20.2. python setup.py bdist_wheel. # verify. python -c ""from sklearn.externals import joblib"". ################################################",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:1283,integrability,protocol,protocolbuffers,1283,"H/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # AT 11.0 environment. source /etc/profile. module load at11.0. export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # python2 and pip environment. export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH. export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. export PYTHON_BIN_PATH=/home/qilibj/inst/bin/python. export PYTHON_LIB_PATH=/home/qilibj/inst/lib/python2.7/site-packages. ```. ## Cmake 3.13.3. Build and install doc: [https://cmake.org/install/](https://cmake.org/install/). >Note: only can be built with system gcc, AT11.0 will cause build failure. ```bash. # download source code. wget https://github.com/Kitware/CMake/releases/download/v3.13.3/cmake-3.13.3.tar.gz. tar -zxvf cmake-3.13.3.tar.gz. cd cmake-3.13.3. # build scirpt. ./bootstrap. make -j20. make -j20 install. export PATH=/usr/local/bin:$PATH. ```. ## Protobuf 3.6.1 C++ static. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from /usr/bin/gcc, static for bazel. ```bash. # download source code. wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --disable-shared --enable-static. make clean. make -j20. make -j20 check # check if protobuf build is good. # install. make install. sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:1354,integrability,protocol,protocolbuffers,1354,"/sbin:/usr/bin:/root/bin. # AT 11.0 environment. source /etc/profile. module load at11.0. export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # python2 and pip environment. export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH. export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. export PYTHON_BIN_PATH=/home/qilibj/inst/bin/python. export PYTHON_LIB_PATH=/home/qilibj/inst/lib/python2.7/site-packages. ```. ## Cmake 3.13.3. Build and install doc: [https://cmake.org/install/](https://cmake.org/install/). >Note: only can be built with system gcc, AT11.0 will cause build failure. ```bash. # download source code. wget https://github.com/Kitware/CMake/releases/download/v3.13.3/cmake-3.13.3.tar.gz. tar -zxvf cmake-3.13.3.tar.gz. cd cmake-3.13.3. # build scirpt. ./bootstrap. make -j20. make -j20 install. export PATH=/usr/local/bin:$PATH. ```. ## Protobuf 3.6.1 C++ static. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from /usr/bin/gcc, static for bazel. ```bash. # download source code. wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --disable-shared --enable-static. make clean. make -j20. make -j20 check # check if protobuf build is good. # install. make install. sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig. #cd .. # verify. echo ""$(which protoc)"". echo ""$(protoc --ve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:1540,integrability,protocol,protocolbuffers,1540,"RARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH. export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. export PYTHON_BIN_PATH=/home/qilibj/inst/bin/python. export PYTHON_LIB_PATH=/home/qilibj/inst/lib/python2.7/site-packages. ```. ## Cmake 3.13.3. Build and install doc: [https://cmake.org/install/](https://cmake.org/install/). >Note: only can be built with system gcc, AT11.0 will cause build failure. ```bash. # download source code. wget https://github.com/Kitware/CMake/releases/download/v3.13.3/cmake-3.13.3.tar.gz. tar -zxvf cmake-3.13.3.tar.gz. cd cmake-3.13.3. # build scirpt. ./bootstrap. make -j20. make -j20 install. export PATH=/usr/local/bin:$PATH. ```. ## Protobuf 3.6.1 C++ static. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from /usr/bin/gcc, static for bazel. ```bash. # download source code. wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --disable-shared --enable-static. make clean. make -j20. make -j20 check # check if protobuf build is good. # install. make install. sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig. #cd .. # verify. echo ""$(which protoc)"". echo ""$(protoc --version)"". ```. ## Bazel 0.15.0. Build instructions: [https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel](https://docs.bazel.build/versions/master/install",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:2061,integrability,configur,configure,2061,"-3.13.3.tar.gz. tar -zxvf cmake-3.13.3.tar.gz. cd cmake-3.13.3. # build scirpt. ./bootstrap. make -j20. make -j20 install. export PATH=/usr/local/bin:$PATH. ```. ## Protobuf 3.6.1 C++ static. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from /usr/bin/gcc, static for bazel. ```bash. # download source code. wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --disable-shared --enable-static. make clean. make -j20. make -j20 check # check if protobuf build is good. # install. make install. sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig. #cd .. # verify. echo ""$(which protoc)"". echo ""$(protoc --version)"". ```. ## Bazel 0.15.0. Build instructions: [https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel](https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel). Edit crosstool: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). > Note: Bazel 0.15.0 should be built from /usr/bin/gcc. ```bash. # download source code. wget https://github.com/bazelbuild/bazel/releases/download/0.15.0/bazel-0.15.0-dist.zip. mkdir bazel-0.15.0. unzip -n bazel-0.15.0-dist.zip -d ./bazel-0.15.0. cd bazel-0.15.0. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export JAVA_HOME=/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:2360,integrability,version,version,2360,"ffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from /usr/bin/gcc, static for bazel. ```bash. # download source code. wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --disable-shared --enable-static. make clean. make -j20. make -j20 check # check if protobuf build is good. # install. make install. sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig. #cd .. # verify. echo ""$(which protoc)"". echo ""$(protoc --version)"". ```. ## Bazel 0.15.0. Build instructions: [https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel](https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel). Edit crosstool: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). > Note: Bazel 0.15.0 should be built from /usr/bin/gcc. ```bash. # download source code. wget https://github.com/bazelbuild/bazel/releases/download/0.15.0/bazel-0.15.0-dist.zip. mkdir bazel-0.15.0. unzip -n bazel-0.15.0-dist.zip -d ./bazel-0.15.0. cd bazel-0.15.0. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export JAVA_HOME=/usr/lib/jvm/java-1.8.0. export PATH=$HOMEPATH/inst/bin:$JAVA_HOME/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # build from scratch. PROTOC=$HOMEPATH/inst/bin/protoc ./compile.sh. rsync -avP output/bazel $HOMEPATH/inst/bin/. # verification. bazel info. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:2439,integrability,version,versions,2439,"lt from /usr/bin/gcc, static for bazel. ```bash. # download source code. wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --disable-shared --enable-static. make clean. make -j20. make -j20 check # check if protobuf build is good. # install. make install. sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig. #cd .. # verify. echo ""$(which protoc)"". echo ""$(protoc --version)"". ```. ## Bazel 0.15.0. Build instructions: [https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel](https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel). Edit crosstool: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). > Note: Bazel 0.15.0 should be built from /usr/bin/gcc. ```bash. # download source code. wget https://github.com/bazelbuild/bazel/releases/download/0.15.0/bazel-0.15.0-dist.zip. mkdir bazel-0.15.0. unzip -n bazel-0.15.0-dist.zip -d ./bazel-0.15.0. cd bazel-0.15.0. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export JAVA_HOME=/usr/lib/jvm/java-1.8.0. export PATH=$HOMEPATH/inst/bin:$JAVA_HOME/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # build from scratch. PROTOC=$HOMEPATH/inst/bin/protoc ./compile.sh. rsync -avP output/bazel $HOMEPATH/inst/bin/. # verification. bazel info. ```. ## Advance Toolchain 11.0. Doc: [https://developer.ibm.com/linuxonpower/ad",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:2525,integrability,version,versions,2525,"github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --disable-shared --enable-static. make clean. make -j20. make -j20 check # check if protobuf build is good. # install. make install. sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig. #cd .. # verify. echo ""$(which protoc)"". echo ""$(protoc --version)"". ```. ## Bazel 0.15.0. Build instructions: [https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel](https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel). Edit crosstool: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). > Note: Bazel 0.15.0 should be built from /usr/bin/gcc. ```bash. # download source code. wget https://github.com/bazelbuild/bazel/releases/download/0.15.0/bazel-0.15.0-dist.zip. mkdir bazel-0.15.0. unzip -n bazel-0.15.0-dist.zip -d ./bazel-0.15.0. cd bazel-0.15.0. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export JAVA_HOME=/usr/lib/jvm/java-1.8.0. export PATH=$HOMEPATH/inst/bin:$JAVA_HOME/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # build from scratch. PROTOC=$HOMEPATH/inst/bin/protoc ./compile.sh. rsync -avP output/bazel $HOMEPATH/inst/bin/. # verification. bazel info. ```. ## Advance Toolchain 11.0. Doc: [https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/](https://developer.ibm.com/linuxonpower/advance-",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:3578,integrability,pub,public,3578,zel). Edit crosstool: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). > Note: Bazel 0.15.0 should be built from /usr/bin/gcc. ```bash. # download source code. wget https://github.com/bazelbuild/bazel/releases/download/0.15.0/bazel-0.15.0-dist.zip. mkdir bazel-0.15.0. unzip -n bazel-0.15.0-dist.zip -d ./bazel-0.15.0. cd bazel-0.15.0. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export JAVA_HOME=/usr/lib/jvm/java-1.8.0. export PATH=$HOMEPATH/inst/bin:$JAVA_HOME/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # build from scratch. PROTOC=$HOMEPATH/inst/bin/protoc ./compile.sh. rsync -avP output/bazel $HOMEPATH/inst/bin/. # verification. bazel info. ```. ## Advance Toolchain 11.0. Doc: [https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/](https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/). ```bash. # gpg public key. wget ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. rpm --import gpg-pubkey-6976a827-5164221b. # Configure the Advance Toolchain repositories. [root@cit1074 deepvariant]# cat /etc/yum.repos.d/advance-toolchain.repo. #Begin of configuration file. [advance-toolchain]. name=Advance Toolchain Unicamp FTP. baseurl=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7. failovermethod=priority. enabled=1. gpgcheck=1. gpgkey=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. # End of configuration file. # Install the Advance Toolchain. yum install advance-toolchain-at11.0-runtime. yum install advance-toolchain-at11.0-devel. yum install advance-toolchain-at11.0-perf. yum install advance-toolchain-at11.0-mcore-libs. # Load enviroment. export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # Do not need to export. # export LD_LIBRARY_PATH=/opt/at11.0/lib64:/lib64:$LD_LIBRARY_PATH . # Load environme,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:3616,integrability,pub,pub,3616,ics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). > Note: Bazel 0.15.0 should be built from /usr/bin/gcc. ```bash. # download source code. wget https://github.com/bazelbuild/bazel/releases/download/0.15.0/bazel-0.15.0-dist.zip. mkdir bazel-0.15.0. unzip -n bazel-0.15.0-dist.zip -d ./bazel-0.15.0. cd bazel-0.15.0. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export JAVA_HOME=/usr/lib/jvm/java-1.8.0. export PATH=$HOMEPATH/inst/bin:$JAVA_HOME/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # build from scratch. PROTOC=$HOMEPATH/inst/bin/protoc ./compile.sh. rsync -avP output/bazel $HOMEPATH/inst/bin/. # verification. bazel info. ```. ## Advance Toolchain 11.0. Doc: [https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/](https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/). ```bash. # gpg public key. wget ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. rpm --import gpg-pubkey-6976a827-5164221b. # Configure the Advance Toolchain repositories. [root@cit1074 deepvariant]# cat /etc/yum.repos.d/advance-toolchain.repo. #Begin of configuration file. [advance-toolchain]. name=Advance Toolchain Unicamp FTP. baseurl=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7. failovermethod=priority. enabled=1. gpgcheck=1. gpgkey=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. # End of configuration file. # Install the Advance Toolchain. yum install advance-toolchain-at11.0-runtime. yum install advance-toolchain-at11.0-devel. yum install advance-toolchain-at11.0-perf. yum install advance-toolchain-at11.0-mcore-libs. # Load enviroment. export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # Do not need to export. # export LD_LIBRARY_PATH=/opt/at11.0/lib64:/lib64:$LD_LIBRARY_PATH . # Load environment. sudo yum install environment-modu,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:3661,integrability,pub,pubkey-,3661,://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). > Note: Bazel 0.15.0 should be built from /usr/bin/gcc. ```bash. # download source code. wget https://github.com/bazelbuild/bazel/releases/download/0.15.0/bazel-0.15.0-dist.zip. mkdir bazel-0.15.0. unzip -n bazel-0.15.0-dist.zip -d ./bazel-0.15.0. cd bazel-0.15.0. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export JAVA_HOME=/usr/lib/jvm/java-1.8.0. export PATH=$HOMEPATH/inst/bin:$JAVA_HOME/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # build from scratch. PROTOC=$HOMEPATH/inst/bin/protoc ./compile.sh. rsync -avP output/bazel $HOMEPATH/inst/bin/. # verification. bazel info. ```. ## Advance Toolchain 11.0. Doc: [https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/](https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/). ```bash. # gpg public key. wget ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. rpm --import gpg-pubkey-6976a827-5164221b. # Configure the Advance Toolchain repositories. [root@cit1074 deepvariant]# cat /etc/yum.repos.d/advance-toolchain.repo. #Begin of configuration file. [advance-toolchain]. name=Advance Toolchain Unicamp FTP. baseurl=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7. failovermethod=priority. enabled=1. gpgcheck=1. gpgkey=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. # End of configuration file. # Install the Advance Toolchain. yum install advance-toolchain-at11.0-runtime. yum install advance-toolchain-at11.0-devel. yum install advance-toolchain-at11.0-perf. yum install advance-toolchain-at11.0-mcore-libs. # Load enviroment. export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # Do not need to export. # export LD_LIBRARY_PATH=/opt/at11.0/lib64:/lib64:$LD_LIBRARY_PATH . # Load environment. sudo yum install environment-modules. source /etc/profile. module load at11.0. m,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:3704,integrability,pub,pubkey-,3704,low.html). > Note: Bazel 0.15.0 should be built from /usr/bin/gcc. ```bash. # download source code. wget https://github.com/bazelbuild/bazel/releases/download/0.15.0/bazel-0.15.0-dist.zip. mkdir bazel-0.15.0. unzip -n bazel-0.15.0-dist.zip -d ./bazel-0.15.0. cd bazel-0.15.0. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export JAVA_HOME=/usr/lib/jvm/java-1.8.0. export PATH=$HOMEPATH/inst/bin:$JAVA_HOME/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # build from scratch. PROTOC=$HOMEPATH/inst/bin/protoc ./compile.sh. rsync -avP output/bazel $HOMEPATH/inst/bin/. # verification. bazel info. ```. ## Advance Toolchain 11.0. Doc: [https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/](https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/). ```bash. # gpg public key. wget ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. rpm --import gpg-pubkey-6976a827-5164221b. # Configure the Advance Toolchain repositories. [root@cit1074 deepvariant]# cat /etc/yum.repos.d/advance-toolchain.repo. #Begin of configuration file. [advance-toolchain]. name=Advance Toolchain Unicamp FTP. baseurl=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7. failovermethod=priority. enabled=1. gpgcheck=1. gpgkey=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. # End of configuration file. # Install the Advance Toolchain. yum install advance-toolchain-at11.0-runtime. yum install advance-toolchain-at11.0-devel. yum install advance-toolchain-at11.0-perf. yum install advance-toolchain-at11.0-mcore-libs. # Load enviroment. export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # Do not need to export. # export LD_LIBRARY_PATH=/opt/at11.0/lib64:/lib64:$LD_LIBRARY_PATH . # Load environment. sudo yum install environment-modules. source /etc/profile. module load at11.0. module unload at11.0. ```. ## Python 2 and P,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:3732,integrability,Configur,Configure,3732,.0 should be built from /usr/bin/gcc. ```bash. # download source code. wget https://github.com/bazelbuild/bazel/releases/download/0.15.0/bazel-0.15.0-dist.zip. mkdir bazel-0.15.0. unzip -n bazel-0.15.0-dist.zip -d ./bazel-0.15.0. cd bazel-0.15.0. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export JAVA_HOME=/usr/lib/jvm/java-1.8.0. export PATH=$HOMEPATH/inst/bin:$JAVA_HOME/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # build from scratch. PROTOC=$HOMEPATH/inst/bin/protoc ./compile.sh. rsync -avP output/bazel $HOMEPATH/inst/bin/. # verification. bazel info. ```. ## Advance Toolchain 11.0. Doc: [https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/](https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/). ```bash. # gpg public key. wget ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. rpm --import gpg-pubkey-6976a827-5164221b. # Configure the Advance Toolchain repositories. [root@cit1074 deepvariant]# cat /etc/yum.repos.d/advance-toolchain.repo. #Begin of configuration file. [advance-toolchain]. name=Advance Toolchain Unicamp FTP. baseurl=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7. failovermethod=priority. enabled=1. gpgcheck=1. gpgkey=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. # End of configuration file. # Install the Advance Toolchain. yum install advance-toolchain-at11.0-runtime. yum install advance-toolchain-at11.0-devel. yum install advance-toolchain-at11.0-perf. yum install advance-toolchain-at11.0-mcore-libs. # Load enviroment. export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # Do not need to export. # export LD_LIBRARY_PATH=/opt/at11.0/lib64:/lib64:$LD_LIBRARY_PATH . # Load environment. sudo yum install environment-modules. source /etc/profile. module load at11.0. module unload at11.0. ```. ## Python 2 and Pip 19.0.1. Build instructions,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:3764,integrability,repositor,repositories,3764,gcc. ```bash. # download source code. wget https://github.com/bazelbuild/bazel/releases/download/0.15.0/bazel-0.15.0-dist.zip. mkdir bazel-0.15.0. unzip -n bazel-0.15.0-dist.zip -d ./bazel-0.15.0. cd bazel-0.15.0. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export JAVA_HOME=/usr/lib/jvm/java-1.8.0. export PATH=$HOMEPATH/inst/bin:$JAVA_HOME/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # build from scratch. PROTOC=$HOMEPATH/inst/bin/protoc ./compile.sh. rsync -avP output/bazel $HOMEPATH/inst/bin/. # verification. bazel info. ```. ## Advance Toolchain 11.0. Doc: [https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/](https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/). ```bash. # gpg public key. wget ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. rpm --import gpg-pubkey-6976a827-5164221b. # Configure the Advance Toolchain repositories. [root@cit1074 deepvariant]# cat /etc/yum.repos.d/advance-toolchain.repo. #Begin of configuration file. [advance-toolchain]. name=Advance Toolchain Unicamp FTP. baseurl=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7. failovermethod=priority. enabled=1. gpgcheck=1. gpgkey=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. # End of configuration file. # Install the Advance Toolchain. yum install advance-toolchain-at11.0-runtime. yum install advance-toolchain-at11.0-devel. yum install advance-toolchain-at11.0-perf. yum install advance-toolchain-at11.0-mcore-libs. # Load enviroment. export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # Do not need to export. # export LD_LIBRARY_PATH=/opt/at11.0/lib64:/lib64:$LD_LIBRARY_PATH . # Load environment. sudo yum install environment-modules. source /etc/profile. module load at11.0. module unload at11.0. ```. ## Python 2 and Pip 19.0.1. Build instructions: [https://docs.python.org/2/usin,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:3861,integrability,configur,configuration,3861,.15.0/bazel-0.15.0-dist.zip. mkdir bazel-0.15.0. unzip -n bazel-0.15.0-dist.zip -d ./bazel-0.15.0. cd bazel-0.15.0. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export JAVA_HOME=/usr/lib/jvm/java-1.8.0. export PATH=$HOMEPATH/inst/bin:$JAVA_HOME/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # build from scratch. PROTOC=$HOMEPATH/inst/bin/protoc ./compile.sh. rsync -avP output/bazel $HOMEPATH/inst/bin/. # verification. bazel info. ```. ## Advance Toolchain 11.0. Doc: [https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/](https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/). ```bash. # gpg public key. wget ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. rpm --import gpg-pubkey-6976a827-5164221b. # Configure the Advance Toolchain repositories. [root@cit1074 deepvariant]# cat /etc/yum.repos.d/advance-toolchain.repo. #Begin of configuration file. [advance-toolchain]. name=Advance Toolchain Unicamp FTP. baseurl=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7. failovermethod=priority. enabled=1. gpgcheck=1. gpgkey=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. # End of configuration file. # Install the Advance Toolchain. yum install advance-toolchain-at11.0-runtime. yum install advance-toolchain-at11.0-devel. yum install advance-toolchain-at11.0-perf. yum install advance-toolchain-at11.0-mcore-libs. # Load enviroment. export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # Do not need to export. # export LD_LIBRARY_PATH=/opt/at11.0/lib64:/lib64:$LD_LIBRARY_PATH . # Load environment. sudo yum install environment-modules. source /etc/profile. module load at11.0. module unload at11.0. ```. ## Python 2 and Pip 19.0.1. Build instructions: [https://docs.python.org/2/using/unix.html](https://docs.python.org/2/using/unix.html). > Note: Python 2 should be built from AT ,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:3967,integrability,pub,pub,3967, bazel-0.15.0. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export JAVA_HOME=/usr/lib/jvm/java-1.8.0. export PATH=$HOMEPATH/inst/bin:$JAVA_HOME/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # build from scratch. PROTOC=$HOMEPATH/inst/bin/protoc ./compile.sh. rsync -avP output/bazel $HOMEPATH/inst/bin/. # verification. bazel info. ```. ## Advance Toolchain 11.0. Doc: [https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/](https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/). ```bash. # gpg public key. wget ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. rpm --import gpg-pubkey-6976a827-5164221b. # Configure the Advance Toolchain repositories. [root@cit1074 deepvariant]# cat /etc/yum.repos.d/advance-toolchain.repo. #Begin of configuration file. [advance-toolchain]. name=Advance Toolchain Unicamp FTP. baseurl=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7. failovermethod=priority. enabled=1. gpgcheck=1. gpgkey=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. # End of configuration file. # Install the Advance Toolchain. yum install advance-toolchain-at11.0-runtime. yum install advance-toolchain-at11.0-devel. yum install advance-toolchain-at11.0-perf. yum install advance-toolchain-at11.0-mcore-libs. # Load enviroment. export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # Do not need to export. # export LD_LIBRARY_PATH=/opt/at11.0/lib64:/lib64:$LD_LIBRARY_PATH . # Load environment. sudo yum install environment-modules. source /etc/profile. module load at11.0. module unload at11.0. ```. ## Python 2 and Pip 19.0.1. Build instructions: [https://docs.python.org/2/using/unix.html](https://docs.python.org/2/using/unix.html). > Note: Python 2 should be built from AT 11.0. ```bash. # download source code . wget https://www.python.org/ftp/python/2.7.15/Python-2.7.15.t,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:4085,integrability,pub,pub,4085,0. export PATH=$HOMEPATH/inst/bin:$JAVA_HOME/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # build from scratch. PROTOC=$HOMEPATH/inst/bin/protoc ./compile.sh. rsync -avP output/bazel $HOMEPATH/inst/bin/. # verification. bazel info. ```. ## Advance Toolchain 11.0. Doc: [https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/](https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/). ```bash. # gpg public key. wget ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. rpm --import gpg-pubkey-6976a827-5164221b. # Configure the Advance Toolchain repositories. [root@cit1074 deepvariant]# cat /etc/yum.repos.d/advance-toolchain.repo. #Begin of configuration file. [advance-toolchain]. name=Advance Toolchain Unicamp FTP. baseurl=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7. failovermethod=priority. enabled=1. gpgcheck=1. gpgkey=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. # End of configuration file. # Install the Advance Toolchain. yum install advance-toolchain-at11.0-runtime. yum install advance-toolchain-at11.0-devel. yum install advance-toolchain-at11.0-perf. yum install advance-toolchain-at11.0-mcore-libs. # Load enviroment. export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # Do not need to export. # export LD_LIBRARY_PATH=/opt/at11.0/lib64:/lib64:$LD_LIBRARY_PATH . # Load environment. sudo yum install environment-modules. source /etc/profile. module load at11.0. module unload at11.0. ```. ## Python 2 and Pip 19.0.1. Build instructions: [https://docs.python.org/2/using/unix.html](https://docs.python.org/2/using/unix.html). > Note: Python 2 should be built from AT 11.0. ```bash. # download source code . wget https://www.python.org/ftp/python/2.7.15/Python-2.7.15.tgz. tar -zxvf Python-2.7.15.tgz. cd Python-2.7.15. # environment. export HOMEPATH=/home/qilibj. export CPU=power8. # c,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:4130,integrability,pub,pubkey-,4130,"n:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # build from scratch. PROTOC=$HOMEPATH/inst/bin/protoc ./compile.sh. rsync -avP output/bazel $HOMEPATH/inst/bin/. # verification. bazel info. ```. ## Advance Toolchain 11.0. Doc: [https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/](https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/). ```bash. # gpg public key. wget ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. rpm --import gpg-pubkey-6976a827-5164221b. # Configure the Advance Toolchain repositories. [root@cit1074 deepvariant]# cat /etc/yum.repos.d/advance-toolchain.repo. #Begin of configuration file. [advance-toolchain]. name=Advance Toolchain Unicamp FTP. baseurl=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7. failovermethod=priority. enabled=1. gpgcheck=1. gpgkey=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. # End of configuration file. # Install the Advance Toolchain. yum install advance-toolchain-at11.0-runtime. yum install advance-toolchain-at11.0-devel. yum install advance-toolchain-at11.0-perf. yum install advance-toolchain-at11.0-mcore-libs. # Load enviroment. export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # Do not need to export. # export LD_LIBRARY_PATH=/opt/at11.0/lib64:/lib64:$LD_LIBRARY_PATH . # Load environment. sudo yum install environment-modules. source /etc/profile. module load at11.0. module unload at11.0. ```. ## Python 2 and Pip 19.0.1. Build instructions: [https://docs.python.org/2/using/unix.html](https://docs.python.org/2/using/unix.html). > Note: Python 2 should be built from AT 11.0. ```bash. # download source code . wget https://www.python.org/ftp/python/2.7.15/Python-2.7.15.tgz. tar -zxvf Python-2.7.15.tgz. cd Python-2.7.15. # environment. export HOMEPATH=/home/qilibj. export CPU=power8. # check gcc before build, should be AT11.0. which ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:4165,integrability,configur,configuration,4165,"sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # build from scratch. PROTOC=$HOMEPATH/inst/bin/protoc ./compile.sh. rsync -avP output/bazel $HOMEPATH/inst/bin/. # verification. bazel info. ```. ## Advance Toolchain 11.0. Doc: [https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/](https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/). ```bash. # gpg public key. wget ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. rpm --import gpg-pubkey-6976a827-5164221b. # Configure the Advance Toolchain repositories. [root@cit1074 deepvariant]# cat /etc/yum.repos.d/advance-toolchain.repo. #Begin of configuration file. [advance-toolchain]. name=Advance Toolchain Unicamp FTP. baseurl=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7. failovermethod=priority. enabled=1. gpgcheck=1. gpgkey=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. # End of configuration file. # Install the Advance Toolchain. yum install advance-toolchain-at11.0-runtime. yum install advance-toolchain-at11.0-devel. yum install advance-toolchain-at11.0-perf. yum install advance-toolchain-at11.0-mcore-libs. # Load enviroment. export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # Do not need to export. # export LD_LIBRARY_PATH=/opt/at11.0/lib64:/lib64:$LD_LIBRARY_PATH . # Load environment. sudo yum install environment-modules. source /etc/profile. module load at11.0. module unload at11.0. ```. ## Python 2 and Pip 19.0.1. Build instructions: [https://docs.python.org/2/using/unix.html](https://docs.python.org/2/using/unix.html). > Note: Python 2 should be built from AT 11.0. ```bash. # download source code . wget https://www.python.org/ftp/python/2.7.15/Python-2.7.15.tgz. tar -zxvf Python-2.7.15.tgz. cd Python-2.7.15. # environment. export HOMEPATH=/home/qilibj. export CPU=power8. # check gcc before build, should be AT11.0. which gcc. # build. CFLAGS=""-mcpu=$CPU -mtun",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:5224,integrability,configur,configure,5224," advance-toolchain-at11.0-runtime. yum install advance-toolchain-at11.0-devel. yum install advance-toolchain-at11.0-perf. yum install advance-toolchain-at11.0-mcore-libs. # Load enviroment. export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # Do not need to export. # export LD_LIBRARY_PATH=/opt/at11.0/lib64:/lib64:$LD_LIBRARY_PATH . # Load environment. sudo yum install environment-modules. source /etc/profile. module load at11.0. module unload at11.0. ```. ## Python 2 and Pip 19.0.1. Build instructions: [https://docs.python.org/2/using/unix.html](https://docs.python.org/2/using/unix.html). > Note: Python 2 should be built from AT 11.0. ```bash. # download source code . wget https://www.python.org/ftp/python/2.7.15/Python-2.7.15.tgz. tar -zxvf Python-2.7.15.tgz. cd Python-2.7.15. # environment. export HOMEPATH=/home/qilibj. export CPU=power8. # check gcc before build, should be AT11.0. which gcc. # build. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CPPFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static. make -j20. make install. # set environment. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH. export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. # verify 2.7.15. echo ""$(python --version)"". # Pip 19.0.2. wget -qc https://bootstrap.pypa.io/get-pip.py --no-check-certificate. $HOMEPATH/inst/bin/python ./get-pip.py --prefix $HOMEPATH/inst. #pip install --upgrade --force-reinstall pip. echo ""$(pip --version)"". pip install setuptools nose asv cython future protobuf==3.6.1 six mock. pip install --upgrade setuptools. ```. ## Protobuf 3.6.1 C++ shared. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from AT 11.0, C++ shared for TF 1.12. ```bash. # downlo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:5615,integrability,version,version,5615,"es. source /etc/profile. module load at11.0. module unload at11.0. ```. ## Python 2 and Pip 19.0.1. Build instructions: [https://docs.python.org/2/using/unix.html](https://docs.python.org/2/using/unix.html). > Note: Python 2 should be built from AT 11.0. ```bash. # download source code . wget https://www.python.org/ftp/python/2.7.15/Python-2.7.15.tgz. tar -zxvf Python-2.7.15.tgz. cd Python-2.7.15. # environment. export HOMEPATH=/home/qilibj. export CPU=power8. # check gcc before build, should be AT11.0. which gcc. # build. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CPPFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static. make -j20. make install. # set environment. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH. export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. # verify 2.7.15. echo ""$(python --version)"". # Pip 19.0.2. wget -qc https://bootstrap.pypa.io/get-pip.py --no-check-certificate. $HOMEPATH/inst/bin/python ./get-pip.py --prefix $HOMEPATH/inst. #pip install --upgrade --force-reinstall pip. echo ""$(pip --version)"". pip install setuptools nose asv cython future protobuf==3.6.1 six mock. pip install --upgrade setuptools. ```. ## Protobuf 3.6.1 C++ shared. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from AT 11.0, C++ shared for TF 1.12. ```bash. # download source code. wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:5834,integrability,version,version,5834,"hon 2 should be built from AT 11.0. ```bash. # download source code . wget https://www.python.org/ftp/python/2.7.15/Python-2.7.15.tgz. tar -zxvf Python-2.7.15.tgz. cd Python-2.7.15. # environment. export HOMEPATH=/home/qilibj. export CPU=power8. # check gcc before build, should be AT11.0. which gcc. # build. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CPPFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static. make -j20. make install. # set environment. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH. export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. # verify 2.7.15. echo ""$(python --version)"". # Pip 19.0.2. wget -qc https://bootstrap.pypa.io/get-pip.py --no-check-certificate. $HOMEPATH/inst/bin/python ./get-pip.py --prefix $HOMEPATH/inst. #pip install --upgrade --force-reinstall pip. echo ""$(pip --version)"". pip install setuptools nose asv cython future protobuf==3.6.1 six mock. pip install --upgrade setuptools. ```. ## Protobuf 3.6.1 C++ shared. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from AT 11.0, C++ shared for TF 1.12. ```bash. # download source code. wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static. make clean. make -j20. # optional. make -j20 check # check if protobuf build is good. # install. make install. sudo ldconfig # refresh shared library cache - /op",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:6011,integrability,protocol,protocolbuffers,6011," # environment. export HOMEPATH=/home/qilibj. export CPU=power8. # check gcc before build, should be AT11.0. which gcc. # build. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CPPFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static. make -j20. make install. # set environment. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH. export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. # verify 2.7.15. echo ""$(python --version)"". # Pip 19.0.2. wget -qc https://bootstrap.pypa.io/get-pip.py --no-check-certificate. $HOMEPATH/inst/bin/python ./get-pip.py --prefix $HOMEPATH/inst. #pip install --upgrade --force-reinstall pip. echo ""$(pip --version)"". pip install setuptools nose asv cython future protobuf==3.6.1 six mock. pip install --upgrade setuptools. ```. ## Protobuf 3.6.1 C++ shared. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from AT 11.0, C++ shared for TF 1.12. ```bash. # download source code. wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static. make clean. make -j20. # optional. make -j20 check # check if protobuf build is good. # install. make install. sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig. #cd .. # verify. echo ""$(which protoc)"". echo ""$(protoc --version)"". ```. ## Protobuf 3.6.1 Python from source code. Python: [https://github.com/protocolbuff",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:6082,integrability,protocol,protocolbuffers,6082,"k gcc before build, should be AT11.0. which gcc. # build. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CPPFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static. make -j20. make install. # set environment. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH. export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. # verify 2.7.15. echo ""$(python --version)"". # Pip 19.0.2. wget -qc https://bootstrap.pypa.io/get-pip.py --no-check-certificate. $HOMEPATH/inst/bin/python ./get-pip.py --prefix $HOMEPATH/inst. #pip install --upgrade --force-reinstall pip. echo ""$(pip --version)"". pip install setuptools nose asv cython future protobuf==3.6.1 six mock. pip install --upgrade setuptools. ```. ## Protobuf 3.6.1 C++ shared. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from AT 11.0, C++ shared for TF 1.12. ```bash. # download source code. wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static. make clean. make -j20. # optional. make -j20 check # check if protobuf build is good. # install. make install. sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig. #cd .. # verify. echo ""$(which protoc)"". echo ""$(protoc --version)"". ```. ## Protobuf 3.6.1 Python from source code. Python: [https://github.com/protocolbuffers/protobuf/blob/master/python/README.md](https://github.com/protocolb",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:6269,integrability,protocol,protocolbuffers,6269,"isable-static. make -j20. make install. # set environment. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH. export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. # verify 2.7.15. echo ""$(python --version)"". # Pip 19.0.2. wget -qc https://bootstrap.pypa.io/get-pip.py --no-check-certificate. $HOMEPATH/inst/bin/python ./get-pip.py --prefix $HOMEPATH/inst. #pip install --upgrade --force-reinstall pip. echo ""$(pip --version)"". pip install setuptools nose asv cython future protobuf==3.6.1 six mock. pip install --upgrade setuptools. ```. ## Protobuf 3.6.1 C++ shared. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from AT 11.0, C++ shared for TF 1.12. ```bash. # download source code. wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static. make clean. make -j20. # optional. make -j20 check # check if protobuf build is good. # install. make install. sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig. #cd .. # verify. echo ""$(which protoc)"". echo ""$(protoc --version)"". ```. ## Protobuf 3.6.1 Python from source code. Python: [https://github.com/protocolbuffers/protobuf/blob/master/python/README.md](https://github.com/protocolbuffers/protobuf/blob/master/python/README.md). > Note: Protobuf 3.6.1 should be built from source code or CLIF cannot find protobuf. ```bash. # share build for Python. python --version # ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:6609,integrability,configur,configure,6609,"-version)"". # Pip 19.0.2. wget -qc https://bootstrap.pypa.io/get-pip.py --no-check-certificate. $HOMEPATH/inst/bin/python ./get-pip.py --prefix $HOMEPATH/inst. #pip install --upgrade --force-reinstall pip. echo ""$(pip --version)"". pip install setuptools nose asv cython future protobuf==3.6.1 six mock. pip install --upgrade setuptools. ```. ## Protobuf 3.6.1 C++ shared. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from AT 11.0, C++ shared for TF 1.12. ```bash. # download source code. wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static. make clean. make -j20. # optional. make -j20 check # check if protobuf build is good. # install. make install. sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig. #cd .. # verify. echo ""$(which protoc)"". echo ""$(protoc --version)"". ```. ## Protobuf 3.6.1 Python from source code. Python: [https://github.com/protocolbuffers/protobuf/blob/master/python/README.md](https://github.com/protocolbuffers/protobuf/blob/master/python/README.md). > Note: Protobuf 3.6.1 should be built from source code or CLIF cannot find protobuf. ```bash. # share build for Python. python --version # python 2.7 or newer. protoc --version. # build. cd protobuf-3.6.1/python/. python setup.py build. python setup.py test. # install from source as deepvariant needed. python setup.py install. # install from wheel. python setup.py bdist_wheel. pip install protobuf-3.6.1-py2.py3-none-any.whl --force-reinstall. # verify. python -c ""import g",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:6920,integrability,version,version,6920,"tall --upgrade setuptools. ```. ## Protobuf 3.6.1 C++ shared. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from AT 11.0, C++ shared for TF 1.12. ```bash. # download source code. wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static. make clean. make -j20. # optional. make -j20 check # check if protobuf build is good. # install. make install. sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig. #cd .. # verify. echo ""$(which protoc)"". echo ""$(protoc --version)"". ```. ## Protobuf 3.6.1 Python from source code. Python: [https://github.com/protocolbuffers/protobuf/blob/master/python/README.md](https://github.com/protocolbuffers/protobuf/blob/master/python/README.md). > Note: Protobuf 3.6.1 should be built from source code or CLIF cannot find protobuf. ```bash. # share build for Python. python --version # python 2.7 or newer. protoc --version. # build. cd protobuf-3.6.1/python/. python setup.py build. python setup.py test. # install from source as deepvariant needed. python setup.py install. # install from wheel. python setup.py bdist_wheel. pip install protobuf-3.6.1-py2.py3-none-any.whl --force-reinstall. # verify. python -c ""import google.protobuf"". ```. ## OpenBLAS 0.3.5. ```bash. git clone -b v0.3.5 https://github.com/xianyi/OpenBLAS.git OpenBLAS-0.3.5. cd OpenBLAS-0.3.5. make TARGET=power8. make TARGET=power8 PREFIX=$HOMEPATH/inst install. ```. ## Boost 1.66.0. ```bash. wget -qc https://dl.bintray.com/boostorg/release/1.66.0/source/bo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:7007,integrability,protocol,protocolbuffers,7007,"ocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from AT 11.0, C++ shared for TF 1.12. ```bash. # download source code. wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static. make clean. make -j20. # optional. make -j20 check # check if protobuf build is good. # install. make install. sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig. #cd .. # verify. echo ""$(which protoc)"". echo ""$(protoc --version)"". ```. ## Protobuf 3.6.1 Python from source code. Python: [https://github.com/protocolbuffers/protobuf/blob/master/python/README.md](https://github.com/protocolbuffers/protobuf/blob/master/python/README.md). > Note: Protobuf 3.6.1 should be built from source code or CLIF cannot find protobuf. ```bash. # share build for Python. python --version # python 2.7 or newer. protoc --version. # build. cd protobuf-3.6.1/python/. python setup.py build. python setup.py test. # install from source as deepvariant needed. python setup.py install. # install from wheel. python setup.py bdist_wheel. pip install protobuf-3.6.1-py2.py3-none-any.whl --force-reinstall. # verify. python -c ""import google.protobuf"". ```. ## OpenBLAS 0.3.5. ```bash. git clone -b v0.3.5 https://github.com/xianyi/OpenBLAS.git OpenBLAS-0.3.5. cd OpenBLAS-0.3.5. make TARGET=power8. make TARGET=power8 PREFIX=$HOMEPATH/inst install. ```. ## Boost 1.66.0. ```bash. wget -qc https://dl.bintray.com/boostorg/release/1.66.0/source/boost_1_66_0.tar.gz. tar xzf boost_1_66_0.tar.gz. cd boost_1_66_0. ./bootstrap.sh --with-tool",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:7081,integrability,protocol,protocolbuffers,7081,"lbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from AT 11.0, C++ shared for TF 1.12. ```bash. # download source code. wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static. make clean. make -j20. # optional. make -j20 check # check if protobuf build is good. # install. make install. sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig. #cd .. # verify. echo ""$(which protoc)"". echo ""$(protoc --version)"". ```. ## Protobuf 3.6.1 Python from source code. Python: [https://github.com/protocolbuffers/protobuf/blob/master/python/README.md](https://github.com/protocolbuffers/protobuf/blob/master/python/README.md). > Note: Protobuf 3.6.1 should be built from source code or CLIF cannot find protobuf. ```bash. # share build for Python. python --version # python 2.7 or newer. protoc --version. # build. cd protobuf-3.6.1/python/. python setup.py build. python setup.py test. # install from source as deepvariant needed. python setup.py install. # install from wheel. python setup.py bdist_wheel. pip install protobuf-3.6.1-py2.py3-none-any.whl --force-reinstall. # verify. python -c ""import google.protobuf"". ```. ## OpenBLAS 0.3.5. ```bash. git clone -b v0.3.5 https://github.com/xianyi/OpenBLAS.git OpenBLAS-0.3.5. cd OpenBLAS-0.3.5. make TARGET=power8. make TARGET=power8 PREFIX=$HOMEPATH/inst install. ```. ## Boost 1.66.0. ```bash. wget -qc https://dl.bintray.com/boostorg/release/1.66.0/source/boost_1_66_0.tar.gz. tar xzf boost_1_66_0.tar.gz. cd boost_1_66_0. ./bootstrap.sh --with-toolset=gcc --prefix=$HOMEPATH/inst. ./b2 dll-path=""$HOMEPATH/inst/lib"" instal",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:7267,integrability,version,version,7267,"otocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static. make clean. make -j20. # optional. make -j20 check # check if protobuf build is good. # install. make install. sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig. #cd .. # verify. echo ""$(which protoc)"". echo ""$(protoc --version)"". ```. ## Protobuf 3.6.1 Python from source code. Python: [https://github.com/protocolbuffers/protobuf/blob/master/python/README.md](https://github.com/protocolbuffers/protobuf/blob/master/python/README.md). > Note: Protobuf 3.6.1 should be built from source code or CLIF cannot find protobuf. ```bash. # share build for Python. python --version # python 2.7 or newer. protoc --version. # build. cd protobuf-3.6.1/python/. python setup.py build. python setup.py test. # install from source as deepvariant needed. python setup.py install. # install from wheel. python setup.py bdist_wheel. pip install protobuf-3.6.1-py2.py3-none-any.whl --force-reinstall. # verify. python -c ""import google.protobuf"". ```. ## OpenBLAS 0.3.5. ```bash. git clone -b v0.3.5 https://github.com/xianyi/OpenBLAS.git OpenBLAS-0.3.5. cd OpenBLAS-0.3.5. make TARGET=power8. make TARGET=power8 PREFIX=$HOMEPATH/inst install. ```. ## Boost 1.66.0. ```bash. wget -qc https://dl.bintray.com/boostorg/release/1.66.0/source/boost_1_66_0.tar.gz. tar xzf boost_1_66_0.tar.gz. cd boost_1_66_0. ./bootstrap.sh --with-toolset=gcc --prefix=$HOMEPATH/inst. ./b2 dll-path=""$HOMEPATH/inst/lib"" install. ```. ## TensorFlow 1.12.0. > Note: Tensorflow 1.12.0 can be built with AT 11.0. Build instructions: [https://www.tensorflow.org/install/source](https://www.tensorflow.org/install/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:7307,integrability,version,version,7307,"/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static. make clean. make -j20. # optional. make -j20 check # check if protobuf build is good. # install. make install. sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig. #cd .. # verify. echo ""$(which protoc)"". echo ""$(protoc --version)"". ```. ## Protobuf 3.6.1 Python from source code. Python: [https://github.com/protocolbuffers/protobuf/blob/master/python/README.md](https://github.com/protocolbuffers/protobuf/blob/master/python/README.md). > Note: Protobuf 3.6.1 should be built from source code or CLIF cannot find protobuf. ```bash. # share build for Python. python --version # python 2.7 or newer. protoc --version. # build. cd protobuf-3.6.1/python/. python setup.py build. python setup.py test. # install from source as deepvariant needed. python setup.py install. # install from wheel. python setup.py bdist_wheel. pip install protobuf-3.6.1-py2.py3-none-any.whl --force-reinstall. # verify. python -c ""import google.protobuf"". ```. ## OpenBLAS 0.3.5. ```bash. git clone -b v0.3.5 https://github.com/xianyi/OpenBLAS.git OpenBLAS-0.3.5. cd OpenBLAS-0.3.5. make TARGET=power8. make TARGET=power8 PREFIX=$HOMEPATH/inst install. ```. ## Boost 1.66.0. ```bash. wget -qc https://dl.bintray.com/boostorg/release/1.66.0/source/boost_1_66_0.tar.gz. tar xzf boost_1_66_0.tar.gz. cd boost_1_66_0. ./bootstrap.sh --with-toolset=gcc --prefix=$HOMEPATH/inst. ./b2 dll-path=""$HOMEPATH/inst/lib"" install. ```. ## TensorFlow 1.12.0. > Note: Tensorflow 1.12.0 can be built with AT 11.0. Build instructions: [https://www.tensorflow.org/install/source](https://www.tensorflow.org/install/source). Edit instructions: [http://biop",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:8640,integrability,depend,dependency,8640,"AS 0.3.5. ```bash. git clone -b v0.3.5 https://github.com/xianyi/OpenBLAS.git OpenBLAS-0.3.5. cd OpenBLAS-0.3.5. make TARGET=power8. make TARGET=power8 PREFIX=$HOMEPATH/inst install. ```. ## Boost 1.66.0. ```bash. wget -qc https://dl.bintray.com/boostorg/release/1.66.0/source/boost_1_66_0.tar.gz. tar xzf boost_1_66_0.tar.gz. cd boost_1_66_0. ./bootstrap.sh --with-toolset=gcc --prefix=$HOMEPATH/inst. ./b2 dll-path=""$HOMEPATH/inst/lib"" install. ```. ## TensorFlow 1.12.0. > Note: Tensorflow 1.12.0 can be built with AT 11.0. Build instructions: [https://www.tensorflow.org/install/source](https://www.tensorflow.org/install/source). Edit instructions: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). Floatn.h error: [https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f](https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f). ```bash. # development packages. yum install python-devel python-pip -y. # dependency of numpy 1.14.6. OPT=""-D_XLOCALE_H=1 -O3 -mcpu=$CPU -mtune=$CPU -maltivec -mvsx -ffast-math -fpeel-loops -funroll-loops -ftree-vectorize -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free"" BLAS=$HOMEPATH/inst/lib/libopenblas.so LAPACK=$HOMEPATH/inst/lib/libopenblas.so ATLAS=$HOMEPATH/inst/lib/libopenblas.so pip install numpy==1.14.6. # verify. python -c ""import numpy"". # dependecy of scipy 1.2.0. OPT=""-O3 -mcpu=$CPU -mtune=$CPU -maltivec -mvsx -ffast-math -fpeel-loops -funroll-loops -ftree-vectorize -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free"" BLAS=$HOMEPATH/inst/lib/libopenblas.so LAPACK=$HOMEPATH/inst/lib/libopenblas.so ATLAS=$HOMEPATH/inst/lib/libopenblas.so pip install scipy==1.2.0. # verify. python -c ""import scipy"". # pip package dependencies. # pip install pip six wheel mock. pip install wheel autograd h5py==2.9.0 enum34. pip install keras_applications==1.0.6 keras_preprocessing==1.0.5. # download source code. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:9053,integrability,depend,dependecy,9053,"ath=""$HOMEPATH/inst/lib"" install. ```. ## TensorFlow 1.12.0. > Note: Tensorflow 1.12.0 can be built with AT 11.0. Build instructions: [https://www.tensorflow.org/install/source](https://www.tensorflow.org/install/source). Edit instructions: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). Floatn.h error: [https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f](https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f). ```bash. # development packages. yum install python-devel python-pip -y. # dependency of numpy 1.14.6. OPT=""-D_XLOCALE_H=1 -O3 -mcpu=$CPU -mtune=$CPU -maltivec -mvsx -ffast-math -fpeel-loops -funroll-loops -ftree-vectorize -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free"" BLAS=$HOMEPATH/inst/lib/libopenblas.so LAPACK=$HOMEPATH/inst/lib/libopenblas.so ATLAS=$HOMEPATH/inst/lib/libopenblas.so pip install numpy==1.14.6. # verify. python -c ""import numpy"". # dependecy of scipy 1.2.0. OPT=""-O3 -mcpu=$CPU -mtune=$CPU -maltivec -mvsx -ffast-math -fpeel-loops -funroll-loops -ftree-vectorize -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free"" BLAS=$HOMEPATH/inst/lib/libopenblas.so LAPACK=$HOMEPATH/inst/lib/libopenblas.so ATLAS=$HOMEPATH/inst/lib/libopenblas.so pip install scipy==1.2.0. # verify. python -c ""import scipy"". # pip package dependencies. # pip install pip six wheel mock. pip install wheel autograd h5py==2.9.0 enum34. pip install keras_applications==1.0.6 keras_preprocessing==1.0.5. # download source code. git clone -b r1.12 https://github.com/tensorflow/tensorflow.git tensorflow-1.12. cd tensorflow-1.12. # configure in tensorflow/.tf_configure.bazelrc. PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python"" \. PYTHON_LIB_PATH=""$HOMEPATH/inst/lib/python2.7/site-packages"" \. TF_NEED_IGNITE=""0"" \. TF_ENABLE_XLA=""0"" \. TF_NEED_OPENCL_SYCL=""0"" \. TF_NEED_ROCM=""0"" \. TF_NEED_MPI=""0"" \. TF_NEED_TENSORRT=""0"" \. TF_NEED_CUDA=""1"" \.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:9460,integrability,depend,dependencies,9460,"93ff2d9bc1bf2cd12bc6e76010da0f](https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f). ```bash. # development packages. yum install python-devel python-pip -y. # dependency of numpy 1.14.6. OPT=""-D_XLOCALE_H=1 -O3 -mcpu=$CPU -mtune=$CPU -maltivec -mvsx -ffast-math -fpeel-loops -funroll-loops -ftree-vectorize -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free"" BLAS=$HOMEPATH/inst/lib/libopenblas.so LAPACK=$HOMEPATH/inst/lib/libopenblas.so ATLAS=$HOMEPATH/inst/lib/libopenblas.so pip install numpy==1.14.6. # verify. python -c ""import numpy"". # dependecy of scipy 1.2.0. OPT=""-O3 -mcpu=$CPU -mtune=$CPU -maltivec -mvsx -ffast-math -fpeel-loops -funroll-loops -ftree-vectorize -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free"" BLAS=$HOMEPATH/inst/lib/libopenblas.so LAPACK=$HOMEPATH/inst/lib/libopenblas.so ATLAS=$HOMEPATH/inst/lib/libopenblas.so pip install scipy==1.2.0. # verify. python -c ""import scipy"". # pip package dependencies. # pip install pip six wheel mock. pip install wheel autograd h5py==2.9.0 enum34. pip install keras_applications==1.0.6 keras_preprocessing==1.0.5. # download source code. git clone -b r1.12 https://github.com/tensorflow/tensorflow.git tensorflow-1.12. cd tensorflow-1.12. # configure in tensorflow/.tf_configure.bazelrc. PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python"" \. PYTHON_LIB_PATH=""$HOMEPATH/inst/lib/python2.7/site-packages"" \. TF_NEED_IGNITE=""0"" \. TF_ENABLE_XLA=""0"" \. TF_NEED_OPENCL_SYCL=""0"" \. TF_NEED_ROCM=""0"" \. TF_NEED_MPI=""0"" \. TF_NEED_TENSORRT=""0"" \. TF_NEED_CUDA=""1"" \. TF_CUDA_VERSION=""10.0"" \. CUDA_TOOLKIT_PATH=""/usr/local/cuda"" \. TF_CUDNN_VERSION=""7"" \. CUDNN_INSTALL_PATH=""/usr/local/cuda-10.0"" \. TF_NCCL_VERSION=""2"" \. NCCL_INSTALL_PATH=""/usr/local/cuda-10.0/targets/ppc64le-linux/lib"" \. NCCL_HDR_PATH=""/usr/local/cuda-10.0/targets/ppc64le-linux/lib/../include"" \. TF_CUDA_COMPUTE_CAPABILITIES=""3.7"" \. TF_CUDA_CLANG=""0"" \. GCC_HOST_COMPILER_PATH=""/opt/at11.0/bin/gcc""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:9748,integrability,configur,configure,9748,"ps -funroll-loops -ftree-vectorize -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free"" BLAS=$HOMEPATH/inst/lib/libopenblas.so LAPACK=$HOMEPATH/inst/lib/libopenblas.so ATLAS=$HOMEPATH/inst/lib/libopenblas.so pip install numpy==1.14.6. # verify. python -c ""import numpy"". # dependecy of scipy 1.2.0. OPT=""-O3 -mcpu=$CPU -mtune=$CPU -maltivec -mvsx -ffast-math -fpeel-loops -funroll-loops -ftree-vectorize -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free"" BLAS=$HOMEPATH/inst/lib/libopenblas.so LAPACK=$HOMEPATH/inst/lib/libopenblas.so ATLAS=$HOMEPATH/inst/lib/libopenblas.so pip install scipy==1.2.0. # verify. python -c ""import scipy"". # pip package dependencies. # pip install pip six wheel mock. pip install wheel autograd h5py==2.9.0 enum34. pip install keras_applications==1.0.6 keras_preprocessing==1.0.5. # download source code. git clone -b r1.12 https://github.com/tensorflow/tensorflow.git tensorflow-1.12. cd tensorflow-1.12. # configure in tensorflow/.tf_configure.bazelrc. PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python"" \. PYTHON_LIB_PATH=""$HOMEPATH/inst/lib/python2.7/site-packages"" \. TF_NEED_IGNITE=""0"" \. TF_ENABLE_XLA=""0"" \. TF_NEED_OPENCL_SYCL=""0"" \. TF_NEED_ROCM=""0"" \. TF_NEED_MPI=""0"" \. TF_NEED_TENSORRT=""0"" \. TF_NEED_CUDA=""1"" \. TF_CUDA_VERSION=""10.0"" \. CUDA_TOOLKIT_PATH=""/usr/local/cuda"" \. TF_CUDNN_VERSION=""7"" \. CUDNN_INSTALL_PATH=""/usr/local/cuda-10.0"" \. TF_NCCL_VERSION=""2"" \. NCCL_INSTALL_PATH=""/usr/local/cuda-10.0/targets/ppc64le-linux/lib"" \. NCCL_HDR_PATH=""/usr/local/cuda-10.0/targets/ppc64le-linux/lib/../include"" \. TF_CUDA_COMPUTE_CAPABILITIES=""3.7"" \. TF_CUDA_CLANG=""0"" \. GCC_HOST_COMPILER_PATH=""/opt/at11.0/bin/gcc"" \. CC_OPT_FLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" \. TF_SET_ANDROID_WORKSPACE=0 \. ./configure. # fix build error. vim /opt/at11.0/include/bits/floatn.h. -------------------------------------. #include <features.h>. /* Defined to 1 if the current compiler invocation provides a. floating-poi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:10547,integrability,configur,configure,10547,"4. pip install keras_applications==1.0.6 keras_preprocessing==1.0.5. # download source code. git clone -b r1.12 https://github.com/tensorflow/tensorflow.git tensorflow-1.12. cd tensorflow-1.12. # configure in tensorflow/.tf_configure.bazelrc. PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python"" \. PYTHON_LIB_PATH=""$HOMEPATH/inst/lib/python2.7/site-packages"" \. TF_NEED_IGNITE=""0"" \. TF_ENABLE_XLA=""0"" \. TF_NEED_OPENCL_SYCL=""0"" \. TF_NEED_ROCM=""0"" \. TF_NEED_MPI=""0"" \. TF_NEED_TENSORRT=""0"" \. TF_NEED_CUDA=""1"" \. TF_CUDA_VERSION=""10.0"" \. CUDA_TOOLKIT_PATH=""/usr/local/cuda"" \. TF_CUDNN_VERSION=""7"" \. CUDNN_INSTALL_PATH=""/usr/local/cuda-10.0"" \. TF_NCCL_VERSION=""2"" \. NCCL_INSTALL_PATH=""/usr/local/cuda-10.0/targets/ppc64le-linux/lib"" \. NCCL_HDR_PATH=""/usr/local/cuda-10.0/targets/ppc64le-linux/lib/../include"" \. TF_CUDA_COMPUTE_CAPABILITIES=""3.7"" \. TF_CUDA_CLANG=""0"" \. GCC_HOST_COMPILER_PATH=""/opt/at11.0/bin/gcc"" \. CC_OPT_FLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" \. TF_SET_ANDROID_WORKSPACE=0 \. ./configure. # fix build error. vim /opt/at11.0/include/bits/floatn.h. -------------------------------------. #include <features.h>. /* Defined to 1 if the current compiler invocation provides a. floating-point type with the IEEE 754 binary128 format, and this glibc. includes corresponding *f128 interfaces for it. */. #if defined _ARCH_PWR8 && defined __LITTLE_ENDIAN__ && (_CALL_ELF == 2) \. && defined __FLOAT128__. # define __HAVE_FLOAT128 1. #else. # define __HAVE_FLOAT128 0. #endif. /* add the following block of fix tensorflow build error */. #if CUDART_VERSION. #undef __HAVE_FLOAT128. #define __HAVE_FLOAT128 0. #endif. /* Defined to 1 if __HAVE_FLOAT128 is 1 and the type is ABI-distinct. from the default float, double and long double types in this glibc. */. #if __HAVE_FLOAT128. -------------------------------------. # build. bazel clean. export PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python"". bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --cxxopt=""-D",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:10842,integrability,interfac,interfaces,10842,"N_LIB_PATH=""$HOMEPATH/inst/lib/python2.7/site-packages"" \. TF_NEED_IGNITE=""0"" \. TF_ENABLE_XLA=""0"" \. TF_NEED_OPENCL_SYCL=""0"" \. TF_NEED_ROCM=""0"" \. TF_NEED_MPI=""0"" \. TF_NEED_TENSORRT=""0"" \. TF_NEED_CUDA=""1"" \. TF_CUDA_VERSION=""10.0"" \. CUDA_TOOLKIT_PATH=""/usr/local/cuda"" \. TF_CUDNN_VERSION=""7"" \. CUDNN_INSTALL_PATH=""/usr/local/cuda-10.0"" \. TF_NCCL_VERSION=""2"" \. NCCL_INSTALL_PATH=""/usr/local/cuda-10.0/targets/ppc64le-linux/lib"" \. NCCL_HDR_PATH=""/usr/local/cuda-10.0/targets/ppc64le-linux/lib/../include"" \. TF_CUDA_COMPUTE_CAPABILITIES=""3.7"" \. TF_CUDA_CLANG=""0"" \. GCC_HOST_COMPILER_PATH=""/opt/at11.0/bin/gcc"" \. CC_OPT_FLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" \. TF_SET_ANDROID_WORKSPACE=0 \. ./configure. # fix build error. vim /opt/at11.0/include/bits/floatn.h. -------------------------------------. #include <features.h>. /* Defined to 1 if the current compiler invocation provides a. floating-point type with the IEEE 754 binary128 format, and this glibc. includes corresponding *f128 interfaces for it. */. #if defined _ARCH_PWR8 && defined __LITTLE_ENDIAN__ && (_CALL_ELF == 2) \. && defined __FLOAT128__. # define __HAVE_FLOAT128 1. #else. # define __HAVE_FLOAT128 0. #endif. /* add the following block of fix tensorflow build error */. #if CUDART_VERSION. #undef __HAVE_FLOAT128. #define __HAVE_FLOAT128 0. #endif. /* Defined to 1 if __HAVE_FLOAT128 is 1 and the type is ABI-distinct. from the default float, double and long double types in this glibc. */. #if __HAVE_FLOAT128. -------------------------------------. # build. bazel clean. export PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python"". bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" --action_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH --test_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH --distinct_host_configuration=false. # generate package. bazel-bin/tensorflow/tools/pip_package/build_pip_package $HOMEPATH/tensorflow_package. # install. pip install $HOMEPATH/te",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:12023,integrability,Repositor,Repository,12023,"#endif. /* add the following block of fix tensorflow build error */. #if CUDART_VERSION. #undef __HAVE_FLOAT128. #define __HAVE_FLOAT128 0. #endif. /* Defined to 1 if __HAVE_FLOAT128 is 1 and the type is ABI-distinct. from the default float, double and long double types in this glibc. */. #if __HAVE_FLOAT128. -------------------------------------. # build. bazel clean. export PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python"". bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" --action_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH --test_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH --distinct_host_configuration=false. # generate package. bazel-bin/tensorflow/tools/pip_package/build_pip_package $HOMEPATH/tensorflow_package. # install. pip install $HOMEPATH/tensorflow_package/tensorflow-1.12.0-*.whl. # verification. python -c ""import tensorflow as tf; print(tf.__version__)"". ```. ## CLIF. > Note: CLIF can be built with AT 11.0. Git Repository: [https://github.com/google/clif](https://github.com/google/clif). ```bash. # Prerequisites. cmake --version #3.5+. protoc --version # 3.2.0+ build from source code for both C++ and Python. pip install virtualenv. pip install pyparsing. yum install subversion. yum install ocaml. pip install 'pyparsing>=2.2.0'. pkg-config --libs python # workable. # download source code. cd $HOMEPATH. git clone https://github.com/google/clif.git. cd clif. # set environment. export INSTALL_DIR=""$HOMEPATH/inst"". export CLIFSRC_DIR=""$HOMEPATH/clif"". export LLVM_DIR=""$CLIFSRC_DIR/../clif_backend"". export BUILD_DIR=""$LLVM_DIR/build_matcher"". export PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python"". export PYTHON_LIB_PATH=""$HOMEPATH/inst/lib/python2.7/site-packages"". export PROTOC_PREFIX_PATH=""$(dirname ""$(dirname ""$(which protoc)"")"")"". export CLIF_VIRTUALENV=""$INSTALL_DIR""/clif. export CLIF_PIP=""$CLIF_VIRTUALENV/bin/pip"". virtualenv -p ""$PYTHON_BIN_PATH"" ""$CLIF_VIRTUALENV"". $CLIF_PIP install --upgrade pip. $",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:12135,integrability,version,version,12135,". #define __HAVE_FLOAT128 0. #endif. /* Defined to 1 if __HAVE_FLOAT128 is 1 and the type is ABI-distinct. from the default float, double and long double types in this glibc. */. #if __HAVE_FLOAT128. -------------------------------------. # build. bazel clean. export PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python"". bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" --action_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH --test_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH --distinct_host_configuration=false. # generate package. bazel-bin/tensorflow/tools/pip_package/build_pip_package $HOMEPATH/tensorflow_package. # install. pip install $HOMEPATH/tensorflow_package/tensorflow-1.12.0-*.whl. # verification. python -c ""import tensorflow as tf; print(tf.__version__)"". ```. ## CLIF. > Note: CLIF can be built with AT 11.0. Git Repository: [https://github.com/google/clif](https://github.com/google/clif). ```bash. # Prerequisites. cmake --version #3.5+. protoc --version # 3.2.0+ build from source code for both C++ and Python. pip install virtualenv. pip install pyparsing. yum install subversion. yum install ocaml. pip install 'pyparsing>=2.2.0'. pkg-config --libs python # workable. # download source code. cd $HOMEPATH. git clone https://github.com/google/clif.git. cd clif. # set environment. export INSTALL_DIR=""$HOMEPATH/inst"". export CLIFSRC_DIR=""$HOMEPATH/clif"". export LLVM_DIR=""$CLIFSRC_DIR/../clif_backend"". export BUILD_DIR=""$LLVM_DIR/build_matcher"". export PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python"". export PYTHON_LIB_PATH=""$HOMEPATH/inst/lib/python2.7/site-packages"". export PROTOC_PREFIX_PATH=""$(dirname ""$(dirname ""$(which protoc)"")"")"". export CLIF_VIRTUALENV=""$INSTALL_DIR""/clif. export CLIF_PIP=""$CLIF_VIRTUALENV/bin/pip"". virtualenv -p ""$PYTHON_BIN_PATH"" ""$CLIF_VIRTUALENV"". $CLIF_PIP install --upgrade pip. $CLIF_PIP install --upgrade setuptools. # Checkout LLVM and Clang source trees. mkdir -p $LLVM_DIR. cd $LLVM_DIR",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:12159,integrability,version,version,12159,"8 0. #endif. /* Defined to 1 if __HAVE_FLOAT128 is 1 and the type is ABI-distinct. from the default float, double and long double types in this glibc. */. #if __HAVE_FLOAT128. -------------------------------------. # build. bazel clean. export PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python"". bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" --action_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH --test_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH --distinct_host_configuration=false. # generate package. bazel-bin/tensorflow/tools/pip_package/build_pip_package $HOMEPATH/tensorflow_package. # install. pip install $HOMEPATH/tensorflow_package/tensorflow-1.12.0-*.whl. # verification. python -c ""import tensorflow as tf; print(tf.__version__)"". ```. ## CLIF. > Note: CLIF can be built with AT 11.0. Git Repository: [https://github.com/google/clif](https://github.com/google/clif). ```bash. # Prerequisites. cmake --version #3.5+. protoc --version # 3.2.0+ build from source code for both C++ and Python. pip install virtualenv. pip install pyparsing. yum install subversion. yum install ocaml. pip install 'pyparsing>=2.2.0'. pkg-config --libs python # workable. # download source code. cd $HOMEPATH. git clone https://github.com/google/clif.git. cd clif. # set environment. export INSTALL_DIR=""$HOMEPATH/inst"". export CLIFSRC_DIR=""$HOMEPATH/clif"". export LLVM_DIR=""$CLIFSRC_DIR/../clif_backend"". export BUILD_DIR=""$LLVM_DIR/build_matcher"". export PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python"". export PYTHON_LIB_PATH=""$HOMEPATH/inst/lib/python2.7/site-packages"". export PROTOC_PREFIX_PATH=""$(dirname ""$(dirname ""$(which protoc)"")"")"". export CLIF_VIRTUALENV=""$INSTALL_DIR""/clif. export CLIF_PIP=""$CLIF_VIRTUALENV/bin/pip"". virtualenv -p ""$PYTHON_BIN_PATH"" ""$CLIF_VIRTUALENV"". $CLIF_PIP install --upgrade pip. $CLIF_PIP install --upgrade setuptools. # Checkout LLVM and Clang source trees. mkdir -p $LLVM_DIR. cd $LLVM_DIR. svn co https://llvm.or",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:12283,integrability,sub,subversion,12283,"uble types in this glibc. */. #if __HAVE_FLOAT128. -------------------------------------. # build. bazel clean. export PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python"". bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" --action_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH --test_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH --distinct_host_configuration=false. # generate package. bazel-bin/tensorflow/tools/pip_package/build_pip_package $HOMEPATH/tensorflow_package. # install. pip install $HOMEPATH/tensorflow_package/tensorflow-1.12.0-*.whl. # verification. python -c ""import tensorflow as tf; print(tf.__version__)"". ```. ## CLIF. > Note: CLIF can be built with AT 11.0. Git Repository: [https://github.com/google/clif](https://github.com/google/clif). ```bash. # Prerequisites. cmake --version #3.5+. protoc --version # 3.2.0+ build from source code for both C++ and Python. pip install virtualenv. pip install pyparsing. yum install subversion. yum install ocaml. pip install 'pyparsing>=2.2.0'. pkg-config --libs python # workable. # download source code. cd $HOMEPATH. git clone https://github.com/google/clif.git. cd clif. # set environment. export INSTALL_DIR=""$HOMEPATH/inst"". export CLIFSRC_DIR=""$HOMEPATH/clif"". export LLVM_DIR=""$CLIFSRC_DIR/../clif_backend"". export BUILD_DIR=""$LLVM_DIR/build_matcher"". export PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python"". export PYTHON_LIB_PATH=""$HOMEPATH/inst/lib/python2.7/site-packages"". export PROTOC_PREFIX_PATH=""$(dirname ""$(dirname ""$(which protoc)"")"")"". export CLIF_VIRTUALENV=""$INSTALL_DIR""/clif. export CLIF_PIP=""$CLIF_VIRTUALENV/bin/pip"". virtualenv -p ""$PYTHON_BIN_PATH"" ""$CLIF_VIRTUALENV"". $CLIF_PIP install --upgrade pip. $CLIF_PIP install --upgrade setuptools. # Checkout LLVM and Clang source trees. mkdir -p $LLVM_DIR. cd $LLVM_DIR. svn co https://llvm.org/svn/llvm-project/llvm/trunk@307315 llvm. cd llvm/tools. svn co https://llvm.org/svn/llvm-project/cfe/trunk@307315 clang. ln",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:14265,integrability,wrap,wrapper,14265,"nk@307315 clang. ln -s -f -n $CLIFSRC_DIR/clif clif. # Builds must be done outside of the LLVM tree. mkdir -p $BUILD_DIR. cd $BUILD_DIR. # Note to remove -DLLVM_TARGETS_TO_BUILD=X86. # ""rm CMakeCache.txt"" to remove cmake cache. cmake -DCMAKE_INSTALL_PREFIX=""$CLIF_VIRTUALENV/clang"" \. -DCMAKE_PREFIX_PATH=""$PROTOC_PREFIX_PATH"" \. -DLLVM_INSTALL_TOOLCHAIN_ONLY=true \. -DCMAKE_BUILD_TYPE=Release \. -DLLVM_BUILD_DOCS=false \. -DLLVM_TARGETS_TO_BUILD=PowerPC \. -DPYTHON_INCLUDE_DIR=""$HOMEPATH/inst/include/python2.7"" \. -DPYTHON_LIBRARY=""$HOMEPATH/inst/lib/libpython2.7.so"" \. -DPYTHON_EXECUTABLE=""$HOMEPATH/inst/bin/python"" \. ""$LLVM_DIR/llvm"". make -j20 clif-matcher. # export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH. make -j20 clif_python_utils_proto_util. make -j20 install. ## Get back to the CLIF Python directory and have pip run setup.py. cd ""$CLIFSRC_DIR"". # Grab the python compiled .proto. cp ""$BUILD_DIR/tools/clif/protos/ast_pb2.py"" clif/protos/. # Grab CLIF generated wrapper implementation for proto_util. cp ""$BUILD_DIR/tools/clif/python/utils/proto_util.cc"" clif/python/utils/. cp ""$BUILD_DIR/tools/clif/python/utils/proto_util.h"" clif/python/utils/. cp ""$BUILD_DIR/tools/clif/python/utils/proto_util.init.cc"" clif/python/utils/. # install. export C_INCLUDE_PATH=/home/qilibj/inst/include. export CPLUS_INCLUDE_PATH=/home/qilibj/inst/include. ""$CLIF_PIP"" install . # echo ""SUCCESS - To use pyclif, run $CLIF_VIRTUALENV/bin/pyclif."". python setup.py bdist_wheel. # Note: pyclif should be installed into virtualenv. ""$CLIF_PIP"" install pyclif-0.3-cp27-none-linux_ppc64le.whl. pip install dist/pyclif-0.3-cp27-cp27m-linux_ppc64le.whl. # verify. python -c ""from clif.python.proto import start"". # link for deepvariant. ln -s /home/qilibj/inst/clif /usr/local/. ```. ## Opencv-python 3.4.5.20. Git repository: [https://github.com/skvark/opencv-python](https://github.com/skvark/opencv-python). ```bash. # Checkout repository and submodules. git clone https://github.com/skva",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:15093,integrability,repositor,repository,15093,"tory and have pip run setup.py. cd ""$CLIFSRC_DIR"". # Grab the python compiled .proto. cp ""$BUILD_DIR/tools/clif/protos/ast_pb2.py"" clif/protos/. # Grab CLIF generated wrapper implementation for proto_util. cp ""$BUILD_DIR/tools/clif/python/utils/proto_util.cc"" clif/python/utils/. cp ""$BUILD_DIR/tools/clif/python/utils/proto_util.h"" clif/python/utils/. cp ""$BUILD_DIR/tools/clif/python/utils/proto_util.init.cc"" clif/python/utils/. # install. export C_INCLUDE_PATH=/home/qilibj/inst/include. export CPLUS_INCLUDE_PATH=/home/qilibj/inst/include. ""$CLIF_PIP"" install . # echo ""SUCCESS - To use pyclif, run $CLIF_VIRTUALENV/bin/pyclif."". python setup.py bdist_wheel. # Note: pyclif should be installed into virtualenv. ""$CLIF_PIP"" install pyclif-0.3-cp27-none-linux_ppc64le.whl. pip install dist/pyclif-0.3-cp27-cp27m-linux_ppc64le.whl. # verify. python -c ""from clif.python.proto import start"". # link for deepvariant. ln -s /home/qilibj/inst/clif /usr/local/. ```. ## Opencv-python 3.4.5.20. Git repository: [https://github.com/skvark/opencv-python](https://github.com/skvark/opencv-python). ```bash. # Checkout repository and submodules. git clone https://github.com/skvark/opencv-python.git. cd opencv-python/. # fetch the tags to your local repository. git fetch --all --tags --prune. # check out tag 3.4.5.20. git checkout tags/20. # load submoduel. git submodule update --init --recursive. # Dependency. pip install pyparsing. yum install qt-devel. # Build. python setup.py bdist_wheel. # Insatll. pip install dist/opencv_python-3.4.5.20-cp27-cp27mu-linux_ppc64le.whl. # Verify in a new session. python -c ""import cv2"". ```. ## DV Prerequisite. ```bash. ####################################################################. # misc setup. ####################################################################. # development packages. yum install python2-pkgconfig zip zlib-devel unzip curl -y. # python packages. yum install python-devel python-pip python-wheel -y. ################################",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:15209,integrability,repositor,repository,15209,"os/ast_pb2.py"" clif/protos/. # Grab CLIF generated wrapper implementation for proto_util. cp ""$BUILD_DIR/tools/clif/python/utils/proto_util.cc"" clif/python/utils/. cp ""$BUILD_DIR/tools/clif/python/utils/proto_util.h"" clif/python/utils/. cp ""$BUILD_DIR/tools/clif/python/utils/proto_util.init.cc"" clif/python/utils/. # install. export C_INCLUDE_PATH=/home/qilibj/inst/include. export CPLUS_INCLUDE_PATH=/home/qilibj/inst/include. ""$CLIF_PIP"" install . # echo ""SUCCESS - To use pyclif, run $CLIF_VIRTUALENV/bin/pyclif."". python setup.py bdist_wheel. # Note: pyclif should be installed into virtualenv. ""$CLIF_PIP"" install pyclif-0.3-cp27-none-linux_ppc64le.whl. pip install dist/pyclif-0.3-cp27-cp27m-linux_ppc64le.whl. # verify. python -c ""from clif.python.proto import start"". # link for deepvariant. ln -s /home/qilibj/inst/clif /usr/local/. ```. ## Opencv-python 3.4.5.20. Git repository: [https://github.com/skvark/opencv-python](https://github.com/skvark/opencv-python). ```bash. # Checkout repository and submodules. git clone https://github.com/skvark/opencv-python.git. cd opencv-python/. # fetch the tags to your local repository. git fetch --all --tags --prune. # check out tag 3.4.5.20. git checkout tags/20. # load submoduel. git submodule update --init --recursive. # Dependency. pip install pyparsing. yum install qt-devel. # Build. python setup.py bdist_wheel. # Insatll. pip install dist/opencv_python-3.4.5.20-cp27-cp27mu-linux_ppc64le.whl. # Verify in a new session. python -c ""import cv2"". ```. ## DV Prerequisite. ```bash. ####################################################################. # misc setup. ####################################################################. # development packages. yum install python2-pkgconfig zip zlib-devel unzip curl -y. # python packages. yum install python-devel python-pip python-wheel -y. ####################################################################. # python packages. ###########################################################",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:15224,integrability,sub,submodules,15224,"clif/protos/. # Grab CLIF generated wrapper implementation for proto_util. cp ""$BUILD_DIR/tools/clif/python/utils/proto_util.cc"" clif/python/utils/. cp ""$BUILD_DIR/tools/clif/python/utils/proto_util.h"" clif/python/utils/. cp ""$BUILD_DIR/tools/clif/python/utils/proto_util.init.cc"" clif/python/utils/. # install. export C_INCLUDE_PATH=/home/qilibj/inst/include. export CPLUS_INCLUDE_PATH=/home/qilibj/inst/include. ""$CLIF_PIP"" install . # echo ""SUCCESS - To use pyclif, run $CLIF_VIRTUALENV/bin/pyclif."". python setup.py bdist_wheel. # Note: pyclif should be installed into virtualenv. ""$CLIF_PIP"" install pyclif-0.3-cp27-none-linux_ppc64le.whl. pip install dist/pyclif-0.3-cp27-cp27m-linux_ppc64le.whl. # verify. python -c ""from clif.python.proto import start"". # link for deepvariant. ln -s /home/qilibj/inst/clif /usr/local/. ```. ## Opencv-python 3.4.5.20. Git repository: [https://github.com/skvark/opencv-python](https://github.com/skvark/opencv-python). ```bash. # Checkout repository and submodules. git clone https://github.com/skvark/opencv-python.git. cd opencv-python/. # fetch the tags to your local repository. git fetch --all --tags --prune. # check out tag 3.4.5.20. git checkout tags/20. # load submoduel. git submodule update --init --recursive. # Dependency. pip install pyparsing. yum install qt-devel. # Build. python setup.py bdist_wheel. # Insatll. pip install dist/opencv_python-3.4.5.20-cp27-cp27mu-linux_ppc64le.whl. # Verify in a new session. python -c ""import cv2"". ```. ## DV Prerequisite. ```bash. ####################################################################. # misc setup. ####################################################################. # development packages. yum install python2-pkgconfig zip zlib-devel unzip curl -y. # python packages. yum install python-devel python-pip python-wheel -y. ####################################################################. # python packages. ####################################################################. # py",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:15341,integrability,repositor,repository,15341,"to_util.cc"" clif/python/utils/. cp ""$BUILD_DIR/tools/clif/python/utils/proto_util.h"" clif/python/utils/. cp ""$BUILD_DIR/tools/clif/python/utils/proto_util.init.cc"" clif/python/utils/. # install. export C_INCLUDE_PATH=/home/qilibj/inst/include. export CPLUS_INCLUDE_PATH=/home/qilibj/inst/include. ""$CLIF_PIP"" install . # echo ""SUCCESS - To use pyclif, run $CLIF_VIRTUALENV/bin/pyclif."". python setup.py bdist_wheel. # Note: pyclif should be installed into virtualenv. ""$CLIF_PIP"" install pyclif-0.3-cp27-none-linux_ppc64le.whl. pip install dist/pyclif-0.3-cp27-cp27m-linux_ppc64le.whl. # verify. python -c ""from clif.python.proto import start"". # link for deepvariant. ln -s /home/qilibj/inst/clif /usr/local/. ```. ## Opencv-python 3.4.5.20. Git repository: [https://github.com/skvark/opencv-python](https://github.com/skvark/opencv-python). ```bash. # Checkout repository and submodules. git clone https://github.com/skvark/opencv-python.git. cd opencv-python/. # fetch the tags to your local repository. git fetch --all --tags --prune. # check out tag 3.4.5.20. git checkout tags/20. # load submoduel. git submodule update --init --recursive. # Dependency. pip install pyparsing. yum install qt-devel. # Build. python setup.py bdist_wheel. # Insatll. pip install dist/opencv_python-3.4.5.20-cp27-cp27mu-linux_ppc64le.whl. # Verify in a new session. python -c ""import cv2"". ```. ## DV Prerequisite. ```bash. ####################################################################. # misc setup. ####################################################################. # development packages. yum install python2-pkgconfig zip zlib-devel unzip curl -y. # python packages. yum install python-devel python-pip python-wheel -y. ####################################################################. # python packages. ####################################################################. # python 2 required. echo ""$(python --version)"". echo ""$(pip --version)"". # Install python packages. pip install contextl",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:15440,integrability,sub,submoduel,15440,"ils/. cp ""$BUILD_DIR/tools/clif/python/utils/proto_util.init.cc"" clif/python/utils/. # install. export C_INCLUDE_PATH=/home/qilibj/inst/include. export CPLUS_INCLUDE_PATH=/home/qilibj/inst/include. ""$CLIF_PIP"" install . # echo ""SUCCESS - To use pyclif, run $CLIF_VIRTUALENV/bin/pyclif."". python setup.py bdist_wheel. # Note: pyclif should be installed into virtualenv. ""$CLIF_PIP"" install pyclif-0.3-cp27-none-linux_ppc64le.whl. pip install dist/pyclif-0.3-cp27-cp27m-linux_ppc64le.whl. # verify. python -c ""from clif.python.proto import start"". # link for deepvariant. ln -s /home/qilibj/inst/clif /usr/local/. ```. ## Opencv-python 3.4.5.20. Git repository: [https://github.com/skvark/opencv-python](https://github.com/skvark/opencv-python). ```bash. # Checkout repository and submodules. git clone https://github.com/skvark/opencv-python.git. cd opencv-python/. # fetch the tags to your local repository. git fetch --all --tags --prune. # check out tag 3.4.5.20. git checkout tags/20. # load submoduel. git submodule update --init --recursive. # Dependency. pip install pyparsing. yum install qt-devel. # Build. python setup.py bdist_wheel. # Insatll. pip install dist/opencv_python-3.4.5.20-cp27-cp27mu-linux_ppc64le.whl. # Verify in a new session. python -c ""import cv2"". ```. ## DV Prerequisite. ```bash. ####################################################################. # misc setup. ####################################################################. # development packages. yum install python2-pkgconfig zip zlib-devel unzip curl -y. # python packages. yum install python-devel python-pip python-wheel -y. ####################################################################. # python packages. ####################################################################. # python 2 required. echo ""$(python --version)"". echo ""$(pip --version)"". # Install python packages. pip install contextlib2. pip install enum34. pip install intervaltree. pip install 'mock>=2.0.0'. # pip install 'numpy=",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:15455,integrability,sub,submodule,15455,"D_DIR/tools/clif/python/utils/proto_util.init.cc"" clif/python/utils/. # install. export C_INCLUDE_PATH=/home/qilibj/inst/include. export CPLUS_INCLUDE_PATH=/home/qilibj/inst/include. ""$CLIF_PIP"" install . # echo ""SUCCESS - To use pyclif, run $CLIF_VIRTUALENV/bin/pyclif."". python setup.py bdist_wheel. # Note: pyclif should be installed into virtualenv. ""$CLIF_PIP"" install pyclif-0.3-cp27-none-linux_ppc64le.whl. pip install dist/pyclif-0.3-cp27-cp27m-linux_ppc64le.whl. # verify. python -c ""from clif.python.proto import start"". # link for deepvariant. ln -s /home/qilibj/inst/clif /usr/local/. ```. ## Opencv-python 3.4.5.20. Git repository: [https://github.com/skvark/opencv-python](https://github.com/skvark/opencv-python). ```bash. # Checkout repository and submodules. git clone https://github.com/skvark/opencv-python.git. cd opencv-python/. # fetch the tags to your local repository. git fetch --all --tags --prune. # check out tag 3.4.5.20. git checkout tags/20. # load submoduel. git submodule update --init --recursive. # Dependency. pip install pyparsing. yum install qt-devel. # Build. python setup.py bdist_wheel. # Insatll. pip install dist/opencv_python-3.4.5.20-cp27-cp27mu-linux_ppc64le.whl. # Verify in a new session. python -c ""import cv2"". ```. ## DV Prerequisite. ```bash. ####################################################################. # misc setup. ####################################################################. # development packages. yum install python2-pkgconfig zip zlib-devel unzip curl -y. # python packages. yum install python-devel python-pip python-wheel -y. ####################################################################. # python packages. ####################################################################. # python 2 required. echo ""$(python --version)"". echo ""$(pip --version)"". # Install python packages. pip install contextlib2. pip install enum34. pip install intervaltree. pip install 'mock>=2.0.0'. # pip install 'numpy==1.14' => skip ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:15494,integrability,Depend,Dependency,15494,"l.init.cc"" clif/python/utils/. # install. export C_INCLUDE_PATH=/home/qilibj/inst/include. export CPLUS_INCLUDE_PATH=/home/qilibj/inst/include. ""$CLIF_PIP"" install . # echo ""SUCCESS - To use pyclif, run $CLIF_VIRTUALENV/bin/pyclif."". python setup.py bdist_wheel. # Note: pyclif should be installed into virtualenv. ""$CLIF_PIP"" install pyclif-0.3-cp27-none-linux_ppc64le.whl. pip install dist/pyclif-0.3-cp27-cp27m-linux_ppc64le.whl. # verify. python -c ""from clif.python.proto import start"". # link for deepvariant. ln -s /home/qilibj/inst/clif /usr/local/. ```. ## Opencv-python 3.4.5.20. Git repository: [https://github.com/skvark/opencv-python](https://github.com/skvark/opencv-python). ```bash. # Checkout repository and submodules. git clone https://github.com/skvark/opencv-python.git. cd opencv-python/. # fetch the tags to your local repository. git fetch --all --tags --prune. # check out tag 3.4.5.20. git checkout tags/20. # load submoduel. git submodule update --init --recursive. # Dependency. pip install pyparsing. yum install qt-devel. # Build. python setup.py bdist_wheel. # Insatll. pip install dist/opencv_python-3.4.5.20-cp27-cp27mu-linux_ppc64le.whl. # Verify in a new session. python -c ""import cv2"". ```. ## DV Prerequisite. ```bash. ####################################################################. # misc setup. ####################################################################. # development packages. yum install python2-pkgconfig zip zlib-devel unzip curl -y. # python packages. yum install python-devel python-pip python-wheel -y. ####################################################################. # python packages. ####################################################################. # python 2 required. echo ""$(python --version)"". echo ""$(pip --version)"". # Install python packages. pip install contextlib2. pip install enum34. pip install intervaltree. pip install 'mock>=2.0.0'. # pip install 'numpy==1.14' => skip as installed in TF. pip install 'reques",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:16263,integrability,version,version,16263,"vark/opencv-python.git. cd opencv-python/. # fetch the tags to your local repository. git fetch --all --tags --prune. # check out tag 3.4.5.20. git checkout tags/20. # load submoduel. git submodule update --init --recursive. # Dependency. pip install pyparsing. yum install qt-devel. # Build. python setup.py bdist_wheel. # Insatll. pip install dist/opencv_python-3.4.5.20-cp27-cp27mu-linux_ppc64le.whl. # Verify in a new session. python -c ""import cv2"". ```. ## DV Prerequisite. ```bash. ####################################################################. # misc setup. ####################################################################. # development packages. yum install python2-pkgconfig zip zlib-devel unzip curl -y. # python packages. yum install python-devel python-pip python-wheel -y. ####################################################################. # python packages. ####################################################################. # python 2 required. echo ""$(python --version)"". echo ""$(pip --version)"". # Install python packages. pip install contextlib2. pip install enum34. pip install intervaltree. pip install 'mock>=2.0.0'. # pip install 'numpy==1.14' => skip as installed in TF. pip install 'requests>=2.18'. # pip install 'scipy==1.0' => skip as installed in TF. pip install 'oauth2client>=4.0.0'. pip install 'crcmod>=1.7'. pip install six. pip install sklearn. pip install pandas. pip install psutil. pip install --upgrade google-api-python-client. ####################################################################. # depend on opencv-python wheel - build from source. ####################################################################. pip install 'tensor2tensor>=1.9.0'. ####################################################################. # depend on - TensorFlow - 1.12 build from source. ####################################################################. pip install tensorflow-1.12.0-cp27-cp27mu-linux_ppc64le.whl. #################################",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:16288,integrability,version,version,16288,"d opencv-python/. # fetch the tags to your local repository. git fetch --all --tags --prune. # check out tag 3.4.5.20. git checkout tags/20. # load submoduel. git submodule update --init --recursive. # Dependency. pip install pyparsing. yum install qt-devel. # Build. python setup.py bdist_wheel. # Insatll. pip install dist/opencv_python-3.4.5.20-cp27-cp27mu-linux_ppc64le.whl. # Verify in a new session. python -c ""import cv2"". ```. ## DV Prerequisite. ```bash. ####################################################################. # misc setup. ####################################################################. # development packages. yum install python2-pkgconfig zip zlib-devel unzip curl -y. # python packages. yum install python-devel python-pip python-wheel -y. ####################################################################. # python packages. ####################################################################. # python 2 required. echo ""$(python --version)"". echo ""$(pip --version)"". # Install python packages. pip install contextlib2. pip install enum34. pip install intervaltree. pip install 'mock>=2.0.0'. # pip install 'numpy==1.14' => skip as installed in TF. pip install 'requests>=2.18'. # pip install 'scipy==1.0' => skip as installed in TF. pip install 'oauth2client>=4.0.0'. pip install 'crcmod>=1.7'. pip install six. pip install sklearn. pip install pandas. pip install psutil. pip install --upgrade google-api-python-client. ####################################################################. # depend on opencv-python wheel - build from source. ####################################################################. pip install 'tensor2tensor>=1.9.0'. ####################################################################. # depend on - TensorFlow - 1.12 build from source. ####################################################################. pip install tensorflow-1.12.0-cp27-cp27mu-linux_ppc64le.whl. ##########################################################",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:16734,integrability,api,api-python-client,16734,"te. ```bash. ####################################################################. # misc setup. ####################################################################. # development packages. yum install python2-pkgconfig zip zlib-devel unzip curl -y. # python packages. yum install python-devel python-pip python-wheel -y. ####################################################################. # python packages. ####################################################################. # python 2 required. echo ""$(python --version)"". echo ""$(pip --version)"". # Install python packages. pip install contextlib2. pip install enum34. pip install intervaltree. pip install 'mock>=2.0.0'. # pip install 'numpy==1.14' => skip as installed in TF. pip install 'requests>=2.18'. # pip install 'scipy==1.0' => skip as installed in TF. pip install 'oauth2client>=4.0.0'. pip install 'crcmod>=1.7'. pip install six. pip install sklearn. pip install pandas. pip install psutil. pip install --upgrade google-api-python-client. ####################################################################. # depend on opencv-python wheel - build from source. ####################################################################. pip install 'tensor2tensor>=1.9.0'. ####################################################################. # depend on - TensorFlow - 1.12 build from source. ####################################################################. pip install tensorflow-1.12.0-cp27-cp27mu-linux_ppc64le.whl. ####################################################################. # Misc dependencies. ####################################################################. yum install openssl-devel curl-devel zlib-devel bzip2-devel xz-devel. yum install boost-devel. ```. ## DeepVariant. Git Repository [https://github.com/google/deepvariant](https://github.com/google/deepvariant). ```bash. # check out source code. git clone https://github.com/google/deepvariant.git. cd deepvariant. # fetch all tags. git fetch --all ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:16825,integrability,depend,depend,16825,"misc setup. ####################################################################. # development packages. yum install python2-pkgconfig zip zlib-devel unzip curl -y. # python packages. yum install python-devel python-pip python-wheel -y. ####################################################################. # python packages. ####################################################################. # python 2 required. echo ""$(python --version)"". echo ""$(pip --version)"". # Install python packages. pip install contextlib2. pip install enum34. pip install intervaltree. pip install 'mock>=2.0.0'. # pip install 'numpy==1.14' => skip as installed in TF. pip install 'requests>=2.18'. # pip install 'scipy==1.0' => skip as installed in TF. pip install 'oauth2client>=4.0.0'. pip install 'crcmod>=1.7'. pip install six. pip install sklearn. pip install pandas. pip install psutil. pip install --upgrade google-api-python-client. ####################################################################. # depend on opencv-python wheel - build from source. ####################################################################. pip install 'tensor2tensor>=1.9.0'. ####################################################################. # depend on - TensorFlow - 1.12 build from source. ####################################################################. pip install tensorflow-1.12.0-cp27-cp27mu-linux_ppc64le.whl. ####################################################################. # Misc dependencies. ####################################################################. yum install openssl-devel curl-devel zlib-devel bzip2-devel xz-devel. yum install boost-devel. ```. ## DeepVariant. Git Repository [https://github.com/google/deepvariant](https://github.com/google/deepvariant). ```bash. # check out source code. git clone https://github.com/google/deepvariant.git. cd deepvariant. # fetch all tags. git fetch --all --tags --prune. # check out tag. git checkout tags/v0.7.2. # Edit ctx.action with use",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:17054,integrability,depend,depend,17054,"heel -y. ####################################################################. # python packages. ####################################################################. # python 2 required. echo ""$(python --version)"". echo ""$(pip --version)"". # Install python packages. pip install contextlib2. pip install enum34. pip install intervaltree. pip install 'mock>=2.0.0'. # pip install 'numpy==1.14' => skip as installed in TF. pip install 'requests>=2.18'. # pip install 'scipy==1.0' => skip as installed in TF. pip install 'oauth2client>=4.0.0'. pip install 'crcmod>=1.7'. pip install six. pip install sklearn. pip install pandas. pip install psutil. pip install --upgrade google-api-python-client. ####################################################################. # depend on opencv-python wheel - build from source. ####################################################################. pip install 'tensor2tensor>=1.9.0'. ####################################################################. # depend on - TensorFlow - 1.12 build from source. ####################################################################. pip install tensorflow-1.12.0-cp27-cp27mu-linux_ppc64le.whl. ####################################################################. # Misc dependencies. ####################################################################. yum install openssl-devel curl-devel zlib-devel bzip2-devel xz-devel. yum install boost-devel. ```. ## DeepVariant. Git Repository [https://github.com/google/deepvariant](https://github.com/google/deepvariant). ```bash. # check out source code. git clone https://github.com/google/deepvariant.git. cd deepvariant. # fetch all tags. git fetch --all --tags --prune. # check out tag. git checkout tags/v0.7.2. # Edit ctx.action with use_default_shell_env=True. vim ./third_party/clif.bzl. # Build and test. export LD_LIBRARY_PATH=/home/qilibj/inst/lib:/qilibj/inst/lib/python2.7/site-packages:$LD_LIBRARY_PATH. export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:17311,integrability,depend,dependencies,17311,"ackages. pip install contextlib2. pip install enum34. pip install intervaltree. pip install 'mock>=2.0.0'. # pip install 'numpy==1.14' => skip as installed in TF. pip install 'requests>=2.18'. # pip install 'scipy==1.0' => skip as installed in TF. pip install 'oauth2client>=4.0.0'. pip install 'crcmod>=1.7'. pip install six. pip install sklearn. pip install pandas. pip install psutil. pip install --upgrade google-api-python-client. ####################################################################. # depend on opencv-python wheel - build from source. ####################################################################. pip install 'tensor2tensor>=1.9.0'. ####################################################################. # depend on - TensorFlow - 1.12 build from source. ####################################################################. pip install tensorflow-1.12.0-cp27-cp27mu-linux_ppc64le.whl. ####################################################################. # Misc dependencies. ####################################################################. yum install openssl-devel curl-devel zlib-devel bzip2-devel xz-devel. yum install boost-devel. ```. ## DeepVariant. Git Repository [https://github.com/google/deepvariant](https://github.com/google/deepvariant). ```bash. # check out source code. git clone https://github.com/google/deepvariant.git. cd deepvariant. # fetch all tags. git fetch --all --tags --prune. # check out tag. git checkout tags/v0.7.2. # Edit ctx.action with use_default_shell_env=True. vim ./third_party/clif.bzl. # Build and test. export LD_LIBRARY_PATH=/home/qilibj/inst/lib:/qilibj/inst/lib/python2.7/site-packages:$LD_LIBRARY_PATH. export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. export PYTHON_BIN_PATH=/home/qilibj/inst/bin/python. export PYTHON_LIB_PATH=/home/qilibj/inst/lib/python2.7/site-packages. export BAZEL_PYTHON=/home/qilibj/inst/bin/python. export DV_COPT_FLAGS=""--copt=-maltivec --copt=-Wno-sign-compare --copt=-Wno-",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:17515,integrability,Repositor,Repository,17515,"all 'scipy==1.0' => skip as installed in TF. pip install 'oauth2client>=4.0.0'. pip install 'crcmod>=1.7'. pip install six. pip install sklearn. pip install pandas. pip install psutil. pip install --upgrade google-api-python-client. ####################################################################. # depend on opencv-python wheel - build from source. ####################################################################. pip install 'tensor2tensor>=1.9.0'. ####################################################################. # depend on - TensorFlow - 1.12 build from source. ####################################################################. pip install tensorflow-1.12.0-cp27-cp27mu-linux_ppc64le.whl. ####################################################################. # Misc dependencies. ####################################################################. yum install openssl-devel curl-devel zlib-devel bzip2-devel xz-devel. yum install boost-devel. ```. ## DeepVariant. Git Repository [https://github.com/google/deepvariant](https://github.com/google/deepvariant). ```bash. # check out source code. git clone https://github.com/google/deepvariant.git. cd deepvariant. # fetch all tags. git fetch --all --tags --prune. # check out tag. git checkout tags/v0.7.2. # Edit ctx.action with use_default_shell_env=True. vim ./third_party/clif.bzl. # Build and test. export LD_LIBRARY_PATH=/home/qilibj/inst/lib:/qilibj/inst/lib/python2.7/site-packages:$LD_LIBRARY_PATH. export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. export PYTHON_BIN_PATH=/home/qilibj/inst/bin/python. export PYTHON_LIB_PATH=/home/qilibj/inst/lib/python2.7/site-packages. export BAZEL_PYTHON=/home/qilibj/inst/bin/python. export DV_COPT_FLAGS=""--copt=-maltivec --copt=-Wno-sign-compare --copt=-Wno-write-strings --copt=-DNO_WARN_X86_INTRINSICS --cxxopt=-std=gnu++11"". # export DV_COPT_FLAGS=""--copt=-maltivec --copt=-Wno-sign-compare --copt=-Wno-write-strings --copt=-DNO_WARN_X86_INTRINSICS --cxxopt=",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:19624,integrability,messag,message,19624,"named google.protobuf"" by install protobuf from source. bazel clean. bazel shutdown. bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" deepvariant/... \. --action_env=LD_LIBRARY_PATH --test_env=LD_LIBRARY_PATH \. --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH \. --action_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH --test_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH \. --action_env=PYTHON_LIB_PATH=$PYTHON_LIB_PATH --test_env=PYTHON_LIB_PATH=$PYTHON_LIB_PATH \. --action_env=BAZEL_PYTHON=$BAZEL_PYTHON --test_env=BAZEL_PYTHON=$BAZEL_PYTHON >& output.log 2>&1 &. bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" deepvariant:gpu_tests --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH. # for CPU only. bazel test -c opt ${DV_COPT_FLAGS} ""$@"" deepvariant/... # build binary. bazel build -c opt ${DV_COPT_FLAGS} ""$@"" deepvariant:binaries --build_python_zip --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH. echo 'Expect a usage message:'. (bazel-bin/deepvariant/call_variants --help || : ) | grep '/call_variants.py:'. bazel build :licenses_zip --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH. ```. ## Fix DV Error. ```bash. ################################################################################. # bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@""//deepvariant:resources_test. # use lscpu to show the actual CPU number. ################################################################################. python -c ""import multiprocessing; print(multiprocessing.cpu_count())"" #160. python -c ""import psutil;print(p/sutil.cpu_count. ())"" #160. vim deepvariant/resources.py. --------------------------------. def _get_cpu_count():. """"""Gets the number of physical cores in this machine. Returns:. int >= 1 if the call to get the cpu_count succeeded, or 0 if not. """""". # return psutil.cpu_count(logical=False) or 0 ==> comment. return 20. --------------------------------. vim deepvariant/resourc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:20794,integrability,depend,depending,20794,"$PYTHONPATH. ```. ## Fix DV Error. ```bash. ################################################################################. # bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@""//deepvariant:resources_test. # use lscpu to show the actual CPU number. ################################################################################. python -c ""import multiprocessing; print(multiprocessing.cpu_count())"" #160. python -c ""import psutil;print(p/sutil.cpu_count. ())"" #160. vim deepvariant/resources.py. --------------------------------. def _get_cpu_count():. """"""Gets the number of physical cores in this machine. Returns:. int >= 1 if the call to get the cpu_count succeeded, or 0 if not. """""". # return psutil.cpu_count(logical=False) or 0 ==> comment. return 20. --------------------------------. vim deepvariant/resources_test.py. --------------------------------. def test_metrics_is_ok_when_cpu_count_returns_none(self):. # Some psutil functions, such as cpu_freq(), can return None depending on. # the environment; make sure we don't crash when that occurs. with mock.patch.object(resources.psutil, 'cpu_count', return_value=None):. with resources.ResourceMonitor() as monitor:. #self.assertEqual(monitor.metrics().physical_core_count, 0) ==> comment. self.assertEqual(monitor.metrics().physical_core_count, 20). --------------------------------. ##########################################################################. # //deepvariant/realigner/allele_count_linear:generate_trained_model_test. # ImportError: /root/.local/lib/python2.7/site-packages/sklearn/__check_build/_check_build.so: undefined symbol: PyUnicodeUCS4_DecodeUTF8. ##########################################################################. # reinstall numpy, scipy, Cpython, scikit-learn to fix with AT built Python. python -c ""import numpy"" # prequests of TF 1.12.0. python -c ""import scipy"" # prequests of TF 1.12.0. pip install Cython --force-reinstall --no-deos. pip install scikit-learn --force-reinstall --no",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:22819,integrability,API,API,22819,"urces.ResourceMonitor() as monitor:. #self.assertEqual(monitor.metrics().physical_core_count, 0) ==> comment. self.assertEqual(monitor.metrics().physical_core_count, 20). --------------------------------. ##########################################################################. # //deepvariant/realigner/allele_count_linear:generate_trained_model_test. # ImportError: /root/.local/lib/python2.7/site-packages/sklearn/__check_build/_check_build.so: undefined symbol: PyUnicodeUCS4_DecodeUTF8. ##########################################################################. # reinstall numpy, scipy, Cpython, scikit-learn to fix with AT built Python. python -c ""import numpy"" # prequests of TF 1.12.0. python -c ""import scipy"" # prequests of TF 1.12.0. pip install Cython --force-reinstall --no-deos. pip install scikit-learn --force-reinstall --no-deos. # build from source. wget https://github.com/scikit-learn/scikit-learn/archive/0.20.2.tar.gz. tar zxvf 0.20.2.tar.gz. cd scikit-learn-0.20.2. python setup.py bdist_wheel. # verify. python -c ""from sklearn.externals import joblib"". ##########################################################################. # //deepvariant/labeler:haplotype_labeler_test. # bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@""//deepvariant/labeler:haplotype_labeler_test --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH. ##########################################################################. # fail due to mock data, open an issue in github. https://github.com/google/deepvariant/issues/154. ##########################################################################. # //deepvariant:make_examples_test. # bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@""//deepvariant:make_examples_test --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH. # internvaltree v3 has some API changes with v2. ##########################################################################. pip install 'intervaltree==2.1.0'. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:1283,interoperability,protocol,protocolbuffers,1283,"H/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # AT 11.0 environment. source /etc/profile. module load at11.0. export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # python2 and pip environment. export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH. export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. export PYTHON_BIN_PATH=/home/qilibj/inst/bin/python. export PYTHON_LIB_PATH=/home/qilibj/inst/lib/python2.7/site-packages. ```. ## Cmake 3.13.3. Build and install doc: [https://cmake.org/install/](https://cmake.org/install/). >Note: only can be built with system gcc, AT11.0 will cause build failure. ```bash. # download source code. wget https://github.com/Kitware/CMake/releases/download/v3.13.3/cmake-3.13.3.tar.gz. tar -zxvf cmake-3.13.3.tar.gz. cd cmake-3.13.3. # build scirpt. ./bootstrap. make -j20. make -j20 install. export PATH=/usr/local/bin:$PATH. ```. ## Protobuf 3.6.1 C++ static. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from /usr/bin/gcc, static for bazel. ```bash. # download source code. wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --disable-shared --enable-static. make clean. make -j20. make -j20 check # check if protobuf build is good. # install. make install. sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:1354,interoperability,protocol,protocolbuffers,1354,"/sbin:/usr/bin:/root/bin. # AT 11.0 environment. source /etc/profile. module load at11.0. export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # python2 and pip environment. export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH. export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. export PYTHON_BIN_PATH=/home/qilibj/inst/bin/python. export PYTHON_LIB_PATH=/home/qilibj/inst/lib/python2.7/site-packages. ```. ## Cmake 3.13.3. Build and install doc: [https://cmake.org/install/](https://cmake.org/install/). >Note: only can be built with system gcc, AT11.0 will cause build failure. ```bash. # download source code. wget https://github.com/Kitware/CMake/releases/download/v3.13.3/cmake-3.13.3.tar.gz. tar -zxvf cmake-3.13.3.tar.gz. cd cmake-3.13.3. # build scirpt. ./bootstrap. make -j20. make -j20 install. export PATH=/usr/local/bin:$PATH. ```. ## Protobuf 3.6.1 C++ static. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from /usr/bin/gcc, static for bazel. ```bash. # download source code. wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --disable-shared --enable-static. make clean. make -j20. make -j20 check # check if protobuf build is good. # install. make install. sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig. #cd .. # verify. echo ""$(which protoc)"". echo ""$(protoc --ve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:1540,interoperability,protocol,protocolbuffers,1540,"RARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH. export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. export PYTHON_BIN_PATH=/home/qilibj/inst/bin/python. export PYTHON_LIB_PATH=/home/qilibj/inst/lib/python2.7/site-packages. ```. ## Cmake 3.13.3. Build and install doc: [https://cmake.org/install/](https://cmake.org/install/). >Note: only can be built with system gcc, AT11.0 will cause build failure. ```bash. # download source code. wget https://github.com/Kitware/CMake/releases/download/v3.13.3/cmake-3.13.3.tar.gz. tar -zxvf cmake-3.13.3.tar.gz. cd cmake-3.13.3. # build scirpt. ./bootstrap. make -j20. make -j20 install. export PATH=/usr/local/bin:$PATH. ```. ## Protobuf 3.6.1 C++ static. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from /usr/bin/gcc, static for bazel. ```bash. # download source code. wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --disable-shared --enable-static. make clean. make -j20. make -j20 check # check if protobuf build is good. # install. make install. sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig. #cd .. # verify. echo ""$(which protoc)"". echo ""$(protoc --version)"". ```. ## Bazel 0.15.0. Build instructions: [https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel](https://docs.bazel.build/versions/master/install",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:1920,interoperability,share,share,1920,"em gcc, AT11.0 will cause build failure. ```bash. # download source code. wget https://github.com/Kitware/CMake/releases/download/v3.13.3/cmake-3.13.3.tar.gz. tar -zxvf cmake-3.13.3.tar.gz. cd cmake-3.13.3. # build scirpt. ./bootstrap. make -j20. make -j20 install. export PATH=/usr/local/bin:$PATH. ```. ## Protobuf 3.6.1 C++ static. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from /usr/bin/gcc, static for bazel. ```bash. # download source code. wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --disable-shared --enable-static. make clean. make -j20. make -j20 check # check if protobuf build is good. # install. make install. sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig. #cd .. # verify. echo ""$(which protoc)"". echo ""$(protoc --version)"". ```. ## Bazel 0.15.0. Build instructions: [https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel](https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel). Edit crosstool: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). > Note: Bazel 0.15.0 should be built from /usr/bin/gcc. ```bash. # download source code. wget https://github.com/bazelbuild/bazel/releases/download/0.15.0/bazel-0.15.0-dist.zip. mkdir bazel-0.15.0. unzip ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:2105,interoperability,share,shared,2105,".gz. cd cmake-3.13.3. # build scirpt. ./bootstrap. make -j20. make -j20 install. export PATH=/usr/local/bin:$PATH. ```. ## Protobuf 3.6.1 C++ static. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from /usr/bin/gcc, static for bazel. ```bash. # download source code. wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --disable-shared --enable-static. make clean. make -j20. make -j20 check # check if protobuf build is good. # install. make install. sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig. #cd .. # verify. echo ""$(which protoc)"". echo ""$(protoc --version)"". ```. ## Bazel 0.15.0. Build instructions: [https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel](https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel). Edit crosstool: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). > Note: Bazel 0.15.0 should be built from /usr/bin/gcc. ```bash. # download source code. wget https://github.com/bazelbuild/bazel/releases/download/0.15.0/bazel-0.15.0-dist.zip. mkdir bazel-0.15.0. unzip -n bazel-0.15.0-dist.zip -d ./bazel-0.15.0. cd bazel-0.15.0. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export JAVA_HOME=/usr/lib/jvm/java-1.8.0. export PATH=$HOMEP",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:2252,interoperability,share,shared,2252,"c. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from /usr/bin/gcc, static for bazel. ```bash. # download source code. wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz. tar -zxvf protobuf-all-3.6.1.tar.gz. cd protobuf-3.6.1/. # clean static protobuf build. make uninstall. make distclean. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++. # install under /usr instead of /usr/local. CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --disable-shared --enable-static. make clean. make -j20. make -j20 check # check if protobuf build is good. # install. make install. sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig. #cd .. # verify. echo ""$(which protoc)"". echo ""$(protoc --version)"". ```. ## Bazel 0.15.0. Build instructions: [https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel](https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel). Edit crosstool: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). > Note: Bazel 0.15.0 should be built from /usr/bin/gcc. ```bash. # download source code. wget https://github.com/bazelbuild/bazel/releases/download/0.15.0/bazel-0.15.0-dist.zip. mkdir bazel-0.15.0. unzip -n bazel-0.15.0-dist.zip -d ./bazel-0.15.0. cd bazel-0.15.0. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export JAVA_HOME=/usr/lib/jvm/java-1.8.0. export PATH=$HOMEPATH/inst/bin:$JAVA_HOME/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # build from scratch. PROTOC=$HOM",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
https://github.com/google/deepvariant/issues/123:3764,interoperability,repositor,repositories,3764,gcc. ```bash. # download source code. wget https://github.com/bazelbuild/bazel/releases/download/0.15.0/bazel-0.15.0-dist.zip. mkdir bazel-0.15.0. unzip -n bazel-0.15.0-dist.zip -d ./bazel-0.15.0. cd bazel-0.15.0. # environment. export CPU=power8. export HOMEPATH=/home/qilibj. export JAVA_HOME=/usr/lib/jvm/java-1.8.0. export PATH=$HOMEPATH/inst/bin:$JAVA_HOME/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # build from scratch. PROTOC=$HOMEPATH/inst/bin/protoc ./compile.sh. rsync -avP output/bazel $HOMEPATH/inst/bin/. # verification. bazel info. ```. ## Advance Toolchain 11.0. Doc: [https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/](https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/). ```bash. # gpg public key. wget ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. rpm --import gpg-pubkey-6976a827-5164221b. # Configure the Advance Toolchain repositories. [root@cit1074 deepvariant]# cat /etc/yum.repos.d/advance-toolchain.repo. #Begin of configuration file. [advance-toolchain]. name=Advance Toolchain Unicamp FTP. baseurl=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7. failovermethod=priority. enabled=1. gpgcheck=1. gpgkey=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b. # End of configuration file. # Install the Advance Toolchain. yum install advance-toolchain-at11.0-runtime. yum install advance-toolchain-at11.0-devel. yum install advance-toolchain-at11.0-perf. yum install advance-toolchain-at11.0-mcore-libs. # Load enviroment. export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # Do not need to export. # export LD_LIBRARY_PATH=/opt/at11.0/lib64:/lib64:$LD_LIBRARY_PATH . # Load environment. sudo yum install environment-modules. source /etc/profile. module load at11.0. module unload at11.0. ```. ## Python 2 and Pip 19.0.1. Build instructions: [https://docs.python.org/2/usin,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/123
