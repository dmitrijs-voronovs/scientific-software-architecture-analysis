id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/google/deepvariant/issues/650:775,testability,log,log,775,"Study on cost benefit of running in GPU mode; **Describe the issue:**. Hi, I am wondering if there's been a study on the cost-benefit of running DV in the GPU mode. Back in the days of PEPPER-DeepVariant-Margin, I remember trying to profile (not as a rigorous study) what benefits there'd be if we were to run the pipeline in the GPU mode. The conclusion back then from my anecdotal runs is that it's not worth it (we can get the CPU version to <$100/sample with little to minimum effort on optimizing cloud resource allocations, but the GPU version is ~$200/sample with P100). Now given that DV has undergone quite a lot of changes since then, I wonder if the conclusion is changed. So I did a test run on a PacBio Hifi 30X bam with DV 1.5.0, and collected the GPU resource log (using `gpustat -a -i 1 `, log attached below). Looking at the log file, it doesn't look like GPU is used much still. So I wonder if you have done any study on this subject and if so, can share some insights. Thank you! Steve. [gpu.usages.log.zip](https://github.com/google/deepvariant/files/11473421/gpu.usages.log.zip).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:806,testability,log,log,806,"Study on cost benefit of running in GPU mode; **Describe the issue:**. Hi, I am wondering if there's been a study on the cost-benefit of running DV in the GPU mode. Back in the days of PEPPER-DeepVariant-Margin, I remember trying to profile (not as a rigorous study) what benefits there'd be if we were to run the pipeline in the GPU mode. The conclusion back then from my anecdotal runs is that it's not worth it (we can get the CPU version to <$100/sample with little to minimum effort on optimizing cloud resource allocations, but the GPU version is ~$200/sample with P100). Now given that DV has undergone quite a lot of changes since then, I wonder if the conclusion is changed. So I did a test run on a PacBio Hifi 30X bam with DV 1.5.0, and collected the GPU resource log (using `gpustat -a -i 1 `, log attached below). Looking at the log file, it doesn't look like GPU is used much still. So I wonder if you have done any study on this subject and if so, can share some insights. Thank you! Steve. [gpu.usages.log.zip](https://github.com/google/deepvariant/files/11473421/gpu.usages.log.zip).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:842,testability,log,log,842,"Study on cost benefit of running in GPU mode; **Describe the issue:**. Hi, I am wondering if there's been a study on the cost-benefit of running DV in the GPU mode. Back in the days of PEPPER-DeepVariant-Margin, I remember trying to profile (not as a rigorous study) what benefits there'd be if we were to run the pipeline in the GPU mode. The conclusion back then from my anecdotal runs is that it's not worth it (we can get the CPU version to <$100/sample with little to minimum effort on optimizing cloud resource allocations, but the GPU version is ~$200/sample with P100). Now given that DV has undergone quite a lot of changes since then, I wonder if the conclusion is changed. So I did a test run on a PacBio Hifi 30X bam with DV 1.5.0, and collected the GPU resource log (using `gpustat -a -i 1 `, log attached below). Looking at the log file, it doesn't look like GPU is used much still. So I wonder if you have done any study on this subject and if so, can share some insights. Thank you! Steve. [gpu.usages.log.zip](https://github.com/google/deepvariant/files/11473421/gpu.usages.log.zip).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:1018,testability,log,log,1018,"Study on cost benefit of running in GPU mode; **Describe the issue:**. Hi, I am wondering if there's been a study on the cost-benefit of running DV in the GPU mode. Back in the days of PEPPER-DeepVariant-Margin, I remember trying to profile (not as a rigorous study) what benefits there'd be if we were to run the pipeline in the GPU mode. The conclusion back then from my anecdotal runs is that it's not worth it (we can get the CPU version to <$100/sample with little to minimum effort on optimizing cloud resource allocations, but the GPU version is ~$200/sample with P100). Now given that DV has undergone quite a lot of changes since then, I wonder if the conclusion is changed. So I did a test run on a PacBio Hifi 30X bam with DV 1.5.0, and collected the GPU resource log (using `gpustat -a -i 1 `, log attached below). Looking at the log file, it doesn't look like GPU is used much still. So I wonder if you have done any study on this subject and if so, can share some insights. Thank you! Steve. [gpu.usages.log.zip](https://github.com/google/deepvariant/files/11473421/gpu.usages.log.zip).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:1091,testability,log,log,1091,"Study on cost benefit of running in GPU mode; **Describe the issue:**. Hi, I am wondering if there's been a study on the cost-benefit of running DV in the GPU mode. Back in the days of PEPPER-DeepVariant-Margin, I remember trying to profile (not as a rigorous study) what benefits there'd be if we were to run the pipeline in the GPU mode. The conclusion back then from my anecdotal runs is that it's not worth it (we can get the CPU version to <$100/sample with little to minimum effort on optimizing cloud resource allocations, but the GPU version is ~$200/sample with P100). Now given that DV has undergone quite a lot of changes since then, I wonder if the conclusion is changed. So I did a test run on a PacBio Hifi 30X bam with DV 1.5.0, and collected the GPU resource log (using `gpustat -a -i 1 `, log attached below). Looking at the log file, it doesn't look like GPU is used much still. So I wonder if you have done any study on this subject and if so, can share some insights. Thank you! Steve. [gpu.usages.log.zip](https://github.com/google/deepvariant/files/11473421/gpu.usages.log.zip).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/650:473,usability,minim,minimum,473,"Study on cost benefit of running in GPU mode; **Describe the issue:**. Hi, I am wondering if there's been a study on the cost-benefit of running DV in the GPU mode. Back in the days of PEPPER-DeepVariant-Margin, I remember trying to profile (not as a rigorous study) what benefits there'd be if we were to run the pipeline in the GPU mode. The conclusion back then from my anecdotal runs is that it's not worth it (we can get the CPU version to <$100/sample with little to minimum effort on optimizing cloud resource allocations, but the GPU version is ~$200/sample with P100). Now given that DV has undergone quite a lot of changes since then, I wonder if the conclusion is changed. So I did a test run on a PacBio Hifi 30X bam with DV 1.5.0, and collected the GPU resource log (using `gpustat -a -i 1 `, log attached below). Looking at the log file, it doesn't look like GPU is used much still. So I wonder if you have done any study on this subject and if so, can share some insights. Thank you! Steve. [gpu.usages.log.zip](https://github.com/google/deepvariant/files/11473421/gpu.usages.log.zip).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/650
https://github.com/google/deepvariant/issues/651:303,availability,Operat,Operating,303,"can DeepVariant be run on a merged bam file? or must each Bam have a seperate run and then the vcfs can be merged?; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/651
https://github.com/google/deepvariant/issues/651:553,availability,Error,Error,553,"can DeepVariant be run on a merged bam file? or must each Bam have a seperate run and then the vcfs can be merged?; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/651
https://github.com/google/deepvariant/issues/651:336,deployability,version,version,336,"can DeepVariant be run on a merged bam file? or must each Bam have a seperate run and then the vcfs can be merged?; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/651
https://github.com/google/deepvariant/issues/651:348,deployability,Instal,Installation,348,"can DeepVariant be run on a merged bam file? or must each Bam have a seperate run and then the vcfs can be merged?; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/651
https://github.com/google/deepvariant/issues/651:336,integrability,version,version,336,"can DeepVariant be run on a merged bam file? or must each Bam have a seperate run and then the vcfs can be merged?; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/651
https://github.com/google/deepvariant/issues/651:336,modifiability,version,version,336,"can DeepVariant be run on a merged bam file? or must each Bam have a seperate run and then the vcfs can be merged?; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/651
https://github.com/google/deepvariant/issues/651:553,performance,Error,Error,553,"can DeepVariant be run on a merged bam file? or must each Bam have a seperate run and then the vcfs can be merged?; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/651
https://github.com/google/deepvariant/issues/651:585,reliability,Doe,Does,585,"can DeepVariant be run on a merged bam file? or must each Bam have a seperate run and then the vcfs can be merged?; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/651
https://github.com/google/deepvariant/issues/651:553,safety,Error,Error,553,"can DeepVariant be run on a merged bam file? or must each Bam have a seperate run and then the vcfs can be merged?; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/651
https://github.com/google/deepvariant/issues/651:606,safety,test,test,606,"can DeepVariant be run on a merged bam file? or must each Bam have a seperate run and then the vcfs can be merged?; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/651
https://github.com/google/deepvariant/issues/651:642,safety,test,test,642,"can DeepVariant be run on a merged bam file? or must each Bam have a seperate run and then the vcfs can be merged?; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/651
https://github.com/google/deepvariant/issues/651:432,testability,instrument,instrument,432,"can DeepVariant be run on a merged bam file? or must each Bam have a seperate run and then the vcfs can be merged?; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/651
https://github.com/google/deepvariant/issues/651:559,testability,trace,trace,559,"can DeepVariant be run on a merged bam file? or must each Bam have a seperate run and then the vcfs can be merged?; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/651
https://github.com/google/deepvariant/issues/651:606,testability,test,test,606,"can DeepVariant be run on a merged bam file? or must each Bam have a seperate run and then the vcfs can be merged?; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/651
https://github.com/google/deepvariant/issues/651:642,testability,test,test,642,"can DeepVariant be run on a merged bam file? or must each Bam have a seperate run and then the vcfs can be merged?; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/651
https://github.com/google/deepvariant/issues/651:817,testability,context,context,817,"can DeepVariant be run on a merged bam file? or must each Bam have a seperate run and then the vcfs can be merged?; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/651
https://github.com/google/deepvariant/issues/651:236,usability,clear,clear,236,"can DeepVariant be run on a merged bam file? or must each Bam have a seperate run and then the vcfs can be merged?; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/651
https://github.com/google/deepvariant/issues/651:541,usability,Command,Command,541,"can DeepVariant be run on a merged bam file? or must each Bam have a seperate run and then the vcfs can be merged?; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/651
https://github.com/google/deepvariant/issues/651:553,usability,Error,Error,553,"can DeepVariant be run on a merged bam file? or must each Bam have a seperate run and then the vcfs can be merged?; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/651
https://github.com/google/deepvariant/issues/653:553,availability,error,error,553,"DeepVariant Runerror, ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). Hi developers,. I'd like to run `DeepVariant` for my `WGS` sequencing data. My sequencing data were from `BGI` platform and were preprocessed by `fastp, bwa+Hs37d5, MarkDuplicatesSpark`. I tried to use the 'sorted and deduplicated bam' file as input for `DeepVariant` in `singularity` mode. However, I always encountered the 'reference index' `not found` error. But my `reference` fasta file and `reference index` fai file does exist. Could you please help me figure it out? **Setup**. - Operating system: Linux version 3.10.0-1127.el7.x86_64 (gcc version 4.8.5 20150623 (Red Hat 4.8.5-39), Computation Node (one node of Clusters). - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.):. - ``` BIN_VERSION=""1.5.0"". docker pull; . singularity pull docker://google/deepvariant:""${BIN_VERSION}"". singularity build --fakeroot deepvariant.sif docker://google/deepvariant:1.5.0```. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). `BGI platform, WGS data, Hs37d5 reference, fastp QC, bwa-mem2 mapping, MarkDuplicatesSpark sort & dedup`. . **Steps to reproduce:**. - Command:. - 1. singularity run /lustre/Data/toolsDB//deepvariant.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=$ref_idx --reads=$dedupbam --output_vcf=$vcfout --output_gvcf=$gvcfout --num_shards=32 >$logx 2>&1. - Error trace: (if applicable). - ```I0522 08:40:36.823651 140633630893888 genomics_reader.py:222] Reading /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam with NativeSamReader. I0522 08:40:36.846348 140633630893888 make_examples_core.py:257] Task 27/32: Preparing inputs. [E::fai_load3_core] Failed to open FASTA index /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:686,availability,Operat,Operating,686,"DeepVariant Runerror, ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). Hi developers,. I'd like to run `DeepVariant` for my `WGS` sequencing data. My sequencing data were from `BGI` platform and were preprocessed by `fastp, bwa+Hs37d5, MarkDuplicatesSpark`. I tried to use the 'sorted and deduplicated bam' file as input for `DeepVariant` in `singularity` mode. However, I always encountered the 'reference index' `not found` error. But my `reference` fasta file and `reference index` fai file does exist. Could you please help me figure it out? **Setup**. - Operating system: Linux version 3.10.0-1127.el7.x86_64 (gcc version 4.8.5 20150623 (Red Hat 4.8.5-39), Computation Node (one node of Clusters). - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.):. - ``` BIN_VERSION=""1.5.0"". docker pull; . singularity pull docker://google/deepvariant:""${BIN_VERSION}"". singularity build --fakeroot deepvariant.sif docker://google/deepvariant:1.5.0```. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). `BGI platform, WGS data, Hs37d5 reference, fastp QC, bwa-mem2 mapping, MarkDuplicatesSpark sort & dedup`. . **Steps to reproduce:**. - Command:. - 1. singularity run /lustre/Data/toolsDB//deepvariant.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=$ref_idx --reads=$dedupbam --output_vcf=$vcfout --output_gvcf=$gvcfout --num_shards=32 >$logx 2>&1. - Error trace: (if applicable). - ```I0522 08:40:36.823651 140633630893888 genomics_reader.py:222] Reading /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam with NativeSamReader. I0522 08:40:36.846348 140633630893888 make_examples_core.py:257] Task 27/32: Preparing inputs. [E::fai_load3_core] Failed to open FASTA index /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:819,availability,Cluster,Clusters,819,"DeepVariant Runerror, ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). Hi developers,. I'd like to run `DeepVariant` for my `WGS` sequencing data. My sequencing data were from `BGI` platform and were preprocessed by `fastp, bwa+Hs37d5, MarkDuplicatesSpark`. I tried to use the 'sorted and deduplicated bam' file as input for `DeepVariant` in `singularity` mode. However, I always encountered the 'reference index' `not found` error. But my `reference` fasta file and `reference index` fai file does exist. Could you please help me figure it out? **Setup**. - Operating system: Linux version 3.10.0-1127.el7.x86_64 (gcc version 4.8.5 20150623 (Red Hat 4.8.5-39), Computation Node (one node of Clusters). - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.):. - ``` BIN_VERSION=""1.5.0"". docker pull; . singularity pull docker://google/deepvariant:""${BIN_VERSION}"". singularity build --fakeroot deepvariant.sif docker://google/deepvariant:1.5.0```. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). `BGI platform, WGS data, Hs37d5 reference, fastp QC, bwa-mem2 mapping, MarkDuplicatesSpark sort & dedup`. . **Steps to reproduce:**. - Command:. - 1. singularity run /lustre/Data/toolsDB//deepvariant.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=$ref_idx --reads=$dedupbam --output_vcf=$vcfout --output_gvcf=$gvcfout --num_shards=32 >$logx 2>&1. - Error trace: (if applicable). - ```I0522 08:40:36.823651 140633630893888 genomics_reader.py:222] Reading /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam with NativeSamReader. I0522 08:40:36.846348 140633630893888 make_examples_core.py:257] Task 27/32: Preparing inputs. [E::fai_load3_core] Failed to open FASTA index /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:1582,availability,Error,Error,1582,"file and `reference index` fai file does exist. Could you please help me figure it out? **Setup**. - Operating system: Linux version 3.10.0-1127.el7.x86_64 (gcc version 4.8.5 20150623 (Red Hat 4.8.5-39), Computation Node (one node of Clusters). - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.):. - ``` BIN_VERSION=""1.5.0"". docker pull; . singularity pull docker://google/deepvariant:""${BIN_VERSION}"". singularity build --fakeroot deepvariant.sif docker://google/deepvariant:1.5.0```. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). `BGI platform, WGS data, Hs37d5 reference, fastp QC, bwa-mem2 mapping, MarkDuplicatesSpark sort & dedup`. . **Steps to reproduce:**. - Command:. - 1. singularity run /lustre/Data/toolsDB//deepvariant.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=$ref_idx --reads=$dedupbam --output_vcf=$vcfout --output_gvcf=$gvcfout --num_shards=32 >$logx 2>&1. - Error trace: (if applicable). - ```I0522 08:40:36.823651 140633630893888 genomics_reader.py:222] Reading /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam with NativeSamReader. I0522 08:40:36.846348 140633630893888 make_examples_core.py:257] Task 27/32: Preparing inputs. [E::fai_load3_core] Failed to open FASTA index /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai: No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main. make_examples_core.make_examples_runner(o",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:8259,availability,operat,operations,8259," test run as normal. ```. 3. reference index does. ```$ ls /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467 Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909010 Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 106669 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6924 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 10196727247 Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 2813 May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 784363628 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```. **************. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-05-19 16:22:21.555857: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (o. neDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0519 16:22:23.193474 139896863770432 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.256151 139896863770432 make_examples_core.py:257] Task 10/32: Preparing inputs. I0519 16:22:23.258605 139896863770432 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.259495 139896863770432 make_examples_core.py:257] Task 10/32: Common contigs are ['chr20']. I0519 16:22:23.239336 140148036429632 genomics_reader.py:222] Reading quickstart-test",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:8557,availability,operat,operations,8557,"tre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 106669 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6924 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 10196727247 Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 2813 May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 784363628 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```. **************. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-05-19 16:22:21.555857: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (o. neDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0519 16:22:23.193474 139896863770432 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.256151 139896863770432 make_examples_core.py:257] Task 10/32: Preparing inputs. I0519 16:22:23.258605 139896863770432 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.259495 139896863770432 make_examples_core.py:257] Task 10/32: Common contigs are ['chr20']. I0519 16:22:23.239336 140148036429632 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.192739 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.235120 140421750466368 make_examples_core.py:257] Task 21/32: Preparing inputs. I0",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:8623,availability,operat,operations,8623,"1 zhoujianglin zhoujianglin 106669 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6924 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 10196727247 Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 2813 May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 784363628 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```. **************. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-05-19 16:22:21.555857: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (o. neDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0519 16:22:23.193474 139896863770432 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.256151 139896863770432 make_examples_core.py:257] Task 10/32: Preparing inputs. I0519 16:22:23.258605 139896863770432 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.259495 139896863770432 make_examples_core.py:257] Task 10/32: Common contigs are ['chr20']. I0519 16:22:23.239336 140148036429632 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.192739 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.235120 140421750466368 make_examples_core.py:257] Task 21/32: Preparing inputs. I0519 16:22:23.239059 140421750466368 genomics_reader.py:222] Readin",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:12254,availability,Restor,Restoring,12254,"les_core.py:257] Task 12/32: Preparing inputs. I0519 16:22:23.236589 139704511100736 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.237330 139704511100736 make_examples_core.py:257] Task 12/32: Common contigs are ['chr20']. I0519 16:22:23.237694 140638923466560 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.289134 140638923466560 make_examples_core.py:257] Task 29/32: Preparing inputs. I0519 16:22:23.215111 139798323177280 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.265897 139798323177280 make_examples_core.py:257] Task 9/32: Preparing inputs. **********. I0519 16:22:46.781010 139665862911808 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0519 16:22:47.119282 139665862911808 modeling.py:418] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt. I0519 16:22:47.119841 139665862911808 saver.py:1410] Restoring parameters from /opt/models/wgs/model.ckpt. I0519 16:22:48.504039 139665862911808 call_variants.py:462] Processed 1 examples in 1 batches [632.440 sec per 100]. I0519 16:22:48.735930 139665862911808 call_variants.py:468] Processed 305 examples in 1 batches [2.084 sec per 100]. I0519 16:22:48.736088 139665862911808 call_variants.py:471] Done calling variants from a total of 305 examples. real 0m8.934s. user 0m37.643s. sys 0m6.426s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --infile ""/tmp/tmp6gzkras0/call_variants_output.tfrecord.gz"" --outfile ""singularity-output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp6gzkras0/gvcf.tfrecord@32.gz"" --gvcf_outfile ""singularity-output/output.g.vcf.gz"". 2023-05-19 16:22:49.638487: I tensorflow/core/platform/cpu_feature_guard.cc:193] Thi",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:12361,availability,Restor,Restoring,12361,"] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.237330 139704511100736 make_examples_core.py:257] Task 12/32: Common contigs are ['chr20']. I0519 16:22:23.237694 140638923466560 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.289134 140638923466560 make_examples_core.py:257] Task 29/32: Preparing inputs. I0519 16:22:23.215111 139798323177280 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.265897 139798323177280 make_examples_core.py:257] Task 9/32: Preparing inputs. **********. I0519 16:22:46.781010 139665862911808 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0519 16:22:47.119282 139665862911808 modeling.py:418] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt. I0519 16:22:47.119841 139665862911808 saver.py:1410] Restoring parameters from /opt/models/wgs/model.ckpt. I0519 16:22:48.504039 139665862911808 call_variants.py:462] Processed 1 examples in 1 batches [632.440 sec per 100]. I0519 16:22:48.735930 139665862911808 call_variants.py:468] Processed 305 examples in 1 batches [2.084 sec per 100]. I0519 16:22:48.736088 139665862911808 call_variants.py:471] Done calling variants from a total of 305 examples. real 0m8.934s. user 0m37.643s. sys 0m6.426s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --infile ""/tmp/tmp6gzkras0/call_variants_output.tfrecord.gz"" --outfile ""singularity-output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp6gzkras0/gvcf.tfrecord@32.gz"" --gvcf_outfile ""singularity-output/output.g.vcf.gz"". 2023-05-19 16:22:49.638487: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:13403,availability,operat,operations,13403,".ckpt. I0519 16:22:48.504039 139665862911808 call_variants.py:462] Processed 1 examples in 1 batches [632.440 sec per 100]. I0519 16:22:48.735930 139665862911808 call_variants.py:468] Processed 305 examples in 1 batches [2.084 sec per 100]. I0519 16:22:48.736088 139665862911808 call_variants.py:471] Done calling variants from a total of 305 examples. real 0m8.934s. user 0m37.643s. sys 0m6.426s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --infile ""/tmp/tmp6gzkras0/call_variants_output.tfrecord.gz"" --outfile ""singularity-output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp6gzkras0/gvcf.tfrecord@32.gz"" --gvcf_outfile ""singularity-output/output.g.vcf.gz"". 2023-05-19 16:22:49.638487: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0519 16:22:51.170790 140640470185792 postprocess_variants.py:972] Using sample name from call_variants output. Sample name: NA12878. 2023-05-19 16:22:51.171487: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/tmp6gzkras0/call_variants_output.tfrecord.gz. 2023-05-19 16:22:51.173038: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 305. I0519 16:22:51.173760 140640470185792 postprocess_variants.py:1037] CVO sorting took 3.902912139892578e-05 minutes. I0519 16:22:51.173915 140640470185792 postprocess_variants.py:1040] Transforming call_variants_output to variants. I0519 16:22:51.204555 140640470185792 postprocess_variants.py:1080] Processing variants (and writing to temporary file) took 0.0005078872044881184 minutes. I0519 16:22:51.582558 140640470185792 postprocess_variants.py:1093] Finished writing VCF and gVCF in 0.0",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:13469,availability,operat,operations,13469," Processed 1 examples in 1 batches [632.440 sec per 100]. I0519 16:22:48.735930 139665862911808 call_variants.py:468] Processed 305 examples in 1 batches [2.084 sec per 100]. I0519 16:22:48.736088 139665862911808 call_variants.py:471] Done calling variants from a total of 305 examples. real 0m8.934s. user 0m37.643s. sys 0m6.426s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --infile ""/tmp/tmp6gzkras0/call_variants_output.tfrecord.gz"" --outfile ""singularity-output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp6gzkras0/gvcf.tfrecord@32.gz"" --gvcf_outfile ""singularity-output/output.g.vcf.gz"". 2023-05-19 16:22:49.638487: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0519 16:22:51.170790 140640470185792 postprocess_variants.py:972] Using sample name from call_variants output. Sample name: NA12878. 2023-05-19 16:22:51.171487: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/tmp6gzkras0/call_variants_output.tfrecord.gz. 2023-05-19 16:22:51.173038: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 305. I0519 16:22:51.173760 140640470185792 postprocess_variants.py:1037] CVO sorting took 3.902912139892578e-05 minutes. I0519 16:22:51.173915 140640470185792 postprocess_variants.py:1040] Transforming call_variants_output to variants. I0519 16:22:51.204555 140640470185792 postprocess_variants.py:1080] Processing variants (and writing to temporary file) took 0.0005078872044881184 minutes. I0519 16:22:51.582558 140640470185792 postprocess_variants.py:1093] Finished writing VCF and gVCF in 0.006296555201212565 minutes. I0519 16:22:51.587275 140640470185792 g",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:710,deployability,version,version,710,"DeepVariant Runerror, ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). Hi developers,. I'd like to run `DeepVariant` for my `WGS` sequencing data. My sequencing data were from `BGI` platform and were preprocessed by `fastp, bwa+Hs37d5, MarkDuplicatesSpark`. I tried to use the 'sorted and deduplicated bam' file as input for `DeepVariant` in `singularity` mode. However, I always encountered the 'reference index' `not found` error. But my `reference` fasta file and `reference index` fai file does exist. Could you please help me figure it out? **Setup**. - Operating system: Linux version 3.10.0-1127.el7.x86_64 (gcc version 4.8.5 20150623 (Red Hat 4.8.5-39), Computation Node (one node of Clusters). - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.):. - ``` BIN_VERSION=""1.5.0"". docker pull; . singularity pull docker://google/deepvariant:""${BIN_VERSION}"". singularity build --fakeroot deepvariant.sif docker://google/deepvariant:1.5.0```. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). `BGI platform, WGS data, Hs37d5 reference, fastp QC, bwa-mem2 mapping, MarkDuplicatesSpark sort & dedup`. . **Steps to reproduce:**. - Command:. - 1. singularity run /lustre/Data/toolsDB//deepvariant.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=$ref_idx --reads=$dedupbam --output_vcf=$vcfout --output_gvcf=$gvcfout --num_shards=32 >$logx 2>&1. - Error trace: (if applicable). - ```I0522 08:40:36.823651 140633630893888 genomics_reader.py:222] Reading /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam with NativeSamReader. I0522 08:40:36.846348 140633630893888 make_examples_core.py:257] Task 27/32: Preparing inputs. [E::fai_load3_core] Failed to open FASTA index /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:746,deployability,version,version,746,"DeepVariant Runerror, ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). Hi developers,. I'd like to run `DeepVariant` for my `WGS` sequencing data. My sequencing data were from `BGI` platform and were preprocessed by `fastp, bwa+Hs37d5, MarkDuplicatesSpark`. I tried to use the 'sorted and deduplicated bam' file as input for `DeepVariant` in `singularity` mode. However, I always encountered the 'reference index' `not found` error. But my `reference` fasta file and `reference index` fai file does exist. Could you please help me figure it out? **Setup**. - Operating system: Linux version 3.10.0-1127.el7.x86_64 (gcc version 4.8.5 20150623 (Red Hat 4.8.5-39), Computation Node (one node of Clusters). - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.):. - ``` BIN_VERSION=""1.5.0"". docker pull; . singularity pull docker://google/deepvariant:""${BIN_VERSION}"". singularity build --fakeroot deepvariant.sif docker://google/deepvariant:1.5.0```. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). `BGI platform, WGS data, Hs37d5 reference, fastp QC, bwa-mem2 mapping, MarkDuplicatesSpark sort & dedup`. . **Steps to reproduce:**. - Command:. - 1. singularity run /lustre/Data/toolsDB//deepvariant.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=$ref_idx --reads=$dedupbam --output_vcf=$vcfout --output_gvcf=$gvcfout --num_shards=32 >$logx 2>&1. - Error trace: (if applicable). - ```I0522 08:40:36.823651 140633630893888 genomics_reader.py:222] Reading /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam with NativeSamReader. I0522 08:40:36.846348 140633630893888 make_examples_core.py:257] Task 27/32: Preparing inputs. [E::fai_load3_core] Failed to open FASTA index /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:819,deployability,Cluster,Clusters,819,"DeepVariant Runerror, ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). Hi developers,. I'd like to run `DeepVariant` for my `WGS` sequencing data. My sequencing data were from `BGI` platform and were preprocessed by `fastp, bwa+Hs37d5, MarkDuplicatesSpark`. I tried to use the 'sorted and deduplicated bam' file as input for `DeepVariant` in `singularity` mode. However, I always encountered the 'reference index' `not found` error. But my `reference` fasta file and `reference index` fai file does exist. Could you please help me figure it out? **Setup**. - Operating system: Linux version 3.10.0-1127.el7.x86_64 (gcc version 4.8.5 20150623 (Red Hat 4.8.5-39), Computation Node (one node of Clusters). - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.):. - ``` BIN_VERSION=""1.5.0"". docker pull; . singularity pull docker://google/deepvariant:""${BIN_VERSION}"". singularity build --fakeroot deepvariant.sif docker://google/deepvariant:1.5.0```. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). `BGI platform, WGS data, Hs37d5 reference, fastp QC, bwa-mem2 mapping, MarkDuplicatesSpark sort & dedup`. . **Steps to reproduce:**. - Command:. - 1. singularity run /lustre/Data/toolsDB//deepvariant.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=$ref_idx --reads=$dedupbam --output_vcf=$vcfout --output_gvcf=$gvcfout --num_shards=32 >$logx 2>&1. - Error trace: (if applicable). - ```I0522 08:40:36.823651 140633630893888 genomics_reader.py:222] Reading /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam with NativeSamReader. I0522 08:40:36.846348 140633630893888 make_examples_core.py:257] Task 27/32: Preparing inputs. [E::fai_load3_core] Failed to open FASTA index /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:844,deployability,version,version,844,"DeepVariant Runerror, ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). Hi developers,. I'd like to run `DeepVariant` for my `WGS` sequencing data. My sequencing data were from `BGI` platform and were preprocessed by `fastp, bwa+Hs37d5, MarkDuplicatesSpark`. I tried to use the 'sorted and deduplicated bam' file as input for `DeepVariant` in `singularity` mode. However, I always encountered the 'reference index' `not found` error. But my `reference` fasta file and `reference index` fai file does exist. Could you please help me figure it out? **Setup**. - Operating system: Linux version 3.10.0-1127.el7.x86_64 (gcc version 4.8.5 20150623 (Red Hat 4.8.5-39), Computation Node (one node of Clusters). - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.):. - ``` BIN_VERSION=""1.5.0"". docker pull; . singularity pull docker://google/deepvariant:""${BIN_VERSION}"". singularity build --fakeroot deepvariant.sif docker://google/deepvariant:1.5.0```. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). `BGI platform, WGS data, Hs37d5 reference, fastp QC, bwa-mem2 mapping, MarkDuplicatesSpark sort & dedup`. . **Steps to reproduce:**. - Command:. - 1. singularity run /lustre/Data/toolsDB//deepvariant.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=$ref_idx --reads=$dedupbam --output_vcf=$vcfout --output_gvcf=$gvcfout --num_shards=32 >$logx 2>&1. - Error trace: (if applicable). - ```I0522 08:40:36.823651 140633630893888 genomics_reader.py:222] Reading /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam with NativeSamReader. I0522 08:40:36.846348 140633630893888 make_examples_core.py:257] Task 27/32: Preparing inputs. [E::fai_load3_core] Failed to open FASTA index /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:862,deployability,Instal,Installation,862,"DeepVariant Runerror, ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). Hi developers,. I'd like to run `DeepVariant` for my `WGS` sequencing data. My sequencing data were from `BGI` platform and were preprocessed by `fastp, bwa+Hs37d5, MarkDuplicatesSpark`. I tried to use the 'sorted and deduplicated bam' file as input for `DeepVariant` in `singularity` mode. However, I always encountered the 'reference index' `not found` error. But my `reference` fasta file and `reference index` fai file does exist. Could you please help me figure it out? **Setup**. - Operating system: Linux version 3.10.0-1127.el7.x86_64 (gcc version 4.8.5 20150623 (Red Hat 4.8.5-39), Computation Node (one node of Clusters). - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.):. - ``` BIN_VERSION=""1.5.0"". docker pull; . singularity pull docker://google/deepvariant:""${BIN_VERSION}"". singularity build --fakeroot deepvariant.sif docker://google/deepvariant:1.5.0```. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). `BGI platform, WGS data, Hs37d5 reference, fastp QC, bwa-mem2 mapping, MarkDuplicatesSpark sort & dedup`. . **Steps to reproduce:**. - Command:. - 1. singularity run /lustre/Data/toolsDB//deepvariant.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=$ref_idx --reads=$dedupbam --output_vcf=$vcfout --output_gvcf=$gvcfout --num_shards=32 >$logx 2>&1. - Error trace: (if applicable). - ```I0522 08:40:36.823651 140633630893888 genomics_reader.py:222] Reading /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam with NativeSamReader. I0522 08:40:36.846348 140633630893888 make_examples_core.py:257] Task 27/32: Preparing inputs. [E::fai_load3_core] Failed to open FASTA index /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:1035,deployability,build,build,1035,"cked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). Hi developers,. I'd like to run `DeepVariant` for my `WGS` sequencing data. My sequencing data were from `BGI` platform and were preprocessed by `fastp, bwa+Hs37d5, MarkDuplicatesSpark`. I tried to use the 'sorted and deduplicated bam' file as input for `DeepVariant` in `singularity` mode. However, I always encountered the 'reference index' `not found` error. But my `reference` fasta file and `reference index` fai file does exist. Could you please help me figure it out? **Setup**. - Operating system: Linux version 3.10.0-1127.el7.x86_64 (gcc version 4.8.5 20150623 (Red Hat 4.8.5-39), Computation Node (one node of Clusters). - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.):. - ``` BIN_VERSION=""1.5.0"". docker pull; . singularity pull docker://google/deepvariant:""${BIN_VERSION}"". singularity build --fakeroot deepvariant.sif docker://google/deepvariant:1.5.0```. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). `BGI platform, WGS data, Hs37d5 reference, fastp QC, bwa-mem2 mapping, MarkDuplicatesSpark sort & dedup`. . **Steps to reproduce:**. - Command:. - 1. singularity run /lustre/Data/toolsDB//deepvariant.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=$ref_idx --reads=$dedupbam --output_vcf=$vcfout --output_gvcf=$gvcfout --num_shards=32 >$logx 2>&1. - Error trace: (if applicable). - ```I0522 08:40:36.823651 140633630893888 genomics_reader.py:222] Reading /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam with NativeSamReader. I0522 08:40:36.846348 140633630893888 make_examples_core.py:257] Task 27/32: Preparing inputs. [E::fai_load3_core] Failed to open FASTA index /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai: No such file or directory. Tr",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:1569,deployability,log,logx,1569,"erence` fasta file and `reference index` fai file does exist. Could you please help me figure it out? **Setup**. - Operating system: Linux version 3.10.0-1127.el7.x86_64 (gcc version 4.8.5 20150623 (Red Hat 4.8.5-39), Computation Node (one node of Clusters). - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.):. - ``` BIN_VERSION=""1.5.0"". docker pull; . singularity pull docker://google/deepvariant:""${BIN_VERSION}"". singularity build --fakeroot deepvariant.sif docker://google/deepvariant:1.5.0```. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). `BGI platform, WGS data, Hs37d5 reference, fastp QC, bwa-mem2 mapping, MarkDuplicatesSpark sort & dedup`. . **Steps to reproduce:**. - Command:. - 1. singularity run /lustre/Data/toolsDB//deepvariant.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=$ref_idx --reads=$dedupbam --output_vcf=$vcfout --output_gvcf=$gvcfout --num_shards=32 >$logx 2>&1. - Error trace: (if applicable). - ```I0522 08:40:36.823651 140633630893888 genomics_reader.py:222] Reading /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam with NativeSamReader. I0522 08:40:36.846348 140633630893888 make_examples_core.py:257] Task 27/32: Preparing inputs. [E::fai_load3_core] Failed to open FASTA index /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai: No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main. make_examples_core.make_exa",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:1924,deployability,Fail,Failed,1924,"_VERSION=""1.5.0"". docker pull; . singularity pull docker://google/deepvariant:""${BIN_VERSION}"". singularity build --fakeroot deepvariant.sif docker://google/deepvariant:1.5.0```. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). `BGI platform, WGS data, Hs37d5 reference, fastp QC, bwa-mem2 mapping, MarkDuplicatesSpark sort & dedup`. . **Steps to reproduce:**. - Command:. - 1. singularity run /lustre/Data/toolsDB//deepvariant.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=$ref_idx --reads=$dedupbam --output_vcf=$vcfout --output_gvcf=$gvcfout --num_shards=32 >$logx 2>&1. - Error trace: (if applicable). - ```I0522 08:40:36.823651 140633630893888 genomics_reader.py:222] Reading /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam with NativeSamReader. I0522 08:40:36.846348 140633630893888 make_examples_core.py:257] Task 27/32: Preparing inputs. [E::fai_load3_core] Failed to open FASTA index /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai: No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2114, in make_examples_runner. regions, calling_regions = processing_regions_from_options(options). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2019, in processi",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:2184,deployability,modul,module,2184,"s unlike the case studies?). `BGI platform, WGS data, Hs37d5 reference, fastp QC, bwa-mem2 mapping, MarkDuplicatesSpark sort & dedup`. . **Steps to reproduce:**. - Command:. - 1. singularity run /lustre/Data/toolsDB//deepvariant.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=$ref_idx --reads=$dedupbam --output_vcf=$vcfout --output_gvcf=$gvcfout --num_shards=32 >$logx 2>&1. - Error trace: (if applicable). - ```I0522 08:40:36.823651 140633630893888 genomics_reader.py:222] Reading /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam with NativeSamReader. I0522 08:40:36.846348 140633630893888 make_examples_core.py:257] Task 27/32: Preparing inputs. [E::fai_load3_core] Failed to open FASTA index /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai: No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2114, in make_examples_runner. regions, calling_regions = processing_regions_from_options(options). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2019, in processing_regions_from_options. ref_contigs = fasta.IndexedFastaReader(. File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 106, in __init__. self._reader = reference.IndexedFastaReader.from_file(. ValueError: NO",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:3598,deployability,Fail,Failed,3598,"tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2114, in make_examples_runner. regions, calling_regions = processing_regions_from_options(options). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2019, in processing_regions_from_options. ref_contigs = fasta.IndexedFastaReader(. File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 106, in __init__. self._reader = reference.IndexedFastaReader.from_file(. ValueError: NOT_FOUND: could not load fasta and/or fai for fasta /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. I0522 08:40:37.063492 139925831092032 genomics_reader.py:222] Reading /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam with NativeSamReader. I0522 08:40:37.081499 139925831092032 make_examples_core.py:257] Task 6/32: Preparing inputs. [E::fai_load3_core] Failed to open FASTA index /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai: No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_luflf_op/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_luflf_op/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_luflf_op/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_luflf_op/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_luflf_op/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2114, in make_examples_runner. regions, calling_regions = processing_regions_from_options(options). File ""/tmp/Bazel.runfiles_luflf_op/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2019, in processi",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:3858,deployability,modul,module,3858,"variant/deepvariant/make_examples_core.py"", line 2019, in processing_regions_from_options. ref_contigs = fasta.IndexedFastaReader(. File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 106, in __init__. self._reader = reference.IndexedFastaReader.from_file(. ValueError: NOT_FOUND: could not load fasta and/or fai for fasta /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. I0522 08:40:37.063492 139925831092032 genomics_reader.py:222] Reading /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam with NativeSamReader. I0522 08:40:37.081499 139925831092032 make_examples_core.py:257] Task 6/32: Preparing inputs. [E::fai_load3_core] Failed to open FASTA index /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai: No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_luflf_op/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_luflf_op/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_luflf_op/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_luflf_op/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_luflf_op/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2114, in make_examples_runner. regions, calling_regions = processing_regions_from_options(options). File ""/tmp/Bazel.runfiles_luflf_op/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2019, in processing_regions_from_options. ref_contigs = fasta.IndexedFastaReader(. File ""/tmp/Bazel.runfiles_luflf_op/runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 106, in __init__. self._reader = reference.IndexedFastaReader.from_file(. **************",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:5278,deployability,Fail,Failed,5278,"zel.runfiles_luflf_op/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2114, in make_examples_runner. regions, calling_regions = processing_regions_from_options(options). File ""/tmp/Bazel.runfiles_luflf_op/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2019, in processing_regions_from_options. ref_contigs = fasta.IndexedFastaReader(. File ""/tmp/Bazel.runfiles_luflf_op/runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 106, in __init__. self._reader = reference.IndexedFastaReader.from_file(. ****************. File ""/tmp/Bazel.runfiles_8r9pgqu5/runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 106, in __init__. self._reader = reference.IndexedFastaReader.from_file(. ValueError: NOT_FOUND: could not load fasta and/or fai for fasta /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. I0522 08:40:37.129531 140035375298368 make_examples_core.py:257] Task 7/32: Preparing inputs. [E::fai_load3_core] Failed to open FASTA index /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai: No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2114, in make_examples_runner. regions, calling_regions = processing_regions_from_options(options). File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2019, in processi",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:5538,deployability,modul,module,5538,"t/deepvariant/make_examples_core.py"", line 2019, in processing_regions_from_options. ref_contigs = fasta.IndexedFastaReader(. File ""/tmp/Bazel.runfiles_luflf_op/runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 106, in __init__. self._reader = reference.IndexedFastaReader.from_file(. ****************. File ""/tmp/Bazel.runfiles_8r9pgqu5/runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 106, in __init__. self._reader = reference.IndexedFastaReader.from_file(. ValueError: NOT_FOUND: could not load fasta and/or fai for fasta /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. I0522 08:40:37.129531 140035375298368 make_examples_core.py:257] Task 7/32: Preparing inputs. [E::fai_load3_core] Failed to open FASTA index /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai: No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2114, in make_examples_runner. regions, calling_regions = processing_regions_from_options(options). File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2019, in processing_regions_from_options. ref_contigs = fasta.IndexedFastaReader(. File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 106, in __init__. self._reader = reference.IndexedFastaReader.from_file(. ValueError: NO",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:6665,deployability,fail,failed,6665," File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2114, in make_examples_runner. regions, calling_regions = processing_regions_from_options(options). File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2019, in processing_regions_from_options. ref_contigs = fasta.IndexedFastaReader(. File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 106, in __init__. self._reader = reference.IndexedFastaReader.from_file(. ValueError: NOT_FOUND: could not load fasta and/or fai for fasta /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa --reads /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam --examples /tmp/tmpfab4tpv7/make_examples.tfrecord@32.gz --channels insert_size --gvcf /tmp/tmpfab4tpv7/gvcf.tfrecord@32.gz --task 18. - ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the quick test run as normal. ```. 3. reference index does. ```$ ls /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467 Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909010 Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 106669 Apr 2",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:3206,energy efficiency,load,load,3206,"File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2114, in make_examples_runner. regions, calling_regions = processing_regions_from_options(options). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2019, in processing_regions_from_options. ref_contigs = fasta.IndexedFastaReader(. File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 106, in __init__. self._reader = reference.IndexedFastaReader.from_file(. ValueError: NOT_FOUND: could not load fasta and/or fai for fasta /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. I0522 08:40:37.063492 139925831092032 genomics_reader.py:222] Reading /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam with NativeSamReader. I0522 08:40:37.081499 139925831092032 make_examples_core.py:257] Task 6/32: Preparing inputs. [E::fai_load3_core] Failed to open FASTA index /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai: No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_luflf_op/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_luflf_op/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_luflf_op/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_luflf_op/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186,",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:5078,energy efficiency,load,load,5078,"ys.exit(main(argv)). File ""/tmp/Bazel.runfiles_luflf_op/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_luflf_op/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2114, in make_examples_runner. regions, calling_regions = processing_regions_from_options(options). File ""/tmp/Bazel.runfiles_luflf_op/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2019, in processing_regions_from_options. ref_contigs = fasta.IndexedFastaReader(. File ""/tmp/Bazel.runfiles_luflf_op/runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 106, in __init__. self._reader = reference.IndexedFastaReader.from_file(. ****************. File ""/tmp/Bazel.runfiles_8r9pgqu5/runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 106, in __init__. self._reader = reference.IndexedFastaReader.from_file(. ValueError: NOT_FOUND: could not load fasta and/or fai for fasta /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. I0522 08:40:37.129531 140035375298368 make_examples_core.py:257] Task 7/32: Preparing inputs. [E::fai_load3_core] Failed to open FASTA index /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai: No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2114, in make_examples_r",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:6560,energy efficiency,load,load,6560,"File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2114, in make_examples_runner. regions, calling_regions = processing_regions_from_options(options). File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2019, in processing_regions_from_options. ref_contigs = fasta.IndexedFastaReader(. File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 106, in __init__. self._reader = reference.IndexedFastaReader.from_file(. ValueError: NOT_FOUND: could not load fasta and/or fai for fasta /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa --reads /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam --examples /tmp/tmpfab4tpv7/make_examples.tfrecord@32.gz --channels insert_size --gvcf /tmp/tmpfab4tpv7/gvcf.tfrecord@32.gz --task 18. - ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the quick test run as normal. ```. 3. reference index does. ```$ ls /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467 Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909010 Apr 26 15:23 /lus",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:8368,energy efficiency,core,core,8368,"d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467 Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909010 Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 106669 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6924 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 10196727247 Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 2813 May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 784363628 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```. **************. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-05-19 16:22:21.555857: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (o. neDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0519 16:22:23.193474 139896863770432 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.256151 139896863770432 make_examples_core.py:257] Task 10/32: Preparing inputs. I0519 16:22:23.258605 139896863770432 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.259495 139896863770432 make_examples_core.py:257] Task 10/32: Common contigs are ['chr20']. I0519 16:22:23.239336 140148036429632 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.192739 140421750466368 genomics_r",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:8434,energy efficiency,optim,optimized,8434,"3 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909010 Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 106669 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6924 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 10196727247 Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 2813 May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 784363628 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```. **************. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-05-19 16:22:21.555857: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (o. neDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0519 16:22:23.193474 139896863770432 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.256151 139896863770432 make_examples_core.py:257] Task 10/32: Preparing inputs. I0519 16:22:23.258605 139896863770432 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.259495 139896863770432 make_examples_core.py:257] Task 10/32: Common contigs are ['chr20']. I0519 16:22:23.239336 140148036429632 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.192739 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:8516,energy efficiency,CPU,CPU,8516,"in zhoujianglin 6274909010 Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 106669 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6924 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 10196727247 Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 2813 May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 784363628 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```. **************. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-05-19 16:22:21.555857: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (o. neDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0519 16:22:23.193474 139896863770432 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.256151 139896863770432 make_examples_core.py:257] Task 10/32: Preparing inputs. I0519 16:22:23.258605 139896863770432 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.259495 139896863770432 make_examples_core.py:257] Task 10/32: Common contigs are ['chr20']. I0519 16:22:23.239336 140148036429632 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.192739 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.235120 140421750466368 make_examples_c",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:12204,energy efficiency,model,modeling,12204,"r. I0519 16:22:23.234176 139704511100736 make_examples_core.py:257] Task 12/32: Preparing inputs. I0519 16:22:23.236589 139704511100736 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.237330 139704511100736 make_examples_core.py:257] Task 12/32: Common contigs are ['chr20']. I0519 16:22:23.237694 140638923466560 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.289134 140638923466560 make_examples_core.py:257] Task 29/32: Preparing inputs. I0519 16:22:23.215111 139798323177280 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.265897 139798323177280 make_examples_core.py:257] Task 9/32: Preparing inputs. **********. I0519 16:22:46.781010 139665862911808 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0519 16:22:47.119282 139665862911808 modeling.py:418] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt. I0519 16:22:47.119841 139665862911808 saver.py:1410] Restoring parameters from /opt/models/wgs/model.ckpt. I0519 16:22:48.504039 139665862911808 call_variants.py:462] Processed 1 examples in 1 batches [632.440 sec per 100]. I0519 16:22:48.735930 139665862911808 call_variants.py:468] Processed 305 examples in 1 batches [2.084 sec per 100]. I0519 16:22:48.736088 139665862911808 call_variants.py:471] Done calling variants from a total of 305 examples. real 0m8.934s. user 0m37.643s. sys 0m6.426s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --infile ""/tmp/tmp6gzkras0/call_variants_output.tfrecord.gz"" --outfile ""singularity-output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp6gzkras0/gvcf.tfrecord@32.gz"" --gvcf_outfile ""singularity-output/output.g.vcf.gz"". 2023-05-19 16:22:49.638487: I ten",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:12285,energy efficiency,model,models,12285,"Preparing inputs. I0519 16:22:23.236589 139704511100736 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.237330 139704511100736 make_examples_core.py:257] Task 12/32: Common contigs are ['chr20']. I0519 16:22:23.237694 140638923466560 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.289134 140638923466560 make_examples_core.py:257] Task 29/32: Preparing inputs. I0519 16:22:23.215111 139798323177280 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.265897 139798323177280 make_examples_core.py:257] Task 9/32: Preparing inputs. **********. I0519 16:22:46.781010 139665862911808 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0519 16:22:47.119282 139665862911808 modeling.py:418] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt. I0519 16:22:47.119841 139665862911808 saver.py:1410] Restoring parameters from /opt/models/wgs/model.ckpt. I0519 16:22:48.504039 139665862911808 call_variants.py:462] Processed 1 examples in 1 batches [632.440 sec per 100]. I0519 16:22:48.735930 139665862911808 call_variants.py:468] Processed 305 examples in 1 batches [2.084 sec per 100]. I0519 16:22:48.736088 139665862911808 call_variants.py:471] Done calling variants from a total of 305 examples. real 0m8.934s. user 0m37.643s. sys 0m6.426s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --infile ""/tmp/tmp6gzkras0/call_variants_output.tfrecord.gz"" --outfile ""singularity-output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp6gzkras0/gvcf.tfrecord@32.gz"" --gvcf_outfile ""singularity-output/output.g.vcf.gz"". 2023-05-19 16:22:49.638487: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimi",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:12296,energy efficiency,model,model,12296,"nputs. I0519 16:22:23.236589 139704511100736 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.237330 139704511100736 make_examples_core.py:257] Task 12/32: Common contigs are ['chr20']. I0519 16:22:23.237694 140638923466560 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.289134 140638923466560 make_examples_core.py:257] Task 29/32: Preparing inputs. I0519 16:22:23.215111 139798323177280 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.265897 139798323177280 make_examples_core.py:257] Task 9/32: Preparing inputs. **********. I0519 16:22:46.781010 139665862911808 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0519 16:22:47.119282 139665862911808 modeling.py:418] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt. I0519 16:22:47.119841 139665862911808 saver.py:1410] Restoring parameters from /opt/models/wgs/model.ckpt. I0519 16:22:48.504039 139665862911808 call_variants.py:462] Processed 1 examples in 1 batches [632.440 sec per 100]. I0519 16:22:48.735930 139665862911808 call_variants.py:468] Processed 305 examples in 1 batches [2.084 sec per 100]. I0519 16:22:48.736088 139665862911808 call_variants.py:471] Done calling variants from a total of 305 examples. real 0m8.934s. user 0m37.643s. sys 0m6.426s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --infile ""/tmp/tmp6gzkras0/call_variants_output.tfrecord.gz"" --outfile ""singularity-output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp6gzkras0/gvcf.tfrecord@32.gz"" --gvcf_outfile ""singularity-output/output.g.vcf.gz"". 2023-05-19 16:22:49.638487: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with on",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:12392,energy efficiency,model,models,12392,"/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.237330 139704511100736 make_examples_core.py:257] Task 12/32: Common contigs are ['chr20']. I0519 16:22:23.237694 140638923466560 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.289134 140638923466560 make_examples_core.py:257] Task 29/32: Preparing inputs. I0519 16:22:23.215111 139798323177280 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.265897 139798323177280 make_examples_core.py:257] Task 9/32: Preparing inputs. **********. I0519 16:22:46.781010 139665862911808 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0519 16:22:47.119282 139665862911808 modeling.py:418] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt. I0519 16:22:47.119841 139665862911808 saver.py:1410] Restoring parameters from /opt/models/wgs/model.ckpt. I0519 16:22:48.504039 139665862911808 call_variants.py:462] Processed 1 examples in 1 batches [632.440 sec per 100]. I0519 16:22:48.735930 139665862911808 call_variants.py:468] Processed 305 examples in 1 batches [2.084 sec per 100]. I0519 16:22:48.736088 139665862911808 call_variants.py:471] Done calling variants from a total of 305 examples. real 0m8.934s. user 0m37.643s. sys 0m6.426s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --infile ""/tmp/tmp6gzkras0/call_variants_output.tfrecord.gz"" --outfile ""singularity-output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp6gzkras0/gvcf.tfrecord@32.gz"" --gvcf_outfile ""singularity-output/output.g.vcf.gz"". 2023-05-19 16:22:49.638487: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-c",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:12403,energy efficiency,model,model,12403,".chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.237330 139704511100736 make_examples_core.py:257] Task 12/32: Common contigs are ['chr20']. I0519 16:22:23.237694 140638923466560 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.289134 140638923466560 make_examples_core.py:257] Task 29/32: Preparing inputs. I0519 16:22:23.215111 139798323177280 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.265897 139798323177280 make_examples_core.py:257] Task 9/32: Preparing inputs. **********. I0519 16:22:46.781010 139665862911808 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0519 16:22:47.119282 139665862911808 modeling.py:418] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt. I0519 16:22:47.119841 139665862911808 saver.py:1410] Restoring parameters from /opt/models/wgs/model.ckpt. I0519 16:22:48.504039 139665862911808 call_variants.py:462] Processed 1 examples in 1 batches [632.440 sec per 100]. I0519 16:22:48.735930 139665862911808 call_variants.py:468] Processed 305 examples in 1 batches [2.084 sec per 100]. I0519 16:22:48.736088 139665862911808 call_variants.py:471] Done calling variants from a total of 305 examples. real 0m8.934s. user 0m37.643s. sys 0m6.426s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --infile ""/tmp/tmp6gzkras0/call_variants_output.tfrecord.gz"" --outfile ""singularity-output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp6gzkras0/gvcf.tfrecord@32.gz"" --gvcf_outfile ""singularity-output/output.g.vcf.gz"". 2023-05-19 16:22:49.638487: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical ope",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:13216,energy efficiency,core,core,13216,"8] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt. I0519 16:22:47.119841 139665862911808 saver.py:1410] Restoring parameters from /opt/models/wgs/model.ckpt. I0519 16:22:48.504039 139665862911808 call_variants.py:462] Processed 1 examples in 1 batches [632.440 sec per 100]. I0519 16:22:48.735930 139665862911808 call_variants.py:468] Processed 305 examples in 1 batches [2.084 sec per 100]. I0519 16:22:48.736088 139665862911808 call_variants.py:471] Done calling variants from a total of 305 examples. real 0m8.934s. user 0m37.643s. sys 0m6.426s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --infile ""/tmp/tmp6gzkras0/call_variants_output.tfrecord.gz"" --outfile ""singularity-output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp6gzkras0/gvcf.tfrecord@32.gz"" --gvcf_outfile ""singularity-output/output.g.vcf.gz"". 2023-05-19 16:22:49.638487: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0519 16:22:51.170790 140640470185792 postprocess_variants.py:972] Using sample name from call_variants output. Sample name: NA12878. 2023-05-19 16:22:51.171487: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/tmp6gzkras0/call_variants_output.tfrecord.gz. 2023-05-19 16:22:51.173038: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 305. I0519 16:22:51.173760 140640470185792 postprocess_variants.py:1037] CVO sorting took 3.902912139892578e-05 minutes. I0519 16:22:51.173915 140640470185792 postprocess_variants.py:1040] Transforming call_variants_output to variants. I0519 16:22:51.204555 140640470185792 postprocess_variants.py:1080] Pr",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:13282,energy efficiency,optim,optimized,13282,"dels/wgs/model.ckpt. I0519 16:22:47.119841 139665862911808 saver.py:1410] Restoring parameters from /opt/models/wgs/model.ckpt. I0519 16:22:48.504039 139665862911808 call_variants.py:462] Processed 1 examples in 1 batches [632.440 sec per 100]. I0519 16:22:48.735930 139665862911808 call_variants.py:468] Processed 305 examples in 1 batches [2.084 sec per 100]. I0519 16:22:48.736088 139665862911808 call_variants.py:471] Done calling variants from a total of 305 examples. real 0m8.934s. user 0m37.643s. sys 0m6.426s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --infile ""/tmp/tmp6gzkras0/call_variants_output.tfrecord.gz"" --outfile ""singularity-output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp6gzkras0/gvcf.tfrecord@32.gz"" --gvcf_outfile ""singularity-output/output.g.vcf.gz"". 2023-05-19 16:22:49.638487: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0519 16:22:51.170790 140640470185792 postprocess_variants.py:972] Using sample name from call_variants output. Sample name: NA12878. 2023-05-19 16:22:51.171487: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/tmp6gzkras0/call_variants_output.tfrecord.gz. 2023-05-19 16:22:51.173038: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 305. I0519 16:22:51.173760 140640470185792 postprocess_variants.py:1037] CVO sorting took 3.902912139892578e-05 minutes. I0519 16:22:51.173915 140640470185792 postprocess_variants.py:1040] Transforming call_variants_output to variants. I0519 16:22:51.204555 140640470185792 postprocess_variants.py:1080] Processing variants (and writing to temporary file) took 0.000507887204",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:13362,energy efficiency,CPU,CPU,13362,"toring parameters from /opt/models/wgs/model.ckpt. I0519 16:22:48.504039 139665862911808 call_variants.py:462] Processed 1 examples in 1 batches [632.440 sec per 100]. I0519 16:22:48.735930 139665862911808 call_variants.py:468] Processed 305 examples in 1 batches [2.084 sec per 100]. I0519 16:22:48.736088 139665862911808 call_variants.py:471] Done calling variants from a total of 305 examples. real 0m8.934s. user 0m37.643s. sys 0m6.426s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --infile ""/tmp/tmp6gzkras0/call_variants_output.tfrecord.gz"" --outfile ""singularity-output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp6gzkras0/gvcf.tfrecord@32.gz"" --gvcf_outfile ""singularity-output/output.g.vcf.gz"". 2023-05-19 16:22:49.638487: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0519 16:22:51.170790 140640470185792 postprocess_variants.py:972] Using sample name from call_variants output. Sample name: NA12878. 2023-05-19 16:22:51.171487: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/tmp6gzkras0/call_variants_output.tfrecord.gz. 2023-05-19 16:22:51.173038: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 305. I0519 16:22:51.173760 140640470185792 postprocess_variants.py:1037] CVO sorting took 3.902912139892578e-05 minutes. I0519 16:22:51.173915 140640470185792 postprocess_variants.py:1040] Transforming call_variants_output to variants. I0519 16:22:51.204555 140640470185792 postprocess_variants.py:1080] Processing variants (and writing to temporary file) took 0.0005078872044881184 minutes. I0519 16:22:51.582558 140640470185792 postprocess_variants.p",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:710,integrability,version,version,710,"DeepVariant Runerror, ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). Hi developers,. I'd like to run `DeepVariant` for my `WGS` sequencing data. My sequencing data were from `BGI` platform and were preprocessed by `fastp, bwa+Hs37d5, MarkDuplicatesSpark`. I tried to use the 'sorted and deduplicated bam' file as input for `DeepVariant` in `singularity` mode. However, I always encountered the 'reference index' `not found` error. But my `reference` fasta file and `reference index` fai file does exist. Could you please help me figure it out? **Setup**. - Operating system: Linux version 3.10.0-1127.el7.x86_64 (gcc version 4.8.5 20150623 (Red Hat 4.8.5-39), Computation Node (one node of Clusters). - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.):. - ``` BIN_VERSION=""1.5.0"". docker pull; . singularity pull docker://google/deepvariant:""${BIN_VERSION}"". singularity build --fakeroot deepvariant.sif docker://google/deepvariant:1.5.0```. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). `BGI platform, WGS data, Hs37d5 reference, fastp QC, bwa-mem2 mapping, MarkDuplicatesSpark sort & dedup`. . **Steps to reproduce:**. - Command:. - 1. singularity run /lustre/Data/toolsDB//deepvariant.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=$ref_idx --reads=$dedupbam --output_vcf=$vcfout --output_gvcf=$gvcfout --num_shards=32 >$logx 2>&1. - Error trace: (if applicable). - ```I0522 08:40:36.823651 140633630893888 genomics_reader.py:222] Reading /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam with NativeSamReader. I0522 08:40:36.846348 140633630893888 make_examples_core.py:257] Task 27/32: Preparing inputs. [E::fai_load3_core] Failed to open FASTA index /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:746,integrability,version,version,746,"DeepVariant Runerror, ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). Hi developers,. I'd like to run `DeepVariant` for my `WGS` sequencing data. My sequencing data were from `BGI` platform and were preprocessed by `fastp, bwa+Hs37d5, MarkDuplicatesSpark`. I tried to use the 'sorted and deduplicated bam' file as input for `DeepVariant` in `singularity` mode. However, I always encountered the 'reference index' `not found` error. But my `reference` fasta file and `reference index` fai file does exist. Could you please help me figure it out? **Setup**. - Operating system: Linux version 3.10.0-1127.el7.x86_64 (gcc version 4.8.5 20150623 (Red Hat 4.8.5-39), Computation Node (one node of Clusters). - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.):. - ``` BIN_VERSION=""1.5.0"". docker pull; . singularity pull docker://google/deepvariant:""${BIN_VERSION}"". singularity build --fakeroot deepvariant.sif docker://google/deepvariant:1.5.0```. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). `BGI platform, WGS data, Hs37d5 reference, fastp QC, bwa-mem2 mapping, MarkDuplicatesSpark sort & dedup`. . **Steps to reproduce:**. - Command:. - 1. singularity run /lustre/Data/toolsDB//deepvariant.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=$ref_idx --reads=$dedupbam --output_vcf=$vcfout --output_gvcf=$gvcfout --num_shards=32 >$logx 2>&1. - Error trace: (if applicable). - ```I0522 08:40:36.823651 140633630893888 genomics_reader.py:222] Reading /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam with NativeSamReader. I0522 08:40:36.846348 140633630893888 make_examples_core.py:257] Task 27/32: Preparing inputs. [E::fai_load3_core] Failed to open FASTA index /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:844,integrability,version,version,844,"DeepVariant Runerror, ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). Hi developers,. I'd like to run `DeepVariant` for my `WGS` sequencing data. My sequencing data were from `BGI` platform and were preprocessed by `fastp, bwa+Hs37d5, MarkDuplicatesSpark`. I tried to use the 'sorted and deduplicated bam' file as input for `DeepVariant` in `singularity` mode. However, I always encountered the 'reference index' `not found` error. But my `reference` fasta file and `reference index` fai file does exist. Could you please help me figure it out? **Setup**. - Operating system: Linux version 3.10.0-1127.el7.x86_64 (gcc version 4.8.5 20150623 (Red Hat 4.8.5-39), Computation Node (one node of Clusters). - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.):. - ``` BIN_VERSION=""1.5.0"". docker pull; . singularity pull docker://google/deepvariant:""${BIN_VERSION}"". singularity build --fakeroot deepvariant.sif docker://google/deepvariant:1.5.0```. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). `BGI platform, WGS data, Hs37d5 reference, fastp QC, bwa-mem2 mapping, MarkDuplicatesSpark sort & dedup`. . **Steps to reproduce:**. - Command:. - 1. singularity run /lustre/Data/toolsDB//deepvariant.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=$ref_idx --reads=$dedupbam --output_vcf=$vcfout --output_gvcf=$gvcfout --num_shards=32 >$logx 2>&1. - Error trace: (if applicable). - ```I0522 08:40:36.823651 140633630893888 genomics_reader.py:222] Reading /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam with NativeSamReader. I0522 08:40:36.846348 140633630893888 make_examples_core.py:257] Task 27/32: Preparing inputs. [E::fai_load3_core] Failed to open FASTA index /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:12501,integrability,batch,batches,12501,".py:257] Task 12/32: Common contigs are ['chr20']. I0519 16:22:23.237694 140638923466560 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.289134 140638923466560 make_examples_core.py:257] Task 29/32: Preparing inputs. I0519 16:22:23.215111 139798323177280 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.265897 139798323177280 make_examples_core.py:257] Task 9/32: Preparing inputs. **********. I0519 16:22:46.781010 139665862911808 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0519 16:22:47.119282 139665862911808 modeling.py:418] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt. I0519 16:22:47.119841 139665862911808 saver.py:1410] Restoring parameters from /opt/models/wgs/model.ckpt. I0519 16:22:48.504039 139665862911808 call_variants.py:462] Processed 1 examples in 1 batches [632.440 sec per 100]. I0519 16:22:48.735930 139665862911808 call_variants.py:468] Processed 305 examples in 1 batches [2.084 sec per 100]. I0519 16:22:48.736088 139665862911808 call_variants.py:471] Done calling variants from a total of 305 examples. real 0m8.934s. user 0m37.643s. sys 0m6.426s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --infile ""/tmp/tmp6gzkras0/call_variants_output.tfrecord.gz"" --outfile ""singularity-output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp6gzkras0/gvcf.tfrecord@32.gz"" --gvcf_outfile ""singularity-output/output.g.vcf.gz"". 2023-05-19 16:22:49.638487: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:12620,integrability,batch,batches,12620,"g quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.289134 140638923466560 make_examples_core.py:257] Task 29/32: Preparing inputs. I0519 16:22:23.215111 139798323177280 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.265897 139798323177280 make_examples_core.py:257] Task 9/32: Preparing inputs. **********. I0519 16:22:46.781010 139665862911808 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0519 16:22:47.119282 139665862911808 modeling.py:418] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt. I0519 16:22:47.119841 139665862911808 saver.py:1410] Restoring parameters from /opt/models/wgs/model.ckpt. I0519 16:22:48.504039 139665862911808 call_variants.py:462] Processed 1 examples in 1 batches [632.440 sec per 100]. I0519 16:22:48.735930 139665862911808 call_variants.py:468] Processed 305 examples in 1 batches [2.084 sec per 100]. I0519 16:22:48.736088 139665862911808 call_variants.py:471] Done calling variants from a total of 305 examples. real 0m8.934s. user 0m37.643s. sys 0m6.426s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --infile ""/tmp/tmp6gzkras0/call_variants_output.tfrecord.gz"" --outfile ""singularity-output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp6gzkras0/gvcf.tfrecord@32.gz"" --gvcf_outfile ""singularity-output/output.g.vcf.gz"". 2023-05-19 16:22:49.638487: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0519 16:22:51.170790 140640470185792 postprocess_variants.py:972] Using sample name fr",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:14101,integrability,Transform,Transforming,14101,"put.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp6gzkras0/gvcf.tfrecord@32.gz"" --gvcf_outfile ""singularity-output/output.g.vcf.gz"". 2023-05-19 16:22:49.638487: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0519 16:22:51.170790 140640470185792 postprocess_variants.py:972] Using sample name from call_variants output. Sample name: NA12878. 2023-05-19 16:22:51.171487: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/tmp6gzkras0/call_variants_output.tfrecord.gz. 2023-05-19 16:22:51.173038: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 305. I0519 16:22:51.173760 140640470185792 postprocess_variants.py:1037] CVO sorting took 3.902912139892578e-05 minutes. I0519 16:22:51.173915 140640470185792 postprocess_variants.py:1040] Transforming call_variants_output to variants. I0519 16:22:51.204555 140640470185792 postprocess_variants.py:1080] Processing variants (and writing to temporary file) took 0.0005078872044881184 minutes. I0519 16:22:51.582558 140640470185792 postprocess_variants.py:1093] Finished writing VCF and gVCF in 0.006296555201212565 minutes. I0519 16:22:51.587275 140640470185792 genomics_reader.py:222] Reading singularity-output/output.vcf.gz with NativeVcfReader. I0519 16:22:51.867608 140640470185792 postprocess_variants.py:1104] Generating VCF stats took 0.004742777347564698 minutes. real 0m3.071s. user 0m5.901s. sys 0m4.455s. INFO: Cleaning up image... ######. $ $ ls singularity-output/. output.g.vcf.gz output.vcf.gz output.visual_report.html srun_output.g.vcf.gz.tbi srun_output.vcf.gz.tbi. output.g.vcf.gz.tbi output.vcf.gz.tbi srun_output.g.vcf.gz srun_output.vcf.gz srun_output.visual_report.html. ```. **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:309,interoperability,platform,platform,309,"DeepVariant Runerror, ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). Hi developers,. I'd like to run `DeepVariant` for my `WGS` sequencing data. My sequencing data were from `BGI` platform and were preprocessed by `fastp, bwa+Hs37d5, MarkDuplicatesSpark`. I tried to use the 'sorted and deduplicated bam' file as input for `DeepVariant` in `singularity` mode. However, I always encountered the 'reference index' `not found` error. But my `reference` fasta file and `reference index` fai file does exist. Could you please help me figure it out? **Setup**. - Operating system: Linux version 3.10.0-1127.el7.x86_64 (gcc version 4.8.5 20150623 (Red Hat 4.8.5-39), Computation Node (one node of Clusters). - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.):. - ``` BIN_VERSION=""1.5.0"". docker pull; . singularity pull docker://google/deepvariant:""${BIN_VERSION}"". singularity build --fakeroot deepvariant.sif docker://google/deepvariant:1.5.0```. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). `BGI platform, WGS data, Hs37d5 reference, fastp QC, bwa-mem2 mapping, MarkDuplicatesSpark sort & dedup`. . **Steps to reproduce:**. - Command:. - 1. singularity run /lustre/Data/toolsDB//deepvariant.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=$ref_idx --reads=$dedupbam --output_vcf=$vcfout --output_gvcf=$gvcfout --num_shards=32 >$logx 2>&1. - Error trace: (if applicable). - ```I0522 08:40:36.823651 140633630893888 genomics_reader.py:222] Reading /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam with NativeSamReader. I0522 08:40:36.846348 140633630893888 make_examples_core.py:257] Task 27/32: Preparing inputs. [E::fai_load3_core] Failed to open FASTA index /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:1221,interoperability,platform,platform,1221," run `DeepVariant` for my `WGS` sequencing data. My sequencing data were from `BGI` platform and were preprocessed by `fastp, bwa+Hs37d5, MarkDuplicatesSpark`. I tried to use the 'sorted and deduplicated bam' file as input for `DeepVariant` in `singularity` mode. However, I always encountered the 'reference index' `not found` error. But my `reference` fasta file and `reference index` fai file does exist. Could you please help me figure it out? **Setup**. - Operating system: Linux version 3.10.0-1127.el7.x86_64 (gcc version 4.8.5 20150623 (Red Hat 4.8.5-39), Computation Node (one node of Clusters). - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.):. - ``` BIN_VERSION=""1.5.0"". docker pull; . singularity pull docker://google/deepvariant:""${BIN_VERSION}"". singularity build --fakeroot deepvariant.sif docker://google/deepvariant:1.5.0```. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). `BGI platform, WGS data, Hs37d5 reference, fastp QC, bwa-mem2 mapping, MarkDuplicatesSpark sort & dedup`. . **Steps to reproduce:**. - Command:. - 1. singularity run /lustre/Data/toolsDB//deepvariant.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=$ref_idx --reads=$dedupbam --output_vcf=$vcfout --output_gvcf=$gvcfout --num_shards=32 >$logx 2>&1. - Error trace: (if applicable). - ```I0522 08:40:36.823651 140633630893888 genomics_reader.py:222] Reading /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam with NativeSamReader. I0522 08:40:36.846348 140633630893888 make_examples_core.py:257] Task 27/32: Preparing inputs. [E::fai_load3_core] Failed to open FASTA index /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai: No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:8373,interoperability,platform,platform,8373," -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467 Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909010 Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 106669 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6924 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 10196727247 Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 2813 May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 784363628 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```. **************. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-05-19 16:22:21.555857: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (o. neDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0519 16:22:23.193474 139896863770432 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.256151 139896863770432 make_examples_core.py:257] Task 10/32: Preparing inputs. I0519 16:22:23.258605 139896863770432 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.259495 139896863770432 make_examples_core.py:257] Task 10/32: Common contigs are ['chr20']. I0519 16:22:23.239336 140148036429632 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.192739 140421750466368 genomics_reader.p",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:13221,interoperability,platform,platform,13221,"ading EMA... INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt. I0519 16:22:47.119841 139665862911808 saver.py:1410] Restoring parameters from /opt/models/wgs/model.ckpt. I0519 16:22:48.504039 139665862911808 call_variants.py:462] Processed 1 examples in 1 batches [632.440 sec per 100]. I0519 16:22:48.735930 139665862911808 call_variants.py:468] Processed 305 examples in 1 batches [2.084 sec per 100]. I0519 16:22:48.736088 139665862911808 call_variants.py:471] Done calling variants from a total of 305 examples. real 0m8.934s. user 0m37.643s. sys 0m6.426s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --infile ""/tmp/tmp6gzkras0/call_variants_output.tfrecord.gz"" --outfile ""singularity-output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp6gzkras0/gvcf.tfrecord@32.gz"" --gvcf_outfile ""singularity-output/output.g.vcf.gz"". 2023-05-19 16:22:49.638487: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0519 16:22:51.170790 140640470185792 postprocess_variants.py:972] Using sample name from call_variants output. Sample name: NA12878. 2023-05-19 16:22:51.171487: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/tmp6gzkras0/call_variants_output.tfrecord.gz. 2023-05-19 16:22:51.173038: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 305. I0519 16:22:51.173760 140640470185792 postprocess_variants.py:1037] CVO sorting took 3.902912139892578e-05 minutes. I0519 16:22:51.173915 140640470185792 postprocess_variants.py:1040] Transforming call_variants_output to variants. I0519 16:22:51.204555 140640470185792 postprocess_variants.py:1080] Processin",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:14101,interoperability,Transform,Transforming,14101,"put.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp6gzkras0/gvcf.tfrecord@32.gz"" --gvcf_outfile ""singularity-output/output.g.vcf.gz"". 2023-05-19 16:22:49.638487: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0519 16:22:51.170790 140640470185792 postprocess_variants.py:972] Using sample name from call_variants output. Sample name: NA12878. 2023-05-19 16:22:51.171487: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/tmp6gzkras0/call_variants_output.tfrecord.gz. 2023-05-19 16:22:51.173038: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 305. I0519 16:22:51.173760 140640470185792 postprocess_variants.py:1037] CVO sorting took 3.902912139892578e-05 minutes. I0519 16:22:51.173915 140640470185792 postprocess_variants.py:1040] Transforming call_variants_output to variants. I0519 16:22:51.204555 140640470185792 postprocess_variants.py:1080] Processing variants (and writing to temporary file) took 0.0005078872044881184 minutes. I0519 16:22:51.582558 140640470185792 postprocess_variants.py:1093] Finished writing VCF and gVCF in 0.006296555201212565 minutes. I0519 16:22:51.587275 140640470185792 genomics_reader.py:222] Reading singularity-output/output.vcf.gz with NativeVcfReader. I0519 16:22:51.867608 140640470185792 postprocess_variants.py:1104] Generating VCF stats took 0.004742777347564698 minutes. real 0m3.071s. user 0m5.901s. sys 0m4.455s. INFO: Cleaning up image... ######. $ $ ls singularity-output/. output.g.vcf.gz output.vcf.gz output.visual_report.html srun_output.g.vcf.gz.tbi srun_output.vcf.gz.tbi. output.g.vcf.gz.tbi output.vcf.gz.tbi srun_output.g.vcf.gz srun_output.vcf.gz srun_output.visual_report.html. ```. **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:710,modifiability,version,version,710,"DeepVariant Runerror, ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). Hi developers,. I'd like to run `DeepVariant` for my `WGS` sequencing data. My sequencing data were from `BGI` platform and were preprocessed by `fastp, bwa+Hs37d5, MarkDuplicatesSpark`. I tried to use the 'sorted and deduplicated bam' file as input for `DeepVariant` in `singularity` mode. However, I always encountered the 'reference index' `not found` error. But my `reference` fasta file and `reference index` fai file does exist. Could you please help me figure it out? **Setup**. - Operating system: Linux version 3.10.0-1127.el7.x86_64 (gcc version 4.8.5 20150623 (Red Hat 4.8.5-39), Computation Node (one node of Clusters). - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.):. - ``` BIN_VERSION=""1.5.0"". docker pull; . singularity pull docker://google/deepvariant:""${BIN_VERSION}"". singularity build --fakeroot deepvariant.sif docker://google/deepvariant:1.5.0```. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). `BGI platform, WGS data, Hs37d5 reference, fastp QC, bwa-mem2 mapping, MarkDuplicatesSpark sort & dedup`. . **Steps to reproduce:**. - Command:. - 1. singularity run /lustre/Data/toolsDB//deepvariant.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=$ref_idx --reads=$dedupbam --output_vcf=$vcfout --output_gvcf=$gvcfout --num_shards=32 >$logx 2>&1. - Error trace: (if applicable). - ```I0522 08:40:36.823651 140633630893888 genomics_reader.py:222] Reading /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam with NativeSamReader. I0522 08:40:36.846348 140633630893888 make_examples_core.py:257] Task 27/32: Preparing inputs. [E::fai_load3_core] Failed to open FASTA index /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:746,modifiability,version,version,746,"DeepVariant Runerror, ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). Hi developers,. I'd like to run `DeepVariant` for my `WGS` sequencing data. My sequencing data were from `BGI` platform and were preprocessed by `fastp, bwa+Hs37d5, MarkDuplicatesSpark`. I tried to use the 'sorted and deduplicated bam' file as input for `DeepVariant` in `singularity` mode. However, I always encountered the 'reference index' `not found` error. But my `reference` fasta file and `reference index` fai file does exist. Could you please help me figure it out? **Setup**. - Operating system: Linux version 3.10.0-1127.el7.x86_64 (gcc version 4.8.5 20150623 (Red Hat 4.8.5-39), Computation Node (one node of Clusters). - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.):. - ``` BIN_VERSION=""1.5.0"". docker pull; . singularity pull docker://google/deepvariant:""${BIN_VERSION}"". singularity build --fakeroot deepvariant.sif docker://google/deepvariant:1.5.0```. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). `BGI platform, WGS data, Hs37d5 reference, fastp QC, bwa-mem2 mapping, MarkDuplicatesSpark sort & dedup`. . **Steps to reproduce:**. - Command:. - 1. singularity run /lustre/Data/toolsDB//deepvariant.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=$ref_idx --reads=$dedupbam --output_vcf=$vcfout --output_gvcf=$gvcfout --num_shards=32 >$logx 2>&1. - Error trace: (if applicable). - ```I0522 08:40:36.823651 140633630893888 genomics_reader.py:222] Reading /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam with NativeSamReader. I0522 08:40:36.846348 140633630893888 make_examples_core.py:257] Task 27/32: Preparing inputs. [E::fai_load3_core] Failed to open FASTA index /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:844,modifiability,version,version,844,"DeepVariant Runerror, ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). Hi developers,. I'd like to run `DeepVariant` for my `WGS` sequencing data. My sequencing data were from `BGI` platform and were preprocessed by `fastp, bwa+Hs37d5, MarkDuplicatesSpark`. I tried to use the 'sorted and deduplicated bam' file as input for `DeepVariant` in `singularity` mode. However, I always encountered the 'reference index' `not found` error. But my `reference` fasta file and `reference index` fai file does exist. Could you please help me figure it out? **Setup**. - Operating system: Linux version 3.10.0-1127.el7.x86_64 (gcc version 4.8.5 20150623 (Red Hat 4.8.5-39), Computation Node (one node of Clusters). - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.):. - ``` BIN_VERSION=""1.5.0"". docker pull; . singularity pull docker://google/deepvariant:""${BIN_VERSION}"". singularity build --fakeroot deepvariant.sif docker://google/deepvariant:1.5.0```. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). `BGI platform, WGS data, Hs37d5 reference, fastp QC, bwa-mem2 mapping, MarkDuplicatesSpark sort & dedup`. . **Steps to reproduce:**. - Command:. - 1. singularity run /lustre/Data/toolsDB//deepvariant.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=$ref_idx --reads=$dedupbam --output_vcf=$vcfout --output_gvcf=$gvcfout --num_shards=32 >$logx 2>&1. - Error trace: (if applicable). - ```I0522 08:40:36.823651 140633630893888 genomics_reader.py:222] Reading /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam with NativeSamReader. I0522 08:40:36.846348 140633630893888 make_examples_core.py:257] Task 27/32: Preparing inputs. [E::fai_load3_core] Failed to open FASTA index /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:2184,modifiability,modul,module,2184,"s unlike the case studies?). `BGI platform, WGS data, Hs37d5 reference, fastp QC, bwa-mem2 mapping, MarkDuplicatesSpark sort & dedup`. . **Steps to reproduce:**. - Command:. - 1. singularity run /lustre/Data/toolsDB//deepvariant.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=$ref_idx --reads=$dedupbam --output_vcf=$vcfout --output_gvcf=$gvcfout --num_shards=32 >$logx 2>&1. - Error trace: (if applicable). - ```I0522 08:40:36.823651 140633630893888 genomics_reader.py:222] Reading /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam with NativeSamReader. I0522 08:40:36.846348 140633630893888 make_examples_core.py:257] Task 27/32: Preparing inputs. [E::fai_load3_core] Failed to open FASTA index /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai: No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2114, in make_examples_runner. regions, calling_regions = processing_regions_from_options(options). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2019, in processing_regions_from_options. ref_contigs = fasta.IndexedFastaReader(. File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 106, in __init__. self._reader = reference.IndexedFastaReader.from_file(. ValueError: NO",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:3858,modifiability,modul,module,3858,"variant/deepvariant/make_examples_core.py"", line 2019, in processing_regions_from_options. ref_contigs = fasta.IndexedFastaReader(. File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 106, in __init__. self._reader = reference.IndexedFastaReader.from_file(. ValueError: NOT_FOUND: could not load fasta and/or fai for fasta /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. I0522 08:40:37.063492 139925831092032 genomics_reader.py:222] Reading /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam with NativeSamReader. I0522 08:40:37.081499 139925831092032 make_examples_core.py:257] Task 6/32: Preparing inputs. [E::fai_load3_core] Failed to open FASTA index /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai: No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_luflf_op/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_luflf_op/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_luflf_op/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_luflf_op/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_luflf_op/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2114, in make_examples_runner. regions, calling_regions = processing_regions_from_options(options). File ""/tmp/Bazel.runfiles_luflf_op/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2019, in processing_regions_from_options. ref_contigs = fasta.IndexedFastaReader(. File ""/tmp/Bazel.runfiles_luflf_op/runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 106, in __init__. self._reader = reference.IndexedFastaReader.from_file(. **************",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:5538,modifiability,modul,module,5538,"t/deepvariant/make_examples_core.py"", line 2019, in processing_regions_from_options. ref_contigs = fasta.IndexedFastaReader(. File ""/tmp/Bazel.runfiles_luflf_op/runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 106, in __init__. self._reader = reference.IndexedFastaReader.from_file(. ****************. File ""/tmp/Bazel.runfiles_8r9pgqu5/runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 106, in __init__. self._reader = reference.IndexedFastaReader.from_file(. ValueError: NOT_FOUND: could not load fasta and/or fai for fasta /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. I0522 08:40:37.129531 140035375298368 make_examples_core.py:257] Task 7/32: Preparing inputs. [E::fai_load3_core] Failed to open FASTA index /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai: No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2114, in make_examples_runner. regions, calling_regions = processing_regions_from_options(options). File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2019, in processing_regions_from_options. ref_contigs = fasta.IndexedFastaReader(. File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 106, in __init__. self._reader = reference.IndexedFastaReader.from_file(. ValueError: NO",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:8209,modifiability,pac,pac,8209,"ce the issue by using the quick start? Yes, the quick test run as normal. ```. 3. reference index does. ```$ ls /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467 Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909010 Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 106669 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6924 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 10196727247 Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 2813 May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 784363628 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```. **************. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-05-19 16:22:21.555857: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (o. neDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0519 16:22:23.193474 139896863770432 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.256151 139896863770432 make_examples_core.py:257] Task 10/32: Preparing inputs. I0519 16:22:23.258605 139896863770432 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.259495 139896863770432 make_examples_core.py:257] Task 10/32: Common contigs are ['chr20']. I0519 16:22:23.239336 1401480364",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:12264,modifiability,paramet,parameters,12264,"y:257] Task 12/32: Preparing inputs. I0519 16:22:23.236589 139704511100736 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.237330 139704511100736 make_examples_core.py:257] Task 12/32: Common contigs are ['chr20']. I0519 16:22:23.237694 140638923466560 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.289134 140638923466560 make_examples_core.py:257] Task 29/32: Preparing inputs. I0519 16:22:23.215111 139798323177280 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.265897 139798323177280 make_examples_core.py:257] Task 9/32: Preparing inputs. **********. I0519 16:22:46.781010 139665862911808 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0519 16:22:47.119282 139665862911808 modeling.py:418] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt. I0519 16:22:47.119841 139665862911808 saver.py:1410] Restoring parameters from /opt/models/wgs/model.ckpt. I0519 16:22:48.504039 139665862911808 call_variants.py:462] Processed 1 examples in 1 batches [632.440 sec per 100]. I0519 16:22:48.735930 139665862911808 call_variants.py:468] Processed 305 examples in 1 batches [2.084 sec per 100]. I0519 16:22:48.736088 139665862911808 call_variants.py:471] Done calling variants from a total of 305 examples. real 0m8.934s. user 0m37.643s. sys 0m6.426s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --infile ""/tmp/tmp6gzkras0/call_variants_output.tfrecord.gz"" --outfile ""singularity-output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp6gzkras0/gvcf.tfrecord@32.gz"" --gvcf_outfile ""singularity-output/output.g.vcf.gz"". 2023-05-19 16:22:49.638487: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:12371,modifiability,paramet,parameters,12371,"quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.237330 139704511100736 make_examples_core.py:257] Task 12/32: Common contigs are ['chr20']. I0519 16:22:23.237694 140638923466560 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.289134 140638923466560 make_examples_core.py:257] Task 29/32: Preparing inputs. I0519 16:22:23.215111 139798323177280 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.265897 139798323177280 make_examples_core.py:257] Task 9/32: Preparing inputs. **********. I0519 16:22:46.781010 139665862911808 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0519 16:22:47.119282 139665862911808 modeling.py:418] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt. I0519 16:22:47.119841 139665862911808 saver.py:1410] Restoring parameters from /opt/models/wgs/model.ckpt. I0519 16:22:48.504039 139665862911808 call_variants.py:462] Processed 1 examples in 1 batches [632.440 sec per 100]. I0519 16:22:48.735930 139665862911808 call_variants.py:468] Processed 305 examples in 1 batches [2.084 sec per 100]. I0519 16:22:48.736088 139665862911808 call_variants.py:471] Done calling variants from a total of 305 examples. real 0m8.934s. user 0m37.643s. sys 0m6.426s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --infile ""/tmp/tmp6gzkras0/call_variants_output.tfrecord.gz"" --outfile ""singularity-output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp6gzkras0/gvcf.tfrecord@32.gz"" --gvcf_outfile ""singularity-output/output.g.vcf.gz"". 2023-05-19 16:22:49.638487: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructio",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:553,performance,error,error,553,"DeepVariant Runerror, ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). Hi developers,. I'd like to run `DeepVariant` for my `WGS` sequencing data. My sequencing data were from `BGI` platform and were preprocessed by `fastp, bwa+Hs37d5, MarkDuplicatesSpark`. I tried to use the 'sorted and deduplicated bam' file as input for `DeepVariant` in `singularity` mode. However, I always encountered the 'reference index' `not found` error. But my `reference` fasta file and `reference index` fai file does exist. Could you please help me figure it out? **Setup**. - Operating system: Linux version 3.10.0-1127.el7.x86_64 (gcc version 4.8.5 20150623 (Red Hat 4.8.5-39), Computation Node (one node of Clusters). - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.):. - ``` BIN_VERSION=""1.5.0"". docker pull; . singularity pull docker://google/deepvariant:""${BIN_VERSION}"". singularity build --fakeroot deepvariant.sif docker://google/deepvariant:1.5.0```. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). `BGI platform, WGS data, Hs37d5 reference, fastp QC, bwa-mem2 mapping, MarkDuplicatesSpark sort & dedup`. . **Steps to reproduce:**. - Command:. - 1. singularity run /lustre/Data/toolsDB//deepvariant.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=$ref_idx --reads=$dedupbam --output_vcf=$vcfout --output_gvcf=$gvcfout --num_shards=32 >$logx 2>&1. - Error trace: (if applicable). - ```I0522 08:40:36.823651 140633630893888 genomics_reader.py:222] Reading /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam with NativeSamReader. I0522 08:40:36.846348 140633630893888 make_examples_core.py:257] Task 27/32: Preparing inputs. [E::fai_load3_core] Failed to open FASTA index /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:1582,performance,Error,Error,1582,"file and `reference index` fai file does exist. Could you please help me figure it out? **Setup**. - Operating system: Linux version 3.10.0-1127.el7.x86_64 (gcc version 4.8.5 20150623 (Red Hat 4.8.5-39), Computation Node (one node of Clusters). - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.):. - ``` BIN_VERSION=""1.5.0"". docker pull; . singularity pull docker://google/deepvariant:""${BIN_VERSION}"". singularity build --fakeroot deepvariant.sif docker://google/deepvariant:1.5.0```. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). `BGI platform, WGS data, Hs37d5 reference, fastp QC, bwa-mem2 mapping, MarkDuplicatesSpark sort & dedup`. . **Steps to reproduce:**. - Command:. - 1. singularity run /lustre/Data/toolsDB//deepvariant.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=$ref_idx --reads=$dedupbam --output_vcf=$vcfout --output_gvcf=$gvcfout --num_shards=32 >$logx 2>&1. - Error trace: (if applicable). - ```I0522 08:40:36.823651 140633630893888 genomics_reader.py:222] Reading /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam with NativeSamReader. I0522 08:40:36.846348 140633630893888 make_examples_core.py:257] Task 27/32: Preparing inputs. [E::fai_load3_core] Failed to open FASTA index /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai: No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main. make_examples_core.make_examples_runner(o",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:3206,performance,load,load,3206,"File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2114, in make_examples_runner. regions, calling_regions = processing_regions_from_options(options). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2019, in processing_regions_from_options. ref_contigs = fasta.IndexedFastaReader(. File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 106, in __init__. self._reader = reference.IndexedFastaReader.from_file(. ValueError: NOT_FOUND: could not load fasta and/or fai for fasta /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. I0522 08:40:37.063492 139925831092032 genomics_reader.py:222] Reading /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam with NativeSamReader. I0522 08:40:37.081499 139925831092032 make_examples_core.py:257] Task 6/32: Preparing inputs. [E::fai_load3_core] Failed to open FASTA index /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai: No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_luflf_op/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_luflf_op/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_luflf_op/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_luflf_op/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186,",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:5078,performance,load,load,5078,"ys.exit(main(argv)). File ""/tmp/Bazel.runfiles_luflf_op/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_luflf_op/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2114, in make_examples_runner. regions, calling_regions = processing_regions_from_options(options). File ""/tmp/Bazel.runfiles_luflf_op/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2019, in processing_regions_from_options. ref_contigs = fasta.IndexedFastaReader(. File ""/tmp/Bazel.runfiles_luflf_op/runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 106, in __init__. self._reader = reference.IndexedFastaReader.from_file(. ****************. File ""/tmp/Bazel.runfiles_8r9pgqu5/runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 106, in __init__. self._reader = reference.IndexedFastaReader.from_file(. ValueError: NOT_FOUND: could not load fasta and/or fai for fasta /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. I0522 08:40:37.129531 140035375298368 make_examples_core.py:257] Task 7/32: Preparing inputs. [E::fai_load3_core] Failed to open FASTA index /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai: No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2114, in make_examples_r",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:6560,performance,load,load,6560,"File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2114, in make_examples_runner. regions, calling_regions = processing_regions_from_options(options). File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2019, in processing_regions_from_options. ref_contigs = fasta.IndexedFastaReader(. File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 106, in __init__. self._reader = reference.IndexedFastaReader.from_file(. ValueError: NOT_FOUND: could not load fasta and/or fai for fasta /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa --reads /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam --examples /tmp/tmpfab4tpv7/make_examples.tfrecord@32.gz --channels insert_size --gvcf /tmp/tmpfab4tpv7/gvcf.tfrecord@32.gz --task 18. - ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the quick test run as normal. ```. 3. reference index does. ```$ ls /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467 Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909010 Apr 26 15:23 /lus",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:6646,performance,parallel,parallel,6646,"_main(main, args). File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2114, in make_examples_runner. regions, calling_regions = processing_regions_from_options(options). File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2019, in processing_regions_from_options. ref_contigs = fasta.IndexedFastaReader(. File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 106, in __init__. self._reader = reference.IndexedFastaReader.from_file(. ValueError: NOT_FOUND: could not load fasta and/or fai for fasta /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa --reads /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam --examples /tmp/tmpfab4tpv7/make_examples.tfrecord@32.gz --channels insert_size --gvcf /tmp/tmpfab4tpv7/gvcf.tfrecord@32.gz --task 18. - ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the quick test run as normal. ```. 3. reference index does. ```$ ls /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467 Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909010 Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujia",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:8434,performance,optimiz,optimized,8434,"3 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909010 Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 106669 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6924 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 10196727247 Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 2813 May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 784363628 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```. **************. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-05-19 16:22:21.555857: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (o. neDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0519 16:22:23.193474 139896863770432 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.256151 139896863770432 make_examples_core.py:257] Task 10/32: Preparing inputs. I0519 16:22:23.258605 139896863770432 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.259495 139896863770432 make_examples_core.py:257] Task 10/32: Common contigs are ['chr20']. I0519 16:22:23.239336 140148036429632 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.192739 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:8468,performance,Network,Network,8468,"uman_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909010 Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 106669 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6924 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 10196727247 Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 2813 May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 784363628 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```. **************. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-05-19 16:22:21.555857: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (o. neDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0519 16:22:23.193474 139896863770432 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.256151 139896863770432 make_examples_core.py:257] Task 10/32: Preparing inputs. I0519 16:22:23.258605 139896863770432 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.259495 139896863770432 make_examples_core.py:257] Task 10/32: Common contigs are ['chr20']. I0519 16:22:23.239336 140148036429632 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.192739 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 1",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:8516,performance,CPU,CPU,8516,"in zhoujianglin 6274909010 Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 106669 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6924 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 10196727247 Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 2813 May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 784363628 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```. **************. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-05-19 16:22:21.555857: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (o. neDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0519 16:22:23.193474 139896863770432 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.256151 139896863770432 make_examples_core.py:257] Task 10/32: Preparing inputs. I0519 16:22:23.258605 139896863770432 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.259495 139896863770432 make_examples_core.py:257] Task 10/32: Common contigs are ['chr20']. I0519 16:22:23.239336 140148036429632 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.192739 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.235120 140421750466368 make_examples_c",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:8536,performance,perform,performance-critical,8536,"pr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 106669 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6924 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 10196727247 Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 2813 May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 784363628 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```. **************. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-05-19 16:22:21.555857: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (o. neDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0519 16:22:23.193474 139896863770432 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.256151 139896863770432 make_examples_core.py:257] Task 10/32: Preparing inputs. I0519 16:22:23.258605 139896863770432 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.259495 139896863770432 make_examples_core.py:257] Task 10/32: Common contigs are ['chr20']. I0519 16:22:23.239336 140148036429632 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.192739 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.235120 140421750466368 make_examples_core.py:257] Task 21/32: Prep",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:12501,performance,batch,batches,12501,".py:257] Task 12/32: Common contigs are ['chr20']. I0519 16:22:23.237694 140638923466560 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.289134 140638923466560 make_examples_core.py:257] Task 29/32: Preparing inputs. I0519 16:22:23.215111 139798323177280 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.265897 139798323177280 make_examples_core.py:257] Task 9/32: Preparing inputs. **********. I0519 16:22:46.781010 139665862911808 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0519 16:22:47.119282 139665862911808 modeling.py:418] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt. I0519 16:22:47.119841 139665862911808 saver.py:1410] Restoring parameters from /opt/models/wgs/model.ckpt. I0519 16:22:48.504039 139665862911808 call_variants.py:462] Processed 1 examples in 1 batches [632.440 sec per 100]. I0519 16:22:48.735930 139665862911808 call_variants.py:468] Processed 305 examples in 1 batches [2.084 sec per 100]. I0519 16:22:48.736088 139665862911808 call_variants.py:471] Done calling variants from a total of 305 examples. real 0m8.934s. user 0m37.643s. sys 0m6.426s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --infile ""/tmp/tmp6gzkras0/call_variants_output.tfrecord.gz"" --outfile ""singularity-output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp6gzkras0/gvcf.tfrecord@32.gz"" --gvcf_outfile ""singularity-output/output.g.vcf.gz"". 2023-05-19 16:22:49.638487: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:12620,performance,batch,batches,12620,"g quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.289134 140638923466560 make_examples_core.py:257] Task 29/32: Preparing inputs. I0519 16:22:23.215111 139798323177280 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.265897 139798323177280 make_examples_core.py:257] Task 9/32: Preparing inputs. **********. I0519 16:22:46.781010 139665862911808 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0519 16:22:47.119282 139665862911808 modeling.py:418] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt. I0519 16:22:47.119841 139665862911808 saver.py:1410] Restoring parameters from /opt/models/wgs/model.ckpt. I0519 16:22:48.504039 139665862911808 call_variants.py:462] Processed 1 examples in 1 batches [632.440 sec per 100]. I0519 16:22:48.735930 139665862911808 call_variants.py:468] Processed 305 examples in 1 batches [2.084 sec per 100]. I0519 16:22:48.736088 139665862911808 call_variants.py:471] Done calling variants from a total of 305 examples. real 0m8.934s. user 0m37.643s. sys 0m6.426s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --infile ""/tmp/tmp6gzkras0/call_variants_output.tfrecord.gz"" --outfile ""singularity-output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp6gzkras0/gvcf.tfrecord@32.gz"" --gvcf_outfile ""singularity-output/output.g.vcf.gz"". 2023-05-19 16:22:49.638487: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0519 16:22:51.170790 140640470185792 postprocess_variants.py:972] Using sample name fr",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:12839,performance,time,time,12839,"reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.265897 139798323177280 make_examples_core.py:257] Task 9/32: Preparing inputs. **********. I0519 16:22:46.781010 139665862911808 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0519 16:22:47.119282 139665862911808 modeling.py:418] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt. I0519 16:22:47.119841 139665862911808 saver.py:1410] Restoring parameters from /opt/models/wgs/model.ckpt. I0519 16:22:48.504039 139665862911808 call_variants.py:462] Processed 1 examples in 1 batches [632.440 sec per 100]. I0519 16:22:48.735930 139665862911808 call_variants.py:468] Processed 305 examples in 1 batches [2.084 sec per 100]. I0519 16:22:48.736088 139665862911808 call_variants.py:471] Done calling variants from a total of 305 examples. real 0m8.934s. user 0m37.643s. sys 0m6.426s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --infile ""/tmp/tmp6gzkras0/call_variants_output.tfrecord.gz"" --outfile ""singularity-output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp6gzkras0/gvcf.tfrecord@32.gz"" --gvcf_outfile ""singularity-output/output.g.vcf.gz"". 2023-05-19 16:22:49.638487: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0519 16:22:51.170790 140640470185792 postprocess_variants.py:972] Using sample name from call_variants output. Sample name: NA12878. 2023-05-19 16:22:51.171487: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/tmp6gzkras0/call_variants_output.tfrecord.gz. 2023-05-19 16:22:51.173038: I deepvari",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:13282,performance,optimiz,optimized,13282,"dels/wgs/model.ckpt. I0519 16:22:47.119841 139665862911808 saver.py:1410] Restoring parameters from /opt/models/wgs/model.ckpt. I0519 16:22:48.504039 139665862911808 call_variants.py:462] Processed 1 examples in 1 batches [632.440 sec per 100]. I0519 16:22:48.735930 139665862911808 call_variants.py:468] Processed 305 examples in 1 batches [2.084 sec per 100]. I0519 16:22:48.736088 139665862911808 call_variants.py:471] Done calling variants from a total of 305 examples. real 0m8.934s. user 0m37.643s. sys 0m6.426s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --infile ""/tmp/tmp6gzkras0/call_variants_output.tfrecord.gz"" --outfile ""singularity-output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp6gzkras0/gvcf.tfrecord@32.gz"" --gvcf_outfile ""singularity-output/output.g.vcf.gz"". 2023-05-19 16:22:49.638487: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0519 16:22:51.170790 140640470185792 postprocess_variants.py:972] Using sample name from call_variants output. Sample name: NA12878. 2023-05-19 16:22:51.171487: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/tmp6gzkras0/call_variants_output.tfrecord.gz. 2023-05-19 16:22:51.173038: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 305. I0519 16:22:51.173760 140640470185792 postprocess_variants.py:1037] CVO sorting took 3.902912139892578e-05 minutes. I0519 16:22:51.173915 140640470185792 postprocess_variants.py:1040] Transforming call_variants_output to variants. I0519 16:22:51.204555 140640470185792 postprocess_variants.py:1080] Processing variants (and writing to temporary file) took 0.000507887204",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:13316,performance,Network,Network,13316,"47.119841 139665862911808 saver.py:1410] Restoring parameters from /opt/models/wgs/model.ckpt. I0519 16:22:48.504039 139665862911808 call_variants.py:462] Processed 1 examples in 1 batches [632.440 sec per 100]. I0519 16:22:48.735930 139665862911808 call_variants.py:468] Processed 305 examples in 1 batches [2.084 sec per 100]. I0519 16:22:48.736088 139665862911808 call_variants.py:471] Done calling variants from a total of 305 examples. real 0m8.934s. user 0m37.643s. sys 0m6.426s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --infile ""/tmp/tmp6gzkras0/call_variants_output.tfrecord.gz"" --outfile ""singularity-output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp6gzkras0/gvcf.tfrecord@32.gz"" --gvcf_outfile ""singularity-output/output.g.vcf.gz"". 2023-05-19 16:22:49.638487: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0519 16:22:51.170790 140640470185792 postprocess_variants.py:972] Using sample name from call_variants output. Sample name: NA12878. 2023-05-19 16:22:51.171487: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/tmp6gzkras0/call_variants_output.tfrecord.gz. 2023-05-19 16:22:51.173038: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 305. I0519 16:22:51.173760 140640470185792 postprocess_variants.py:1037] CVO sorting took 3.902912139892578e-05 minutes. I0519 16:22:51.173915 140640470185792 postprocess_variants.py:1040] Transforming call_variants_output to variants. I0519 16:22:51.204555 140640470185792 postprocess_variants.py:1080] Processing variants (and writing to temporary file) took 0.0005078872044881184 minutes. I0519 16:22:51.5",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:13362,performance,CPU,CPU,13362,"toring parameters from /opt/models/wgs/model.ckpt. I0519 16:22:48.504039 139665862911808 call_variants.py:462] Processed 1 examples in 1 batches [632.440 sec per 100]. I0519 16:22:48.735930 139665862911808 call_variants.py:468] Processed 305 examples in 1 batches [2.084 sec per 100]. I0519 16:22:48.736088 139665862911808 call_variants.py:471] Done calling variants from a total of 305 examples. real 0m8.934s. user 0m37.643s. sys 0m6.426s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --infile ""/tmp/tmp6gzkras0/call_variants_output.tfrecord.gz"" --outfile ""singularity-output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp6gzkras0/gvcf.tfrecord@32.gz"" --gvcf_outfile ""singularity-output/output.g.vcf.gz"". 2023-05-19 16:22:49.638487: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0519 16:22:51.170790 140640470185792 postprocess_variants.py:972] Using sample name from call_variants output. Sample name: NA12878. 2023-05-19 16:22:51.171487: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/tmp6gzkras0/call_variants_output.tfrecord.gz. 2023-05-19 16:22:51.173038: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 305. I0519 16:22:51.173760 140640470185792 postprocess_variants.py:1037] CVO sorting took 3.902912139892578e-05 minutes. I0519 16:22:51.173915 140640470185792 postprocess_variants.py:1040] Transforming call_variants_output to variants. I0519 16:22:51.204555 140640470185792 postprocess_variants.py:1080] Processing variants (and writing to temporary file) took 0.0005078872044881184 minutes. I0519 16:22:51.582558 140640470185792 postprocess_variants.p",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:13382,performance,perform,performance-critical,13382,"models/wgs/model.ckpt. I0519 16:22:48.504039 139665862911808 call_variants.py:462] Processed 1 examples in 1 batches [632.440 sec per 100]. I0519 16:22:48.735930 139665862911808 call_variants.py:468] Processed 305 examples in 1 batches [2.084 sec per 100]. I0519 16:22:48.736088 139665862911808 call_variants.py:471] Done calling variants from a total of 305 examples. real 0m8.934s. user 0m37.643s. sys 0m6.426s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --infile ""/tmp/tmp6gzkras0/call_variants_output.tfrecord.gz"" --outfile ""singularity-output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp6gzkras0/gvcf.tfrecord@32.gz"" --gvcf_outfile ""singularity-output/output.g.vcf.gz"". 2023-05-19 16:22:49.638487: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0519 16:22:51.170790 140640470185792 postprocess_variants.py:972] Using sample name from call_variants output. Sample name: NA12878. 2023-05-19 16:22:51.171487: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/tmp6gzkras0/call_variants_output.tfrecord.gz. 2023-05-19 16:22:51.173038: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 305. I0519 16:22:51.173760 140640470185792 postprocess_variants.py:1037] CVO sorting took 3.902912139892578e-05 minutes. I0519 16:22:51.173915 140640470185792 postprocess_variants.py:1040] Transforming call_variants_output to variants. I0519 16:22:51.204555 140640470185792 postprocess_variants.py:1080] Processing variants (and writing to temporary file) took 0.0005078872044881184 minutes. I0519 16:22:51.582558 140640470185792 postprocess_variants.py:1093] Finished writing VCF",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:621,reliability,doe,does,621,"DeepVariant Runerror, ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). Hi developers,. I'd like to run `DeepVariant` for my `WGS` sequencing data. My sequencing data were from `BGI` platform and were preprocessed by `fastp, bwa+Hs37d5, MarkDuplicatesSpark`. I tried to use the 'sorted and deduplicated bam' file as input for `DeepVariant` in `singularity` mode. However, I always encountered the 'reference index' `not found` error. But my `reference` fasta file and `reference index` fai file does exist. Could you please help me figure it out? **Setup**. - Operating system: Linux version 3.10.0-1127.el7.x86_64 (gcc version 4.8.5 20150623 (Red Hat 4.8.5-39), Computation Node (one node of Clusters). - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.):. - ``` BIN_VERSION=""1.5.0"". docker pull; . singularity pull docker://google/deepvariant:""${BIN_VERSION}"". singularity build --fakeroot deepvariant.sif docker://google/deepvariant:1.5.0```. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). `BGI platform, WGS data, Hs37d5 reference, fastp QC, bwa-mem2 mapping, MarkDuplicatesSpark sort & dedup`. . **Steps to reproduce:**. - Command:. - 1. singularity run /lustre/Data/toolsDB//deepvariant.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=$ref_idx --reads=$dedupbam --output_vcf=$vcfout --output_gvcf=$gvcfout --num_shards=32 >$logx 2>&1. - Error trace: (if applicable). - ```I0522 08:40:36.823651 140633630893888 genomics_reader.py:222] Reading /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam with NativeSamReader. I0522 08:40:36.846348 140633630893888 make_examples_core.py:257] Task 27/32: Preparing inputs. [E::fai_load3_core] Failed to open FASTA index /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:1924,reliability,Fail,Failed,1924,"_VERSION=""1.5.0"". docker pull; . singularity pull docker://google/deepvariant:""${BIN_VERSION}"". singularity build --fakeroot deepvariant.sif docker://google/deepvariant:1.5.0```. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). `BGI platform, WGS data, Hs37d5 reference, fastp QC, bwa-mem2 mapping, MarkDuplicatesSpark sort & dedup`. . **Steps to reproduce:**. - Command:. - 1. singularity run /lustre/Data/toolsDB//deepvariant.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=$ref_idx --reads=$dedupbam --output_vcf=$vcfout --output_gvcf=$gvcfout --num_shards=32 >$logx 2>&1. - Error trace: (if applicable). - ```I0522 08:40:36.823651 140633630893888 genomics_reader.py:222] Reading /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam with NativeSamReader. I0522 08:40:36.846348 140633630893888 make_examples_core.py:257] Task 27/32: Preparing inputs. [E::fai_load3_core] Failed to open FASTA index /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai: No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2114, in make_examples_runner. regions, calling_regions = processing_regions_from_options(options). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2019, in processi",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:3598,reliability,Fail,Failed,3598,"tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2114, in make_examples_runner. regions, calling_regions = processing_regions_from_options(options). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2019, in processing_regions_from_options. ref_contigs = fasta.IndexedFastaReader(. File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 106, in __init__. self._reader = reference.IndexedFastaReader.from_file(. ValueError: NOT_FOUND: could not load fasta and/or fai for fasta /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. I0522 08:40:37.063492 139925831092032 genomics_reader.py:222] Reading /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam with NativeSamReader. I0522 08:40:37.081499 139925831092032 make_examples_core.py:257] Task 6/32: Preparing inputs. [E::fai_load3_core] Failed to open FASTA index /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai: No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_luflf_op/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_luflf_op/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_luflf_op/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_luflf_op/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_luflf_op/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2114, in make_examples_runner. regions, calling_regions = processing_regions_from_options(options). File ""/tmp/Bazel.runfiles_luflf_op/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2019, in processi",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:5278,reliability,Fail,Failed,5278,"zel.runfiles_luflf_op/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2114, in make_examples_runner. regions, calling_regions = processing_regions_from_options(options). File ""/tmp/Bazel.runfiles_luflf_op/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2019, in processing_regions_from_options. ref_contigs = fasta.IndexedFastaReader(. File ""/tmp/Bazel.runfiles_luflf_op/runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 106, in __init__. self._reader = reference.IndexedFastaReader.from_file(. ****************. File ""/tmp/Bazel.runfiles_8r9pgqu5/runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 106, in __init__. self._reader = reference.IndexedFastaReader.from_file(. ValueError: NOT_FOUND: could not load fasta and/or fai for fasta /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. I0522 08:40:37.129531 140035375298368 make_examples_core.py:257] Task 7/32: Preparing inputs. [E::fai_load3_core] Failed to open FASTA index /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai: No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2114, in make_examples_runner. regions, calling_regions = processing_regions_from_options(options). File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2019, in processi",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:6665,reliability,fail,failed,6665," File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2114, in make_examples_runner. regions, calling_regions = processing_regions_from_options(options). File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2019, in processing_regions_from_options. ref_contigs = fasta.IndexedFastaReader(. File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 106, in __init__. self._reader = reference.IndexedFastaReader.from_file(. ValueError: NOT_FOUND: could not load fasta and/or fai for fasta /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa --reads /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam --examples /tmp/tmpfab4tpv7/make_examples.tfrecord@32.gz --channels insert_size --gvcf /tmp/tmpfab4tpv7/gvcf.tfrecord@32.gz --task 18. - ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the quick test run as normal. ```. 3. reference index does. ```$ ls /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467 Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909010 Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 106669 Apr 2",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:7035,reliability,Doe,Does,7035,"les_core.py"", line 2114, in make_examples_runner. regions, calling_regions = processing_regions_from_options(options). File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2019, in processing_regions_from_options. ref_contigs = fasta.IndexedFastaReader(. File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 106, in __init__. self._reader = reference.IndexedFastaReader.from_file(. ValueError: NOT_FOUND: could not load fasta and/or fai for fasta /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa --reads /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam --examples /tmp/tmpfab4tpv7/make_examples.tfrecord@32.gz --channels insert_size --gvcf /tmp/tmpfab4tpv7/gvcf.tfrecord@32.gz --task 18. - ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the quick test run as normal. ```. 3. reference index does. ```$ ls /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467 Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909010 Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 106669 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6924 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 10196727247 Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 2813 May 19 17:15 /",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:7309,reliability,doe,does,7309,"ontigs = fasta.IndexedFastaReader(. File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 106, in __init__. self._reader = reference.IndexedFastaReader.from_file(. ValueError: NOT_FOUND: could not load fasta and/or fai for fasta /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa --reads /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam --examples /tmp/tmpfab4tpv7/make_examples.tfrecord@32.gz --channels insert_size --gvcf /tmp/tmpfab4tpv7/gvcf.tfrecord@32.gz --task 18. - ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the quick test run as normal. ```. 3. reference index does. ```$ ls /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467 Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909010 Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 106669 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6924 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 10196727247 Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 2813 May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 784363628 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```. **************. To enable them in other operations, rebuild TensorFlow with the appropriate ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:12254,reliability,Restor,Restoring,12254,"les_core.py:257] Task 12/32: Preparing inputs. I0519 16:22:23.236589 139704511100736 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.237330 139704511100736 make_examples_core.py:257] Task 12/32: Common contigs are ['chr20']. I0519 16:22:23.237694 140638923466560 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.289134 140638923466560 make_examples_core.py:257] Task 29/32: Preparing inputs. I0519 16:22:23.215111 139798323177280 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.265897 139798323177280 make_examples_core.py:257] Task 9/32: Preparing inputs. **********. I0519 16:22:46.781010 139665862911808 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0519 16:22:47.119282 139665862911808 modeling.py:418] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt. I0519 16:22:47.119841 139665862911808 saver.py:1410] Restoring parameters from /opt/models/wgs/model.ckpt. I0519 16:22:48.504039 139665862911808 call_variants.py:462] Processed 1 examples in 1 batches [632.440 sec per 100]. I0519 16:22:48.735930 139665862911808 call_variants.py:468] Processed 305 examples in 1 batches [2.084 sec per 100]. I0519 16:22:48.736088 139665862911808 call_variants.py:471] Done calling variants from a total of 305 examples. real 0m8.934s. user 0m37.643s. sys 0m6.426s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --infile ""/tmp/tmp6gzkras0/call_variants_output.tfrecord.gz"" --outfile ""singularity-output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp6gzkras0/gvcf.tfrecord@32.gz"" --gvcf_outfile ""singularity-output/output.g.vcf.gz"". 2023-05-19 16:22:49.638487: I tensorflow/core/platform/cpu_feature_guard.cc:193] Thi",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:12361,reliability,Restor,Restoring,12361,"] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.237330 139704511100736 make_examples_core.py:257] Task 12/32: Common contigs are ['chr20']. I0519 16:22:23.237694 140638923466560 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.289134 140638923466560 make_examples_core.py:257] Task 29/32: Preparing inputs. I0519 16:22:23.215111 139798323177280 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.265897 139798323177280 make_examples_core.py:257] Task 9/32: Preparing inputs. **********. I0519 16:22:46.781010 139665862911808 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0519 16:22:47.119282 139665862911808 modeling.py:418] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt. I0519 16:22:47.119841 139665862911808 saver.py:1410] Restoring parameters from /opt/models/wgs/model.ckpt. I0519 16:22:48.504039 139665862911808 call_variants.py:462] Processed 1 examples in 1 batches [632.440 sec per 100]. I0519 16:22:48.735930 139665862911808 call_variants.py:468] Processed 305 examples in 1 batches [2.084 sec per 100]. I0519 16:22:48.736088 139665862911808 call_variants.py:471] Done calling variants from a total of 305 examples. real 0m8.934s. user 0m37.643s. sys 0m6.426s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --infile ""/tmp/tmp6gzkras0/call_variants_output.tfrecord.gz"" --outfile ""singularity-output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp6gzkras0/gvcf.tfrecord@32.gz"" --gvcf_outfile ""singularity-output/output.g.vcf.gz"". 2023-05-19 16:22:49.638487: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:442,safety,input,input,442,"DeepVariant Runerror, ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). Hi developers,. I'd like to run `DeepVariant` for my `WGS` sequencing data. My sequencing data were from `BGI` platform and were preprocessed by `fastp, bwa+Hs37d5, MarkDuplicatesSpark`. I tried to use the 'sorted and deduplicated bam' file as input for `DeepVariant` in `singularity` mode. However, I always encountered the 'reference index' `not found` error. But my `reference` fasta file and `reference index` fai file does exist. Could you please help me figure it out? **Setup**. - Operating system: Linux version 3.10.0-1127.el7.x86_64 (gcc version 4.8.5 20150623 (Red Hat 4.8.5-39), Computation Node (one node of Clusters). - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.):. - ``` BIN_VERSION=""1.5.0"". docker pull; . singularity pull docker://google/deepvariant:""${BIN_VERSION}"". singularity build --fakeroot deepvariant.sif docker://google/deepvariant:1.5.0```. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). `BGI platform, WGS data, Hs37d5 reference, fastp QC, bwa-mem2 mapping, MarkDuplicatesSpark sort & dedup`. . **Steps to reproduce:**. - Command:. - 1. singularity run /lustre/Data/toolsDB//deepvariant.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=$ref_idx --reads=$dedupbam --output_vcf=$vcfout --output_gvcf=$gvcfout --num_shards=32 >$logx 2>&1. - Error trace: (if applicable). - ```I0522 08:40:36.823651 140633630893888 genomics_reader.py:222] Reading /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam with NativeSamReader. I0522 08:40:36.846348 140633630893888 make_examples_core.py:257] Task 27/32: Preparing inputs. [E::fai_load3_core] Failed to open FASTA index /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:553,safety,error,error,553,"DeepVariant Runerror, ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). Hi developers,. I'd like to run `DeepVariant` for my `WGS` sequencing data. My sequencing data were from `BGI` platform and were preprocessed by `fastp, bwa+Hs37d5, MarkDuplicatesSpark`. I tried to use the 'sorted and deduplicated bam' file as input for `DeepVariant` in `singularity` mode. However, I always encountered the 'reference index' `not found` error. But my `reference` fasta file and `reference index` fai file does exist. Could you please help me figure it out? **Setup**. - Operating system: Linux version 3.10.0-1127.el7.x86_64 (gcc version 4.8.5 20150623 (Red Hat 4.8.5-39), Computation Node (one node of Clusters). - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.):. - ``` BIN_VERSION=""1.5.0"". docker pull; . singularity pull docker://google/deepvariant:""${BIN_VERSION}"". singularity build --fakeroot deepvariant.sif docker://google/deepvariant:1.5.0```. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). `BGI platform, WGS data, Hs37d5 reference, fastp QC, bwa-mem2 mapping, MarkDuplicatesSpark sort & dedup`. . **Steps to reproduce:**. - Command:. - 1. singularity run /lustre/Data/toolsDB//deepvariant.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=$ref_idx --reads=$dedupbam --output_vcf=$vcfout --output_gvcf=$gvcfout --num_shards=32 >$logx 2>&1. - Error trace: (if applicable). - ```I0522 08:40:36.823651 140633630893888 genomics_reader.py:222] Reading /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam with NativeSamReader. I0522 08:40:36.846348 140633630893888 make_examples_core.py:257] Task 27/32: Preparing inputs. [E::fai_load3_core] Failed to open FASTA index /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:1569,safety,log,logx,1569,"erence` fasta file and `reference index` fai file does exist. Could you please help me figure it out? **Setup**. - Operating system: Linux version 3.10.0-1127.el7.x86_64 (gcc version 4.8.5 20150623 (Red Hat 4.8.5-39), Computation Node (one node of Clusters). - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.):. - ``` BIN_VERSION=""1.5.0"". docker pull; . singularity pull docker://google/deepvariant:""${BIN_VERSION}"". singularity build --fakeroot deepvariant.sif docker://google/deepvariant:1.5.0```. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). `BGI platform, WGS data, Hs37d5 reference, fastp QC, bwa-mem2 mapping, MarkDuplicatesSpark sort & dedup`. . **Steps to reproduce:**. - Command:. - 1. singularity run /lustre/Data/toolsDB//deepvariant.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=$ref_idx --reads=$dedupbam --output_vcf=$vcfout --output_gvcf=$gvcfout --num_shards=32 >$logx 2>&1. - Error trace: (if applicable). - ```I0522 08:40:36.823651 140633630893888 genomics_reader.py:222] Reading /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam with NativeSamReader. I0522 08:40:36.846348 140633630893888 make_examples_core.py:257] Task 27/32: Preparing inputs. [E::fai_load3_core] Failed to open FASTA index /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai: No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main. make_examples_core.make_exa",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:1582,safety,Error,Error,1582,"file and `reference index` fai file does exist. Could you please help me figure it out? **Setup**. - Operating system: Linux version 3.10.0-1127.el7.x86_64 (gcc version 4.8.5 20150623 (Red Hat 4.8.5-39), Computation Node (one node of Clusters). - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.):. - ``` BIN_VERSION=""1.5.0"". docker pull; . singularity pull docker://google/deepvariant:""${BIN_VERSION}"". singularity build --fakeroot deepvariant.sif docker://google/deepvariant:1.5.0```. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). `BGI platform, WGS data, Hs37d5 reference, fastp QC, bwa-mem2 mapping, MarkDuplicatesSpark sort & dedup`. . **Steps to reproduce:**. - Command:. - 1. singularity run /lustre/Data/toolsDB//deepvariant.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=$ref_idx --reads=$dedupbam --output_vcf=$vcfout --output_gvcf=$gvcfout --num_shards=32 >$logx 2>&1. - Error trace: (if applicable). - ```I0522 08:40:36.823651 140633630893888 genomics_reader.py:222] Reading /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam with NativeSamReader. I0522 08:40:36.846348 140633630893888 make_examples_core.py:257] Task 27/32: Preparing inputs. [E::fai_load3_core] Failed to open FASTA index /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai: No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main. make_examples_core.make_examples_runner(o",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:1896,safety,input,inputs,1896,"om source, etc.):. - ``` BIN_VERSION=""1.5.0"". docker pull; . singularity pull docker://google/deepvariant:""${BIN_VERSION}"". singularity build --fakeroot deepvariant.sif docker://google/deepvariant:1.5.0```. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). `BGI platform, WGS data, Hs37d5 reference, fastp QC, bwa-mem2 mapping, MarkDuplicatesSpark sort & dedup`. . **Steps to reproduce:**. - Command:. - 1. singularity run /lustre/Data/toolsDB//deepvariant.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=$ref_idx --reads=$dedupbam --output_vcf=$vcfout --output_gvcf=$gvcfout --num_shards=32 >$logx 2>&1. - Error trace: (if applicable). - ```I0522 08:40:36.823651 140633630893888 genomics_reader.py:222] Reading /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam with NativeSamReader. I0522 08:40:36.846348 140633630893888 make_examples_core.py:257] Task 27/32: Preparing inputs. [E::fai_load3_core] Failed to open FASTA index /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai: No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2114, in make_examples_runner. regions, calling_regions = processing_regions_from_options(options). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/deepvariant/make_examples_core",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:2184,safety,modul,module,2184,"s unlike the case studies?). `BGI platform, WGS data, Hs37d5 reference, fastp QC, bwa-mem2 mapping, MarkDuplicatesSpark sort & dedup`. . **Steps to reproduce:**. - Command:. - 1. singularity run /lustre/Data/toolsDB//deepvariant.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=$ref_idx --reads=$dedupbam --output_vcf=$vcfout --output_gvcf=$gvcfout --num_shards=32 >$logx 2>&1. - Error trace: (if applicable). - ```I0522 08:40:36.823651 140633630893888 genomics_reader.py:222] Reading /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam with NativeSamReader. I0522 08:40:36.846348 140633630893888 make_examples_core.py:257] Task 27/32: Preparing inputs. [E::fai_load3_core] Failed to open FASTA index /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai: No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2114, in make_examples_runner. regions, calling_regions = processing_regions_from_options(options). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2019, in processing_regions_from_options. ref_contigs = fasta.IndexedFastaReader(. File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 106, in __init__. self._reader = reference.IndexedFastaReader.from_file(. ValueError: NO",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:3570,safety,input,inputs,3570,"les_runner(options). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2114, in make_examples_runner. regions, calling_regions = processing_regions_from_options(options). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2019, in processing_regions_from_options. ref_contigs = fasta.IndexedFastaReader(. File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 106, in __init__. self._reader = reference.IndexedFastaReader.from_file(. ValueError: NOT_FOUND: could not load fasta and/or fai for fasta /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. I0522 08:40:37.063492 139925831092032 genomics_reader.py:222] Reading /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam with NativeSamReader. I0522 08:40:37.081499 139925831092032 make_examples_core.py:257] Task 6/32: Preparing inputs. [E::fai_load3_core] Failed to open FASTA index /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai: No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_luflf_op/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_luflf_op/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_luflf_op/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_luflf_op/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_luflf_op/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2114, in make_examples_runner. regions, calling_regions = processing_regions_from_options(options). File ""/tmp/Bazel.runfiles_luflf_op/runfiles/com_google_deepvariant/deepvariant/make_examples_core",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:3858,safety,modul,module,3858,"variant/deepvariant/make_examples_core.py"", line 2019, in processing_regions_from_options. ref_contigs = fasta.IndexedFastaReader(. File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 106, in __init__. self._reader = reference.IndexedFastaReader.from_file(. ValueError: NOT_FOUND: could not load fasta and/or fai for fasta /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. I0522 08:40:37.063492 139925831092032 genomics_reader.py:222] Reading /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam with NativeSamReader. I0522 08:40:37.081499 139925831092032 make_examples_core.py:257] Task 6/32: Preparing inputs. [E::fai_load3_core] Failed to open FASTA index /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai: No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_luflf_op/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_luflf_op/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_luflf_op/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_luflf_op/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_luflf_op/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2114, in make_examples_runner. regions, calling_regions = processing_regions_from_options(options). File ""/tmp/Bazel.runfiles_luflf_op/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2019, in processing_regions_from_options. ref_contigs = fasta.IndexedFastaReader(. File ""/tmp/Bazel.runfiles_luflf_op/runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 106, in __init__. self._reader = reference.IndexedFastaReader.from_file(. **************",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:5250,safety,input,inputs,5250,"nner(options). File ""/tmp/Bazel.runfiles_luflf_op/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2114, in make_examples_runner. regions, calling_regions = processing_regions_from_options(options). File ""/tmp/Bazel.runfiles_luflf_op/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2019, in processing_regions_from_options. ref_contigs = fasta.IndexedFastaReader(. File ""/tmp/Bazel.runfiles_luflf_op/runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 106, in __init__. self._reader = reference.IndexedFastaReader.from_file(. ****************. File ""/tmp/Bazel.runfiles_8r9pgqu5/runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 106, in __init__. self._reader = reference.IndexedFastaReader.from_file(. ValueError: NOT_FOUND: could not load fasta and/or fai for fasta /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. I0522 08:40:37.129531 140035375298368 make_examples_core.py:257] Task 7/32: Preparing inputs. [E::fai_load3_core] Failed to open FASTA index /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai: No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2114, in make_examples_runner. regions, calling_regions = processing_regions_from_options(options). File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/deepvariant/make_examples_core",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:5538,safety,modul,module,5538,"t/deepvariant/make_examples_core.py"", line 2019, in processing_regions_from_options. ref_contigs = fasta.IndexedFastaReader(. File ""/tmp/Bazel.runfiles_luflf_op/runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 106, in __init__. self._reader = reference.IndexedFastaReader.from_file(. ****************. File ""/tmp/Bazel.runfiles_8r9pgqu5/runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 106, in __init__. self._reader = reference.IndexedFastaReader.from_file(. ValueError: NOT_FOUND: could not load fasta and/or fai for fasta /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. I0522 08:40:37.129531 140035375298368 make_examples_core.py:257] Task 7/32: Preparing inputs. [E::fai_load3_core] Failed to open FASTA index /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai: No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2114, in make_examples_runner. regions, calling_regions = processing_regions_from_options(options). File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2019, in processing_regions_from_options. ref_contigs = fasta.IndexedFastaReader(. File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 106, in __init__. self._reader = reference.IndexedFastaReader.from_file(. ValueError: NO",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:7056,safety,test,test,7056,"14, in make_examples_runner. regions, calling_regions = processing_regions_from_options(options). File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2019, in processing_regions_from_options. ref_contigs = fasta.IndexedFastaReader(. File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 106, in __init__. self._reader = reference.IndexedFastaReader.from_file(. ValueError: NOT_FOUND: could not load fasta and/or fai for fasta /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa --reads /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam --examples /tmp/tmpfab4tpv7/make_examples.tfrecord@32.gz --channels insert_size --gvcf /tmp/tmpfab4tpv7/gvcf.tfrecord@32.gz --task 18. - ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the quick test run as normal. ```. 3. reference index does. ```$ ls /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467 Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909010 Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 106669 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6924 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 10196727247 Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 2813 May 19 17:15 /lustre/Data/toolsDB/H",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:7092,safety,test,test,7092,", calling_regions = processing_regions_from_options(options). File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2019, in processing_regions_from_options. ref_contigs = fasta.IndexedFastaReader(. File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 106, in __init__. self._reader = reference.IndexedFastaReader.from_file(. ValueError: NOT_FOUND: could not load fasta and/or fai for fasta /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa --reads /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam --examples /tmp/tmpfab4tpv7/make_examples.tfrecord@32.gz --channels insert_size --gvcf /tmp/tmpfab4tpv7/gvcf.tfrecord@32.gz --task 18. - ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the quick test run as normal. ```. 3. reference index does. ```$ ls /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467 Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909010 Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 106669 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6924 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 10196727247 Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 2813 May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:7265,safety,test,test,7265,"9, in processing_regions_from_options. ref_contigs = fasta.IndexedFastaReader(. File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 106, in __init__. self._reader = reference.IndexedFastaReader.from_file(. ValueError: NOT_FOUND: could not load fasta and/or fai for fasta /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa --reads /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam --examples /tmp/tmpfab4tpv7/make_examples.tfrecord@32.gz --channels insert_size --gvcf /tmp/tmpfab4tpv7/gvcf.tfrecord@32.gz --task 18. - ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the quick test run as normal. ```. 3. reference index does. ```$ ls /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467 Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909010 Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 106669 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6924 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 10196727247 Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 2813 May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 784363628 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```. **************. To enable them in other operatio",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:8772,safety,test,testdata,8772,"4 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 10196727247 Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 2813 May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 784363628 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```. **************. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-05-19 16:22:21.555857: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (o. neDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0519 16:22:23.193474 139896863770432 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.256151 139896863770432 make_examples_core.py:257] Task 10/32: Preparing inputs. I0519 16:22:23.258605 139896863770432 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.259495 139896863770432 make_examples_core.py:257] Task 10/32: Common contigs are ['chr20']. I0519 16:22:23.239336 140148036429632 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.192739 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.235120 140421750466368 make_examples_core.py:257] Task 21/32: Preparing inputs. I0519 16:22:23.239059 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.240968 140421750466368 make_examples_core.py:257] Task 21/",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:8921,safety,input,inputs,8921,"a/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 2813 May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 784363628 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```. **************. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-05-19 16:22:21.555857: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (o. neDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0519 16:22:23.193474 139896863770432 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.256151 139896863770432 make_examples_core.py:257] Task 10/32: Preparing inputs. I0519 16:22:23.258605 139896863770432 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.259495 139896863770432 make_examples_core.py:257] Task 10/32: Common contigs are ['chr20']. I0519 16:22:23.239336 140148036429632 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.192739 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.235120 140421750466368 make_examples_core.py:257] Task 21/32: Preparing inputs. I0519 16:22:23.239059 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.240968 140421750466368 make_examples_core.py:257] Task 21/32: Common contigs are ['chr20']. I0519 16:22:23.242177 140053689509696 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:9010,safety,test,testdata,9010,"lin 2813 May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 784363628 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```. **************. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-05-19 16:22:21.555857: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (o. neDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0519 16:22:23.193474 139896863770432 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.256151 139896863770432 make_examples_core.py:257] Task 10/32: Preparing inputs. I0519 16:22:23.258605 139896863770432 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.259495 139896863770432 make_examples_core.py:257] Task 10/32: Common contigs are ['chr20']. I0519 16:22:23.239336 140148036429632 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.192739 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.235120 140421750466368 make_examples_core.py:257] Task 21/32: Preparing inputs. I0519 16:22:23.239059 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.240968 140421750466368 make_examples_core.py:257] Task 21/32: Common contigs are ['chr20']. I0519 16:22:23.242177 140053689509696 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.227729 140555533080384 genomics_reader.py:222]",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:9260,safety,test,testdata,9260,"tions, rebuild TensorFlow with the appropriate compiler flags. 2023-05-19 16:22:21.555857: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (o. neDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0519 16:22:23.193474 139896863770432 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.256151 139896863770432 make_examples_core.py:257] Task 10/32: Preparing inputs. I0519 16:22:23.258605 139896863770432 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.259495 139896863770432 make_examples_core.py:257] Task 10/32: Common contigs are ['chr20']. I0519 16:22:23.239336 140148036429632 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.192739 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.235120 140421750466368 make_examples_core.py:257] Task 21/32: Preparing inputs. I0519 16:22:23.239059 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.240968 140421750466368 make_examples_core.py:257] Task 21/32: Common contigs are ['chr20']. I0519 16:22:23.242177 140053689509696 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.227729 140555533080384 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.280361 140555533080384 make_examples_core.py:257] Task 6/32: Preparing inputs. I0519 16:22:23.282453 140555533080384 genomics_reader.py:222] Reading quic",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:9403,safety,test,testdata,9403," This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (o. neDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0519 16:22:23.193474 139896863770432 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.256151 139896863770432 make_examples_core.py:257] Task 10/32: Preparing inputs. I0519 16:22:23.258605 139896863770432 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.259495 139896863770432 make_examples_core.py:257] Task 10/32: Common contigs are ['chr20']. I0519 16:22:23.239336 140148036429632 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.192739 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.235120 140421750466368 make_examples_core.py:257] Task 21/32: Preparing inputs. I0519 16:22:23.239059 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.240968 140421750466368 make_examples_core.py:257] Task 21/32: Common contigs are ['chr20']. I0519 16:22:23.242177 140053689509696 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.227729 140555533080384 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.280361 140555533080384 make_examples_core.py:257] Task 6/32: Preparing inputs. I0519 16:22:23.282453 140555533080384 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.283533 140555533080384 make_examples_core.py:257] Task 6/32",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:9552,safety,input,inputs,9552,"l operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0519 16:22:23.193474 139896863770432 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.256151 139896863770432 make_examples_core.py:257] Task 10/32: Preparing inputs. I0519 16:22:23.258605 139896863770432 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.259495 139896863770432 make_examples_core.py:257] Task 10/32: Common contigs are ['chr20']. I0519 16:22:23.239336 140148036429632 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.192739 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.235120 140421750466368 make_examples_core.py:257] Task 21/32: Preparing inputs. I0519 16:22:23.239059 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.240968 140421750466368 make_examples_core.py:257] Task 21/32: Common contigs are ['chr20']. I0519 16:22:23.242177 140053689509696 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.227729 140555533080384 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.280361 140555533080384 make_examples_core.py:257] Task 6/32: Preparing inputs. I0519 16:22:23.282453 140555533080384 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.283533 140555533080384 make_examples_core.py:257] Task 6/32: Common contigs are ['chr20']. I0519 16:22:23.228248 140552972691264 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:9641,safety,test,testdata,9641,nsorFlow with the appropriate compiler flags. I0519 16:22:23.193474 139896863770432 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.256151 139896863770432 make_examples_core.py:257] Task 10/32: Preparing inputs. I0519 16:22:23.258605 139896863770432 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.259495 139896863770432 make_examples_core.py:257] Task 10/32: Common contigs are ['chr20']. I0519 16:22:23.239336 140148036429632 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.192739 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.235120 140421750466368 make_examples_core.py:257] Task 21/32: Preparing inputs. I0519 16:22:23.239059 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.240968 140421750466368 make_examples_core.py:257] Task 21/32: Common contigs are ['chr20']. I0519 16:22:23.242177 140053689509696 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.227729 140555533080384 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.280361 140555533080384 make_examples_core.py:257] Task 6/32: Preparing inputs. I0519 16:22:23.282453 140555533080384 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.283533 140555533080384 make_examples_core.py:257] Task 6/32: Common contigs are ['chr20']. I0519 16:22:23.228248 140552972691264 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.279789 140552972691264 make_examples_core.py:257,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:9891,safety,test,testdata,9891,57] Task 10/32: Preparing inputs. I0519 16:22:23.258605 139896863770432 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.259495 139896863770432 make_examples_core.py:257] Task 10/32: Common contigs are ['chr20']. I0519 16:22:23.239336 140148036429632 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.192739 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.235120 140421750466368 make_examples_core.py:257] Task 21/32: Preparing inputs. I0519 16:22:23.239059 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.240968 140421750466368 make_examples_core.py:257] Task 21/32: Common contigs are ['chr20']. I0519 16:22:23.242177 140053689509696 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.227729 140555533080384 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.280361 140555533080384 make_examples_core.py:257] Task 6/32: Preparing inputs. I0519 16:22:23.282453 140555533080384 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.283533 140555533080384 make_examples_core.py:257] Task 6/32: Common contigs are ['chr20']. I0519 16:22:23.228248 140552972691264 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.279789 140552972691264 make_examples_core.py:257] Task 8/32: Preparing inputs. I0519 16:22:23.281702 140552972691264 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.282378 140552972691264 make_examples_core.py:257] Task 8/32: ,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:10034,safety,test,testdata,10034,_10p1mb.bam with NativeSamReader. I0519 16:22:23.259495 139896863770432 make_examples_core.py:257] Task 10/32: Common contigs are ['chr20']. I0519 16:22:23.239336 140148036429632 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.192739 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.235120 140421750466368 make_examples_core.py:257] Task 21/32: Preparing inputs. I0519 16:22:23.239059 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.240968 140421750466368 make_examples_core.py:257] Task 21/32: Common contigs are ['chr20']. I0519 16:22:23.242177 140053689509696 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.227729 140555533080384 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.280361 140555533080384 make_examples_core.py:257] Task 6/32: Preparing inputs. I0519 16:22:23.282453 140555533080384 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.283533 140555533080384 make_examples_core.py:257] Task 6/32: Common contigs are ['chr20']. I0519 16:22:23.228248 140552972691264 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.279789 140552972691264 make_examples_core.py:257] Task 8/32: Preparing inputs. I0519 16:22:23.281702 140552972691264 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.282378 140552972691264 make_examples_core.py:257] Task 8/32: Common contigs are ['chr20']. I0519 16:22:23.271428 140087439025984 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:10182,safety,input,inputs,10182,16:22:23.239336 140148036429632 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.192739 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.235120 140421750466368 make_examples_core.py:257] Task 21/32: Preparing inputs. I0519 16:22:23.239059 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.240968 140421750466368 make_examples_core.py:257] Task 21/32: Common contigs are ['chr20']. I0519 16:22:23.242177 140053689509696 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.227729 140555533080384 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.280361 140555533080384 make_examples_core.py:257] Task 6/32: Preparing inputs. I0519 16:22:23.282453 140555533080384 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.283533 140555533080384 make_examples_core.py:257] Task 6/32: Common contigs are ['chr20']. I0519 16:22:23.228248 140552972691264 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.279789 140552972691264 make_examples_core.py:257] Task 8/32: Preparing inputs. I0519 16:22:23.281702 140552972691264 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.282378 140552972691264 make_examples_core.py:257] Task 8/32: Common contigs are ['chr20']. I0519 16:22:23.271428 140087439025984 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.192873 139704511100736 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:10271,safety,test,testdata,10271,8_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.192739 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.235120 140421750466368 make_examples_core.py:257] Task 21/32: Preparing inputs. I0519 16:22:23.239059 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.240968 140421750466368 make_examples_core.py:257] Task 21/32: Common contigs are ['chr20']. I0519 16:22:23.242177 140053689509696 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.227729 140555533080384 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.280361 140555533080384 make_examples_core.py:257] Task 6/32: Preparing inputs. I0519 16:22:23.282453 140555533080384 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.283533 140555533080384 make_examples_core.py:257] Task 6/32: Common contigs are ['chr20']. I0519 16:22:23.228248 140552972691264 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.279789 140552972691264 make_examples_core.py:257] Task 8/32: Preparing inputs. I0519 16:22:23.281702 140552972691264 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.282378 140552972691264 make_examples_core.py:257] Task 8/32: Common contigs are ['chr20']. I0519 16:22:23.271428 140087439025984 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.192873 139704511100736 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.234176 139704511100736 make_examples_core.py:257],MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:10520,safety,test,testdata,10520,:257] Task 21/32: Preparing inputs. I0519 16:22:23.239059 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.240968 140421750466368 make_examples_core.py:257] Task 21/32: Common contigs are ['chr20']. I0519 16:22:23.242177 140053689509696 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.227729 140555533080384 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.280361 140555533080384 make_examples_core.py:257] Task 6/32: Preparing inputs. I0519 16:22:23.282453 140555533080384 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.283533 140555533080384 make_examples_core.py:257] Task 6/32: Common contigs are ['chr20']. I0519 16:22:23.228248 140552972691264 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.279789 140552972691264 make_examples_core.py:257] Task 8/32: Preparing inputs. I0519 16:22:23.281702 140552972691264 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.282378 140552972691264 make_examples_core.py:257] Task 8/32: Common contigs are ['chr20']. I0519 16:22:23.271428 140087439025984 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.192873 139704511100736 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.234176 139704511100736 make_examples_core.py:257] Task 12/32: Preparing inputs. I0519 16:22:23.236589 139704511100736 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.237330 139704511100736 make_examples_core.py:257] Task 12/32,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:10668,safety,input,inputs,10668,0p1mb.bam with NativeSamReader. I0519 16:22:23.240968 140421750466368 make_examples_core.py:257] Task 21/32: Common contigs are ['chr20']. I0519 16:22:23.242177 140053689509696 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.227729 140555533080384 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.280361 140555533080384 make_examples_core.py:257] Task 6/32: Preparing inputs. I0519 16:22:23.282453 140555533080384 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.283533 140555533080384 make_examples_core.py:257] Task 6/32: Common contigs are ['chr20']. I0519 16:22:23.228248 140552972691264 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.279789 140552972691264 make_examples_core.py:257] Task 8/32: Preparing inputs. I0519 16:22:23.281702 140552972691264 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.282378 140552972691264 make_examples_core.py:257] Task 8/32: Common contigs are ['chr20']. I0519 16:22:23.271428 140087439025984 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.192873 139704511100736 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.234176 139704511100736 make_examples_core.py:257] Task 12/32: Preparing inputs. I0519 16:22:23.236589 139704511100736 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.237330 139704511100736 make_examples_core.py:257] Task 12/32: Common contigs are ['chr20']. I0519 16:22:23.237694 140638923466560 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1m,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:10757,safety,test,testdata,10757,y:257] Task 21/32: Common contigs are ['chr20']. I0519 16:22:23.242177 140053689509696 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.227729 140555533080384 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.280361 140555533080384 make_examples_core.py:257] Task 6/32: Preparing inputs. I0519 16:22:23.282453 140555533080384 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.283533 140555533080384 make_examples_core.py:257] Task 6/32: Common contigs are ['chr20']. I0519 16:22:23.228248 140552972691264 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.279789 140552972691264 make_examples_core.py:257] Task 8/32: Preparing inputs. I0519 16:22:23.281702 140552972691264 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.282378 140552972691264 make_examples_core.py:257] Task 8/32: Common contigs are ['chr20']. I0519 16:22:23.271428 140087439025984 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.192873 139704511100736 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.234176 139704511100736 make_examples_core.py:257] Task 12/32: Preparing inputs. I0519 16:22:23.236589 139704511100736 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.237330 139704511100736 make_examples_core.py:257] Task 12/32: Common contigs are ['chr20']. I0519 16:22:23.237694 140638923466560 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.289134 140638923466560 make_examples_core.py:25,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:11006,safety,test,testdata,11006,222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.280361 140555533080384 make_examples_core.py:257] Task 6/32: Preparing inputs. I0519 16:22:23.282453 140555533080384 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.283533 140555533080384 make_examples_core.py:257] Task 6/32: Common contigs are ['chr20']. I0519 16:22:23.228248 140552972691264 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.279789 140552972691264 make_examples_core.py:257] Task 8/32: Preparing inputs. I0519 16:22:23.281702 140552972691264 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.282378 140552972691264 make_examples_core.py:257] Task 8/32: Common contigs are ['chr20']. I0519 16:22:23.271428 140087439025984 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.192873 139704511100736 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.234176 139704511100736 make_examples_core.py:257] Task 12/32: Preparing inputs. I0519 16:22:23.236589 139704511100736 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.237330 139704511100736 make_examples_core.py:257] Task 12/32: Common contigs are ['chr20']. I0519 16:22:23.237694 140638923466560 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.289134 140638923466560 make_examples_core.py:257] Task 29/32: Preparing inputs. I0519 16:22:23.215111 139798323177280 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.265897 139798323177280 make_examples_core.py:257] Task 9/3,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:11149,safety,test,testdata,11149,py:257] Task 6/32: Preparing inputs. I0519 16:22:23.282453 140555533080384 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.283533 140555533080384 make_examples_core.py:257] Task 6/32: Common contigs are ['chr20']. I0519 16:22:23.228248 140552972691264 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.279789 140552972691264 make_examples_core.py:257] Task 8/32: Preparing inputs. I0519 16:22:23.281702 140552972691264 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.282378 140552972691264 make_examples_core.py:257] Task 8/32: Common contigs are ['chr20']. I0519 16:22:23.271428 140087439025984 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.192873 139704511100736 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.234176 139704511100736 make_examples_core.py:257] Task 12/32: Preparing inputs. I0519 16:22:23.236589 139704511100736 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.237330 139704511100736 make_examples_core.py:257] Task 12/32: Common contigs are ['chr20']. I0519 16:22:23.237694 140638923466560 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.289134 140638923466560 make_examples_core.py:257] Task 29/32: Preparing inputs. I0519 16:22:23.215111 139798323177280 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.265897 139798323177280 make_examples_core.py:257] Task 9/32: Preparing inputs. **********. I0519 16:22:46.781010 139665862911808 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Relo,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:11298,safety,input,inputs,11298,0p1mb.bam with NativeSamReader. I0519 16:22:23.283533 140555533080384 make_examples_core.py:257] Task 6/32: Common contigs are ['chr20']. I0519 16:22:23.228248 140552972691264 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.279789 140552972691264 make_examples_core.py:257] Task 8/32: Preparing inputs. I0519 16:22:23.281702 140552972691264 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.282378 140552972691264 make_examples_core.py:257] Task 8/32: Common contigs are ['chr20']. I0519 16:22:23.271428 140087439025984 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.192873 139704511100736 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.234176 139704511100736 make_examples_core.py:257] Task 12/32: Preparing inputs. I0519 16:22:23.236589 139704511100736 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.237330 139704511100736 make_examples_core.py:257] Task 12/32: Common contigs are ['chr20']. I0519 16:22:23.237694 140638923466560 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.289134 140638923466560 make_examples_core.py:257] Task 29/32: Preparing inputs. I0519 16:22:23.215111 139798323177280 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.265897 139798323177280 make_examples_core.py:257] Task 9/32: Preparing inputs. **********. I0519 16:22:46.781010 139665862911808 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0519 16:22:47.119282 139665862911808 modeling.py:418] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/wgs/model,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:11387,safety,test,testdata,11387,y:257] Task 6/32: Common contigs are ['chr20']. I0519 16:22:23.228248 140552972691264 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.279789 140552972691264 make_examples_core.py:257] Task 8/32: Preparing inputs. I0519 16:22:23.281702 140552972691264 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.282378 140552972691264 make_examples_core.py:257] Task 8/32: Common contigs are ['chr20']. I0519 16:22:23.271428 140087439025984 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.192873 139704511100736 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.234176 139704511100736 make_examples_core.py:257] Task 12/32: Preparing inputs. I0519 16:22:23.236589 139704511100736 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.237330 139704511100736 make_examples_core.py:257] Task 12/32: Common contigs are ['chr20']. I0519 16:22:23.237694 140638923466560 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.289134 140638923466560 make_examples_core.py:257] Task 29/32: Preparing inputs. I0519 16:22:23.215111 139798323177280 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.265897 139798323177280 make_examples_core.py:257] Task 9/32: Preparing inputs. **********. I0519 16:22:46.781010 139665862911808 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0519 16:22:47.119282 139665862911808 modeling.py:418] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt. I0519 16:22:47.119841 139665862911808 saver.py:1410] Restoring parameters from /opt,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:11637,safety,test,testdata,11637,:257] Task 8/32: Preparing inputs. I0519 16:22:23.281702 140552972691264 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.282378 140552972691264 make_examples_core.py:257] Task 8/32: Common contigs are ['chr20']. I0519 16:22:23.271428 140087439025984 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.192873 139704511100736 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.234176 139704511100736 make_examples_core.py:257] Task 12/32: Preparing inputs. I0519 16:22:23.236589 139704511100736 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.237330 139704511100736 make_examples_core.py:257] Task 12/32: Common contigs are ['chr20']. I0519 16:22:23.237694 140638923466560 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.289134 140638923466560 make_examples_core.py:257] Task 29/32: Preparing inputs. I0519 16:22:23.215111 139798323177280 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.265897 139798323177280 make_examples_core.py:257] Task 9/32: Preparing inputs. **********. I0519 16:22:46.781010 139665862911808 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0519 16:22:47.119282 139665862911808 modeling.py:418] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt. I0519 16:22:47.119841 139665862911808 saver.py:1410] Restoring parameters from /opt/models/wgs/model.ckpt. I0519 16:22:48.504039 139665862911808 call_variants.py:462] Processed 1 examples in 1 batches [632.440 sec per 100]. I0519 16:22:48.735930 139665862911808 call_variants.py:468] Processed 305 examples in 1 batches [2.084 sec pe,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:11786,safety,input,inputs,11786,1mb.bam with NativeSamReader. I0519 16:22:23.282378 140552972691264 make_examples_core.py:257] Task 8/32: Common contigs are ['chr20']. I0519 16:22:23.271428 140087439025984 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.192873 139704511100736 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.234176 139704511100736 make_examples_core.py:257] Task 12/32: Preparing inputs. I0519 16:22:23.236589 139704511100736 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.237330 139704511100736 make_examples_core.py:257] Task 12/32: Common contigs are ['chr20']. I0519 16:22:23.237694 140638923466560 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.289134 140638923466560 make_examples_core.py:257] Task 29/32: Preparing inputs. I0519 16:22:23.215111 139798323177280 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.265897 139798323177280 make_examples_core.py:257] Task 9/32: Preparing inputs. **********. I0519 16:22:46.781010 139665862911808 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0519 16:22:47.119282 139665862911808 modeling.py:418] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt. I0519 16:22:47.119841 139665862911808 saver.py:1410] Restoring parameters from /opt/models/wgs/model.ckpt. I0519 16:22:48.504039 139665862911808 call_variants.py:462] Processed 1 examples in 1 batches [632.440 sec per 100]. I0519 16:22:48.735930 139665862911808 call_variants.py:468] Processed 305 examples in 1 batches [2.084 sec per 100]. I0519 16:22:48.736088 139665862911808 call_variants.py:471] Done calling variants from a total of 305 examples. real 0m8.934s. user 0m37.643,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:11875,safety,test,testdata,11875,257] Task 8/32: Common contigs are ['chr20']. I0519 16:22:23.271428 140087439025984 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.192873 139704511100736 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.234176 139704511100736 make_examples_core.py:257] Task 12/32: Preparing inputs. I0519 16:22:23.236589 139704511100736 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.237330 139704511100736 make_examples_core.py:257] Task 12/32: Common contigs are ['chr20']. I0519 16:22:23.237694 140638923466560 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.289134 140638923466560 make_examples_core.py:257] Task 29/32: Preparing inputs. I0519 16:22:23.215111 139798323177280 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.265897 139798323177280 make_examples_core.py:257] Task 9/32: Preparing inputs. **********. I0519 16:22:46.781010 139665862911808 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0519 16:22:47.119282 139665862911808 modeling.py:418] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt. I0519 16:22:47.119841 139665862911808 saver.py:1410] Restoring parameters from /opt/models/wgs/model.ckpt. I0519 16:22:48.504039 139665862911808 call_variants.py:462] Processed 1 examples in 1 batches [632.440 sec per 100]. I0519 16:22:48.735930 139665862911808 call_variants.py:468] Processed 305 examples in 1 batches [2.084 sec per 100]. I0519 16:22:48.736088 139665862911808 call_variants.py:471] Done calling variants from a total of 305 examples. real 0m8.934s. user 0m37.643s. sys 0m6.426s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_va,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:12023,safety,input,inputs,12023,"chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.192873 139704511100736 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.234176 139704511100736 make_examples_core.py:257] Task 12/32: Preparing inputs. I0519 16:22:23.236589 139704511100736 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.237330 139704511100736 make_examples_core.py:257] Task 12/32: Common contigs are ['chr20']. I0519 16:22:23.237694 140638923466560 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.289134 140638923466560 make_examples_core.py:257] Task 29/32: Preparing inputs. I0519 16:22:23.215111 139798323177280 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.265897 139798323177280 make_examples_core.py:257] Task 9/32: Preparing inputs. **********. I0519 16:22:46.781010 139665862911808 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0519 16:22:47.119282 139665862911808 modeling.py:418] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt. I0519 16:22:47.119841 139665862911808 saver.py:1410] Restoring parameters from /opt/models/wgs/model.ckpt. I0519 16:22:48.504039 139665862911808 call_variants.py:462] Processed 1 examples in 1 batches [632.440 sec per 100]. I0519 16:22:48.735930 139665862911808 call_variants.py:468] Processed 305 examples in 1 batches [2.084 sec per 100]. I0519 16:22:48.736088 139665862911808 call_variants.py:471] Done calling variants from a total of 305 examples. real 0m8.934s. user 0m37.643s. sys 0m6.426s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --infile ""/tmp/tmp6gzkras0/call_variants_output.tfrecord.gz"" --outfile ""singulari",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:12904,safety,test,testdata,12904,"mb.bam with NativeSamReader. I0519 16:22:23.265897 139798323177280 make_examples_core.py:257] Task 9/32: Preparing inputs. **********. I0519 16:22:46.781010 139665862911808 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0519 16:22:47.119282 139665862911808 modeling.py:418] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt. I0519 16:22:47.119841 139665862911808 saver.py:1410] Restoring parameters from /opt/models/wgs/model.ckpt. I0519 16:22:48.504039 139665862911808 call_variants.py:462] Processed 1 examples in 1 batches [632.440 sec per 100]. I0519 16:22:48.735930 139665862911808 call_variants.py:468] Processed 305 examples in 1 batches [2.084 sec per 100]. I0519 16:22:48.736088 139665862911808 call_variants.py:471] Done calling variants from a total of 305 examples. real 0m8.934s. user 0m37.643s. sys 0m6.426s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --infile ""/tmp/tmp6gzkras0/call_variants_output.tfrecord.gz"" --outfile ""singularity-output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp6gzkras0/gvcf.tfrecord@32.gz"" --gvcf_outfile ""singularity-output/output.g.vcf.gz"". 2023-05-19 16:22:49.638487: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0519 16:22:51.170790 140640470185792 postprocess_variants.py:972] Using sample name from call_variants output. Sample name: NA12878. 2023-05-19 16:22:51.171487: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/tmp6gzkras0/call_variants_output.tfrecord.gz. 2023-05-19 16:22:51.173038: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_call",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:1569,security,log,logx,1569,"erence` fasta file and `reference index` fai file does exist. Could you please help me figure it out? **Setup**. - Operating system: Linux version 3.10.0-1127.el7.x86_64 (gcc version 4.8.5 20150623 (Red Hat 4.8.5-39), Computation Node (one node of Clusters). - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.):. - ``` BIN_VERSION=""1.5.0"". docker pull; . singularity pull docker://google/deepvariant:""${BIN_VERSION}"". singularity build --fakeroot deepvariant.sif docker://google/deepvariant:1.5.0```. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). `BGI platform, WGS data, Hs37d5 reference, fastp QC, bwa-mem2 mapping, MarkDuplicatesSpark sort & dedup`. . **Steps to reproduce:**. - Command:. - 1. singularity run /lustre/Data/toolsDB//deepvariant.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=$ref_idx --reads=$dedupbam --output_vcf=$vcfout --output_gvcf=$gvcfout --num_shards=32 >$logx 2>&1. - Error trace: (if applicable). - ```I0522 08:40:36.823651 140633630893888 genomics_reader.py:222] Reading /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam with NativeSamReader. I0522 08:40:36.846348 140633630893888 make_examples_core.py:257] Task 27/32: Preparing inputs. [E::fai_load3_core] Failed to open FASTA index /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai: No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main. make_examples_core.make_exa",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:8468,security,Network,Network,8468,"uman_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909010 Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 106669 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6924 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 10196727247 Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 2813 May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 784363628 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```. **************. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-05-19 16:22:21.555857: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (o. neDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0519 16:22:23.193474 139896863770432 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.256151 139896863770432 make_examples_core.py:257] Task 10/32: Preparing inputs. I0519 16:22:23.258605 139896863770432 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.259495 139896863770432 make_examples_core.py:257] Task 10/32: Common contigs are ['chr20']. I0519 16:22:23.239336 140148036429632 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.192739 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 1",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:12204,security,model,modeling,12204,"r. I0519 16:22:23.234176 139704511100736 make_examples_core.py:257] Task 12/32: Preparing inputs. I0519 16:22:23.236589 139704511100736 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.237330 139704511100736 make_examples_core.py:257] Task 12/32: Common contigs are ['chr20']. I0519 16:22:23.237694 140638923466560 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.289134 140638923466560 make_examples_core.py:257] Task 29/32: Preparing inputs. I0519 16:22:23.215111 139798323177280 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.265897 139798323177280 make_examples_core.py:257] Task 9/32: Preparing inputs. **********. I0519 16:22:46.781010 139665862911808 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0519 16:22:47.119282 139665862911808 modeling.py:418] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt. I0519 16:22:47.119841 139665862911808 saver.py:1410] Restoring parameters from /opt/models/wgs/model.ckpt. I0519 16:22:48.504039 139665862911808 call_variants.py:462] Processed 1 examples in 1 batches [632.440 sec per 100]. I0519 16:22:48.735930 139665862911808 call_variants.py:468] Processed 305 examples in 1 batches [2.084 sec per 100]. I0519 16:22:48.736088 139665862911808 call_variants.py:471] Done calling variants from a total of 305 examples. real 0m8.934s. user 0m37.643s. sys 0m6.426s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --infile ""/tmp/tmp6gzkras0/call_variants_output.tfrecord.gz"" --outfile ""singularity-output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp6gzkras0/gvcf.tfrecord@32.gz"" --gvcf_outfile ""singularity-output/output.g.vcf.gz"". 2023-05-19 16:22:49.638487: I ten",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:12285,security,model,models,12285,"Preparing inputs. I0519 16:22:23.236589 139704511100736 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.237330 139704511100736 make_examples_core.py:257] Task 12/32: Common contigs are ['chr20']. I0519 16:22:23.237694 140638923466560 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.289134 140638923466560 make_examples_core.py:257] Task 29/32: Preparing inputs. I0519 16:22:23.215111 139798323177280 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.265897 139798323177280 make_examples_core.py:257] Task 9/32: Preparing inputs. **********. I0519 16:22:46.781010 139665862911808 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0519 16:22:47.119282 139665862911808 modeling.py:418] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt. I0519 16:22:47.119841 139665862911808 saver.py:1410] Restoring parameters from /opt/models/wgs/model.ckpt. I0519 16:22:48.504039 139665862911808 call_variants.py:462] Processed 1 examples in 1 batches [632.440 sec per 100]. I0519 16:22:48.735930 139665862911808 call_variants.py:468] Processed 305 examples in 1 batches [2.084 sec per 100]. I0519 16:22:48.736088 139665862911808 call_variants.py:471] Done calling variants from a total of 305 examples. real 0m8.934s. user 0m37.643s. sys 0m6.426s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --infile ""/tmp/tmp6gzkras0/call_variants_output.tfrecord.gz"" --outfile ""singularity-output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp6gzkras0/gvcf.tfrecord@32.gz"" --gvcf_outfile ""singularity-output/output.g.vcf.gz"". 2023-05-19 16:22:49.638487: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimi",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:12296,security,model,model,12296,"nputs. I0519 16:22:23.236589 139704511100736 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.237330 139704511100736 make_examples_core.py:257] Task 12/32: Common contigs are ['chr20']. I0519 16:22:23.237694 140638923466560 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.289134 140638923466560 make_examples_core.py:257] Task 29/32: Preparing inputs. I0519 16:22:23.215111 139798323177280 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.265897 139798323177280 make_examples_core.py:257] Task 9/32: Preparing inputs. **********. I0519 16:22:46.781010 139665862911808 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0519 16:22:47.119282 139665862911808 modeling.py:418] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt. I0519 16:22:47.119841 139665862911808 saver.py:1410] Restoring parameters from /opt/models/wgs/model.ckpt. I0519 16:22:48.504039 139665862911808 call_variants.py:462] Processed 1 examples in 1 batches [632.440 sec per 100]. I0519 16:22:48.735930 139665862911808 call_variants.py:468] Processed 305 examples in 1 batches [2.084 sec per 100]. I0519 16:22:48.736088 139665862911808 call_variants.py:471] Done calling variants from a total of 305 examples. real 0m8.934s. user 0m37.643s. sys 0m6.426s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --infile ""/tmp/tmp6gzkras0/call_variants_output.tfrecord.gz"" --outfile ""singularity-output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp6gzkras0/gvcf.tfrecord@32.gz"" --gvcf_outfile ""singularity-output/output.g.vcf.gz"". 2023-05-19 16:22:49.638487: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with on",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:12392,security,model,models,12392,"/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.237330 139704511100736 make_examples_core.py:257] Task 12/32: Common contigs are ['chr20']. I0519 16:22:23.237694 140638923466560 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.289134 140638923466560 make_examples_core.py:257] Task 29/32: Preparing inputs. I0519 16:22:23.215111 139798323177280 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.265897 139798323177280 make_examples_core.py:257] Task 9/32: Preparing inputs. **********. I0519 16:22:46.781010 139665862911808 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0519 16:22:47.119282 139665862911808 modeling.py:418] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt. I0519 16:22:47.119841 139665862911808 saver.py:1410] Restoring parameters from /opt/models/wgs/model.ckpt. I0519 16:22:48.504039 139665862911808 call_variants.py:462] Processed 1 examples in 1 batches [632.440 sec per 100]. I0519 16:22:48.735930 139665862911808 call_variants.py:468] Processed 305 examples in 1 batches [2.084 sec per 100]. I0519 16:22:48.736088 139665862911808 call_variants.py:471] Done calling variants from a total of 305 examples. real 0m8.934s. user 0m37.643s. sys 0m6.426s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --infile ""/tmp/tmp6gzkras0/call_variants_output.tfrecord.gz"" --outfile ""singularity-output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp6gzkras0/gvcf.tfrecord@32.gz"" --gvcf_outfile ""singularity-output/output.g.vcf.gz"". 2023-05-19 16:22:49.638487: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-c",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:12403,security,model,model,12403,".chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.237330 139704511100736 make_examples_core.py:257] Task 12/32: Common contigs are ['chr20']. I0519 16:22:23.237694 140638923466560 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.289134 140638923466560 make_examples_core.py:257] Task 29/32: Preparing inputs. I0519 16:22:23.215111 139798323177280 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.265897 139798323177280 make_examples_core.py:257] Task 9/32: Preparing inputs. **********. I0519 16:22:46.781010 139665862911808 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0519 16:22:47.119282 139665862911808 modeling.py:418] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt. I0519 16:22:47.119841 139665862911808 saver.py:1410] Restoring parameters from /opt/models/wgs/model.ckpt. I0519 16:22:48.504039 139665862911808 call_variants.py:462] Processed 1 examples in 1 batches [632.440 sec per 100]. I0519 16:22:48.735930 139665862911808 call_variants.py:468] Processed 305 examples in 1 batches [2.084 sec per 100]. I0519 16:22:48.736088 139665862911808 call_variants.py:471] Done calling variants from a total of 305 examples. real 0m8.934s. user 0m37.643s. sys 0m6.426s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --infile ""/tmp/tmp6gzkras0/call_variants_output.tfrecord.gz"" --outfile ""singularity-output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp6gzkras0/gvcf.tfrecord@32.gz"" --gvcf_outfile ""singularity-output/output.g.vcf.gz"". 2023-05-19 16:22:49.638487: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical ope",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:13316,security,Network,Network,13316,"47.119841 139665862911808 saver.py:1410] Restoring parameters from /opt/models/wgs/model.ckpt. I0519 16:22:48.504039 139665862911808 call_variants.py:462] Processed 1 examples in 1 batches [632.440 sec per 100]. I0519 16:22:48.735930 139665862911808 call_variants.py:468] Processed 305 examples in 1 batches [2.084 sec per 100]. I0519 16:22:48.736088 139665862911808 call_variants.py:471] Done calling variants from a total of 305 examples. real 0m8.934s. user 0m37.643s. sys 0m6.426s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --infile ""/tmp/tmp6gzkras0/call_variants_output.tfrecord.gz"" --outfile ""singularity-output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp6gzkras0/gvcf.tfrecord@32.gz"" --gvcf_outfile ""singularity-output/output.g.vcf.gz"". 2023-05-19 16:22:49.638487: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0519 16:22:51.170790 140640470185792 postprocess_variants.py:972] Using sample name from call_variants output. Sample name: NA12878. 2023-05-19 16:22:51.171487: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/tmp6gzkras0/call_variants_output.tfrecord.gz. 2023-05-19 16:22:51.173038: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 305. I0519 16:22:51.173760 140640470185792 postprocess_variants.py:1037] CVO sorting took 3.902912139892578e-05 minutes. I0519 16:22:51.173915 140640470185792 postprocess_variants.py:1040] Transforming call_variants_output to variants. I0519 16:22:51.204555 140640470185792 postprocess_variants.py:1080] Processing variants (and writing to temporary file) took 0.0005078872044881184 minutes. I0519 16:22:51.5",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:1134,testability,instrument,instrument,1134,". (A clear and concise description of what the issue is.). Hi developers,. I'd like to run `DeepVariant` for my `WGS` sequencing data. My sequencing data were from `BGI` platform and were preprocessed by `fastp, bwa+Hs37d5, MarkDuplicatesSpark`. I tried to use the 'sorted and deduplicated bam' file as input for `DeepVariant` in `singularity` mode. However, I always encountered the 'reference index' `not found` error. But my `reference` fasta file and `reference index` fai file does exist. Could you please help me figure it out? **Setup**. - Operating system: Linux version 3.10.0-1127.el7.x86_64 (gcc version 4.8.5 20150623 (Red Hat 4.8.5-39), Computation Node (one node of Clusters). - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.):. - ``` BIN_VERSION=""1.5.0"". docker pull; . singularity pull docker://google/deepvariant:""${BIN_VERSION}"". singularity build --fakeroot deepvariant.sif docker://google/deepvariant:1.5.0```. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). `BGI platform, WGS data, Hs37d5 reference, fastp QC, bwa-mem2 mapping, MarkDuplicatesSpark sort & dedup`. . **Steps to reproduce:**. - Command:. - 1. singularity run /lustre/Data/toolsDB//deepvariant.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=$ref_idx --reads=$dedupbam --output_vcf=$vcfout --output_gvcf=$gvcfout --num_shards=32 >$logx 2>&1. - Error trace: (if applicable). - ```I0522 08:40:36.823651 140633630893888 genomics_reader.py:222] Reading /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam with NativeSamReader. I0522 08:40:36.846348 140633630893888 make_examples_core.py:257] Task 27/32: Preparing inputs. [E::fai_load3_core] Failed to open FASTA index /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai: No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:1569,testability,log,logx,1569,"erence` fasta file and `reference index` fai file does exist. Could you please help me figure it out? **Setup**. - Operating system: Linux version 3.10.0-1127.el7.x86_64 (gcc version 4.8.5 20150623 (Red Hat 4.8.5-39), Computation Node (one node of Clusters). - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.):. - ``` BIN_VERSION=""1.5.0"". docker pull; . singularity pull docker://google/deepvariant:""${BIN_VERSION}"". singularity build --fakeroot deepvariant.sif docker://google/deepvariant:1.5.0```. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). `BGI platform, WGS data, Hs37d5 reference, fastp QC, bwa-mem2 mapping, MarkDuplicatesSpark sort & dedup`. . **Steps to reproduce:**. - Command:. - 1. singularity run /lustre/Data/toolsDB//deepvariant.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=$ref_idx --reads=$dedupbam --output_vcf=$vcfout --output_gvcf=$gvcfout --num_shards=32 >$logx 2>&1. - Error trace: (if applicable). - ```I0522 08:40:36.823651 140633630893888 genomics_reader.py:222] Reading /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam with NativeSamReader. I0522 08:40:36.846348 140633630893888 make_examples_core.py:257] Task 27/32: Preparing inputs. [E::fai_load3_core] Failed to open FASTA index /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai: No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main. make_examples_core.make_exa",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:1588,testability,trace,trace,1588,"nd `reference index` fai file does exist. Could you please help me figure it out? **Setup**. - Operating system: Linux version 3.10.0-1127.el7.x86_64 (gcc version 4.8.5 20150623 (Red Hat 4.8.5-39), Computation Node (one node of Clusters). - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.):. - ``` BIN_VERSION=""1.5.0"". docker pull; . singularity pull docker://google/deepvariant:""${BIN_VERSION}"". singularity build --fakeroot deepvariant.sif docker://google/deepvariant:1.5.0```. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). `BGI platform, WGS data, Hs37d5 reference, fastp QC, bwa-mem2 mapping, MarkDuplicatesSpark sort & dedup`. . **Steps to reproduce:**. - Command:. - 1. singularity run /lustre/Data/toolsDB//deepvariant.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=$ref_idx --reads=$dedupbam --output_vcf=$vcfout --output_gvcf=$gvcfout --num_shards=32 >$logx 2>&1. - Error trace: (if applicable). - ```I0522 08:40:36.823651 140633630893888 genomics_reader.py:222] Reading /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam with NativeSamReader. I0522 08:40:36.846348 140633630893888 make_examples_core.py:257] Task 27/32: Preparing inputs. [E::fai_load3_core] Failed to open FASTA index /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai: No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main. make_examples_core.make_examples_runner(options",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:2036,testability,Trace,Traceback,2036,"--fakeroot deepvariant.sif docker://google/deepvariant:1.5.0```. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). `BGI platform, WGS data, Hs37d5 reference, fastp QC, bwa-mem2 mapping, MarkDuplicatesSpark sort & dedup`. . **Steps to reproduce:**. - Command:. - 1. singularity run /lustre/Data/toolsDB//deepvariant.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=$ref_idx --reads=$dedupbam --output_vcf=$vcfout --output_gvcf=$gvcfout --num_shards=32 >$logx 2>&1. - Error trace: (if applicable). - ```I0522 08:40:36.823651 140633630893888 genomics_reader.py:222] Reading /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam with NativeSamReader. I0522 08:40:36.846348 140633630893888 make_examples_core.py:257] Task 27/32: Preparing inputs. [E::fai_load3_core] Failed to open FASTA index /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai: No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2114, in make_examples_runner. regions, calling_regions = processing_regions_from_options(options). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2019, in processing_regions_from_options. ref_contigs = fasta.IndexedFastaReader(. File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:3710,testability,Trace,Traceback,3710,"_examples_runner. regions, calling_regions = processing_regions_from_options(options). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2019, in processing_regions_from_options. ref_contigs = fasta.IndexedFastaReader(. File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 106, in __init__. self._reader = reference.IndexedFastaReader.from_file(. ValueError: NOT_FOUND: could not load fasta and/or fai for fasta /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. I0522 08:40:37.063492 139925831092032 genomics_reader.py:222] Reading /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam with NativeSamReader. I0522 08:40:37.081499 139925831092032 make_examples_core.py:257] Task 6/32: Preparing inputs. [E::fai_load3_core] Failed to open FASTA index /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai: No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_luflf_op/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_luflf_op/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_luflf_op/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_luflf_op/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_luflf_op/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2114, in make_examples_runner. regions, calling_regions = processing_regions_from_options(options). File ""/tmp/Bazel.runfiles_luflf_op/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2019, in processing_regions_from_options. ref_contigs = fasta.IndexedFastaReader(. File ""/tmp/Bazel.runfiles_luflf_op/runfiles/com_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:5390,testability,Trace,Traceback,5390,"les_runner. regions, calling_regions = processing_regions_from_options(options). File ""/tmp/Bazel.runfiles_luflf_op/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2019, in processing_regions_from_options. ref_contigs = fasta.IndexedFastaReader(. File ""/tmp/Bazel.runfiles_luflf_op/runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 106, in __init__. self._reader = reference.IndexedFastaReader.from_file(. ****************. File ""/tmp/Bazel.runfiles_8r9pgqu5/runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 106, in __init__. self._reader = reference.IndexedFastaReader.from_file(. ValueError: NOT_FOUND: could not load fasta and/or fai for fasta /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. I0522 08:40:37.129531 140035375298368 make_examples_core.py:257] Task 7/32: Preparing inputs. [E::fai_load3_core] Failed to open FASTA index /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai: No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2114, in make_examples_runner. regions, calling_regions = processing_regions_from_options(options). File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2019, in processing_regions_from_options. ref_contigs = fasta.IndexedFastaReader(. File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:7056,testability,test,test,7056,"14, in make_examples_runner. regions, calling_regions = processing_regions_from_options(options). File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2019, in processing_regions_from_options. ref_contigs = fasta.IndexedFastaReader(. File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 106, in __init__. self._reader = reference.IndexedFastaReader.from_file(. ValueError: NOT_FOUND: could not load fasta and/or fai for fasta /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa --reads /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam --examples /tmp/tmpfab4tpv7/make_examples.tfrecord@32.gz --channels insert_size --gvcf /tmp/tmpfab4tpv7/gvcf.tfrecord@32.gz --task 18. - ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the quick test run as normal. ```. 3. reference index does. ```$ ls /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467 Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909010 Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 106669 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6924 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 10196727247 Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 2813 May 19 17:15 /lustre/Data/toolsDB/H",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:7092,testability,test,test,7092,", calling_regions = processing_regions_from_options(options). File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2019, in processing_regions_from_options. ref_contigs = fasta.IndexedFastaReader(. File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 106, in __init__. self._reader = reference.IndexedFastaReader.from_file(. ValueError: NOT_FOUND: could not load fasta and/or fai for fasta /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa --reads /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam --examples /tmp/tmpfab4tpv7/make_examples.tfrecord@32.gz --channels insert_size --gvcf /tmp/tmpfab4tpv7/gvcf.tfrecord@32.gz --task 18. - ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the quick test run as normal. ```. 3. reference index does. ```$ ls /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467 Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909010 Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 106669 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6924 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 10196727247 Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 2813 May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:7265,testability,test,test,7265,"9, in processing_regions_from_options. ref_contigs = fasta.IndexedFastaReader(. File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 106, in __init__. self._reader = reference.IndexedFastaReader.from_file(. ValueError: NOT_FOUND: could not load fasta and/or fai for fasta /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa --reads /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam --examples /tmp/tmpfab4tpv7/make_examples.tfrecord@32.gz --channels insert_size --gvcf /tmp/tmpfab4tpv7/gvcf.tfrecord@32.gz --task 18. - ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the quick test run as normal. ```. 3. reference index does. ```$ ls /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467 Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909010 Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 106669 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6924 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 10196727247 Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 2813 May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 784363628 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```. **************. To enable them in other operatio",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:8772,testability,test,testdata,8772,"4 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 10196727247 Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 2813 May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 784363628 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```. **************. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-05-19 16:22:21.555857: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (o. neDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0519 16:22:23.193474 139896863770432 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.256151 139896863770432 make_examples_core.py:257] Task 10/32: Preparing inputs. I0519 16:22:23.258605 139896863770432 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.259495 139896863770432 make_examples_core.py:257] Task 10/32: Common contigs are ['chr20']. I0519 16:22:23.239336 140148036429632 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.192739 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.235120 140421750466368 make_examples_core.py:257] Task 21/32: Preparing inputs. I0519 16:22:23.239059 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.240968 140421750466368 make_examples_core.py:257] Task 21/",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:9010,testability,test,testdata,9010,"lin 2813 May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 784363628 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```. **************. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-05-19 16:22:21.555857: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (o. neDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0519 16:22:23.193474 139896863770432 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.256151 139896863770432 make_examples_core.py:257] Task 10/32: Preparing inputs. I0519 16:22:23.258605 139896863770432 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.259495 139896863770432 make_examples_core.py:257] Task 10/32: Common contigs are ['chr20']. I0519 16:22:23.239336 140148036429632 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.192739 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.235120 140421750466368 make_examples_core.py:257] Task 21/32: Preparing inputs. I0519 16:22:23.239059 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.240968 140421750466368 make_examples_core.py:257] Task 21/32: Common contigs are ['chr20']. I0519 16:22:23.242177 140053689509696 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.227729 140555533080384 genomics_reader.py:222]",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:9260,testability,test,testdata,9260,"tions, rebuild TensorFlow with the appropriate compiler flags. 2023-05-19 16:22:21.555857: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (o. neDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0519 16:22:23.193474 139896863770432 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.256151 139896863770432 make_examples_core.py:257] Task 10/32: Preparing inputs. I0519 16:22:23.258605 139896863770432 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.259495 139896863770432 make_examples_core.py:257] Task 10/32: Common contigs are ['chr20']. I0519 16:22:23.239336 140148036429632 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.192739 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.235120 140421750466368 make_examples_core.py:257] Task 21/32: Preparing inputs. I0519 16:22:23.239059 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.240968 140421750466368 make_examples_core.py:257] Task 21/32: Common contigs are ['chr20']. I0519 16:22:23.242177 140053689509696 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.227729 140555533080384 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.280361 140555533080384 make_examples_core.py:257] Task 6/32: Preparing inputs. I0519 16:22:23.282453 140555533080384 genomics_reader.py:222] Reading quic",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:9403,testability,test,testdata,9403," This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (o. neDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0519 16:22:23.193474 139896863770432 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.256151 139896863770432 make_examples_core.py:257] Task 10/32: Preparing inputs. I0519 16:22:23.258605 139896863770432 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.259495 139896863770432 make_examples_core.py:257] Task 10/32: Common contigs are ['chr20']. I0519 16:22:23.239336 140148036429632 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.192739 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.235120 140421750466368 make_examples_core.py:257] Task 21/32: Preparing inputs. I0519 16:22:23.239059 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.240968 140421750466368 make_examples_core.py:257] Task 21/32: Common contigs are ['chr20']. I0519 16:22:23.242177 140053689509696 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.227729 140555533080384 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.280361 140555533080384 make_examples_core.py:257] Task 6/32: Preparing inputs. I0519 16:22:23.282453 140555533080384 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.283533 140555533080384 make_examples_core.py:257] Task 6/32",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:9641,testability,test,testdata,9641,nsorFlow with the appropriate compiler flags. I0519 16:22:23.193474 139896863770432 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.256151 139896863770432 make_examples_core.py:257] Task 10/32: Preparing inputs. I0519 16:22:23.258605 139896863770432 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.259495 139896863770432 make_examples_core.py:257] Task 10/32: Common contigs are ['chr20']. I0519 16:22:23.239336 140148036429632 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.192739 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.235120 140421750466368 make_examples_core.py:257] Task 21/32: Preparing inputs. I0519 16:22:23.239059 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.240968 140421750466368 make_examples_core.py:257] Task 21/32: Common contigs are ['chr20']. I0519 16:22:23.242177 140053689509696 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.227729 140555533080384 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.280361 140555533080384 make_examples_core.py:257] Task 6/32: Preparing inputs. I0519 16:22:23.282453 140555533080384 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.283533 140555533080384 make_examples_core.py:257] Task 6/32: Common contigs are ['chr20']. I0519 16:22:23.228248 140552972691264 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.279789 140552972691264 make_examples_core.py:257,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:9891,testability,test,testdata,9891,57] Task 10/32: Preparing inputs. I0519 16:22:23.258605 139896863770432 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.259495 139896863770432 make_examples_core.py:257] Task 10/32: Common contigs are ['chr20']. I0519 16:22:23.239336 140148036429632 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.192739 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.235120 140421750466368 make_examples_core.py:257] Task 21/32: Preparing inputs. I0519 16:22:23.239059 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.240968 140421750466368 make_examples_core.py:257] Task 21/32: Common contigs are ['chr20']. I0519 16:22:23.242177 140053689509696 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.227729 140555533080384 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.280361 140555533080384 make_examples_core.py:257] Task 6/32: Preparing inputs. I0519 16:22:23.282453 140555533080384 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.283533 140555533080384 make_examples_core.py:257] Task 6/32: Common contigs are ['chr20']. I0519 16:22:23.228248 140552972691264 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.279789 140552972691264 make_examples_core.py:257] Task 8/32: Preparing inputs. I0519 16:22:23.281702 140552972691264 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.282378 140552972691264 make_examples_core.py:257] Task 8/32: ,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:10034,testability,test,testdata,10034,_10p1mb.bam with NativeSamReader. I0519 16:22:23.259495 139896863770432 make_examples_core.py:257] Task 10/32: Common contigs are ['chr20']. I0519 16:22:23.239336 140148036429632 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.192739 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.235120 140421750466368 make_examples_core.py:257] Task 21/32: Preparing inputs. I0519 16:22:23.239059 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.240968 140421750466368 make_examples_core.py:257] Task 21/32: Common contigs are ['chr20']. I0519 16:22:23.242177 140053689509696 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.227729 140555533080384 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.280361 140555533080384 make_examples_core.py:257] Task 6/32: Preparing inputs. I0519 16:22:23.282453 140555533080384 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.283533 140555533080384 make_examples_core.py:257] Task 6/32: Common contigs are ['chr20']. I0519 16:22:23.228248 140552972691264 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.279789 140552972691264 make_examples_core.py:257] Task 8/32: Preparing inputs. I0519 16:22:23.281702 140552972691264 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.282378 140552972691264 make_examples_core.py:257] Task 8/32: Common contigs are ['chr20']. I0519 16:22:23.271428 140087439025984 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:10271,testability,test,testdata,10271,8_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.192739 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.235120 140421750466368 make_examples_core.py:257] Task 21/32: Preparing inputs. I0519 16:22:23.239059 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.240968 140421750466368 make_examples_core.py:257] Task 21/32: Common contigs are ['chr20']. I0519 16:22:23.242177 140053689509696 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.227729 140555533080384 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.280361 140555533080384 make_examples_core.py:257] Task 6/32: Preparing inputs. I0519 16:22:23.282453 140555533080384 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.283533 140555533080384 make_examples_core.py:257] Task 6/32: Common contigs are ['chr20']. I0519 16:22:23.228248 140552972691264 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.279789 140552972691264 make_examples_core.py:257] Task 8/32: Preparing inputs. I0519 16:22:23.281702 140552972691264 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.282378 140552972691264 make_examples_core.py:257] Task 8/32: Common contigs are ['chr20']. I0519 16:22:23.271428 140087439025984 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.192873 139704511100736 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.234176 139704511100736 make_examples_core.py:257],MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:10520,testability,test,testdata,10520,:257] Task 21/32: Preparing inputs. I0519 16:22:23.239059 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.240968 140421750466368 make_examples_core.py:257] Task 21/32: Common contigs are ['chr20']. I0519 16:22:23.242177 140053689509696 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.227729 140555533080384 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.280361 140555533080384 make_examples_core.py:257] Task 6/32: Preparing inputs. I0519 16:22:23.282453 140555533080384 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.283533 140555533080384 make_examples_core.py:257] Task 6/32: Common contigs are ['chr20']. I0519 16:22:23.228248 140552972691264 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.279789 140552972691264 make_examples_core.py:257] Task 8/32: Preparing inputs. I0519 16:22:23.281702 140552972691264 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.282378 140552972691264 make_examples_core.py:257] Task 8/32: Common contigs are ['chr20']. I0519 16:22:23.271428 140087439025984 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.192873 139704511100736 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.234176 139704511100736 make_examples_core.py:257] Task 12/32: Preparing inputs. I0519 16:22:23.236589 139704511100736 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.237330 139704511100736 make_examples_core.py:257] Task 12/32,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:10757,testability,test,testdata,10757,y:257] Task 21/32: Common contigs are ['chr20']. I0519 16:22:23.242177 140053689509696 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.227729 140555533080384 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.280361 140555533080384 make_examples_core.py:257] Task 6/32: Preparing inputs. I0519 16:22:23.282453 140555533080384 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.283533 140555533080384 make_examples_core.py:257] Task 6/32: Common contigs are ['chr20']. I0519 16:22:23.228248 140552972691264 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.279789 140552972691264 make_examples_core.py:257] Task 8/32: Preparing inputs. I0519 16:22:23.281702 140552972691264 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.282378 140552972691264 make_examples_core.py:257] Task 8/32: Common contigs are ['chr20']. I0519 16:22:23.271428 140087439025984 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.192873 139704511100736 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.234176 139704511100736 make_examples_core.py:257] Task 12/32: Preparing inputs. I0519 16:22:23.236589 139704511100736 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.237330 139704511100736 make_examples_core.py:257] Task 12/32: Common contigs are ['chr20']. I0519 16:22:23.237694 140638923466560 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.289134 140638923466560 make_examples_core.py:25,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:11006,testability,test,testdata,11006,222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.280361 140555533080384 make_examples_core.py:257] Task 6/32: Preparing inputs. I0519 16:22:23.282453 140555533080384 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.283533 140555533080384 make_examples_core.py:257] Task 6/32: Common contigs are ['chr20']. I0519 16:22:23.228248 140552972691264 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.279789 140552972691264 make_examples_core.py:257] Task 8/32: Preparing inputs. I0519 16:22:23.281702 140552972691264 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.282378 140552972691264 make_examples_core.py:257] Task 8/32: Common contigs are ['chr20']. I0519 16:22:23.271428 140087439025984 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.192873 139704511100736 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.234176 139704511100736 make_examples_core.py:257] Task 12/32: Preparing inputs. I0519 16:22:23.236589 139704511100736 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.237330 139704511100736 make_examples_core.py:257] Task 12/32: Common contigs are ['chr20']. I0519 16:22:23.237694 140638923466560 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.289134 140638923466560 make_examples_core.py:257] Task 29/32: Preparing inputs. I0519 16:22:23.215111 139798323177280 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.265897 139798323177280 make_examples_core.py:257] Task 9/3,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:11149,testability,test,testdata,11149,py:257] Task 6/32: Preparing inputs. I0519 16:22:23.282453 140555533080384 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.283533 140555533080384 make_examples_core.py:257] Task 6/32: Common contigs are ['chr20']. I0519 16:22:23.228248 140552972691264 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.279789 140552972691264 make_examples_core.py:257] Task 8/32: Preparing inputs. I0519 16:22:23.281702 140552972691264 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.282378 140552972691264 make_examples_core.py:257] Task 8/32: Common contigs are ['chr20']. I0519 16:22:23.271428 140087439025984 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.192873 139704511100736 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.234176 139704511100736 make_examples_core.py:257] Task 12/32: Preparing inputs. I0519 16:22:23.236589 139704511100736 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.237330 139704511100736 make_examples_core.py:257] Task 12/32: Common contigs are ['chr20']. I0519 16:22:23.237694 140638923466560 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.289134 140638923466560 make_examples_core.py:257] Task 29/32: Preparing inputs. I0519 16:22:23.215111 139798323177280 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.265897 139798323177280 make_examples_core.py:257] Task 9/32: Preparing inputs. **********. I0519 16:22:46.781010 139665862911808 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Relo,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:11387,testability,test,testdata,11387,y:257] Task 6/32: Common contigs are ['chr20']. I0519 16:22:23.228248 140552972691264 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.279789 140552972691264 make_examples_core.py:257] Task 8/32: Preparing inputs. I0519 16:22:23.281702 140552972691264 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.282378 140552972691264 make_examples_core.py:257] Task 8/32: Common contigs are ['chr20']. I0519 16:22:23.271428 140087439025984 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.192873 139704511100736 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.234176 139704511100736 make_examples_core.py:257] Task 12/32: Preparing inputs. I0519 16:22:23.236589 139704511100736 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.237330 139704511100736 make_examples_core.py:257] Task 12/32: Common contigs are ['chr20']. I0519 16:22:23.237694 140638923466560 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.289134 140638923466560 make_examples_core.py:257] Task 29/32: Preparing inputs. I0519 16:22:23.215111 139798323177280 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.265897 139798323177280 make_examples_core.py:257] Task 9/32: Preparing inputs. **********. I0519 16:22:46.781010 139665862911808 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0519 16:22:47.119282 139665862911808 modeling.py:418] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt. I0519 16:22:47.119841 139665862911808 saver.py:1410] Restoring parameters from /opt,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:11637,testability,test,testdata,11637,:257] Task 8/32: Preparing inputs. I0519 16:22:23.281702 140552972691264 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.282378 140552972691264 make_examples_core.py:257] Task 8/32: Common contigs are ['chr20']. I0519 16:22:23.271428 140087439025984 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.192873 139704511100736 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.234176 139704511100736 make_examples_core.py:257] Task 12/32: Preparing inputs. I0519 16:22:23.236589 139704511100736 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.237330 139704511100736 make_examples_core.py:257] Task 12/32: Common contigs are ['chr20']. I0519 16:22:23.237694 140638923466560 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.289134 140638923466560 make_examples_core.py:257] Task 29/32: Preparing inputs. I0519 16:22:23.215111 139798323177280 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.265897 139798323177280 make_examples_core.py:257] Task 9/32: Preparing inputs. **********. I0519 16:22:46.781010 139665862911808 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0519 16:22:47.119282 139665862911808 modeling.py:418] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt. I0519 16:22:47.119841 139665862911808 saver.py:1410] Restoring parameters from /opt/models/wgs/model.ckpt. I0519 16:22:48.504039 139665862911808 call_variants.py:462] Processed 1 examples in 1 batches [632.440 sec per 100]. I0519 16:22:48.735930 139665862911808 call_variants.py:468] Processed 305 examples in 1 batches [2.084 sec pe,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:11875,testability,test,testdata,11875,257] Task 8/32: Common contigs are ['chr20']. I0519 16:22:23.271428 140087439025984 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.192873 139704511100736 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.234176 139704511100736 make_examples_core.py:257] Task 12/32: Preparing inputs. I0519 16:22:23.236589 139704511100736 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.237330 139704511100736 make_examples_core.py:257] Task 12/32: Common contigs are ['chr20']. I0519 16:22:23.237694 140638923466560 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.289134 140638923466560 make_examples_core.py:257] Task 29/32: Preparing inputs. I0519 16:22:23.215111 139798323177280 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.265897 139798323177280 make_examples_core.py:257] Task 9/32: Preparing inputs. **********. I0519 16:22:46.781010 139665862911808 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0519 16:22:47.119282 139665862911808 modeling.py:418] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt. I0519 16:22:47.119841 139665862911808 saver.py:1410] Restoring parameters from /opt/models/wgs/model.ckpt. I0519 16:22:48.504039 139665862911808 call_variants.py:462] Processed 1 examples in 1 batches [632.440 sec per 100]. I0519 16:22:48.735930 139665862911808 call_variants.py:468] Processed 305 examples in 1 batches [2.084 sec per 100]. I0519 16:22:48.736088 139665862911808 call_variants.py:471] Done calling variants from a total of 305 examples. real 0m8.934s. user 0m37.643s. sys 0m6.426s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_va,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:12904,testability,test,testdata,12904,"mb.bam with NativeSamReader. I0519 16:22:23.265897 139798323177280 make_examples_core.py:257] Task 9/32: Preparing inputs. **********. I0519 16:22:46.781010 139665862911808 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0519 16:22:47.119282 139665862911808 modeling.py:418] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt. I0519 16:22:47.119841 139665862911808 saver.py:1410] Restoring parameters from /opt/models/wgs/model.ckpt. I0519 16:22:48.504039 139665862911808 call_variants.py:462] Processed 1 examples in 1 batches [632.440 sec per 100]. I0519 16:22:48.735930 139665862911808 call_variants.py:468] Processed 305 examples in 1 batches [2.084 sec per 100]. I0519 16:22:48.736088 139665862911808 call_variants.py:471] Done calling variants from a total of 305 examples. real 0m8.934s. user 0m37.643s. sys 0m6.426s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --infile ""/tmp/tmp6gzkras0/call_variants_output.tfrecord.gz"" --outfile ""singularity-output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp6gzkras0/gvcf.tfrecord@32.gz"" --gvcf_outfile ""singularity-output/output.g.vcf.gz"". 2023-05-19 16:22:49.638487: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0519 16:22:51.170790 140640470185792 postprocess_variants.py:972] Using sample name from call_variants output. Sample name: NA12878. 2023-05-19 16:22:51.171487: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/tmp6gzkras0/call_variants_output.tfrecord.gz. 2023-05-19 16:22:51.173038: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_call",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:12929,testability,unit,unittest,12929,"er. I0519 16:22:23.265897 139798323177280 make_examples_core.py:257] Task 9/32: Preparing inputs. **********. I0519 16:22:46.781010 139665862911808 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0519 16:22:47.119282 139665862911808 modeling.py:418] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt. I0519 16:22:47.119841 139665862911808 saver.py:1410] Restoring parameters from /opt/models/wgs/model.ckpt. I0519 16:22:48.504039 139665862911808 call_variants.py:462] Processed 1 examples in 1 batches [632.440 sec per 100]. I0519 16:22:48.735930 139665862911808 call_variants.py:468] Processed 305 examples in 1 batches [2.084 sec per 100]. I0519 16:22:48.736088 139665862911808 call_variants.py:471] Done calling variants from a total of 305 examples. real 0m8.934s. user 0m37.643s. sys 0m6.426s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --infile ""/tmp/tmp6gzkras0/call_variants_output.tfrecord.gz"" --outfile ""singularity-output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp6gzkras0/gvcf.tfrecord@32.gz"" --gvcf_outfile ""singularity-output/output.g.vcf.gz"". 2023-05-19 16:22:49.638487: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0519 16:22:51.170790 140640470185792 postprocess_variants.py:972] Using sample name from call_variants output. Sample name: NA12878. 2023-05-19 16:22:51.171487: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/tmp6gzkras0/call_variants_output.tfrecord.gz. 2023-05-19 16:22:51.173038: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 305. I0519 16:22:51.1",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:15028,testability,context,context,15028,"put.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp6gzkras0/gvcf.tfrecord@32.gz"" --gvcf_outfile ""singularity-output/output.g.vcf.gz"". 2023-05-19 16:22:49.638487: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0519 16:22:51.170790 140640470185792 postprocess_variants.py:972] Using sample name from call_variants output. Sample name: NA12878. 2023-05-19 16:22:51.171487: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/tmp6gzkras0/call_variants_output.tfrecord.gz. 2023-05-19 16:22:51.173038: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 305. I0519 16:22:51.173760 140640470185792 postprocess_variants.py:1037] CVO sorting took 3.902912139892578e-05 minutes. I0519 16:22:51.173915 140640470185792 postprocess_variants.py:1040] Transforming call_variants_output to variants. I0519 16:22:51.204555 140640470185792 postprocess_variants.py:1080] Processing variants (and writing to temporary file) took 0.0005078872044881184 minutes. I0519 16:22:51.582558 140640470185792 postprocess_variants.py:1093] Finished writing VCF and gVCF in 0.006296555201212565 minutes. I0519 16:22:51.587275 140640470185792 genomics_reader.py:222] Reading singularity-output/output.vcf.gz with NativeVcfReader. I0519 16:22:51.867608 140640470185792 postprocess_variants.py:1104] Generating VCF stats took 0.004742777347564698 minutes. real 0m3.071s. user 0m5.901s. sys 0m4.455s. INFO: Cleaning up image... ######. $ $ ls singularity-output/. output.g.vcf.gz output.vcf.gz output.visual_report.html srun_output.g.vcf.gz.tbi srun_output.vcf.gz.tbi. output.g.vcf.gz.tbi output.vcf.gz.tbi srun_output.g.vcf.gz srun_output.vcf.gz srun_output.visual_report.html. ```. **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:144,usability,clear,clear,144,"DeepVariant Runerror, ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). Hi developers,. I'd like to run `DeepVariant` for my `WGS` sequencing data. My sequencing data were from `BGI` platform and were preprocessed by `fastp, bwa+Hs37d5, MarkDuplicatesSpark`. I tried to use the 'sorted and deduplicated bam' file as input for `DeepVariant` in `singularity` mode. However, I always encountered the 'reference index' `not found` error. But my `reference` fasta file and `reference index` fai file does exist. Could you please help me figure it out? **Setup**. - Operating system: Linux version 3.10.0-1127.el7.x86_64 (gcc version 4.8.5 20150623 (Red Hat 4.8.5-39), Computation Node (one node of Clusters). - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.):. - ``` BIN_VERSION=""1.5.0"". docker pull; . singularity pull docker://google/deepvariant:""${BIN_VERSION}"". singularity build --fakeroot deepvariant.sif docker://google/deepvariant:1.5.0```. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). `BGI platform, WGS data, Hs37d5 reference, fastp QC, bwa-mem2 mapping, MarkDuplicatesSpark sort & dedup`. . **Steps to reproduce:**. - Command:. - 1. singularity run /lustre/Data/toolsDB//deepvariant.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=$ref_idx --reads=$dedupbam --output_vcf=$vcfout --output_gvcf=$gvcfout --num_shards=32 >$logx 2>&1. - Error trace: (if applicable). - ```I0522 08:40:36.823651 140633630893888 genomics_reader.py:222] Reading /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam with NativeSamReader. I0522 08:40:36.846348 140633630893888 make_examples_core.py:257] Task 27/32: Preparing inputs. [E::fai_load3_core] Failed to open FASTA index /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:442,usability,input,input,442,"DeepVariant Runerror, ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). Hi developers,. I'd like to run `DeepVariant` for my `WGS` sequencing data. My sequencing data were from `BGI` platform and were preprocessed by `fastp, bwa+Hs37d5, MarkDuplicatesSpark`. I tried to use the 'sorted and deduplicated bam' file as input for `DeepVariant` in `singularity` mode. However, I always encountered the 'reference index' `not found` error. But my `reference` fasta file and `reference index` fai file does exist. Could you please help me figure it out? **Setup**. - Operating system: Linux version 3.10.0-1127.el7.x86_64 (gcc version 4.8.5 20150623 (Red Hat 4.8.5-39), Computation Node (one node of Clusters). - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.):. - ``` BIN_VERSION=""1.5.0"". docker pull; . singularity pull docker://google/deepvariant:""${BIN_VERSION}"". singularity build --fakeroot deepvariant.sif docker://google/deepvariant:1.5.0```. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). `BGI platform, WGS data, Hs37d5 reference, fastp QC, bwa-mem2 mapping, MarkDuplicatesSpark sort & dedup`. . **Steps to reproduce:**. - Command:. - 1. singularity run /lustre/Data/toolsDB//deepvariant.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=$ref_idx --reads=$dedupbam --output_vcf=$vcfout --output_gvcf=$gvcfout --num_shards=32 >$logx 2>&1. - Error trace: (if applicable). - ```I0522 08:40:36.823651 140633630893888 genomics_reader.py:222] Reading /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam with NativeSamReader. I0522 08:40:36.846348 140633630893888 make_examples_core.py:257] Task 27/32: Preparing inputs. [E::fai_load3_core] Failed to open FASTA index /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:553,usability,error,error,553,"DeepVariant Runerror, ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). Hi developers,. I'd like to run `DeepVariant` for my `WGS` sequencing data. My sequencing data were from `BGI` platform and were preprocessed by `fastp, bwa+Hs37d5, MarkDuplicatesSpark`. I tried to use the 'sorted and deduplicated bam' file as input for `DeepVariant` in `singularity` mode. However, I always encountered the 'reference index' `not found` error. But my `reference` fasta file and `reference index` fai file does exist. Could you please help me figure it out? **Setup**. - Operating system: Linux version 3.10.0-1127.el7.x86_64 (gcc version 4.8.5 20150623 (Red Hat 4.8.5-39), Computation Node (one node of Clusters). - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.):. - ``` BIN_VERSION=""1.5.0"". docker pull; . singularity pull docker://google/deepvariant:""${BIN_VERSION}"". singularity build --fakeroot deepvariant.sif docker://google/deepvariant:1.5.0```. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). `BGI platform, WGS data, Hs37d5 reference, fastp QC, bwa-mem2 mapping, MarkDuplicatesSpark sort & dedup`. . **Steps to reproduce:**. - Command:. - 1. singularity run /lustre/Data/toolsDB//deepvariant.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=$ref_idx --reads=$dedupbam --output_vcf=$vcfout --output_gvcf=$gvcfout --num_shards=32 >$logx 2>&1. - Error trace: (if applicable). - ```I0522 08:40:36.823651 140633630893888 genomics_reader.py:222] Reading /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam with NativeSamReader. I0522 08:40:36.846348 140633630893888 make_examples_core.py:257] Task 27/32: Preparing inputs. [E::fai_load3_core] Failed to open FASTA index /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:650,usability,help,help,650,"DeepVariant Runerror, ; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). Hi developers,. I'd like to run `DeepVariant` for my `WGS` sequencing data. My sequencing data were from `BGI` platform and were preprocessed by `fastp, bwa+Hs37d5, MarkDuplicatesSpark`. I tried to use the 'sorted and deduplicated bam' file as input for `DeepVariant` in `singularity` mode. However, I always encountered the 'reference index' `not found` error. But my `reference` fasta file and `reference index` fai file does exist. Could you please help me figure it out? **Setup**. - Operating system: Linux version 3.10.0-1127.el7.x86_64 (gcc version 4.8.5 20150623 (Red Hat 4.8.5-39), Computation Node (one node of Clusters). - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.):. - ``` BIN_VERSION=""1.5.0"". docker pull; . singularity pull docker://google/deepvariant:""${BIN_VERSION}"". singularity build --fakeroot deepvariant.sif docker://google/deepvariant:1.5.0```. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). `BGI platform, WGS data, Hs37d5 reference, fastp QC, bwa-mem2 mapping, MarkDuplicatesSpark sort & dedup`. . **Steps to reproduce:**. - Command:. - 1. singularity run /lustre/Data/toolsDB//deepvariant.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=$ref_idx --reads=$dedupbam --output_vcf=$vcfout --output_gvcf=$gvcfout --num_shards=32 >$logx 2>&1. - Error trace: (if applicable). - ```I0522 08:40:36.823651 140633630893888 genomics_reader.py:222] Reading /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam with NativeSamReader. I0522 08:40:36.846348 140633630893888 make_examples_core.py:257] Task 27/32: Preparing inputs. [E::fai_load3_core] Failed to open FASTA index /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:1351,usability,Command,Command,1351,"Hs37d5, MarkDuplicatesSpark`. I tried to use the 'sorted and deduplicated bam' file as input for `DeepVariant` in `singularity` mode. However, I always encountered the 'reference index' `not found` error. But my `reference` fasta file and `reference index` fai file does exist. Could you please help me figure it out? **Setup**. - Operating system: Linux version 3.10.0-1127.el7.x86_64 (gcc version 4.8.5 20150623 (Red Hat 4.8.5-39), Computation Node (one node of Clusters). - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.):. - ``` BIN_VERSION=""1.5.0"". docker pull; . singularity pull docker://google/deepvariant:""${BIN_VERSION}"". singularity build --fakeroot deepvariant.sif docker://google/deepvariant:1.5.0```. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). `BGI platform, WGS data, Hs37d5 reference, fastp QC, bwa-mem2 mapping, MarkDuplicatesSpark sort & dedup`. . **Steps to reproduce:**. - Command:. - 1. singularity run /lustre/Data/toolsDB//deepvariant.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=$ref_idx --reads=$dedupbam --output_vcf=$vcfout --output_gvcf=$gvcfout --num_shards=32 >$logx 2>&1. - Error trace: (if applicable). - ```I0522 08:40:36.823651 140633630893888 genomics_reader.py:222] Reading /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam with NativeSamReader. I0522 08:40:36.846348 140633630893888 make_examples_core.py:257] Task 27/32: Preparing inputs. [E::fai_load3_core] Failed to open FASTA index /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai: No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_c22i4j8u/runfi",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:1395,usability,tool,toolsDB,1395," the 'sorted and deduplicated bam' file as input for `DeepVariant` in `singularity` mode. However, I always encountered the 'reference index' `not found` error. But my `reference` fasta file and `reference index` fai file does exist. Could you please help me figure it out? **Setup**. - Operating system: Linux version 3.10.0-1127.el7.x86_64 (gcc version 4.8.5 20150623 (Red Hat 4.8.5-39), Computation Node (one node of Clusters). - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.):. - ``` BIN_VERSION=""1.5.0"". docker pull; . singularity pull docker://google/deepvariant:""${BIN_VERSION}"". singularity build --fakeroot deepvariant.sif docker://google/deepvariant:1.5.0```. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). `BGI platform, WGS data, Hs37d5 reference, fastp QC, bwa-mem2 mapping, MarkDuplicatesSpark sort & dedup`. . **Steps to reproduce:**. - Command:. - 1. singularity run /lustre/Data/toolsDB//deepvariant.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=$ref_idx --reads=$dedupbam --output_vcf=$vcfout --output_gvcf=$gvcfout --num_shards=32 >$logx 2>&1. - Error trace: (if applicable). - ```I0522 08:40:36.823651 140633630893888 genomics_reader.py:222] Reading /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam with NativeSamReader. I0522 08:40:36.846348 140633630893888 make_examples_core.py:257] Task 27/32: Preparing inputs. [E::fai_load3_core] Failed to open FASTA index /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai: No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/absl_py/absl/app.py"", line 258, in _run_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:1582,usability,Error,Error,1582,"file and `reference index` fai file does exist. Could you please help me figure it out? **Setup**. - Operating system: Linux version 3.10.0-1127.el7.x86_64 (gcc version 4.8.5 20150623 (Red Hat 4.8.5-39), Computation Node (one node of Clusters). - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.):. - ``` BIN_VERSION=""1.5.0"". docker pull; . singularity pull docker://google/deepvariant:""${BIN_VERSION}"". singularity build --fakeroot deepvariant.sif docker://google/deepvariant:1.5.0```. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). `BGI platform, WGS data, Hs37d5 reference, fastp QC, bwa-mem2 mapping, MarkDuplicatesSpark sort & dedup`. . **Steps to reproduce:**. - Command:. - 1. singularity run /lustre/Data/toolsDB//deepvariant.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=$ref_idx --reads=$dedupbam --output_vcf=$vcfout --output_gvcf=$gvcfout --num_shards=32 >$logx 2>&1. - Error trace: (if applicable). - ```I0522 08:40:36.823651 140633630893888 genomics_reader.py:222] Reading /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam with NativeSamReader. I0522 08:40:36.846348 140633630893888 make_examples_core.py:257] Task 27/32: Preparing inputs. [E::fai_load3_core] Failed to open FASTA index /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai: No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main. make_examples_core.make_examples_runner(o",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:1896,usability,input,inputs,1896,"om source, etc.):. - ``` BIN_VERSION=""1.5.0"". docker pull; . singularity pull docker://google/deepvariant:""${BIN_VERSION}"". singularity build --fakeroot deepvariant.sif docker://google/deepvariant:1.5.0```. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). `BGI platform, WGS data, Hs37d5 reference, fastp QC, bwa-mem2 mapping, MarkDuplicatesSpark sort & dedup`. . **Steps to reproduce:**. - Command:. - 1. singularity run /lustre/Data/toolsDB//deepvariant.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=$ref_idx --reads=$dedupbam --output_vcf=$vcfout --output_gvcf=$gvcfout --num_shards=32 >$logx 2>&1. - Error trace: (if applicable). - ```I0522 08:40:36.823651 140633630893888 genomics_reader.py:222] Reading /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam with NativeSamReader. I0522 08:40:36.846348 140633630893888 make_examples_core.py:257] Task 27/32: Preparing inputs. [E::fai_load3_core] Failed to open FASTA index /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai: No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2114, in make_examples_runner. regions, calling_regions = processing_regions_from_options(options). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/deepvariant/make_examples_core",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:1964,usability,tool,toolsDB,1964,"ity pull docker://google/deepvariant:""${BIN_VERSION}"". singularity build --fakeroot deepvariant.sif docker://google/deepvariant:1.5.0```. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). `BGI platform, WGS data, Hs37d5 reference, fastp QC, bwa-mem2 mapping, MarkDuplicatesSpark sort & dedup`. . **Steps to reproduce:**. - Command:. - 1. singularity run /lustre/Data/toolsDB//deepvariant.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=$ref_idx --reads=$dedupbam --output_vcf=$vcfout --output_gvcf=$gvcfout --num_shards=32 >$logx 2>&1. - Error trace: (if applicable). - ```I0522 08:40:36.823651 140633630893888 genomics_reader.py:222] Reading /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam with NativeSamReader. I0522 08:40:36.846348 140633630893888 make_examples_core.py:257] Task 27/32: Preparing inputs. [E::fai_load3_core] Failed to open FASTA index /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai: No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2114, in make_examples_runner. regions, calling_regions = processing_regions_from_options(options). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2019, in processing_regions_from_options. ref_contigs = fa",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:3251,usability,tool,toolsDB,3251,"l_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2114, in make_examples_runner. regions, calling_regions = processing_regions_from_options(options). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2019, in processing_regions_from_options. ref_contigs = fasta.IndexedFastaReader(. File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 106, in __init__. self._reader = reference.IndexedFastaReader.from_file(. ValueError: NOT_FOUND: could not load fasta and/or fai for fasta /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. I0522 08:40:37.063492 139925831092032 genomics_reader.py:222] Reading /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam with NativeSamReader. I0522 08:40:37.081499 139925831092032 make_examples_core.py:257] Task 6/32: Preparing inputs. [E::fai_load3_core] Failed to open FASTA index /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai: No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_luflf_op/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_luflf_op/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_luflf_op/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_luflf_op/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main. make_examples_core.make_examples_runn",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:3570,usability,input,inputs,3570,"les_runner(options). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2114, in make_examples_runner. regions, calling_regions = processing_regions_from_options(options). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2019, in processing_regions_from_options. ref_contigs = fasta.IndexedFastaReader(. File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 106, in __init__. self._reader = reference.IndexedFastaReader.from_file(. ValueError: NOT_FOUND: could not load fasta and/or fai for fasta /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. I0522 08:40:37.063492 139925831092032 genomics_reader.py:222] Reading /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam with NativeSamReader. I0522 08:40:37.081499 139925831092032 make_examples_core.py:257] Task 6/32: Preparing inputs. [E::fai_load3_core] Failed to open FASTA index /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai: No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_luflf_op/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_luflf_op/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_luflf_op/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_luflf_op/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_luflf_op/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2114, in make_examples_runner. regions, calling_regions = processing_regions_from_options(options). File ""/tmp/Bazel.runfiles_luflf_op/runfiles/com_google_deepvariant/deepvariant/make_examples_core",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:3638,usability,tool,toolsDB,3638,"google_deepvariant/deepvariant/make_examples_core.py"", line 2114, in make_examples_runner. regions, calling_regions = processing_regions_from_options(options). File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2019, in processing_regions_from_options. ref_contigs = fasta.IndexedFastaReader(. File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 106, in __init__. self._reader = reference.IndexedFastaReader.from_file(. ValueError: NOT_FOUND: could not load fasta and/or fai for fasta /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. I0522 08:40:37.063492 139925831092032 genomics_reader.py:222] Reading /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam with NativeSamReader. I0522 08:40:37.081499 139925831092032 make_examples_core.py:257] Task 6/32: Preparing inputs. [E::fai_load3_core] Failed to open FASTA index /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai: No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_luflf_op/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_luflf_op/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_luflf_op/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_luflf_op/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_luflf_op/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2114, in make_examples_runner. regions, calling_regions = processing_regions_from_options(options). File ""/tmp/Bazel.runfiles_luflf_op/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2019, in processing_regions_from_options. ref_contigs = fa",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:5123,usability,tool,toolsDB,5123,"luflf_op/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_luflf_op/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2114, in make_examples_runner. regions, calling_regions = processing_regions_from_options(options). File ""/tmp/Bazel.runfiles_luflf_op/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2019, in processing_regions_from_options. ref_contigs = fasta.IndexedFastaReader(. File ""/tmp/Bazel.runfiles_luflf_op/runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 106, in __init__. self._reader = reference.IndexedFastaReader.from_file(. ****************. File ""/tmp/Bazel.runfiles_8r9pgqu5/runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 106, in __init__. self._reader = reference.IndexedFastaReader.from_file(. ValueError: NOT_FOUND: could not load fasta and/or fai for fasta /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. I0522 08:40:37.129531 140035375298368 make_examples_core.py:257] Task 7/32: Preparing inputs. [E::fai_load3_core] Failed to open FASTA index /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai: No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2114, in make_examples_runner. regions, calling_regions = processing_re",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:5250,usability,input,inputs,5250,"nner(options). File ""/tmp/Bazel.runfiles_luflf_op/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2114, in make_examples_runner. regions, calling_regions = processing_regions_from_options(options). File ""/tmp/Bazel.runfiles_luflf_op/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2019, in processing_regions_from_options. ref_contigs = fasta.IndexedFastaReader(. File ""/tmp/Bazel.runfiles_luflf_op/runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 106, in __init__. self._reader = reference.IndexedFastaReader.from_file(. ****************. File ""/tmp/Bazel.runfiles_8r9pgqu5/runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 106, in __init__. self._reader = reference.IndexedFastaReader.from_file(. ValueError: NOT_FOUND: could not load fasta and/or fai for fasta /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. I0522 08:40:37.129531 140035375298368 make_examples_core.py:257] Task 7/32: Preparing inputs. [E::fai_load3_core] Failed to open FASTA index /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai: No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2114, in make_examples_runner. regions, calling_regions = processing_regions_from_options(options). File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/deepvariant/make_examples_core",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:5318,usability,tool,toolsDB,5318,"_deepvariant/deepvariant/make_examples_core.py"", line 2114, in make_examples_runner. regions, calling_regions = processing_regions_from_options(options). File ""/tmp/Bazel.runfiles_luflf_op/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2019, in processing_regions_from_options. ref_contigs = fasta.IndexedFastaReader(. File ""/tmp/Bazel.runfiles_luflf_op/runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 106, in __init__. self._reader = reference.IndexedFastaReader.from_file(. ****************. File ""/tmp/Bazel.runfiles_8r9pgqu5/runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 106, in __init__. self._reader = reference.IndexedFastaReader.from_file(. ValueError: NOT_FOUND: could not load fasta and/or fai for fasta /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. I0522 08:40:37.129531 140035375298368 make_examples_core.py:257] Task 7/32: Preparing inputs. [E::fai_load3_core] Failed to open FASTA index /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai: No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2114, in make_examples_runner. regions, calling_regions = processing_regions_from_options(options). File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2019, in processing_regions_from_options. ref_contigs = fa",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:6605,usability,tool,toolsDB,6605,"l_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2114, in make_examples_runner. regions, calling_regions = processing_regions_from_options(options). File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2019, in processing_regions_from_options. ref_contigs = fasta.IndexedFastaReader(. File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 106, in __init__. self._reader = reference.IndexedFastaReader.from_file(. ValueError: NOT_FOUND: could not load fasta and/or fai for fasta /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa --reads /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam --examples /tmp/tmpfab4tpv7/make_examples.tfrecord@32.gz --channels insert_size --gvcf /tmp/tmpfab4tpv7/gvcf.tfrecord@32.gz --task 18. - ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the quick test run as normal. ```. 3. reference index does. ```$ ls /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467 Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909010 Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.f",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:6743,usability,tool,toolsDB,6743," _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2114, in make_examples_runner. regions, calling_regions = processing_regions_from_options(options). File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2019, in processing_regions_from_options. ref_contigs = fasta.IndexedFastaReader(. File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 106, in __init__. self._reader = reference.IndexedFastaReader.from_file(. ValueError: NOT_FOUND: could not load fasta and/or fai for fasta /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa --reads /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam --examples /tmp/tmpfab4tpv7/make_examples.tfrecord@32.gz --channels insert_size --gvcf /tmp/tmpfab4tpv7/gvcf.tfrecord@32.gz --task 18. - ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the quick test run as normal. ```. 3. reference index does. ```$ ls /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467 Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909010 Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 106669 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:7336,usability,tool,toolsDB,7336,"ader(. File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 106, in __init__. self._reader = reference.IndexedFastaReader.from_file(. ValueError: NOT_FOUND: could not load fasta and/or fai for fasta /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa --reads /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam --examples /tmp/tmpfab4tpv7/make_examples.tfrecord@32.gz --channels insert_size --gvcf /tmp/tmpfab4tpv7/gvcf.tfrecord@32.gz --task 18. - ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the quick test run as normal. ```. 3. reference index does. ```$ ls /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467 Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909010 Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 106669 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6924 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 10196727247 Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 2813 May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 784363628 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```. **************. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-05-19 16",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:7454,usability,tool,toolsDB,7454,"in __init__. self._reader = reference.IndexedFastaReader.from_file(. ValueError: NOT_FOUND: could not load fasta and/or fai for fasta /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa --reads /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam --examples /tmp/tmpfab4tpv7/make_examples.tfrecord@32.gz --channels insert_size --gvcf /tmp/tmpfab4tpv7/gvcf.tfrecord@32.gz --task 18. - ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the quick test run as normal. ```. 3. reference index does. ```$ ls /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467 Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909010 Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 106669 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6924 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 10196727247 Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 2813 May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 784363628 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```. **************. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-05-19 16:22:21.555857: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI De",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:7571,usability,tool,toolsDB,7571,"or fai for fasta /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa --reads /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam --examples /tmp/tmpfab4tpv7/make_examples.tfrecord@32.gz --channels insert_size --gvcf /tmp/tmpfab4tpv7/gvcf.tfrecord@32.gz --task 18. - ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the quick test run as normal. ```. 3. reference index does. ```$ ls /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467 Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909010 Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 106669 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6924 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 10196727247 Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 2813 May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 784363628 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```. **************. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-05-19 16:22:21.555857: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (o. neDNN) to use the following CPU instructions in performance-critical operations: AVX2 A",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:7689,usability,tool,toolsDB,7689,"n/make_examples --mode calling --ref /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa --reads /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam --examples /tmp/tmpfab4tpv7/make_examples.tfrecord@32.gz --channels insert_size --gvcf /tmp/tmpfab4tpv7/gvcf.tfrecord@32.gz --task 18. - ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the quick test run as normal. ```. 3. reference index does. ```$ ls /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467 Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909010 Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 106669 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6924 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 10196727247 Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 2813 May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 784363628 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```. **************. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-05-19 16:22:21.555857: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (o. neDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:7804,usability,tool,toolsDB,7804,"jianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam --examples /tmp/tmpfab4tpv7/make_examples.tfrecord@32.gz --channels insert_size --gvcf /tmp/tmpfab4tpv7/gvcf.tfrecord@32.gz --task 18. - ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the quick test run as normal. ```. 3. reference index does. ```$ ls /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467 Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909010 Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 106669 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6924 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 10196727247 Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 2813 May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 784363628 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```. **************. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-05-19 16:22:21.555857: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (o. neDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0519 16:22:23.193474 139896863770432 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:7926,usability,tool,toolsDB,7926,"es.tfrecord@32.gz --channels insert_size --gvcf /tmp/tmpfab4tpv7/gvcf.tfrecord@32.gz --task 18. - ```. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the quick test run as normal. ```. 3. reference index does. ```$ ls /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467 Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909010 Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 106669 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6924 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 10196727247 Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 2813 May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 784363628 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```. **************. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-05-19 16:22:21.555857: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (o. neDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0519 16:22:23.193474 139896863770432 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.256151 139896863770432 make_examples_core.py:257] Task 10/32: Preparing inputs. I",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:8049,usability,tool,toolsDB,8049,"rt test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the quick test run as normal. ```. 3. reference index does. ```$ ls /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467 Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909010 Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 106669 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6924 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 10196727247 Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 2813 May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 784363628 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```. **************. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-05-19 16:22:21.555857: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (o. neDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0519 16:22:23.193474 139896863770432 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.256151 139896863770432 make_examples_core.py:257] Task 10/32: Preparing inputs. I0519 16:22:23.258605 139896863770432 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam wit",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:8169,usability,tool,toolsDB,8169,"-start.md. Is there any way to reproduce the issue by using the quick start? Yes, the quick test run as normal. ```. 3. reference index does. ```$ ls /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467 Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909010 Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 106669 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6924 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 10196727247 Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 2813 May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 784363628 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```. **************. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-05-19 16:22:21.555857: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (o. neDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0519 16:22:23.193474 139896863770432 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.256151 139896863770432 make_examples_core.py:257] Task 10/32: Preparing inputs. I0519 16:22:23.258605 139896863770432 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.259495 139896863770432 make_examples_core.py:257] Task 10/32: Common contigs are ['chr",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:8536,usability,perform,performance-critical,8536,"pr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123. -rw-rw-r-- 1 zhoujianglin zhoujianglin 106669 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb. -rw-rw-r-- 1 zhoujianglin zhoujianglin 6924 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann. -rw-rw-r-- 1 zhoujianglin zhoujianglin 10196727247 Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 2813 May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 784363628 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```. **************. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-05-19 16:22:21.555857: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (o. neDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0519 16:22:23.193474 139896863770432 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.256151 139896863770432 make_examples_core.py:257] Task 10/32: Preparing inputs. I0519 16:22:23.258605 139896863770432 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.259495 139896863770432 make_examples_core.py:257] Task 10/32: Common contigs are ['chr20']. I0519 16:22:23.239336 140148036429632 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.192739 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.235120 140421750466368 make_examples_core.py:257] Task 21/32: Prep",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:8921,usability,input,inputs,8921,"a/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64. -rw-rw-r-- 1 zhoujianglin zhoujianglin 2813 May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai. -rw-rw-r-- 1 zhoujianglin zhoujianglin 784363628 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```. **************. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-05-19 16:22:21.555857: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (o. neDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0519 16:22:23.193474 139896863770432 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.256151 139896863770432 make_examples_core.py:257] Task 10/32: Preparing inputs. I0519 16:22:23.258605 139896863770432 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.259495 139896863770432 make_examples_core.py:257] Task 10/32: Common contigs are ['chr20']. I0519 16:22:23.239336 140148036429632 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.192739 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.235120 140421750466368 make_examples_core.py:257] Task 21/32: Preparing inputs. I0519 16:22:23.239059 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.240968 140421750466368 make_examples_core.py:257] Task 21/32: Common contigs are ['chr20']. I0519 16:22:23.242177 140053689509696 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:9552,usability,input,inputs,9552,"l operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0519 16:22:23.193474 139896863770432 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.256151 139896863770432 make_examples_core.py:257] Task 10/32: Preparing inputs. I0519 16:22:23.258605 139896863770432 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.259495 139896863770432 make_examples_core.py:257] Task 10/32: Common contigs are ['chr20']. I0519 16:22:23.239336 140148036429632 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.192739 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.235120 140421750466368 make_examples_core.py:257] Task 21/32: Preparing inputs. I0519 16:22:23.239059 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.240968 140421750466368 make_examples_core.py:257] Task 21/32: Common contigs are ['chr20']. I0519 16:22:23.242177 140053689509696 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.227729 140555533080384 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.280361 140555533080384 make_examples_core.py:257] Task 6/32: Preparing inputs. I0519 16:22:23.282453 140555533080384 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.283533 140555533080384 make_examples_core.py:257] Task 6/32: Common contigs are ['chr20']. I0519 16:22:23.228248 140552972691264 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:10182,usability,input,inputs,10182,16:22:23.239336 140148036429632 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.192739 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.235120 140421750466368 make_examples_core.py:257] Task 21/32: Preparing inputs. I0519 16:22:23.239059 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.240968 140421750466368 make_examples_core.py:257] Task 21/32: Common contigs are ['chr20']. I0519 16:22:23.242177 140053689509696 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.227729 140555533080384 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.280361 140555533080384 make_examples_core.py:257] Task 6/32: Preparing inputs. I0519 16:22:23.282453 140555533080384 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.283533 140555533080384 make_examples_core.py:257] Task 6/32: Common contigs are ['chr20']. I0519 16:22:23.228248 140552972691264 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.279789 140552972691264 make_examples_core.py:257] Task 8/32: Preparing inputs. I0519 16:22:23.281702 140552972691264 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.282378 140552972691264 make_examples_core.py:257] Task 8/32: Common contigs are ['chr20']. I0519 16:22:23.271428 140087439025984 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.192873 139704511100736 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:10668,usability,input,inputs,10668,0p1mb.bam with NativeSamReader. I0519 16:22:23.240968 140421750466368 make_examples_core.py:257] Task 21/32: Common contigs are ['chr20']. I0519 16:22:23.242177 140053689509696 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.227729 140555533080384 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.280361 140555533080384 make_examples_core.py:257] Task 6/32: Preparing inputs. I0519 16:22:23.282453 140555533080384 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.283533 140555533080384 make_examples_core.py:257] Task 6/32: Common contigs are ['chr20']. I0519 16:22:23.228248 140552972691264 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.279789 140552972691264 make_examples_core.py:257] Task 8/32: Preparing inputs. I0519 16:22:23.281702 140552972691264 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.282378 140552972691264 make_examples_core.py:257] Task 8/32: Common contigs are ['chr20']. I0519 16:22:23.271428 140087439025984 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.192873 139704511100736 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.234176 139704511100736 make_examples_core.py:257] Task 12/32: Preparing inputs. I0519 16:22:23.236589 139704511100736 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.237330 139704511100736 make_examples_core.py:257] Task 12/32: Common contigs are ['chr20']. I0519 16:22:23.237694 140638923466560 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1m,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:11298,usability,input,inputs,11298,0p1mb.bam with NativeSamReader. I0519 16:22:23.283533 140555533080384 make_examples_core.py:257] Task 6/32: Common contigs are ['chr20']. I0519 16:22:23.228248 140552972691264 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.279789 140552972691264 make_examples_core.py:257] Task 8/32: Preparing inputs. I0519 16:22:23.281702 140552972691264 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.282378 140552972691264 make_examples_core.py:257] Task 8/32: Common contigs are ['chr20']. I0519 16:22:23.271428 140087439025984 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.192873 139704511100736 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.234176 139704511100736 make_examples_core.py:257] Task 12/32: Preparing inputs. I0519 16:22:23.236589 139704511100736 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.237330 139704511100736 make_examples_core.py:257] Task 12/32: Common contigs are ['chr20']. I0519 16:22:23.237694 140638923466560 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.289134 140638923466560 make_examples_core.py:257] Task 29/32: Preparing inputs. I0519 16:22:23.215111 139798323177280 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.265897 139798323177280 make_examples_core.py:257] Task 9/32: Preparing inputs. **********. I0519 16:22:46.781010 139665862911808 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0519 16:22:47.119282 139665862911808 modeling.py:418] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/wgs/model,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:11786,usability,input,inputs,11786,1mb.bam with NativeSamReader. I0519 16:22:23.282378 140552972691264 make_examples_core.py:257] Task 8/32: Common contigs are ['chr20']. I0519 16:22:23.271428 140087439025984 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.192873 139704511100736 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.234176 139704511100736 make_examples_core.py:257] Task 12/32: Preparing inputs. I0519 16:22:23.236589 139704511100736 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.237330 139704511100736 make_examples_core.py:257] Task 12/32: Common contigs are ['chr20']. I0519 16:22:23.237694 140638923466560 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.289134 140638923466560 make_examples_core.py:257] Task 29/32: Preparing inputs. I0519 16:22:23.215111 139798323177280 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.265897 139798323177280 make_examples_core.py:257] Task 9/32: Preparing inputs. **********. I0519 16:22:46.781010 139665862911808 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0519 16:22:47.119282 139665862911808 modeling.py:418] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt. I0519 16:22:47.119841 139665862911808 saver.py:1410] Restoring parameters from /opt/models/wgs/model.ckpt. I0519 16:22:48.504039 139665862911808 call_variants.py:462] Processed 1 examples in 1 batches [632.440 sec per 100]. I0519 16:22:48.735930 139665862911808 call_variants.py:468] Processed 305 examples in 1 batches [2.084 sec per 100]. I0519 16:22:48.736088 139665862911808 call_variants.py:471] Done calling variants from a total of 305 examples. real 0m8.934s. user 0m37.643,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:12023,usability,input,inputs,12023,"chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.192873 139704511100736 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.234176 139704511100736 make_examples_core.py:257] Task 12/32: Preparing inputs. I0519 16:22:23.236589 139704511100736 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.237330 139704511100736 make_examples_core.py:257] Task 12/32: Common contigs are ['chr20']. I0519 16:22:23.237694 140638923466560 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.289134 140638923466560 make_examples_core.py:257] Task 29/32: Preparing inputs. I0519 16:22:23.215111 139798323177280 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.265897 139798323177280 make_examples_core.py:257] Task 9/32: Preparing inputs. **********. I0519 16:22:46.781010 139665862911808 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0519 16:22:47.119282 139665862911808 modeling.py:418] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt. I0519 16:22:47.119841 139665862911808 saver.py:1410] Restoring parameters from /opt/models/wgs/model.ckpt. I0519 16:22:48.504039 139665862911808 call_variants.py:462] Processed 1 examples in 1 batches [632.440 sec per 100]. I0519 16:22:48.735930 139665862911808 call_variants.py:468] Processed 305 examples in 1 batches [2.084 sec per 100]. I0519 16:22:48.736088 139665862911808 call_variants.py:471] Done calling variants from a total of 305 examples. real 0m8.934s. user 0m37.643s. sys 0m6.426s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --infile ""/tmp/tmp6gzkras0/call_variants_output.tfrecord.gz"" --outfile ""singulari",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:12776,usability,user,user,12776,"eparing inputs. I0519 16:22:23.215111 139798323177280 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.265897 139798323177280 make_examples_core.py:257] Task 9/32: Preparing inputs. **********. I0519 16:22:46.781010 139665862911808 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0519 16:22:47.119282 139665862911808 modeling.py:418] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt. I0519 16:22:47.119841 139665862911808 saver.py:1410] Restoring parameters from /opt/models/wgs/model.ckpt. I0519 16:22:48.504039 139665862911808 call_variants.py:462] Processed 1 examples in 1 batches [632.440 sec per 100]. I0519 16:22:48.735930 139665862911808 call_variants.py:468] Processed 305 examples in 1 batches [2.084 sec per 100]. I0519 16:22:48.736088 139665862911808 call_variants.py:471] Done calling variants from a total of 305 examples. real 0m8.934s. user 0m37.643s. sys 0m6.426s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --infile ""/tmp/tmp6gzkras0/call_variants_output.tfrecord.gz"" --outfile ""singularity-output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp6gzkras0/gvcf.tfrecord@32.gz"" --gvcf_outfile ""singularity-output/output.g.vcf.gz"". 2023-05-19 16:22:49.638487: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0519 16:22:51.170790 140640470185792 postprocess_variants.py:972] Using sample name from call_variants output. Sample name: NA12878. 2023-05-19 16:22:51.171487: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/tmp6gzkras0/call_vari",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:12824,usability,command,command,12824,"280 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0519 16:22:23.265897 139798323177280 make_examples_core.py:257] Task 9/32: Preparing inputs. **********. I0519 16:22:46.781010 139665862911808 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0519 16:22:47.119282 139665862911808 modeling.py:418] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt. I0519 16:22:47.119841 139665862911808 saver.py:1410] Restoring parameters from /opt/models/wgs/model.ckpt. I0519 16:22:48.504039 139665862911808 call_variants.py:462] Processed 1 examples in 1 batches [632.440 sec per 100]. I0519 16:22:48.735930 139665862911808 call_variants.py:468] Processed 305 examples in 1 batches [2.084 sec per 100]. I0519 16:22:48.736088 139665862911808 call_variants.py:471] Done calling variants from a total of 305 examples. real 0m8.934s. user 0m37.643s. sys 0m6.426s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --infile ""/tmp/tmp6gzkras0/call_variants_output.tfrecord.gz"" --outfile ""singularity-output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp6gzkras0/gvcf.tfrecord@32.gz"" --gvcf_outfile ""singularity-output/output.g.vcf.gz"". 2023-05-19 16:22:49.638487: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0519 16:22:51.170790 140640470185792 postprocess_variants.py:972] Using sample name from call_variants output. Sample name: NA12878. 2023-05-19 16:22:51.171487: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/tmp6gzkras0/call_variants_output.tfrecord.gz. 2023-05-19 16:22:51.17303",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:13382,usability,perform,performance-critical,13382,"models/wgs/model.ckpt. I0519 16:22:48.504039 139665862911808 call_variants.py:462] Processed 1 examples in 1 batches [632.440 sec per 100]. I0519 16:22:48.735930 139665862911808 call_variants.py:468] Processed 305 examples in 1 batches [2.084 sec per 100]. I0519 16:22:48.736088 139665862911808 call_variants.py:471] Done calling variants from a total of 305 examples. real 0m8.934s. user 0m37.643s. sys 0m6.426s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --infile ""/tmp/tmp6gzkras0/call_variants_output.tfrecord.gz"" --outfile ""singularity-output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp6gzkras0/gvcf.tfrecord@32.gz"" --gvcf_outfile ""singularity-output/output.g.vcf.gz"". 2023-05-19 16:22:49.638487: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0519 16:22:51.170790 140640470185792 postprocess_variants.py:972] Using sample name from call_variants output. Sample name: NA12878. 2023-05-19 16:22:51.171487: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/tmp6gzkras0/call_variants_output.tfrecord.gz. 2023-05-19 16:22:51.173038: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 305. I0519 16:22:51.173760 140640470185792 postprocess_variants.py:1037] CVO sorting took 3.902912139892578e-05 minutes. I0519 16:22:51.173915 140640470185792 postprocess_variants.py:1040] Transforming call_variants_output to variants. I0519 16:22:51.204555 140640470185792 postprocess_variants.py:1080] Processing variants (and writing to temporary file) took 0.0005078872044881184 minutes. I0519 16:22:51.582558 140640470185792 postprocess_variants.py:1093] Finished writing VCF",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/issues/653:14699,usability,user,user,14699,"put.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp6gzkras0/gvcf.tfrecord@32.gz"" --gvcf_outfile ""singularity-output/output.g.vcf.gz"". 2023-05-19 16:22:49.638487: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0519 16:22:51.170790 140640470185792 postprocess_variants.py:972] Using sample name from call_variants output. Sample name: NA12878. 2023-05-19 16:22:51.171487: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/tmp6gzkras0/call_variants_output.tfrecord.gz. 2023-05-19 16:22:51.173038: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 305. I0519 16:22:51.173760 140640470185792 postprocess_variants.py:1037] CVO sorting took 3.902912139892578e-05 minutes. I0519 16:22:51.173915 140640470185792 postprocess_variants.py:1040] Transforming call_variants_output to variants. I0519 16:22:51.204555 140640470185792 postprocess_variants.py:1080] Processing variants (and writing to temporary file) took 0.0005078872044881184 minutes. I0519 16:22:51.582558 140640470185792 postprocess_variants.py:1093] Finished writing VCF and gVCF in 0.006296555201212565 minutes. I0519 16:22:51.587275 140640470185792 genomics_reader.py:222] Reading singularity-output/output.vcf.gz with NativeVcfReader. I0519 16:22:51.867608 140640470185792 postprocess_variants.py:1104] Generating VCF stats took 0.004742777347564698 minutes. real 0m3.071s. user 0m5.901s. sys 0m4.455s. INFO: Cleaning up image... ######. $ $ ls singularity-output/. output.g.vcf.gz output.vcf.gz output.visual_report.html srun_output.g.vcf.gz.tbi srun_output.vcf.gz.tbi. output.g.vcf.gz.tbi output.vcf.gz.tbi srun_output.g.vcf.gz srun_output.vcf.gz srun_output.visual_report.html. ```. **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/653
https://github.com/google/deepvariant/pull/654:84,deployability,updat,update,84,Adding links to new blog posts.; We are not taking pull requests at this time. This update is pushed by the team member (pichuan@) to the gh-pages for https://goo.gl/deepvariant.,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/654
https://github.com/google/deepvariant/pull/654:73,performance,time,time,73,Adding links to new blog posts.; We are not taking pull requests at this time. This update is pushed by the team member (pichuan@) to the gh-pages for https://goo.gl/deepvariant.,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/654
https://github.com/google/deepvariant/pull/654:84,safety,updat,update,84,Adding links to new blog posts.; We are not taking pull requests at this time. This update is pushed by the team member (pichuan@) to the gh-pages for https://goo.gl/deepvariant.,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/654
https://github.com/google/deepvariant/pull/654:84,security,updat,update,84,Adding links to new blog posts.; We are not taking pull requests at this time. This update is pushed by the team member (pichuan@) to the gh-pages for https://goo.gl/deepvariant.,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/654
https://github.com/google/deepvariant/pull/654:108,security,team,team,108,Adding links to new blog posts.; We are not taking pull requests at this time. This update is pushed by the team member (pichuan@) to the gh-pages for https://goo.gl/deepvariant.,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/654
https://github.com/google/deepvariant/issues/655:9,availability,error,error,9,"Genotype error in VCF result; hello,. I tested a NGS sample on DV1.4. An error occurred in calling a variant at a specific locus.The VCF results show that the genotype at this locus is 1/1, but the first-generation sequencing results did not reveal a homozygous mutation. I checked the BAM file, and I couldn't draw a conclusion about the homozygous mutation either. How was the 1/1 result determined? Can you explain the reasons and methods to avoid the error from happening? ![image](https://github.com/google/deepvariant/assets/70870741/1c0710d4-f9a4-40ef-a2d7-c982e42eac1b). ![image](https://github.com/google/deepvariant/assets/70870741/4665a415-6236-46a2-ad0a-958ddf4ca2bf). ![image](https://github.com/google/deepvariant/assets/70870741/5433bce6-1b83-4d8a-88d3-8173708ac8ed). Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/655
https://github.com/google/deepvariant/issues/655:73,availability,error,error,73,"Genotype error in VCF result; hello,. I tested a NGS sample on DV1.4. An error occurred in calling a variant at a specific locus.The VCF results show that the genotype at this locus is 1/1, but the first-generation sequencing results did not reveal a homozygous mutation. I checked the BAM file, and I couldn't draw a conclusion about the homozygous mutation either. How was the 1/1 result determined? Can you explain the reasons and methods to avoid the error from happening? ![image](https://github.com/google/deepvariant/assets/70870741/1c0710d4-f9a4-40ef-a2d7-c982e42eac1b). ![image](https://github.com/google/deepvariant/assets/70870741/4665a415-6236-46a2-ad0a-958ddf4ca2bf). ![image](https://github.com/google/deepvariant/assets/70870741/5433bce6-1b83-4d8a-88d3-8173708ac8ed). Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/655
https://github.com/google/deepvariant/issues/655:455,availability,error,error,455,"Genotype error in VCF result; hello,. I tested a NGS sample on DV1.4. An error occurred in calling a variant at a specific locus.The VCF results show that the genotype at this locus is 1/1, but the first-generation sequencing results did not reveal a homozygous mutation. I checked the BAM file, and I couldn't draw a conclusion about the homozygous mutation either. How was the 1/1 result determined? Can you explain the reasons and methods to avoid the error from happening? ![image](https://github.com/google/deepvariant/assets/70870741/1c0710d4-f9a4-40ef-a2d7-c982e42eac1b). ![image](https://github.com/google/deepvariant/assets/70870741/4665a415-6236-46a2-ad0a-958ddf4ca2bf). ![image](https://github.com/google/deepvariant/assets/70870741/5433bce6-1b83-4d8a-88d3-8173708ac8ed). Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/655
https://github.com/google/deepvariant/issues/655:311,energy efficiency,draw,draw,311,"Genotype error in VCF result; hello,. I tested a NGS sample on DV1.4. An error occurred in calling a variant at a specific locus.The VCF results show that the genotype at this locus is 1/1, but the first-generation sequencing results did not reveal a homozygous mutation. I checked the BAM file, and I couldn't draw a conclusion about the homozygous mutation either. How was the 1/1 result determined? Can you explain the reasons and methods to avoid the error from happening? ![image](https://github.com/google/deepvariant/assets/70870741/1c0710d4-f9a4-40ef-a2d7-c982e42eac1b). ![image](https://github.com/google/deepvariant/assets/70870741/4665a415-6236-46a2-ad0a-958ddf4ca2bf). ![image](https://github.com/google/deepvariant/assets/70870741/5433bce6-1b83-4d8a-88d3-8173708ac8ed). Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/655
https://github.com/google/deepvariant/issues/655:114,interoperability,specif,specific,114,"Genotype error in VCF result; hello,. I tested a NGS sample on DV1.4. An error occurred in calling a variant at a specific locus.The VCF results show that the genotype at this locus is 1/1, but the first-generation sequencing results did not reveal a homozygous mutation. I checked the BAM file, and I couldn't draw a conclusion about the homozygous mutation either. How was the 1/1 result determined? Can you explain the reasons and methods to avoid the error from happening? ![image](https://github.com/google/deepvariant/assets/70870741/1c0710d4-f9a4-40ef-a2d7-c982e42eac1b). ![image](https://github.com/google/deepvariant/assets/70870741/4665a415-6236-46a2-ad0a-958ddf4ca2bf). ![image](https://github.com/google/deepvariant/assets/70870741/5433bce6-1b83-4d8a-88d3-8173708ac8ed). Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/655
https://github.com/google/deepvariant/issues/655:9,performance,error,error,9,"Genotype error in VCF result; hello,. I tested a NGS sample on DV1.4. An error occurred in calling a variant at a specific locus.The VCF results show that the genotype at this locus is 1/1, but the first-generation sequencing results did not reveal a homozygous mutation. I checked the BAM file, and I couldn't draw a conclusion about the homozygous mutation either. How was the 1/1 result determined? Can you explain the reasons and methods to avoid the error from happening? ![image](https://github.com/google/deepvariant/assets/70870741/1c0710d4-f9a4-40ef-a2d7-c982e42eac1b). ![image](https://github.com/google/deepvariant/assets/70870741/4665a415-6236-46a2-ad0a-958ddf4ca2bf). ![image](https://github.com/google/deepvariant/assets/70870741/5433bce6-1b83-4d8a-88d3-8173708ac8ed). Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/655
https://github.com/google/deepvariant/issues/655:73,performance,error,error,73,"Genotype error in VCF result; hello,. I tested a NGS sample on DV1.4. An error occurred in calling a variant at a specific locus.The VCF results show that the genotype at this locus is 1/1, but the first-generation sequencing results did not reveal a homozygous mutation. I checked the BAM file, and I couldn't draw a conclusion about the homozygous mutation either. How was the 1/1 result determined? Can you explain the reasons and methods to avoid the error from happening? ![image](https://github.com/google/deepvariant/assets/70870741/1c0710d4-f9a4-40ef-a2d7-c982e42eac1b). ![image](https://github.com/google/deepvariant/assets/70870741/4665a415-6236-46a2-ad0a-958ddf4ca2bf). ![image](https://github.com/google/deepvariant/assets/70870741/5433bce6-1b83-4d8a-88d3-8173708ac8ed). Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/655
https://github.com/google/deepvariant/issues/655:455,performance,error,error,455,"Genotype error in VCF result; hello,. I tested a NGS sample on DV1.4. An error occurred in calling a variant at a specific locus.The VCF results show that the genotype at this locus is 1/1, but the first-generation sequencing results did not reveal a homozygous mutation. I checked the BAM file, and I couldn't draw a conclusion about the homozygous mutation either. How was the 1/1 result determined? Can you explain the reasons and methods to avoid the error from happening? ![image](https://github.com/google/deepvariant/assets/70870741/1c0710d4-f9a4-40ef-a2d7-c982e42eac1b). ![image](https://github.com/google/deepvariant/assets/70870741/4665a415-6236-46a2-ad0a-958ddf4ca2bf). ![image](https://github.com/google/deepvariant/assets/70870741/5433bce6-1b83-4d8a-88d3-8173708ac8ed). Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/655
https://github.com/google/deepvariant/issues/655:9,safety,error,error,9,"Genotype error in VCF result; hello,. I tested a NGS sample on DV1.4. An error occurred in calling a variant at a specific locus.The VCF results show that the genotype at this locus is 1/1, but the first-generation sequencing results did not reveal a homozygous mutation. I checked the BAM file, and I couldn't draw a conclusion about the homozygous mutation either. How was the 1/1 result determined? Can you explain the reasons and methods to avoid the error from happening? ![image](https://github.com/google/deepvariant/assets/70870741/1c0710d4-f9a4-40ef-a2d7-c982e42eac1b). ![image](https://github.com/google/deepvariant/assets/70870741/4665a415-6236-46a2-ad0a-958ddf4ca2bf). ![image](https://github.com/google/deepvariant/assets/70870741/5433bce6-1b83-4d8a-88d3-8173708ac8ed). Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/655
https://github.com/google/deepvariant/issues/655:40,safety,test,tested,40,"Genotype error in VCF result; hello,. I tested a NGS sample on DV1.4. An error occurred in calling a variant at a specific locus.The VCF results show that the genotype at this locus is 1/1, but the first-generation sequencing results did not reveal a homozygous mutation. I checked the BAM file, and I couldn't draw a conclusion about the homozygous mutation either. How was the 1/1 result determined? Can you explain the reasons and methods to avoid the error from happening? ![image](https://github.com/google/deepvariant/assets/70870741/1c0710d4-f9a4-40ef-a2d7-c982e42eac1b). ![image](https://github.com/google/deepvariant/assets/70870741/4665a415-6236-46a2-ad0a-958ddf4ca2bf). ![image](https://github.com/google/deepvariant/assets/70870741/5433bce6-1b83-4d8a-88d3-8173708ac8ed). Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/655
https://github.com/google/deepvariant/issues/655:73,safety,error,error,73,"Genotype error in VCF result; hello,. I tested a NGS sample on DV1.4. An error occurred in calling a variant at a specific locus.The VCF results show that the genotype at this locus is 1/1, but the first-generation sequencing results did not reveal a homozygous mutation. I checked the BAM file, and I couldn't draw a conclusion about the homozygous mutation either. How was the 1/1 result determined? Can you explain the reasons and methods to avoid the error from happening? ![image](https://github.com/google/deepvariant/assets/70870741/1c0710d4-f9a4-40ef-a2d7-c982e42eac1b). ![image](https://github.com/google/deepvariant/assets/70870741/4665a415-6236-46a2-ad0a-958ddf4ca2bf). ![image](https://github.com/google/deepvariant/assets/70870741/5433bce6-1b83-4d8a-88d3-8173708ac8ed). Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/655
https://github.com/google/deepvariant/issues/655:445,safety,avoid,avoid,445,"Genotype error in VCF result; hello,. I tested a NGS sample on DV1.4. An error occurred in calling a variant at a specific locus.The VCF results show that the genotype at this locus is 1/1, but the first-generation sequencing results did not reveal a homozygous mutation. I checked the BAM file, and I couldn't draw a conclusion about the homozygous mutation either. How was the 1/1 result determined? Can you explain the reasons and methods to avoid the error from happening? ![image](https://github.com/google/deepvariant/assets/70870741/1c0710d4-f9a4-40ef-a2d7-c982e42eac1b). ![image](https://github.com/google/deepvariant/assets/70870741/4665a415-6236-46a2-ad0a-958ddf4ca2bf). ![image](https://github.com/google/deepvariant/assets/70870741/5433bce6-1b83-4d8a-88d3-8173708ac8ed). Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/655
https://github.com/google/deepvariant/issues/655:455,safety,error,error,455,"Genotype error in VCF result; hello,. I tested a NGS sample on DV1.4. An error occurred in calling a variant at a specific locus.The VCF results show that the genotype at this locus is 1/1, but the first-generation sequencing results did not reveal a homozygous mutation. I checked the BAM file, and I couldn't draw a conclusion about the homozygous mutation either. How was the 1/1 result determined? Can you explain the reasons and methods to avoid the error from happening? ![image](https://github.com/google/deepvariant/assets/70870741/1c0710d4-f9a4-40ef-a2d7-c982e42eac1b). ![image](https://github.com/google/deepvariant/assets/70870741/4665a415-6236-46a2-ad0a-958ddf4ca2bf). ![image](https://github.com/google/deepvariant/assets/70870741/5433bce6-1b83-4d8a-88d3-8173708ac8ed). Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/655
https://github.com/google/deepvariant/issues/655:40,testability,test,tested,40,"Genotype error in VCF result; hello,. I tested a NGS sample on DV1.4. An error occurred in calling a variant at a specific locus.The VCF results show that the genotype at this locus is 1/1, but the first-generation sequencing results did not reveal a homozygous mutation. I checked the BAM file, and I couldn't draw a conclusion about the homozygous mutation either. How was the 1/1 result determined? Can you explain the reasons and methods to avoid the error from happening? ![image](https://github.com/google/deepvariant/assets/70870741/1c0710d4-f9a4-40ef-a2d7-c982e42eac1b). ![image](https://github.com/google/deepvariant/assets/70870741/4665a415-6236-46a2-ad0a-958ddf4ca2bf). ![image](https://github.com/google/deepvariant/assets/70870741/5433bce6-1b83-4d8a-88d3-8173708ac8ed). Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/655
https://github.com/google/deepvariant/issues/655:9,usability,error,error,9,"Genotype error in VCF result; hello,. I tested a NGS sample on DV1.4. An error occurred in calling a variant at a specific locus.The VCF results show that the genotype at this locus is 1/1, but the first-generation sequencing results did not reveal a homozygous mutation. I checked the BAM file, and I couldn't draw a conclusion about the homozygous mutation either. How was the 1/1 result determined? Can you explain the reasons and methods to avoid the error from happening? ![image](https://github.com/google/deepvariant/assets/70870741/1c0710d4-f9a4-40ef-a2d7-c982e42eac1b). ![image](https://github.com/google/deepvariant/assets/70870741/4665a415-6236-46a2-ad0a-958ddf4ca2bf). ![image](https://github.com/google/deepvariant/assets/70870741/5433bce6-1b83-4d8a-88d3-8173708ac8ed). Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/655
https://github.com/google/deepvariant/issues/655:73,usability,error,error,73,"Genotype error in VCF result; hello,. I tested a NGS sample on DV1.4. An error occurred in calling a variant at a specific locus.The VCF results show that the genotype at this locus is 1/1, but the first-generation sequencing results did not reveal a homozygous mutation. I checked the BAM file, and I couldn't draw a conclusion about the homozygous mutation either. How was the 1/1 result determined? Can you explain the reasons and methods to avoid the error from happening? ![image](https://github.com/google/deepvariant/assets/70870741/1c0710d4-f9a4-40ef-a2d7-c982e42eac1b). ![image](https://github.com/google/deepvariant/assets/70870741/4665a415-6236-46a2-ad0a-958ddf4ca2bf). ![image](https://github.com/google/deepvariant/assets/70870741/5433bce6-1b83-4d8a-88d3-8173708ac8ed). Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/655
https://github.com/google/deepvariant/issues/655:455,usability,error,error,455,"Genotype error in VCF result; hello,. I tested a NGS sample on DV1.4. An error occurred in calling a variant at a specific locus.The VCF results show that the genotype at this locus is 1/1, but the first-generation sequencing results did not reveal a homozygous mutation. I checked the BAM file, and I couldn't draw a conclusion about the homozygous mutation either. How was the 1/1 result determined? Can you explain the reasons and methods to avoid the error from happening? ![image](https://github.com/google/deepvariant/assets/70870741/1c0710d4-f9a4-40ef-a2d7-c982e42eac1b). ![image](https://github.com/google/deepvariant/assets/70870741/4665a415-6236-46a2-ad0a-958ddf4ca2bf). ![image](https://github.com/google/deepvariant/assets/70870741/5433bce6-1b83-4d8a-88d3-8173708ac8ed). Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/655
https://github.com/google/deepvariant/issues/656:4,interoperability,coordinat,coordinates,4,"The coordinates for ""--regions"" in run_deepvariant; Hello,. The flag ""--regions"" of run_deepvariant sets the regions to be processed. When region literals (like chr20:10-20) are provided, are the coordinates 1-based or 0-based? Is this region an open or closed interval? Thank you very much!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/656
https://github.com/google/deepvariant/issues/656:196,interoperability,coordinat,coordinates,196,"The coordinates for ""--regions"" in run_deepvariant; Hello,. The flag ""--regions"" of run_deepvariant sets the regions to be processed. When region literals (like chr20:10-20) are provided, are the coordinates 1-based or 0-based? Is this region an open or closed interval? Thank you very much!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/656
https://github.com/google/deepvariant/issues/656:254,usability,close,closed,254,"The coordinates for ""--regions"" in run_deepvariant; Hello,. The flag ""--regions"" of run_deepvariant sets the regions to be processed. When region literals (like chr20:10-20) are provided, are the coordinates 1-based or 0-based? Is this region an open or closed interval? Thank you very much!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/656
https://github.com/google/deepvariant/issues/657:0,deployability,Instal,Installing,0,"Installing deepvariant using Docker Desktop on a Mac (apple silicon - M1/M2); Goal: Installing deepvariant using Docker Desktop on a Mac (apple silicon - M1/M2). I have been troubleshooting for days and to build from source, but failed to do so. . Now I ended up installing Ubuntu 20.04 using mac's UTM, but still facing a lot of problems. . Is there a detailed step-by-step instruction on how to install on a mac (apple silicon)? You mentioned: ""It can likely be built and run on other unix-based systems with some minimal modifications to these scripts."". from https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. What is the ""minimal modifications"" in here? Changing everything about the build-prereq.sh, setting.sh, tools/build_clif.sh, and other .sh, proves to be a hard task. Otherwise, I can try to explain the problem of Ubuntu 20.04 using mac's UTM. Thank you for your help!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:84,deployability,Instal,Installing,84,"Installing deepvariant using Docker Desktop on a Mac (apple silicon - M1/M2); Goal: Installing deepvariant using Docker Desktop on a Mac (apple silicon - M1/M2). I have been troubleshooting for days and to build from source, but failed to do so. . Now I ended up installing Ubuntu 20.04 using mac's UTM, but still facing a lot of problems. . Is there a detailed step-by-step instruction on how to install on a mac (apple silicon)? You mentioned: ""It can likely be built and run on other unix-based systems with some minimal modifications to these scripts."". from https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. What is the ""minimal modifications"" in here? Changing everything about the build-prereq.sh, setting.sh, tools/build_clif.sh, and other .sh, proves to be a hard task. Otherwise, I can try to explain the problem of Ubuntu 20.04 using mac's UTM. Thank you for your help!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:206,deployability,build,build,206,"Installing deepvariant using Docker Desktop on a Mac (apple silicon - M1/M2); Goal: Installing deepvariant using Docker Desktop on a Mac (apple silicon - M1/M2). I have been troubleshooting for days and to build from source, but failed to do so. . Now I ended up installing Ubuntu 20.04 using mac's UTM, but still facing a lot of problems. . Is there a detailed step-by-step instruction on how to install on a mac (apple silicon)? You mentioned: ""It can likely be built and run on other unix-based systems with some minimal modifications to these scripts."". from https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. What is the ""minimal modifications"" in here? Changing everything about the build-prereq.sh, setting.sh, tools/build_clif.sh, and other .sh, proves to be a hard task. Otherwise, I can try to explain the problem of Ubuntu 20.04 using mac's UTM. Thank you for your help!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:229,deployability,fail,failed,229,"Installing deepvariant using Docker Desktop on a Mac (apple silicon - M1/M2); Goal: Installing deepvariant using Docker Desktop on a Mac (apple silicon - M1/M2). I have been troubleshooting for days and to build from source, but failed to do so. . Now I ended up installing Ubuntu 20.04 using mac's UTM, but still facing a lot of problems. . Is there a detailed step-by-step instruction on how to install on a mac (apple silicon)? You mentioned: ""It can likely be built and run on other unix-based systems with some minimal modifications to these scripts."". from https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. What is the ""minimal modifications"" in here? Changing everything about the build-prereq.sh, setting.sh, tools/build_clif.sh, and other .sh, proves to be a hard task. Otherwise, I can try to explain the problem of Ubuntu 20.04 using mac's UTM. Thank you for your help!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:263,deployability,instal,installing,263,"Installing deepvariant using Docker Desktop on a Mac (apple silicon - M1/M2); Goal: Installing deepvariant using Docker Desktop on a Mac (apple silicon - M1/M2). I have been troubleshooting for days and to build from source, but failed to do so. . Now I ended up installing Ubuntu 20.04 using mac's UTM, but still facing a lot of problems. . Is there a detailed step-by-step instruction on how to install on a mac (apple silicon)? You mentioned: ""It can likely be built and run on other unix-based systems with some minimal modifications to these scripts."". from https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. What is the ""minimal modifications"" in here? Changing everything about the build-prereq.sh, setting.sh, tools/build_clif.sh, and other .sh, proves to be a hard task. Otherwise, I can try to explain the problem of Ubuntu 20.04 using mac's UTM. Thank you for your help!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:397,deployability,instal,install,397,"Installing deepvariant using Docker Desktop on a Mac (apple silicon - M1/M2); Goal: Installing deepvariant using Docker Desktop on a Mac (apple silicon - M1/M2). I have been troubleshooting for days and to build from source, but failed to do so. . Now I ended up installing Ubuntu 20.04 using mac's UTM, but still facing a lot of problems. . Is there a detailed step-by-step instruction on how to install on a mac (apple silicon)? You mentioned: ""It can likely be built and run on other unix-based systems with some minimal modifications to these scripts."". from https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. What is the ""minimal modifications"" in here? Changing everything about the build-prereq.sh, setting.sh, tools/build_clif.sh, and other .sh, proves to be a hard task. Otherwise, I can try to explain the problem of Ubuntu 20.04 using mac's UTM. Thank you for your help!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:628,deployability,build,build-test,628,"Installing deepvariant using Docker Desktop on a Mac (apple silicon - M1/M2); Goal: Installing deepvariant using Docker Desktop on a Mac (apple silicon - M1/M2). I have been troubleshooting for days and to build from source, but failed to do so. . Now I ended up installing Ubuntu 20.04 using mac's UTM, but still facing a lot of problems. . Is there a detailed step-by-step instruction on how to install on a mac (apple silicon)? You mentioned: ""It can likely be built and run on other unix-based systems with some minimal modifications to these scripts."". from https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. What is the ""minimal modifications"" in here? Changing everything about the build-prereq.sh, setting.sh, tools/build_clif.sh, and other .sh, proves to be a hard task. Otherwise, I can try to explain the problem of Ubuntu 20.04 using mac's UTM. Thank you for your help!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:718,deployability,build,build-prereq,718,"Installing deepvariant using Docker Desktop on a Mac (apple silicon - M1/M2); Goal: Installing deepvariant using Docker Desktop on a Mac (apple silicon - M1/M2). I have been troubleshooting for days and to build from source, but failed to do so. . Now I ended up installing Ubuntu 20.04 using mac's UTM, but still facing a lot of problems. . Is there a detailed step-by-step instruction on how to install on a mac (apple silicon)? You mentioned: ""It can likely be built and run on other unix-based systems with some minimal modifications to these scripts."". from https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. What is the ""minimal modifications"" in here? Changing everything about the build-prereq.sh, setting.sh, tools/build_clif.sh, and other .sh, proves to be a hard task. Otherwise, I can try to explain the problem of Ubuntu 20.04 using mac's UTM. Thank you for your help!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:229,reliability,fail,failed,229,"Installing deepvariant using Docker Desktop on a Mac (apple silicon - M1/M2); Goal: Installing deepvariant using Docker Desktop on a Mac (apple silicon - M1/M2). I have been troubleshooting for days and to build from source, but failed to do so. . Now I ended up installing Ubuntu 20.04 using mac's UTM, but still facing a lot of problems. . Is there a detailed step-by-step instruction on how to install on a mac (apple silicon)? You mentioned: ""It can likely be built and run on other unix-based systems with some minimal modifications to these scripts."". from https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. What is the ""minimal modifications"" in here? Changing everything about the build-prereq.sh, setting.sh, tools/build_clif.sh, and other .sh, proves to be a hard task. Otherwise, I can try to explain the problem of Ubuntu 20.04 using mac's UTM. Thank you for your help!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:634,safety,test,test,634,"Installing deepvariant using Docker Desktop on a Mac (apple silicon - M1/M2); Goal: Installing deepvariant using Docker Desktop on a Mac (apple silicon - M1/M2). I have been troubleshooting for days and to build from source, but failed to do so. . Now I ended up installing Ubuntu 20.04 using mac's UTM, but still facing a lot of problems. . Is there a detailed step-by-step instruction on how to install on a mac (apple silicon)? You mentioned: ""It can likely be built and run on other unix-based systems with some minimal modifications to these scripts."". from https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. What is the ""minimal modifications"" in here? Changing everything about the build-prereq.sh, setting.sh, tools/build_clif.sh, and other .sh, proves to be a hard task. Otherwise, I can try to explain the problem of Ubuntu 20.04 using mac's UTM. Thank you for your help!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:524,security,modif,modifications,524,"Installing deepvariant using Docker Desktop on a Mac (apple silicon - M1/M2); Goal: Installing deepvariant using Docker Desktop on a Mac (apple silicon - M1/M2). I have been troubleshooting for days and to build from source, but failed to do so. . Now I ended up installing Ubuntu 20.04 using mac's UTM, but still facing a lot of problems. . Is there a detailed step-by-step instruction on how to install on a mac (apple silicon)? You mentioned: ""It can likely be built and run on other unix-based systems with some minimal modifications to these scripts."". from https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. What is the ""minimal modifications"" in here? Changing everything about the build-prereq.sh, setting.sh, tools/build_clif.sh, and other .sh, proves to be a hard task. Otherwise, I can try to explain the problem of Ubuntu 20.04 using mac's UTM. Thank you for your help!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:664,security,modif,modifications,664,"Installing deepvariant using Docker Desktop on a Mac (apple silicon - M1/M2); Goal: Installing deepvariant using Docker Desktop on a Mac (apple silicon - M1/M2). I have been troubleshooting for days and to build from source, but failed to do so. . Now I ended up installing Ubuntu 20.04 using mac's UTM, but still facing a lot of problems. . Is there a detailed step-by-step instruction on how to install on a mac (apple silicon)? You mentioned: ""It can likely be built and run on other unix-based systems with some minimal modifications to these scripts."". from https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. What is the ""minimal modifications"" in here? Changing everything about the build-prereq.sh, setting.sh, tools/build_clif.sh, and other .sh, proves to be a hard task. Otherwise, I can try to explain the problem of Ubuntu 20.04 using mac's UTM. Thank you for your help!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:634,testability,test,test,634,"Installing deepvariant using Docker Desktop on a Mac (apple silicon - M1/M2); Goal: Installing deepvariant using Docker Desktop on a Mac (apple silicon - M1/M2). I have been troubleshooting for days and to build from source, but failed to do so. . Now I ended up installing Ubuntu 20.04 using mac's UTM, but still facing a lot of problems. . Is there a detailed step-by-step instruction on how to install on a mac (apple silicon)? You mentioned: ""It can likely be built and run on other unix-based systems with some minimal modifications to these scripts."". from https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. What is the ""minimal modifications"" in here? Changing everything about the build-prereq.sh, setting.sh, tools/build_clif.sh, and other .sh, proves to be a hard task. Otherwise, I can try to explain the problem of Ubuntu 20.04 using mac's UTM. Thank you for your help!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:516,usability,minim,minimal,516,"Installing deepvariant using Docker Desktop on a Mac (apple silicon - M1/M2); Goal: Installing deepvariant using Docker Desktop on a Mac (apple silicon - M1/M2). I have been troubleshooting for days and to build from source, but failed to do so. . Now I ended up installing Ubuntu 20.04 using mac's UTM, but still facing a lot of problems. . Is there a detailed step-by-step instruction on how to install on a mac (apple silicon)? You mentioned: ""It can likely be built and run on other unix-based systems with some minimal modifications to these scripts."". from https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. What is the ""minimal modifications"" in here? Changing everything about the build-prereq.sh, setting.sh, tools/build_clif.sh, and other .sh, proves to be a hard task. Otherwise, I can try to explain the problem of Ubuntu 20.04 using mac's UTM. Thank you for your help!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:656,usability,minim,minimal,656,"Installing deepvariant using Docker Desktop on a Mac (apple silicon - M1/M2); Goal: Installing deepvariant using Docker Desktop on a Mac (apple silicon - M1/M2). I have been troubleshooting for days and to build from source, but failed to do so. . Now I ended up installing Ubuntu 20.04 using mac's UTM, but still facing a lot of problems. . Is there a detailed step-by-step instruction on how to install on a mac (apple silicon)? You mentioned: ""It can likely be built and run on other unix-based systems with some minimal modifications to these scripts."". from https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. What is the ""minimal modifications"" in here? Changing everything about the build-prereq.sh, setting.sh, tools/build_clif.sh, and other .sh, proves to be a hard task. Otherwise, I can try to explain the problem of Ubuntu 20.04 using mac's UTM. Thank you for your help!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:747,usability,tool,tools,747,"Installing deepvariant using Docker Desktop on a Mac (apple silicon - M1/M2); Goal: Installing deepvariant using Docker Desktop on a Mac (apple silicon - M1/M2). I have been troubleshooting for days and to build from source, but failed to do so. . Now I ended up installing Ubuntu 20.04 using mac's UTM, but still facing a lot of problems. . Is there a detailed step-by-step instruction on how to install on a mac (apple silicon)? You mentioned: ""It can likely be built and run on other unix-based systems with some minimal modifications to these scripts."". from https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. What is the ""minimal modifications"" in here? Changing everything about the build-prereq.sh, setting.sh, tools/build_clif.sh, and other .sh, proves to be a hard task. Otherwise, I can try to explain the problem of Ubuntu 20.04 using mac's UTM. Thank you for your help!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:905,usability,help,help,905,"Installing deepvariant using Docker Desktop on a Mac (apple silicon - M1/M2); Goal: Installing deepvariant using Docker Desktop on a Mac (apple silicon - M1/M2). I have been troubleshooting for days and to build from source, but failed to do so. . Now I ended up installing Ubuntu 20.04 using mac's UTM, but still facing a lot of problems. . Is there a detailed step-by-step instruction on how to install on a mac (apple silicon)? You mentioned: ""It can likely be built and run on other unix-based systems with some minimal modifications to these scripts."". from https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. What is the ""minimal modifications"" in here? Changing everything about the build-prereq.sh, setting.sh, tools/build_clif.sh, and other .sh, proves to be a hard task. Otherwise, I can try to explain the problem of Ubuntu 20.04 using mac's UTM. Thank you for your help!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/659:0,availability,Error,Error,0,"Error in pbc_varicall; Hi i have deepvariant 1.5.0 version singularity SIF file,. I ran successfully from command line for PacBio data, but when i ran the same through NextFlow as below. #nextflow.config. ```. singularity {. process.container = '/data/shared/clinical/LongRead/DeepVariant/deepvariant.simg'. cacheDir = ""/data/shared/clinical/LongRead/cache/"". singularity.enabled = true. singularity.autoMounts = true. SINGULARITY_BINDPATH = ""/data"". }. conda. {. enabled = true. cacheDir = ""/data/shared/clinical/LongRead/Programs/"". }. params. {. path=""/data/shared/clinical/LongRead/"". ref_genome=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"". pbhg38_tdx=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.mmi"". at=18. st=6. data_input=""/data/shared/clinical/LongRead/Data/"". }. ```. #deepvariant.nf. ```. process pbc_varicall {. publishDir ""/data/shared/clinical/LongRead/Data/resources/"". container 'docker://google/deepvariant:1.5.0'. input:. path 'fa'. output:. file ""*"". path 'm84011_220902_175841_NF_sif.vcf.gz'. script:. """""". run_deepvariant --model_type PACBIO --ref ${params.ref_genome} --reads ${params.data_input}/m84011_220902_175841_Aln.bam --output_vcf ${params.data_input}/Analysis/out_m84011_220902_175841_NF_sif.vcf.gz --num_shards 40. """""". }. workflow {. fa=channel.fromPath(""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta""). pbc_varicall(fa). }. ```. after running for several hours i do not get any output, instead during run , i get msg as. `TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: `. this is during `make_examples` step",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:1616,availability,error,error,1616,"Error in pbc_varicall; Hi i have deepvariant 1.5.0 version singularity SIF file,. I ran successfully from command line for PacBio data, but when i ran the same through NextFlow as below. #nextflow.config. ```. singularity {. process.container = '/data/shared/clinical/LongRead/DeepVariant/deepvariant.simg'. cacheDir = ""/data/shared/clinical/LongRead/cache/"". singularity.enabled = true. singularity.autoMounts = true. SINGULARITY_BINDPATH = ""/data"". }. conda. {. enabled = true. cacheDir = ""/data/shared/clinical/LongRead/Programs/"". }. params. {. path=""/data/shared/clinical/LongRead/"". ref_genome=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"". pbhg38_tdx=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.mmi"". at=18. st=6. data_input=""/data/shared/clinical/LongRead/Data/"". }. ```. #deepvariant.nf. ```. process pbc_varicall {. publishDir ""/data/shared/clinical/LongRead/Data/resources/"". container 'docker://google/deepvariant:1.5.0'. input:. path 'fa'. output:. file ""*"". path 'm84011_220902_175841_NF_sif.vcf.gz'. script:. """""". run_deepvariant --model_type PACBIO --ref ${params.ref_genome} --reads ${params.data_input}/m84011_220902_175841_Aln.bam --output_vcf ${params.data_input}/Analysis/out_m84011_220902_175841_NF_sif.vcf.gz --num_shards 40. """""". }. workflow {. fa=channel.fromPath(""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta""). pbc_varicall(fa). }. ```. after running for several hours i do not get any output, instead during run , i get msg as. `TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: `. this is during `make_examples` step",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:51,deployability,version,version,51,"Error in pbc_varicall; Hi i have deepvariant 1.5.0 version singularity SIF file,. I ran successfully from command line for PacBio data, but when i ran the same through NextFlow as below. #nextflow.config. ```. singularity {. process.container = '/data/shared/clinical/LongRead/DeepVariant/deepvariant.simg'. cacheDir = ""/data/shared/clinical/LongRead/cache/"". singularity.enabled = true. singularity.autoMounts = true. SINGULARITY_BINDPATH = ""/data"". }. conda. {. enabled = true. cacheDir = ""/data/shared/clinical/LongRead/Programs/"". }. params. {. path=""/data/shared/clinical/LongRead/"". ref_genome=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"". pbhg38_tdx=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.mmi"". at=18. st=6. data_input=""/data/shared/clinical/LongRead/Data/"". }. ```. #deepvariant.nf. ```. process pbc_varicall {. publishDir ""/data/shared/clinical/LongRead/Data/resources/"". container 'docker://google/deepvariant:1.5.0'. input:. path 'fa'. output:. file ""*"". path 'm84011_220902_175841_NF_sif.vcf.gz'. script:. """""". run_deepvariant --model_type PACBIO --ref ${params.ref_genome} --reads ${params.data_input}/m84011_220902_175841_Aln.bam --output_vcf ${params.data_input}/Analysis/out_m84011_220902_175841_NF_sif.vcf.gz --num_shards 40. """""". }. workflow {. fa=channel.fromPath(""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta""). pbc_varicall(fa). }. ```. after running for several hours i do not get any output, instead during run , i get msg as. `TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: `. this is during `make_examples` step",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:233,deployability,contain,container,233,"Error in pbc_varicall; Hi i have deepvariant 1.5.0 version singularity SIF file,. I ran successfully from command line for PacBio data, but when i ran the same through NextFlow as below. #nextflow.config. ```. singularity {. process.container = '/data/shared/clinical/LongRead/DeepVariant/deepvariant.simg'. cacheDir = ""/data/shared/clinical/LongRead/cache/"". singularity.enabled = true. singularity.autoMounts = true. SINGULARITY_BINDPATH = ""/data"". }. conda. {. enabled = true. cacheDir = ""/data/shared/clinical/LongRead/Programs/"". }. params. {. path=""/data/shared/clinical/LongRead/"". ref_genome=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"". pbhg38_tdx=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.mmi"". at=18. st=6. data_input=""/data/shared/clinical/LongRead/Data/"". }. ```. #deepvariant.nf. ```. process pbc_varicall {. publishDir ""/data/shared/clinical/LongRead/Data/resources/"". container 'docker://google/deepvariant:1.5.0'. input:. path 'fa'. output:. file ""*"". path 'm84011_220902_175841_NF_sif.vcf.gz'. script:. """""". run_deepvariant --model_type PACBIO --ref ${params.ref_genome} --reads ${params.data_input}/m84011_220902_175841_Aln.bam --output_vcf ${params.data_input}/Analysis/out_m84011_220902_175841_NF_sif.vcf.gz --num_shards 40. """""". }. workflow {. fa=channel.fromPath(""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta""). pbc_varicall(fa). }. ```. after running for several hours i do not get any output, instead during run , i get msg as. `TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: `. this is during `make_examples` step",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:637,deployability,resourc,resources,637,"Error in pbc_varicall; Hi i have deepvariant 1.5.0 version singularity SIF file,. I ran successfully from command line for PacBio data, but when i ran the same through NextFlow as below. #nextflow.config. ```. singularity {. process.container = '/data/shared/clinical/LongRead/DeepVariant/deepvariant.simg'. cacheDir = ""/data/shared/clinical/LongRead/cache/"". singularity.enabled = true. singularity.autoMounts = true. SINGULARITY_BINDPATH = ""/data"". }. conda. {. enabled = true. cacheDir = ""/data/shared/clinical/LongRead/Programs/"". }. params. {. path=""/data/shared/clinical/LongRead/"". ref_genome=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"". pbhg38_tdx=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.mmi"". at=18. st=6. data_input=""/data/shared/clinical/LongRead/Data/"". }. ```. #deepvariant.nf. ```. process pbc_varicall {. publishDir ""/data/shared/clinical/LongRead/Data/resources/"". container 'docker://google/deepvariant:1.5.0'. input:. path 'fa'. output:. file ""*"". path 'm84011_220902_175841_NF_sif.vcf.gz'. script:. """""". run_deepvariant --model_type PACBIO --ref ${params.ref_genome} --reads ${params.data_input}/m84011_220902_175841_Aln.bam --output_vcf ${params.data_input}/Analysis/out_m84011_220902_175841_NF_sif.vcf.gz --num_shards 40. """""". }. workflow {. fa=channel.fromPath(""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta""). pbc_varicall(fa). }. ```. after running for several hours i do not get any output, instead during run , i get msg as. `TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: `. this is during `make_examples` step",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:727,deployability,resourc,resources,727,"Error in pbc_varicall; Hi i have deepvariant 1.5.0 version singularity SIF file,. I ran successfully from command line for PacBio data, but when i ran the same through NextFlow as below. #nextflow.config. ```. singularity {. process.container = '/data/shared/clinical/LongRead/DeepVariant/deepvariant.simg'. cacheDir = ""/data/shared/clinical/LongRead/cache/"". singularity.enabled = true. singularity.autoMounts = true. SINGULARITY_BINDPATH = ""/data"". }. conda. {. enabled = true. cacheDir = ""/data/shared/clinical/LongRead/Programs/"". }. params. {. path=""/data/shared/clinical/LongRead/"". ref_genome=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"". pbhg38_tdx=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.mmi"". at=18. st=6. data_input=""/data/shared/clinical/LongRead/Data/"". }. ```. #deepvariant.nf. ```. process pbc_varicall {. publishDir ""/data/shared/clinical/LongRead/Data/resources/"". container 'docker://google/deepvariant:1.5.0'. input:. path 'fa'. output:. file ""*"". path 'm84011_220902_175841_NF_sif.vcf.gz'. script:. """""". run_deepvariant --model_type PACBIO --ref ${params.ref_genome} --reads ${params.data_input}/m84011_220902_175841_Aln.bam --output_vcf ${params.data_input}/Analysis/out_m84011_220902_175841_NF_sif.vcf.gz --num_shards 40. """""". }. workflow {. fa=channel.fromPath(""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta""). pbc_varicall(fa). }. ```. after running for several hours i do not get any output, instead during run , i get msg as. `TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: `. this is during `make_examples` step",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:933,deployability,resourc,resources,933,"Error in pbc_varicall; Hi i have deepvariant 1.5.0 version singularity SIF file,. I ran successfully from command line for PacBio data, but when i ran the same through NextFlow as below. #nextflow.config. ```. singularity {. process.container = '/data/shared/clinical/LongRead/DeepVariant/deepvariant.simg'. cacheDir = ""/data/shared/clinical/LongRead/cache/"". singularity.enabled = true. singularity.autoMounts = true. SINGULARITY_BINDPATH = ""/data"". }. conda. {. enabled = true. cacheDir = ""/data/shared/clinical/LongRead/Programs/"". }. params. {. path=""/data/shared/clinical/LongRead/"". ref_genome=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"". pbhg38_tdx=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.mmi"". at=18. st=6. data_input=""/data/shared/clinical/LongRead/Data/"". }. ```. #deepvariant.nf. ```. process pbc_varicall {. publishDir ""/data/shared/clinical/LongRead/Data/resources/"". container 'docker://google/deepvariant:1.5.0'. input:. path 'fa'. output:. file ""*"". path 'm84011_220902_175841_NF_sif.vcf.gz'. script:. """""". run_deepvariant --model_type PACBIO --ref ${params.ref_genome} --reads ${params.data_input}/m84011_220902_175841_Aln.bam --output_vcf ${params.data_input}/Analysis/out_m84011_220902_175841_NF_sif.vcf.gz --num_shards 40. """""". }. workflow {. fa=channel.fromPath(""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta""). pbc_varicall(fa). }. ```. after running for several hours i do not get any output, instead during run , i get msg as. `TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: `. this is during `make_examples` step",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:946,deployability,contain,container,946,"Error in pbc_varicall; Hi i have deepvariant 1.5.0 version singularity SIF file,. I ran successfully from command line for PacBio data, but when i ran the same through NextFlow as below. #nextflow.config. ```. singularity {. process.container = '/data/shared/clinical/LongRead/DeepVariant/deepvariant.simg'. cacheDir = ""/data/shared/clinical/LongRead/cache/"". singularity.enabled = true. singularity.autoMounts = true. SINGULARITY_BINDPATH = ""/data"". }. conda. {. enabled = true. cacheDir = ""/data/shared/clinical/LongRead/Programs/"". }. params. {. path=""/data/shared/clinical/LongRead/"". ref_genome=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"". pbhg38_tdx=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.mmi"". at=18. st=6. data_input=""/data/shared/clinical/LongRead/Data/"". }. ```. #deepvariant.nf. ```. process pbc_varicall {. publishDir ""/data/shared/clinical/LongRead/Data/resources/"". container 'docker://google/deepvariant:1.5.0'. input:. path 'fa'. output:. file ""*"". path 'm84011_220902_175841_NF_sif.vcf.gz'. script:. """""". run_deepvariant --model_type PACBIO --ref ${params.ref_genome} --reads ${params.data_input}/m84011_220902_175841_Aln.bam --output_vcf ${params.data_input}/Analysis/out_m84011_220902_175841_NF_sif.vcf.gz --num_shards 40. """""". }. workflow {. fa=channel.fromPath(""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta""). pbc_varicall(fa). }. ```. after running for several hours i do not get any output, instead during run , i get msg as. `TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: `. this is during `make_examples` step",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:1385,deployability,resourc,resources,1385,"Error in pbc_varicall; Hi i have deepvariant 1.5.0 version singularity SIF file,. I ran successfully from command line for PacBio data, but when i ran the same through NextFlow as below. #nextflow.config. ```. singularity {. process.container = '/data/shared/clinical/LongRead/DeepVariant/deepvariant.simg'. cacheDir = ""/data/shared/clinical/LongRead/cache/"". singularity.enabled = true. singularity.autoMounts = true. SINGULARITY_BINDPATH = ""/data"". }. conda. {. enabled = true. cacheDir = ""/data/shared/clinical/LongRead/Programs/"". }. params. {. path=""/data/shared/clinical/LongRead/"". ref_genome=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"". pbhg38_tdx=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.mmi"". at=18. st=6. data_input=""/data/shared/clinical/LongRead/Data/"". }. ```. #deepvariant.nf. ```. process pbc_varicall {. publishDir ""/data/shared/clinical/LongRead/Data/resources/"". container 'docker://google/deepvariant:1.5.0'. input:. path 'fa'. output:. file ""*"". path 'm84011_220902_175841_NF_sif.vcf.gz'. script:. """""". run_deepvariant --model_type PACBIO --ref ${params.ref_genome} --reads ${params.data_input}/m84011_220902_175841_Aln.bam --output_vcf ${params.data_input}/Analysis/out_m84011_220902_175841_NF_sif.vcf.gz --num_shards 40. """""". }. workflow {. fa=channel.fromPath(""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta""). pbc_varicall(fa). }. ```. after running for several hours i do not get any output, instead during run , i get msg as. `TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: `. this is during `make_examples` step",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:637,energy efficiency,resourc,resources,637,"Error in pbc_varicall; Hi i have deepvariant 1.5.0 version singularity SIF file,. I ran successfully from command line for PacBio data, but when i ran the same through NextFlow as below. #nextflow.config. ```. singularity {. process.container = '/data/shared/clinical/LongRead/DeepVariant/deepvariant.simg'. cacheDir = ""/data/shared/clinical/LongRead/cache/"". singularity.enabled = true. singularity.autoMounts = true. SINGULARITY_BINDPATH = ""/data"". }. conda. {. enabled = true. cacheDir = ""/data/shared/clinical/LongRead/Programs/"". }. params. {. path=""/data/shared/clinical/LongRead/"". ref_genome=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"". pbhg38_tdx=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.mmi"". at=18. st=6. data_input=""/data/shared/clinical/LongRead/Data/"". }. ```. #deepvariant.nf. ```. process pbc_varicall {. publishDir ""/data/shared/clinical/LongRead/Data/resources/"". container 'docker://google/deepvariant:1.5.0'. input:. path 'fa'. output:. file ""*"". path 'm84011_220902_175841_NF_sif.vcf.gz'. script:. """""". run_deepvariant --model_type PACBIO --ref ${params.ref_genome} --reads ${params.data_input}/m84011_220902_175841_Aln.bam --output_vcf ${params.data_input}/Analysis/out_m84011_220902_175841_NF_sif.vcf.gz --num_shards 40. """""". }. workflow {. fa=channel.fromPath(""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta""). pbc_varicall(fa). }. ```. after running for several hours i do not get any output, instead during run , i get msg as. `TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: `. this is during `make_examples` step",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:727,energy efficiency,resourc,resources,727,"Error in pbc_varicall; Hi i have deepvariant 1.5.0 version singularity SIF file,. I ran successfully from command line for PacBio data, but when i ran the same through NextFlow as below. #nextflow.config. ```. singularity {. process.container = '/data/shared/clinical/LongRead/DeepVariant/deepvariant.simg'. cacheDir = ""/data/shared/clinical/LongRead/cache/"". singularity.enabled = true. singularity.autoMounts = true. SINGULARITY_BINDPATH = ""/data"". }. conda. {. enabled = true. cacheDir = ""/data/shared/clinical/LongRead/Programs/"". }. params. {. path=""/data/shared/clinical/LongRead/"". ref_genome=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"". pbhg38_tdx=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.mmi"". at=18. st=6. data_input=""/data/shared/clinical/LongRead/Data/"". }. ```. #deepvariant.nf. ```. process pbc_varicall {. publishDir ""/data/shared/clinical/LongRead/Data/resources/"". container 'docker://google/deepvariant:1.5.0'. input:. path 'fa'. output:. file ""*"". path 'm84011_220902_175841_NF_sif.vcf.gz'. script:. """""". run_deepvariant --model_type PACBIO --ref ${params.ref_genome} --reads ${params.data_input}/m84011_220902_175841_Aln.bam --output_vcf ${params.data_input}/Analysis/out_m84011_220902_175841_NF_sif.vcf.gz --num_shards 40. """""". }. workflow {. fa=channel.fromPath(""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta""). pbc_varicall(fa). }. ```. after running for several hours i do not get any output, instead during run , i get msg as. `TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: `. this is during `make_examples` step",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:933,energy efficiency,resourc,resources,933,"Error in pbc_varicall; Hi i have deepvariant 1.5.0 version singularity SIF file,. I ran successfully from command line for PacBio data, but when i ran the same through NextFlow as below. #nextflow.config. ```. singularity {. process.container = '/data/shared/clinical/LongRead/DeepVariant/deepvariant.simg'. cacheDir = ""/data/shared/clinical/LongRead/cache/"". singularity.enabled = true. singularity.autoMounts = true. SINGULARITY_BINDPATH = ""/data"". }. conda. {. enabled = true. cacheDir = ""/data/shared/clinical/LongRead/Programs/"". }. params. {. path=""/data/shared/clinical/LongRead/"". ref_genome=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"". pbhg38_tdx=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.mmi"". at=18. st=6. data_input=""/data/shared/clinical/LongRead/Data/"". }. ```. #deepvariant.nf. ```. process pbc_varicall {. publishDir ""/data/shared/clinical/LongRead/Data/resources/"". container 'docker://google/deepvariant:1.5.0'. input:. path 'fa'. output:. file ""*"". path 'm84011_220902_175841_NF_sif.vcf.gz'. script:. """""". run_deepvariant --model_type PACBIO --ref ${params.ref_genome} --reads ${params.data_input}/m84011_220902_175841_Aln.bam --output_vcf ${params.data_input}/Analysis/out_m84011_220902_175841_NF_sif.vcf.gz --num_shards 40. """""". }. workflow {. fa=channel.fromPath(""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta""). pbc_varicall(fa). }. ```. after running for several hours i do not get any output, instead during run , i get msg as. `TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: `. this is during `make_examples` step",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:1385,energy efficiency,resourc,resources,1385,"Error in pbc_varicall; Hi i have deepvariant 1.5.0 version singularity SIF file,. I ran successfully from command line for PacBio data, but when i ran the same through NextFlow as below. #nextflow.config. ```. singularity {. process.container = '/data/shared/clinical/LongRead/DeepVariant/deepvariant.simg'. cacheDir = ""/data/shared/clinical/LongRead/cache/"". singularity.enabled = true. singularity.autoMounts = true. SINGULARITY_BINDPATH = ""/data"". }. conda. {. enabled = true. cacheDir = ""/data/shared/clinical/LongRead/Programs/"". }. params. {. path=""/data/shared/clinical/LongRead/"". ref_genome=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"". pbhg38_tdx=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.mmi"". at=18. st=6. data_input=""/data/shared/clinical/LongRead/Data/"". }. ```. #deepvariant.nf. ```. process pbc_varicall {. publishDir ""/data/shared/clinical/LongRead/Data/resources/"". container 'docker://google/deepvariant:1.5.0'. input:. path 'fa'. output:. file ""*"". path 'm84011_220902_175841_NF_sif.vcf.gz'. script:. """""". run_deepvariant --model_type PACBIO --ref ${params.ref_genome} --reads ${params.data_input}/m84011_220902_175841_Aln.bam --output_vcf ${params.data_input}/Analysis/out_m84011_220902_175841_NF_sif.vcf.gz --num_shards 40. """""". }. workflow {. fa=channel.fromPath(""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta""). pbc_varicall(fa). }. ```. after running for several hours i do not get any output, instead during run , i get msg as. `TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: `. this is during `make_examples` step",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:51,integrability,version,version,51,"Error in pbc_varicall; Hi i have deepvariant 1.5.0 version singularity SIF file,. I ran successfully from command line for PacBio data, but when i ran the same through NextFlow as below. #nextflow.config. ```. singularity {. process.container = '/data/shared/clinical/LongRead/DeepVariant/deepvariant.simg'. cacheDir = ""/data/shared/clinical/LongRead/cache/"". singularity.enabled = true. singularity.autoMounts = true. SINGULARITY_BINDPATH = ""/data"". }. conda. {. enabled = true. cacheDir = ""/data/shared/clinical/LongRead/Programs/"". }. params. {. path=""/data/shared/clinical/LongRead/"". ref_genome=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"". pbhg38_tdx=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.mmi"". at=18. st=6. data_input=""/data/shared/clinical/LongRead/Data/"". }. ```. #deepvariant.nf. ```. process pbc_varicall {. publishDir ""/data/shared/clinical/LongRead/Data/resources/"". container 'docker://google/deepvariant:1.5.0'. input:. path 'fa'. output:. file ""*"". path 'm84011_220902_175841_NF_sif.vcf.gz'. script:. """""". run_deepvariant --model_type PACBIO --ref ${params.ref_genome} --reads ${params.data_input}/m84011_220902_175841_Aln.bam --output_vcf ${params.data_input}/Analysis/out_m84011_220902_175841_NF_sif.vcf.gz --num_shards 40. """""". }. workflow {. fa=channel.fromPath(""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta""). pbc_varicall(fa). }. ```. after running for several hours i do not get any output, instead during run , i get msg as. `TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: `. this is during `make_examples` step",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:885,integrability,pub,publishDir,885,"Error in pbc_varicall; Hi i have deepvariant 1.5.0 version singularity SIF file,. I ran successfully from command line for PacBio data, but when i ran the same through NextFlow as below. #nextflow.config. ```. singularity {. process.container = '/data/shared/clinical/LongRead/DeepVariant/deepvariant.simg'. cacheDir = ""/data/shared/clinical/LongRead/cache/"". singularity.enabled = true. singularity.autoMounts = true. SINGULARITY_BINDPATH = ""/data"". }. conda. {. enabled = true. cacheDir = ""/data/shared/clinical/LongRead/Programs/"". }. params. {. path=""/data/shared/clinical/LongRead/"". ref_genome=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"". pbhg38_tdx=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.mmi"". at=18. st=6. data_input=""/data/shared/clinical/LongRead/Data/"". }. ```. #deepvariant.nf. ```. process pbc_varicall {. publishDir ""/data/shared/clinical/LongRead/Data/resources/"". container 'docker://google/deepvariant:1.5.0'. input:. path 'fa'. output:. file ""*"". path 'm84011_220902_175841_NF_sif.vcf.gz'. script:. """""". run_deepvariant --model_type PACBIO --ref ${params.ref_genome} --reads ${params.data_input}/m84011_220902_175841_Aln.bam --output_vcf ${params.data_input}/Analysis/out_m84011_220902_175841_NF_sif.vcf.gz --num_shards 40. """""". }. workflow {. fa=channel.fromPath(""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta""). pbc_varicall(fa). }. ```. after running for several hours i do not get any output, instead during run , i get msg as. `TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: `. this is during `make_examples` step",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:252,interoperability,share,shared,252,"Error in pbc_varicall; Hi i have deepvariant 1.5.0 version singularity SIF file,. I ran successfully from command line for PacBio data, but when i ran the same through NextFlow as below. #nextflow.config. ```. singularity {. process.container = '/data/shared/clinical/LongRead/DeepVariant/deepvariant.simg'. cacheDir = ""/data/shared/clinical/LongRead/cache/"". singularity.enabled = true. singularity.autoMounts = true. SINGULARITY_BINDPATH = ""/data"". }. conda. {. enabled = true. cacheDir = ""/data/shared/clinical/LongRead/Programs/"". }. params. {. path=""/data/shared/clinical/LongRead/"". ref_genome=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"". pbhg38_tdx=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.mmi"". at=18. st=6. data_input=""/data/shared/clinical/LongRead/Data/"". }. ```. #deepvariant.nf. ```. process pbc_varicall {. publishDir ""/data/shared/clinical/LongRead/Data/resources/"". container 'docker://google/deepvariant:1.5.0'. input:. path 'fa'. output:. file ""*"". path 'm84011_220902_175841_NF_sif.vcf.gz'. script:. """""". run_deepvariant --model_type PACBIO --ref ${params.ref_genome} --reads ${params.data_input}/m84011_220902_175841_Aln.bam --output_vcf ${params.data_input}/Analysis/out_m84011_220902_175841_NF_sif.vcf.gz --num_shards 40. """""". }. workflow {. fa=channel.fromPath(""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta""). pbc_varicall(fa). }. ```. after running for several hours i do not get any output, instead during run , i get msg as. `TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: `. this is during `make_examples` step",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:326,interoperability,share,shared,326,"Error in pbc_varicall; Hi i have deepvariant 1.5.0 version singularity SIF file,. I ran successfully from command line for PacBio data, but when i ran the same through NextFlow as below. #nextflow.config. ```. singularity {. process.container = '/data/shared/clinical/LongRead/DeepVariant/deepvariant.simg'. cacheDir = ""/data/shared/clinical/LongRead/cache/"". singularity.enabled = true. singularity.autoMounts = true. SINGULARITY_BINDPATH = ""/data"". }. conda. {. enabled = true. cacheDir = ""/data/shared/clinical/LongRead/Programs/"". }. params. {. path=""/data/shared/clinical/LongRead/"". ref_genome=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"". pbhg38_tdx=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.mmi"". at=18. st=6. data_input=""/data/shared/clinical/LongRead/Data/"". }. ```. #deepvariant.nf. ```. process pbc_varicall {. publishDir ""/data/shared/clinical/LongRead/Data/resources/"". container 'docker://google/deepvariant:1.5.0'. input:. path 'fa'. output:. file ""*"". path 'm84011_220902_175841_NF_sif.vcf.gz'. script:. """""". run_deepvariant --model_type PACBIO --ref ${params.ref_genome} --reads ${params.data_input}/m84011_220902_175841_Aln.bam --output_vcf ${params.data_input}/Analysis/out_m84011_220902_175841_NF_sif.vcf.gz --num_shards 40. """""". }. workflow {. fa=channel.fromPath(""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta""). pbc_varicall(fa). }. ```. after running for several hours i do not get any output, instead during run , i get msg as. `TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: `. this is during `make_examples` step",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:498,interoperability,share,shared,498,"Error in pbc_varicall; Hi i have deepvariant 1.5.0 version singularity SIF file,. I ran successfully from command line for PacBio data, but when i ran the same through NextFlow as below. #nextflow.config. ```. singularity {. process.container = '/data/shared/clinical/LongRead/DeepVariant/deepvariant.simg'. cacheDir = ""/data/shared/clinical/LongRead/cache/"". singularity.enabled = true. singularity.autoMounts = true. SINGULARITY_BINDPATH = ""/data"". }. conda. {. enabled = true. cacheDir = ""/data/shared/clinical/LongRead/Programs/"". }. params. {. path=""/data/shared/clinical/LongRead/"". ref_genome=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"". pbhg38_tdx=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.mmi"". at=18. st=6. data_input=""/data/shared/clinical/LongRead/Data/"". }. ```. #deepvariant.nf. ```. process pbc_varicall {. publishDir ""/data/shared/clinical/LongRead/Data/resources/"". container 'docker://google/deepvariant:1.5.0'. input:. path 'fa'. output:. file ""*"". path 'm84011_220902_175841_NF_sif.vcf.gz'. script:. """""". run_deepvariant --model_type PACBIO --ref ${params.ref_genome} --reads ${params.data_input}/m84011_220902_175841_Aln.bam --output_vcf ${params.data_input}/Analysis/out_m84011_220902_175841_NF_sif.vcf.gz --num_shards 40. """""". }. workflow {. fa=channel.fromPath(""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta""). pbc_varicall(fa). }. ```. after running for several hours i do not get any output, instead during run , i get msg as. `TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: `. this is during `make_examples` step",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:561,interoperability,share,shared,561,"Error in pbc_varicall; Hi i have deepvariant 1.5.0 version singularity SIF file,. I ran successfully from command line for PacBio data, but when i ran the same through NextFlow as below. #nextflow.config. ```. singularity {. process.container = '/data/shared/clinical/LongRead/DeepVariant/deepvariant.simg'. cacheDir = ""/data/shared/clinical/LongRead/cache/"". singularity.enabled = true. singularity.autoMounts = true. SINGULARITY_BINDPATH = ""/data"". }. conda. {. enabled = true. cacheDir = ""/data/shared/clinical/LongRead/Programs/"". }. params. {. path=""/data/shared/clinical/LongRead/"". ref_genome=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"". pbhg38_tdx=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.mmi"". at=18. st=6. data_input=""/data/shared/clinical/LongRead/Data/"". }. ```. #deepvariant.nf. ```. process pbc_varicall {. publishDir ""/data/shared/clinical/LongRead/Data/resources/"". container 'docker://google/deepvariant:1.5.0'. input:. path 'fa'. output:. file ""*"". path 'm84011_220902_175841_NF_sif.vcf.gz'. script:. """""". run_deepvariant --model_type PACBIO --ref ${params.ref_genome} --reads ${params.data_input}/m84011_220902_175841_Aln.bam --output_vcf ${params.data_input}/Analysis/out_m84011_220902_175841_NF_sif.vcf.gz --num_shards 40. """""". }. workflow {. fa=channel.fromPath(""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta""). pbc_varicall(fa). }. ```. after running for several hours i do not get any output, instead during run , i get msg as. `TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: `. this is during `make_examples` step",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:607,interoperability,share,shared,607,"Error in pbc_varicall; Hi i have deepvariant 1.5.0 version singularity SIF file,. I ran successfully from command line for PacBio data, but when i ran the same through NextFlow as below. #nextflow.config. ```. singularity {. process.container = '/data/shared/clinical/LongRead/DeepVariant/deepvariant.simg'. cacheDir = ""/data/shared/clinical/LongRead/cache/"". singularity.enabled = true. singularity.autoMounts = true. SINGULARITY_BINDPATH = ""/data"". }. conda. {. enabled = true. cacheDir = ""/data/shared/clinical/LongRead/Programs/"". }. params. {. path=""/data/shared/clinical/LongRead/"". ref_genome=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"". pbhg38_tdx=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.mmi"". at=18. st=6. data_input=""/data/shared/clinical/LongRead/Data/"". }. ```. #deepvariant.nf. ```. process pbc_varicall {. publishDir ""/data/shared/clinical/LongRead/Data/resources/"". container 'docker://google/deepvariant:1.5.0'. input:. path 'fa'. output:. file ""*"". path 'm84011_220902_175841_NF_sif.vcf.gz'. script:. """""". run_deepvariant --model_type PACBIO --ref ${params.ref_genome} --reads ${params.data_input}/m84011_220902_175841_Aln.bam --output_vcf ${params.data_input}/Analysis/out_m84011_220902_175841_NF_sif.vcf.gz --num_shards 40. """""". }. workflow {. fa=channel.fromPath(""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta""). pbc_varicall(fa). }. ```. after running for several hours i do not get any output, instead during run , i get msg as. `TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: `. this is during `make_examples` step",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:697,interoperability,share,shared,697,"Error in pbc_varicall; Hi i have deepvariant 1.5.0 version singularity SIF file,. I ran successfully from command line for PacBio data, but when i ran the same through NextFlow as below. #nextflow.config. ```. singularity {. process.container = '/data/shared/clinical/LongRead/DeepVariant/deepvariant.simg'. cacheDir = ""/data/shared/clinical/LongRead/cache/"". singularity.enabled = true. singularity.autoMounts = true. SINGULARITY_BINDPATH = ""/data"". }. conda. {. enabled = true. cacheDir = ""/data/shared/clinical/LongRead/Programs/"". }. params. {. path=""/data/shared/clinical/LongRead/"". ref_genome=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"". pbhg38_tdx=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.mmi"". at=18. st=6. data_input=""/data/shared/clinical/LongRead/Data/"". }. ```. #deepvariant.nf. ```. process pbc_varicall {. publishDir ""/data/shared/clinical/LongRead/Data/resources/"". container 'docker://google/deepvariant:1.5.0'. input:. path 'fa'. output:. file ""*"". path 'm84011_220902_175841_NF_sif.vcf.gz'. script:. """""". run_deepvariant --model_type PACBIO --ref ${params.ref_genome} --reads ${params.data_input}/m84011_220902_175841_Aln.bam --output_vcf ${params.data_input}/Analysis/out_m84011_220902_175841_NF_sif.vcf.gz --num_shards 40. """""". }. workflow {. fa=channel.fromPath(""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta""). pbc_varicall(fa). }. ```. after running for several hours i do not get any output, instead during run , i get msg as. `TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: `. this is during `make_examples` step",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:798,interoperability,share,shared,798,"Error in pbc_varicall; Hi i have deepvariant 1.5.0 version singularity SIF file,. I ran successfully from command line for PacBio data, but when i ran the same through NextFlow as below. #nextflow.config. ```. singularity {. process.container = '/data/shared/clinical/LongRead/DeepVariant/deepvariant.simg'. cacheDir = ""/data/shared/clinical/LongRead/cache/"". singularity.enabled = true. singularity.autoMounts = true. SINGULARITY_BINDPATH = ""/data"". }. conda. {. enabled = true. cacheDir = ""/data/shared/clinical/LongRead/Programs/"". }. params. {. path=""/data/shared/clinical/LongRead/"". ref_genome=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"". pbhg38_tdx=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.mmi"". at=18. st=6. data_input=""/data/shared/clinical/LongRead/Data/"". }. ```. #deepvariant.nf. ```. process pbc_varicall {. publishDir ""/data/shared/clinical/LongRead/Data/resources/"". container 'docker://google/deepvariant:1.5.0'. input:. path 'fa'. output:. file ""*"". path 'm84011_220902_175841_NF_sif.vcf.gz'. script:. """""". run_deepvariant --model_type PACBIO --ref ${params.ref_genome} --reads ${params.data_input}/m84011_220902_175841_Aln.bam --output_vcf ${params.data_input}/Analysis/out_m84011_220902_175841_NF_sif.vcf.gz --num_shards 40. """""". }. workflow {. fa=channel.fromPath(""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta""). pbc_varicall(fa). }. ```. after running for several hours i do not get any output, instead during run , i get msg as. `TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: `. this is during `make_examples` step",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:903,interoperability,share,shared,903,"Error in pbc_varicall; Hi i have deepvariant 1.5.0 version singularity SIF file,. I ran successfully from command line for PacBio data, but when i ran the same through NextFlow as below. #nextflow.config. ```. singularity {. process.container = '/data/shared/clinical/LongRead/DeepVariant/deepvariant.simg'. cacheDir = ""/data/shared/clinical/LongRead/cache/"". singularity.enabled = true. singularity.autoMounts = true. SINGULARITY_BINDPATH = ""/data"". }. conda. {. enabled = true. cacheDir = ""/data/shared/clinical/LongRead/Programs/"". }. params. {. path=""/data/shared/clinical/LongRead/"". ref_genome=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"". pbhg38_tdx=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.mmi"". at=18. st=6. data_input=""/data/shared/clinical/LongRead/Data/"". }. ```. #deepvariant.nf. ```. process pbc_varicall {. publishDir ""/data/shared/clinical/LongRead/Data/resources/"". container 'docker://google/deepvariant:1.5.0'. input:. path 'fa'. output:. file ""*"". path 'm84011_220902_175841_NF_sif.vcf.gz'. script:. """""". run_deepvariant --model_type PACBIO --ref ${params.ref_genome} --reads ${params.data_input}/m84011_220902_175841_Aln.bam --output_vcf ${params.data_input}/Analysis/out_m84011_220902_175841_NF_sif.vcf.gz --num_shards 40. """""". }. workflow {. fa=channel.fromPath(""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta""). pbc_varicall(fa). }. ```. after running for several hours i do not get any output, instead during run , i get msg as. `TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: `. this is during `make_examples` step",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:1355,interoperability,share,shared,1355,"Error in pbc_varicall; Hi i have deepvariant 1.5.0 version singularity SIF file,. I ran successfully from command line for PacBio data, but when i ran the same through NextFlow as below. #nextflow.config. ```. singularity {. process.container = '/data/shared/clinical/LongRead/DeepVariant/deepvariant.simg'. cacheDir = ""/data/shared/clinical/LongRead/cache/"". singularity.enabled = true. singularity.autoMounts = true. SINGULARITY_BINDPATH = ""/data"". }. conda. {. enabled = true. cacheDir = ""/data/shared/clinical/LongRead/Programs/"". }. params. {. path=""/data/shared/clinical/LongRead/"". ref_genome=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"". pbhg38_tdx=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.mmi"". at=18. st=6. data_input=""/data/shared/clinical/LongRead/Data/"". }. ```. #deepvariant.nf. ```. process pbc_varicall {. publishDir ""/data/shared/clinical/LongRead/Data/resources/"". container 'docker://google/deepvariant:1.5.0'. input:. path 'fa'. output:. file ""*"". path 'm84011_220902_175841_NF_sif.vcf.gz'. script:. """""". run_deepvariant --model_type PACBIO --ref ${params.ref_genome} --reads ${params.data_input}/m84011_220902_175841_Aln.bam --output_vcf ${params.data_input}/Analysis/out_m84011_220902_175841_NF_sif.vcf.gz --num_shards 40. """""". }. workflow {. fa=channel.fromPath(""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta""). pbc_varicall(fa). }. ```. after running for several hours i do not get any output, instead during run , i get msg as. `TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: `. this is during `make_examples` step",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:51,modifiability,version,version,51,"Error in pbc_varicall; Hi i have deepvariant 1.5.0 version singularity SIF file,. I ran successfully from command line for PacBio data, but when i ran the same through NextFlow as below. #nextflow.config. ```. singularity {. process.container = '/data/shared/clinical/LongRead/DeepVariant/deepvariant.simg'. cacheDir = ""/data/shared/clinical/LongRead/cache/"". singularity.enabled = true. singularity.autoMounts = true. SINGULARITY_BINDPATH = ""/data"". }. conda. {. enabled = true. cacheDir = ""/data/shared/clinical/LongRead/Programs/"". }. params. {. path=""/data/shared/clinical/LongRead/"". ref_genome=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"". pbhg38_tdx=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.mmi"". at=18. st=6. data_input=""/data/shared/clinical/LongRead/Data/"". }. ```. #deepvariant.nf. ```. process pbc_varicall {. publishDir ""/data/shared/clinical/LongRead/Data/resources/"". container 'docker://google/deepvariant:1.5.0'. input:. path 'fa'. output:. file ""*"". path 'm84011_220902_175841_NF_sif.vcf.gz'. script:. """""". run_deepvariant --model_type PACBIO --ref ${params.ref_genome} --reads ${params.data_input}/m84011_220902_175841_Aln.bam --output_vcf ${params.data_input}/Analysis/out_m84011_220902_175841_NF_sif.vcf.gz --num_shards 40. """""". }. workflow {. fa=channel.fromPath(""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta""). pbc_varicall(fa). }. ```. after running for several hours i do not get any output, instead during run , i get msg as. `TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: `. this is during `make_examples` step",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:123,modifiability,Pac,PacBio,123,"Error in pbc_varicall; Hi i have deepvariant 1.5.0 version singularity SIF file,. I ran successfully from command line for PacBio data, but when i ran the same through NextFlow as below. #nextflow.config. ```. singularity {. process.container = '/data/shared/clinical/LongRead/DeepVariant/deepvariant.simg'. cacheDir = ""/data/shared/clinical/LongRead/cache/"". singularity.enabled = true. singularity.autoMounts = true. SINGULARITY_BINDPATH = ""/data"". }. conda. {. enabled = true. cacheDir = ""/data/shared/clinical/LongRead/Programs/"". }. params. {. path=""/data/shared/clinical/LongRead/"". ref_genome=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"". pbhg38_tdx=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.mmi"". at=18. st=6. data_input=""/data/shared/clinical/LongRead/Data/"". }. ```. #deepvariant.nf. ```. process pbc_varicall {. publishDir ""/data/shared/clinical/LongRead/Data/resources/"". container 'docker://google/deepvariant:1.5.0'. input:. path 'fa'. output:. file ""*"". path 'm84011_220902_175841_NF_sif.vcf.gz'. script:. """""". run_deepvariant --model_type PACBIO --ref ${params.ref_genome} --reads ${params.data_input}/m84011_220902_175841_Aln.bam --output_vcf ${params.data_input}/Analysis/out_m84011_220902_175841_NF_sif.vcf.gz --num_shards 40. """""". }. workflow {. fa=channel.fromPath(""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta""). pbc_varicall(fa). }. ```. after running for several hours i do not get any output, instead during run , i get msg as. `TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: `. this is during `make_examples` step",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:1117,modifiability,PAC,PACBIO,1117,"Error in pbc_varicall; Hi i have deepvariant 1.5.0 version singularity SIF file,. I ran successfully from command line for PacBio data, but when i ran the same through NextFlow as below. #nextflow.config. ```. singularity {. process.container = '/data/shared/clinical/LongRead/DeepVariant/deepvariant.simg'. cacheDir = ""/data/shared/clinical/LongRead/cache/"". singularity.enabled = true. singularity.autoMounts = true. SINGULARITY_BINDPATH = ""/data"". }. conda. {. enabled = true. cacheDir = ""/data/shared/clinical/LongRead/Programs/"". }. params. {. path=""/data/shared/clinical/LongRead/"". ref_genome=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"". pbhg38_tdx=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.mmi"". at=18. st=6. data_input=""/data/shared/clinical/LongRead/Data/"". }. ```. #deepvariant.nf. ```. process pbc_varicall {. publishDir ""/data/shared/clinical/LongRead/Data/resources/"". container 'docker://google/deepvariant:1.5.0'. input:. path 'fa'. output:. file ""*"". path 'm84011_220902_175841_NF_sif.vcf.gz'. script:. """""". run_deepvariant --model_type PACBIO --ref ${params.ref_genome} --reads ${params.data_input}/m84011_220902_175841_Aln.bam --output_vcf ${params.data_input}/Analysis/out_m84011_220902_175841_NF_sif.vcf.gz --num_shards 40. """""". }. workflow {. fa=channel.fromPath(""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta""). pbc_varicall(fa). }. ```. after running for several hours i do not get any output, instead during run , i get msg as. `TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: `. this is during `make_examples` step",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:0,performance,Error,Error,0,"Error in pbc_varicall; Hi i have deepvariant 1.5.0 version singularity SIF file,. I ran successfully from command line for PacBio data, but when i ran the same through NextFlow as below. #nextflow.config. ```. singularity {. process.container = '/data/shared/clinical/LongRead/DeepVariant/deepvariant.simg'. cacheDir = ""/data/shared/clinical/LongRead/cache/"". singularity.enabled = true. singularity.autoMounts = true. SINGULARITY_BINDPATH = ""/data"". }. conda. {. enabled = true. cacheDir = ""/data/shared/clinical/LongRead/Programs/"". }. params. {. path=""/data/shared/clinical/LongRead/"". ref_genome=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"". pbhg38_tdx=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.mmi"". at=18. st=6. data_input=""/data/shared/clinical/LongRead/Data/"". }. ```. #deepvariant.nf. ```. process pbc_varicall {. publishDir ""/data/shared/clinical/LongRead/Data/resources/"". container 'docker://google/deepvariant:1.5.0'. input:. path 'fa'. output:. file ""*"". path 'm84011_220902_175841_NF_sif.vcf.gz'. script:. """""". run_deepvariant --model_type PACBIO --ref ${params.ref_genome} --reads ${params.data_input}/m84011_220902_175841_Aln.bam --output_vcf ${params.data_input}/Analysis/out_m84011_220902_175841_NF_sif.vcf.gz --num_shards 40. """""". }. workflow {. fa=channel.fromPath(""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta""). pbc_varicall(fa). }. ```. after running for several hours i do not get any output, instead during run , i get msg as. `TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: `. this is during `make_examples` step",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:308,performance,cach,cacheDir,308,"Error in pbc_varicall; Hi i have deepvariant 1.5.0 version singularity SIF file,. I ran successfully from command line for PacBio data, but when i ran the same through NextFlow as below. #nextflow.config. ```. singularity {. process.container = '/data/shared/clinical/LongRead/DeepVariant/deepvariant.simg'. cacheDir = ""/data/shared/clinical/LongRead/cache/"". singularity.enabled = true. singularity.autoMounts = true. SINGULARITY_BINDPATH = ""/data"". }. conda. {. enabled = true. cacheDir = ""/data/shared/clinical/LongRead/Programs/"". }. params. {. path=""/data/shared/clinical/LongRead/"". ref_genome=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"". pbhg38_tdx=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.mmi"". at=18. st=6. data_input=""/data/shared/clinical/LongRead/Data/"". }. ```. #deepvariant.nf. ```. process pbc_varicall {. publishDir ""/data/shared/clinical/LongRead/Data/resources/"". container 'docker://google/deepvariant:1.5.0'. input:. path 'fa'. output:. file ""*"". path 'm84011_220902_175841_NF_sif.vcf.gz'. script:. """""". run_deepvariant --model_type PACBIO --ref ${params.ref_genome} --reads ${params.data_input}/m84011_220902_175841_Aln.bam --output_vcf ${params.data_input}/Analysis/out_m84011_220902_175841_NF_sif.vcf.gz --num_shards 40. """""". }. workflow {. fa=channel.fromPath(""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta""). pbc_varicall(fa). }. ```. after running for several hours i do not get any output, instead during run , i get msg as. `TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: `. this is during `make_examples` step",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:351,performance,cach,cache,351,"Error in pbc_varicall; Hi i have deepvariant 1.5.0 version singularity SIF file,. I ran successfully from command line for PacBio data, but when i ran the same through NextFlow as below. #nextflow.config. ```. singularity {. process.container = '/data/shared/clinical/LongRead/DeepVariant/deepvariant.simg'. cacheDir = ""/data/shared/clinical/LongRead/cache/"". singularity.enabled = true. singularity.autoMounts = true. SINGULARITY_BINDPATH = ""/data"". }. conda. {. enabled = true. cacheDir = ""/data/shared/clinical/LongRead/Programs/"". }. params. {. path=""/data/shared/clinical/LongRead/"". ref_genome=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"". pbhg38_tdx=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.mmi"". at=18. st=6. data_input=""/data/shared/clinical/LongRead/Data/"". }. ```. #deepvariant.nf. ```. process pbc_varicall {. publishDir ""/data/shared/clinical/LongRead/Data/resources/"". container 'docker://google/deepvariant:1.5.0'. input:. path 'fa'. output:. file ""*"". path 'm84011_220902_175841_NF_sif.vcf.gz'. script:. """""". run_deepvariant --model_type PACBIO --ref ${params.ref_genome} --reads ${params.data_input}/m84011_220902_175841_Aln.bam --output_vcf ${params.data_input}/Analysis/out_m84011_220902_175841_NF_sif.vcf.gz --num_shards 40. """""". }. workflow {. fa=channel.fromPath(""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta""). pbc_varicall(fa). }. ```. after running for several hours i do not get any output, instead during run , i get msg as. `TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: `. this is during `make_examples` step",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:480,performance,cach,cacheDir,480,"Error in pbc_varicall; Hi i have deepvariant 1.5.0 version singularity SIF file,. I ran successfully from command line for PacBio data, but when i ran the same through NextFlow as below. #nextflow.config. ```. singularity {. process.container = '/data/shared/clinical/LongRead/DeepVariant/deepvariant.simg'. cacheDir = ""/data/shared/clinical/LongRead/cache/"". singularity.enabled = true. singularity.autoMounts = true. SINGULARITY_BINDPATH = ""/data"". }. conda. {. enabled = true. cacheDir = ""/data/shared/clinical/LongRead/Programs/"". }. params. {. path=""/data/shared/clinical/LongRead/"". ref_genome=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"". pbhg38_tdx=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.mmi"". at=18. st=6. data_input=""/data/shared/clinical/LongRead/Data/"". }. ```. #deepvariant.nf. ```. process pbc_varicall {. publishDir ""/data/shared/clinical/LongRead/Data/resources/"". container 'docker://google/deepvariant:1.5.0'. input:. path 'fa'. output:. file ""*"". path 'm84011_220902_175841_NF_sif.vcf.gz'. script:. """""". run_deepvariant --model_type PACBIO --ref ${params.ref_genome} --reads ${params.data_input}/m84011_220902_175841_Aln.bam --output_vcf ${params.data_input}/Analysis/out_m84011_220902_175841_NF_sif.vcf.gz --num_shards 40. """""". }. workflow {. fa=channel.fromPath(""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta""). pbc_varicall(fa). }. ```. after running for several hours i do not get any output, instead during run , i get msg as. `TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: `. this is during `make_examples` step",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:637,performance,resourc,resources,637,"Error in pbc_varicall; Hi i have deepvariant 1.5.0 version singularity SIF file,. I ran successfully from command line for PacBio data, but when i ran the same through NextFlow as below. #nextflow.config. ```. singularity {. process.container = '/data/shared/clinical/LongRead/DeepVariant/deepvariant.simg'. cacheDir = ""/data/shared/clinical/LongRead/cache/"". singularity.enabled = true. singularity.autoMounts = true. SINGULARITY_BINDPATH = ""/data"". }. conda. {. enabled = true. cacheDir = ""/data/shared/clinical/LongRead/Programs/"". }. params. {. path=""/data/shared/clinical/LongRead/"". ref_genome=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"". pbhg38_tdx=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.mmi"". at=18. st=6. data_input=""/data/shared/clinical/LongRead/Data/"". }. ```. #deepvariant.nf. ```. process pbc_varicall {. publishDir ""/data/shared/clinical/LongRead/Data/resources/"". container 'docker://google/deepvariant:1.5.0'. input:. path 'fa'. output:. file ""*"". path 'm84011_220902_175841_NF_sif.vcf.gz'. script:. """""". run_deepvariant --model_type PACBIO --ref ${params.ref_genome} --reads ${params.data_input}/m84011_220902_175841_Aln.bam --output_vcf ${params.data_input}/Analysis/out_m84011_220902_175841_NF_sif.vcf.gz --num_shards 40. """""". }. workflow {. fa=channel.fromPath(""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta""). pbc_varicall(fa). }. ```. after running for several hours i do not get any output, instead during run , i get msg as. `TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: `. this is during `make_examples` step",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:727,performance,resourc,resources,727,"Error in pbc_varicall; Hi i have deepvariant 1.5.0 version singularity SIF file,. I ran successfully from command line for PacBio data, but when i ran the same through NextFlow as below. #nextflow.config. ```. singularity {. process.container = '/data/shared/clinical/LongRead/DeepVariant/deepvariant.simg'. cacheDir = ""/data/shared/clinical/LongRead/cache/"". singularity.enabled = true. singularity.autoMounts = true. SINGULARITY_BINDPATH = ""/data"". }. conda. {. enabled = true. cacheDir = ""/data/shared/clinical/LongRead/Programs/"". }. params. {. path=""/data/shared/clinical/LongRead/"". ref_genome=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"". pbhg38_tdx=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.mmi"". at=18. st=6. data_input=""/data/shared/clinical/LongRead/Data/"". }. ```. #deepvariant.nf. ```. process pbc_varicall {. publishDir ""/data/shared/clinical/LongRead/Data/resources/"". container 'docker://google/deepvariant:1.5.0'. input:. path 'fa'. output:. file ""*"". path 'm84011_220902_175841_NF_sif.vcf.gz'. script:. """""". run_deepvariant --model_type PACBIO --ref ${params.ref_genome} --reads ${params.data_input}/m84011_220902_175841_Aln.bam --output_vcf ${params.data_input}/Analysis/out_m84011_220902_175841_NF_sif.vcf.gz --num_shards 40. """""". }. workflow {. fa=channel.fromPath(""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta""). pbc_varicall(fa). }. ```. after running for several hours i do not get any output, instead during run , i get msg as. `TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: `. this is during `make_examples` step",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:933,performance,resourc,resources,933,"Error in pbc_varicall; Hi i have deepvariant 1.5.0 version singularity SIF file,. I ran successfully from command line for PacBio data, but when i ran the same through NextFlow as below. #nextflow.config. ```. singularity {. process.container = '/data/shared/clinical/LongRead/DeepVariant/deepvariant.simg'. cacheDir = ""/data/shared/clinical/LongRead/cache/"". singularity.enabled = true. singularity.autoMounts = true. SINGULARITY_BINDPATH = ""/data"". }. conda. {. enabled = true. cacheDir = ""/data/shared/clinical/LongRead/Programs/"". }. params. {. path=""/data/shared/clinical/LongRead/"". ref_genome=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"". pbhg38_tdx=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.mmi"". at=18. st=6. data_input=""/data/shared/clinical/LongRead/Data/"". }. ```. #deepvariant.nf. ```. process pbc_varicall {. publishDir ""/data/shared/clinical/LongRead/Data/resources/"". container 'docker://google/deepvariant:1.5.0'. input:. path 'fa'. output:. file ""*"". path 'm84011_220902_175841_NF_sif.vcf.gz'. script:. """""". run_deepvariant --model_type PACBIO --ref ${params.ref_genome} --reads ${params.data_input}/m84011_220902_175841_Aln.bam --output_vcf ${params.data_input}/Analysis/out_m84011_220902_175841_NF_sif.vcf.gz --num_shards 40. """""". }. workflow {. fa=channel.fromPath(""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta""). pbc_varicall(fa). }. ```. after running for several hours i do not get any output, instead during run , i get msg as. `TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: `. this is during `make_examples` step",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:1385,performance,resourc,resources,1385,"Error in pbc_varicall; Hi i have deepvariant 1.5.0 version singularity SIF file,. I ran successfully from command line for PacBio data, but when i ran the same through NextFlow as below. #nextflow.config. ```. singularity {. process.container = '/data/shared/clinical/LongRead/DeepVariant/deepvariant.simg'. cacheDir = ""/data/shared/clinical/LongRead/cache/"". singularity.enabled = true. singularity.autoMounts = true. SINGULARITY_BINDPATH = ""/data"". }. conda. {. enabled = true. cacheDir = ""/data/shared/clinical/LongRead/Programs/"". }. params. {. path=""/data/shared/clinical/LongRead/"". ref_genome=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"". pbhg38_tdx=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.mmi"". at=18. st=6. data_input=""/data/shared/clinical/LongRead/Data/"". }. ```. #deepvariant.nf. ```. process pbc_varicall {. publishDir ""/data/shared/clinical/LongRead/Data/resources/"". container 'docker://google/deepvariant:1.5.0'. input:. path 'fa'. output:. file ""*"". path 'm84011_220902_175841_NF_sif.vcf.gz'. script:. """""". run_deepvariant --model_type PACBIO --ref ${params.ref_genome} --reads ${params.data_input}/m84011_220902_175841_Aln.bam --output_vcf ${params.data_input}/Analysis/out_m84011_220902_175841_NF_sif.vcf.gz --num_shards 40. """""". }. workflow {. fa=channel.fromPath(""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta""). pbc_varicall(fa). }. ```. after running for several hours i do not get any output, instead during run , i get msg as. `TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: `. this is during `make_examples` step",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:1616,performance,error,error,1616,"Error in pbc_varicall; Hi i have deepvariant 1.5.0 version singularity SIF file,. I ran successfully from command line for PacBio data, but when i ran the same through NextFlow as below. #nextflow.config. ```. singularity {. process.container = '/data/shared/clinical/LongRead/DeepVariant/deepvariant.simg'. cacheDir = ""/data/shared/clinical/LongRead/cache/"". singularity.enabled = true. singularity.autoMounts = true. SINGULARITY_BINDPATH = ""/data"". }. conda. {. enabled = true. cacheDir = ""/data/shared/clinical/LongRead/Programs/"". }. params. {. path=""/data/shared/clinical/LongRead/"". ref_genome=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"". pbhg38_tdx=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.mmi"". at=18. st=6. data_input=""/data/shared/clinical/LongRead/Data/"". }. ```. #deepvariant.nf. ```. process pbc_varicall {. publishDir ""/data/shared/clinical/LongRead/Data/resources/"". container 'docker://google/deepvariant:1.5.0'. input:. path 'fa'. output:. file ""*"". path 'm84011_220902_175841_NF_sif.vcf.gz'. script:. """""". run_deepvariant --model_type PACBIO --ref ${params.ref_genome} --reads ${params.data_input}/m84011_220902_175841_Aln.bam --output_vcf ${params.data_input}/Analysis/out_m84011_220902_175841_NF_sif.vcf.gz --num_shards 40. """""". }. workflow {. fa=channel.fromPath(""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta""). pbc_varicall(fa). }. ```. after running for several hours i do not get any output, instead during run , i get msg as. `TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: `. this is during `make_examples` step",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:0,safety,Error,Error,0,"Error in pbc_varicall; Hi i have deepvariant 1.5.0 version singularity SIF file,. I ran successfully from command line for PacBio data, but when i ran the same through NextFlow as below. #nextflow.config. ```. singularity {. process.container = '/data/shared/clinical/LongRead/DeepVariant/deepvariant.simg'. cacheDir = ""/data/shared/clinical/LongRead/cache/"". singularity.enabled = true. singularity.autoMounts = true. SINGULARITY_BINDPATH = ""/data"". }. conda. {. enabled = true. cacheDir = ""/data/shared/clinical/LongRead/Programs/"". }. params. {. path=""/data/shared/clinical/LongRead/"". ref_genome=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"". pbhg38_tdx=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.mmi"". at=18. st=6. data_input=""/data/shared/clinical/LongRead/Data/"". }. ```. #deepvariant.nf. ```. process pbc_varicall {. publishDir ""/data/shared/clinical/LongRead/Data/resources/"". container 'docker://google/deepvariant:1.5.0'. input:. path 'fa'. output:. file ""*"". path 'm84011_220902_175841_NF_sif.vcf.gz'. script:. """""". run_deepvariant --model_type PACBIO --ref ${params.ref_genome} --reads ${params.data_input}/m84011_220902_175841_Aln.bam --output_vcf ${params.data_input}/Analysis/out_m84011_220902_175841_NF_sif.vcf.gz --num_shards 40. """""". }. workflow {. fa=channel.fromPath(""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta""). pbc_varicall(fa). }. ```. after running for several hours i do not get any output, instead during run , i get msg as. `TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: `. this is during `make_examples` step",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:637,safety,resourc,resources,637,"Error in pbc_varicall; Hi i have deepvariant 1.5.0 version singularity SIF file,. I ran successfully from command line for PacBio data, but when i ran the same through NextFlow as below. #nextflow.config. ```. singularity {. process.container = '/data/shared/clinical/LongRead/DeepVariant/deepvariant.simg'. cacheDir = ""/data/shared/clinical/LongRead/cache/"". singularity.enabled = true. singularity.autoMounts = true. SINGULARITY_BINDPATH = ""/data"". }. conda. {. enabled = true. cacheDir = ""/data/shared/clinical/LongRead/Programs/"". }. params. {. path=""/data/shared/clinical/LongRead/"". ref_genome=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"". pbhg38_tdx=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.mmi"". at=18. st=6. data_input=""/data/shared/clinical/LongRead/Data/"". }. ```. #deepvariant.nf. ```. process pbc_varicall {. publishDir ""/data/shared/clinical/LongRead/Data/resources/"". container 'docker://google/deepvariant:1.5.0'. input:. path 'fa'. output:. file ""*"". path 'm84011_220902_175841_NF_sif.vcf.gz'. script:. """""". run_deepvariant --model_type PACBIO --ref ${params.ref_genome} --reads ${params.data_input}/m84011_220902_175841_Aln.bam --output_vcf ${params.data_input}/Analysis/out_m84011_220902_175841_NF_sif.vcf.gz --num_shards 40. """""". }. workflow {. fa=channel.fromPath(""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta""). pbc_varicall(fa). }. ```. after running for several hours i do not get any output, instead during run , i get msg as. `TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: `. this is during `make_examples` step",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:727,safety,resourc,resources,727,"Error in pbc_varicall; Hi i have deepvariant 1.5.0 version singularity SIF file,. I ran successfully from command line for PacBio data, but when i ran the same through NextFlow as below. #nextflow.config. ```. singularity {. process.container = '/data/shared/clinical/LongRead/DeepVariant/deepvariant.simg'. cacheDir = ""/data/shared/clinical/LongRead/cache/"". singularity.enabled = true. singularity.autoMounts = true. SINGULARITY_BINDPATH = ""/data"". }. conda. {. enabled = true. cacheDir = ""/data/shared/clinical/LongRead/Programs/"". }. params. {. path=""/data/shared/clinical/LongRead/"". ref_genome=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"". pbhg38_tdx=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.mmi"". at=18. st=6. data_input=""/data/shared/clinical/LongRead/Data/"". }. ```. #deepvariant.nf. ```. process pbc_varicall {. publishDir ""/data/shared/clinical/LongRead/Data/resources/"". container 'docker://google/deepvariant:1.5.0'. input:. path 'fa'. output:. file ""*"". path 'm84011_220902_175841_NF_sif.vcf.gz'. script:. """""". run_deepvariant --model_type PACBIO --ref ${params.ref_genome} --reads ${params.data_input}/m84011_220902_175841_Aln.bam --output_vcf ${params.data_input}/Analysis/out_m84011_220902_175841_NF_sif.vcf.gz --num_shards 40. """""". }. workflow {. fa=channel.fromPath(""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta""). pbc_varicall(fa). }. ```. after running for several hours i do not get any output, instead during run , i get msg as. `TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: `. this is during `make_examples` step",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:933,safety,resourc,resources,933,"Error in pbc_varicall; Hi i have deepvariant 1.5.0 version singularity SIF file,. I ran successfully from command line for PacBio data, but when i ran the same through NextFlow as below. #nextflow.config. ```. singularity {. process.container = '/data/shared/clinical/LongRead/DeepVariant/deepvariant.simg'. cacheDir = ""/data/shared/clinical/LongRead/cache/"". singularity.enabled = true. singularity.autoMounts = true. SINGULARITY_BINDPATH = ""/data"". }. conda. {. enabled = true. cacheDir = ""/data/shared/clinical/LongRead/Programs/"". }. params. {. path=""/data/shared/clinical/LongRead/"". ref_genome=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"". pbhg38_tdx=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.mmi"". at=18. st=6. data_input=""/data/shared/clinical/LongRead/Data/"". }. ```. #deepvariant.nf. ```. process pbc_varicall {. publishDir ""/data/shared/clinical/LongRead/Data/resources/"". container 'docker://google/deepvariant:1.5.0'. input:. path 'fa'. output:. file ""*"". path 'm84011_220902_175841_NF_sif.vcf.gz'. script:. """""". run_deepvariant --model_type PACBIO --ref ${params.ref_genome} --reads ${params.data_input}/m84011_220902_175841_Aln.bam --output_vcf ${params.data_input}/Analysis/out_m84011_220902_175841_NF_sif.vcf.gz --num_shards 40. """""". }. workflow {. fa=channel.fromPath(""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta""). pbc_varicall(fa). }. ```. after running for several hours i do not get any output, instead during run , i get msg as. `TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: `. this is during `make_examples` step",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:993,safety,input,input,993,"Error in pbc_varicall; Hi i have deepvariant 1.5.0 version singularity SIF file,. I ran successfully from command line for PacBio data, but when i ran the same through NextFlow as below. #nextflow.config. ```. singularity {. process.container = '/data/shared/clinical/LongRead/DeepVariant/deepvariant.simg'. cacheDir = ""/data/shared/clinical/LongRead/cache/"". singularity.enabled = true. singularity.autoMounts = true. SINGULARITY_BINDPATH = ""/data"". }. conda. {. enabled = true. cacheDir = ""/data/shared/clinical/LongRead/Programs/"". }. params. {. path=""/data/shared/clinical/LongRead/"". ref_genome=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"". pbhg38_tdx=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.mmi"". at=18. st=6. data_input=""/data/shared/clinical/LongRead/Data/"". }. ```. #deepvariant.nf. ```. process pbc_varicall {. publishDir ""/data/shared/clinical/LongRead/Data/resources/"". container 'docker://google/deepvariant:1.5.0'. input:. path 'fa'. output:. file ""*"". path 'm84011_220902_175841_NF_sif.vcf.gz'. script:. """""". run_deepvariant --model_type PACBIO --ref ${params.ref_genome} --reads ${params.data_input}/m84011_220902_175841_Aln.bam --output_vcf ${params.data_input}/Analysis/out_m84011_220902_175841_NF_sif.vcf.gz --num_shards 40. """""". }. workflow {. fa=channel.fromPath(""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta""). pbc_varicall(fa). }. ```. after running for several hours i do not get any output, instead during run , i get msg as. `TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: `. this is during `make_examples` step",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:1385,safety,resourc,resources,1385,"Error in pbc_varicall; Hi i have deepvariant 1.5.0 version singularity SIF file,. I ran successfully from command line for PacBio data, but when i ran the same through NextFlow as below. #nextflow.config. ```. singularity {. process.container = '/data/shared/clinical/LongRead/DeepVariant/deepvariant.simg'. cacheDir = ""/data/shared/clinical/LongRead/cache/"". singularity.enabled = true. singularity.autoMounts = true. SINGULARITY_BINDPATH = ""/data"". }. conda. {. enabled = true. cacheDir = ""/data/shared/clinical/LongRead/Programs/"". }. params. {. path=""/data/shared/clinical/LongRead/"". ref_genome=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"". pbhg38_tdx=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.mmi"". at=18. st=6. data_input=""/data/shared/clinical/LongRead/Data/"". }. ```. #deepvariant.nf. ```. process pbc_varicall {. publishDir ""/data/shared/clinical/LongRead/Data/resources/"". container 'docker://google/deepvariant:1.5.0'. input:. path 'fa'. output:. file ""*"". path 'm84011_220902_175841_NF_sif.vcf.gz'. script:. """""". run_deepvariant --model_type PACBIO --ref ${params.ref_genome} --reads ${params.data_input}/m84011_220902_175841_Aln.bam --output_vcf ${params.data_input}/Analysis/out_m84011_220902_175841_NF_sif.vcf.gz --num_shards 40. """""". }. workflow {. fa=channel.fromPath(""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta""). pbc_varicall(fa). }. ```. after running for several hours i do not get any output, instead during run , i get msg as. `TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: `. this is during `make_examples` step",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:1616,safety,error,error,1616,"Error in pbc_varicall; Hi i have deepvariant 1.5.0 version singularity SIF file,. I ran successfully from command line for PacBio data, but when i ran the same through NextFlow as below. #nextflow.config. ```. singularity {. process.container = '/data/shared/clinical/LongRead/DeepVariant/deepvariant.simg'. cacheDir = ""/data/shared/clinical/LongRead/cache/"". singularity.enabled = true. singularity.autoMounts = true. SINGULARITY_BINDPATH = ""/data"". }. conda. {. enabled = true. cacheDir = ""/data/shared/clinical/LongRead/Programs/"". }. params. {. path=""/data/shared/clinical/LongRead/"". ref_genome=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"". pbhg38_tdx=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.mmi"". at=18. st=6. data_input=""/data/shared/clinical/LongRead/Data/"". }. ```. #deepvariant.nf. ```. process pbc_varicall {. publishDir ""/data/shared/clinical/LongRead/Data/resources/"". container 'docker://google/deepvariant:1.5.0'. input:. path 'fa'. output:. file ""*"". path 'm84011_220902_175841_NF_sif.vcf.gz'. script:. """""". run_deepvariant --model_type PACBIO --ref ${params.ref_genome} --reads ${params.data_input}/m84011_220902_175841_Aln.bam --output_vcf ${params.data_input}/Analysis/out_m84011_220902_175841_NF_sif.vcf.gz --num_shards 40. """""". }. workflow {. fa=channel.fromPath(""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta""). pbc_varicall(fa). }. ```. after running for several hours i do not get any output, instead during run , i get msg as. `TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: `. this is during `make_examples` step",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:637,testability,resourc,resources,637,"Error in pbc_varicall; Hi i have deepvariant 1.5.0 version singularity SIF file,. I ran successfully from command line for PacBio data, but when i ran the same through NextFlow as below. #nextflow.config. ```. singularity {. process.container = '/data/shared/clinical/LongRead/DeepVariant/deepvariant.simg'. cacheDir = ""/data/shared/clinical/LongRead/cache/"". singularity.enabled = true. singularity.autoMounts = true. SINGULARITY_BINDPATH = ""/data"". }. conda. {. enabled = true. cacheDir = ""/data/shared/clinical/LongRead/Programs/"". }. params. {. path=""/data/shared/clinical/LongRead/"". ref_genome=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"". pbhg38_tdx=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.mmi"". at=18. st=6. data_input=""/data/shared/clinical/LongRead/Data/"". }. ```. #deepvariant.nf. ```. process pbc_varicall {. publishDir ""/data/shared/clinical/LongRead/Data/resources/"". container 'docker://google/deepvariant:1.5.0'. input:. path 'fa'. output:. file ""*"". path 'm84011_220902_175841_NF_sif.vcf.gz'. script:. """""". run_deepvariant --model_type PACBIO --ref ${params.ref_genome} --reads ${params.data_input}/m84011_220902_175841_Aln.bam --output_vcf ${params.data_input}/Analysis/out_m84011_220902_175841_NF_sif.vcf.gz --num_shards 40. """""". }. workflow {. fa=channel.fromPath(""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta""). pbc_varicall(fa). }. ```. after running for several hours i do not get any output, instead during run , i get msg as. `TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: `. this is during `make_examples` step",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:727,testability,resourc,resources,727,"Error in pbc_varicall; Hi i have deepvariant 1.5.0 version singularity SIF file,. I ran successfully from command line for PacBio data, but when i ran the same through NextFlow as below. #nextflow.config. ```. singularity {. process.container = '/data/shared/clinical/LongRead/DeepVariant/deepvariant.simg'. cacheDir = ""/data/shared/clinical/LongRead/cache/"". singularity.enabled = true. singularity.autoMounts = true. SINGULARITY_BINDPATH = ""/data"". }. conda. {. enabled = true. cacheDir = ""/data/shared/clinical/LongRead/Programs/"". }. params. {. path=""/data/shared/clinical/LongRead/"". ref_genome=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"". pbhg38_tdx=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.mmi"". at=18. st=6. data_input=""/data/shared/clinical/LongRead/Data/"". }. ```. #deepvariant.nf. ```. process pbc_varicall {. publishDir ""/data/shared/clinical/LongRead/Data/resources/"". container 'docker://google/deepvariant:1.5.0'. input:. path 'fa'. output:. file ""*"". path 'm84011_220902_175841_NF_sif.vcf.gz'. script:. """""". run_deepvariant --model_type PACBIO --ref ${params.ref_genome} --reads ${params.data_input}/m84011_220902_175841_Aln.bam --output_vcf ${params.data_input}/Analysis/out_m84011_220902_175841_NF_sif.vcf.gz --num_shards 40. """""". }. workflow {. fa=channel.fromPath(""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta""). pbc_varicall(fa). }. ```. after running for several hours i do not get any output, instead during run , i get msg as. `TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: `. this is during `make_examples` step",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:933,testability,resourc,resources,933,"Error in pbc_varicall; Hi i have deepvariant 1.5.0 version singularity SIF file,. I ran successfully from command line for PacBio data, but when i ran the same through NextFlow as below. #nextflow.config. ```. singularity {. process.container = '/data/shared/clinical/LongRead/DeepVariant/deepvariant.simg'. cacheDir = ""/data/shared/clinical/LongRead/cache/"". singularity.enabled = true. singularity.autoMounts = true. SINGULARITY_BINDPATH = ""/data"". }. conda. {. enabled = true. cacheDir = ""/data/shared/clinical/LongRead/Programs/"". }. params. {. path=""/data/shared/clinical/LongRead/"". ref_genome=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"". pbhg38_tdx=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.mmi"". at=18. st=6. data_input=""/data/shared/clinical/LongRead/Data/"". }. ```. #deepvariant.nf. ```. process pbc_varicall {. publishDir ""/data/shared/clinical/LongRead/Data/resources/"". container 'docker://google/deepvariant:1.5.0'. input:. path 'fa'. output:. file ""*"". path 'm84011_220902_175841_NF_sif.vcf.gz'. script:. """""". run_deepvariant --model_type PACBIO --ref ${params.ref_genome} --reads ${params.data_input}/m84011_220902_175841_Aln.bam --output_vcf ${params.data_input}/Analysis/out_m84011_220902_175841_NF_sif.vcf.gz --num_shards 40. """""". }. workflow {. fa=channel.fromPath(""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta""). pbc_varicall(fa). }. ```. after running for several hours i do not get any output, instead during run , i get msg as. `TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: `. this is during `make_examples` step",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:1385,testability,resourc,resources,1385,"Error in pbc_varicall; Hi i have deepvariant 1.5.0 version singularity SIF file,. I ran successfully from command line for PacBio data, but when i ran the same through NextFlow as below. #nextflow.config. ```. singularity {. process.container = '/data/shared/clinical/LongRead/DeepVariant/deepvariant.simg'. cacheDir = ""/data/shared/clinical/LongRead/cache/"". singularity.enabled = true. singularity.autoMounts = true. SINGULARITY_BINDPATH = ""/data"". }. conda. {. enabled = true. cacheDir = ""/data/shared/clinical/LongRead/Programs/"". }. params. {. path=""/data/shared/clinical/LongRead/"". ref_genome=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"". pbhg38_tdx=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.mmi"". at=18. st=6. data_input=""/data/shared/clinical/LongRead/Data/"". }. ```. #deepvariant.nf. ```. process pbc_varicall {. publishDir ""/data/shared/clinical/LongRead/Data/resources/"". container 'docker://google/deepvariant:1.5.0'. input:. path 'fa'. output:. file ""*"". path 'm84011_220902_175841_NF_sif.vcf.gz'. script:. """""". run_deepvariant --model_type PACBIO --ref ${params.ref_genome} --reads ${params.data_input}/m84011_220902_175841_Aln.bam --output_vcf ${params.data_input}/Analysis/out_m84011_220902_175841_NF_sif.vcf.gz --num_shards 40. """""". }. workflow {. fa=channel.fromPath(""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta""). pbc_varicall(fa). }. ```. after running for several hours i do not get any output, instead during run , i get msg as. `TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: `. this is during `make_examples` step",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:0,usability,Error,Error,0,"Error in pbc_varicall; Hi i have deepvariant 1.5.0 version singularity SIF file,. I ran successfully from command line for PacBio data, but when i ran the same through NextFlow as below. #nextflow.config. ```. singularity {. process.container = '/data/shared/clinical/LongRead/DeepVariant/deepvariant.simg'. cacheDir = ""/data/shared/clinical/LongRead/cache/"". singularity.enabled = true. singularity.autoMounts = true. SINGULARITY_BINDPATH = ""/data"". }. conda. {. enabled = true. cacheDir = ""/data/shared/clinical/LongRead/Programs/"". }. params. {. path=""/data/shared/clinical/LongRead/"". ref_genome=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"". pbhg38_tdx=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.mmi"". at=18. st=6. data_input=""/data/shared/clinical/LongRead/Data/"". }. ```. #deepvariant.nf. ```. process pbc_varicall {. publishDir ""/data/shared/clinical/LongRead/Data/resources/"". container 'docker://google/deepvariant:1.5.0'. input:. path 'fa'. output:. file ""*"". path 'm84011_220902_175841_NF_sif.vcf.gz'. script:. """""". run_deepvariant --model_type PACBIO --ref ${params.ref_genome} --reads ${params.data_input}/m84011_220902_175841_Aln.bam --output_vcf ${params.data_input}/Analysis/out_m84011_220902_175841_NF_sif.vcf.gz --num_shards 40. """""". }. workflow {. fa=channel.fromPath(""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta""). pbc_varicall(fa). }. ```. after running for several hours i do not get any output, instead during run , i get msg as. `TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: `. this is during `make_examples` step",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:106,usability,command,command,106,"Error in pbc_varicall; Hi i have deepvariant 1.5.0 version singularity SIF file,. I ran successfully from command line for PacBio data, but when i ran the same through NextFlow as below. #nextflow.config. ```. singularity {. process.container = '/data/shared/clinical/LongRead/DeepVariant/deepvariant.simg'. cacheDir = ""/data/shared/clinical/LongRead/cache/"". singularity.enabled = true. singularity.autoMounts = true. SINGULARITY_BINDPATH = ""/data"". }. conda. {. enabled = true. cacheDir = ""/data/shared/clinical/LongRead/Programs/"". }. params. {. path=""/data/shared/clinical/LongRead/"". ref_genome=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"". pbhg38_tdx=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.mmi"". at=18. st=6. data_input=""/data/shared/clinical/LongRead/Data/"". }. ```. #deepvariant.nf. ```. process pbc_varicall {. publishDir ""/data/shared/clinical/LongRead/Data/resources/"". container 'docker://google/deepvariant:1.5.0'. input:. path 'fa'. output:. file ""*"". path 'm84011_220902_175841_NF_sif.vcf.gz'. script:. """""". run_deepvariant --model_type PACBIO --ref ${params.ref_genome} --reads ${params.data_input}/m84011_220902_175841_Aln.bam --output_vcf ${params.data_input}/Analysis/out_m84011_220902_175841_NF_sif.vcf.gz --num_shards 40. """""". }. workflow {. fa=channel.fromPath(""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta""). pbc_varicall(fa). }. ```. after running for several hours i do not get any output, instead during run , i get msg as. `TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: `. this is during `make_examples` step",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:993,usability,input,input,993,"Error in pbc_varicall; Hi i have deepvariant 1.5.0 version singularity SIF file,. I ran successfully from command line for PacBio data, but when i ran the same through NextFlow as below. #nextflow.config. ```. singularity {. process.container = '/data/shared/clinical/LongRead/DeepVariant/deepvariant.simg'. cacheDir = ""/data/shared/clinical/LongRead/cache/"". singularity.enabled = true. singularity.autoMounts = true. SINGULARITY_BINDPATH = ""/data"". }. conda. {. enabled = true. cacheDir = ""/data/shared/clinical/LongRead/Programs/"". }. params. {. path=""/data/shared/clinical/LongRead/"". ref_genome=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"". pbhg38_tdx=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.mmi"". at=18. st=6. data_input=""/data/shared/clinical/LongRead/Data/"". }. ```. #deepvariant.nf. ```. process pbc_varicall {. publishDir ""/data/shared/clinical/LongRead/Data/resources/"". container 'docker://google/deepvariant:1.5.0'. input:. path 'fa'. output:. file ""*"". path 'm84011_220902_175841_NF_sif.vcf.gz'. script:. """""". run_deepvariant --model_type PACBIO --ref ${params.ref_genome} --reads ${params.data_input}/m84011_220902_175841_Aln.bam --output_vcf ${params.data_input}/Analysis/out_m84011_220902_175841_NF_sif.vcf.gz --num_shards 40. """""". }. workflow {. fa=channel.fromPath(""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta""). pbc_varicall(fa). }. ```. after running for several hours i do not get any output, instead during run , i get msg as. `TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: `. this is during `make_examples` step",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:1316,usability,workflow,workflow,1316,"Error in pbc_varicall; Hi i have deepvariant 1.5.0 version singularity SIF file,. I ran successfully from command line for PacBio data, but when i ran the same through NextFlow as below. #nextflow.config. ```. singularity {. process.container = '/data/shared/clinical/LongRead/DeepVariant/deepvariant.simg'. cacheDir = ""/data/shared/clinical/LongRead/cache/"". singularity.enabled = true. singularity.autoMounts = true. SINGULARITY_BINDPATH = ""/data"". }. conda. {. enabled = true. cacheDir = ""/data/shared/clinical/LongRead/Programs/"". }. params. {. path=""/data/shared/clinical/LongRead/"". ref_genome=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"". pbhg38_tdx=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.mmi"". at=18. st=6. data_input=""/data/shared/clinical/LongRead/Data/"". }. ```. #deepvariant.nf. ```. process pbc_varicall {. publishDir ""/data/shared/clinical/LongRead/Data/resources/"". container 'docker://google/deepvariant:1.5.0'. input:. path 'fa'. output:. file ""*"". path 'm84011_220902_175841_NF_sif.vcf.gz'. script:. """""". run_deepvariant --model_type PACBIO --ref ${params.ref_genome} --reads ${params.data_input}/m84011_220902_175841_Aln.bam --output_vcf ${params.data_input}/Analysis/out_m84011_220902_175841_NF_sif.vcf.gz --num_shards 40. """""". }. workflow {. fa=channel.fromPath(""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta""). pbc_varicall(fa). }. ```. after running for several hours i do not get any output, instead during run , i get msg as. `TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: `. this is during `make_examples` step",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:1590,usability,statu,status,1590,"Error in pbc_varicall; Hi i have deepvariant 1.5.0 version singularity SIF file,. I ran successfully from command line for PacBio data, but when i ran the same through NextFlow as below. #nextflow.config. ```. singularity {. process.container = '/data/shared/clinical/LongRead/DeepVariant/deepvariant.simg'. cacheDir = ""/data/shared/clinical/LongRead/cache/"". singularity.enabled = true. singularity.autoMounts = true. SINGULARITY_BINDPATH = ""/data"". }. conda. {. enabled = true. cacheDir = ""/data/shared/clinical/LongRead/Programs/"". }. params. {. path=""/data/shared/clinical/LongRead/"". ref_genome=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"". pbhg38_tdx=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.mmi"". at=18. st=6. data_input=""/data/shared/clinical/LongRead/Data/"". }. ```. #deepvariant.nf. ```. process pbc_varicall {. publishDir ""/data/shared/clinical/LongRead/Data/resources/"". container 'docker://google/deepvariant:1.5.0'. input:. path 'fa'. output:. file ""*"". path 'm84011_220902_175841_NF_sif.vcf.gz'. script:. """""". run_deepvariant --model_type PACBIO --ref ${params.ref_genome} --reads ${params.data_input}/m84011_220902_175841_Aln.bam --output_vcf ${params.data_input}/Analysis/out_m84011_220902_175841_NF_sif.vcf.gz --num_shards 40. """""". }. workflow {. fa=channel.fromPath(""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta""). pbc_varicall(fa). }. ```. after running for several hours i do not get any output, instead during run , i get msg as. `TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: `. this is during `make_examples` step",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:1616,usability,error,error,1616,"Error in pbc_varicall; Hi i have deepvariant 1.5.0 version singularity SIF file,. I ran successfully from command line for PacBio data, but when i ran the same through NextFlow as below. #nextflow.config. ```. singularity {. process.container = '/data/shared/clinical/LongRead/DeepVariant/deepvariant.simg'. cacheDir = ""/data/shared/clinical/LongRead/cache/"". singularity.enabled = true. singularity.autoMounts = true. SINGULARITY_BINDPATH = ""/data"". }. conda. {. enabled = true. cacheDir = ""/data/shared/clinical/LongRead/Programs/"". }. params. {. path=""/data/shared/clinical/LongRead/"". ref_genome=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"". pbhg38_tdx=""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.mmi"". at=18. st=6. data_input=""/data/shared/clinical/LongRead/Data/"". }. ```. #deepvariant.nf. ```. process pbc_varicall {. publishDir ""/data/shared/clinical/LongRead/Data/resources/"". container 'docker://google/deepvariant:1.5.0'. input:. path 'fa'. output:. file ""*"". path 'm84011_220902_175841_NF_sif.vcf.gz'. script:. """""". run_deepvariant --model_type PACBIO --ref ${params.ref_genome} --reads ${params.data_input}/m84011_220902_175841_Aln.bam --output_vcf ${params.data_input}/Analysis/out_m84011_220902_175841_NF_sif.vcf.gz --num_shards 40. """""". }. workflow {. fa=channel.fromPath(""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta""). pbc_varicall(fa). }. ```. after running for several hours i do not get any output, instead during run , i get msg as. `TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: `. this is during `make_examples` step",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/660:264,availability,consist,consistently,264,"Maybe false positive calls in deepvariant analysis of HiFi reads; . Hi DeepVariant team,. I have been using DeepVariant （v1.1.0）to call small variants for a human genome with approximately 50X coverage of HiFi reads. During my analysis, I noticed that DeepVariant consistently reports some false positives in some regions. For example, at position chr10:89013075-89013077, there is a SNV (see the IGV snapshot of both Illumina and HiFi reads), but DeepVariant identifies three variants at that location. The vcf records by deepvariant:. ```. chr10 89013075 . T TC 31.7 PASS . GT:GQ:DP:AD:VAF:PL 0/1:31:63:35,28:0.444444:31,0,43. chr10 89013076 . CA C 28.2 PASS . GT:GQ:DP:AD:VAF:PL 0/1:27:63:35,28:0.444444:28,0,34. chr10 89013077 . A C 30.6 PASS . GT:GQ:DP:AD:VAF:PL 0/1:18:34:0,34:1:30,0,17. ```. The IGV snapshots:. ![image](https://github.com/google/deepvariant/assets/44404441/9618a4b9-7594-42e0-bb65-173761f803a8). I haven't identified any patterns in these regions. Have you encountered similar situations before? Could you please explain the possible reasons behind these false positive calls? I have provided three examples, including small BAM files, resulting VCF files, and my code, for you to replicate and investigate. . [deepvariant_fp.zip](https://github.com/google/deepvariant/files/11735375/deepvariant_fp.zip). Other information of my test: . - Operating system: Red Hat 4.8.5-36. - DeepVariant version: 1.1.0. - Installation method (Docker, built from source, etc.): Docker. - reference : GRCh38_full_analysis_set_plus_decoy_hla.fa. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). ~50x Human HiFi seuquencing data. Thank you very much for your attention and support. I look forward to your response. Best regards,. Peng Jia.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:1206,availability,replic,replicate,1206,"Maybe false positive calls in deepvariant analysis of HiFi reads; . Hi DeepVariant team,. I have been using DeepVariant （v1.1.0）to call small variants for a human genome with approximately 50X coverage of HiFi reads. During my analysis, I noticed that DeepVariant consistently reports some false positives in some regions. For example, at position chr10:89013075-89013077, there is a SNV (see the IGV snapshot of both Illumina and HiFi reads), but DeepVariant identifies three variants at that location. The vcf records by deepvariant:. ```. chr10 89013075 . T TC 31.7 PASS . GT:GQ:DP:AD:VAF:PL 0/1:31:63:35,28:0.444444:31,0,43. chr10 89013076 . CA C 28.2 PASS . GT:GQ:DP:AD:VAF:PL 0/1:27:63:35,28:0.444444:28,0,34. chr10 89013077 . A C 30.6 PASS . GT:GQ:DP:AD:VAF:PL 0/1:18:34:0,34:1:30,0,17. ```. The IGV snapshots:. ![image](https://github.com/google/deepvariant/assets/44404441/9618a4b9-7594-42e0-bb65-173761f803a8). I haven't identified any patterns in these regions. Have you encountered similar situations before? Could you please explain the possible reasons behind these false positive calls? I have provided three examples, including small BAM files, resulting VCF files, and my code, for you to replicate and investigate. . [deepvariant_fp.zip](https://github.com/google/deepvariant/files/11735375/deepvariant_fp.zip). Other information of my test: . - Operating system: Red Hat 4.8.5-36. - DeepVariant version: 1.1.0. - Installation method (Docker, built from source, etc.): Docker. - reference : GRCh38_full_analysis_set_plus_decoy_hla.fa. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). ~50x Human HiFi seuquencing data. Thank you very much for your attention and support. I look forward to your response. Best regards,. Peng Jia.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:1364,availability,Operat,Operating,1364,"Maybe false positive calls in deepvariant analysis of HiFi reads; . Hi DeepVariant team,. I have been using DeepVariant （v1.1.0）to call small variants for a human genome with approximately 50X coverage of HiFi reads. During my analysis, I noticed that DeepVariant consistently reports some false positives in some regions. For example, at position chr10:89013075-89013077, there is a SNV (see the IGV snapshot of both Illumina and HiFi reads), but DeepVariant identifies three variants at that location. The vcf records by deepvariant:. ```. chr10 89013075 . T TC 31.7 PASS . GT:GQ:DP:AD:VAF:PL 0/1:31:63:35,28:0.444444:31,0,43. chr10 89013076 . CA C 28.2 PASS . GT:GQ:DP:AD:VAF:PL 0/1:27:63:35,28:0.444444:28,0,34. chr10 89013077 . A C 30.6 PASS . GT:GQ:DP:AD:VAF:PL 0/1:18:34:0,34:1:30,0,17. ```. The IGV snapshots:. ![image](https://github.com/google/deepvariant/assets/44404441/9618a4b9-7594-42e0-bb65-173761f803a8). I haven't identified any patterns in these regions. Have you encountered similar situations before? Could you please explain the possible reasons behind these false positive calls? I have provided three examples, including small BAM files, resulting VCF files, and my code, for you to replicate and investigate. . [deepvariant_fp.zip](https://github.com/google/deepvariant/files/11735375/deepvariant_fp.zip). Other information of my test: . - Operating system: Red Hat 4.8.5-36. - DeepVariant version: 1.1.0. - Installation method (Docker, built from source, etc.): Docker. - reference : GRCh38_full_analysis_set_plus_decoy_hla.fa. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). ~50x Human HiFi seuquencing data. Thank you very much for your attention and support. I look forward to your response. Best regards,. Peng Jia.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:1414,deployability,version,version,1414,"Maybe false positive calls in deepvariant analysis of HiFi reads; . Hi DeepVariant team,. I have been using DeepVariant （v1.1.0）to call small variants for a human genome with approximately 50X coverage of HiFi reads. During my analysis, I noticed that DeepVariant consistently reports some false positives in some regions. For example, at position chr10:89013075-89013077, there is a SNV (see the IGV snapshot of both Illumina and HiFi reads), but DeepVariant identifies three variants at that location. The vcf records by deepvariant:. ```. chr10 89013075 . T TC 31.7 PASS . GT:GQ:DP:AD:VAF:PL 0/1:31:63:35,28:0.444444:31,0,43. chr10 89013076 . CA C 28.2 PASS . GT:GQ:DP:AD:VAF:PL 0/1:27:63:35,28:0.444444:28,0,34. chr10 89013077 . A C 30.6 PASS . GT:GQ:DP:AD:VAF:PL 0/1:18:34:0,34:1:30,0,17. ```. The IGV snapshots:. ![image](https://github.com/google/deepvariant/assets/44404441/9618a4b9-7594-42e0-bb65-173761f803a8). I haven't identified any patterns in these regions. Have you encountered similar situations before? Could you please explain the possible reasons behind these false positive calls? I have provided three examples, including small BAM files, resulting VCF files, and my code, for you to replicate and investigate. . [deepvariant_fp.zip](https://github.com/google/deepvariant/files/11735375/deepvariant_fp.zip). Other information of my test: . - Operating system: Red Hat 4.8.5-36. - DeepVariant version: 1.1.0. - Installation method (Docker, built from source, etc.): Docker. - reference : GRCh38_full_analysis_set_plus_decoy_hla.fa. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). ~50x Human HiFi seuquencing data. Thank you very much for your attention and support. I look forward to your response. Best regards,. Peng Jia.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:1432,deployability,Instal,Installation,1432,"Maybe false positive calls in deepvariant analysis of HiFi reads; . Hi DeepVariant team,. I have been using DeepVariant （v1.1.0）to call small variants for a human genome with approximately 50X coverage of HiFi reads. During my analysis, I noticed that DeepVariant consistently reports some false positives in some regions. For example, at position chr10:89013075-89013077, there is a SNV (see the IGV snapshot of both Illumina and HiFi reads), but DeepVariant identifies three variants at that location. The vcf records by deepvariant:. ```. chr10 89013075 . T TC 31.7 PASS . GT:GQ:DP:AD:VAF:PL 0/1:31:63:35,28:0.444444:31,0,43. chr10 89013076 . CA C 28.2 PASS . GT:GQ:DP:AD:VAF:PL 0/1:27:63:35,28:0.444444:28,0,34. chr10 89013077 . A C 30.6 PASS . GT:GQ:DP:AD:VAF:PL 0/1:18:34:0,34:1:30,0,17. ```. The IGV snapshots:. ![image](https://github.com/google/deepvariant/assets/44404441/9618a4b9-7594-42e0-bb65-173761f803a8). I haven't identified any patterns in these regions. Have you encountered similar situations before? Could you please explain the possible reasons behind these false positive calls? I have provided three examples, including small BAM files, resulting VCF files, and my code, for you to replicate and investigate. . [deepvariant_fp.zip](https://github.com/google/deepvariant/files/11735375/deepvariant_fp.zip). Other information of my test: . - Operating system: Red Hat 4.8.5-36. - DeepVariant version: 1.1.0. - Installation method (Docker, built from source, etc.): Docker. - reference : GRCh38_full_analysis_set_plus_decoy_hla.fa. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). ~50x Human HiFi seuquencing data. Thank you very much for your attention and support. I look forward to your response. Best regards,. Peng Jia.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:1414,integrability,version,version,1414,"Maybe false positive calls in deepvariant analysis of HiFi reads; . Hi DeepVariant team,. I have been using DeepVariant （v1.1.0）to call small variants for a human genome with approximately 50X coverage of HiFi reads. During my analysis, I noticed that DeepVariant consistently reports some false positives in some regions. For example, at position chr10:89013075-89013077, there is a SNV (see the IGV snapshot of both Illumina and HiFi reads), but DeepVariant identifies three variants at that location. The vcf records by deepvariant:. ```. chr10 89013075 . T TC 31.7 PASS . GT:GQ:DP:AD:VAF:PL 0/1:31:63:35,28:0.444444:31,0,43. chr10 89013076 . CA C 28.2 PASS . GT:GQ:DP:AD:VAF:PL 0/1:27:63:35,28:0.444444:28,0,34. chr10 89013077 . A C 30.6 PASS . GT:GQ:DP:AD:VAF:PL 0/1:18:34:0,34:1:30,0,17. ```. The IGV snapshots:. ![image](https://github.com/google/deepvariant/assets/44404441/9618a4b9-7594-42e0-bb65-173761f803a8). I haven't identified any patterns in these regions. Have you encountered similar situations before? Could you please explain the possible reasons behind these false positive calls? I have provided three examples, including small BAM files, resulting VCF files, and my code, for you to replicate and investigate. . [deepvariant_fp.zip](https://github.com/google/deepvariant/files/11735375/deepvariant_fp.zip). Other information of my test: . - Operating system: Red Hat 4.8.5-36. - DeepVariant version: 1.1.0. - Installation method (Docker, built from source, etc.): Docker. - reference : GRCh38_full_analysis_set_plus_decoy_hla.fa. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). ~50x Human HiFi seuquencing data. Thank you very much for your attention and support. I look forward to your response. Best regards,. Peng Jia.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:1414,modifiability,version,version,1414,"Maybe false positive calls in deepvariant analysis of HiFi reads; . Hi DeepVariant team,. I have been using DeepVariant （v1.1.0）to call small variants for a human genome with approximately 50X coverage of HiFi reads. During my analysis, I noticed that DeepVariant consistently reports some false positives in some regions. For example, at position chr10:89013075-89013077, there is a SNV (see the IGV snapshot of both Illumina and HiFi reads), but DeepVariant identifies three variants at that location. The vcf records by deepvariant:. ```. chr10 89013075 . T TC 31.7 PASS . GT:GQ:DP:AD:VAF:PL 0/1:31:63:35,28:0.444444:31,0,43. chr10 89013076 . CA C 28.2 PASS . GT:GQ:DP:AD:VAF:PL 0/1:27:63:35,28:0.444444:28,0,34. chr10 89013077 . A C 30.6 PASS . GT:GQ:DP:AD:VAF:PL 0/1:18:34:0,34:1:30,0,17. ```. The IGV snapshots:. ![image](https://github.com/google/deepvariant/assets/44404441/9618a4b9-7594-42e0-bb65-173761f803a8). I haven't identified any patterns in these regions. Have you encountered similar situations before? Could you please explain the possible reasons behind these false positive calls? I have provided three examples, including small BAM files, resulting VCF files, and my code, for you to replicate and investigate. . [deepvariant_fp.zip](https://github.com/google/deepvariant/files/11735375/deepvariant_fp.zip). Other information of my test: . - Operating system: Red Hat 4.8.5-36. - DeepVariant version: 1.1.0. - Installation method (Docker, built from source, etc.): Docker. - reference : GRCh38_full_analysis_set_plus_decoy_hla.fa. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). ~50x Human HiFi seuquencing data. Thank you very much for your attention and support. I look forward to your response. Best regards,. Peng Jia.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:1354,safety,test,test,1354,"Maybe false positive calls in deepvariant analysis of HiFi reads; . Hi DeepVariant team,. I have been using DeepVariant （v1.1.0）to call small variants for a human genome with approximately 50X coverage of HiFi reads. During my analysis, I noticed that DeepVariant consistently reports some false positives in some regions. For example, at position chr10:89013075-89013077, there is a SNV (see the IGV snapshot of both Illumina and HiFi reads), but DeepVariant identifies three variants at that location. The vcf records by deepvariant:. ```. chr10 89013075 . T TC 31.7 PASS . GT:GQ:DP:AD:VAF:PL 0/1:31:63:35,28:0.444444:31,0,43. chr10 89013076 . CA C 28.2 PASS . GT:GQ:DP:AD:VAF:PL 0/1:27:63:35,28:0.444444:28,0,34. chr10 89013077 . A C 30.6 PASS . GT:GQ:DP:AD:VAF:PL 0/1:18:34:0,34:1:30,0,17. ```. The IGV snapshots:. ![image](https://github.com/google/deepvariant/assets/44404441/9618a4b9-7594-42e0-bb65-173761f803a8). I haven't identified any patterns in these regions. Have you encountered similar situations before? Could you please explain the possible reasons behind these false positive calls? I have provided three examples, including small BAM files, resulting VCF files, and my code, for you to replicate and investigate. . [deepvariant_fp.zip](https://github.com/google/deepvariant/files/11735375/deepvariant_fp.zip). Other information of my test: . - Operating system: Red Hat 4.8.5-36. - DeepVariant version: 1.1.0. - Installation method (Docker, built from source, etc.): Docker. - reference : GRCh38_full_analysis_set_plus_decoy_hla.fa. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). ~50x Human HiFi seuquencing data. Thank you very much for your attention and support. I look forward to your response. Best regards,. Peng Jia.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:83,security,team,team,83,"Maybe false positive calls in deepvariant analysis of HiFi reads; . Hi DeepVariant team,. I have been using DeepVariant （v1.1.0）to call small variants for a human genome with approximately 50X coverage of HiFi reads. During my analysis, I noticed that DeepVariant consistently reports some false positives in some regions. For example, at position chr10:89013075-89013077, there is a SNV (see the IGV snapshot of both Illumina and HiFi reads), but DeepVariant identifies three variants at that location. The vcf records by deepvariant:. ```. chr10 89013075 . T TC 31.7 PASS . GT:GQ:DP:AD:VAF:PL 0/1:31:63:35,28:0.444444:31,0,43. chr10 89013076 . CA C 28.2 PASS . GT:GQ:DP:AD:VAF:PL 0/1:27:63:35,28:0.444444:28,0,34. chr10 89013077 . A C 30.6 PASS . GT:GQ:DP:AD:VAF:PL 0/1:18:34:0,34:1:30,0,17. ```. The IGV snapshots:. ![image](https://github.com/google/deepvariant/assets/44404441/9618a4b9-7594-42e0-bb65-173761f803a8). I haven't identified any patterns in these regions. Have you encountered similar situations before? Could you please explain the possible reasons behind these false positive calls? I have provided three examples, including small BAM files, resulting VCF files, and my code, for you to replicate and investigate. . [deepvariant_fp.zip](https://github.com/google/deepvariant/files/11735375/deepvariant_fp.zip). Other information of my test: . - Operating system: Red Hat 4.8.5-36. - DeepVariant version: 1.1.0. - Installation method (Docker, built from source, etc.): Docker. - reference : GRCh38_full_analysis_set_plus_decoy_hla.fa. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). ~50x Human HiFi seuquencing data. Thank you very much for your attention and support. I look forward to your response. Best regards,. Peng Jia.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:460,security,ident,identifies,460,"Maybe false positive calls in deepvariant analysis of HiFi reads; . Hi DeepVariant team,. I have been using DeepVariant （v1.1.0）to call small variants for a human genome with approximately 50X coverage of HiFi reads. During my analysis, I noticed that DeepVariant consistently reports some false positives in some regions. For example, at position chr10:89013075-89013077, there is a SNV (see the IGV snapshot of both Illumina and HiFi reads), but DeepVariant identifies three variants at that location. The vcf records by deepvariant:. ```. chr10 89013075 . T TC 31.7 PASS . GT:GQ:DP:AD:VAF:PL 0/1:31:63:35,28:0.444444:31,0,43. chr10 89013076 . CA C 28.2 PASS . GT:GQ:DP:AD:VAF:PL 0/1:27:63:35,28:0.444444:28,0,34. chr10 89013077 . A C 30.6 PASS . GT:GQ:DP:AD:VAF:PL 0/1:18:34:0,34:1:30,0,17. ```. The IGV snapshots:. ![image](https://github.com/google/deepvariant/assets/44404441/9618a4b9-7594-42e0-bb65-173761f803a8). I haven't identified any patterns in these regions. Have you encountered similar situations before? Could you please explain the possible reasons behind these false positive calls? I have provided three examples, including small BAM files, resulting VCF files, and my code, for you to replicate and investigate. . [deepvariant_fp.zip](https://github.com/google/deepvariant/files/11735375/deepvariant_fp.zip). Other information of my test: . - Operating system: Red Hat 4.8.5-36. - DeepVariant version: 1.1.0. - Installation method (Docker, built from source, etc.): Docker. - reference : GRCh38_full_analysis_set_plus_decoy_hla.fa. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). ~50x Human HiFi seuquencing data. Thank you very much for your attention and support. I look forward to your response. Best regards,. Peng Jia.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:931,security,ident,identified,931,"Maybe false positive calls in deepvariant analysis of HiFi reads; . Hi DeepVariant team,. I have been using DeepVariant （v1.1.0）to call small variants for a human genome with approximately 50X coverage of HiFi reads. During my analysis, I noticed that DeepVariant consistently reports some false positives in some regions. For example, at position chr10:89013075-89013077, there is a SNV (see the IGV snapshot of both Illumina and HiFi reads), but DeepVariant identifies three variants at that location. The vcf records by deepvariant:. ```. chr10 89013075 . T TC 31.7 PASS . GT:GQ:DP:AD:VAF:PL 0/1:31:63:35,28:0.444444:31,0,43. chr10 89013076 . CA C 28.2 PASS . GT:GQ:DP:AD:VAF:PL 0/1:27:63:35,28:0.444444:28,0,34. chr10 89013077 . A C 30.6 PASS . GT:GQ:DP:AD:VAF:PL 0/1:18:34:0,34:1:30,0,17. ```. The IGV snapshots:. ![image](https://github.com/google/deepvariant/assets/44404441/9618a4b9-7594-42e0-bb65-173761f803a8). I haven't identified any patterns in these regions. Have you encountered similar situations before? Could you please explain the possible reasons behind these false positive calls? I have provided three examples, including small BAM files, resulting VCF files, and my code, for you to replicate and investigate. . [deepvariant_fp.zip](https://github.com/google/deepvariant/files/11735375/deepvariant_fp.zip). Other information of my test: . - Operating system: Red Hat 4.8.5-36. - DeepVariant version: 1.1.0. - Installation method (Docker, built from source, etc.): Docker. - reference : GRCh38_full_analysis_set_plus_decoy_hla.fa. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). ~50x Human HiFi seuquencing data. Thank you very much for your attention and support. I look forward to your response. Best regards,. Peng Jia.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:193,testability,coverag,coverage,193,"Maybe false positive calls in deepvariant analysis of HiFi reads; . Hi DeepVariant team,. I have been using DeepVariant （v1.1.0）to call small variants for a human genome with approximately 50X coverage of HiFi reads. During my analysis, I noticed that DeepVariant consistently reports some false positives in some regions. For example, at position chr10:89013075-89013077, there is a SNV (see the IGV snapshot of both Illumina and HiFi reads), but DeepVariant identifies three variants at that location. The vcf records by deepvariant:. ```. chr10 89013075 . T TC 31.7 PASS . GT:GQ:DP:AD:VAF:PL 0/1:31:63:35,28:0.444444:31,0,43. chr10 89013076 . CA C 28.2 PASS . GT:GQ:DP:AD:VAF:PL 0/1:27:63:35,28:0.444444:28,0,34. chr10 89013077 . A C 30.6 PASS . GT:GQ:DP:AD:VAF:PL 0/1:18:34:0,34:1:30,0,17. ```. The IGV snapshots:. ![image](https://github.com/google/deepvariant/assets/44404441/9618a4b9-7594-42e0-bb65-173761f803a8). I haven't identified any patterns in these regions. Have you encountered similar situations before? Could you please explain the possible reasons behind these false positive calls? I have provided three examples, including small BAM files, resulting VCF files, and my code, for you to replicate and investigate. . [deepvariant_fp.zip](https://github.com/google/deepvariant/files/11735375/deepvariant_fp.zip). Other information of my test: . - Operating system: Red Hat 4.8.5-36. - DeepVariant version: 1.1.0. - Installation method (Docker, built from source, etc.): Docker. - reference : GRCh38_full_analysis_set_plus_decoy_hla.fa. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). ~50x Human HiFi seuquencing data. Thank you very much for your attention and support. I look forward to your response. Best regards,. Peng Jia.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:1354,testability,test,test,1354,"Maybe false positive calls in deepvariant analysis of HiFi reads; . Hi DeepVariant team,. I have been using DeepVariant （v1.1.0）to call small variants for a human genome with approximately 50X coverage of HiFi reads. During my analysis, I noticed that DeepVariant consistently reports some false positives in some regions. For example, at position chr10:89013075-89013077, there is a SNV (see the IGV snapshot of both Illumina and HiFi reads), but DeepVariant identifies three variants at that location. The vcf records by deepvariant:. ```. chr10 89013075 . T TC 31.7 PASS . GT:GQ:DP:AD:VAF:PL 0/1:31:63:35,28:0.444444:31,0,43. chr10 89013076 . CA C 28.2 PASS . GT:GQ:DP:AD:VAF:PL 0/1:27:63:35,28:0.444444:28,0,34. chr10 89013077 . A C 30.6 PASS . GT:GQ:DP:AD:VAF:PL 0/1:18:34:0,34:1:30,0,17. ```. The IGV snapshots:. ![image](https://github.com/google/deepvariant/assets/44404441/9618a4b9-7594-42e0-bb65-173761f803a8). I haven't identified any patterns in these regions. Have you encountered similar situations before? Could you please explain the possible reasons behind these false positive calls? I have provided three examples, including small BAM files, resulting VCF files, and my code, for you to replicate and investigate. . [deepvariant_fp.zip](https://github.com/google/deepvariant/files/11735375/deepvariant_fp.zip). Other information of my test: . - Operating system: Red Hat 4.8.5-36. - DeepVariant version: 1.1.0. - Installation method (Docker, built from source, etc.): Docker. - reference : GRCh38_full_analysis_set_plus_decoy_hla.fa. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). ~50x Human HiFi seuquencing data. Thank you very much for your attention and support. I look forward to your response. Best regards,. Peng Jia.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:1581,testability,instrument,instrument,1581,"Maybe false positive calls in deepvariant analysis of HiFi reads; . Hi DeepVariant team,. I have been using DeepVariant （v1.1.0）to call small variants for a human genome with approximately 50X coverage of HiFi reads. During my analysis, I noticed that DeepVariant consistently reports some false positives in some regions. For example, at position chr10:89013075-89013077, there is a SNV (see the IGV snapshot of both Illumina and HiFi reads), but DeepVariant identifies three variants at that location. The vcf records by deepvariant:. ```. chr10 89013075 . T TC 31.7 PASS . GT:GQ:DP:AD:VAF:PL 0/1:31:63:35,28:0.444444:31,0,43. chr10 89013076 . CA C 28.2 PASS . GT:GQ:DP:AD:VAF:PL 0/1:27:63:35,28:0.444444:28,0,34. chr10 89013077 . A C 30.6 PASS . GT:GQ:DP:AD:VAF:PL 0/1:18:34:0,34:1:30,0,17. ```. The IGV snapshots:. ![image](https://github.com/google/deepvariant/assets/44404441/9618a4b9-7594-42e0-bb65-173761f803a8). I haven't identified any patterns in these regions. Have you encountered similar situations before? Could you please explain the possible reasons behind these false positive calls? I have provided three examples, including small BAM files, resulting VCF files, and my code, for you to replicate and investigate. . [deepvariant_fp.zip](https://github.com/google/deepvariant/files/11735375/deepvariant_fp.zip). Other information of my test: . - Operating system: Red Hat 4.8.5-36. - DeepVariant version: 1.1.0. - Installation method (Docker, built from source, etc.): Docker. - reference : GRCh38_full_analysis_set_plus_decoy_hla.fa. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). ~50x Human HiFi seuquencing data. Thank you very much for your attention and support. I look forward to your response. Best regards,. Peng Jia.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:264,usability,consist,consistently,264,"Maybe false positive calls in deepvariant analysis of HiFi reads; . Hi DeepVariant team,. I have been using DeepVariant （v1.1.0）to call small variants for a human genome with approximately 50X coverage of HiFi reads. During my analysis, I noticed that DeepVariant consistently reports some false positives in some regions. For example, at position chr10:89013075-89013077, there is a SNV (see the IGV snapshot of both Illumina and HiFi reads), but DeepVariant identifies three variants at that location. The vcf records by deepvariant:. ```. chr10 89013075 . T TC 31.7 PASS . GT:GQ:DP:AD:VAF:PL 0/1:31:63:35,28:0.444444:31,0,43. chr10 89013076 . CA C 28.2 PASS . GT:GQ:DP:AD:VAF:PL 0/1:27:63:35,28:0.444444:28,0,34. chr10 89013077 . A C 30.6 PASS . GT:GQ:DP:AD:VAF:PL 0/1:18:34:0,34:1:30,0,17. ```. The IGV snapshots:. ![image](https://github.com/google/deepvariant/assets/44404441/9618a4b9-7594-42e0-bb65-173761f803a8). I haven't identified any patterns in these regions. Have you encountered similar situations before? Could you please explain the possible reasons behind these false positive calls? I have provided three examples, including small BAM files, resulting VCF files, and my code, for you to replicate and investigate. . [deepvariant_fp.zip](https://github.com/google/deepvariant/files/11735375/deepvariant_fp.zip). Other information of my test: . - Operating system: Red Hat 4.8.5-36. - DeepVariant version: 1.1.0. - Installation method (Docker, built from source, etc.): Docker. - reference : GRCh38_full_analysis_set_plus_decoy_hla.fa. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). ~50x Human HiFi seuquencing data. Thank you very much for your attention and support. I look forward to your response. Best regards,. Peng Jia.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:1740,usability,support,support,1740,"Maybe false positive calls in deepvariant analysis of HiFi reads; . Hi DeepVariant team,. I have been using DeepVariant （v1.1.0）to call small variants for a human genome with approximately 50X coverage of HiFi reads. During my analysis, I noticed that DeepVariant consistently reports some false positives in some regions. For example, at position chr10:89013075-89013077, there is a SNV (see the IGV snapshot of both Illumina and HiFi reads), but DeepVariant identifies three variants at that location. The vcf records by deepvariant:. ```. chr10 89013075 . T TC 31.7 PASS . GT:GQ:DP:AD:VAF:PL 0/1:31:63:35,28:0.444444:31,0,43. chr10 89013076 . CA C 28.2 PASS . GT:GQ:DP:AD:VAF:PL 0/1:27:63:35,28:0.444444:28,0,34. chr10 89013077 . A C 30.6 PASS . GT:GQ:DP:AD:VAF:PL 0/1:18:34:0,34:1:30,0,17. ```. The IGV snapshots:. ![image](https://github.com/google/deepvariant/assets/44404441/9618a4b9-7594-42e0-bb65-173761f803a8). I haven't identified any patterns in these regions. Have you encountered similar situations before? Could you please explain the possible reasons behind these false positive calls? I have provided three examples, including small BAM files, resulting VCF files, and my code, for you to replicate and investigate. . [deepvariant_fp.zip](https://github.com/google/deepvariant/files/11735375/deepvariant_fp.zip). Other information of my test: . - Operating system: Red Hat 4.8.5-36. - DeepVariant version: 1.1.0. - Installation method (Docker, built from source, etc.): Docker. - reference : GRCh38_full_analysis_set_plus_decoy_hla.fa. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). ~50x Human HiFi seuquencing data. Thank you very much for your attention and support. I look forward to your response. Best regards,. Peng Jia.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/661:198,deployability,releas,release,198,"Any prospect about non-human models?; Hello, . I was wondering ... I know you have a mosquito model that seems to work for when there is a high heterozygosity, but I was wondering if you planned to release those mdels... We, who work with non-model organisms, would be really happy to use DeepVariant. . Sorry if this is out of place here, I didn't really know where to ask. Feel free to lock . Cheers. Alex",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:29,energy efficiency,model,models,29,"Any prospect about non-human models?; Hello, . I was wondering ... I know you have a mosquito model that seems to work for when there is a high heterozygosity, but I was wondering if you planned to release those mdels... We, who work with non-model organisms, would be really happy to use DeepVariant. . Sorry if this is out of place here, I didn't really know where to ask. Feel free to lock . Cheers. Alex",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:94,energy efficiency,model,model,94,"Any prospect about non-human models?; Hello, . I was wondering ... I know you have a mosquito model that seems to work for when there is a high heterozygosity, but I was wondering if you planned to release those mdels... We, who work with non-model organisms, would be really happy to use DeepVariant. . Sorry if this is out of place here, I didn't really know where to ask. Feel free to lock . Cheers. Alex",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:243,energy efficiency,model,model,243,"Any prospect about non-human models?; Hello, . I was wondering ... I know you have a mosquito model that seems to work for when there is a high heterozygosity, but I was wondering if you planned to release those mdels... We, who work with non-model organisms, would be really happy to use DeepVariant. . Sorry if this is out of place here, I didn't really know where to ask. Feel free to lock . Cheers. Alex",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:388,performance,lock,lock,388,"Any prospect about non-human models?; Hello, . I was wondering ... I know you have a mosquito model that seems to work for when there is a high heterozygosity, but I was wondering if you planned to release those mdels... We, who work with non-model organisms, would be really happy to use DeepVariant. . Sorry if this is out of place here, I didn't really know where to ask. Feel free to lock . Cheers. Alex",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:29,security,model,models,29,"Any prospect about non-human models?; Hello, . I was wondering ... I know you have a mosquito model that seems to work for when there is a high heterozygosity, but I was wondering if you planned to release those mdels... We, who work with non-model organisms, would be really happy to use DeepVariant. . Sorry if this is out of place here, I didn't really know where to ask. Feel free to lock . Cheers. Alex",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:94,security,model,model,94,"Any prospect about non-human models?; Hello, . I was wondering ... I know you have a mosquito model that seems to work for when there is a high heterozygosity, but I was wondering if you planned to release those mdels... We, who work with non-model organisms, would be really happy to use DeepVariant. . Sorry if this is out of place here, I didn't really know where to ask. Feel free to lock . Cheers. Alex",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:243,security,model,model,243,"Any prospect about non-human models?; Hello, . I was wondering ... I know you have a mosquito model that seems to work for when there is a high heterozygosity, but I was wondering if you planned to release those mdels... We, who work with non-model organisms, would be really happy to use DeepVariant. . Sorry if this is out of place here, I didn't really know where to ask. Feel free to lock . Cheers. Alex",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:388,security,lock,lock,388,"Any prospect about non-human models?; Hello, . I was wondering ... I know you have a mosquito model that seems to work for when there is a high heterozygosity, but I was wondering if you planned to release those mdels... We, who work with non-model organisms, would be really happy to use DeepVariant. . Sorry if this is out of place here, I didn't really know where to ask. Feel free to lock . Cheers. Alex",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:187,testability,plan,planned,187,"Any prospect about non-human models?; Hello, . I was wondering ... I know you have a mosquito model that seems to work for when there is a high heterozygosity, but I was wondering if you planned to release those mdels... We, who work with non-model organisms, would be really happy to use DeepVariant. . Sorry if this is out of place here, I didn't really know where to ask. Feel free to lock . Cheers. Alex",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/664:0,availability,Error,Error,0,"Error running bioconda installed version; . Installed DeepVariant 1.5.0 from bioconda using micromamba. Running make examples command produces the following error:. ```. micromamba run --name dv /opt/conda/envs/dv/bin/python /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. Traceback (most recent call last):. File ""/opt/conda/envs/dv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main. ""__main__"", mod_spec). File ""/opt/conda/envs/dv/lib/python3.6/runpy.py"", line 85, in _run_code. exec(code, run_globals). File ""/opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip/__main__.py"", line 392, in <module>. File ""/opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip/__main__.py"", line 365, in Main. File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 287, in call. with Popen(*popenargs, **kwargs) as p:. File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 729, in __init__. restore_signals, start_new_session). File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 1364, in _execute_child. raise child_exception_type(errno_num, err_msg, err_filename). FileNotFoundError: [Errno 2] No such file or directory: '/usr/bin/python3': '/usr/bin/python3'. ```. I noticed inside the bazel .zip files the python binary is hard-coded:. /share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip Line 60: `PYTHON_BINARY = '/usr/bin/python3'`. Is there any way to override this value to select a different python binary? The micromamba python binary is not in the standard location and there is also multiple python binaries in this system. DeepVarian",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:157,availability,error,error,157,"Error running bioconda installed version; . Installed DeepVariant 1.5.0 from bioconda using micromamba. Running make examples command produces the following error:. ```. micromamba run --name dv /opt/conda/envs/dv/bin/python /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. Traceback (most recent call last):. File ""/opt/conda/envs/dv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main. ""__main__"", mod_spec). File ""/opt/conda/envs/dv/lib/python3.6/runpy.py"", line 85, in _run_code. exec(code, run_globals). File ""/opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip/__main__.py"", line 392, in <module>. File ""/opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip/__main__.py"", line 365, in Main. File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 287, in call. with Popen(*popenargs, **kwargs) as p:. File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 729, in __init__. restore_signals, start_new_session). File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 1364, in _execute_child. raise child_exception_type(errno_num, err_msg, err_filename). FileNotFoundError: [Errno 2] No such file or directory: '/usr/bin/python3': '/usr/bin/python3'. ```. I noticed inside the bazel .zip files the python binary is hard-coded:. /share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip Line 60: `PYTHON_BINARY = '/usr/bin/python3'`. Is there any way to override this value to select a different python binary? The micromamba python binary is not in the standard location and there is also multiple python binaries in this system. DeepVarian",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:23,deployability,instal,installed,23,"Error running bioconda installed version; . Installed DeepVariant 1.5.0 from bioconda using micromamba. Running make examples command produces the following error:. ```. micromamba run --name dv /opt/conda/envs/dv/bin/python /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. Traceback (most recent call last):. File ""/opt/conda/envs/dv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main. ""__main__"", mod_spec). File ""/opt/conda/envs/dv/lib/python3.6/runpy.py"", line 85, in _run_code. exec(code, run_globals). File ""/opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip/__main__.py"", line 392, in <module>. File ""/opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip/__main__.py"", line 365, in Main. File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 287, in call. with Popen(*popenargs, **kwargs) as p:. File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 729, in __init__. restore_signals, start_new_session). File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 1364, in _execute_child. raise child_exception_type(errno_num, err_msg, err_filename). FileNotFoundError: [Errno 2] No such file or directory: '/usr/bin/python3': '/usr/bin/python3'. ```. I noticed inside the bazel .zip files the python binary is hard-coded:. /share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip Line 60: `PYTHON_BINARY = '/usr/bin/python3'`. Is there any way to override this value to select a different python binary? The micromamba python binary is not in the standard location and there is also multiple python binaries in this system. DeepVarian",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:33,deployability,version,version,33,"Error running bioconda installed version; . Installed DeepVariant 1.5.0 from bioconda using micromamba. Running make examples command produces the following error:. ```. micromamba run --name dv /opt/conda/envs/dv/bin/python /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. Traceback (most recent call last):. File ""/opt/conda/envs/dv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main. ""__main__"", mod_spec). File ""/opt/conda/envs/dv/lib/python3.6/runpy.py"", line 85, in _run_code. exec(code, run_globals). File ""/opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip/__main__.py"", line 392, in <module>. File ""/opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip/__main__.py"", line 365, in Main. File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 287, in call. with Popen(*popenargs, **kwargs) as p:. File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 729, in __init__. restore_signals, start_new_session). File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 1364, in _execute_child. raise child_exception_type(errno_num, err_msg, err_filename). FileNotFoundError: [Errno 2] No such file or directory: '/usr/bin/python3': '/usr/bin/python3'. ```. I noticed inside the bazel .zip files the python binary is hard-coded:. /share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip Line 60: `PYTHON_BINARY = '/usr/bin/python3'`. Is there any way to override this value to select a different python binary? The micromamba python binary is not in the standard location and there is also multiple python binaries in this system. DeepVarian",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:44,deployability,Instal,Installed,44,"Error running bioconda installed version; . Installed DeepVariant 1.5.0 from bioconda using micromamba. Running make examples command produces the following error:. ```. micromamba run --name dv /opt/conda/envs/dv/bin/python /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. Traceback (most recent call last):. File ""/opt/conda/envs/dv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main. ""__main__"", mod_spec). File ""/opt/conda/envs/dv/lib/python3.6/runpy.py"", line 85, in _run_code. exec(code, run_globals). File ""/opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip/__main__.py"", line 392, in <module>. File ""/opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip/__main__.py"", line 365, in Main. File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 287, in call. with Popen(*popenargs, **kwargs) as p:. File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 729, in __init__. restore_signals, start_new_session). File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 1364, in _execute_child. raise child_exception_type(errno_num, err_msg, err_filename). FileNotFoundError: [Errno 2] No such file or directory: '/usr/bin/python3': '/usr/bin/python3'. ```. I noticed inside the bazel .zip files the python binary is hard-coded:. /share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip Line 60: `PYTHON_BINARY = '/usr/bin/python3'`. Is there any way to override this value to select a different python binary? The micromamba python binary is not in the standard location and there is also multiple python binaries in this system. DeepVarian",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:951,deployability,modul,module,951,"Error running bioconda installed version; . Installed DeepVariant 1.5.0 from bioconda using micromamba. Running make examples command produces the following error:. ```. micromamba run --name dv /opt/conda/envs/dv/bin/python /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. Traceback (most recent call last):. File ""/opt/conda/envs/dv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main. ""__main__"", mod_spec). File ""/opt/conda/envs/dv/lib/python3.6/runpy.py"", line 85, in _run_code. exec(code, run_globals). File ""/opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip/__main__.py"", line 392, in <module>. File ""/opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip/__main__.py"", line 365, in Main. File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 287, in call. with Popen(*popenargs, **kwargs) as p:. File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 729, in __init__. restore_signals, start_new_session). File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 1364, in _execute_child. raise child_exception_type(errno_num, err_msg, err_filename). FileNotFoundError: [Errno 2] No such file or directory: '/usr/bin/python3': '/usr/bin/python3'. ```. I noticed inside the bazel .zip files the python binary is hard-coded:. /share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip Line 60: `PYTHON_BINARY = '/usr/bin/python3'`. Is there any way to override this value to select a different python binary? The micromamba python binary is not in the standard location and there is also multiple python binaries in this system. DeepVarian",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:2006,deployability,instal,installed,2006,"pVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. Traceback (most recent call last):. File ""/opt/conda/envs/dv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main. ""__main__"", mod_spec). File ""/opt/conda/envs/dv/lib/python3.6/runpy.py"", line 85, in _run_code. exec(code, run_globals). File ""/opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip/__main__.py"", line 392, in <module>. File ""/opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip/__main__.py"", line 365, in Main. File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 287, in call. with Popen(*popenargs, **kwargs) as p:. File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 729, in __init__. restore_signals, start_new_session). File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 1364, in _execute_child. raise child_exception_type(errno_num, err_msg, err_filename). FileNotFoundError: [Errno 2] No such file or directory: '/usr/bin/python3': '/usr/bin/python3'. ```. I noticed inside the bazel .zip files the python binary is hard-coded:. /share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip Line 60: `PYTHON_BINARY = '/usr/bin/python3'`. Is there any way to override this value to select a different python binary? The micromamba python binary is not in the standard location and there is also multiple python binaries in this system. DeepVariant was installed using micromamba:. ```. micromamba create -n dv && micromamba install -y -n dv -f /tmp/env_deepvariant.yaml. ``` . With environment file:. ```. name: dv. channels:. - conda-forge. - bioconda. - defaults. dependencies:. - python=3.6. - deepvariant=1.5.0. ```. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:2078,deployability,instal,install,2078,"pVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. Traceback (most recent call last):. File ""/opt/conda/envs/dv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main. ""__main__"", mod_spec). File ""/opt/conda/envs/dv/lib/python3.6/runpy.py"", line 85, in _run_code. exec(code, run_globals). File ""/opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip/__main__.py"", line 392, in <module>. File ""/opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip/__main__.py"", line 365, in Main. File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 287, in call. with Popen(*popenargs, **kwargs) as p:. File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 729, in __init__. restore_signals, start_new_session). File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 1364, in _execute_child. raise child_exception_type(errno_num, err_msg, err_filename). FileNotFoundError: [Errno 2] No such file or directory: '/usr/bin/python3': '/usr/bin/python3'. ```. I noticed inside the bazel .zip files the python binary is hard-coded:. /share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip Line 60: `PYTHON_BINARY = '/usr/bin/python3'`. Is there any way to override this value to select a different python binary? The micromamba python binary is not in the standard location and there is also multiple python binaries in this system. DeepVariant was installed using micromamba:. ```. micromamba create -n dv && micromamba install -y -n dv -f /tmp/env_deepvariant.yaml. ``` . With environment file:. ```. name: dv. channels:. - conda-forge. - bioconda. - defaults. dependencies:. - python=3.6. - deepvariant=1.5.0. ```. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:2220,deployability,depend,dependencies,2220,"pVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. Traceback (most recent call last):. File ""/opt/conda/envs/dv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main. ""__main__"", mod_spec). File ""/opt/conda/envs/dv/lib/python3.6/runpy.py"", line 85, in _run_code. exec(code, run_globals). File ""/opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip/__main__.py"", line 392, in <module>. File ""/opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip/__main__.py"", line 365, in Main. File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 287, in call. with Popen(*popenargs, **kwargs) as p:. File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 729, in __init__. restore_signals, start_new_session). File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 1364, in _execute_child. raise child_exception_type(errno_num, err_msg, err_filename). FileNotFoundError: [Errno 2] No such file or directory: '/usr/bin/python3': '/usr/bin/python3'. ```. I noticed inside the bazel .zip files the python binary is hard-coded:. /share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip Line 60: `PYTHON_BINARY = '/usr/bin/python3'`. Is there any way to override this value to select a different python binary? The micromamba python binary is not in the standard location and there is also multiple python binaries in this system. DeepVariant was installed using micromamba:. ```. micromamba create -n dv && micromamba install -y -n dv -f /tmp/env_deepvariant.yaml. ``` . With environment file:. ```. name: dv. channels:. - conda-forge. - bioconda. - defaults. dependencies:. - python=3.6. - deepvariant=1.5.0. ```. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:33,integrability,version,version,33,"Error running bioconda installed version; . Installed DeepVariant 1.5.0 from bioconda using micromamba. Running make examples command produces the following error:. ```. micromamba run --name dv /opt/conda/envs/dv/bin/python /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. Traceback (most recent call last):. File ""/opt/conda/envs/dv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main. ""__main__"", mod_spec). File ""/opt/conda/envs/dv/lib/python3.6/runpy.py"", line 85, in _run_code. exec(code, run_globals). File ""/opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip/__main__.py"", line 392, in <module>. File ""/opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip/__main__.py"", line 365, in Main. File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 287, in call. with Popen(*popenargs, **kwargs) as p:. File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 729, in __init__. restore_signals, start_new_session). File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 1364, in _execute_child. raise child_exception_type(errno_num, err_msg, err_filename). FileNotFoundError: [Errno 2] No such file or directory: '/usr/bin/python3': '/usr/bin/python3'. ```. I noticed inside the bazel .zip files the python binary is hard-coded:. /share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip Line 60: `PYTHON_BINARY = '/usr/bin/python3'`. Is there any way to override this value to select a different python binary? The micromamba python binary is not in the standard location and there is also multiple python binaries in this system. DeepVarian",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:1146,integrability,sub,subprocess,1146,"owing error:. ```. micromamba run --name dv /opt/conda/envs/dv/bin/python /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. Traceback (most recent call last):. File ""/opt/conda/envs/dv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main. ""__main__"", mod_spec). File ""/opt/conda/envs/dv/lib/python3.6/runpy.py"", line 85, in _run_code. exec(code, run_globals). File ""/opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip/__main__.py"", line 392, in <module>. File ""/opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip/__main__.py"", line 365, in Main. File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 287, in call. with Popen(*popenargs, **kwargs) as p:. File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 729, in __init__. restore_signals, start_new_session). File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 1364, in _execute_child. raise child_exception_type(errno_num, err_msg, err_filename). FileNotFoundError: [Errno 2] No such file or directory: '/usr/bin/python3': '/usr/bin/python3'. ```. I noticed inside the bazel .zip files the python binary is hard-coded:. /share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip Line 60: `PYTHON_BINARY = '/usr/bin/python3'`. Is there any way to override this value to select a different python binary? The micromamba python binary is not in the standard location and there is also multiple python binaries in this system. DeepVariant was installed using micromamba:. ```. micromamba create -n dv && micromamba install -y -n dv -f /tmp/env_deepvariant.yaml. ``` . With environment fil",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:1260,integrability,sub,subprocess,1260,".0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. Traceback (most recent call last):. File ""/opt/conda/envs/dv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main. ""__main__"", mod_spec). File ""/opt/conda/envs/dv/lib/python3.6/runpy.py"", line 85, in _run_code. exec(code, run_globals). File ""/opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip/__main__.py"", line 392, in <module>. File ""/opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip/__main__.py"", line 365, in Main. File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 287, in call. with Popen(*popenargs, **kwargs) as p:. File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 729, in __init__. restore_signals, start_new_session). File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 1364, in _execute_child. raise child_exception_type(errno_num, err_msg, err_filename). FileNotFoundError: [Errno 2] No such file or directory: '/usr/bin/python3': '/usr/bin/python3'. ```. I noticed inside the bazel .zip files the python binary is hard-coded:. /share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip Line 60: `PYTHON_BINARY = '/usr/bin/python3'`. Is there any way to override this value to select a different python binary? The micromamba python binary is not in the standard location and there is also multiple python binaries in this system. DeepVariant was installed using micromamba:. ```. micromamba create -n dv && micromamba install -y -n dv -f /tmp/env_deepvariant.yaml. ``` . With environment file:. ```. name: dv. channels:. - conda-forge. - bioconda. - defaults. dependencies:. - python=3.6. - deepvariant=1.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:1375,integrability,sub,subprocess,1375,"pVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. Traceback (most recent call last):. File ""/opt/conda/envs/dv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main. ""__main__"", mod_spec). File ""/opt/conda/envs/dv/lib/python3.6/runpy.py"", line 85, in _run_code. exec(code, run_globals). File ""/opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip/__main__.py"", line 392, in <module>. File ""/opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip/__main__.py"", line 365, in Main. File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 287, in call. with Popen(*popenargs, **kwargs) as p:. File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 729, in __init__. restore_signals, start_new_session). File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 1364, in _execute_child. raise child_exception_type(errno_num, err_msg, err_filename). FileNotFoundError: [Errno 2] No such file or directory: '/usr/bin/python3': '/usr/bin/python3'. ```. I noticed inside the bazel .zip files the python binary is hard-coded:. /share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip Line 60: `PYTHON_BINARY = '/usr/bin/python3'`. Is there any way to override this value to select a different python binary? The micromamba python binary is not in the standard location and there is also multiple python binaries in this system. DeepVariant was installed using micromamba:. ```. micromamba create -n dv && micromamba install -y -n dv -f /tmp/env_deepvariant.yaml. ``` . With environment file:. ```. name: dv. channels:. - conda-forge. - bioconda. - defaults. dependencies:. - python=3.6. - deepvariant=1.5.0. ```. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:2220,integrability,depend,dependencies,2220,"pVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. Traceback (most recent call last):. File ""/opt/conda/envs/dv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main. ""__main__"", mod_spec). File ""/opt/conda/envs/dv/lib/python3.6/runpy.py"", line 85, in _run_code. exec(code, run_globals). File ""/opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip/__main__.py"", line 392, in <module>. File ""/opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip/__main__.py"", line 365, in Main. File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 287, in call. with Popen(*popenargs, **kwargs) as p:. File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 729, in __init__. restore_signals, start_new_session). File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 1364, in _execute_child. raise child_exception_type(errno_num, err_msg, err_filename). FileNotFoundError: [Errno 2] No such file or directory: '/usr/bin/python3': '/usr/bin/python3'. ```. I noticed inside the bazel .zip files the python binary is hard-coded:. /share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip Line 60: `PYTHON_BINARY = '/usr/bin/python3'`. Is there any way to override this value to select a different python binary? The micromamba python binary is not in the standard location and there is also multiple python binaries in this system. DeepVariant was installed using micromamba:. ```. micromamba create -n dv && micromamba install -y -n dv -f /tmp/env_deepvariant.yaml. ``` . With environment file:. ```. name: dv. channels:. - conda-forge. - bioconda. - defaults. dependencies:. - python=3.6. - deepvariant=1.5.0. ```. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:244,interoperability,share,share,244,"Error running bioconda installed version; . Installed DeepVariant 1.5.0 from bioconda using micromamba. Running make examples command produces the following error:. ```. micromamba run --name dv /opt/conda/envs/dv/bin/python /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. Traceback (most recent call last):. File ""/opt/conda/envs/dv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main. ""__main__"", mod_spec). File ""/opt/conda/envs/dv/lib/python3.6/runpy.py"", line 85, in _run_code. exec(code, run_globals). File ""/opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip/__main__.py"", line 392, in <module>. File ""/opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip/__main__.py"", line 365, in Main. File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 287, in call. with Popen(*popenargs, **kwargs) as p:. File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 729, in __init__. restore_signals, start_new_session). File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 1364, in _execute_child. raise child_exception_type(errno_num, err_msg, err_filename). FileNotFoundError: [Errno 2] No such file or directory: '/usr/bin/python3': '/usr/bin/python3'. ```. I noticed inside the bazel .zip files the python binary is hard-coded:. /share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip Line 60: `PYTHON_BINARY = '/usr/bin/python3'`. Is there any way to override this value to select a different python binary? The micromamba python binary is not in the standard location and there is also multiple python binaries in this system. DeepVarian",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:834,interoperability,share,share,834,"Error running bioconda installed version; . Installed DeepVariant 1.5.0 from bioconda using micromamba. Running make examples command produces the following error:. ```. micromamba run --name dv /opt/conda/envs/dv/bin/python /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. Traceback (most recent call last):. File ""/opt/conda/envs/dv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main. ""__main__"", mod_spec). File ""/opt/conda/envs/dv/lib/python3.6/runpy.py"", line 85, in _run_code. exec(code, run_globals). File ""/opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip/__main__.py"", line 392, in <module>. File ""/opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip/__main__.py"", line 365, in Main. File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 287, in call. with Popen(*popenargs, **kwargs) as p:. File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 729, in __init__. restore_signals, start_new_session). File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 1364, in _execute_child. raise child_exception_type(errno_num, err_msg, err_filename). FileNotFoundError: [Errno 2] No such file or directory: '/usr/bin/python3': '/usr/bin/python3'. ```. I noticed inside the bazel .zip files the python binary is hard-coded:. /share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip Line 60: `PYTHON_BINARY = '/usr/bin/python3'`. Is there any way to override this value to select a different python binary? The micromamba python binary is not in the standard location and there is also multiple python binaries in this system. DeepVarian",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:985,interoperability,share,share,985,"Error running bioconda installed version; . Installed DeepVariant 1.5.0 from bioconda using micromamba. Running make examples command produces the following error:. ```. micromamba run --name dv /opt/conda/envs/dv/bin/python /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. Traceback (most recent call last):. File ""/opt/conda/envs/dv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main. ""__main__"", mod_spec). File ""/opt/conda/envs/dv/lib/python3.6/runpy.py"", line 85, in _run_code. exec(code, run_globals). File ""/opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip/__main__.py"", line 392, in <module>. File ""/opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip/__main__.py"", line 365, in Main. File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 287, in call. with Popen(*popenargs, **kwargs) as p:. File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 729, in __init__. restore_signals, start_new_session). File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 1364, in _execute_child. raise child_exception_type(errno_num, err_msg, err_filename). FileNotFoundError: [Errno 2] No such file or directory: '/usr/bin/python3': '/usr/bin/python3'. ```. I noticed inside the bazel .zip files the python binary is hard-coded:. /share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip Line 60: `PYTHON_BINARY = '/usr/bin/python3'`. Is there any way to override this value to select a different python binary? The micromamba python binary is not in the standard location and there is also multiple python binaries in this system. DeepVarian",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:1657,interoperability,share,share,1657,"pVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. Traceback (most recent call last):. File ""/opt/conda/envs/dv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main. ""__main__"", mod_spec). File ""/opt/conda/envs/dv/lib/python3.6/runpy.py"", line 85, in _run_code. exec(code, run_globals). File ""/opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip/__main__.py"", line 392, in <module>. File ""/opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip/__main__.py"", line 365, in Main. File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 287, in call. with Popen(*popenargs, **kwargs) as p:. File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 729, in __init__. restore_signals, start_new_session). File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 1364, in _execute_child. raise child_exception_type(errno_num, err_msg, err_filename). FileNotFoundError: [Errno 2] No such file or directory: '/usr/bin/python3': '/usr/bin/python3'. ```. I noticed inside the bazel .zip files the python binary is hard-coded:. /share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip Line 60: `PYTHON_BINARY = '/usr/bin/python3'`. Is there any way to override this value to select a different python binary? The micromamba python binary is not in the standard location and there is also multiple python binaries in this system. DeepVariant was installed using micromamba:. ```. micromamba create -n dv && micromamba install -y -n dv -f /tmp/env_deepvariant.yaml. ``` . With environment file:. ```. name: dv. channels:. - conda-forge. - bioconda. - defaults. dependencies:. - python=3.6. - deepvariant=1.5.0. ```. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:1913,interoperability,standard,standard,1913,"pVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. Traceback (most recent call last):. File ""/opt/conda/envs/dv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main. ""__main__"", mod_spec). File ""/opt/conda/envs/dv/lib/python3.6/runpy.py"", line 85, in _run_code. exec(code, run_globals). File ""/opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip/__main__.py"", line 392, in <module>. File ""/opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip/__main__.py"", line 365, in Main. File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 287, in call. with Popen(*popenargs, **kwargs) as p:. File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 729, in __init__. restore_signals, start_new_session). File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 1364, in _execute_child. raise child_exception_type(errno_num, err_msg, err_filename). FileNotFoundError: [Errno 2] No such file or directory: '/usr/bin/python3': '/usr/bin/python3'. ```. I noticed inside the bazel .zip files the python binary is hard-coded:. /share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip Line 60: `PYTHON_BINARY = '/usr/bin/python3'`. Is there any way to override this value to select a different python binary? The micromamba python binary is not in the standard location and there is also multiple python binaries in this system. DeepVariant was installed using micromamba:. ```. micromamba create -n dv && micromamba install -y -n dv -f /tmp/env_deepvariant.yaml. ``` . With environment file:. ```. name: dv. channels:. - conda-forge. - bioconda. - defaults. dependencies:. - python=3.6. - deepvariant=1.5.0. ```. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:33,modifiability,version,version,33,"Error running bioconda installed version; . Installed DeepVariant 1.5.0 from bioconda using micromamba. Running make examples command produces the following error:. ```. micromamba run --name dv /opt/conda/envs/dv/bin/python /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. Traceback (most recent call last):. File ""/opt/conda/envs/dv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main. ""__main__"", mod_spec). File ""/opt/conda/envs/dv/lib/python3.6/runpy.py"", line 85, in _run_code. exec(code, run_globals). File ""/opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip/__main__.py"", line 392, in <module>. File ""/opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip/__main__.py"", line 365, in Main. File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 287, in call. with Popen(*popenargs, **kwargs) as p:. File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 729, in __init__. restore_signals, start_new_session). File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 1364, in _execute_child. raise child_exception_type(errno_num, err_msg, err_filename). FileNotFoundError: [Errno 2] No such file or directory: '/usr/bin/python3': '/usr/bin/python3'. ```. I noticed inside the bazel .zip files the python binary is hard-coded:. /share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip Line 60: `PYTHON_BINARY = '/usr/bin/python3'`. Is there any way to override this value to select a different python binary? The micromamba python binary is not in the standard location and there is also multiple python binaries in this system. DeepVarian",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:951,modifiability,modul,module,951,"Error running bioconda installed version; . Installed DeepVariant 1.5.0 from bioconda using micromamba. Running make examples command produces the following error:. ```. micromamba run --name dv /opt/conda/envs/dv/bin/python /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. Traceback (most recent call last):. File ""/opt/conda/envs/dv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main. ""__main__"", mod_spec). File ""/opt/conda/envs/dv/lib/python3.6/runpy.py"", line 85, in _run_code. exec(code, run_globals). File ""/opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip/__main__.py"", line 392, in <module>. File ""/opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip/__main__.py"", line 365, in Main. File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 287, in call. with Popen(*popenargs, **kwargs) as p:. File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 729, in __init__. restore_signals, start_new_session). File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 1364, in _execute_child. raise child_exception_type(errno_num, err_msg, err_filename). FileNotFoundError: [Errno 2] No such file or directory: '/usr/bin/python3': '/usr/bin/python3'. ```. I noticed inside the bazel .zip files the python binary is hard-coded:. /share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip Line 60: `PYTHON_BINARY = '/usr/bin/python3'`. Is there any way to override this value to select a different python binary? The micromamba python binary is not in the standard location and there is also multiple python binaries in this system. DeepVarian",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:2220,modifiability,depend,dependencies,2220,"pVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. Traceback (most recent call last):. File ""/opt/conda/envs/dv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main. ""__main__"", mod_spec). File ""/opt/conda/envs/dv/lib/python3.6/runpy.py"", line 85, in _run_code. exec(code, run_globals). File ""/opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip/__main__.py"", line 392, in <module>. File ""/opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip/__main__.py"", line 365, in Main. File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 287, in call. with Popen(*popenargs, **kwargs) as p:. File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 729, in __init__. restore_signals, start_new_session). File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 1364, in _execute_child. raise child_exception_type(errno_num, err_msg, err_filename). FileNotFoundError: [Errno 2] No such file or directory: '/usr/bin/python3': '/usr/bin/python3'. ```. I noticed inside the bazel .zip files the python binary is hard-coded:. /share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip Line 60: `PYTHON_BINARY = '/usr/bin/python3'`. Is there any way to override this value to select a different python binary? The micromamba python binary is not in the standard location and there is also multiple python binaries in this system. DeepVariant was installed using micromamba:. ```. micromamba create -n dv && micromamba install -y -n dv -f /tmp/env_deepvariant.yaml. ``` . With environment file:. ```. name: dv. channels:. - conda-forge. - bioconda. - defaults. dependencies:. - python=3.6. - deepvariant=1.5.0. ```. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:0,performance,Error,Error,0,"Error running bioconda installed version; . Installed DeepVariant 1.5.0 from bioconda using micromamba. Running make examples command produces the following error:. ```. micromamba run --name dv /opt/conda/envs/dv/bin/python /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. Traceback (most recent call last):. File ""/opt/conda/envs/dv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main. ""__main__"", mod_spec). File ""/opt/conda/envs/dv/lib/python3.6/runpy.py"", line 85, in _run_code. exec(code, run_globals). File ""/opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip/__main__.py"", line 392, in <module>. File ""/opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip/__main__.py"", line 365, in Main. File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 287, in call. with Popen(*popenargs, **kwargs) as p:. File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 729, in __init__. restore_signals, start_new_session). File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 1364, in _execute_child. raise child_exception_type(errno_num, err_msg, err_filename). FileNotFoundError: [Errno 2] No such file or directory: '/usr/bin/python3': '/usr/bin/python3'. ```. I noticed inside the bazel .zip files the python binary is hard-coded:. /share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip Line 60: `PYTHON_BINARY = '/usr/bin/python3'`. Is there any way to override this value to select a different python binary? The micromamba python binary is not in the standard location and there is also multiple python binaries in this system. DeepVarian",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:157,performance,error,error,157,"Error running bioconda installed version; . Installed DeepVariant 1.5.0 from bioconda using micromamba. Running make examples command produces the following error:. ```. micromamba run --name dv /opt/conda/envs/dv/bin/python /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. Traceback (most recent call last):. File ""/opt/conda/envs/dv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main. ""__main__"", mod_spec). File ""/opt/conda/envs/dv/lib/python3.6/runpy.py"", line 85, in _run_code. exec(code, run_globals). File ""/opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip/__main__.py"", line 392, in <module>. File ""/opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip/__main__.py"", line 365, in Main. File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 287, in call. with Popen(*popenargs, **kwargs) as p:. File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 729, in __init__. restore_signals, start_new_session). File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 1364, in _execute_child. raise child_exception_type(errno_num, err_msg, err_filename). FileNotFoundError: [Errno 2] No such file or directory: '/usr/bin/python3': '/usr/bin/python3'. ```. I noticed inside the bazel .zip files the python binary is hard-coded:. /share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip Line 60: `PYTHON_BINARY = '/usr/bin/python3'`. Is there any way to override this value to select a different python binary? The micromamba python binary is not in the standard location and there is also multiple python binaries in this system. DeepVarian",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:0,safety,Error,Error,0,"Error running bioconda installed version; . Installed DeepVariant 1.5.0 from bioconda using micromamba. Running make examples command produces the following error:. ```. micromamba run --name dv /opt/conda/envs/dv/bin/python /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. Traceback (most recent call last):. File ""/opt/conda/envs/dv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main. ""__main__"", mod_spec). File ""/opt/conda/envs/dv/lib/python3.6/runpy.py"", line 85, in _run_code. exec(code, run_globals). File ""/opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip/__main__.py"", line 392, in <module>. File ""/opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip/__main__.py"", line 365, in Main. File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 287, in call. with Popen(*popenargs, **kwargs) as p:. File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 729, in __init__. restore_signals, start_new_session). File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 1364, in _execute_child. raise child_exception_type(errno_num, err_msg, err_filename). FileNotFoundError: [Errno 2] No such file or directory: '/usr/bin/python3': '/usr/bin/python3'. ```. I noticed inside the bazel .zip files the python binary is hard-coded:. /share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip Line 60: `PYTHON_BINARY = '/usr/bin/python3'`. Is there any way to override this value to select a different python binary? The micromamba python binary is not in the standard location and there is also multiple python binaries in this system. DeepVarian",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:157,safety,error,error,157,"Error running bioconda installed version; . Installed DeepVariant 1.5.0 from bioconda using micromamba. Running make examples command produces the following error:. ```. micromamba run --name dv /opt/conda/envs/dv/bin/python /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. Traceback (most recent call last):. File ""/opt/conda/envs/dv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main. ""__main__"", mod_spec). File ""/opt/conda/envs/dv/lib/python3.6/runpy.py"", line 85, in _run_code. exec(code, run_globals). File ""/opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip/__main__.py"", line 392, in <module>. File ""/opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip/__main__.py"", line 365, in Main. File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 287, in call. with Popen(*popenargs, **kwargs) as p:. File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 729, in __init__. restore_signals, start_new_session). File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 1364, in _execute_child. raise child_exception_type(errno_num, err_msg, err_filename). FileNotFoundError: [Errno 2] No such file or directory: '/usr/bin/python3': '/usr/bin/python3'. ```. I noticed inside the bazel .zip files the python binary is hard-coded:. /share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip Line 60: `PYTHON_BINARY = '/usr/bin/python3'`. Is there any way to override this value to select a different python binary? The micromamba python binary is not in the standard location and there is also multiple python binaries in this system. DeepVarian",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:951,safety,modul,module,951,"Error running bioconda installed version; . Installed DeepVariant 1.5.0 from bioconda using micromamba. Running make examples command produces the following error:. ```. micromamba run --name dv /opt/conda/envs/dv/bin/python /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. Traceback (most recent call last):. File ""/opt/conda/envs/dv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main. ""__main__"", mod_spec). File ""/opt/conda/envs/dv/lib/python3.6/runpy.py"", line 85, in _run_code. exec(code, run_globals). File ""/opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip/__main__.py"", line 392, in <module>. File ""/opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip/__main__.py"", line 365, in Main. File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 287, in call. with Popen(*popenargs, **kwargs) as p:. File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 729, in __init__. restore_signals, start_new_session). File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 1364, in _execute_child. raise child_exception_type(errno_num, err_msg, err_filename). FileNotFoundError: [Errno 2] No such file or directory: '/usr/bin/python3': '/usr/bin/python3'. ```. I noticed inside the bazel .zip files the python binary is hard-coded:. /share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip Line 60: `PYTHON_BINARY = '/usr/bin/python3'`. Is there any way to override this value to select a different python binary? The micromamba python binary is not in the standard location and there is also multiple python binaries in this system. DeepVarian",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:2220,safety,depend,dependencies,2220,"pVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. Traceback (most recent call last):. File ""/opt/conda/envs/dv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main. ""__main__"", mod_spec). File ""/opt/conda/envs/dv/lib/python3.6/runpy.py"", line 85, in _run_code. exec(code, run_globals). File ""/opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip/__main__.py"", line 392, in <module>. File ""/opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip/__main__.py"", line 365, in Main. File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 287, in call. with Popen(*popenargs, **kwargs) as p:. File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 729, in __init__. restore_signals, start_new_session). File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 1364, in _execute_child. raise child_exception_type(errno_num, err_msg, err_filename). FileNotFoundError: [Errno 2] No such file or directory: '/usr/bin/python3': '/usr/bin/python3'. ```. I noticed inside the bazel .zip files the python binary is hard-coded:. /share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip Line 60: `PYTHON_BINARY = '/usr/bin/python3'`. Is there any way to override this value to select a different python binary? The micromamba python binary is not in the standard location and there is also multiple python binaries in this system. DeepVariant was installed using micromamba:. ```. micromamba create -n dv && micromamba install -y -n dv -f /tmp/env_deepvariant.yaml. ``` . With environment file:. ```. name: dv. channels:. - conda-forge. - bioconda. - defaults. dependencies:. - python=3.6. - deepvariant=1.5.0. ```. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:568,testability,Trace,Traceback,568,"Error running bioconda installed version; . Installed DeepVariant 1.5.0 from bioconda using micromamba. Running make examples command produces the following error:. ```. micromamba run --name dv /opt/conda/envs/dv/bin/python /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. Traceback (most recent call last):. File ""/opt/conda/envs/dv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main. ""__main__"", mod_spec). File ""/opt/conda/envs/dv/lib/python3.6/runpy.py"", line 85, in _run_code. exec(code, run_globals). File ""/opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip/__main__.py"", line 392, in <module>. File ""/opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip/__main__.py"", line 365, in Main. File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 287, in call. with Popen(*popenargs, **kwargs) as p:. File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 729, in __init__. restore_signals, start_new_session). File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 1364, in _execute_child. raise child_exception_type(errno_num, err_msg, err_filename). FileNotFoundError: [Errno 2] No such file or directory: '/usr/bin/python3': '/usr/bin/python3'. ```. I noticed inside the bazel .zip files the python binary is hard-coded:. /share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip Line 60: `PYTHON_BINARY = '/usr/bin/python3'`. Is there any way to override this value to select a different python binary? The micromamba python binary is not in the standard location and there is also multiple python binaries in this system. DeepVarian",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:2220,testability,depend,dependencies,2220,"pVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. Traceback (most recent call last):. File ""/opt/conda/envs/dv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main. ""__main__"", mod_spec). File ""/opt/conda/envs/dv/lib/python3.6/runpy.py"", line 85, in _run_code. exec(code, run_globals). File ""/opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip/__main__.py"", line 392, in <module>. File ""/opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip/__main__.py"", line 365, in Main. File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 287, in call. with Popen(*popenargs, **kwargs) as p:. File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 729, in __init__. restore_signals, start_new_session). File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 1364, in _execute_child. raise child_exception_type(errno_num, err_msg, err_filename). FileNotFoundError: [Errno 2] No such file or directory: '/usr/bin/python3': '/usr/bin/python3'. ```. I noticed inside the bazel .zip files the python binary is hard-coded:. /share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip Line 60: `PYTHON_BINARY = '/usr/bin/python3'`. Is there any way to override this value to select a different python binary? The micromamba python binary is not in the standard location and there is also multiple python binaries in this system. DeepVariant was installed using micromamba:. ```. micromamba create -n dv && micromamba install -y -n dv -f /tmp/env_deepvariant.yaml. ``` . With environment file:. ```. name: dv. channels:. - conda-forge. - bioconda. - defaults. dependencies:. - python=3.6. - deepvariant=1.5.0. ```. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:0,usability,Error,Error,0,"Error running bioconda installed version; . Installed DeepVariant 1.5.0 from bioconda using micromamba. Running make examples command produces the following error:. ```. micromamba run --name dv /opt/conda/envs/dv/bin/python /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. Traceback (most recent call last):. File ""/opt/conda/envs/dv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main. ""__main__"", mod_spec). File ""/opt/conda/envs/dv/lib/python3.6/runpy.py"", line 85, in _run_code. exec(code, run_globals). File ""/opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip/__main__.py"", line 392, in <module>. File ""/opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip/__main__.py"", line 365, in Main. File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 287, in call. with Popen(*popenargs, **kwargs) as p:. File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 729, in __init__. restore_signals, start_new_session). File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 1364, in _execute_child. raise child_exception_type(errno_num, err_msg, err_filename). FileNotFoundError: [Errno 2] No such file or directory: '/usr/bin/python3': '/usr/bin/python3'. ```. I noticed inside the bazel .zip files the python binary is hard-coded:. /share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip Line 60: `PYTHON_BINARY = '/usr/bin/python3'`. Is there any way to override this value to select a different python binary? The micromamba python binary is not in the standard location and there is also multiple python binaries in this system. DeepVarian",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:126,usability,command,command,126,"Error running bioconda installed version; . Installed DeepVariant 1.5.0 from bioconda using micromamba. Running make examples command produces the following error:. ```. micromamba run --name dv /opt/conda/envs/dv/bin/python /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. Traceback (most recent call last):. File ""/opt/conda/envs/dv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main. ""__main__"", mod_spec). File ""/opt/conda/envs/dv/lib/python3.6/runpy.py"", line 85, in _run_code. exec(code, run_globals). File ""/opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip/__main__.py"", line 392, in <module>. File ""/opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip/__main__.py"", line 365, in Main. File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 287, in call. with Popen(*popenargs, **kwargs) as p:. File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 729, in __init__. restore_signals, start_new_session). File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 1364, in _execute_child. raise child_exception_type(errno_num, err_msg, err_filename). FileNotFoundError: [Errno 2] No such file or directory: '/usr/bin/python3': '/usr/bin/python3'. ```. I noticed inside the bazel .zip files the python binary is hard-coded:. /share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip Line 60: `PYTHON_BINARY = '/usr/bin/python3'`. Is there any way to override this value to select a different python binary? The micromamba python binary is not in the standard location and there is also multiple python binaries in this system. DeepVarian",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:157,usability,error,error,157,"Error running bioconda installed version; . Installed DeepVariant 1.5.0 from bioconda using micromamba. Running make examples command produces the following error:. ```. micromamba run --name dv /opt/conda/envs/dv/bin/python /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. Traceback (most recent call last):. File ""/opt/conda/envs/dv/lib/python3.6/runpy.py"", line 193, in _run_module_as_main. ""__main__"", mod_spec). File ""/opt/conda/envs/dv/lib/python3.6/runpy.py"", line 85, in _run_code. exec(code, run_globals). File ""/opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip/__main__.py"", line 392, in <module>. File ""/opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip/__main__.py"", line 365, in Main. File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 287, in call. with Popen(*popenargs, **kwargs) as p:. File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 729, in __init__. restore_signals, start_new_session). File ""/opt/conda/envs/dv/lib/python3.6/subprocess.py"", line 1364, in _execute_child. raise child_exception_type(errno_num, err_msg, err_filename). FileNotFoundError: [Errno 2] No such file or directory: '/usr/bin/python3': '/usr/bin/python3'. ```. I noticed inside the bazel .zip files the python binary is hard-coded:. /share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip Line 60: `PYTHON_BINARY = '/usr/bin/python3'`. Is there any way to override this value to select a different python binary? The micromamba python binary is not in the standard location and there is also multiple python binaries in this system. DeepVarian",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/666:406,availability,error,error,406,"Deepvariant fails to recognize PacBio BAM index file; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. Yes. . **Describe the issue:**. I followed the [PACBIO example](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md) and also added the flags from [this issue](https://github.com/google/deepvariant/issues/458). . . This error indicates that Deepvariant is not able to find an index file for the bam file but the index file is there see :. . ``` . (base) ✔ /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam[master L|…5] . 22:08 $ ls. GFX.bam GFX.bam.pbi GFX_hg19.bam GFX_hg19.bam.pbi readlength.txt tmp. ```. I also re-indexd the file using `pbindex` from [pbbam](https://github.com/pacificbiosciences/pbbam/). As one can see from the `ls` output I also tried to realign the bam file to some other reference panel using [pbmm2](https://github.com/PacificBiosciences/pbmm2/). . **Setup**. - Operating system: Linux Mint 21.1 x86_64 . - Kernel: 5.15.0-69-generic . - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): Docker image run through Singularity. - Singularity Verion : singularity-ce version 3.11.3. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) PACBIO CCS data aligned to GRCh37.fa reference genome. No special observations in the file can be reported. . ```. (base) ✔ /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam [master L|…5] . 20:46 $ samtools flagstat GFX.bam . ^[[1;5C940551 + 0 in total (QC-passed reads + QC-failed reads). 881297 + 0 primary. 0 + 0 secondary. 59254 + 0 supplementary. 0 + 0 duplicates. 0 + 0 primary duplicates. 940551 + 0 mapped (100.00% : N/A). 881297 + 0 primary mapped (100.00% : N/A). 0 + 0 paired in sequencing. 0 + 0 read1. 0 + 0 read2. 0 + 0 properly paired (N/A : N/A). 0 + 0 with itself and mate mapped. 0 + 0 singletons (N/A : N/A). 0 + 0 with mate mapped",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:982,availability,Operat,Operating,982,"Deepvariant fails to recognize PacBio BAM index file; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. Yes. . **Describe the issue:**. I followed the [PACBIO example](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md) and also added the flags from [this issue](https://github.com/google/deepvariant/issues/458). . . This error indicates that Deepvariant is not able to find an index file for the bam file but the index file is there see :. . ``` . (base) ✔ /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam[master L|…5] . 22:08 $ ls. GFX.bam GFX.bam.pbi GFX_hg19.bam GFX_hg19.bam.pbi readlength.txt tmp. ```. I also re-indexd the file using `pbindex` from [pbbam](https://github.com/pacificbiosciences/pbbam/). As one can see from the `ls` output I also tried to realign the bam file to some other reference panel using [pbmm2](https://github.com/PacificBiosciences/pbmm2/). . **Setup**. - Operating system: Linux Mint 21.1 x86_64 . - Kernel: 5.15.0-69-generic . - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): Docker image run through Singularity. - Singularity Verion : singularity-ce version 3.11.3. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) PACBIO CCS data aligned to GRCh37.fa reference genome. No special observations in the file can be reported. . ```. (base) ✔ /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam [master L|…5] . 20:46 $ samtools flagstat GFX.bam . ^[[1;5C940551 + 0 in total (QC-passed reads + QC-failed reads). 881297 + 0 primary. 0 + 0 secondary. 59254 + 0 supplementary. 0 + 0 duplicates. 0 + 0 primary duplicates. 940551 + 0 mapped (100.00% : N/A). 881297 + 0 primary mapped (100.00% : N/A). 0 + 0 paired in sequencing. 0 + 0 read1. 0 + 0 read2. 0 + 0 properly paired (N/A : N/A). 0 + 0 with itself and mate mapped. 0 + 0 singletons (N/A : N/A). 0 + 0 with mate mapped",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:2887,availability,Error,Error,2887,"ly paired (N/A : N/A). 0 + 0 with itself and mate mapped. 0 + 0 singletons (N/A : N/A). 0 + 0 with mate mapped to a different chr. 0 + 0 with mate mapped to a different chr (mapQ>=5). ```. . **Steps to reproduce:**. - Command: Follow the [PACBIO example](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md) and install the mentioned singularity version. Then run . `singularity exec --bind /usr/lib/locale/,/media/nils/nils_ssd_01/Genomics_prac_guide/reference/unsorted/,/media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/ docker://google/deepvariant:1.5.0 /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /media/nils/nils_ssd_01/Genomics_prac_guide/reference/unsorted/hg19.fa --reads /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/GFX.bam --output_vcf /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/GFX.vcf.gz --num_shards 22 --dry_run=false --make_examples_extra_args='sort_by_haplotypes=false,parse_sam_aux_fields=false'`. - Error trace: . The error message including the pipeline call is :. . ```. E::idx_find_and_load] Could not retrieve index file for '/media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam/GFX.bam'. 2023-06-20 22:48:40.272832: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. 2023-06-20 22:48:40.272881: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag BC: in RG line, ignoring: @RG	ID:408781da/26--26	PL:PACBIO	DS:READTYPE=CCS;BINDINGKIT=101-894-200;SEQUENCINGKIT=101-826-100;BASECALLERVERSION=5.0.0;FRAMERATEHZ=100.000000;BarcodeFile=m64023e_230515_162401.barcodes.fasta;BarcodeHash=86d73e586a6d3ede0295785b51105eea;BarcodeCount=96;BarcodeMode=Symmetric;BarcodeQuality=Score	LB:Pool_18_20_GFX0455704_GFX	PU:m64023e_230515_162401	SM:GFX. PM:SEQUELII	BC:TGACTGTAGCGAGTAT	CM:S/P5-C2/5.0-8M. 2023-06-20 22:48:40.272889: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG	ID:408781da/26--26	PL",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:2906,availability,error,error,2906,"A). 0 + 0 with itself and mate mapped. 0 + 0 singletons (N/A : N/A). 0 + 0 with mate mapped to a different chr. 0 + 0 with mate mapped to a different chr (mapQ>=5). ```. . **Steps to reproduce:**. - Command: Follow the [PACBIO example](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md) and install the mentioned singularity version. Then run . `singularity exec --bind /usr/lib/locale/,/media/nils/nils_ssd_01/Genomics_prac_guide/reference/unsorted/,/media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/ docker://google/deepvariant:1.5.0 /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /media/nils/nils_ssd_01/Genomics_prac_guide/reference/unsorted/hg19.fa --reads /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/GFX.bam --output_vcf /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/GFX.vcf.gz --num_shards 22 --dry_run=false --make_examples_extra_args='sort_by_haplotypes=false,parse_sam_aux_fields=false'`. - Error trace: . The error message including the pipeline call is :. . ```. E::idx_find_and_load] Could not retrieve index file for '/media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam/GFX.bam'. 2023-06-20 22:48:40.272832: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. 2023-06-20 22:48:40.272881: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag BC: in RG line, ignoring: @RG	ID:408781da/26--26	PL:PACBIO	DS:READTYPE=CCS;BINDINGKIT=101-894-200;SEQUENCINGKIT=101-826-100;BASECALLERVERSION=5.0.0;FRAMERATEHZ=100.000000;BarcodeFile=m64023e_230515_162401.barcodes.fasta;BarcodeHash=86d73e586a6d3ede0295785b51105eea;BarcodeCount=96;BarcodeMode=Symmetric;BarcodeQuality=Score	LB:Pool_18_20_GFX0455704_GFX	PU:m64023e_230515_162401	SM:GFX. PM:SEQUELII	BC:TGACTGTAGCGAGTAT	CM:S/P5-C2/5.0-8M. 2023-06-20 22:48:40.272889: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG	ID:408781da/26--26	PL:PACBIO	DS:READTYPE",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:12,deployability,fail,fails,12,"Deepvariant fails to recognize PacBio BAM index file; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. Yes. . **Describe the issue:**. I followed the [PACBIO example](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md) and also added the flags from [this issue](https://github.com/google/deepvariant/issues/458). . . This error indicates that Deepvariant is not able to find an index file for the bam file but the index file is there see :. . ``` . (base) ✔ /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam[master L|…5] . 22:08 $ ls. GFX.bam GFX.bam.pbi GFX_hg19.bam GFX_hg19.bam.pbi readlength.txt tmp. ```. I also re-indexd the file using `pbindex` from [pbbam](https://github.com/pacificbiosciences/pbbam/). As one can see from the `ls` output I also tried to realign the bam file to some other reference panel using [pbmm2](https://github.com/PacificBiosciences/pbmm2/). . **Setup**. - Operating system: Linux Mint 21.1 x86_64 . - Kernel: 5.15.0-69-generic . - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): Docker image run through Singularity. - Singularity Verion : singularity-ce version 3.11.3. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) PACBIO CCS data aligned to GRCh37.fa reference genome. No special observations in the file can be reported. . ```. (base) ✔ /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam [master L|…5] . 20:46 $ samtools flagstat GFX.bam . ^[[1;5C940551 + 0 in total (QC-passed reads + QC-failed reads). 881297 + 0 primary. 0 + 0 secondary. 59254 + 0 supplementary. 0 + 0 duplicates. 0 + 0 primary duplicates. 940551 + 0 mapped (100.00% : N/A). 881297 + 0 primary mapped (100.00% : N/A). 0 + 0 paired in sequencing. 0 + 0 read1. 0 + 0 read2. 0 + 0 properly paired (N/A : N/A). 0 + 0 with itself and mate mapped. 0 + 0 singletons (N/A : N/A). 0 + 0 with mate mapped",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1069,deployability,version,version,1069,"the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. Yes. . **Describe the issue:**. I followed the [PACBIO example](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md) and also added the flags from [this issue](https://github.com/google/deepvariant/issues/458). . . This error indicates that Deepvariant is not able to find an index file for the bam file but the index file is there see :. . ``` . (base) ✔ /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam[master L|…5] . 22:08 $ ls. GFX.bam GFX.bam.pbi GFX_hg19.bam GFX_hg19.bam.pbi readlength.txt tmp. ```. I also re-indexd the file using `pbindex` from [pbbam](https://github.com/pacificbiosciences/pbbam/). As one can see from the `ls` output I also tried to realign the bam file to some other reference panel using [pbmm2](https://github.com/PacificBiosciences/pbmm2/). . **Setup**. - Operating system: Linux Mint 21.1 x86_64 . - Kernel: 5.15.0-69-generic . - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): Docker image run through Singularity. - Singularity Verion : singularity-ce version 3.11.3. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) PACBIO CCS data aligned to GRCh37.fa reference genome. No special observations in the file can be reported. . ```. (base) ✔ /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam [master L|…5] . 20:46 $ samtools flagstat GFX.bam . ^[[1;5C940551 + 0 in total (QC-passed reads + QC-failed reads). 881297 + 0 primary. 0 + 0 secondary. 59254 + 0 supplementary. 0 + 0 duplicates. 0 + 0 primary duplicates. 940551 + 0 mapped (100.00% : N/A). 881297 + 0 primary mapped (100.00% : N/A). 0 + 0 paired in sequencing. 0 + 0 read1. 0 + 0 read2. 0 + 0 properly paired (N/A : N/A). 0 + 0 with itself and mate mapped. 0 + 0 singletons (N/A : N/A). 0 + 0 with mate mapped to a different chr. 0 + 0 with mate mapped to a different chr (mapQ>=5).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1087,deployability,Instal,Installation,1087,"hub.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. Yes. . **Describe the issue:**. I followed the [PACBIO example](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md) and also added the flags from [this issue](https://github.com/google/deepvariant/issues/458). . . This error indicates that Deepvariant is not able to find an index file for the bam file but the index file is there see :. . ``` . (base) ✔ /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam[master L|…5] . 22:08 $ ls. GFX.bam GFX.bam.pbi GFX_hg19.bam GFX_hg19.bam.pbi readlength.txt tmp. ```. I also re-indexd the file using `pbindex` from [pbbam](https://github.com/pacificbiosciences/pbbam/). As one can see from the `ls` output I also tried to realign the bam file to some other reference panel using [pbmm2](https://github.com/PacificBiosciences/pbmm2/). . **Setup**. - Operating system: Linux Mint 21.1 x86_64 . - Kernel: 5.15.0-69-generic . - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): Docker image run through Singularity. - Singularity Verion : singularity-ce version 3.11.3. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) PACBIO CCS data aligned to GRCh37.fa reference genome. No special observations in the file can be reported. . ```. (base) ✔ /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam [master L|…5] . 20:46 $ samtools flagstat GFX.bam . ^[[1;5C940551 + 0 in total (QC-passed reads + QC-failed reads). 881297 + 0 primary. 0 + 0 secondary. 59254 + 0 supplementary. 0 + 0 duplicates. 0 + 0 primary duplicates. 940551 + 0 mapped (100.00% : N/A). 881297 + 0 primary mapped (100.00% : N/A). 0 + 0 paired in sequencing. 0 + 0 read1. 0 + 0 read2. 0 + 0 properly paired (N/A : N/A). 0 + 0 with itself and mate mapped. 0 + 0 singletons (N/A : N/A). 0 + 0 with mate mapped to a different chr. 0 + 0 with mate mapped to a different chr (mapQ>=5). ```. . **Steps to r",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1218,deployability,version,version,1218,"ub.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md) and also added the flags from [this issue](https://github.com/google/deepvariant/issues/458). . . This error indicates that Deepvariant is not able to find an index file for the bam file but the index file is there see :. . ``` . (base) ✔ /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam[master L|…5] . 22:08 $ ls. GFX.bam GFX.bam.pbi GFX_hg19.bam GFX_hg19.bam.pbi readlength.txt tmp. ```. I also re-indexd the file using `pbindex` from [pbbam](https://github.com/pacificbiosciences/pbbam/). As one can see from the `ls` output I also tried to realign the bam file to some other reference panel using [pbmm2](https://github.com/PacificBiosciences/pbmm2/). . **Setup**. - Operating system: Linux Mint 21.1 x86_64 . - Kernel: 5.15.0-69-generic . - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): Docker image run through Singularity. - Singularity Verion : singularity-ce version 3.11.3. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) PACBIO CCS data aligned to GRCh37.fa reference genome. No special observations in the file can be reported. . ```. (base) ✔ /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam [master L|…5] . 20:46 $ samtools flagstat GFX.bam . ^[[1;5C940551 + 0 in total (QC-passed reads + QC-failed reads). 881297 + 0 primary. 0 + 0 secondary. 59254 + 0 supplementary. 0 + 0 duplicates. 0 + 0 primary duplicates. 940551 + 0 mapped (100.00% : N/A). 881297 + 0 primary mapped (100.00% : N/A). 0 + 0 paired in sequencing. 0 + 0 read1. 0 + 0 read2. 0 + 0 properly paired (N/A : N/A). 0 + 0 with itself and mate mapped. 0 + 0 singletons (N/A : N/A). 0 + 0 with mate mapped to a different chr. 0 + 0 with mate mapped to a different chr (mapQ>=5). ```. . **Steps to reproduce:**. - Command: Follow the [PACBIO example](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1409,deployability,observ,observations,1409,"icates that Deepvariant is not able to find an index file for the bam file but the index file is there see :. . ``` . (base) ✔ /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam[master L|…5] . 22:08 $ ls. GFX.bam GFX.bam.pbi GFX_hg19.bam GFX_hg19.bam.pbi readlength.txt tmp. ```. I also re-indexd the file using `pbindex` from [pbbam](https://github.com/pacificbiosciences/pbbam/). As one can see from the `ls` output I also tried to realign the bam file to some other reference panel using [pbmm2](https://github.com/PacificBiosciences/pbmm2/). . **Setup**. - Operating system: Linux Mint 21.1 x86_64 . - Kernel: 5.15.0-69-generic . - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): Docker image run through Singularity. - Singularity Verion : singularity-ce version 3.11.3. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) PACBIO CCS data aligned to GRCh37.fa reference genome. No special observations in the file can be reported. . ```. (base) ✔ /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam [master L|…5] . 20:46 $ samtools flagstat GFX.bam . ^[[1;5C940551 + 0 in total (QC-passed reads + QC-failed reads). 881297 + 0 primary. 0 + 0 secondary. 59254 + 0 supplementary. 0 + 0 duplicates. 0 + 0 primary duplicates. 940551 + 0 mapped (100.00% : N/A). 881297 + 0 primary mapped (100.00% : N/A). 0 + 0 paired in sequencing. 0 + 0 read1. 0 + 0 read2. 0 + 0 properly paired (N/A : N/A). 0 + 0 with itself and mate mapped. 0 + 0 singletons (N/A : N/A). 0 + 0 with mate mapped to a different chr. 0 + 0 with mate mapped to a different chr (mapQ>=5). ```. . **Steps to reproduce:**. - Command: Follow the [PACBIO example](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md) and install the mentioned singularity version. Then run . `singularity exec --bind /usr/lib/locale/,/media/nils/nils_ssd_01/Genomics_prac_guide/reference/unsorted/,/media/nils/n",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1625,deployability,fail,failed,1625,"X.bam GFX.bam.pbi GFX_hg19.bam GFX_hg19.bam.pbi readlength.txt tmp. ```. I also re-indexd the file using `pbindex` from [pbbam](https://github.com/pacificbiosciences/pbbam/). As one can see from the `ls` output I also tried to realign the bam file to some other reference panel using [pbmm2](https://github.com/PacificBiosciences/pbmm2/). . **Setup**. - Operating system: Linux Mint 21.1 x86_64 . - Kernel: 5.15.0-69-generic . - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): Docker image run through Singularity. - Singularity Verion : singularity-ce version 3.11.3. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) PACBIO CCS data aligned to GRCh37.fa reference genome. No special observations in the file can be reported. . ```. (base) ✔ /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam [master L|…5] . 20:46 $ samtools flagstat GFX.bam . ^[[1;5C940551 + 0 in total (QC-passed reads + QC-failed reads). 881297 + 0 primary. 0 + 0 secondary. 59254 + 0 supplementary. 0 + 0 duplicates. 0 + 0 primary duplicates. 940551 + 0 mapped (100.00% : N/A). 881297 + 0 primary mapped (100.00% : N/A). 0 + 0 paired in sequencing. 0 + 0 read1. 0 + 0 read2. 0 + 0 properly paired (N/A : N/A). 0 + 0 with itself and mate mapped. 0 + 0 singletons (N/A : N/A). 0 + 0 with mate mapped to a different chr. 0 + 0 with mate mapped to a different chr (mapQ>=5). ```. . **Steps to reproduce:**. - Command: Follow the [PACBIO example](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md) and install the mentioned singularity version. Then run . `singularity exec --bind /usr/lib/locale/,/media/nils/nils_ssd_01/Genomics_prac_guide/reference/unsorted/,/media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/ docker://google/deepvariant:1.5.0 /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /media/nils/nils_ssd_01/Genomics_prac_guide/reference/unsorted/hg19.fa --re",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:2242,deployability,instal,install,2242,"ta: (sequencing instrument, reference genome, anything special that is unlike the case studies?) PACBIO CCS data aligned to GRCh37.fa reference genome. No special observations in the file can be reported. . ```. (base) ✔ /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam [master L|…5] . 20:46 $ samtools flagstat GFX.bam . ^[[1;5C940551 + 0 in total (QC-passed reads + QC-failed reads). 881297 + 0 primary. 0 + 0 secondary. 59254 + 0 supplementary. 0 + 0 duplicates. 0 + 0 primary duplicates. 940551 + 0 mapped (100.00% : N/A). 881297 + 0 primary mapped (100.00% : N/A). 0 + 0 paired in sequencing. 0 + 0 read1. 0 + 0 read2. 0 + 0 properly paired (N/A : N/A). 0 + 0 with itself and mate mapped. 0 + 0 singletons (N/A : N/A). 0 + 0 with mate mapped to a different chr. 0 + 0 with mate mapped to a different chr (mapQ>=5). ```. . **Steps to reproduce:**. - Command: Follow the [PACBIO example](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md) and install the mentioned singularity version. Then run . `singularity exec --bind /usr/lib/locale/,/media/nils/nils_ssd_01/Genomics_prac_guide/reference/unsorted/,/media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/ docker://google/deepvariant:1.5.0 /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /media/nils/nils_ssd_01/Genomics_prac_guide/reference/unsorted/hg19.fa --reads /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/GFX.bam --output_vcf /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/GFX.vcf.gz --num_shards 22 --dry_run=false --make_examples_extra_args='sort_by_haplotypes=false,parse_sam_aux_fields=false'`. - Error trace: . The error message including the pipeline call is :. . ```. E::idx_find_and_load] Could not retrieve index file for '/media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam/GFX.bam'. 2023-06-20 22:48:40.272832: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. 2023-06-20 2",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:2276,deployability,version,version,2276,"nce genome, anything special that is unlike the case studies?) PACBIO CCS data aligned to GRCh37.fa reference genome. No special observations in the file can be reported. . ```. (base) ✔ /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam [master L|…5] . 20:46 $ samtools flagstat GFX.bam . ^[[1;5C940551 + 0 in total (QC-passed reads + QC-failed reads). 881297 + 0 primary. 0 + 0 secondary. 59254 + 0 supplementary. 0 + 0 duplicates. 0 + 0 primary duplicates. 940551 + 0 mapped (100.00% : N/A). 881297 + 0 primary mapped (100.00% : N/A). 0 + 0 paired in sequencing. 0 + 0 read1. 0 + 0 read2. 0 + 0 properly paired (N/A : N/A). 0 + 0 with itself and mate mapped. 0 + 0 singletons (N/A : N/A). 0 + 0 with mate mapped to a different chr. 0 + 0 with mate mapped to a different chr (mapQ>=5). ```. . **Steps to reproduce:**. - Command: Follow the [PACBIO example](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md) and install the mentioned singularity version. Then run . `singularity exec --bind /usr/lib/locale/,/media/nils/nils_ssd_01/Genomics_prac_guide/reference/unsorted/,/media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/ docker://google/deepvariant:1.5.0 /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /media/nils/nils_ssd_01/Genomics_prac_guide/reference/unsorted/hg19.fa --reads /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/GFX.bam --output_vcf /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/GFX.vcf.gz --num_shards 22 --dry_run=false --make_examples_extra_args='sort_by_haplotypes=false,parse_sam_aux_fields=false'`. - Error trace: . The error message including the pipeline call is :. . ```. E::idx_find_and_load] Could not retrieve index file for '/media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam/GFX.bam'. 2023-06-20 22:48:40.272832: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. 2023-06-20 22:48:40.272881: W third_party/nucl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:2934,deployability,pipelin,pipeline,2934,"e mapped. 0 + 0 singletons (N/A : N/A). 0 + 0 with mate mapped to a different chr. 0 + 0 with mate mapped to a different chr (mapQ>=5). ```. . **Steps to reproduce:**. - Command: Follow the [PACBIO example](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md) and install the mentioned singularity version. Then run . `singularity exec --bind /usr/lib/locale/,/media/nils/nils_ssd_01/Genomics_prac_guide/reference/unsorted/,/media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/ docker://google/deepvariant:1.5.0 /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /media/nils/nils_ssd_01/Genomics_prac_guide/reference/unsorted/hg19.fa --reads /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/GFX.bam --output_vcf /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/GFX.vcf.gz --num_shards 22 --dry_run=false --make_examples_extra_args='sort_by_haplotypes=false,parse_sam_aux_fields=false'`. - Error trace: . The error message including the pipeline call is :. . ```. E::idx_find_and_load] Could not retrieve index file for '/media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam/GFX.bam'. 2023-06-20 22:48:40.272832: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. 2023-06-20 22:48:40.272881: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag BC: in RG line, ignoring: @RG	ID:408781da/26--26	PL:PACBIO	DS:READTYPE=CCS;BINDINGKIT=101-894-200;SEQUENCINGKIT=101-826-100;BASECALLERVERSION=5.0.0;FRAMERATEHZ=100.000000;BarcodeFile=m64023e_230515_162401.barcodes.fasta;BarcodeHash=86d73e586a6d3ede0295785b51105eea;BarcodeCount=96;BarcodeMode=Symmetric;BarcodeQuality=Score	LB:Pool_18_20_GFX0455704_GFX	PU:m64023e_230515_162401	SM:GFX. PM:SEQUELII	BC:TGACTGTAGCGAGTAT	CM:S/P5-C2/5.0-8M. 2023-06-20 22:48:40.272889: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG	ID:408781da/26--26	PL:PACBIO	DS:READTYPE=CCS;BINDINGKIT=101-894-200;S",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:4854,deployability,modul,module,4854,"noring: @RG	ID:408781da/26--26	PL:PACBIO	DS:READTYPE=CCS;BINDINGKIT=101-894-200;SEQUENCINGKIT=101-826-100;BASECALLERVERSION=5.0.0;FRAMERATEHZ=100.000000;BarcodeFile=m64023e_230515_162401.barcodes.fasta;BarcodeHash=86d73e586a6d3ede0295785b51105eea;BarcodeCount=96;BarcodeMode=Symmetric;BarcodeQuality=Score	LB:Pool_18_20_GFX0455704_GFX	PU:m64023e_230515_162401	SM:GFX. PM:SEQUELII	BC:TGACTGTAGCGAGTAT	CM:S/P5-C2/5.0-8M. I0620 22:48:40.273113 140449601988416 genomics_reader.py:222] Reading /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam/GFX.bam with NativeSamReader. I0620 22:48:40.273825 140449601988416 make_examples_core.py:257] Task 8/22: Writing examples to /tmp/tmpwjk24y8t/make_examples.tfrecord-00008-of-00022.gz. I0620 22:48:40.274013 140449601988416 make_examples_core.py:257] Task 8/22: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2183, in make_examples_runner. runtimes) = region_processor.process(region). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1285, in process. sample_reads = self.region_reads_norealign(. File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1376, in region_reads_norealign. reads = itertools.chain(reads, sam_reader.query(region)). File ""/tmp/Bazel.runfiles_ztv_d7ra/run",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:6222,deployability,fail,failed,6222,"_run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2183, in make_examples_runner. runtimes) = region_processor.process(region). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1285, in process. sample_reads = self.region_reads_norealign(. File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1376, in region_reads_norealign. reads = itertools.chain(reads, sam_reader.query(region)). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 247, in query. return self._reader.query(region). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 250, in query. return self._reader.query(region). ValueError: FAILED_PRECONDITION: Cannot query without an index. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /media/nils/nils_ssd_01/Genomics_prac_guide/reference/GRCh37/hs37d5.fa --reads /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam/GFX.bam --examples /tmp/tmpwjk24y8t/make_examples.tfrecord@22.gz --add_hp_channel --alt_aligned_pileup diff_channels --max_reads_per_partition 600 --min_mapping_quality 1 --noparse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --nosort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --task 1. . ```. . **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Quickstart works. This issue also happens when I try to run the pipeline with docker-only. . **Any additional context:**. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:7005,deployability,pipelin,pipeline,7005,"_run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2183, in make_examples_runner. runtimes) = region_processor.process(region). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1285, in process. sample_reads = self.region_reads_norealign(. File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1376, in region_reads_norealign. reads = itertools.chain(reads, sam_reader.query(region)). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 247, in query. return self._reader.query(region). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 250, in query. return self._reader.query(region). ValueError: FAILED_PRECONDITION: Cannot query without an index. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /media/nils/nils_ssd_01/Genomics_prac_guide/reference/GRCh37/hs37d5.fa --reads /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam/GFX.bam --examples /tmp/tmpwjk24y8t/make_examples.tfrecord@22.gz --add_hp_channel --alt_aligned_pileup diff_channels --max_reads_per_partition 600 --min_mapping_quality 1 --noparse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --nosort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --task 1. . ```. . **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Quickstart works. This issue also happens when I try to run the pipeline with docker-only. . **Any additional context:**. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:282,energy efficiency,model,model-case-study,282,"Deepvariant fails to recognize PacBio BAM index file; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. Yes. . **Describe the issue:**. I followed the [PACBIO example](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md) and also added the flags from [this issue](https://github.com/google/deepvariant/issues/458). . . This error indicates that Deepvariant is not able to find an index file for the bam file but the index file is there see :. . ``` . (base) ✔ /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam[master L|…5] . 22:08 $ ls. GFX.bam GFX.bam.pbi GFX_hg19.bam GFX_hg19.bam.pbi readlength.txt tmp. ```. I also re-indexd the file using `pbindex` from [pbbam](https://github.com/pacificbiosciences/pbbam/). As one can see from the `ls` output I also tried to realign the bam file to some other reference panel using [pbmm2](https://github.com/PacificBiosciences/pbmm2/). . **Setup**. - Operating system: Linux Mint 21.1 x86_64 . - Kernel: 5.15.0-69-generic . - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): Docker image run through Singularity. - Singularity Verion : singularity-ce version 3.11.3. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) PACBIO CCS data aligned to GRCh37.fa reference genome. No special observations in the file can be reported. . ```. (base) ✔ /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam [master L|…5] . 20:46 $ samtools flagstat GFX.bam . ^[[1;5C940551 + 0 in total (QC-passed reads + QC-failed reads). 881297 + 0 primary. 0 + 0 secondary. 59254 + 0 supplementary. 0 + 0 duplicates. 0 + 0 primary duplicates. 940551 + 0 mapped (100.00% : N/A). 881297 + 0 primary mapped (100.00% : N/A). 0 + 0 paired in sequencing. 0 + 0 read1. 0 + 0 read2. 0 + 0 properly paired (N/A : N/A). 0 + 0 with itself and mate mapped. 0 + 0 singletons (N/A : N/A). 0 + 0 with mate mapped",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:2217,energy efficiency,model,model-case-study,2217," 3.11.3. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) PACBIO CCS data aligned to GRCh37.fa reference genome. No special observations in the file can be reported. . ```. (base) ✔ /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam [master L|…5] . 20:46 $ samtools flagstat GFX.bam . ^[[1;5C940551 + 0 in total (QC-passed reads + QC-failed reads). 881297 + 0 primary. 0 + 0 secondary. 59254 + 0 supplementary. 0 + 0 duplicates. 0 + 0 primary duplicates. 940551 + 0 mapped (100.00% : N/A). 881297 + 0 primary mapped (100.00% : N/A). 0 + 0 paired in sequencing. 0 + 0 read1. 0 + 0 read2. 0 + 0 properly paired (N/A : N/A). 0 + 0 with itself and mate mapped. 0 + 0 singletons (N/A : N/A). 0 + 0 with mate mapped to a different chr. 0 + 0 with mate mapped to a different chr (mapQ>=5). ```. . **Steps to reproduce:**. - Command: Follow the [PACBIO example](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md) and install the mentioned singularity version. Then run . `singularity exec --bind /usr/lib/locale/,/media/nils/nils_ssd_01/Genomics_prac_guide/reference/unsorted/,/media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/ docker://google/deepvariant:1.5.0 /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /media/nils/nils_ssd_01/Genomics_prac_guide/reference/unsorted/hg19.fa --reads /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/GFX.bam --output_vcf /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/GFX.vcf.gz --num_shards 22 --dry_run=false --make_examples_extra_args='sort_by_haplotypes=false,parse_sam_aux_fields=false'`. - Error trace: . The error message including the pipeline call is :. . ```. E::idx_find_and_load] Could not retrieve index file for '/media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam/GFX.bam'. 2023-06-20 22:48:40.272832: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	p",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1069,integrability,version,version,1069,"the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. Yes. . **Describe the issue:**. I followed the [PACBIO example](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md) and also added the flags from [this issue](https://github.com/google/deepvariant/issues/458). . . This error indicates that Deepvariant is not able to find an index file for the bam file but the index file is there see :. . ``` . (base) ✔ /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam[master L|…5] . 22:08 $ ls. GFX.bam GFX.bam.pbi GFX_hg19.bam GFX_hg19.bam.pbi readlength.txt tmp. ```. I also re-indexd the file using `pbindex` from [pbbam](https://github.com/pacificbiosciences/pbbam/). As one can see from the `ls` output I also tried to realign the bam file to some other reference panel using [pbmm2](https://github.com/PacificBiosciences/pbmm2/). . **Setup**. - Operating system: Linux Mint 21.1 x86_64 . - Kernel: 5.15.0-69-generic . - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): Docker image run through Singularity. - Singularity Verion : singularity-ce version 3.11.3. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) PACBIO CCS data aligned to GRCh37.fa reference genome. No special observations in the file can be reported. . ```. (base) ✔ /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam [master L|…5] . 20:46 $ samtools flagstat GFX.bam . ^[[1;5C940551 + 0 in total (QC-passed reads + QC-failed reads). 881297 + 0 primary. 0 + 0 secondary. 59254 + 0 supplementary. 0 + 0 duplicates. 0 + 0 primary duplicates. 940551 + 0 mapped (100.00% : N/A). 881297 + 0 primary mapped (100.00% : N/A). 0 + 0 paired in sequencing. 0 + 0 read1. 0 + 0 read2. 0 + 0 properly paired (N/A : N/A). 0 + 0 with itself and mate mapped. 0 + 0 singletons (N/A : N/A). 0 + 0 with mate mapped to a different chr. 0 + 0 with mate mapped to a different chr (mapQ>=5).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1218,integrability,version,version,1218,"ub.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md) and also added the flags from [this issue](https://github.com/google/deepvariant/issues/458). . . This error indicates that Deepvariant is not able to find an index file for the bam file but the index file is there see :. . ``` . (base) ✔ /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam[master L|…5] . 22:08 $ ls. GFX.bam GFX.bam.pbi GFX_hg19.bam GFX_hg19.bam.pbi readlength.txt tmp. ```. I also re-indexd the file using `pbindex` from [pbbam](https://github.com/pacificbiosciences/pbbam/). As one can see from the `ls` output I also tried to realign the bam file to some other reference panel using [pbmm2](https://github.com/PacificBiosciences/pbmm2/). . **Setup**. - Operating system: Linux Mint 21.1 x86_64 . - Kernel: 5.15.0-69-generic . - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): Docker image run through Singularity. - Singularity Verion : singularity-ce version 3.11.3. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) PACBIO CCS data aligned to GRCh37.fa reference genome. No special observations in the file can be reported. . ```. (base) ✔ /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam [master L|…5] . 20:46 $ samtools flagstat GFX.bam . ^[[1;5C940551 + 0 in total (QC-passed reads + QC-failed reads). 881297 + 0 primary. 0 + 0 secondary. 59254 + 0 supplementary. 0 + 0 duplicates. 0 + 0 primary duplicates. 940551 + 0 mapped (100.00% : N/A). 881297 + 0 primary mapped (100.00% : N/A). 0 + 0 paired in sequencing. 0 + 0 read1. 0 + 0 read2. 0 + 0 properly paired (N/A : N/A). 0 + 0 with itself and mate mapped. 0 + 0 singletons (N/A : N/A). 0 + 0 with mate mapped to a different chr. 0 + 0 with mate mapped to a different chr (mapQ>=5). ```. . **Steps to reproduce:**. - Command: Follow the [PACBIO example](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:2276,integrability,version,version,2276,"nce genome, anything special that is unlike the case studies?) PACBIO CCS data aligned to GRCh37.fa reference genome. No special observations in the file can be reported. . ```. (base) ✔ /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam [master L|…5] . 20:46 $ samtools flagstat GFX.bam . ^[[1;5C940551 + 0 in total (QC-passed reads + QC-failed reads). 881297 + 0 primary. 0 + 0 secondary. 59254 + 0 supplementary. 0 + 0 duplicates. 0 + 0 primary duplicates. 940551 + 0 mapped (100.00% : N/A). 881297 + 0 primary mapped (100.00% : N/A). 0 + 0 paired in sequencing. 0 + 0 read1. 0 + 0 read2. 0 + 0 properly paired (N/A : N/A). 0 + 0 with itself and mate mapped. 0 + 0 singletons (N/A : N/A). 0 + 0 with mate mapped to a different chr. 0 + 0 with mate mapped to a different chr (mapQ>=5). ```. . **Steps to reproduce:**. - Command: Follow the [PACBIO example](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md) and install the mentioned singularity version. Then run . `singularity exec --bind /usr/lib/locale/,/media/nils/nils_ssd_01/Genomics_prac_guide/reference/unsorted/,/media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/ docker://google/deepvariant:1.5.0 /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /media/nils/nils_ssd_01/Genomics_prac_guide/reference/unsorted/hg19.fa --reads /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/GFX.bam --output_vcf /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/GFX.vcf.gz --num_shards 22 --dry_run=false --make_examples_extra_args='sort_by_haplotypes=false,parse_sam_aux_fields=false'`. - Error trace: . The error message including the pipeline call is :. . ```. E::idx_find_and_load] Could not retrieve index file for '/media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam/GFX.bam'. 2023-06-20 22:48:40.272832: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. 2023-06-20 22:48:40.272881: W third_party/nucl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:2912,integrability,messag,message,2912," 0 with itself and mate mapped. 0 + 0 singletons (N/A : N/A). 0 + 0 with mate mapped to a different chr. 0 + 0 with mate mapped to a different chr (mapQ>=5). ```. . **Steps to reproduce:**. - Command: Follow the [PACBIO example](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md) and install the mentioned singularity version. Then run . `singularity exec --bind /usr/lib/locale/,/media/nils/nils_ssd_01/Genomics_prac_guide/reference/unsorted/,/media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/ docker://google/deepvariant:1.5.0 /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /media/nils/nils_ssd_01/Genomics_prac_guide/reference/unsorted/hg19.fa --reads /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/GFX.bam --output_vcf /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/GFX.vcf.gz --num_shards 22 --dry_run=false --make_examples_extra_args='sort_by_haplotypes=false,parse_sam_aux_fields=false'`. - Error trace: . The error message including the pipeline call is :. . ```. E::idx_find_and_load] Could not retrieve index file for '/media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam/GFX.bam'. 2023-06-20 22:48:40.272832: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. 2023-06-20 22:48:40.272881: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag BC: in RG line, ignoring: @RG	ID:408781da/26--26	PL:PACBIO	DS:READTYPE=CCS;BINDINGKIT=101-894-200;SEQUENCINGKIT=101-826-100;BASECALLERVERSION=5.0.0;FRAMERATEHZ=100.000000;BarcodeFile=m64023e_230515_162401.barcodes.fasta;BarcodeHash=86d73e586a6d3ede0295785b51105eea;BarcodeCount=96;BarcodeMode=Symmetric;BarcodeQuality=Score	LB:Pool_18_20_GFX0455704_GFX	PU:m64023e_230515_162401	SM:GFX. PM:SEQUELII	BC:TGACTGTAGCGAGTAT	CM:S/P5-C2/5.0-8M. 2023-06-20 22:48:40.272889: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG	ID:408781da/26--26	PL:PACBIO	DS:READTYPE=CCS;BI",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:2934,integrability,pipelin,pipeline,2934,"e mapped. 0 + 0 singletons (N/A : N/A). 0 + 0 with mate mapped to a different chr. 0 + 0 with mate mapped to a different chr (mapQ>=5). ```. . **Steps to reproduce:**. - Command: Follow the [PACBIO example](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md) and install the mentioned singularity version. Then run . `singularity exec --bind /usr/lib/locale/,/media/nils/nils_ssd_01/Genomics_prac_guide/reference/unsorted/,/media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/ docker://google/deepvariant:1.5.0 /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /media/nils/nils_ssd_01/Genomics_prac_guide/reference/unsorted/hg19.fa --reads /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/GFX.bam --output_vcf /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/GFX.vcf.gz --num_shards 22 --dry_run=false --make_examples_extra_args='sort_by_haplotypes=false,parse_sam_aux_fields=false'`. - Error trace: . The error message including the pipeline call is :. . ```. E::idx_find_and_load] Could not retrieve index file for '/media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam/GFX.bam'. 2023-06-20 22:48:40.272832: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. 2023-06-20 22:48:40.272881: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag BC: in RG line, ignoring: @RG	ID:408781da/26--26	PL:PACBIO	DS:READTYPE=CCS;BINDINGKIT=101-894-200;SEQUENCINGKIT=101-826-100;BASECALLERVERSION=5.0.0;FRAMERATEHZ=100.000000;BarcodeFile=m64023e_230515_162401.barcodes.fasta;BarcodeHash=86d73e586a6d3ede0295785b51105eea;BarcodeCount=96;BarcodeMode=Symmetric;BarcodeQuality=Score	LB:Pool_18_20_GFX0455704_GFX	PU:m64023e_230515_162401	SM:GFX. PM:SEQUELII	BC:TGACTGTAGCGAGTAT	CM:S/P5-C2/5.0-8M. 2023-06-20 22:48:40.272889: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG	ID:408781da/26--26	PL:PACBIO	DS:READTYPE=CCS;BINDINGKIT=101-894-200;S",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:7005,integrability,pipelin,pipeline,7005,"_run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2183, in make_examples_runner. runtimes) = region_processor.process(region). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1285, in process. sample_reads = self.region_reads_norealign(. File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1376, in region_reads_norealign. reads = itertools.chain(reads, sam_reader.query(region)). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 247, in query. return self._reader.query(region). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 250, in query. return self._reader.query(region). ValueError: FAILED_PRECONDITION: Cannot query without an index. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /media/nils/nils_ssd_01/Genomics_prac_guide/reference/GRCh37/hs37d5.fa --reads /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam/GFX.bam --examples /tmp/tmpwjk24y8t/make_examples.tfrecord@22.gz --add_hp_channel --alt_aligned_pileup diff_channels --max_reads_per_partition 600 --min_mapping_quality 1 --noparse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --nosort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --task 1. . ```. . **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Quickstart works. This issue also happens when I try to run the pipeline with docker-only. . **Any additional context:**. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:2316,interoperability,bind,bind,2316,"nlike the case studies?) PACBIO CCS data aligned to GRCh37.fa reference genome. No special observations in the file can be reported. . ```. (base) ✔ /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam [master L|…5] . 20:46 $ samtools flagstat GFX.bam . ^[[1;5C940551 + 0 in total (QC-passed reads + QC-failed reads). 881297 + 0 primary. 0 + 0 secondary. 59254 + 0 supplementary. 0 + 0 duplicates. 0 + 0 primary duplicates. 940551 + 0 mapped (100.00% : N/A). 881297 + 0 primary mapped (100.00% : N/A). 0 + 0 paired in sequencing. 0 + 0 read1. 0 + 0 read2. 0 + 0 properly paired (N/A : N/A). 0 + 0 with itself and mate mapped. 0 + 0 singletons (N/A : N/A). 0 + 0 with mate mapped to a different chr. 0 + 0 with mate mapped to a different chr (mapQ>=5). ```. . **Steps to reproduce:**. - Command: Follow the [PACBIO example](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md) and install the mentioned singularity version. Then run . `singularity exec --bind /usr/lib/locale/,/media/nils/nils_ssd_01/Genomics_prac_guide/reference/unsorted/,/media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/ docker://google/deepvariant:1.5.0 /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /media/nils/nils_ssd_01/Genomics_prac_guide/reference/unsorted/hg19.fa --reads /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/GFX.bam --output_vcf /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/GFX.vcf.gz --num_shards 22 --dry_run=false --make_examples_extra_args='sort_by_haplotypes=false,parse_sam_aux_fields=false'`. - Error trace: . The error message including the pipeline call is :. . ```. E::idx_find_and_load] Could not retrieve index file for '/media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam/GFX.bam'. 2023-06-20 22:48:40.272832: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. 2023-06-20 22:48:40.272881: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:2912,interoperability,messag,message,2912," 0 with itself and mate mapped. 0 + 0 singletons (N/A : N/A). 0 + 0 with mate mapped to a different chr. 0 + 0 with mate mapped to a different chr (mapQ>=5). ```. . **Steps to reproduce:**. - Command: Follow the [PACBIO example](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md) and install the mentioned singularity version. Then run . `singularity exec --bind /usr/lib/locale/,/media/nils/nils_ssd_01/Genomics_prac_guide/reference/unsorted/,/media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/ docker://google/deepvariant:1.5.0 /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /media/nils/nils_ssd_01/Genomics_prac_guide/reference/unsorted/hg19.fa --reads /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/GFX.bam --output_vcf /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/GFX.vcf.gz --num_shards 22 --dry_run=false --make_examples_extra_args='sort_by_haplotypes=false,parse_sam_aux_fields=false'`. - Error trace: . The error message including the pipeline call is :. . ```. E::idx_find_and_load] Could not retrieve index file for '/media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam/GFX.bam'. 2023-06-20 22:48:40.272832: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. 2023-06-20 22:48:40.272881: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag BC: in RG line, ignoring: @RG	ID:408781da/26--26	PL:PACBIO	DS:READTYPE=CCS;BINDINGKIT=101-894-200;SEQUENCINGKIT=101-826-100;BASECALLERVERSION=5.0.0;FRAMERATEHZ=100.000000;BarcodeFile=m64023e_230515_162401.barcodes.fasta;BarcodeHash=86d73e586a6d3ede0295785b51105eea;BarcodeCount=96;BarcodeMode=Symmetric;BarcodeQuality=Score	LB:Pool_18_20_GFX0455704_GFX	PU:m64023e_230515_162401	SM:GFX. PM:SEQUELII	BC:TGACTGTAGCGAGTAT	CM:S/P5-C2/5.0-8M. 2023-06-20 22:48:40.272889: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG	ID:408781da/26--26	PL:PACBIO	DS:READTYPE=CCS;BI",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:3213,interoperability,coordinat,coordinate,3213,"odel-case-study.md) and install the mentioned singularity version. Then run . `singularity exec --bind /usr/lib/locale/,/media/nils/nils_ssd_01/Genomics_prac_guide/reference/unsorted/,/media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/ docker://google/deepvariant:1.5.0 /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /media/nils/nils_ssd_01/Genomics_prac_guide/reference/unsorted/hg19.fa --reads /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/GFX.bam --output_vcf /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/GFX.vcf.gz --num_shards 22 --dry_run=false --make_examples_extra_args='sort_by_haplotypes=false,parse_sam_aux_fields=false'`. - Error trace: . The error message including the pipeline call is :. . ```. E::idx_find_and_load] Could not retrieve index file for '/media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam/GFX.bam'. 2023-06-20 22:48:40.272832: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. 2023-06-20 22:48:40.272881: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag BC: in RG line, ignoring: @RG	ID:408781da/26--26	PL:PACBIO	DS:READTYPE=CCS;BINDINGKIT=101-894-200;SEQUENCINGKIT=101-826-100;BASECALLERVERSION=5.0.0;FRAMERATEHZ=100.000000;BarcodeFile=m64023e_230515_162401.barcodes.fasta;BarcodeHash=86d73e586a6d3ede0295785b51105eea;BarcodeCount=96;BarcodeMode=Symmetric;BarcodeQuality=Score	LB:Pool_18_20_GFX0455704_GFX	PU:m64023e_230515_162401	SM:GFX. PM:SEQUELII	BC:TGACTGTAGCGAGTAT	CM:S/P5-C2/5.0-8M. 2023-06-20 22:48:40.272889: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG	ID:408781da/26--26	PL:PACBIO	DS:READTYPE=CCS;BINDINGKIT=101-894-200;SEQUENCINGKIT=101-826-100;BASECALLERVERSION=5.0.0;FRAMERATEHZ=100.000000;BarcodeFile=m64023e_230515_162401.barcodes.fasta;BarcodeHash=86d73e586a6d3ede0295785b51105eea;BarcodeCount=96;BarcodeMode=Symmetric;BarcodeQuality=Score	LB:Pool_18_20_GFX0455704_GFX	PU:m64023e_230515_162401	S",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:3393,interoperability,BIND,BINDINGKIT,3393,"ed/,/media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/ docker://google/deepvariant:1.5.0 /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /media/nils/nils_ssd_01/Genomics_prac_guide/reference/unsorted/hg19.fa --reads /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/GFX.bam --output_vcf /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/GFX.vcf.gz --num_shards 22 --dry_run=false --make_examples_extra_args='sort_by_haplotypes=false,parse_sam_aux_fields=false'`. - Error trace: . The error message including the pipeline call is :. . ```. E::idx_find_and_load] Could not retrieve index file for '/media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam/GFX.bam'. 2023-06-20 22:48:40.272832: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. 2023-06-20 22:48:40.272881: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag BC: in RG line, ignoring: @RG	ID:408781da/26--26	PL:PACBIO	DS:READTYPE=CCS;BINDINGKIT=101-894-200;SEQUENCINGKIT=101-826-100;BASECALLERVERSION=5.0.0;FRAMERATEHZ=100.000000;BarcodeFile=m64023e_230515_162401.barcodes.fasta;BarcodeHash=86d73e586a6d3ede0295785b51105eea;BarcodeCount=96;BarcodeMode=Symmetric;BarcodeQuality=Score	LB:Pool_18_20_GFX0455704_GFX	PU:m64023e_230515_162401	SM:GFX. PM:SEQUELII	BC:TGACTGTAGCGAGTAT	CM:S/P5-C2/5.0-8M. 2023-06-20 22:48:40.272889: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG	ID:408781da/26--26	PL:PACBIO	DS:READTYPE=CCS;BINDINGKIT=101-894-200;SEQUENCINGKIT=101-826-100;BASECALLERVERSION=5.0.0;FRAMERATEHZ=100.000000;BarcodeFile=m64023e_230515_162401.barcodes.fasta;BarcodeHash=86d73e586a6d3ede0295785b51105eea;BarcodeCount=96;BarcodeMode=Symmetric;BarcodeQuality=Score	LB:Pool_18_20_GFX0455704_GFX	PU:m64023e_230515_162401	SM:GFX. PM:SEQUELII	BC:TGACTGTAGCGAGTAT	CM:S/P5-C2/5.0-8M. I0620 22:48:40.273113 140449601988416 genomics_reader.py:222] Reading /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:3914,interoperability,BIND,BINDINGKIT,3914," including the pipeline call is :. . ```. E::idx_find_and_load] Could not retrieve index file for '/media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam/GFX.bam'. 2023-06-20 22:48:40.272832: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. 2023-06-20 22:48:40.272881: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag BC: in RG line, ignoring: @RG	ID:408781da/26--26	PL:PACBIO	DS:READTYPE=CCS;BINDINGKIT=101-894-200;SEQUENCINGKIT=101-826-100;BASECALLERVERSION=5.0.0;FRAMERATEHZ=100.000000;BarcodeFile=m64023e_230515_162401.barcodes.fasta;BarcodeHash=86d73e586a6d3ede0295785b51105eea;BarcodeCount=96;BarcodeMode=Symmetric;BarcodeQuality=Score	LB:Pool_18_20_GFX0455704_GFX	PU:m64023e_230515_162401	SM:GFX. PM:SEQUELII	BC:TGACTGTAGCGAGTAT	CM:S/P5-C2/5.0-8M. 2023-06-20 22:48:40.272889: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG	ID:408781da/26--26	PL:PACBIO	DS:READTYPE=CCS;BINDINGKIT=101-894-200;SEQUENCINGKIT=101-826-100;BASECALLERVERSION=5.0.0;FRAMERATEHZ=100.000000;BarcodeFile=m64023e_230515_162401.barcodes.fasta;BarcodeHash=86d73e586a6d3ede0295785b51105eea;BarcodeCount=96;BarcodeMode=Symmetric;BarcodeQuality=Score	LB:Pool_18_20_GFX0455704_GFX	PU:m64023e_230515_162401	SM:GFX. PM:SEQUELII	BC:TGACTGTAGCGAGTAT	CM:S/P5-C2/5.0-8M. I0620 22:48:40.273113 140449601988416 genomics_reader.py:222] Reading /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam/GFX.bam with NativeSamReader. I0620 22:48:40.273825 140449601988416 make_examples_core.py:257] Task 8/22: Writing examples to /tmp/tmpwjk24y8t/make_examples.tfrecord-00008-of-00022.gz. I0620 22:48:40.274013 140449601988416 make_examples_core.py:257] Task 8/22: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfil",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:31,modifiability,Pac,PacBio,31,"Deepvariant fails to recognize PacBio BAM index file; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. Yes. . **Describe the issue:**. I followed the [PACBIO example](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md) and also added the flags from [this issue](https://github.com/google/deepvariant/issues/458). . . This error indicates that Deepvariant is not able to find an index file for the bam file but the index file is there see :. . ``` . (base) ✔ /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam[master L|…5] . 22:08 $ ls. GFX.bam GFX.bam.pbi GFX_hg19.bam GFX_hg19.bam.pbi readlength.txt tmp. ```. I also re-indexd the file using `pbindex` from [pbbam](https://github.com/pacificbiosciences/pbbam/). As one can see from the `ls` output I also tried to realign the bam file to some other reference panel using [pbmm2](https://github.com/PacificBiosciences/pbmm2/). . **Setup**. - Operating system: Linux Mint 21.1 x86_64 . - Kernel: 5.15.0-69-generic . - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): Docker image run through Singularity. - Singularity Verion : singularity-ce version 3.11.3. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) PACBIO CCS data aligned to GRCh37.fa reference genome. No special observations in the file can be reported. . ```. (base) ✔ /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam [master L|…5] . 20:46 $ samtools flagstat GFX.bam . ^[[1;5C940551 + 0 in total (QC-passed reads + QC-failed reads). 881297 + 0 primary. 0 + 0 secondary. 59254 + 0 supplementary. 0 + 0 duplicates. 0 + 0 primary duplicates. 940551 + 0 mapped (100.00% : N/A). 881297 + 0 primary mapped (100.00% : N/A). 0 + 0 paired in sequencing. 0 + 0 read1. 0 + 0 read2. 0 + 0 properly paired (N/A : N/A). 0 + 0 with itself and mate mapped. 0 + 0 singletons (N/A : N/A). 0 + 0 with mate mapped",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:194,modifiability,PAC,PACBIO,194,"Deepvariant fails to recognize PacBio BAM index file; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. Yes. . **Describe the issue:**. I followed the [PACBIO example](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md) and also added the flags from [this issue](https://github.com/google/deepvariant/issues/458). . . This error indicates that Deepvariant is not able to find an index file for the bam file but the index file is there see :. . ``` . (base) ✔ /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam[master L|…5] . 22:08 $ ls. GFX.bam GFX.bam.pbi GFX_hg19.bam GFX_hg19.bam.pbi readlength.txt tmp. ```. I also re-indexd the file using `pbindex` from [pbbam](https://github.com/pacificbiosciences/pbbam/). As one can see from the `ls` output I also tried to realign the bam file to some other reference panel using [pbmm2](https://github.com/PacificBiosciences/pbmm2/). . **Setup**. - Operating system: Linux Mint 21.1 x86_64 . - Kernel: 5.15.0-69-generic . - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): Docker image run through Singularity. - Singularity Verion : singularity-ce version 3.11.3. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) PACBIO CCS data aligned to GRCh37.fa reference genome. No special observations in the file can be reported. . ```. (base) ✔ /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam [master L|…5] . 20:46 $ samtools flagstat GFX.bam . ^[[1;5C940551 + 0 in total (QC-passed reads + QC-failed reads). 881297 + 0 primary. 0 + 0 secondary. 59254 + 0 supplementary. 0 + 0 duplicates. 0 + 0 primary duplicates. 940551 + 0 mapped (100.00% : N/A). 881297 + 0 primary mapped (100.00% : N/A). 0 + 0 paired in sequencing. 0 + 0 read1. 0 + 0 read2. 0 + 0 properly paired (N/A : N/A). 0 + 0 with itself and mate mapped. 0 + 0 singletons (N/A : N/A). 0 + 0 with mate mapped",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:275,modifiability,pac,pacbio-model-case-study,275,"Deepvariant fails to recognize PacBio BAM index file; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. Yes. . **Describe the issue:**. I followed the [PACBIO example](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md) and also added the flags from [this issue](https://github.com/google/deepvariant/issues/458). . . This error indicates that Deepvariant is not able to find an index file for the bam file but the index file is there see :. . ``` . (base) ✔ /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam[master L|…5] . 22:08 $ ls. GFX.bam GFX.bam.pbi GFX_hg19.bam GFX_hg19.bam.pbi readlength.txt tmp. ```. I also re-indexd the file using `pbindex` from [pbbam](https://github.com/pacificbiosciences/pbbam/). As one can see from the `ls` output I also tried to realign the bam file to some other reference panel using [pbmm2](https://github.com/PacificBiosciences/pbmm2/). . **Setup**. - Operating system: Linux Mint 21.1 x86_64 . - Kernel: 5.15.0-69-generic . - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): Docker image run through Singularity. - Singularity Verion : singularity-ce version 3.11.3. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) PACBIO CCS data aligned to GRCh37.fa reference genome. No special observations in the file can be reported. . ```. (base) ✔ /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam [master L|…5] . 20:46 $ samtools flagstat GFX.bam . ^[[1;5C940551 + 0 in total (QC-passed reads + QC-failed reads). 881297 + 0 primary. 0 + 0 secondary. 59254 + 0 supplementary. 0 + 0 duplicates. 0 + 0 primary duplicates. 940551 + 0 mapped (100.00% : N/A). 881297 + 0 primary mapped (100.00% : N/A). 0 + 0 paired in sequencing. 0 + 0 read1. 0 + 0 read2. 0 + 0 properly paired (N/A : N/A). 0 + 0 with itself and mate mapped. 0 + 0 singletons (N/A : N/A). 0 + 0 with mate mapped",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:775,modifiability,pac,pacificbiosciences,775,"Deepvariant fails to recognize PacBio BAM index file; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. Yes. . **Describe the issue:**. I followed the [PACBIO example](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md) and also added the flags from [this issue](https://github.com/google/deepvariant/issues/458). . . This error indicates that Deepvariant is not able to find an index file for the bam file but the index file is there see :. . ``` . (base) ✔ /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam[master L|…5] . 22:08 $ ls. GFX.bam GFX.bam.pbi GFX_hg19.bam GFX_hg19.bam.pbi readlength.txt tmp. ```. I also re-indexd the file using `pbindex` from [pbbam](https://github.com/pacificbiosciences/pbbam/). As one can see from the `ls` output I also tried to realign the bam file to some other reference panel using [pbmm2](https://github.com/PacificBiosciences/pbmm2/). . **Setup**. - Operating system: Linux Mint 21.1 x86_64 . - Kernel: 5.15.0-69-generic . - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): Docker image run through Singularity. - Singularity Verion : singularity-ce version 3.11.3. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) PACBIO CCS data aligned to GRCh37.fa reference genome. No special observations in the file can be reported. . ```. (base) ✔ /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam [master L|…5] . 20:46 $ samtools flagstat GFX.bam . ^[[1;5C940551 + 0 in total (QC-passed reads + QC-failed reads). 881297 + 0 primary. 0 + 0 secondary. 59254 + 0 supplementary. 0 + 0 duplicates. 0 + 0 primary duplicates. 940551 + 0 mapped (100.00% : N/A). 881297 + 0 primary mapped (100.00% : N/A). 0 + 0 paired in sequencing. 0 + 0 read1. 0 + 0 read2. 0 + 0 properly paired (N/A : N/A). 0 + 0 with itself and mate mapped. 0 + 0 singletons (N/A : N/A). 0 + 0 with mate mapped",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:939,modifiability,Pac,PacificBiosciences,939,"Deepvariant fails to recognize PacBio BAM index file; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. Yes. . **Describe the issue:**. I followed the [PACBIO example](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md) and also added the flags from [this issue](https://github.com/google/deepvariant/issues/458). . . This error indicates that Deepvariant is not able to find an index file for the bam file but the index file is there see :. . ``` . (base) ✔ /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam[master L|…5] . 22:08 $ ls. GFX.bam GFX.bam.pbi GFX_hg19.bam GFX_hg19.bam.pbi readlength.txt tmp. ```. I also re-indexd the file using `pbindex` from [pbbam](https://github.com/pacificbiosciences/pbbam/). As one can see from the `ls` output I also tried to realign the bam file to some other reference panel using [pbmm2](https://github.com/PacificBiosciences/pbmm2/). . **Setup**. - Operating system: Linux Mint 21.1 x86_64 . - Kernel: 5.15.0-69-generic . - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): Docker image run through Singularity. - Singularity Verion : singularity-ce version 3.11.3. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) PACBIO CCS data aligned to GRCh37.fa reference genome. No special observations in the file can be reported. . ```. (base) ✔ /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam [master L|…5] . 20:46 $ samtools flagstat GFX.bam . ^[[1;5C940551 + 0 in total (QC-passed reads + QC-failed reads). 881297 + 0 primary. 0 + 0 secondary. 59254 + 0 supplementary. 0 + 0 duplicates. 0 + 0 primary duplicates. 940551 + 0 mapped (100.00% : N/A). 881297 + 0 primary mapped (100.00% : N/A). 0 + 0 paired in sequencing. 0 + 0 read1. 0 + 0 read2. 0 + 0 properly paired (N/A : N/A). 0 + 0 with itself and mate mapped. 0 + 0 singletons (N/A : N/A). 0 + 0 with mate mapped",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1069,modifiability,version,version,1069,"the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. Yes. . **Describe the issue:**. I followed the [PACBIO example](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md) and also added the flags from [this issue](https://github.com/google/deepvariant/issues/458). . . This error indicates that Deepvariant is not able to find an index file for the bam file but the index file is there see :. . ``` . (base) ✔ /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam[master L|…5] . 22:08 $ ls. GFX.bam GFX.bam.pbi GFX_hg19.bam GFX_hg19.bam.pbi readlength.txt tmp. ```. I also re-indexd the file using `pbindex` from [pbbam](https://github.com/pacificbiosciences/pbbam/). As one can see from the `ls` output I also tried to realign the bam file to some other reference panel using [pbmm2](https://github.com/PacificBiosciences/pbmm2/). . **Setup**. - Operating system: Linux Mint 21.1 x86_64 . - Kernel: 5.15.0-69-generic . - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): Docker image run through Singularity. - Singularity Verion : singularity-ce version 3.11.3. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) PACBIO CCS data aligned to GRCh37.fa reference genome. No special observations in the file can be reported. . ```. (base) ✔ /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam [master L|…5] . 20:46 $ samtools flagstat GFX.bam . ^[[1;5C940551 + 0 in total (QC-passed reads + QC-failed reads). 881297 + 0 primary. 0 + 0 secondary. 59254 + 0 supplementary. 0 + 0 duplicates. 0 + 0 primary duplicates. 940551 + 0 mapped (100.00% : N/A). 881297 + 0 primary mapped (100.00% : N/A). 0 + 0 paired in sequencing. 0 + 0 read1. 0 + 0 read2. 0 + 0 properly paired (N/A : N/A). 0 + 0 with itself and mate mapped. 0 + 0 singletons (N/A : N/A). 0 + 0 with mate mapped to a different chr. 0 + 0 with mate mapped to a different chr (mapQ>=5).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1218,modifiability,version,version,1218,"ub.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md) and also added the flags from [this issue](https://github.com/google/deepvariant/issues/458). . . This error indicates that Deepvariant is not able to find an index file for the bam file but the index file is there see :. . ``` . (base) ✔ /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam[master L|…5] . 22:08 $ ls. GFX.bam GFX.bam.pbi GFX_hg19.bam GFX_hg19.bam.pbi readlength.txt tmp. ```. I also re-indexd the file using `pbindex` from [pbbam](https://github.com/pacificbiosciences/pbbam/). As one can see from the `ls` output I also tried to realign the bam file to some other reference panel using [pbmm2](https://github.com/PacificBiosciences/pbmm2/). . **Setup**. - Operating system: Linux Mint 21.1 x86_64 . - Kernel: 5.15.0-69-generic . - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): Docker image run through Singularity. - Singularity Verion : singularity-ce version 3.11.3. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) PACBIO CCS data aligned to GRCh37.fa reference genome. No special observations in the file can be reported. . ```. (base) ✔ /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam [master L|…5] . 20:46 $ samtools flagstat GFX.bam . ^[[1;5C940551 + 0 in total (QC-passed reads + QC-failed reads). 881297 + 0 primary. 0 + 0 secondary. 59254 + 0 supplementary. 0 + 0 duplicates. 0 + 0 primary duplicates. 940551 + 0 mapped (100.00% : N/A). 881297 + 0 primary mapped (100.00% : N/A). 0 + 0 paired in sequencing. 0 + 0 read1. 0 + 0 read2. 0 + 0 properly paired (N/A : N/A). 0 + 0 with itself and mate mapped. 0 + 0 singletons (N/A : N/A). 0 + 0 with mate mapped to a different chr. 0 + 0 with mate mapped to a different chr (mapQ>=5). ```. . **Steps to reproduce:**. - Command: Follow the [PACBIO example](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1343,modifiability,PAC,PACBIO,1343,"https://github.com/google/deepvariant/issues/458). . . This error indicates that Deepvariant is not able to find an index file for the bam file but the index file is there see :. . ``` . (base) ✔ /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam[master L|…5] . 22:08 $ ls. GFX.bam GFX.bam.pbi GFX_hg19.bam GFX_hg19.bam.pbi readlength.txt tmp. ```. I also re-indexd the file using `pbindex` from [pbbam](https://github.com/pacificbiosciences/pbbam/). As one can see from the `ls` output I also tried to realign the bam file to some other reference panel using [pbmm2](https://github.com/PacificBiosciences/pbmm2/). . **Setup**. - Operating system: Linux Mint 21.1 x86_64 . - Kernel: 5.15.0-69-generic . - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): Docker image run through Singularity. - Singularity Verion : singularity-ce version 3.11.3. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) PACBIO CCS data aligned to GRCh37.fa reference genome. No special observations in the file can be reported. . ```. (base) ✔ /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam [master L|…5] . 20:46 $ samtools flagstat GFX.bam . ^[[1;5C940551 + 0 in total (QC-passed reads + QC-failed reads). 881297 + 0 primary. 0 + 0 secondary. 59254 + 0 supplementary. 0 + 0 duplicates. 0 + 0 primary duplicates. 940551 + 0 mapped (100.00% : N/A). 881297 + 0 primary mapped (100.00% : N/A). 0 + 0 paired in sequencing. 0 + 0 read1. 0 + 0 read2. 0 + 0 properly paired (N/A : N/A). 0 + 0 with itself and mate mapped. 0 + 0 singletons (N/A : N/A). 0 + 0 with mate mapped to a different chr. 0 + 0 with mate mapped to a different chr (mapQ>=5). ```. . **Steps to reproduce:**. - Command: Follow the [PACBIO example](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md) and install the mentioned singularity version. Then run . `singularity exec --bind /usr/lib/locale/,/media/n",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:2129,modifiability,PAC,PACBIO,2129,"e, etc.): Docker image run through Singularity. - Singularity Verion : singularity-ce version 3.11.3. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) PACBIO CCS data aligned to GRCh37.fa reference genome. No special observations in the file can be reported. . ```. (base) ✔ /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam [master L|…5] . 20:46 $ samtools flagstat GFX.bam . ^[[1;5C940551 + 0 in total (QC-passed reads + QC-failed reads). 881297 + 0 primary. 0 + 0 secondary. 59254 + 0 supplementary. 0 + 0 duplicates. 0 + 0 primary duplicates. 940551 + 0 mapped (100.00% : N/A). 881297 + 0 primary mapped (100.00% : N/A). 0 + 0 paired in sequencing. 0 + 0 read1. 0 + 0 read2. 0 + 0 properly paired (N/A : N/A). 0 + 0 with itself and mate mapped. 0 + 0 singletons (N/A : N/A). 0 + 0 with mate mapped to a different chr. 0 + 0 with mate mapped to a different chr (mapQ>=5). ```. . **Steps to reproduce:**. - Command: Follow the [PACBIO example](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md) and install the mentioned singularity version. Then run . `singularity exec --bind /usr/lib/locale/,/media/nils/nils_ssd_01/Genomics_prac_guide/reference/unsorted/,/media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/ docker://google/deepvariant:1.5.0 /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /media/nils/nils_ssd_01/Genomics_prac_guide/reference/unsorted/hg19.fa --reads /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/GFX.bam --output_vcf /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/GFX.vcf.gz --num_shards 22 --dry_run=false --make_examples_extra_args='sort_by_haplotypes=false,parse_sam_aux_fields=false'`. - Error trace: . The error message including the pipeline call is :. . ```. E::idx_find_and_load] Could not retrieve index file for '/media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam/GFX.bam'. 2023-06-20 22:48:40.272832: W third_party/nucle",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:2210,modifiability,pac,pacbio-model-case-study,2210,"ion 3.11.3. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) PACBIO CCS data aligned to GRCh37.fa reference genome. No special observations in the file can be reported. . ```. (base) ✔ /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam [master L|…5] . 20:46 $ samtools flagstat GFX.bam . ^[[1;5C940551 + 0 in total (QC-passed reads + QC-failed reads). 881297 + 0 primary. 0 + 0 secondary. 59254 + 0 supplementary. 0 + 0 duplicates. 0 + 0 primary duplicates. 940551 + 0 mapped (100.00% : N/A). 881297 + 0 primary mapped (100.00% : N/A). 0 + 0 paired in sequencing. 0 + 0 read1. 0 + 0 read2. 0 + 0 properly paired (N/A : N/A). 0 + 0 with itself and mate mapped. 0 + 0 singletons (N/A : N/A). 0 + 0 with mate mapped to a different chr. 0 + 0 with mate mapped to a different chr (mapQ>=5). ```. . **Steps to reproduce:**. - Command: Follow the [PACBIO example](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md) and install the mentioned singularity version. Then run . `singularity exec --bind /usr/lib/locale/,/media/nils/nils_ssd_01/Genomics_prac_guide/reference/unsorted/,/media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/ docker://google/deepvariant:1.5.0 /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /media/nils/nils_ssd_01/Genomics_prac_guide/reference/unsorted/hg19.fa --reads /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/GFX.bam --output_vcf /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/GFX.vcf.gz --num_shards 22 --dry_run=false --make_examples_extra_args='sort_by_haplotypes=false,parse_sam_aux_fields=false'`. - Error trace: . The error message including the pipeline call is :. . ```. E::idx_find_and_load] Could not retrieve index file for '/media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam/GFX.bam'. 2023-06-20 22:48:40.272832: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinat",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:2276,modifiability,version,version,2276,"nce genome, anything special that is unlike the case studies?) PACBIO CCS data aligned to GRCh37.fa reference genome. No special observations in the file can be reported. . ```. (base) ✔ /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam [master L|…5] . 20:46 $ samtools flagstat GFX.bam . ^[[1;5C940551 + 0 in total (QC-passed reads + QC-failed reads). 881297 + 0 primary. 0 + 0 secondary. 59254 + 0 supplementary. 0 + 0 duplicates. 0 + 0 primary duplicates. 940551 + 0 mapped (100.00% : N/A). 881297 + 0 primary mapped (100.00% : N/A). 0 + 0 paired in sequencing. 0 + 0 read1. 0 + 0 read2. 0 + 0 properly paired (N/A : N/A). 0 + 0 with itself and mate mapped. 0 + 0 singletons (N/A : N/A). 0 + 0 with mate mapped to a different chr. 0 + 0 with mate mapped to a different chr (mapQ>=5). ```. . **Steps to reproduce:**. - Command: Follow the [PACBIO example](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md) and install the mentioned singularity version. Then run . `singularity exec --bind /usr/lib/locale/,/media/nils/nils_ssd_01/Genomics_prac_guide/reference/unsorted/,/media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/ docker://google/deepvariant:1.5.0 /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /media/nils/nils_ssd_01/Genomics_prac_guide/reference/unsorted/hg19.fa --reads /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/GFX.bam --output_vcf /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/GFX.vcf.gz --num_shards 22 --dry_run=false --make_examples_extra_args='sort_by_haplotypes=false,parse_sam_aux_fields=false'`. - Error trace: . The error message including the pipeline call is :. . ```. E::idx_find_and_load] Could not retrieve index file for '/media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam/GFX.bam'. 2023-06-20 22:48:40.272832: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. 2023-06-20 22:48:40.272881: W third_party/nucl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:2316,modifiability,bind,bind,2316,"nlike the case studies?) PACBIO CCS data aligned to GRCh37.fa reference genome. No special observations in the file can be reported. . ```. (base) ✔ /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam [master L|…5] . 20:46 $ samtools flagstat GFX.bam . ^[[1;5C940551 + 0 in total (QC-passed reads + QC-failed reads). 881297 + 0 primary. 0 + 0 secondary. 59254 + 0 supplementary. 0 + 0 duplicates. 0 + 0 primary duplicates. 940551 + 0 mapped (100.00% : N/A). 881297 + 0 primary mapped (100.00% : N/A). 0 + 0 paired in sequencing. 0 + 0 read1. 0 + 0 read2. 0 + 0 properly paired (N/A : N/A). 0 + 0 with itself and mate mapped. 0 + 0 singletons (N/A : N/A). 0 + 0 with mate mapped to a different chr. 0 + 0 with mate mapped to a different chr (mapQ>=5). ```. . **Steps to reproduce:**. - Command: Follow the [PACBIO example](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md) and install the mentioned singularity version. Then run . `singularity exec --bind /usr/lib/locale/,/media/nils/nils_ssd_01/Genomics_prac_guide/reference/unsorted/,/media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/ docker://google/deepvariant:1.5.0 /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /media/nils/nils_ssd_01/Genomics_prac_guide/reference/unsorted/hg19.fa --reads /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/GFX.bam --output_vcf /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/GFX.vcf.gz --num_shards 22 --dry_run=false --make_examples_extra_args='sort_by_haplotypes=false,parse_sam_aux_fields=false'`. - Error trace: . The error message including the pipeline call is :. . ```. E::idx_find_and_load] Could not retrieve index file for '/media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam/GFX.bam'. 2023-06-20 22:48:40.272832: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. 2023-06-20 22:48:40.272881: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:2540,modifiability,PAC,PACBIO,2540,"46 $ samtools flagstat GFX.bam . ^[[1;5C940551 + 0 in total (QC-passed reads + QC-failed reads). 881297 + 0 primary. 0 + 0 secondary. 59254 + 0 supplementary. 0 + 0 duplicates. 0 + 0 primary duplicates. 940551 + 0 mapped (100.00% : N/A). 881297 + 0 primary mapped (100.00% : N/A). 0 + 0 paired in sequencing. 0 + 0 read1. 0 + 0 read2. 0 + 0 properly paired (N/A : N/A). 0 + 0 with itself and mate mapped. 0 + 0 singletons (N/A : N/A). 0 + 0 with mate mapped to a different chr. 0 + 0 with mate mapped to a different chr (mapQ>=5). ```. . **Steps to reproduce:**. - Command: Follow the [PACBIO example](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md) and install the mentioned singularity version. Then run . `singularity exec --bind /usr/lib/locale/,/media/nils/nils_ssd_01/Genomics_prac_guide/reference/unsorted/,/media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/ docker://google/deepvariant:1.5.0 /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /media/nils/nils_ssd_01/Genomics_prac_guide/reference/unsorted/hg19.fa --reads /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/GFX.bam --output_vcf /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/GFX.vcf.gz --num_shards 22 --dry_run=false --make_examples_extra_args='sort_by_haplotypes=false,parse_sam_aux_fields=false'`. - Error trace: . The error message including the pipeline call is :. . ```. E::idx_find_and_load] Could not retrieve index file for '/media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam/GFX.bam'. 2023-06-20 22:48:40.272832: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. 2023-06-20 22:48:40.272881: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag BC: in RG line, ignoring: @RG	ID:408781da/26--26	PL:PACBIO	DS:READTYPE=CCS;BINDINGKIT=101-894-200;SEQUENCINGKIT=101-826-100;BASECALLERVERSION=5.0.0;FRAMERATEHZ=100.000000;BarcodeFile=m64023e_230515_162401.barcodes.fasta;Barco",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:3370,modifiability,PAC,PACBIO,3370,"ac_guide/reference/unsorted/,/media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/ docker://google/deepvariant:1.5.0 /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /media/nils/nils_ssd_01/Genomics_prac_guide/reference/unsorted/hg19.fa --reads /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/GFX.bam --output_vcf /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/GFX.vcf.gz --num_shards 22 --dry_run=false --make_examples_extra_args='sort_by_haplotypes=false,parse_sam_aux_fields=false'`. - Error trace: . The error message including the pipeline call is :. . ```. E::idx_find_and_load] Could not retrieve index file for '/media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam/GFX.bam'. 2023-06-20 22:48:40.272832: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. 2023-06-20 22:48:40.272881: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag BC: in RG line, ignoring: @RG	ID:408781da/26--26	PL:PACBIO	DS:READTYPE=CCS;BINDINGKIT=101-894-200;SEQUENCINGKIT=101-826-100;BASECALLERVERSION=5.0.0;FRAMERATEHZ=100.000000;BarcodeFile=m64023e_230515_162401.barcodes.fasta;BarcodeHash=86d73e586a6d3ede0295785b51105eea;BarcodeCount=96;BarcodeMode=Symmetric;BarcodeQuality=Score	LB:Pool_18_20_GFX0455704_GFX	PU:m64023e_230515_162401	SM:GFX. PM:SEQUELII	BC:TGACTGTAGCGAGTAT	CM:S/P5-C2/5.0-8M. 2023-06-20 22:48:40.272889: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG	ID:408781da/26--26	PL:PACBIO	DS:READTYPE=CCS;BINDINGKIT=101-894-200;SEQUENCINGKIT=101-826-100;BASECALLERVERSION=5.0.0;FRAMERATEHZ=100.000000;BarcodeFile=m64023e_230515_162401.barcodes.fasta;BarcodeHash=86d73e586a6d3ede0295785b51105eea;BarcodeCount=96;BarcodeMode=Symmetric;BarcodeQuality=Score	LB:Pool_18_20_GFX0455704_GFX	PU:m64023e_230515_162401	SM:GFX. PM:SEQUELII	BC:TGACTGTAGCGAGTAT	CM:S/P5-C2/5.0-8M. I0620 22:48:40.273113 140449601988416 genomics_reader.py:222] Reading /media/nils/nils_ssd_01/Cal",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:3393,modifiability,BIND,BINDINGKIT,3393,"ed/,/media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/ docker://google/deepvariant:1.5.0 /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /media/nils/nils_ssd_01/Genomics_prac_guide/reference/unsorted/hg19.fa --reads /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/GFX.bam --output_vcf /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/GFX.vcf.gz --num_shards 22 --dry_run=false --make_examples_extra_args='sort_by_haplotypes=false,parse_sam_aux_fields=false'`. - Error trace: . The error message including the pipeline call is :. . ```. E::idx_find_and_load] Could not retrieve index file for '/media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam/GFX.bam'. 2023-06-20 22:48:40.272832: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. 2023-06-20 22:48:40.272881: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag BC: in RG line, ignoring: @RG	ID:408781da/26--26	PL:PACBIO	DS:READTYPE=CCS;BINDINGKIT=101-894-200;SEQUENCINGKIT=101-826-100;BASECALLERVERSION=5.0.0;FRAMERATEHZ=100.000000;BarcodeFile=m64023e_230515_162401.barcodes.fasta;BarcodeHash=86d73e586a6d3ede0295785b51105eea;BarcodeCount=96;BarcodeMode=Symmetric;BarcodeQuality=Score	LB:Pool_18_20_GFX0455704_GFX	PU:m64023e_230515_162401	SM:GFX. PM:SEQUELII	BC:TGACTGTAGCGAGTAT	CM:S/P5-C2/5.0-8M. 2023-06-20 22:48:40.272889: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG	ID:408781da/26--26	PL:PACBIO	DS:READTYPE=CCS;BINDINGKIT=101-894-200;SEQUENCINGKIT=101-826-100;BASECALLERVERSION=5.0.0;FRAMERATEHZ=100.000000;BarcodeFile=m64023e_230515_162401.barcodes.fasta;BarcodeHash=86d73e586a6d3ede0295785b51105eea;BarcodeCount=96;BarcodeMode=Symmetric;BarcodeQuality=Score	LB:Pool_18_20_GFX0455704_GFX	PU:m64023e_230515_162401	SM:GFX. PM:SEQUELII	BC:TGACTGTAGCGAGTAT	CM:S/P5-C2/5.0-8M. I0620 22:48:40.273113 140449601988416 genomics_reader.py:222] Reading /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:3891,modifiability,PAC,PACBIO,3891,"race: . The error message including the pipeline call is :. . ```. E::idx_find_and_load] Could not retrieve index file for '/media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam/GFX.bam'. 2023-06-20 22:48:40.272832: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. 2023-06-20 22:48:40.272881: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag BC: in RG line, ignoring: @RG	ID:408781da/26--26	PL:PACBIO	DS:READTYPE=CCS;BINDINGKIT=101-894-200;SEQUENCINGKIT=101-826-100;BASECALLERVERSION=5.0.0;FRAMERATEHZ=100.000000;BarcodeFile=m64023e_230515_162401.barcodes.fasta;BarcodeHash=86d73e586a6d3ede0295785b51105eea;BarcodeCount=96;BarcodeMode=Symmetric;BarcodeQuality=Score	LB:Pool_18_20_GFX0455704_GFX	PU:m64023e_230515_162401	SM:GFX. PM:SEQUELII	BC:TGACTGTAGCGAGTAT	CM:S/P5-C2/5.0-8M. 2023-06-20 22:48:40.272889: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG	ID:408781da/26--26	PL:PACBIO	DS:READTYPE=CCS;BINDINGKIT=101-894-200;SEQUENCINGKIT=101-826-100;BASECALLERVERSION=5.0.0;FRAMERATEHZ=100.000000;BarcodeFile=m64023e_230515_162401.barcodes.fasta;BarcodeHash=86d73e586a6d3ede0295785b51105eea;BarcodeCount=96;BarcodeMode=Symmetric;BarcodeQuality=Score	LB:Pool_18_20_GFX0455704_GFX	PU:m64023e_230515_162401	SM:GFX. PM:SEQUELII	BC:TGACTGTAGCGAGTAT	CM:S/P5-C2/5.0-8M. I0620 22:48:40.273113 140449601988416 genomics_reader.py:222] Reading /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam/GFX.bam with NativeSamReader. I0620 22:48:40.273825 140449601988416 make_examples_core.py:257] Task 8/22: Writing examples to /tmp/tmpwjk24y8t/make_examples.tfrecord-00008-of-00022.gz. I0620 22:48:40.274013 140449601988416 make_examples_core.py:257] Task 8/22: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:3914,modifiability,BIND,BINDINGKIT,3914," including the pipeline call is :. . ```. E::idx_find_and_load] Could not retrieve index file for '/media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam/GFX.bam'. 2023-06-20 22:48:40.272832: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. 2023-06-20 22:48:40.272881: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag BC: in RG line, ignoring: @RG	ID:408781da/26--26	PL:PACBIO	DS:READTYPE=CCS;BINDINGKIT=101-894-200;SEQUENCINGKIT=101-826-100;BASECALLERVERSION=5.0.0;FRAMERATEHZ=100.000000;BarcodeFile=m64023e_230515_162401.barcodes.fasta;BarcodeHash=86d73e586a6d3ede0295785b51105eea;BarcodeCount=96;BarcodeMode=Symmetric;BarcodeQuality=Score	LB:Pool_18_20_GFX0455704_GFX	PU:m64023e_230515_162401	SM:GFX. PM:SEQUELII	BC:TGACTGTAGCGAGTAT	CM:S/P5-C2/5.0-8M. 2023-06-20 22:48:40.272889: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG	ID:408781da/26--26	PL:PACBIO	DS:READTYPE=CCS;BINDINGKIT=101-894-200;SEQUENCINGKIT=101-826-100;BASECALLERVERSION=5.0.0;FRAMERATEHZ=100.000000;BarcodeFile=m64023e_230515_162401.barcodes.fasta;BarcodeHash=86d73e586a6d3ede0295785b51105eea;BarcodeCount=96;BarcodeMode=Symmetric;BarcodeQuality=Score	LB:Pool_18_20_GFX0455704_GFX	PU:m64023e_230515_162401	SM:GFX. PM:SEQUELII	BC:TGACTGTAGCGAGTAT	CM:S/P5-C2/5.0-8M. I0620 22:48:40.273113 140449601988416 genomics_reader.py:222] Reading /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam/GFX.bam with NativeSamReader. I0620 22:48:40.273825 140449601988416 make_examples_core.py:257] Task 8/22: Writing examples to /tmp/tmpwjk24y8t/make_examples.tfrecord-00008-of-00022.gz. I0620 22:48:40.274013 140449601988416 make_examples_core.py:257] Task 8/22: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfil",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:4854,modifiability,modul,module,4854,"noring: @RG	ID:408781da/26--26	PL:PACBIO	DS:READTYPE=CCS;BINDINGKIT=101-894-200;SEQUENCINGKIT=101-826-100;BASECALLERVERSION=5.0.0;FRAMERATEHZ=100.000000;BarcodeFile=m64023e_230515_162401.barcodes.fasta;BarcodeHash=86d73e586a6d3ede0295785b51105eea;BarcodeCount=96;BarcodeMode=Symmetric;BarcodeQuality=Score	LB:Pool_18_20_GFX0455704_GFX	PU:m64023e_230515_162401	SM:GFX. PM:SEQUELII	BC:TGACTGTAGCGAGTAT	CM:S/P5-C2/5.0-8M. I0620 22:48:40.273113 140449601988416 genomics_reader.py:222] Reading /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam/GFX.bam with NativeSamReader. I0620 22:48:40.273825 140449601988416 make_examples_core.py:257] Task 8/22: Writing examples to /tmp/tmpwjk24y8t/make_examples.tfrecord-00008-of-00022.gz. I0620 22:48:40.274013 140449601988416 make_examples_core.py:257] Task 8/22: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2183, in make_examples_runner. runtimes) = region_processor.process(region). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1285, in process. sample_reads = self.region_reads_norealign(. File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1376, in region_reads_norealign. reads = itertools.chain(reads, sam_reader.query(region)). File ""/tmp/Bazel.runfiles_ztv_d7ra/run",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:406,performance,error,error,406,"Deepvariant fails to recognize PacBio BAM index file; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. Yes. . **Describe the issue:**. I followed the [PACBIO example](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md) and also added the flags from [this issue](https://github.com/google/deepvariant/issues/458). . . This error indicates that Deepvariant is not able to find an index file for the bam file but the index file is there see :. . ``` . (base) ✔ /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam[master L|…5] . 22:08 $ ls. GFX.bam GFX.bam.pbi GFX_hg19.bam GFX_hg19.bam.pbi readlength.txt tmp. ```. I also re-indexd the file using `pbindex` from [pbbam](https://github.com/pacificbiosciences/pbbam/). As one can see from the `ls` output I also tried to realign the bam file to some other reference panel using [pbmm2](https://github.com/PacificBiosciences/pbmm2/). . **Setup**. - Operating system: Linux Mint 21.1 x86_64 . - Kernel: 5.15.0-69-generic . - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): Docker image run through Singularity. - Singularity Verion : singularity-ce version 3.11.3. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) PACBIO CCS data aligned to GRCh37.fa reference genome. No special observations in the file can be reported. . ```. (base) ✔ /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam [master L|…5] . 20:46 $ samtools flagstat GFX.bam . ^[[1;5C940551 + 0 in total (QC-passed reads + QC-failed reads). 881297 + 0 primary. 0 + 0 secondary. 59254 + 0 supplementary. 0 + 0 duplicates. 0 + 0 primary duplicates. 940551 + 0 mapped (100.00% : N/A). 881297 + 0 primary mapped (100.00% : N/A). 0 + 0 paired in sequencing. 0 + 0 read1. 0 + 0 read2. 0 + 0 properly paired (N/A : N/A). 0 + 0 with itself and mate mapped. 0 + 0 singletons (N/A : N/A). 0 + 0 with mate mapped",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:2887,performance,Error,Error,2887,"ly paired (N/A : N/A). 0 + 0 with itself and mate mapped. 0 + 0 singletons (N/A : N/A). 0 + 0 with mate mapped to a different chr. 0 + 0 with mate mapped to a different chr (mapQ>=5). ```. . **Steps to reproduce:**. - Command: Follow the [PACBIO example](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md) and install the mentioned singularity version. Then run . `singularity exec --bind /usr/lib/locale/,/media/nils/nils_ssd_01/Genomics_prac_guide/reference/unsorted/,/media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/ docker://google/deepvariant:1.5.0 /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /media/nils/nils_ssd_01/Genomics_prac_guide/reference/unsorted/hg19.fa --reads /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/GFX.bam --output_vcf /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/GFX.vcf.gz --num_shards 22 --dry_run=false --make_examples_extra_args='sort_by_haplotypes=false,parse_sam_aux_fields=false'`. - Error trace: . The error message including the pipeline call is :. . ```. E::idx_find_and_load] Could not retrieve index file for '/media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam/GFX.bam'. 2023-06-20 22:48:40.272832: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. 2023-06-20 22:48:40.272881: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag BC: in RG line, ignoring: @RG	ID:408781da/26--26	PL:PACBIO	DS:READTYPE=CCS;BINDINGKIT=101-894-200;SEQUENCINGKIT=101-826-100;BASECALLERVERSION=5.0.0;FRAMERATEHZ=100.000000;BarcodeFile=m64023e_230515_162401.barcodes.fasta;BarcodeHash=86d73e586a6d3ede0295785b51105eea;BarcodeCount=96;BarcodeMode=Symmetric;BarcodeQuality=Score	LB:Pool_18_20_GFX0455704_GFX	PU:m64023e_230515_162401	SM:GFX. PM:SEQUELII	BC:TGACTGTAGCGAGTAT	CM:S/P5-C2/5.0-8M. 2023-06-20 22:48:40.272889: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG	ID:408781da/26--26	PL",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:2906,performance,error,error,2906,"A). 0 + 0 with itself and mate mapped. 0 + 0 singletons (N/A : N/A). 0 + 0 with mate mapped to a different chr. 0 + 0 with mate mapped to a different chr (mapQ>=5). ```. . **Steps to reproduce:**. - Command: Follow the [PACBIO example](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md) and install the mentioned singularity version. Then run . `singularity exec --bind /usr/lib/locale/,/media/nils/nils_ssd_01/Genomics_prac_guide/reference/unsorted/,/media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/ docker://google/deepvariant:1.5.0 /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /media/nils/nils_ssd_01/Genomics_prac_guide/reference/unsorted/hg19.fa --reads /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/GFX.bam --output_vcf /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/GFX.vcf.gz --num_shards 22 --dry_run=false --make_examples_extra_args='sort_by_haplotypes=false,parse_sam_aux_fields=false'`. - Error trace: . The error message including the pipeline call is :. . ```. E::idx_find_and_load] Could not retrieve index file for '/media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam/GFX.bam'. 2023-06-20 22:48:40.272832: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. 2023-06-20 22:48:40.272881: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag BC: in RG line, ignoring: @RG	ID:408781da/26--26	PL:PACBIO	DS:READTYPE=CCS;BINDINGKIT=101-894-200;SEQUENCINGKIT=101-826-100;BASECALLERVERSION=5.0.0;FRAMERATEHZ=100.000000;BarcodeFile=m64023e_230515_162401.barcodes.fasta;BarcodeHash=86d73e586a6d3ede0295785b51105eea;BarcodeCount=96;BarcodeMode=Symmetric;BarcodeQuality=Score	LB:Pool_18_20_GFX0455704_GFX	PU:m64023e_230515_162401	SM:GFX. PM:SEQUELII	BC:TGACTGTAGCGAGTAT	CM:S/P5-C2/5.0-8M. 2023-06-20 22:48:40.272889: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG	ID:408781da/26--26	PL:PACBIO	DS:READTYPE",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:4664,performance,Overhead,Overhead,4664,"FX	PU:m64023e_230515_162401	SM:GFX. PM:SEQUELII	BC:TGACTGTAGCGAGTAT	CM:S/P5-C2/5.0-8M. 2023-06-20 22:48:40.272889: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG	ID:408781da/26--26	PL:PACBIO	DS:READTYPE=CCS;BINDINGKIT=101-894-200;SEQUENCINGKIT=101-826-100;BASECALLERVERSION=5.0.0;FRAMERATEHZ=100.000000;BarcodeFile=m64023e_230515_162401.barcodes.fasta;BarcodeHash=86d73e586a6d3ede0295785b51105eea;BarcodeCount=96;BarcodeMode=Symmetric;BarcodeQuality=Score	LB:Pool_18_20_GFX0455704_GFX	PU:m64023e_230515_162401	SM:GFX. PM:SEQUELII	BC:TGACTGTAGCGAGTAT	CM:S/P5-C2/5.0-8M. I0620 22:48:40.273113 140449601988416 genomics_reader.py:222] Reading /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam/GFX.bam with NativeSamReader. I0620 22:48:40.273825 140449601988416 make_examples_core.py:257] Task 8/22: Writing examples to /tmp/tmpwjk24y8t/make_examples.tfrecord-00008-of-00022.gz. I0620 22:48:40.274013 140449601988416 make_examples_core.py:257] Task 8/22: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2183, in make_examples_runner. runtimes) = region_processor.process(region). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1285, in process. sample_reads = self.region_reads_norealign(. File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:6203,performance,parallel,parallel,6203,"_run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2183, in make_examples_runner. runtimes) = region_processor.process(region). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1285, in process. sample_reads = self.region_reads_norealign(. File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1376, in region_reads_norealign. reads = itertools.chain(reads, sam_reader.query(region)). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 247, in query. return self._reader.query(region). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 250, in query. return self._reader.query(region). ValueError: FAILED_PRECONDITION: Cannot query without an index. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /media/nils/nils_ssd_01/Genomics_prac_guide/reference/GRCh37/hs37d5.fa --reads /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam/GFX.bam --examples /tmp/tmpwjk24y8t/make_examples.tfrecord@22.gz --add_hp_channel --alt_aligned_pileup diff_channels --max_reads_per_partition 600 --min_mapping_quality 1 --noparse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --nosort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --task 1. . ```. . **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Quickstart works. This issue also happens when I try to run the pipeline with docker-only. . **Any additional context:**. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:12,reliability,fail,fails,12,"Deepvariant fails to recognize PacBio BAM index file; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. Yes. . **Describe the issue:**. I followed the [PACBIO example](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md) and also added the flags from [this issue](https://github.com/google/deepvariant/issues/458). . . This error indicates that Deepvariant is not able to find an index file for the bam file but the index file is there see :. . ``` . (base) ✔ /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam[master L|…5] . 22:08 $ ls. GFX.bam GFX.bam.pbi GFX_hg19.bam GFX_hg19.bam.pbi readlength.txt tmp. ```. I also re-indexd the file using `pbindex` from [pbbam](https://github.com/pacificbiosciences/pbbam/). As one can see from the `ls` output I also tried to realign the bam file to some other reference panel using [pbmm2](https://github.com/PacificBiosciences/pbmm2/). . **Setup**. - Operating system: Linux Mint 21.1 x86_64 . - Kernel: 5.15.0-69-generic . - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): Docker image run through Singularity. - Singularity Verion : singularity-ce version 3.11.3. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) PACBIO CCS data aligned to GRCh37.fa reference genome. No special observations in the file can be reported. . ```. (base) ✔ /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam [master L|…5] . 20:46 $ samtools flagstat GFX.bam . ^[[1;5C940551 + 0 in total (QC-passed reads + QC-failed reads). 881297 + 0 primary. 0 + 0 secondary. 59254 + 0 supplementary. 0 + 0 duplicates. 0 + 0 primary duplicates. 940551 + 0 mapped (100.00% : N/A). 881297 + 0 primary mapped (100.00% : N/A). 0 + 0 paired in sequencing. 0 + 0 read1. 0 + 0 read2. 0 + 0 properly paired (N/A : N/A). 0 + 0 with itself and mate mapped. 0 + 0 singletons (N/A : N/A). 0 + 0 with mate mapped",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1625,reliability,fail,failed,1625,"X.bam GFX.bam.pbi GFX_hg19.bam GFX_hg19.bam.pbi readlength.txt tmp. ```. I also re-indexd the file using `pbindex` from [pbbam](https://github.com/pacificbiosciences/pbbam/). As one can see from the `ls` output I also tried to realign the bam file to some other reference panel using [pbmm2](https://github.com/PacificBiosciences/pbmm2/). . **Setup**. - Operating system: Linux Mint 21.1 x86_64 . - Kernel: 5.15.0-69-generic . - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): Docker image run through Singularity. - Singularity Verion : singularity-ce version 3.11.3. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) PACBIO CCS data aligned to GRCh37.fa reference genome. No special observations in the file can be reported. . ```. (base) ✔ /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam [master L|…5] . 20:46 $ samtools flagstat GFX.bam . ^[[1;5C940551 + 0 in total (QC-passed reads + QC-failed reads). 881297 + 0 primary. 0 + 0 secondary. 59254 + 0 supplementary. 0 + 0 duplicates. 0 + 0 primary duplicates. 940551 + 0 mapped (100.00% : N/A). 881297 + 0 primary mapped (100.00% : N/A). 0 + 0 paired in sequencing. 0 + 0 read1. 0 + 0 read2. 0 + 0 properly paired (N/A : N/A). 0 + 0 with itself and mate mapped. 0 + 0 singletons (N/A : N/A). 0 + 0 with mate mapped to a different chr. 0 + 0 with mate mapped to a different chr (mapQ>=5). ```. . **Steps to reproduce:**. - Command: Follow the [PACBIO example](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md) and install the mentioned singularity version. Then run . `singularity exec --bind /usr/lib/locale/,/media/nils/nils_ssd_01/Genomics_prac_guide/reference/unsorted/,/media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/ docker://google/deepvariant:1.5.0 /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /media/nils/nils_ssd_01/Genomics_prac_guide/reference/unsorted/hg19.fa --re",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:6222,reliability,fail,failed,6222,"_run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2183, in make_examples_runner. runtimes) = region_processor.process(region). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1285, in process. sample_reads = self.region_reads_norealign(. File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1376, in region_reads_norealign. reads = itertools.chain(reads, sam_reader.query(region)). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 247, in query. return self._reader.query(region). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 250, in query. return self._reader.query(region). ValueError: FAILED_PRECONDITION: Cannot query without an index. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /media/nils/nils_ssd_01/Genomics_prac_guide/reference/GRCh37/hs37d5.fa --reads /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam/GFX.bam --examples /tmp/tmpwjk24y8t/make_examples.tfrecord@22.gz --add_hp_channel --alt_aligned_pileup diff_channels --max_reads_per_partition 600 --min_mapping_quality 1 --noparse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --nosort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --task 1. . ```. . **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Quickstart works. This issue also happens when I try to run the pipeline with docker-only. . **Any additional context:**. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:6792,reliability,Doe,Does,6792,"_run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2183, in make_examples_runner. runtimes) = region_processor.process(region). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1285, in process. sample_reads = self.region_reads_norealign(. File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1376, in region_reads_norealign. reads = itertools.chain(reads, sam_reader.query(region)). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 247, in query. return self._reader.query(region). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 250, in query. return self._reader.query(region). ValueError: FAILED_PRECONDITION: Cannot query without an index. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /media/nils/nils_ssd_01/Genomics_prac_guide/reference/GRCh37/hs37d5.fa --reads /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam/GFX.bam --examples /tmp/tmpwjk24y8t/make_examples.tfrecord@22.gz --add_hp_channel --alt_aligned_pileup diff_channels --max_reads_per_partition 600 --min_mapping_quality 1 --noparse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --nosort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --task 1. . ```. . **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Quickstart works. This issue also happens when I try to run the pipeline with docker-only. . **Any additional context:**. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:406,safety,error,error,406,"Deepvariant fails to recognize PacBio BAM index file; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. Yes. . **Describe the issue:**. I followed the [PACBIO example](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md) and also added the flags from [this issue](https://github.com/google/deepvariant/issues/458). . . This error indicates that Deepvariant is not able to find an index file for the bam file but the index file is there see :. . ``` . (base) ✔ /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam[master L|…5] . 22:08 $ ls. GFX.bam GFX.bam.pbi GFX_hg19.bam GFX_hg19.bam.pbi readlength.txt tmp. ```. I also re-indexd the file using `pbindex` from [pbbam](https://github.com/pacificbiosciences/pbbam/). As one can see from the `ls` output I also tried to realign the bam file to some other reference panel using [pbmm2](https://github.com/PacificBiosciences/pbmm2/). . **Setup**. - Operating system: Linux Mint 21.1 x86_64 . - Kernel: 5.15.0-69-generic . - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): Docker image run through Singularity. - Singularity Verion : singularity-ce version 3.11.3. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) PACBIO CCS data aligned to GRCh37.fa reference genome. No special observations in the file can be reported. . ```. (base) ✔ /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam [master L|…5] . 20:46 $ samtools flagstat GFX.bam . ^[[1;5C940551 + 0 in total (QC-passed reads + QC-failed reads). 881297 + 0 primary. 0 + 0 secondary. 59254 + 0 supplementary. 0 + 0 duplicates. 0 + 0 primary duplicates. 940551 + 0 mapped (100.00% : N/A). 881297 + 0 primary mapped (100.00% : N/A). 0 + 0 paired in sequencing. 0 + 0 read1. 0 + 0 read2. 0 + 0 properly paired (N/A : N/A). 0 + 0 with itself and mate mapped. 0 + 0 singletons (N/A : N/A). 0 + 0 with mate mapped",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:2887,safety,Error,Error,2887,"ly paired (N/A : N/A). 0 + 0 with itself and mate mapped. 0 + 0 singletons (N/A : N/A). 0 + 0 with mate mapped to a different chr. 0 + 0 with mate mapped to a different chr (mapQ>=5). ```. . **Steps to reproduce:**. - Command: Follow the [PACBIO example](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md) and install the mentioned singularity version. Then run . `singularity exec --bind /usr/lib/locale/,/media/nils/nils_ssd_01/Genomics_prac_guide/reference/unsorted/,/media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/ docker://google/deepvariant:1.5.0 /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /media/nils/nils_ssd_01/Genomics_prac_guide/reference/unsorted/hg19.fa --reads /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/GFX.bam --output_vcf /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/GFX.vcf.gz --num_shards 22 --dry_run=false --make_examples_extra_args='sort_by_haplotypes=false,parse_sam_aux_fields=false'`. - Error trace: . The error message including the pipeline call is :. . ```. E::idx_find_and_load] Could not retrieve index file for '/media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam/GFX.bam'. 2023-06-20 22:48:40.272832: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. 2023-06-20 22:48:40.272881: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag BC: in RG line, ignoring: @RG	ID:408781da/26--26	PL:PACBIO	DS:READTYPE=CCS;BINDINGKIT=101-894-200;SEQUENCINGKIT=101-826-100;BASECALLERVERSION=5.0.0;FRAMERATEHZ=100.000000;BarcodeFile=m64023e_230515_162401.barcodes.fasta;BarcodeHash=86d73e586a6d3ede0295785b51105eea;BarcodeCount=96;BarcodeMode=Symmetric;BarcodeQuality=Score	LB:Pool_18_20_GFX0455704_GFX	PU:m64023e_230515_162401	SM:GFX. PM:SEQUELII	BC:TGACTGTAGCGAGTAT	CM:S/P5-C2/5.0-8M. 2023-06-20 22:48:40.272889: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG	ID:408781da/26--26	PL",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:2906,safety,error,error,2906,"A). 0 + 0 with itself and mate mapped. 0 + 0 singletons (N/A : N/A). 0 + 0 with mate mapped to a different chr. 0 + 0 with mate mapped to a different chr (mapQ>=5). ```. . **Steps to reproduce:**. - Command: Follow the [PACBIO example](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md) and install the mentioned singularity version. Then run . `singularity exec --bind /usr/lib/locale/,/media/nils/nils_ssd_01/Genomics_prac_guide/reference/unsorted/,/media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/ docker://google/deepvariant:1.5.0 /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /media/nils/nils_ssd_01/Genomics_prac_guide/reference/unsorted/hg19.fa --reads /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/GFX.bam --output_vcf /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/GFX.vcf.gz --num_shards 22 --dry_run=false --make_examples_extra_args='sort_by_haplotypes=false,parse_sam_aux_fields=false'`. - Error trace: . The error message including the pipeline call is :. . ```. E::idx_find_and_load] Could not retrieve index file for '/media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam/GFX.bam'. 2023-06-20 22:48:40.272832: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. 2023-06-20 22:48:40.272881: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag BC: in RG line, ignoring: @RG	ID:408781da/26--26	PL:PACBIO	DS:READTYPE=CCS;BINDINGKIT=101-894-200;SEQUENCINGKIT=101-826-100;BASECALLERVERSION=5.0.0;FRAMERATEHZ=100.000000;BarcodeFile=m64023e_230515_162401.barcodes.fasta;BarcodeHash=86d73e586a6d3ede0295785b51105eea;BarcodeCount=96;BarcodeMode=Symmetric;BarcodeQuality=Score	LB:Pool_18_20_GFX0455704_GFX	PU:m64023e_230515_162401	SM:GFX. PM:SEQUELII	BC:TGACTGTAGCGAGTAT	CM:S/P5-C2/5.0-8M. 2023-06-20 22:48:40.272889: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG	ID:408781da/26--26	PL:PACBIO	DS:READTYPE",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:4687,safety,input,inputs,4687,"62401	SM:GFX. PM:SEQUELII	BC:TGACTGTAGCGAGTAT	CM:S/P5-C2/5.0-8M. 2023-06-20 22:48:40.272889: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG	ID:408781da/26--26	PL:PACBIO	DS:READTYPE=CCS;BINDINGKIT=101-894-200;SEQUENCINGKIT=101-826-100;BASECALLERVERSION=5.0.0;FRAMERATEHZ=100.000000;BarcodeFile=m64023e_230515_162401.barcodes.fasta;BarcodeHash=86d73e586a6d3ede0295785b51105eea;BarcodeCount=96;BarcodeMode=Symmetric;BarcodeQuality=Score	LB:Pool_18_20_GFX0455704_GFX	PU:m64023e_230515_162401	SM:GFX. PM:SEQUELII	BC:TGACTGTAGCGAGTAT	CM:S/P5-C2/5.0-8M. I0620 22:48:40.273113 140449601988416 genomics_reader.py:222] Reading /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam/GFX.bam with NativeSamReader. I0620 22:48:40.273825 140449601988416 make_examples_core.py:257] Task 8/22: Writing examples to /tmp/tmpwjk24y8t/make_examples.tfrecord-00008-of-00022.gz. I0620 22:48:40.274013 140449601988416 make_examples_core.py:257] Task 8/22: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2183, in make_examples_runner. runtimes) = region_processor.process(region). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1285, in process. sample_reads = self.region_reads_norealign(. File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/dee",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:4854,safety,modul,module,4854,"noring: @RG	ID:408781da/26--26	PL:PACBIO	DS:READTYPE=CCS;BINDINGKIT=101-894-200;SEQUENCINGKIT=101-826-100;BASECALLERVERSION=5.0.0;FRAMERATEHZ=100.000000;BarcodeFile=m64023e_230515_162401.barcodes.fasta;BarcodeHash=86d73e586a6d3ede0295785b51105eea;BarcodeCount=96;BarcodeMode=Symmetric;BarcodeQuality=Score	LB:Pool_18_20_GFX0455704_GFX	PU:m64023e_230515_162401	SM:GFX. PM:SEQUELII	BC:TGACTGTAGCGAGTAT	CM:S/P5-C2/5.0-8M. I0620 22:48:40.273113 140449601988416 genomics_reader.py:222] Reading /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam/GFX.bam with NativeSamReader. I0620 22:48:40.273825 140449601988416 make_examples_core.py:257] Task 8/22: Writing examples to /tmp/tmpwjk24y8t/make_examples.tfrecord-00008-of-00022.gz. I0620 22:48:40.274013 140449601988416 make_examples_core.py:257] Task 8/22: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2183, in make_examples_runner. runtimes) = region_processor.process(region). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1285, in process. sample_reads = self.region_reads_norealign(. File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1376, in region_reads_norealign. reads = itertools.chain(reads, sam_reader.query(region)). File ""/tmp/Bazel.runfiles_ztv_d7ra/run",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:6813,safety,test,test,6813,"_run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2183, in make_examples_runner. runtimes) = region_processor.process(region). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1285, in process. sample_reads = self.region_reads_norealign(. File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1376, in region_reads_norealign. reads = itertools.chain(reads, sam_reader.query(region)). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 247, in query. return self._reader.query(region). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 250, in query. return self._reader.query(region). ValueError: FAILED_PRECONDITION: Cannot query without an index. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /media/nils/nils_ssd_01/Genomics_prac_guide/reference/GRCh37/hs37d5.fa --reads /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam/GFX.bam --examples /tmp/tmpwjk24y8t/make_examples.tfrecord@22.gz --add_hp_channel --alt_aligned_pileup diff_channels --max_reads_per_partition 600 --min_mapping_quality 1 --noparse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --nosort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --task 1. . ```. . **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Quickstart works. This issue also happens when I try to run the pipeline with docker-only. . **Any additional context:**. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:6849,safety,test,test,6849,"_run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2183, in make_examples_runner. runtimes) = region_processor.process(region). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1285, in process. sample_reads = self.region_reads_norealign(. File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1376, in region_reads_norealign. reads = itertools.chain(reads, sam_reader.query(region)). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 247, in query. return self._reader.query(region). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 250, in query. return self._reader.query(region). ValueError: FAILED_PRECONDITION: Cannot query without an index. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /media/nils/nils_ssd_01/Genomics_prac_guide/reference/GRCh37/hs37d5.fa --reads /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam/GFX.bam --examples /tmp/tmpwjk24y8t/make_examples.tfrecord@22.gz --add_hp_channel --alt_aligned_pileup diff_channels --max_reads_per_partition 600 --min_mapping_quality 1 --noparse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --nosort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --task 1. . ```. . **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Quickstart works. This issue also happens when I try to run the pipeline with docker-only. . **Any additional context:**. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:282,security,model,model-case-study,282,"Deepvariant fails to recognize PacBio BAM index file; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. Yes. . **Describe the issue:**. I followed the [PACBIO example](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md) and also added the flags from [this issue](https://github.com/google/deepvariant/issues/458). . . This error indicates that Deepvariant is not able to find an index file for the bam file but the index file is there see :. . ``` . (base) ✔ /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam[master L|…5] . 22:08 $ ls. GFX.bam GFX.bam.pbi GFX_hg19.bam GFX_hg19.bam.pbi readlength.txt tmp. ```. I also re-indexd the file using `pbindex` from [pbbam](https://github.com/pacificbiosciences/pbbam/). As one can see from the `ls` output I also tried to realign the bam file to some other reference panel using [pbmm2](https://github.com/PacificBiosciences/pbmm2/). . **Setup**. - Operating system: Linux Mint 21.1 x86_64 . - Kernel: 5.15.0-69-generic . - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): Docker image run through Singularity. - Singularity Verion : singularity-ce version 3.11.3. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) PACBIO CCS data aligned to GRCh37.fa reference genome. No special observations in the file can be reported. . ```. (base) ✔ /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam [master L|…5] . 20:46 $ samtools flagstat GFX.bam . ^[[1;5C940551 + 0 in total (QC-passed reads + QC-failed reads). 881297 + 0 primary. 0 + 0 secondary. 59254 + 0 supplementary. 0 + 0 duplicates. 0 + 0 primary duplicates. 940551 + 0 mapped (100.00% : N/A). 881297 + 0 primary mapped (100.00% : N/A). 0 + 0 paired in sequencing. 0 + 0 read1. 0 + 0 read2. 0 + 0 properly paired (N/A : N/A). 0 + 0 with itself and mate mapped. 0 + 0 singletons (N/A : N/A). 0 + 0 with mate mapped",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:2217,security,model,model-case-study,2217," 3.11.3. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) PACBIO CCS data aligned to GRCh37.fa reference genome. No special observations in the file can be reported. . ```. (base) ✔ /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam [master L|…5] . 20:46 $ samtools flagstat GFX.bam . ^[[1;5C940551 + 0 in total (QC-passed reads + QC-failed reads). 881297 + 0 primary. 0 + 0 secondary. 59254 + 0 supplementary. 0 + 0 duplicates. 0 + 0 primary duplicates. 940551 + 0 mapped (100.00% : N/A). 881297 + 0 primary mapped (100.00% : N/A). 0 + 0 paired in sequencing. 0 + 0 read1. 0 + 0 read2. 0 + 0 properly paired (N/A : N/A). 0 + 0 with itself and mate mapped. 0 + 0 singletons (N/A : N/A). 0 + 0 with mate mapped to a different chr. 0 + 0 with mate mapped to a different chr (mapQ>=5). ```. . **Steps to reproduce:**. - Command: Follow the [PACBIO example](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md) and install the mentioned singularity version. Then run . `singularity exec --bind /usr/lib/locale/,/media/nils/nils_ssd_01/Genomics_prac_guide/reference/unsorted/,/media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/ docker://google/deepvariant:1.5.0 /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /media/nils/nils_ssd_01/Genomics_prac_guide/reference/unsorted/hg19.fa --reads /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/GFX.bam --output_vcf /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/GFX.vcf.gz --num_shards 22 --dry_run=false --make_examples_extra_args='sort_by_haplotypes=false,parse_sam_aux_fields=false'`. - Error trace: . The error message including the pipeline call is :. . ```. E::idx_find_and_load] Could not retrieve index file for '/media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam/GFX.bam'. 2023-06-20 22:48:40.272832: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	p",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1262,testability,instrument,instrument,1262,"variant-pacbio-model-case-study.md) and also added the flags from [this issue](https://github.com/google/deepvariant/issues/458). . . This error indicates that Deepvariant is not able to find an index file for the bam file but the index file is there see :. . ``` . (base) ✔ /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam[master L|…5] . 22:08 $ ls. GFX.bam GFX.bam.pbi GFX_hg19.bam GFX_hg19.bam.pbi readlength.txt tmp. ```. I also re-indexd the file using `pbindex` from [pbbam](https://github.com/pacificbiosciences/pbbam/). As one can see from the `ls` output I also tried to realign the bam file to some other reference panel using [pbmm2](https://github.com/PacificBiosciences/pbmm2/). . **Setup**. - Operating system: Linux Mint 21.1 x86_64 . - Kernel: 5.15.0-69-generic . - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): Docker image run through Singularity. - Singularity Verion : singularity-ce version 3.11.3. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) PACBIO CCS data aligned to GRCh37.fa reference genome. No special observations in the file can be reported. . ```. (base) ✔ /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam [master L|…5] . 20:46 $ samtools flagstat GFX.bam . ^[[1;5C940551 + 0 in total (QC-passed reads + QC-failed reads). 881297 + 0 primary. 0 + 0 secondary. 59254 + 0 supplementary. 0 + 0 duplicates. 0 + 0 primary duplicates. 940551 + 0 mapped (100.00% : N/A). 881297 + 0 primary mapped (100.00% : N/A). 0 + 0 paired in sequencing. 0 + 0 read1. 0 + 0 read2. 0 + 0 properly paired (N/A : N/A). 0 + 0 with itself and mate mapped. 0 + 0 singletons (N/A : N/A). 0 + 0 with mate mapped to a different chr. 0 + 0 with mate mapped to a different chr (mapQ>=5). ```. . **Steps to reproduce:**. - Command: Follow the [PACBIO example](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md) and install the mentioned sin",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1409,testability,observ,observations,1409,"icates that Deepvariant is not able to find an index file for the bam file but the index file is there see :. . ``` . (base) ✔ /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam[master L|…5] . 22:08 $ ls. GFX.bam GFX.bam.pbi GFX_hg19.bam GFX_hg19.bam.pbi readlength.txt tmp. ```. I also re-indexd the file using `pbindex` from [pbbam](https://github.com/pacificbiosciences/pbbam/). As one can see from the `ls` output I also tried to realign the bam file to some other reference panel using [pbmm2](https://github.com/PacificBiosciences/pbmm2/). . **Setup**. - Operating system: Linux Mint 21.1 x86_64 . - Kernel: 5.15.0-69-generic . - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): Docker image run through Singularity. - Singularity Verion : singularity-ce version 3.11.3. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) PACBIO CCS data aligned to GRCh37.fa reference genome. No special observations in the file can be reported. . ```. (base) ✔ /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam [master L|…5] . 20:46 $ samtools flagstat GFX.bam . ^[[1;5C940551 + 0 in total (QC-passed reads + QC-failed reads). 881297 + 0 primary. 0 + 0 secondary. 59254 + 0 supplementary. 0 + 0 duplicates. 0 + 0 primary duplicates. 940551 + 0 mapped (100.00% : N/A). 881297 + 0 primary mapped (100.00% : N/A). 0 + 0 paired in sequencing. 0 + 0 read1. 0 + 0 read2. 0 + 0 properly paired (N/A : N/A). 0 + 0 with itself and mate mapped. 0 + 0 singletons (N/A : N/A). 0 + 0 with mate mapped to a different chr. 0 + 0 with mate mapped to a different chr (mapQ>=5). ```. . **Steps to reproduce:**. - Command: Follow the [PACBIO example](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md) and install the mentioned singularity version. Then run . `singularity exec --bind /usr/lib/locale/,/media/nils/nils_ssd_01/Genomics_prac_guide/reference/unsorted/,/media/nils/n",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:2893,testability,trace,trace,2893,"red (N/A : N/A). 0 + 0 with itself and mate mapped. 0 + 0 singletons (N/A : N/A). 0 + 0 with mate mapped to a different chr. 0 + 0 with mate mapped to a different chr (mapQ>=5). ```. . **Steps to reproduce:**. - Command: Follow the [PACBIO example](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md) and install the mentioned singularity version. Then run . `singularity exec --bind /usr/lib/locale/,/media/nils/nils_ssd_01/Genomics_prac_guide/reference/unsorted/,/media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/ docker://google/deepvariant:1.5.0 /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /media/nils/nils_ssd_01/Genomics_prac_guide/reference/unsorted/hg19.fa --reads /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/GFX.bam --output_vcf /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/GFX.vcf.gz --num_shards 22 --dry_run=false --make_examples_extra_args='sort_by_haplotypes=false,parse_sam_aux_fields=false'`. - Error trace: . The error message including the pipeline call is :. . ```. E::idx_find_and_load] Could not retrieve index file for '/media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam/GFX.bam'. 2023-06-20 22:48:40.272832: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. 2023-06-20 22:48:40.272881: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag BC: in RG line, ignoring: @RG	ID:408781da/26--26	PL:PACBIO	DS:READTYPE=CCS;BINDINGKIT=101-894-200;SEQUENCINGKIT=101-826-100;BASECALLERVERSION=5.0.0;FRAMERATEHZ=100.000000;BarcodeFile=m64023e_230515_162401.barcodes.fasta;BarcodeHash=86d73e586a6d3ede0295785b51105eea;BarcodeCount=96;BarcodeMode=Symmetric;BarcodeQuality=Score	LB:Pool_18_20_GFX0455704_GFX	PU:m64023e_230515_162401	SM:GFX. PM:SEQUELII	BC:TGACTGTAGCGAGTAT	CM:S/P5-C2/5.0-8M. 2023-06-20 22:48:40.272889: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG	ID:408781da/26--26	PL:PACBI",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:4706,testability,Trace,Traceback,4706,"ELII	BC:TGACTGTAGCGAGTAT	CM:S/P5-C2/5.0-8M. 2023-06-20 22:48:40.272889: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG	ID:408781da/26--26	PL:PACBIO	DS:READTYPE=CCS;BINDINGKIT=101-894-200;SEQUENCINGKIT=101-826-100;BASECALLERVERSION=5.0.0;FRAMERATEHZ=100.000000;BarcodeFile=m64023e_230515_162401.barcodes.fasta;BarcodeHash=86d73e586a6d3ede0295785b51105eea;BarcodeCount=96;BarcodeMode=Symmetric;BarcodeQuality=Score	LB:Pool_18_20_GFX0455704_GFX	PU:m64023e_230515_162401	SM:GFX. PM:SEQUELII	BC:TGACTGTAGCGAGTAT	CM:S/P5-C2/5.0-8M. I0620 22:48:40.273113 140449601988416 genomics_reader.py:222] Reading /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam/GFX.bam with NativeSamReader. I0620 22:48:40.273825 140449601988416 make_examples_core.py:257] Task 8/22: Writing examples to /tmp/tmpwjk24y8t/make_examples.tfrecord-00008-of-00022.gz. I0620 22:48:40.274013 140449601988416 make_examples_core.py:257] Task 8/22: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2183, in make_examples_runner. runtimes) = region_processor.process(region). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1285, in process. sample_reads = self.region_reads_norealign(. File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/deepvariant/make_example",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:6813,testability,test,test,6813,"_run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2183, in make_examples_runner. runtimes) = region_processor.process(region). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1285, in process. sample_reads = self.region_reads_norealign(. File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1376, in region_reads_norealign. reads = itertools.chain(reads, sam_reader.query(region)). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 247, in query. return self._reader.query(region). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 250, in query. return self._reader.query(region). ValueError: FAILED_PRECONDITION: Cannot query without an index. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /media/nils/nils_ssd_01/Genomics_prac_guide/reference/GRCh37/hs37d5.fa --reads /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam/GFX.bam --examples /tmp/tmpwjk24y8t/make_examples.tfrecord@22.gz --add_hp_channel --alt_aligned_pileup diff_channels --max_reads_per_partition 600 --min_mapping_quality 1 --noparse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --nosort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --task 1. . ```. . **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Quickstart works. This issue also happens when I try to run the pipeline with docker-only. . **Any additional context:**. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:6849,testability,test,test,6849,"_run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2183, in make_examples_runner. runtimes) = region_processor.process(region). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1285, in process. sample_reads = self.region_reads_norealign(. File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1376, in region_reads_norealign. reads = itertools.chain(reads, sam_reader.query(region)). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 247, in query. return self._reader.query(region). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 250, in query. return self._reader.query(region). ValueError: FAILED_PRECONDITION: Cannot query without an index. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /media/nils/nils_ssd_01/Genomics_prac_guide/reference/GRCh37/hs37d5.fa --reads /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam/GFX.bam --examples /tmp/tmpwjk24y8t/make_examples.tfrecord@22.gz --add_hp_channel --alt_aligned_pileup diff_channels --max_reads_per_partition 600 --min_mapping_quality 1 --noparse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --nosort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --task 1. . ```. . **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Quickstart works. This issue also happens when I try to run the pipeline with docker-only. . **Any additional context:**. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:7051,testability,context,context,7051,"_run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2183, in make_examples_runner. runtimes) = region_processor.process(region). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1285, in process. sample_reads = self.region_reads_norealign(. File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1376, in region_reads_norealign. reads = itertools.chain(reads, sam_reader.query(region)). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 247, in query. return self._reader.query(region). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 250, in query. return self._reader.query(region). ValueError: FAILED_PRECONDITION: Cannot query without an index. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /media/nils/nils_ssd_01/Genomics_prac_guide/reference/GRCh37/hs37d5.fa --reads /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam/GFX.bam --examples /tmp/tmpwjk24y8t/make_examples.tfrecord@22.gz --add_hp_channel --alt_aligned_pileup diff_channels --max_reads_per_partition 600 --min_mapping_quality 1 --noparse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --nosort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --task 1. . ```. . **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Quickstart works. This issue also happens when I try to run the pipeline with docker-only. . **Any additional context:**. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:406,usability,error,error,406,"Deepvariant fails to recognize PacBio BAM index file; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. Yes. . **Describe the issue:**. I followed the [PACBIO example](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md) and also added the flags from [this issue](https://github.com/google/deepvariant/issues/458). . . This error indicates that Deepvariant is not able to find an index file for the bam file but the index file is there see :. . ``` . (base) ✔ /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam[master L|…5] . 22:08 $ ls. GFX.bam GFX.bam.pbi GFX_hg19.bam GFX_hg19.bam.pbi readlength.txt tmp. ```. I also re-indexd the file using `pbindex` from [pbbam](https://github.com/pacificbiosciences/pbbam/). As one can see from the `ls` output I also tried to realign the bam file to some other reference panel using [pbmm2](https://github.com/PacificBiosciences/pbmm2/). . **Setup**. - Operating system: Linux Mint 21.1 x86_64 . - Kernel: 5.15.0-69-generic . - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): Docker image run through Singularity. - Singularity Verion : singularity-ce version 3.11.3. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) PACBIO CCS data aligned to GRCh37.fa reference genome. No special observations in the file can be reported. . ```. (base) ✔ /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam [master L|…5] . 20:46 $ samtools flagstat GFX.bam . ^[[1;5C940551 + 0 in total (QC-passed reads + QC-failed reads). 881297 + 0 primary. 0 + 0 secondary. 59254 + 0 supplementary. 0 + 0 duplicates. 0 + 0 primary duplicates. 940551 + 0 mapped (100.00% : N/A). 881297 + 0 primary mapped (100.00% : N/A). 0 + 0 paired in sequencing. 0 + 0 read1. 0 + 0 read2. 0 + 0 properly paired (N/A : N/A). 0 + 0 with itself and mate mapped. 0 + 0 singletons (N/A : N/A). 0 + 0 with mate mapped",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:412,usability,indicat,indicates,412,"Deepvariant fails to recognize PacBio BAM index file; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. Yes. . **Describe the issue:**. I followed the [PACBIO example](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md) and also added the flags from [this issue](https://github.com/google/deepvariant/issues/458). . . This error indicates that Deepvariant is not able to find an index file for the bam file but the index file is there see :. . ``` . (base) ✔ /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam[master L|…5] . 22:08 $ ls. GFX.bam GFX.bam.pbi GFX_hg19.bam GFX_hg19.bam.pbi readlength.txt tmp. ```. I also re-indexd the file using `pbindex` from [pbbam](https://github.com/pacificbiosciences/pbbam/). As one can see from the `ls` output I also tried to realign the bam file to some other reference panel using [pbmm2](https://github.com/PacificBiosciences/pbmm2/). . **Setup**. - Operating system: Linux Mint 21.1 x86_64 . - Kernel: 5.15.0-69-generic . - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.): Docker image run through Singularity. - Singularity Verion : singularity-ce version 3.11.3. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) PACBIO CCS data aligned to GRCh37.fa reference genome. No special observations in the file can be reported. . ```. (base) ✔ /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam [master L|…5] . 20:46 $ samtools flagstat GFX.bam . ^[[1;5C940551 + 0 in total (QC-passed reads + QC-failed reads). 881297 + 0 primary. 0 + 0 secondary. 59254 + 0 supplementary. 0 + 0 duplicates. 0 + 0 primary duplicates. 940551 + 0 mapped (100.00% : N/A). 881297 + 0 primary mapped (100.00% : N/A). 0 + 0 paired in sequencing. 0 + 0 read1. 0 + 0 read2. 0 + 0 properly paired (N/A : N/A). 0 + 0 with itself and mate mapped. 0 + 0 singletons (N/A : N/A). 0 + 0 with mate mapped",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:2108,usability,Command,Command,2108,"er, built from source, etc.): Docker image run through Singularity. - Singularity Verion : singularity-ce version 3.11.3. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) PACBIO CCS data aligned to GRCh37.fa reference genome. No special observations in the file can be reported. . ```. (base) ✔ /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam [master L|…5] . 20:46 $ samtools flagstat GFX.bam . ^[[1;5C940551 + 0 in total (QC-passed reads + QC-failed reads). 881297 + 0 primary. 0 + 0 secondary. 59254 + 0 supplementary. 0 + 0 duplicates. 0 + 0 primary duplicates. 940551 + 0 mapped (100.00% : N/A). 881297 + 0 primary mapped (100.00% : N/A). 0 + 0 paired in sequencing. 0 + 0 read1. 0 + 0 read2. 0 + 0 properly paired (N/A : N/A). 0 + 0 with itself and mate mapped. 0 + 0 singletons (N/A : N/A). 0 + 0 with mate mapped to a different chr. 0 + 0 with mate mapped to a different chr (mapQ>=5). ```. . **Steps to reproduce:**. - Command: Follow the [PACBIO example](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md) and install the mentioned singularity version. Then run . `singularity exec --bind /usr/lib/locale/,/media/nils/nils_ssd_01/Genomics_prac_guide/reference/unsorted/,/media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/ docker://google/deepvariant:1.5.0 /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /media/nils/nils_ssd_01/Genomics_prac_guide/reference/unsorted/hg19.fa --reads /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/GFX.bam --output_vcf /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/GFX.vcf.gz --num_shards 22 --dry_run=false --make_examples_extra_args='sort_by_haplotypes=false,parse_sam_aux_fields=false'`. - Error trace: . The error message including the pipeline call is :. . ```. E::idx_find_and_load] Could not retrieve index file for '/media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam/GFX.bam'. 2023-06-20 22:48:40.272832:",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:2887,usability,Error,Error,2887,"ly paired (N/A : N/A). 0 + 0 with itself and mate mapped. 0 + 0 singletons (N/A : N/A). 0 + 0 with mate mapped to a different chr. 0 + 0 with mate mapped to a different chr (mapQ>=5). ```. . **Steps to reproduce:**. - Command: Follow the [PACBIO example](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md) and install the mentioned singularity version. Then run . `singularity exec --bind /usr/lib/locale/,/media/nils/nils_ssd_01/Genomics_prac_guide/reference/unsorted/,/media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/ docker://google/deepvariant:1.5.0 /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /media/nils/nils_ssd_01/Genomics_prac_guide/reference/unsorted/hg19.fa --reads /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/GFX.bam --output_vcf /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/GFX.vcf.gz --num_shards 22 --dry_run=false --make_examples_extra_args='sort_by_haplotypes=false,parse_sam_aux_fields=false'`. - Error trace: . The error message including the pipeline call is :. . ```. E::idx_find_and_load] Could not retrieve index file for '/media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam/GFX.bam'. 2023-06-20 22:48:40.272832: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. 2023-06-20 22:48:40.272881: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag BC: in RG line, ignoring: @RG	ID:408781da/26--26	PL:PACBIO	DS:READTYPE=CCS;BINDINGKIT=101-894-200;SEQUENCINGKIT=101-826-100;BASECALLERVERSION=5.0.0;FRAMERATEHZ=100.000000;BarcodeFile=m64023e_230515_162401.barcodes.fasta;BarcodeHash=86d73e586a6d3ede0295785b51105eea;BarcodeCount=96;BarcodeMode=Symmetric;BarcodeQuality=Score	LB:Pool_18_20_GFX0455704_GFX	PU:m64023e_230515_162401	SM:GFX. PM:SEQUELII	BC:TGACTGTAGCGAGTAT	CM:S/P5-C2/5.0-8M. 2023-06-20 22:48:40.272889: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG	ID:408781da/26--26	PL",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:2906,usability,error,error,2906,"A). 0 + 0 with itself and mate mapped. 0 + 0 singletons (N/A : N/A). 0 + 0 with mate mapped to a different chr. 0 + 0 with mate mapped to a different chr (mapQ>=5). ```. . **Steps to reproduce:**. - Command: Follow the [PACBIO example](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md) and install the mentioned singularity version. Then run . `singularity exec --bind /usr/lib/locale/,/media/nils/nils_ssd_01/Genomics_prac_guide/reference/unsorted/,/media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/ docker://google/deepvariant:1.5.0 /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /media/nils/nils_ssd_01/Genomics_prac_guide/reference/unsorted/hg19.fa --reads /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/GFX.bam --output_vcf /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/GFX.vcf.gz --num_shards 22 --dry_run=false --make_examples_extra_args='sort_by_haplotypes=false,parse_sam_aux_fields=false'`. - Error trace: . The error message including the pipeline call is :. . ```. E::idx_find_and_load] Could not retrieve index file for '/media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam/GFX.bam'. 2023-06-20 22:48:40.272832: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD	VN:1.6	SO:coordinate	pb:5.0.0. 2023-06-20 22:48:40.272881: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag BC: in RG line, ignoring: @RG	ID:408781da/26--26	PL:PACBIO	DS:READTYPE=CCS;BINDINGKIT=101-894-200;SEQUENCINGKIT=101-826-100;BASECALLERVERSION=5.0.0;FRAMERATEHZ=100.000000;BarcodeFile=m64023e_230515_162401.barcodes.fasta;BarcodeHash=86d73e586a6d3ede0295785b51105eea;BarcodeCount=96;BarcodeMode=Symmetric;BarcodeQuality=Score	LB:Pool_18_20_GFX0455704_GFX	PU:m64023e_230515_162401	SM:GFX. PM:SEQUELII	BC:TGACTGTAGCGAGTAT	CM:S/P5-C2/5.0-8M. 2023-06-20 22:48:40.272889: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG	ID:408781da/26--26	PL:PACBIO	DS:READTYPE",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:4687,usability,input,inputs,4687,"62401	SM:GFX. PM:SEQUELII	BC:TGACTGTAGCGAGTAT	CM:S/P5-C2/5.0-8M. 2023-06-20 22:48:40.272889: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG	ID:408781da/26--26	PL:PACBIO	DS:READTYPE=CCS;BINDINGKIT=101-894-200;SEQUENCINGKIT=101-826-100;BASECALLERVERSION=5.0.0;FRAMERATEHZ=100.000000;BarcodeFile=m64023e_230515_162401.barcodes.fasta;BarcodeHash=86d73e586a6d3ede0295785b51105eea;BarcodeCount=96;BarcodeMode=Symmetric;BarcodeQuality=Score	LB:Pool_18_20_GFX0455704_GFX	PU:m64023e_230515_162401	SM:GFX. PM:SEQUELII	BC:TGACTGTAGCGAGTAT	CM:S/P5-C2/5.0-8M. I0620 22:48:40.273113 140449601988416 genomics_reader.py:222] Reading /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam/GFX.bam with NativeSamReader. I0620 22:48:40.273825 140449601988416 make_examples_core.py:257] Task 8/22: Writing examples to /tmp/tmpwjk24y8t/make_examples.tfrecord-00008-of-00022.gz. I0620 22:48:40.274013 140449601988416 make_examples_core.py:257] Task 8/22: Overhead for preparing inputs: 0 seconds. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2183, in make_examples_runner. runtimes) = region_processor.process(region). File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1285, in process. sample_reads = self.region_reads_norealign(. File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/dee",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/667:252,interoperability,format,formate,252,"Can the pan-genome alignment bam file be used to call variation information by DeepVariant; Hi deepvariant developer,. Is DeepVariant suitable for performing variation detection analysis, particularly **calling SNP and indel information**, using **bam formate** alignment results obtained from **vg software pan-genome analysis** and NGS resequencing data as input files? Thanks",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/667
https://github.com/google/deepvariant/issues/667:147,performance,perform,performing,147,"Can the pan-genome alignment bam file be used to call variation information by DeepVariant; Hi deepvariant developer,. Is DeepVariant suitable for performing variation detection analysis, particularly **calling SNP and indel information**, using **bam formate** alignment results obtained from **vg software pan-genome analysis** and NGS resequencing data as input files? Thanks",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/667
https://github.com/google/deepvariant/issues/667:168,safety,detect,detection,168,"Can the pan-genome alignment bam file be used to call variation information by DeepVariant; Hi deepvariant developer,. Is DeepVariant suitable for performing variation detection analysis, particularly **calling SNP and indel information**, using **bam formate** alignment results obtained from **vg software pan-genome analysis** and NGS resequencing data as input files? Thanks",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/667
https://github.com/google/deepvariant/issues/667:359,safety,input,input,359,"Can the pan-genome alignment bam file be used to call variation information by DeepVariant; Hi deepvariant developer,. Is DeepVariant suitable for performing variation detection analysis, particularly **calling SNP and indel information**, using **bam formate** alignment results obtained from **vg software pan-genome analysis** and NGS resequencing data as input files? Thanks",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/667
https://github.com/google/deepvariant/issues/667:168,security,detect,detection,168,"Can the pan-genome alignment bam file be used to call variation information by DeepVariant; Hi deepvariant developer,. Is DeepVariant suitable for performing variation detection analysis, particularly **calling SNP and indel information**, using **bam formate** alignment results obtained from **vg software pan-genome analysis** and NGS resequencing data as input files? Thanks",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/667
https://github.com/google/deepvariant/issues/667:147,usability,perform,performing,147,"Can the pan-genome alignment bam file be used to call variation information by DeepVariant; Hi deepvariant developer,. Is DeepVariant suitable for performing variation detection analysis, particularly **calling SNP and indel information**, using **bam formate** alignment results obtained from **vg software pan-genome analysis** and NGS resequencing data as input files? Thanks",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/667
https://github.com/google/deepvariant/issues/667:359,usability,input,input,359,"Can the pan-genome alignment bam file be used to call variation information by DeepVariant; Hi deepvariant developer,. Is DeepVariant suitable for performing variation detection analysis, particularly **calling SNP and indel information**, using **bam formate** alignment results obtained from **vg software pan-genome analysis** and NGS resequencing data as input files? Thanks",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/667
https://github.com/google/deepvariant/issues/668:20,availability,error,errors,20,"singularity install errors; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). I want to use singularity to install software **DeepVariant**, but it generates an error, is there some suggestion.thanks. **Setup**. - Operating system: linux（Centos）. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.):singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: **/projects/liming/Software/mambaforge-pypy3/envs/singularity/bin/singularity pull /projects/liming/Software/deepvariant/deepvariant.sif docker://google/deepvariant:""1.5.0""**. - Error trace: (if applicable). <img width=""953"" alt=""image"" src=""https://github.com/google/deepvariant/assets/26595839/035ed38c-3a15-45e8-8bb3-dc0e0cfc3200"">. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/668
https://github.com/google/deepvariant/issues/668:285,availability,error,error,285,"singularity install errors; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). I want to use singularity to install software **DeepVariant**, but it generates an error, is there some suggestion.thanks. **Setup**. - Operating system: linux（Centos）. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.):singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: **/projects/liming/Software/mambaforge-pypy3/envs/singularity/bin/singularity pull /projects/liming/Software/deepvariant/deepvariant.sif docker://google/deepvariant:""1.5.0""**. - Error trace: (if applicable). <img width=""953"" alt=""image"" src=""https://github.com/google/deepvariant/assets/26595839/035ed38c-3a15-45e8-8bb3-dc0e0cfc3200"">. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/668
https://github.com/google/deepvariant/issues/668:338,availability,Operat,Operating,338,"singularity install errors; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). I want to use singularity to install software **DeepVariant**, but it generates an error, is there some suggestion.thanks. **Setup**. - Operating system: linux（Centos）. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.):singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: **/projects/liming/Software/mambaforge-pypy3/envs/singularity/bin/singularity pull /projects/liming/Software/deepvariant/deepvariant.sif docker://google/deepvariant:""1.5.0""**. - Error trace: (if applicable). <img width=""953"" alt=""image"" src=""https://github.com/google/deepvariant/assets/26595839/035ed38c-3a15-45e8-8bb3-dc0e0cfc3200"">. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/668
https://github.com/google/deepvariant/issues/668:794,availability,Error,Error,794,"singularity install errors; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). I want to use singularity to install software **DeepVariant**, but it generates an error, is there some suggestion.thanks. **Setup**. - Operating system: linux（Centos）. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.):singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: **/projects/liming/Software/mambaforge-pypy3/envs/singularity/bin/singularity pull /projects/liming/Software/deepvariant/deepvariant.sif docker://google/deepvariant:""1.5.0""**. - Error trace: (if applicable). <img width=""953"" alt=""image"" src=""https://github.com/google/deepvariant/assets/26595839/035ed38c-3a15-45e8-8bb3-dc0e0cfc3200"">. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/668
https://github.com/google/deepvariant/issues/668:12,deployability,instal,install,12,"singularity install errors; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). I want to use singularity to install software **DeepVariant**, but it generates an error, is there some suggestion.thanks. **Setup**. - Operating system: linux（Centos）. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.):singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: **/projects/liming/Software/mambaforge-pypy3/envs/singularity/bin/singularity pull /projects/liming/Software/deepvariant/deepvariant.sif docker://google/deepvariant:""1.5.0""**. - Error trace: (if applicable). <img width=""953"" alt=""image"" src=""https://github.com/google/deepvariant/assets/26595839/035ed38c-3a15-45e8-8bb3-dc0e0cfc3200"">. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/668
https://github.com/google/deepvariant/issues/668:231,deployability,instal,install,231,"singularity install errors; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). I want to use singularity to install software **DeepVariant**, but it generates an error, is there some suggestion.thanks. **Setup**. - Operating system: linux（Centos）. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.):singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: **/projects/liming/Software/mambaforge-pypy3/envs/singularity/bin/singularity pull /projects/liming/Software/deepvariant/deepvariant.sif docker://google/deepvariant:""1.5.0""**. - Error trace: (if applicable). <img width=""953"" alt=""image"" src=""https://github.com/google/deepvariant/assets/26595839/035ed38c-3a15-45e8-8bb3-dc0e0cfc3200"">. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/668
https://github.com/google/deepvariant/issues/668:385,deployability,version,version,385,"singularity install errors; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). I want to use singularity to install software **DeepVariant**, but it generates an error, is there some suggestion.thanks. **Setup**. - Operating system: linux（Centos）. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.):singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: **/projects/liming/Software/mambaforge-pypy3/envs/singularity/bin/singularity pull /projects/liming/Software/deepvariant/deepvariant.sif docker://google/deepvariant:""1.5.0""**. - Error trace: (if applicable). <img width=""953"" alt=""image"" src=""https://github.com/google/deepvariant/assets/26595839/035ed38c-3a15-45e8-8bb3-dc0e0cfc3200"">. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/668
https://github.com/google/deepvariant/issues/668:403,deployability,Instal,Installation,403,"singularity install errors; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). I want to use singularity to install software **DeepVariant**, but it generates an error, is there some suggestion.thanks. **Setup**. - Operating system: linux（Centos）. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.):singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: **/projects/liming/Software/mambaforge-pypy3/envs/singularity/bin/singularity pull /projects/liming/Software/deepvariant/deepvariant.sif docker://google/deepvariant:""1.5.0""**. - Error trace: (if applicable). <img width=""953"" alt=""image"" src=""https://github.com/google/deepvariant/assets/26595839/035ed38c-3a15-45e8-8bb3-dc0e0cfc3200"">. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/668
https://github.com/google/deepvariant/issues/668:385,integrability,version,version,385,"singularity install errors; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). I want to use singularity to install software **DeepVariant**, but it generates an error, is there some suggestion.thanks. **Setup**. - Operating system: linux（Centos）. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.):singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: **/projects/liming/Software/mambaforge-pypy3/envs/singularity/bin/singularity pull /projects/liming/Software/deepvariant/deepvariant.sif docker://google/deepvariant:""1.5.0""**. - Error trace: (if applicable). <img width=""953"" alt=""image"" src=""https://github.com/google/deepvariant/assets/26595839/035ed38c-3a15-45e8-8bb3-dc0e0cfc3200"">. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/668
https://github.com/google/deepvariant/issues/668:385,modifiability,version,version,385,"singularity install errors; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). I want to use singularity to install software **DeepVariant**, but it generates an error, is there some suggestion.thanks. **Setup**. - Operating system: linux（Centos）. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.):singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: **/projects/liming/Software/mambaforge-pypy3/envs/singularity/bin/singularity pull /projects/liming/Software/deepvariant/deepvariant.sif docker://google/deepvariant:""1.5.0""**. - Error trace: (if applicable). <img width=""953"" alt=""image"" src=""https://github.com/google/deepvariant/assets/26595839/035ed38c-3a15-45e8-8bb3-dc0e0cfc3200"">. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/668
https://github.com/google/deepvariant/issues/668:20,performance,error,errors,20,"singularity install errors; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). I want to use singularity to install software **DeepVariant**, but it generates an error, is there some suggestion.thanks. **Setup**. - Operating system: linux（Centos）. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.):singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: **/projects/liming/Software/mambaforge-pypy3/envs/singularity/bin/singularity pull /projects/liming/Software/deepvariant/deepvariant.sif docker://google/deepvariant:""1.5.0""**. - Error trace: (if applicable). <img width=""953"" alt=""image"" src=""https://github.com/google/deepvariant/assets/26595839/035ed38c-3a15-45e8-8bb3-dc0e0cfc3200"">. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/668
https://github.com/google/deepvariant/issues/668:285,performance,error,error,285,"singularity install errors; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). I want to use singularity to install software **DeepVariant**, but it generates an error, is there some suggestion.thanks. **Setup**. - Operating system: linux（Centos）. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.):singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: **/projects/liming/Software/mambaforge-pypy3/envs/singularity/bin/singularity pull /projects/liming/Software/deepvariant/deepvariant.sif docker://google/deepvariant:""1.5.0""**. - Error trace: (if applicable). <img width=""953"" alt=""image"" src=""https://github.com/google/deepvariant/assets/26595839/035ed38c-3a15-45e8-8bb3-dc0e0cfc3200"">. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/668
https://github.com/google/deepvariant/issues/668:794,performance,Error,Error,794,"singularity install errors; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). I want to use singularity to install software **DeepVariant**, but it generates an error, is there some suggestion.thanks. **Setup**. - Operating system: linux（Centos）. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.):singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: **/projects/liming/Software/mambaforge-pypy3/envs/singularity/bin/singularity pull /projects/liming/Software/deepvariant/deepvariant.sif docker://google/deepvariant:""1.5.0""**. - Error trace: (if applicable). <img width=""953"" alt=""image"" src=""https://github.com/google/deepvariant/assets/26595839/035ed38c-3a15-45e8-8bb3-dc0e0cfc3200"">. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/668
https://github.com/google/deepvariant/issues/668:954,reliability,Doe,Does,954,"singularity install errors; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). I want to use singularity to install software **DeepVariant**, but it generates an error, is there some suggestion.thanks. **Setup**. - Operating system: linux（Centos）. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.):singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: **/projects/liming/Software/mambaforge-pypy3/envs/singularity/bin/singularity pull /projects/liming/Software/deepvariant/deepvariant.sif docker://google/deepvariant:""1.5.0""**. - Error trace: (if applicable). <img width=""953"" alt=""image"" src=""https://github.com/google/deepvariant/assets/26595839/035ed38c-3a15-45e8-8bb3-dc0e0cfc3200"">. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/668
https://github.com/google/deepvariant/issues/668:20,safety,error,errors,20,"singularity install errors; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). I want to use singularity to install software **DeepVariant**, but it generates an error, is there some suggestion.thanks. **Setup**. - Operating system: linux（Centos）. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.):singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: **/projects/liming/Software/mambaforge-pypy3/envs/singularity/bin/singularity pull /projects/liming/Software/deepvariant/deepvariant.sif docker://google/deepvariant:""1.5.0""**. - Error trace: (if applicable). <img width=""953"" alt=""image"" src=""https://github.com/google/deepvariant/assets/26595839/035ed38c-3a15-45e8-8bb3-dc0e0cfc3200"">. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/668
https://github.com/google/deepvariant/issues/668:285,safety,error,error,285,"singularity install errors; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). I want to use singularity to install software **DeepVariant**, but it generates an error, is there some suggestion.thanks. **Setup**. - Operating system: linux（Centos）. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.):singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: **/projects/liming/Software/mambaforge-pypy3/envs/singularity/bin/singularity pull /projects/liming/Software/deepvariant/deepvariant.sif docker://google/deepvariant:""1.5.0""**. - Error trace: (if applicable). <img width=""953"" alt=""image"" src=""https://github.com/google/deepvariant/assets/26595839/035ed38c-3a15-45e8-8bb3-dc0e0cfc3200"">. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/668
https://github.com/google/deepvariant/issues/668:794,safety,Error,Error,794,"singularity install errors; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). I want to use singularity to install software **DeepVariant**, but it generates an error, is there some suggestion.thanks. **Setup**. - Operating system: linux（Centos）. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.):singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: **/projects/liming/Software/mambaforge-pypy3/envs/singularity/bin/singularity pull /projects/liming/Software/deepvariant/deepvariant.sif docker://google/deepvariant:""1.5.0""**. - Error trace: (if applicable). <img width=""953"" alt=""image"" src=""https://github.com/google/deepvariant/assets/26595839/035ed38c-3a15-45e8-8bb3-dc0e0cfc3200"">. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/668
https://github.com/google/deepvariant/issues/668:975,safety,test,test,975,"singularity install errors; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). I want to use singularity to install software **DeepVariant**, but it generates an error, is there some suggestion.thanks. **Setup**. - Operating system: linux（Centos）. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.):singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: **/projects/liming/Software/mambaforge-pypy3/envs/singularity/bin/singularity pull /projects/liming/Software/deepvariant/deepvariant.sif docker://google/deepvariant:""1.5.0""**. - Error trace: (if applicable). <img width=""953"" alt=""image"" src=""https://github.com/google/deepvariant/assets/26595839/035ed38c-3a15-45e8-8bb3-dc0e0cfc3200"">. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/668
https://github.com/google/deepvariant/issues/668:1011,safety,test,test,1011,"singularity install errors; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). I want to use singularity to install software **DeepVariant**, but it generates an error, is there some suggestion.thanks. **Setup**. - Operating system: linux（Centos）. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.):singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: **/projects/liming/Software/mambaforge-pypy3/envs/singularity/bin/singularity pull /projects/liming/Software/deepvariant/deepvariant.sif docker://google/deepvariant:""1.5.0""**. - Error trace: (if applicable). <img width=""953"" alt=""image"" src=""https://github.com/google/deepvariant/assets/26595839/035ed38c-3a15-45e8-8bb3-dc0e0cfc3200"">. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/668
https://github.com/google/deepvariant/issues/668:498,testability,instrument,instrument,498,"singularity install errors; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). I want to use singularity to install software **DeepVariant**, but it generates an error, is there some suggestion.thanks. **Setup**. - Operating system: linux（Centos）. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.):singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: **/projects/liming/Software/mambaforge-pypy3/envs/singularity/bin/singularity pull /projects/liming/Software/deepvariant/deepvariant.sif docker://google/deepvariant:""1.5.0""**. - Error trace: (if applicable). <img width=""953"" alt=""image"" src=""https://github.com/google/deepvariant/assets/26595839/035ed38c-3a15-45e8-8bb3-dc0e0cfc3200"">. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/668
https://github.com/google/deepvariant/issues/668:800,testability,trace,trace,800,"singularity install errors; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). I want to use singularity to install software **DeepVariant**, but it generates an error, is there some suggestion.thanks. **Setup**. - Operating system: linux（Centos）. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.):singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: **/projects/liming/Software/mambaforge-pypy3/envs/singularity/bin/singularity pull /projects/liming/Software/deepvariant/deepvariant.sif docker://google/deepvariant:""1.5.0""**. - Error trace: (if applicable). <img width=""953"" alt=""image"" src=""https://github.com/google/deepvariant/assets/26595839/035ed38c-3a15-45e8-8bb3-dc0e0cfc3200"">. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/668
https://github.com/google/deepvariant/issues/668:975,testability,test,test,975,"singularity install errors; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). I want to use singularity to install software **DeepVariant**, but it generates an error, is there some suggestion.thanks. **Setup**. - Operating system: linux（Centos）. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.):singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: **/projects/liming/Software/mambaforge-pypy3/envs/singularity/bin/singularity pull /projects/liming/Software/deepvariant/deepvariant.sif docker://google/deepvariant:""1.5.0""**. - Error trace: (if applicable). <img width=""953"" alt=""image"" src=""https://github.com/google/deepvariant/assets/26595839/035ed38c-3a15-45e8-8bb3-dc0e0cfc3200"">. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/668
https://github.com/google/deepvariant/issues/668:1011,testability,test,test,1011,"singularity install errors; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). I want to use singularity to install software **DeepVariant**, but it generates an error, is there some suggestion.thanks. **Setup**. - Operating system: linux（Centos）. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.):singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: **/projects/liming/Software/mambaforge-pypy3/envs/singularity/bin/singularity pull /projects/liming/Software/deepvariant/deepvariant.sif docker://google/deepvariant:""1.5.0""**. - Error trace: (if applicable). <img width=""953"" alt=""image"" src=""https://github.com/google/deepvariant/assets/26595839/035ed38c-3a15-45e8-8bb3-dc0e0cfc3200"">. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/668
https://github.com/google/deepvariant/issues/668:1186,testability,context,context,1186,"singularity install errors; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). I want to use singularity to install software **DeepVariant**, but it generates an error, is there some suggestion.thanks. **Setup**. - Operating system: linux（Centos）. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.):singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: **/projects/liming/Software/mambaforge-pypy3/envs/singularity/bin/singularity pull /projects/liming/Software/deepvariant/deepvariant.sif docker://google/deepvariant:""1.5.0""**. - Error trace: (if applicable). <img width=""953"" alt=""image"" src=""https://github.com/google/deepvariant/assets/26595839/035ed38c-3a15-45e8-8bb3-dc0e0cfc3200"">. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/668
https://github.com/google/deepvariant/issues/668:20,usability,error,errors,20,"singularity install errors; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). I want to use singularity to install software **DeepVariant**, but it generates an error, is there some suggestion.thanks. **Setup**. - Operating system: linux（Centos）. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.):singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: **/projects/liming/Software/mambaforge-pypy3/envs/singularity/bin/singularity pull /projects/liming/Software/deepvariant/deepvariant.sif docker://google/deepvariant:""1.5.0""**. - Error trace: (if applicable). <img width=""953"" alt=""image"" src=""https://github.com/google/deepvariant/assets/26595839/035ed38c-3a15-45e8-8bb3-dc0e0cfc3200"">. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/668
https://github.com/google/deepvariant/issues/668:148,usability,clear,clear,148,"singularity install errors; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). I want to use singularity to install software **DeepVariant**, but it generates an error, is there some suggestion.thanks. **Setup**. - Operating system: linux（Centos）. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.):singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: **/projects/liming/Software/mambaforge-pypy3/envs/singularity/bin/singularity pull /projects/liming/Software/deepvariant/deepvariant.sif docker://google/deepvariant:""1.5.0""**. - Error trace: (if applicable). <img width=""953"" alt=""image"" src=""https://github.com/google/deepvariant/assets/26595839/035ed38c-3a15-45e8-8bb3-dc0e0cfc3200"">. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/668
https://github.com/google/deepvariant/issues/668:285,usability,error,error,285,"singularity install errors; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). I want to use singularity to install software **DeepVariant**, but it generates an error, is there some suggestion.thanks. **Setup**. - Operating system: linux（Centos）. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.):singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: **/projects/liming/Software/mambaforge-pypy3/envs/singularity/bin/singularity pull /projects/liming/Software/deepvariant/deepvariant.sif docker://google/deepvariant:""1.5.0""**. - Error trace: (if applicable). <img width=""953"" alt=""image"" src=""https://github.com/google/deepvariant/assets/26595839/035ed38c-3a15-45e8-8bb3-dc0e0cfc3200"">. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/668
https://github.com/google/deepvariant/issues/668:607,usability,Command,Command,607,"singularity install errors; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). I want to use singularity to install software **DeepVariant**, but it generates an error, is there some suggestion.thanks. **Setup**. - Operating system: linux（Centos）. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.):singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: **/projects/liming/Software/mambaforge-pypy3/envs/singularity/bin/singularity pull /projects/liming/Software/deepvariant/deepvariant.sif docker://google/deepvariant:""1.5.0""**. - Error trace: (if applicable). <img width=""953"" alt=""image"" src=""https://github.com/google/deepvariant/assets/26595839/035ed38c-3a15-45e8-8bb3-dc0e0cfc3200"">. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/668
https://github.com/google/deepvariant/issues/668:794,usability,Error,Error,794,"singularity install errors; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**. (A clear and concise description of what the issue is.). I want to use singularity to install software **DeepVariant**, but it generates an error, is there some suggestion.thanks. **Setup**. - Operating system: linux（Centos）. - DeepVariant version: 1.5.0. - Installation method (Docker, built from source, etc.):singularity. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command: **/projects/liming/Software/mambaforge-pypy3/envs/singularity/bin/singularity pull /projects/liming/Software/deepvariant/deepvariant.sif docker://google/deepvariant:""1.5.0""**. - Error trace: (if applicable). <img width=""953"" alt=""image"" src=""https://github.com/google/deepvariant/assets/26595839/035ed38c-3a15-45e8-8bb3-dc0e0cfc3200"">. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/668
https://github.com/google/deepvariant/issues/669:47,deployability,instal,installed,47,"Running dv_make_examples.py from conda; Hi,. I installed deepvariant from conda. However, when I run dv_make_examples.py all jobs fail, and in their log is reported as:. ```. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_v62e2j9f/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_v62e2j9f/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 41, in <module>. from etils import epath. ModuleNotFoundError: No module named 'etils' . ```. Do you have any suggestions?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669
https://github.com/google/deepvariant/issues/669:130,deployability,fail,fail,130,"Running dv_make_examples.py from conda; Hi,. I installed deepvariant from conda. However, when I run dv_make_examples.py all jobs fail, and in their log is reported as:. ```. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_v62e2j9f/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_v62e2j9f/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 41, in <module>. from etils import epath. ModuleNotFoundError: No module named 'etils' . ```. Do you have any suggestions?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669
https://github.com/google/deepvariant/issues/669:149,deployability,log,log,149,"Running dv_make_examples.py from conda; Hi,. I installed deepvariant from conda. However, when I run dv_make_examples.py all jobs fail, and in their log is reported as:. ```. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_v62e2j9f/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_v62e2j9f/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 41, in <module>. from etils import epath. ModuleNotFoundError: No module named 'etils' . ```. Do you have any suggestions?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669
https://github.com/google/deepvariant/issues/669:322,deployability,modul,module,322,"Running dv_make_examples.py from conda; Hi,. I installed deepvariant from conda. However, when I run dv_make_examples.py all jobs fail, and in their log is reported as:. ```. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_v62e2j9f/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_v62e2j9f/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 41, in <module>. from etils import epath. ModuleNotFoundError: No module named 'etils' . ```. Do you have any suggestions?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669
https://github.com/google/deepvariant/issues/669:491,deployability,modul,module,491,"Running dv_make_examples.py from conda; Hi,. I installed deepvariant from conda. However, when I run dv_make_examples.py all jobs fail, and in their log is reported as:. ```. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_v62e2j9f/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_v62e2j9f/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 41, in <module>. from etils import epath. ModuleNotFoundError: No module named 'etils' . ```. Do you have any suggestions?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669
https://github.com/google/deepvariant/issues/669:525,deployability,Modul,ModuleNotFoundError,525,"Running dv_make_examples.py from conda; Hi,. I installed deepvariant from conda. However, when I run dv_make_examples.py all jobs fail, and in their log is reported as:. ```. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_v62e2j9f/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_v62e2j9f/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 41, in <module>. from etils import epath. ModuleNotFoundError: No module named 'etils' . ```. Do you have any suggestions?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669
https://github.com/google/deepvariant/issues/669:549,deployability,modul,module,549,"Running dv_make_examples.py from conda; Hi,. I installed deepvariant from conda. However, when I run dv_make_examples.py all jobs fail, and in their log is reported as:. ```. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_v62e2j9f/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_v62e2j9f/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 41, in <module>. from etils import epath. ModuleNotFoundError: No module named 'etils' . ```. Do you have any suggestions?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669
https://github.com/google/deepvariant/issues/669:322,modifiability,modul,module,322,"Running dv_make_examples.py from conda; Hi,. I installed deepvariant from conda. However, when I run dv_make_examples.py all jobs fail, and in their log is reported as:. ```. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_v62e2j9f/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_v62e2j9f/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 41, in <module>. from etils import epath. ModuleNotFoundError: No module named 'etils' . ```. Do you have any suggestions?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669
https://github.com/google/deepvariant/issues/669:491,modifiability,modul,module,491,"Running dv_make_examples.py from conda; Hi,. I installed deepvariant from conda. However, when I run dv_make_examples.py all jobs fail, and in their log is reported as:. ```. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_v62e2j9f/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_v62e2j9f/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 41, in <module>. from etils import epath. ModuleNotFoundError: No module named 'etils' . ```. Do you have any suggestions?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669
https://github.com/google/deepvariant/issues/669:525,modifiability,Modul,ModuleNotFoundError,525,"Running dv_make_examples.py from conda; Hi,. I installed deepvariant from conda. However, when I run dv_make_examples.py all jobs fail, and in their log is reported as:. ```. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_v62e2j9f/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_v62e2j9f/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 41, in <module>. from etils import epath. ModuleNotFoundError: No module named 'etils' . ```. Do you have any suggestions?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669
https://github.com/google/deepvariant/issues/669:549,modifiability,modul,module,549,"Running dv_make_examples.py from conda; Hi,. I installed deepvariant from conda. However, when I run dv_make_examples.py all jobs fail, and in their log is reported as:. ```. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_v62e2j9f/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_v62e2j9f/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 41, in <module>. from etils import epath. ModuleNotFoundError: No module named 'etils' . ```. Do you have any suggestions?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669
https://github.com/google/deepvariant/issues/669:130,reliability,fail,fail,130,"Running dv_make_examples.py from conda; Hi,. I installed deepvariant from conda. However, when I run dv_make_examples.py all jobs fail, and in their log is reported as:. ```. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_v62e2j9f/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_v62e2j9f/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 41, in <module>. from etils import epath. ModuleNotFoundError: No module named 'etils' . ```. Do you have any suggestions?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669
https://github.com/google/deepvariant/issues/669:149,safety,log,log,149,"Running dv_make_examples.py from conda; Hi,. I installed deepvariant from conda. However, when I run dv_make_examples.py all jobs fail, and in their log is reported as:. ```. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_v62e2j9f/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_v62e2j9f/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 41, in <module>. from etils import epath. ModuleNotFoundError: No module named 'etils' . ```. Do you have any suggestions?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669
https://github.com/google/deepvariant/issues/669:322,safety,modul,module,322,"Running dv_make_examples.py from conda; Hi,. I installed deepvariant from conda. However, when I run dv_make_examples.py all jobs fail, and in their log is reported as:. ```. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_v62e2j9f/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_v62e2j9f/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 41, in <module>. from etils import epath. ModuleNotFoundError: No module named 'etils' . ```. Do you have any suggestions?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669
https://github.com/google/deepvariant/issues/669:491,safety,modul,module,491,"Running dv_make_examples.py from conda; Hi,. I installed deepvariant from conda. However, when I run dv_make_examples.py all jobs fail, and in their log is reported as:. ```. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_v62e2j9f/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_v62e2j9f/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 41, in <module>. from etils import epath. ModuleNotFoundError: No module named 'etils' . ```. Do you have any suggestions?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669
https://github.com/google/deepvariant/issues/669:525,safety,Modul,ModuleNotFoundError,525,"Running dv_make_examples.py from conda; Hi,. I installed deepvariant from conda. However, when I run dv_make_examples.py all jobs fail, and in their log is reported as:. ```. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_v62e2j9f/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_v62e2j9f/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 41, in <module>. from etils import epath. ModuleNotFoundError: No module named 'etils' . ```. Do you have any suggestions?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669
https://github.com/google/deepvariant/issues/669:549,safety,modul,module,549,"Running dv_make_examples.py from conda; Hi,. I installed deepvariant from conda. However, when I run dv_make_examples.py all jobs fail, and in their log is reported as:. ```. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_v62e2j9f/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_v62e2j9f/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 41, in <module>. from etils import epath. ModuleNotFoundError: No module named 'etils' . ```. Do you have any suggestions?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669
https://github.com/google/deepvariant/issues/669:149,security,log,log,149,"Running dv_make_examples.py from conda; Hi,. I installed deepvariant from conda. However, when I run dv_make_examples.py all jobs fail, and in their log is reported as:. ```. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_v62e2j9f/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_v62e2j9f/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 41, in <module>. from etils import epath. ModuleNotFoundError: No module named 'etils' . ```. Do you have any suggestions?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669
https://github.com/google/deepvariant/issues/669:149,testability,log,log,149,"Running dv_make_examples.py from conda; Hi,. I installed deepvariant from conda. However, when I run dv_make_examples.py all jobs fail, and in their log is reported as:. ```. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_v62e2j9f/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_v62e2j9f/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 41, in <module>. from etils import epath. ModuleNotFoundError: No module named 'etils' . ```. Do you have any suggestions?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669
https://github.com/google/deepvariant/issues/669:175,testability,Trace,Traceback,175,"Running dv_make_examples.py from conda; Hi,. I installed deepvariant from conda. However, when I run dv_make_examples.py all jobs fail, and in their log is reported as:. ```. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_v62e2j9f/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_v62e2j9f/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 41, in <module>. from etils import epath. ModuleNotFoundError: No module named 'etils' . ```. Do you have any suggestions?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669
https://github.com/google/deepvariant/issues/670:85,availability,down,download,85,"Some of the sample result vcf files have the SRR suffix in the sample name; Hello. I download 40 sample in 1kgp, first I use oqfe to BWA. Then I send the CRAM to Deepvariant to call variants. But recently, I had found a concern problem, these 40 samples execute same pipeline exactly. But the sample name of result vcf, some of it is right, last have a SRR suffix just as the next figure. And I confirm that this is the SRR suffix exactly as downloaded fastq SRR of per sample from 1KGP. And I also confirm that they all have right and same CRAM. I don't find the reason of this circumstances and I don't know there will lead to some error to my analysis? So if someone give me some explain or advice? very Thanks! ![1687788687108](https://github.com/google/deepvariant/assets/63234787/2aea3b53-babd-4874-9351-e2024fbc81df).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/670
https://github.com/google/deepvariant/issues/670:442,availability,down,downloaded,442,"Some of the sample result vcf files have the SRR suffix in the sample name; Hello. I download 40 sample in 1kgp, first I use oqfe to BWA. Then I send the CRAM to Deepvariant to call variants. But recently, I had found a concern problem, these 40 samples execute same pipeline exactly. But the sample name of result vcf, some of it is right, last have a SRR suffix just as the next figure. And I confirm that this is the SRR suffix exactly as downloaded fastq SRR of per sample from 1KGP. And I also confirm that they all have right and same CRAM. I don't find the reason of this circumstances and I don't know there will lead to some error to my analysis? So if someone give me some explain or advice? very Thanks! ![1687788687108](https://github.com/google/deepvariant/assets/63234787/2aea3b53-babd-4874-9351-e2024fbc81df).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/670
https://github.com/google/deepvariant/issues/670:634,availability,error,error,634,"Some of the sample result vcf files have the SRR suffix in the sample name; Hello. I download 40 sample in 1kgp, first I use oqfe to BWA. Then I send the CRAM to Deepvariant to call variants. But recently, I had found a concern problem, these 40 samples execute same pipeline exactly. But the sample name of result vcf, some of it is right, last have a SRR suffix just as the next figure. And I confirm that this is the SRR suffix exactly as downloaded fastq SRR of per sample from 1KGP. And I also confirm that they all have right and same CRAM. I don't find the reason of this circumstances and I don't know there will lead to some error to my analysis? So if someone give me some explain or advice? very Thanks! ![1687788687108](https://github.com/google/deepvariant/assets/63234787/2aea3b53-babd-4874-9351-e2024fbc81df).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/670
https://github.com/google/deepvariant/issues/670:267,deployability,pipelin,pipeline,267,"Some of the sample result vcf files have the SRR suffix in the sample name; Hello. I download 40 sample in 1kgp, first I use oqfe to BWA. Then I send the CRAM to Deepvariant to call variants. But recently, I had found a concern problem, these 40 samples execute same pipeline exactly. But the sample name of result vcf, some of it is right, last have a SRR suffix just as the next figure. And I confirm that this is the SRR suffix exactly as downloaded fastq SRR of per sample from 1KGP. And I also confirm that they all have right and same CRAM. I don't find the reason of this circumstances and I don't know there will lead to some error to my analysis? So if someone give me some explain or advice? very Thanks! ![1687788687108](https://github.com/google/deepvariant/assets/63234787/2aea3b53-babd-4874-9351-e2024fbc81df).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/670
https://github.com/google/deepvariant/issues/670:267,integrability,pipelin,pipeline,267,"Some of the sample result vcf files have the SRR suffix in the sample name; Hello. I download 40 sample in 1kgp, first I use oqfe to BWA. Then I send the CRAM to Deepvariant to call variants. But recently, I had found a concern problem, these 40 samples execute same pipeline exactly. But the sample name of result vcf, some of it is right, last have a SRR suffix just as the next figure. And I confirm that this is the SRR suffix exactly as downloaded fastq SRR of per sample from 1KGP. And I also confirm that they all have right and same CRAM. I don't find the reason of this circumstances and I don't know there will lead to some error to my analysis? So if someone give me some explain or advice? very Thanks! ![1687788687108](https://github.com/google/deepvariant/assets/63234787/2aea3b53-babd-4874-9351-e2024fbc81df).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/670
https://github.com/google/deepvariant/issues/670:220,modifiability,concern,concern,220,"Some of the sample result vcf files have the SRR suffix in the sample name; Hello. I download 40 sample in 1kgp, first I use oqfe to BWA. Then I send the CRAM to Deepvariant to call variants. But recently, I had found a concern problem, these 40 samples execute same pipeline exactly. But the sample name of result vcf, some of it is right, last have a SRR suffix just as the next figure. And I confirm that this is the SRR suffix exactly as downloaded fastq SRR of per sample from 1KGP. And I also confirm that they all have right and same CRAM. I don't find the reason of this circumstances and I don't know there will lead to some error to my analysis? So if someone give me some explain or advice? very Thanks! ![1687788687108](https://github.com/google/deepvariant/assets/63234787/2aea3b53-babd-4874-9351-e2024fbc81df).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/670
https://github.com/google/deepvariant/issues/670:634,performance,error,error,634,"Some of the sample result vcf files have the SRR suffix in the sample name; Hello. I download 40 sample in 1kgp, first I use oqfe to BWA. Then I send the CRAM to Deepvariant to call variants. But recently, I had found a concern problem, these 40 samples execute same pipeline exactly. But the sample name of result vcf, some of it is right, last have a SRR suffix just as the next figure. And I confirm that this is the SRR suffix exactly as downloaded fastq SRR of per sample from 1KGP. And I also confirm that they all have right and same CRAM. I don't find the reason of this circumstances and I don't know there will lead to some error to my analysis? So if someone give me some explain or advice? very Thanks! ![1687788687108](https://github.com/google/deepvariant/assets/63234787/2aea3b53-babd-4874-9351-e2024fbc81df).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/670
https://github.com/google/deepvariant/issues/670:634,safety,error,error,634,"Some of the sample result vcf files have the SRR suffix in the sample name; Hello. I download 40 sample in 1kgp, first I use oqfe to BWA. Then I send the CRAM to Deepvariant to call variants. But recently, I had found a concern problem, these 40 samples execute same pipeline exactly. But the sample name of result vcf, some of it is right, last have a SRR suffix just as the next figure. And I confirm that this is the SRR suffix exactly as downloaded fastq SRR of per sample from 1KGP. And I also confirm that they all have right and same CRAM. I don't find the reason of this circumstances and I don't know there will lead to some error to my analysis? So if someone give me some explain or advice? very Thanks! ![1687788687108](https://github.com/google/deepvariant/assets/63234787/2aea3b53-babd-4874-9351-e2024fbc81df).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/670
https://github.com/google/deepvariant/issues/670:220,testability,concern,concern,220,"Some of the sample result vcf files have the SRR suffix in the sample name; Hello. I download 40 sample in 1kgp, first I use oqfe to BWA. Then I send the CRAM to Deepvariant to call variants. But recently, I had found a concern problem, these 40 samples execute same pipeline exactly. But the sample name of result vcf, some of it is right, last have a SRR suffix just as the next figure. And I confirm that this is the SRR suffix exactly as downloaded fastq SRR of per sample from 1KGP. And I also confirm that they all have right and same CRAM. I don't find the reason of this circumstances and I don't know there will lead to some error to my analysis? So if someone give me some explain or advice? very Thanks! ![1687788687108](https://github.com/google/deepvariant/assets/63234787/2aea3b53-babd-4874-9351-e2024fbc81df).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/670
https://github.com/google/deepvariant/issues/670:395,usability,confirm,confirm,395,"Some of the sample result vcf files have the SRR suffix in the sample name; Hello. I download 40 sample in 1kgp, first I use oqfe to BWA. Then I send the CRAM to Deepvariant to call variants. But recently, I had found a concern problem, these 40 samples execute same pipeline exactly. But the sample name of result vcf, some of it is right, last have a SRR suffix just as the next figure. And I confirm that this is the SRR suffix exactly as downloaded fastq SRR of per sample from 1KGP. And I also confirm that they all have right and same CRAM. I don't find the reason of this circumstances and I don't know there will lead to some error to my analysis? So if someone give me some explain or advice? very Thanks! ![1687788687108](https://github.com/google/deepvariant/assets/63234787/2aea3b53-babd-4874-9351-e2024fbc81df).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/670
https://github.com/google/deepvariant/issues/670:499,usability,confirm,confirm,499,"Some of the sample result vcf files have the SRR suffix in the sample name; Hello. I download 40 sample in 1kgp, first I use oqfe to BWA. Then I send the CRAM to Deepvariant to call variants. But recently, I had found a concern problem, these 40 samples execute same pipeline exactly. But the sample name of result vcf, some of it is right, last have a SRR suffix just as the next figure. And I confirm that this is the SRR suffix exactly as downloaded fastq SRR of per sample from 1KGP. And I also confirm that they all have right and same CRAM. I don't find the reason of this circumstances and I don't know there will lead to some error to my analysis? So if someone give me some explain or advice? very Thanks! ![1687788687108](https://github.com/google/deepvariant/assets/63234787/2aea3b53-babd-4874-9351-e2024fbc81df).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/670
https://github.com/google/deepvariant/issues/670:634,usability,error,error,634,"Some of the sample result vcf files have the SRR suffix in the sample name; Hello. I download 40 sample in 1kgp, first I use oqfe to BWA. Then I send the CRAM to Deepvariant to call variants. But recently, I had found a concern problem, these 40 samples execute same pipeline exactly. But the sample name of result vcf, some of it is right, last have a SRR suffix just as the next figure. And I confirm that this is the SRR suffix exactly as downloaded fastq SRR of per sample from 1KGP. And I also confirm that they all have right and same CRAM. I don't find the reason of this circumstances and I don't know there will lead to some error to my analysis? So if someone give me some explain or advice? very Thanks! ![1687788687108](https://github.com/google/deepvariant/assets/63234787/2aea3b53-babd-4874-9351-e2024fbc81df).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/670
https://github.com/google/deepvariant/issues/671:7,integrability,filter,filter,7,"How to filter SNPS; Hi Developer,. I used the hg38 reference genome, and I successfully ran deepvariant. . I have the VCF file, how do I filter the SNPS next? Which metrics should I use, and what is the appropriate threshold for filtering metrics for deepvariant results? vcf file:. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT TAM-42-6-6th. chr1 15274 . A G 21.90 PASS . GT:AD:DP:GQ:PL:VAF 1/1:0,16:16:19:21,22,0:1. chr1 15820 . G T 15.70 PASS . GT:AD:DP:GQ:PL:VAF 1/1:0,8:8:13:15,15,0:1. chr1 16841 . G T 4.90 PASS . GT:AD:DP:GQ:PL:VAF 0/1:12,11:23:4:2,0,9:0.478261. chr1 19004 . A G 5 PASS . GT:AD:DP:GQ:PL:VAF 0/1:1,3:4:2:0,0,0:0.75. Best,. Jamie.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/671
https://github.com/google/deepvariant/issues/671:137,integrability,filter,filter,137,"How to filter SNPS; Hi Developer,. I used the hg38 reference genome, and I successfully ran deepvariant. . I have the VCF file, how do I filter the SNPS next? Which metrics should I use, and what is the appropriate threshold for filtering metrics for deepvariant results? vcf file:. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT TAM-42-6-6th. chr1 15274 . A G 21.90 PASS . GT:AD:DP:GQ:PL:VAF 1/1:0,16:16:19:21,22,0:1. chr1 15820 . G T 15.70 PASS . GT:AD:DP:GQ:PL:VAF 1/1:0,8:8:13:15,15,0:1. chr1 16841 . G T 4.90 PASS . GT:AD:DP:GQ:PL:VAF 0/1:12,11:23:4:2,0,9:0.478261. chr1 19004 . A G 5 PASS . GT:AD:DP:GQ:PL:VAF 0/1:1,3:4:2:0,0,0:0.75. Best,. Jamie.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/671
https://github.com/google/deepvariant/issues/671:229,integrability,filter,filtering,229,"How to filter SNPS; Hi Developer,. I used the hg38 reference genome, and I successfully ran deepvariant. . I have the VCF file, how do I filter the SNPS next? Which metrics should I use, and what is the appropriate threshold for filtering metrics for deepvariant results? vcf file:. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT TAM-42-6-6th. chr1 15274 . A G 21.90 PASS . GT:AD:DP:GQ:PL:VAF 1/1:0,16:16:19:21,22,0:1. chr1 15820 . G T 15.70 PASS . GT:AD:DP:GQ:PL:VAF 1/1:0,8:8:13:15,15,0:1. chr1 16841 . G T 4.90 PASS . GT:AD:DP:GQ:PL:VAF 0/1:12,11:23:4:2,0,9:0.478261. chr1 19004 . A G 5 PASS . GT:AD:DP:GQ:PL:VAF 0/1:1,3:4:2:0,0,0:0.75. Best,. Jamie.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/671
https://github.com/google/deepvariant/issues/671:310,integrability,FILTER,FILTER,310,"How to filter SNPS; Hi Developer,. I used the hg38 reference genome, and I successfully ran deepvariant. . I have the VCF file, how do I filter the SNPS next? Which metrics should I use, and what is the appropriate threshold for filtering metrics for deepvariant results? vcf file:. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT TAM-42-6-6th. chr1 15274 . A G 21.90 PASS . GT:AD:DP:GQ:PL:VAF 1/1:0,16:16:19:21,22,0:1. chr1 15820 . G T 15.70 PASS . GT:AD:DP:GQ:PL:VAF 1/1:0,8:8:13:15,15,0:1. chr1 16841 . G T 4.90 PASS . GT:AD:DP:GQ:PL:VAF 0/1:12,11:23:4:2,0,9:0.478261. chr1 19004 . A G 5 PASS . GT:AD:DP:GQ:PL:VAF 0/1:1,3:4:2:0,0,0:0.75. Best,. Jamie.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/671
https://github.com/google/deepvariant/issues/671:322,interoperability,FORMAT,FORMAT,322,"How to filter SNPS; Hi Developer,. I used the hg38 reference genome, and I successfully ran deepvariant. . I have the VCF file, how do I filter the SNPS next? Which metrics should I use, and what is the appropriate threshold for filtering metrics for deepvariant results? vcf file:. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT TAM-42-6-6th. chr1 15274 . A G 21.90 PASS . GT:AD:DP:GQ:PL:VAF 1/1:0,16:16:19:21,22,0:1. chr1 15820 . G T 15.70 PASS . GT:AD:DP:GQ:PL:VAF 1/1:0,8:8:13:15,15,0:1. chr1 16841 . G T 4.90 PASS . GT:AD:DP:GQ:PL:VAF 0/1:12,11:23:4:2,0,9:0.478261. chr1 19004 . A G 5 PASS . GT:AD:DP:GQ:PL:VAF 0/1:1,3:4:2:0,0,0:0.75. Best,. Jamie.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/671
https://github.com/google/deepvariant/issues/672:18,availability,Error,Error,18,"Pacbio revio data Error; hello,. When analyzing PacBio data, I encountered some problems. I tested the example data provided by DeepConsensus and aligned it using pbmm2(1.12.0). When I analyze with DV1.5, I got an error. Data: gs://brain-genomics-public/research/deepconsensus/quickstart/v1.2/n1000.subreads.bam. My cmd:. 1. pbmm2 align hs37d5.fasta fa.fofn n1000.subreads_to_ccs_aligned.bam --sort --rg '@RG\tID:test1\tSM:test1'. 2. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref hs37d5.fasta \. --reads n1000.subreads_to_ccs_aligned.bam \. --output_vcf n1000.depv.vcf.gz \. --output_gvcf n1000.depv.g.vcf.gz \. --num_shards 32 \. --intermediate_results_dir intermediate_results_dir. Error:. ![1688017046760](https://github.com/google/deepvariant/assets/70870741/e5973e9a-c44b-4a12-9179-f4657f63f4bb). Could you help me to solve the problem? Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/672
https://github.com/google/deepvariant/issues/672:214,availability,error,error,214,"Pacbio revio data Error; hello,. When analyzing PacBio data, I encountered some problems. I tested the example data provided by DeepConsensus and aligned it using pbmm2(1.12.0). When I analyze with DV1.5, I got an error. Data: gs://brain-genomics-public/research/deepconsensus/quickstart/v1.2/n1000.subreads.bam. My cmd:. 1. pbmm2 align hs37d5.fasta fa.fofn n1000.subreads_to_ccs_aligned.bam --sort --rg '@RG\tID:test1\tSM:test1'. 2. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref hs37d5.fasta \. --reads n1000.subreads_to_ccs_aligned.bam \. --output_vcf n1000.depv.vcf.gz \. --output_gvcf n1000.depv.g.vcf.gz \. --num_shards 32 \. --intermediate_results_dir intermediate_results_dir. Error:. ![1688017046760](https://github.com/google/deepvariant/assets/70870741/e5973e9a-c44b-4a12-9179-f4657f63f4bb). Could you help me to solve the problem? Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/672
https://github.com/google/deepvariant/issues/672:707,availability,Error,Error,707,"Pacbio revio data Error; hello,. When analyzing PacBio data, I encountered some problems. I tested the example data provided by DeepConsensus and aligned it using pbmm2(1.12.0). When I analyze with DV1.5, I got an error. Data: gs://brain-genomics-public/research/deepconsensus/quickstart/v1.2/n1000.subreads.bam. My cmd:. 1. pbmm2 align hs37d5.fasta fa.fofn n1000.subreads_to_ccs_aligned.bam --sort --rg '@RG\tID:test1\tSM:test1'. 2. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref hs37d5.fasta \. --reads n1000.subreads_to_ccs_aligned.bam \. --output_vcf n1000.depv.vcf.gz \. --output_gvcf n1000.depv.g.vcf.gz \. --num_shards 32 \. --intermediate_results_dir intermediate_results_dir. Error:. ![1688017046760](https://github.com/google/deepvariant/assets/70870741/e5973e9a-c44b-4a12-9179-f4657f63f4bb). Could you help me to solve the problem? Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/672
https://github.com/google/deepvariant/issues/672:247,integrability,pub,public,247,"Pacbio revio data Error; hello,. When analyzing PacBio data, I encountered some problems. I tested the example data provided by DeepConsensus and aligned it using pbmm2(1.12.0). When I analyze with DV1.5, I got an error. Data: gs://brain-genomics-public/research/deepconsensus/quickstart/v1.2/n1000.subreads.bam. My cmd:. 1. pbmm2 align hs37d5.fasta fa.fofn n1000.subreads_to_ccs_aligned.bam --sort --rg '@RG\tID:test1\tSM:test1'. 2. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref hs37d5.fasta \. --reads n1000.subreads_to_ccs_aligned.bam \. --output_vcf n1000.depv.vcf.gz \. --output_gvcf n1000.depv.g.vcf.gz \. --num_shards 32 \. --intermediate_results_dir intermediate_results_dir. Error:. ![1688017046760](https://github.com/google/deepvariant/assets/70870741/e5973e9a-c44b-4a12-9179-f4657f63f4bb). Could you help me to solve the problem? Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/672
https://github.com/google/deepvariant/issues/672:299,integrability,sub,subreads,299,"Pacbio revio data Error; hello,. When analyzing PacBio data, I encountered some problems. I tested the example data provided by DeepConsensus and aligned it using pbmm2(1.12.0). When I analyze with DV1.5, I got an error. Data: gs://brain-genomics-public/research/deepconsensus/quickstart/v1.2/n1000.subreads.bam. My cmd:. 1. pbmm2 align hs37d5.fasta fa.fofn n1000.subreads_to_ccs_aligned.bam --sort --rg '@RG\tID:test1\tSM:test1'. 2. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref hs37d5.fasta \. --reads n1000.subreads_to_ccs_aligned.bam \. --output_vcf n1000.depv.vcf.gz \. --output_gvcf n1000.depv.g.vcf.gz \. --num_shards 32 \. --intermediate_results_dir intermediate_results_dir. Error:. ![1688017046760](https://github.com/google/deepvariant/assets/70870741/e5973e9a-c44b-4a12-9179-f4657f63f4bb). Could you help me to solve the problem? Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/672
https://github.com/google/deepvariant/issues/672:0,modifiability,Pac,Pacbio,0,"Pacbio revio data Error; hello,. When analyzing PacBio data, I encountered some problems. I tested the example data provided by DeepConsensus and aligned it using pbmm2(1.12.0). When I analyze with DV1.5, I got an error. Data: gs://brain-genomics-public/research/deepconsensus/quickstart/v1.2/n1000.subreads.bam. My cmd:. 1. pbmm2 align hs37d5.fasta fa.fofn n1000.subreads_to_ccs_aligned.bam --sort --rg '@RG\tID:test1\tSM:test1'. 2. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref hs37d5.fasta \. --reads n1000.subreads_to_ccs_aligned.bam \. --output_vcf n1000.depv.vcf.gz \. --output_gvcf n1000.depv.g.vcf.gz \. --num_shards 32 \. --intermediate_results_dir intermediate_results_dir. Error:. ![1688017046760](https://github.com/google/deepvariant/assets/70870741/e5973e9a-c44b-4a12-9179-f4657f63f4bb). Could you help me to solve the problem? Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/672
https://github.com/google/deepvariant/issues/672:48,modifiability,Pac,PacBio,48,"Pacbio revio data Error; hello,. When analyzing PacBio data, I encountered some problems. I tested the example data provided by DeepConsensus and aligned it using pbmm2(1.12.0). When I analyze with DV1.5, I got an error. Data: gs://brain-genomics-public/research/deepconsensus/quickstart/v1.2/n1000.subreads.bam. My cmd:. 1. pbmm2 align hs37d5.fasta fa.fofn n1000.subreads_to_ccs_aligned.bam --sort --rg '@RG\tID:test1\tSM:test1'. 2. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref hs37d5.fasta \. --reads n1000.subreads_to_ccs_aligned.bam \. --output_vcf n1000.depv.vcf.gz \. --output_gvcf n1000.depv.g.vcf.gz \. --num_shards 32 \. --intermediate_results_dir intermediate_results_dir. Error:. ![1688017046760](https://github.com/google/deepvariant/assets/70870741/e5973e9a-c44b-4a12-9179-f4657f63f4bb). Could you help me to solve the problem? Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/672
https://github.com/google/deepvariant/issues/672:487,modifiability,PAC,PACBIO,487,"Pacbio revio data Error; hello,. When analyzing PacBio data, I encountered some problems. I tested the example data provided by DeepConsensus and aligned it using pbmm2(1.12.0). When I analyze with DV1.5, I got an error. Data: gs://brain-genomics-public/research/deepconsensus/quickstart/v1.2/n1000.subreads.bam. My cmd:. 1. pbmm2 align hs37d5.fasta fa.fofn n1000.subreads_to_ccs_aligned.bam --sort --rg '@RG\tID:test1\tSM:test1'. 2. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref hs37d5.fasta \. --reads n1000.subreads_to_ccs_aligned.bam \. --output_vcf n1000.depv.vcf.gz \. --output_gvcf n1000.depv.g.vcf.gz \. --num_shards 32 \. --intermediate_results_dir intermediate_results_dir. Error:. ![1688017046760](https://github.com/google/deepvariant/assets/70870741/e5973e9a-c44b-4a12-9179-f4657f63f4bb). Could you help me to solve the problem? Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/672
https://github.com/google/deepvariant/issues/672:18,performance,Error,Error,18,"Pacbio revio data Error; hello,. When analyzing PacBio data, I encountered some problems. I tested the example data provided by DeepConsensus and aligned it using pbmm2(1.12.0). When I analyze with DV1.5, I got an error. Data: gs://brain-genomics-public/research/deepconsensus/quickstart/v1.2/n1000.subreads.bam. My cmd:. 1. pbmm2 align hs37d5.fasta fa.fofn n1000.subreads_to_ccs_aligned.bam --sort --rg '@RG\tID:test1\tSM:test1'. 2. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref hs37d5.fasta \. --reads n1000.subreads_to_ccs_aligned.bam \. --output_vcf n1000.depv.vcf.gz \. --output_gvcf n1000.depv.g.vcf.gz \. --num_shards 32 \. --intermediate_results_dir intermediate_results_dir. Error:. ![1688017046760](https://github.com/google/deepvariant/assets/70870741/e5973e9a-c44b-4a12-9179-f4657f63f4bb). Could you help me to solve the problem? Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/672
https://github.com/google/deepvariant/issues/672:214,performance,error,error,214,"Pacbio revio data Error; hello,. When analyzing PacBio data, I encountered some problems. I tested the example data provided by DeepConsensus and aligned it using pbmm2(1.12.0). When I analyze with DV1.5, I got an error. Data: gs://brain-genomics-public/research/deepconsensus/quickstart/v1.2/n1000.subreads.bam. My cmd:. 1. pbmm2 align hs37d5.fasta fa.fofn n1000.subreads_to_ccs_aligned.bam --sort --rg '@RG\tID:test1\tSM:test1'. 2. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref hs37d5.fasta \. --reads n1000.subreads_to_ccs_aligned.bam \. --output_vcf n1000.depv.vcf.gz \. --output_gvcf n1000.depv.g.vcf.gz \. --num_shards 32 \. --intermediate_results_dir intermediate_results_dir. Error:. ![1688017046760](https://github.com/google/deepvariant/assets/70870741/e5973e9a-c44b-4a12-9179-f4657f63f4bb). Could you help me to solve the problem? Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/672
https://github.com/google/deepvariant/issues/672:707,performance,Error,Error,707,"Pacbio revio data Error; hello,. When analyzing PacBio data, I encountered some problems. I tested the example data provided by DeepConsensus and aligned it using pbmm2(1.12.0). When I analyze with DV1.5, I got an error. Data: gs://brain-genomics-public/research/deepconsensus/quickstart/v1.2/n1000.subreads.bam. My cmd:. 1. pbmm2 align hs37d5.fasta fa.fofn n1000.subreads_to_ccs_aligned.bam --sort --rg '@RG\tID:test1\tSM:test1'. 2. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref hs37d5.fasta \. --reads n1000.subreads_to_ccs_aligned.bam \. --output_vcf n1000.depv.vcf.gz \. --output_gvcf n1000.depv.g.vcf.gz \. --num_shards 32 \. --intermediate_results_dir intermediate_results_dir. Error:. ![1688017046760](https://github.com/google/deepvariant/assets/70870741/e5973e9a-c44b-4a12-9179-f4657f63f4bb). Could you help me to solve the problem? Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/672
https://github.com/google/deepvariant/issues/672:18,safety,Error,Error,18,"Pacbio revio data Error; hello,. When analyzing PacBio data, I encountered some problems. I tested the example data provided by DeepConsensus and aligned it using pbmm2(1.12.0). When I analyze with DV1.5, I got an error. Data: gs://brain-genomics-public/research/deepconsensus/quickstart/v1.2/n1000.subreads.bam. My cmd:. 1. pbmm2 align hs37d5.fasta fa.fofn n1000.subreads_to_ccs_aligned.bam --sort --rg '@RG\tID:test1\tSM:test1'. 2. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref hs37d5.fasta \. --reads n1000.subreads_to_ccs_aligned.bam \. --output_vcf n1000.depv.vcf.gz \. --output_gvcf n1000.depv.g.vcf.gz \. --num_shards 32 \. --intermediate_results_dir intermediate_results_dir. Error:. ![1688017046760](https://github.com/google/deepvariant/assets/70870741/e5973e9a-c44b-4a12-9179-f4657f63f4bb). Could you help me to solve the problem? Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/672
https://github.com/google/deepvariant/issues/672:92,safety,test,tested,92,"Pacbio revio data Error; hello,. When analyzing PacBio data, I encountered some problems. I tested the example data provided by DeepConsensus and aligned it using pbmm2(1.12.0). When I analyze with DV1.5, I got an error. Data: gs://brain-genomics-public/research/deepconsensus/quickstart/v1.2/n1000.subreads.bam. My cmd:. 1. pbmm2 align hs37d5.fasta fa.fofn n1000.subreads_to_ccs_aligned.bam --sort --rg '@RG\tID:test1\tSM:test1'. 2. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref hs37d5.fasta \. --reads n1000.subreads_to_ccs_aligned.bam \. --output_vcf n1000.depv.vcf.gz \. --output_gvcf n1000.depv.g.vcf.gz \. --num_shards 32 \. --intermediate_results_dir intermediate_results_dir. Error:. ![1688017046760](https://github.com/google/deepvariant/assets/70870741/e5973e9a-c44b-4a12-9179-f4657f63f4bb). Could you help me to solve the problem? Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/672
https://github.com/google/deepvariant/issues/672:214,safety,error,error,214,"Pacbio revio data Error; hello,. When analyzing PacBio data, I encountered some problems. I tested the example data provided by DeepConsensus and aligned it using pbmm2(1.12.0). When I analyze with DV1.5, I got an error. Data: gs://brain-genomics-public/research/deepconsensus/quickstart/v1.2/n1000.subreads.bam. My cmd:. 1. pbmm2 align hs37d5.fasta fa.fofn n1000.subreads_to_ccs_aligned.bam --sort --rg '@RG\tID:test1\tSM:test1'. 2. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref hs37d5.fasta \. --reads n1000.subreads_to_ccs_aligned.bam \. --output_vcf n1000.depv.vcf.gz \. --output_gvcf n1000.depv.g.vcf.gz \. --num_shards 32 \. --intermediate_results_dir intermediate_results_dir. Error:. ![1688017046760](https://github.com/google/deepvariant/assets/70870741/e5973e9a-c44b-4a12-9179-f4657f63f4bb). Could you help me to solve the problem? Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/672
https://github.com/google/deepvariant/issues/672:707,safety,Error,Error,707,"Pacbio revio data Error; hello,. When analyzing PacBio data, I encountered some problems. I tested the example data provided by DeepConsensus and aligned it using pbmm2(1.12.0). When I analyze with DV1.5, I got an error. Data: gs://brain-genomics-public/research/deepconsensus/quickstart/v1.2/n1000.subreads.bam. My cmd:. 1. pbmm2 align hs37d5.fasta fa.fofn n1000.subreads_to_ccs_aligned.bam --sort --rg '@RG\tID:test1\tSM:test1'. 2. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref hs37d5.fasta \. --reads n1000.subreads_to_ccs_aligned.bam \. --output_vcf n1000.depv.vcf.gz \. --output_gvcf n1000.depv.g.vcf.gz \. --num_shards 32 \. --intermediate_results_dir intermediate_results_dir. Error:. ![1688017046760](https://github.com/google/deepvariant/assets/70870741/e5973e9a-c44b-4a12-9179-f4657f63f4bb). Could you help me to solve the problem? Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/672
https://github.com/google/deepvariant/issues/672:92,testability,test,tested,92,"Pacbio revio data Error; hello,. When analyzing PacBio data, I encountered some problems. I tested the example data provided by DeepConsensus and aligned it using pbmm2(1.12.0). When I analyze with DV1.5, I got an error. Data: gs://brain-genomics-public/research/deepconsensus/quickstart/v1.2/n1000.subreads.bam. My cmd:. 1. pbmm2 align hs37d5.fasta fa.fofn n1000.subreads_to_ccs_aligned.bam --sort --rg '@RG\tID:test1\tSM:test1'. 2. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref hs37d5.fasta \. --reads n1000.subreads_to_ccs_aligned.bam \. --output_vcf n1000.depv.vcf.gz \. --output_gvcf n1000.depv.g.vcf.gz \. --num_shards 32 \. --intermediate_results_dir intermediate_results_dir. Error:. ![1688017046760](https://github.com/google/deepvariant/assets/70870741/e5973e9a-c44b-4a12-9179-f4657f63f4bb). Could you help me to solve the problem? Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/672
https://github.com/google/deepvariant/issues/672:18,usability,Error,Error,18,"Pacbio revio data Error; hello,. When analyzing PacBio data, I encountered some problems. I tested the example data provided by DeepConsensus and aligned it using pbmm2(1.12.0). When I analyze with DV1.5, I got an error. Data: gs://brain-genomics-public/research/deepconsensus/quickstart/v1.2/n1000.subreads.bam. My cmd:. 1. pbmm2 align hs37d5.fasta fa.fofn n1000.subreads_to_ccs_aligned.bam --sort --rg '@RG\tID:test1\tSM:test1'. 2. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref hs37d5.fasta \. --reads n1000.subreads_to_ccs_aligned.bam \. --output_vcf n1000.depv.vcf.gz \. --output_gvcf n1000.depv.g.vcf.gz \. --num_shards 32 \. --intermediate_results_dir intermediate_results_dir. Error:. ![1688017046760](https://github.com/google/deepvariant/assets/70870741/e5973e9a-c44b-4a12-9179-f4657f63f4bb). Could you help me to solve the problem? Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/672
https://github.com/google/deepvariant/issues/672:214,usability,error,error,214,"Pacbio revio data Error; hello,. When analyzing PacBio data, I encountered some problems. I tested the example data provided by DeepConsensus and aligned it using pbmm2(1.12.0). When I analyze with DV1.5, I got an error. Data: gs://brain-genomics-public/research/deepconsensus/quickstart/v1.2/n1000.subreads.bam. My cmd:. 1. pbmm2 align hs37d5.fasta fa.fofn n1000.subreads_to_ccs_aligned.bam --sort --rg '@RG\tID:test1\tSM:test1'. 2. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref hs37d5.fasta \. --reads n1000.subreads_to_ccs_aligned.bam \. --output_vcf n1000.depv.vcf.gz \. --output_gvcf n1000.depv.g.vcf.gz \. --num_shards 32 \. --intermediate_results_dir intermediate_results_dir. Error:. ![1688017046760](https://github.com/google/deepvariant/assets/70870741/e5973e9a-c44b-4a12-9179-f4657f63f4bb). Could you help me to solve the problem? Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/672
https://github.com/google/deepvariant/issues/672:707,usability,Error,Error,707,"Pacbio revio data Error; hello,. When analyzing PacBio data, I encountered some problems. I tested the example data provided by DeepConsensus and aligned it using pbmm2(1.12.0). When I analyze with DV1.5, I got an error. Data: gs://brain-genomics-public/research/deepconsensus/quickstart/v1.2/n1000.subreads.bam. My cmd:. 1. pbmm2 align hs37d5.fasta fa.fofn n1000.subreads_to_ccs_aligned.bam --sort --rg '@RG\tID:test1\tSM:test1'. 2. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref hs37d5.fasta \. --reads n1000.subreads_to_ccs_aligned.bam \. --output_vcf n1000.depv.vcf.gz \. --output_gvcf n1000.depv.g.vcf.gz \. --num_shards 32 \. --intermediate_results_dir intermediate_results_dir. Error:. ![1688017046760](https://github.com/google/deepvariant/assets/70870741/e5973e9a-c44b-4a12-9179-f4657f63f4bb). Could you help me to solve the problem? Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/672
https://github.com/google/deepvariant/issues/672:835,usability,help,help,835,"Pacbio revio data Error; hello,. When analyzing PacBio data, I encountered some problems. I tested the example data provided by DeepConsensus and aligned it using pbmm2(1.12.0). When I analyze with DV1.5, I got an error. Data: gs://brain-genomics-public/research/deepconsensus/quickstart/v1.2/n1000.subreads.bam. My cmd:. 1. pbmm2 align hs37d5.fasta fa.fofn n1000.subreads_to_ccs_aligned.bam --sort --rg '@RG\tID:test1\tSM:test1'. 2. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref hs37d5.fasta \. --reads n1000.subreads_to_ccs_aligned.bam \. --output_vcf n1000.depv.vcf.gz \. --output_gvcf n1000.depv.g.vcf.gz \. --num_shards 32 \. --intermediate_results_dir intermediate_results_dir. Error:. ![1688017046760](https://github.com/google/deepvariant/assets/70870741/e5973e9a-c44b-4a12-9179-f4657f63f4bb). Could you help me to solve the problem? Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/672
https://github.com/google/deepvariant/issues/673:168,availability,error,error,168,"ImportError: cannot import name 'path_helpers' from 'tensorflow.python.saved_model'; Dear all,. I was trying to run deepvariant from singularity but I encountered this error:. time /opt/deepvariant/bin/call_variants --outfile ""deepvariant/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""deepvariant/interm. ediate_results_dir/make_examples.tfrecord@12.gz"" --checkpoint ""/opt/models/wes/model.ckpt"". /mnt/.local/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this . version of SciPy (detected version 1.24.2. warnings.warn(f""A NumPy version >={np_minversion} and <{np_maxversion}"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 45, in <module>. from deepvariant import modeling. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 56, in <module>. from tensorflow.python.tpu import tpu_config . File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/tpu_config.py"", line 18, in <module>. from tensorflow_estimator.python.estimator.tpu.tpu_config import *. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1 import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1.estimator import experimental. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 8, in <module>. from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder. File ""/mnt.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py"", line 26, in <module>. from tensorflow_estimator.python.estimator import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimat",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/673
https://github.com/google/deepvariant/issues/673:381,availability,checkpoint,checkpoint,381,"ImportError: cannot import name 'path_helpers' from 'tensorflow.python.saved_model'; Dear all,. I was trying to run deepvariant from singularity but I encountered this error:. time /opt/deepvariant/bin/call_variants --outfile ""deepvariant/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""deepvariant/interm. ediate_results_dir/make_examples.tfrecord@12.gz"" --checkpoint ""/opt/models/wes/model.ckpt"". /mnt/.local/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this . version of SciPy (detected version 1.24.2. warnings.warn(f""A NumPy version >={np_minversion} and <{np_maxversion}"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 45, in <module>. from deepvariant import modeling. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 56, in <module>. from tensorflow.python.tpu import tpu_config . File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/tpu_config.py"", line 18, in <module>. from tensorflow_estimator.python.estimator.tpu.tpu_config import *. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1 import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1.estimator import experimental. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 8, in <module>. from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder. File ""/mnt.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py"", line 26, in <module>. from tensorflow_estimator.python.estimator import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimat",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/673
https://github.com/google/deepvariant/issues/673:2665,availability,cluster,cluster,2665,":. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 45, in <module>. from deepvariant import modeling. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 56, in <module>. from tensorflow.python.tpu import tpu_config . File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/tpu_config.py"", line 18, in <module>. from tensorflow_estimator.python.estimator.tpu.tpu_config import *. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1 import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1.estimator import experimental. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 8, in <module>. from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder. File ""/mnt.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py"", line 26, in <module>. from tensorflow_estimator.python.estimator import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 40, in <module>. from tensorflow.python.saved_model import path_helpers. ImportError: cannot import name 'path_helpers' from 'tensorflow.python.saved_model' (/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_mode. l/__init__.py). Strangely, when I checked the Numpy installed in the conda environment it says 1.22.4. - Command: singularity exec deepvariant.simg /opt/deepvariant/bin/run_deepvariant --model_type WES --ref ref.fasta --reads aln_sncr_fc.bam --output_vcf deepvariant/deepvariant_calls.vcf --num_shards 12 --intermediate_results_dir deepvariant/intermediate_results_dir . I am running it on a slurm cluster. . Thank you very much! Best,. CW.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/673
https://github.com/google/deepvariant/issues/673:506,deployability,version,version,506,"ImportError: cannot import name 'path_helpers' from 'tensorflow.python.saved_model'; Dear all,. I was trying to run deepvariant from singularity but I encountered this error:. time /opt/deepvariant/bin/call_variants --outfile ""deepvariant/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""deepvariant/interm. ediate_results_dir/make_examples.tfrecord@12.gz"" --checkpoint ""/opt/models/wes/model.ckpt"". /mnt/.local/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this . version of SciPy (detected version 1.24.2. warnings.warn(f""A NumPy version >={np_minversion} and <{np_maxversion}"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 45, in <module>. from deepvariant import modeling. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 56, in <module>. from tensorflow.python.tpu import tpu_config . File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/tpu_config.py"", line 18, in <module>. from tensorflow_estimator.python.estimator.tpu.tpu_config import *. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1 import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1.estimator import experimental. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 8, in <module>. from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder. File ""/mnt.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py"", line 26, in <module>. from tensorflow_estimator.python.estimator import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimat",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/673
https://github.com/google/deepvariant/issues/673:558,deployability,version,version,558,"ImportError: cannot import name 'path_helpers' from 'tensorflow.python.saved_model'; Dear all,. I was trying to run deepvariant from singularity but I encountered this error:. time /opt/deepvariant/bin/call_variants --outfile ""deepvariant/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""deepvariant/interm. ediate_results_dir/make_examples.tfrecord@12.gz"" --checkpoint ""/opt/models/wes/model.ckpt"". /mnt/.local/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this . version of SciPy (detected version 1.24.2. warnings.warn(f""A NumPy version >={np_minversion} and <{np_maxversion}"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 45, in <module>. from deepvariant import modeling. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 56, in <module>. from tensorflow.python.tpu import tpu_config . File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/tpu_config.py"", line 18, in <module>. from tensorflow_estimator.python.estimator.tpu.tpu_config import *. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1 import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1.estimator import experimental. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 8, in <module>. from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder. File ""/mnt.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py"", line 26, in <module>. from tensorflow_estimator.python.estimator import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimat",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/673
https://github.com/google/deepvariant/issues/673:585,deployability,version,version,585,"ImportError: cannot import name 'path_helpers' from 'tensorflow.python.saved_model'; Dear all,. I was trying to run deepvariant from singularity but I encountered this error:. time /opt/deepvariant/bin/call_variants --outfile ""deepvariant/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""deepvariant/interm. ediate_results_dir/make_examples.tfrecord@12.gz"" --checkpoint ""/opt/models/wes/model.ckpt"". /mnt/.local/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this . version of SciPy (detected version 1.24.2. warnings.warn(f""A NumPy version >={np_minversion} and <{np_maxversion}"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 45, in <module>. from deepvariant import modeling. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 56, in <module>. from tensorflow.python.tpu import tpu_config . File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/tpu_config.py"", line 18, in <module>. from tensorflow_estimator.python.estimator.tpu.tpu_config import *. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1 import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1.estimator import experimental. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 8, in <module>. from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder. File ""/mnt.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py"", line 26, in <module>. from tensorflow_estimator.python.estimator import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimat",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/673
https://github.com/google/deepvariant/issues/673:625,deployability,version,version,625,"ImportError: cannot import name 'path_helpers' from 'tensorflow.python.saved_model'; Dear all,. I was trying to run deepvariant from singularity but I encountered this error:. time /opt/deepvariant/bin/call_variants --outfile ""deepvariant/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""deepvariant/interm. ediate_results_dir/make_examples.tfrecord@12.gz"" --checkpoint ""/opt/models/wes/model.ckpt"". /mnt/.local/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this . version of SciPy (detected version 1.24.2. warnings.warn(f""A NumPy version >={np_minversion} and <{np_maxversion}"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 45, in <module>. from deepvariant import modeling. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 56, in <module>. from tensorflow.python.tpu import tpu_config . File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/tpu_config.py"", line 18, in <module>. from tensorflow_estimator.python.estimator.tpu.tpu_config import *. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1 import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1.estimator import experimental. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 8, in <module>. from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder. File ""/mnt.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py"", line 26, in <module>. from tensorflow_estimator.python.estimator import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimat",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/673
https://github.com/google/deepvariant/issues/673:821,deployability,modul,module,821,"ImportError: cannot import name 'path_helpers' from 'tensorflow.python.saved_model'; Dear all,. I was trying to run deepvariant from singularity but I encountered this error:. time /opt/deepvariant/bin/call_variants --outfile ""deepvariant/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""deepvariant/interm. ediate_results_dir/make_examples.tfrecord@12.gz"" --checkpoint ""/opt/models/wes/model.ckpt"". /mnt/.local/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this . version of SciPy (detected version 1.24.2. warnings.warn(f""A NumPy version >={np_minversion} and <{np_maxversion}"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 45, in <module>. from deepvariant import modeling. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 56, in <module>. from tensorflow.python.tpu import tpu_config . File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/tpu_config.py"", line 18, in <module>. from tensorflow_estimator.python.estimator.tpu.tpu_config import *. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1 import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1.estimator import experimental. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 8, in <module>. from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder. File ""/mnt.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py"", line 26, in <module>. from tensorflow_estimator.python.estimator import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimat",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/673
https://github.com/google/deepvariant/issues/673:970,deployability,modul,module,970,"ImportError: cannot import name 'path_helpers' from 'tensorflow.python.saved_model'; Dear all,. I was trying to run deepvariant from singularity but I encountered this error:. time /opt/deepvariant/bin/call_variants --outfile ""deepvariant/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""deepvariant/interm. ediate_results_dir/make_examples.tfrecord@12.gz"" --checkpoint ""/opt/models/wes/model.ckpt"". /mnt/.local/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this . version of SciPy (detected version 1.24.2. warnings.warn(f""A NumPy version >={np_minversion} and <{np_maxversion}"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 45, in <module>. from deepvariant import modeling. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 56, in <module>. from tensorflow.python.tpu import tpu_config . File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/tpu_config.py"", line 18, in <module>. from tensorflow_estimator.python.estimator.tpu.tpu_config import *. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1 import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1.estimator import experimental. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 8, in <module>. from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder. File ""/mnt.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py"", line 26, in <module>. from tensorflow_estimator.python.estimator import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimat",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/673
https://github.com/google/deepvariant/issues/673:1122,deployability,modul,module,1122,"nt from singularity but I encountered this error:. time /opt/deepvariant/bin/call_variants --outfile ""deepvariant/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""deepvariant/interm. ediate_results_dir/make_examples.tfrecord@12.gz"" --checkpoint ""/opt/models/wes/model.ckpt"". /mnt/.local/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this . version of SciPy (detected version 1.24.2. warnings.warn(f""A NumPy version >={np_minversion} and <{np_maxversion}"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 45, in <module>. from deepvariant import modeling. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 56, in <module>. from tensorflow.python.tpu import tpu_config . File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/tpu_config.py"", line 18, in <module>. from tensorflow_estimator.python.estimator.tpu.tpu_config import *. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1 import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1.estimator import experimental. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 8, in <module>. from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder. File ""/mnt.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py"", line 26, in <module>. from tensorflow_estimator.python.estimator import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 40, in <module>. from tensorflow.python.saved_model import path_helpers. ImportError:",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/673
https://github.com/google/deepvariant/issues/673:1292,deployability,modul,module,1292,"z"" --examples ""deepvariant/interm. ediate_results_dir/make_examples.tfrecord@12.gz"" --checkpoint ""/opt/models/wes/model.ckpt"". /mnt/.local/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this . version of SciPy (detected version 1.24.2. warnings.warn(f""A NumPy version >={np_minversion} and <{np_maxversion}"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 45, in <module>. from deepvariant import modeling. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 56, in <module>. from tensorflow.python.tpu import tpu_config . File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/tpu_config.py"", line 18, in <module>. from tensorflow_estimator.python.estimator.tpu.tpu_config import *. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1 import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1.estimator import experimental. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 8, in <module>. from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder. File ""/mnt.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py"", line 26, in <module>. from tensorflow_estimator.python.estimator import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 40, in <module>. from tensorflow.python.saved_model import path_helpers. ImportError: cannot import name 'path_helpers' from 'tensorflow.python.saved_model' (/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_mode. l/__init__.py). Strangely, w",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/673
https://github.com/google/deepvariant/issues/673:1464,deployability,modul,module,1464,"/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this . version of SciPy (detected version 1.24.2. warnings.warn(f""A NumPy version >={np_minversion} and <{np_maxversion}"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 45, in <module>. from deepvariant import modeling. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 56, in <module>. from tensorflow.python.tpu import tpu_config . File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/tpu_config.py"", line 18, in <module>. from tensorflow_estimator.python.estimator.tpu.tpu_config import *. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1 import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1.estimator import experimental. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 8, in <module>. from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder. File ""/mnt.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py"", line 26, in <module>. from tensorflow_estimator.python.estimator import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 40, in <module>. from tensorflow.python.saved_model import path_helpers. ImportError: cannot import name 'path_helpers' from 'tensorflow.python.saved_model' (/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_mode. l/__init__.py). Strangely, when I checked the Numpy installed in the conda environment it says 1.22.4. - Command: singularity exec deepvariant.simg /opt/deepvariant/bin/run_deepvariant --model_type WE",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/673
https://github.com/google/deepvariant/issues/673:1662,deployability,modul,module,1662,"rsion}"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 45, in <module>. from deepvariant import modeling. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 56, in <module>. from tensorflow.python.tpu import tpu_config . File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/tpu_config.py"", line 18, in <module>. from tensorflow_estimator.python.estimator.tpu.tpu_config import *. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1 import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1.estimator import experimental. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 8, in <module>. from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder. File ""/mnt.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py"", line 26, in <module>. from tensorflow_estimator.python.estimator import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 40, in <module>. from tensorflow.python.saved_model import path_helpers. ImportError: cannot import name 'path_helpers' from 'tensorflow.python.saved_model' (/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_mode. l/__init__.py). Strangely, when I checked the Numpy installed in the conda environment it says 1.22.4. - Command: singularity exec deepvariant.simg /opt/deepvariant/bin/run_deepvariant --model_type WES --ref ref.fasta --reads aln_sncr_fc.bam --output_vcf deepvariant/deepvariant_calls.vcf --num_shards 12 --intermediate_results_dir deepvariant/intermediate_results_dir . I am running it on a slurm ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/673
https://github.com/google/deepvariant/issues/673:1866,deployability,modul,module,1866,":. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 45, in <module>. from deepvariant import modeling. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 56, in <module>. from tensorflow.python.tpu import tpu_config . File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/tpu_config.py"", line 18, in <module>. from tensorflow_estimator.python.estimator.tpu.tpu_config import *. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1 import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1.estimator import experimental. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 8, in <module>. from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder. File ""/mnt.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py"", line 26, in <module>. from tensorflow_estimator.python.estimator import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 40, in <module>. from tensorflow.python.saved_model import path_helpers. ImportError: cannot import name 'path_helpers' from 'tensorflow.python.saved_model' (/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_mode. l/__init__.py). Strangely, when I checked the Numpy installed in the conda environment it says 1.22.4. - Command: singularity exec deepvariant.simg /opt/deepvariant/bin/run_deepvariant --model_type WES --ref ref.fasta --reads aln_sncr_fc.bam --output_vcf deepvariant/deepvariant_calls.vcf --num_shards 12 --intermediate_results_dir deepvariant/intermediate_results_dir . I am running it on a slurm cluster. . Thank you very much! Best,. CW.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/673
https://github.com/google/deepvariant/issues/673:2048,deployability,modul,module,2048,":. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 45, in <module>. from deepvariant import modeling. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 56, in <module>. from tensorflow.python.tpu import tpu_config . File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/tpu_config.py"", line 18, in <module>. from tensorflow_estimator.python.estimator.tpu.tpu_config import *. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1 import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1.estimator import experimental. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 8, in <module>. from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder. File ""/mnt.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py"", line 26, in <module>. from tensorflow_estimator.python.estimator import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 40, in <module>. from tensorflow.python.saved_model import path_helpers. ImportError: cannot import name 'path_helpers' from 'tensorflow.python.saved_model' (/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_mode. l/__init__.py). Strangely, when I checked the Numpy installed in the conda environment it says 1.22.4. - Command: singularity exec deepvariant.simg /opt/deepvariant/bin/run_deepvariant --model_type WES --ref ref.fasta --reads aln_sncr_fc.bam --output_vcf deepvariant/deepvariant_calls.vcf --num_shards 12 --intermediate_results_dir deepvariant/intermediate_results_dir . I am running it on a slurm cluster. . Thank you very much! Best,. CW.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/673
https://github.com/google/deepvariant/issues/673:2319,deployability,instal,installed,2319,":. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 45, in <module>. from deepvariant import modeling. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 56, in <module>. from tensorflow.python.tpu import tpu_config . File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/tpu_config.py"", line 18, in <module>. from tensorflow_estimator.python.estimator.tpu.tpu_config import *. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1 import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1.estimator import experimental. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 8, in <module>. from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder. File ""/mnt.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py"", line 26, in <module>. from tensorflow_estimator.python.estimator import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 40, in <module>. from tensorflow.python.saved_model import path_helpers. ImportError: cannot import name 'path_helpers' from 'tensorflow.python.saved_model' (/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_mode. l/__init__.py). Strangely, when I checked the Numpy installed in the conda environment it says 1.22.4. - Command: singularity exec deepvariant.simg /opt/deepvariant/bin/run_deepvariant --model_type WES --ref ref.fasta --reads aln_sncr_fc.bam --output_vcf deepvariant/deepvariant_calls.vcf --num_shards 12 --intermediate_results_dir deepvariant/intermediate_results_dir . I am running it on a slurm cluster. . Thank you very much! Best,. CW.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/673
https://github.com/google/deepvariant/issues/673:2665,deployability,cluster,cluster,2665,":. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 45, in <module>. from deepvariant import modeling. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 56, in <module>. from tensorflow.python.tpu import tpu_config . File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/tpu_config.py"", line 18, in <module>. from tensorflow_estimator.python.estimator.tpu.tpu_config import *. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1 import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1.estimator import experimental. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 8, in <module>. from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder. File ""/mnt.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py"", line 26, in <module>. from tensorflow_estimator.python.estimator import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 40, in <module>. from tensorflow.python.saved_model import path_helpers. ImportError: cannot import name 'path_helpers' from 'tensorflow.python.saved_model' (/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_mode. l/__init__.py). Strangely, when I checked the Numpy installed in the conda environment it says 1.22.4. - Command: singularity exec deepvariant.simg /opt/deepvariant/bin/run_deepvariant --model_type WES --ref ref.fasta --reads aln_sncr_fc.bam --output_vcf deepvariant/deepvariant_calls.vcf --num_shards 12 --intermediate_results_dir deepvariant/intermediate_results_dir . I am running it on a slurm cluster. . Thank you very much! Best,. CW.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/673
https://github.com/google/deepvariant/issues/673:398,energy efficiency,model,models,398,"ImportError: cannot import name 'path_helpers' from 'tensorflow.python.saved_model'; Dear all,. I was trying to run deepvariant from singularity but I encountered this error:. time /opt/deepvariant/bin/call_variants --outfile ""deepvariant/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""deepvariant/interm. ediate_results_dir/make_examples.tfrecord@12.gz"" --checkpoint ""/opt/models/wes/model.ckpt"". /mnt/.local/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this . version of SciPy (detected version 1.24.2. warnings.warn(f""A NumPy version >={np_minversion} and <{np_maxversion}"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 45, in <module>. from deepvariant import modeling. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 56, in <module>. from tensorflow.python.tpu import tpu_config . File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/tpu_config.py"", line 18, in <module>. from tensorflow_estimator.python.estimator.tpu.tpu_config import *. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1 import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1.estimator import experimental. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 8, in <module>. from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder. File ""/mnt.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py"", line 26, in <module>. from tensorflow_estimator.python.estimator import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimat",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/673
https://github.com/google/deepvariant/issues/673:409,energy efficiency,model,model,409,"ImportError: cannot import name 'path_helpers' from 'tensorflow.python.saved_model'; Dear all,. I was trying to run deepvariant from singularity but I encountered this error:. time /opt/deepvariant/bin/call_variants --outfile ""deepvariant/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""deepvariant/interm. ediate_results_dir/make_examples.tfrecord@12.gz"" --checkpoint ""/opt/models/wes/model.ckpt"". /mnt/.local/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this . version of SciPy (detected version 1.24.2. warnings.warn(f""A NumPy version >={np_minversion} and <{np_maxversion}"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 45, in <module>. from deepvariant import modeling. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 56, in <module>. from tensorflow.python.tpu import tpu_config . File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/tpu_config.py"", line 18, in <module>. from tensorflow_estimator.python.estimator.tpu.tpu_config import *. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1 import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1.estimator import experimental. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 8, in <module>. from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder. File ""/mnt.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py"", line 26, in <module>. from tensorflow_estimator.python.estimator import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimat",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/673
https://github.com/google/deepvariant/issues/673:854,energy efficiency,model,modeling,854,"ImportError: cannot import name 'path_helpers' from 'tensorflow.python.saved_model'; Dear all,. I was trying to run deepvariant from singularity but I encountered this error:. time /opt/deepvariant/bin/call_variants --outfile ""deepvariant/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""deepvariant/interm. ediate_results_dir/make_examples.tfrecord@12.gz"" --checkpoint ""/opt/models/wes/model.ckpt"". /mnt/.local/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this . version of SciPy (detected version 1.24.2. warnings.warn(f""A NumPy version >={np_minversion} and <{np_maxversion}"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 45, in <module>. from deepvariant import modeling. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 56, in <module>. from tensorflow.python.tpu import tpu_config . File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/tpu_config.py"", line 18, in <module>. from tensorflow_estimator.python.estimator.tpu.tpu_config import *. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1 import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1.estimator import experimental. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 8, in <module>. from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder. File ""/mnt.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py"", line 26, in <module>. from tensorflow_estimator.python.estimator import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimat",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/673
https://github.com/google/deepvariant/issues/673:943,energy efficiency,model,modeling,943,"ImportError: cannot import name 'path_helpers' from 'tensorflow.python.saved_model'; Dear all,. I was trying to run deepvariant from singularity but I encountered this error:. time /opt/deepvariant/bin/call_variants --outfile ""deepvariant/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""deepvariant/interm. ediate_results_dir/make_examples.tfrecord@12.gz"" --checkpoint ""/opt/models/wes/model.ckpt"". /mnt/.local/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this . version of SciPy (detected version 1.24.2. warnings.warn(f""A NumPy version >={np_minversion} and <{np_maxversion}"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 45, in <module>. from deepvariant import modeling. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 56, in <module>. from tensorflow.python.tpu import tpu_config . File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/tpu_config.py"", line 18, in <module>. from tensorflow_estimator.python.estimator.tpu.tpu_config import *. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1 import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1.estimator import experimental. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 8, in <module>. from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder. File ""/mnt.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py"", line 26, in <module>. from tensorflow_estimator.python.estimator import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimat",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/673
https://github.com/google/deepvariant/issues/673:1164,energy efficiency,estimat,estimator,1164,"rror:. time /opt/deepvariant/bin/call_variants --outfile ""deepvariant/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""deepvariant/interm. ediate_results_dir/make_examples.tfrecord@12.gz"" --checkpoint ""/opt/models/wes/model.ckpt"". /mnt/.local/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this . version of SciPy (detected version 1.24.2. warnings.warn(f""A NumPy version >={np_minversion} and <{np_maxversion}"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 45, in <module>. from deepvariant import modeling. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 56, in <module>. from tensorflow.python.tpu import tpu_config . File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/tpu_config.py"", line 18, in <module>. from tensorflow_estimator.python.estimator.tpu.tpu_config import *. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1 import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1.estimator import experimental. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 8, in <module>. from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder. File ""/mnt.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py"", line 26, in <module>. from tensorflow_estimator.python.estimator import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 40, in <module>. from tensorflow.python.saved_model import path_helpers. ImportError: cannot import name 'path_helpers' from 'ten",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/673
https://github.com/google/deepvariant/issues/673:1342,energy efficiency,estimat,estimator,1342,"r/make_examples.tfrecord@12.gz"" --checkpoint ""/opt/models/wes/model.ckpt"". /mnt/.local/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this . version of SciPy (detected version 1.24.2. warnings.warn(f""A NumPy version >={np_minversion} and <{np_maxversion}"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 45, in <module>. from deepvariant import modeling. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 56, in <module>. from tensorflow.python.tpu import tpu_config . File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/tpu_config.py"", line 18, in <module>. from tensorflow_estimator.python.estimator.tpu.tpu_config import *. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1 import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1.estimator import experimental. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 8, in <module>. from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder. File ""/mnt.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py"", line 26, in <module>. from tensorflow_estimator.python.estimator import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 40, in <module>. from tensorflow.python.saved_model import path_helpers. ImportError: cannot import name 'path_helpers' from 'tensorflow.python.saved_model' (/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_mode. l/__init__.py). Strangely, when I checked the Numpy installed in the conda envir",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/673
https://github.com/google/deepvariant/issues/673:1428,energy efficiency,estimat,estimator,1428,"/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this . version of SciPy (detected version 1.24.2. warnings.warn(f""A NumPy version >={np_minversion} and <{np_maxversion}"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 45, in <module>. from deepvariant import modeling. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 56, in <module>. from tensorflow.python.tpu import tpu_config . File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/tpu_config.py"", line 18, in <module>. from tensorflow_estimator.python.estimator.tpu.tpu_config import *. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1 import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1.estimator import experimental. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 8, in <module>. from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder. File ""/mnt.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py"", line 26, in <module>. from tensorflow_estimator.python.estimator import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 40, in <module>. from tensorflow.python.saved_model import path_helpers. ImportError: cannot import name 'path_helpers' from 'tensorflow.python.saved_model' (/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_mode. l/__init__.py). Strangely, when I checked the Numpy installed in the conda environment it says 1.22.4. - Command: singularity exec deepvariant.simg /opt/deepvariant/b",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/673
https://github.com/google/deepvariant/issues/673:1507,energy efficiency,estimat,estimator,1507,"n >=1.16.5 and <1.23.0 is required for this . version of SciPy (detected version 1.24.2. warnings.warn(f""A NumPy version >={np_minversion} and <{np_maxversion}"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 45, in <module>. from deepvariant import modeling. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 56, in <module>. from tensorflow.python.tpu import tpu_config . File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/tpu_config.py"", line 18, in <module>. from tensorflow_estimator.python.estimator.tpu.tpu_config import *. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1 import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1.estimator import experimental. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 8, in <module>. from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder. File ""/mnt.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py"", line 26, in <module>. from tensorflow_estimator.python.estimator import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 40, in <module>. from tensorflow.python.saved_model import path_helpers. ImportError: cannot import name 'path_helpers' from 'tensorflow.python.saved_model' (/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_mode. l/__init__.py). Strangely, when I checked the Numpy installed in the conda environment it says 1.22.4. - Command: singularity exec deepvariant.simg /opt/deepvariant/bin/run_deepvariant --model_type WES --ref ref.fasta --reads aln_sncr_fc.bam --o",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/673
https://github.com/google/deepvariant/issues/673:1613,energy efficiency,estimat,estimator,1613," NumPy version >={np_minversion} and <{np_maxversion}"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 45, in <module>. from deepvariant import modeling. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 56, in <module>. from tensorflow.python.tpu import tpu_config . File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/tpu_config.py"", line 18, in <module>. from tensorflow_estimator.python.estimator.tpu.tpu_config import *. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1 import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1.estimator import experimental. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 8, in <module>. from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder. File ""/mnt.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py"", line 26, in <module>. from tensorflow_estimator.python.estimator import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 40, in <module>. from tensorflow.python.saved_model import path_helpers. ImportError: cannot import name 'path_helpers' from 'tensorflow.python.saved_model' (/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_mode. l/__init__.py). Strangely, when I checked the Numpy installed in the conda environment it says 1.22.4. - Command: singularity exec deepvariant.simg /opt/deepvariant/bin/run_deepvariant --model_type WES --ref ref.fasta --reads aln_sncr_fc.bam --output_vcf deepvariant/deepvariant_calls.vcf --num_shards 12 --intermediate_results_dir deepvariant/interme",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/673
https://github.com/google/deepvariant/issues/673:1704,energy efficiency,estimat,estimator,1704,":. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 45, in <module>. from deepvariant import modeling. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 56, in <module>. from tensorflow.python.tpu import tpu_config . File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/tpu_config.py"", line 18, in <module>. from tensorflow_estimator.python.estimator.tpu.tpu_config import *. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1 import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1.estimator import experimental. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 8, in <module>. from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder. File ""/mnt.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py"", line 26, in <module>. from tensorflow_estimator.python.estimator import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 40, in <module>. from tensorflow.python.saved_model import path_helpers. ImportError: cannot import name 'path_helpers' from 'tensorflow.python.saved_model' (/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_mode. l/__init__.py). Strangely, when I checked the Numpy installed in the conda environment it says 1.22.4. - Command: singularity exec deepvariant.simg /opt/deepvariant/bin/run_deepvariant --model_type WES --ref ref.fasta --reads aln_sncr_fc.bam --output_vcf deepvariant/deepvariant_calls.vcf --num_shards 12 --intermediate_results_dir deepvariant/intermediate_results_dir . I am running it on a slurm cluster. . Thank you very much! Best,. CW.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/673
https://github.com/google/deepvariant/issues/673:1827,energy efficiency,estimat,estimator,1827,":. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 45, in <module>. from deepvariant import modeling. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 56, in <module>. from tensorflow.python.tpu import tpu_config . File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/tpu_config.py"", line 18, in <module>. from tensorflow_estimator.python.estimator.tpu.tpu_config import *. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1 import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1.estimator import experimental. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 8, in <module>. from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder. File ""/mnt.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py"", line 26, in <module>. from tensorflow_estimator.python.estimator import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 40, in <module>. from tensorflow.python.saved_model import path_helpers. ImportError: cannot import name 'path_helpers' from 'tensorflow.python.saved_model' (/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_mode. l/__init__.py). Strangely, when I checked the Numpy installed in the conda environment it says 1.22.4. - Command: singularity exec deepvariant.simg /opt/deepvariant/bin/run_deepvariant --model_type WES --ref ref.fasta --reads aln_sncr_fc.bam --output_vcf deepvariant/deepvariant_calls.vcf --num_shards 12 --intermediate_results_dir deepvariant/intermediate_results_dir . I am running it on a slurm cluster. . Thank you very much! Best,. CW.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/673
https://github.com/google/deepvariant/issues/673:1908,energy efficiency,estimat,estimator,1908,":. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 45, in <module>. from deepvariant import modeling. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 56, in <module>. from tensorflow.python.tpu import tpu_config . File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/tpu_config.py"", line 18, in <module>. from tensorflow_estimator.python.estimator.tpu.tpu_config import *. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1 import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1.estimator import experimental. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 8, in <module>. from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder. File ""/mnt.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py"", line 26, in <module>. from tensorflow_estimator.python.estimator import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 40, in <module>. from tensorflow.python.saved_model import path_helpers. ImportError: cannot import name 'path_helpers' from 'tensorflow.python.saved_model' (/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_mode. l/__init__.py). Strangely, when I checked the Numpy installed in the conda environment it says 1.22.4. - Command: singularity exec deepvariant.simg /opt/deepvariant/bin/run_deepvariant --model_type WES --ref ref.fasta --reads aln_sncr_fc.bam --output_vcf deepvariant/deepvariant_calls.vcf --num_shards 12 --intermediate_results_dir deepvariant/intermediate_results_dir . I am running it on a slurm cluster. . Thank you very much! Best,. CW.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/673
https://github.com/google/deepvariant/issues/673:1925,energy efficiency,estimat,estimator,1925,":. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 45, in <module>. from deepvariant import modeling. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 56, in <module>. from tensorflow.python.tpu import tpu_config . File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/tpu_config.py"", line 18, in <module>. from tensorflow_estimator.python.estimator.tpu.tpu_config import *. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1 import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1.estimator import experimental. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 8, in <module>. from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder. File ""/mnt.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py"", line 26, in <module>. from tensorflow_estimator.python.estimator import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 40, in <module>. from tensorflow.python.saved_model import path_helpers. ImportError: cannot import name 'path_helpers' from 'tensorflow.python.saved_model' (/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_mode. l/__init__.py). Strangely, when I checked the Numpy installed in the conda environment it says 1.22.4. - Command: singularity exec deepvariant.simg /opt/deepvariant/bin/run_deepvariant --model_type WES --ref ref.fasta --reads aln_sncr_fc.bam --output_vcf deepvariant/deepvariant_calls.vcf --num_shards 12 --intermediate_results_dir deepvariant/intermediate_results_dir . I am running it on a slurm cluster. . Thank you very much! Best,. CW.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/673
https://github.com/google/deepvariant/issues/673:2010,energy efficiency,estimat,estimator,2010,":. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 45, in <module>. from deepvariant import modeling. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 56, in <module>. from tensorflow.python.tpu import tpu_config . File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/tpu_config.py"", line 18, in <module>. from tensorflow_estimator.python.estimator.tpu.tpu_config import *. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1 import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1.estimator import experimental. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 8, in <module>. from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder. File ""/mnt.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py"", line 26, in <module>. from tensorflow_estimator.python.estimator import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 40, in <module>. from tensorflow.python.saved_model import path_helpers. ImportError: cannot import name 'path_helpers' from 'tensorflow.python.saved_model' (/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_mode. l/__init__.py). Strangely, when I checked the Numpy installed in the conda environment it says 1.22.4. - Command: singularity exec deepvariant.simg /opt/deepvariant/bin/run_deepvariant --model_type WES --ref ref.fasta --reads aln_sncr_fc.bam --output_vcf deepvariant/deepvariant_calls.vcf --num_shards 12 --intermediate_results_dir deepvariant/intermediate_results_dir . I am running it on a slurm cluster. . Thank you very much! Best,. CW.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/673
https://github.com/google/deepvariant/issues/673:2020,energy efficiency,estimat,estimator,2020,":. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 45, in <module>. from deepvariant import modeling. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 56, in <module>. from tensorflow.python.tpu import tpu_config . File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/tpu_config.py"", line 18, in <module>. from tensorflow_estimator.python.estimator.tpu.tpu_config import *. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1 import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1.estimator import experimental. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 8, in <module>. from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder. File ""/mnt.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py"", line 26, in <module>. from tensorflow_estimator.python.estimator import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 40, in <module>. from tensorflow.python.saved_model import path_helpers. ImportError: cannot import name 'path_helpers' from 'tensorflow.python.saved_model' (/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_mode. l/__init__.py). Strangely, when I checked the Numpy installed in the conda environment it says 1.22.4. - Command: singularity exec deepvariant.simg /opt/deepvariant/bin/run_deepvariant --model_type WES --ref ref.fasta --reads aln_sncr_fc.bam --output_vcf deepvariant/deepvariant_calls.vcf --num_shards 12 --intermediate_results_dir deepvariant/intermediate_results_dir . I am running it on a slurm cluster. . Thank you very much! Best,. CW.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/673
https://github.com/google/deepvariant/issues/673:506,integrability,version,version,506,"ImportError: cannot import name 'path_helpers' from 'tensorflow.python.saved_model'; Dear all,. I was trying to run deepvariant from singularity but I encountered this error:. time /opt/deepvariant/bin/call_variants --outfile ""deepvariant/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""deepvariant/interm. ediate_results_dir/make_examples.tfrecord@12.gz"" --checkpoint ""/opt/models/wes/model.ckpt"". /mnt/.local/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this . version of SciPy (detected version 1.24.2. warnings.warn(f""A NumPy version >={np_minversion} and <{np_maxversion}"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 45, in <module>. from deepvariant import modeling. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 56, in <module>. from tensorflow.python.tpu import tpu_config . File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/tpu_config.py"", line 18, in <module>. from tensorflow_estimator.python.estimator.tpu.tpu_config import *. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1 import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1.estimator import experimental. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 8, in <module>. from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder. File ""/mnt.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py"", line 26, in <module>. from tensorflow_estimator.python.estimator import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimat",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/673
https://github.com/google/deepvariant/issues/673:558,integrability,version,version,558,"ImportError: cannot import name 'path_helpers' from 'tensorflow.python.saved_model'; Dear all,. I was trying to run deepvariant from singularity but I encountered this error:. time /opt/deepvariant/bin/call_variants --outfile ""deepvariant/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""deepvariant/interm. ediate_results_dir/make_examples.tfrecord@12.gz"" --checkpoint ""/opt/models/wes/model.ckpt"". /mnt/.local/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this . version of SciPy (detected version 1.24.2. warnings.warn(f""A NumPy version >={np_minversion} and <{np_maxversion}"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 45, in <module>. from deepvariant import modeling. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 56, in <module>. from tensorflow.python.tpu import tpu_config . File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/tpu_config.py"", line 18, in <module>. from tensorflow_estimator.python.estimator.tpu.tpu_config import *. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1 import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1.estimator import experimental. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 8, in <module>. from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder. File ""/mnt.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py"", line 26, in <module>. from tensorflow_estimator.python.estimator import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimat",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/673
https://github.com/google/deepvariant/issues/673:585,integrability,version,version,585,"ImportError: cannot import name 'path_helpers' from 'tensorflow.python.saved_model'; Dear all,. I was trying to run deepvariant from singularity but I encountered this error:. time /opt/deepvariant/bin/call_variants --outfile ""deepvariant/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""deepvariant/interm. ediate_results_dir/make_examples.tfrecord@12.gz"" --checkpoint ""/opt/models/wes/model.ckpt"". /mnt/.local/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this . version of SciPy (detected version 1.24.2. warnings.warn(f""A NumPy version >={np_minversion} and <{np_maxversion}"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 45, in <module>. from deepvariant import modeling. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 56, in <module>. from tensorflow.python.tpu import tpu_config . File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/tpu_config.py"", line 18, in <module>. from tensorflow_estimator.python.estimator.tpu.tpu_config import *. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1 import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1.estimator import experimental. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 8, in <module>. from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder. File ""/mnt.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py"", line 26, in <module>. from tensorflow_estimator.python.estimator import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimat",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/673
https://github.com/google/deepvariant/issues/673:625,integrability,version,version,625,"ImportError: cannot import name 'path_helpers' from 'tensorflow.python.saved_model'; Dear all,. I was trying to run deepvariant from singularity but I encountered this error:. time /opt/deepvariant/bin/call_variants --outfile ""deepvariant/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""deepvariant/interm. ediate_results_dir/make_examples.tfrecord@12.gz"" --checkpoint ""/opt/models/wes/model.ckpt"". /mnt/.local/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this . version of SciPy (detected version 1.24.2. warnings.warn(f""A NumPy version >={np_minversion} and <{np_maxversion}"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 45, in <module>. from deepvariant import modeling. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 56, in <module>. from tensorflow.python.tpu import tpu_config . File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/tpu_config.py"", line 18, in <module>. from tensorflow_estimator.python.estimator.tpu.tpu_config import *. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1 import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1.estimator import experimental. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 8, in <module>. from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder. File ""/mnt.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py"", line 26, in <module>. from tensorflow_estimator.python.estimator import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimat",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/673
https://github.com/google/deepvariant/issues/673:322,modifiability,interm,interm,322,"ImportError: cannot import name 'path_helpers' from 'tensorflow.python.saved_model'; Dear all,. I was trying to run deepvariant from singularity but I encountered this error:. time /opt/deepvariant/bin/call_variants --outfile ""deepvariant/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""deepvariant/interm. ediate_results_dir/make_examples.tfrecord@12.gz"" --checkpoint ""/opt/models/wes/model.ckpt"". /mnt/.local/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this . version of SciPy (detected version 1.24.2. warnings.warn(f""A NumPy version >={np_minversion} and <{np_maxversion}"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 45, in <module>. from deepvariant import modeling. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 56, in <module>. from tensorflow.python.tpu import tpu_config . File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/tpu_config.py"", line 18, in <module>. from tensorflow_estimator.python.estimator.tpu.tpu_config import *. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1 import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1.estimator import experimental. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 8, in <module>. from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder. File ""/mnt.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py"", line 26, in <module>. from tensorflow_estimator.python.estimator import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimat",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/673
https://github.com/google/deepvariant/issues/673:453,modifiability,pac,packages,453,"ImportError: cannot import name 'path_helpers' from 'tensorflow.python.saved_model'; Dear all,. I was trying to run deepvariant from singularity but I encountered this error:. time /opt/deepvariant/bin/call_variants --outfile ""deepvariant/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""deepvariant/interm. ediate_results_dir/make_examples.tfrecord@12.gz"" --checkpoint ""/opt/models/wes/model.ckpt"". /mnt/.local/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this . version of SciPy (detected version 1.24.2. warnings.warn(f""A NumPy version >={np_minversion} and <{np_maxversion}"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 45, in <module>. from deepvariant import modeling. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 56, in <module>. from tensorflow.python.tpu import tpu_config . File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/tpu_config.py"", line 18, in <module>. from tensorflow_estimator.python.estimator.tpu.tpu_config import *. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1 import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1.estimator import experimental. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 8, in <module>. from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder. File ""/mnt.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py"", line 26, in <module>. from tensorflow_estimator.python.estimator import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimat",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/673
https://github.com/google/deepvariant/issues/673:506,modifiability,version,version,506,"ImportError: cannot import name 'path_helpers' from 'tensorflow.python.saved_model'; Dear all,. I was trying to run deepvariant from singularity but I encountered this error:. time /opt/deepvariant/bin/call_variants --outfile ""deepvariant/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""deepvariant/interm. ediate_results_dir/make_examples.tfrecord@12.gz"" --checkpoint ""/opt/models/wes/model.ckpt"". /mnt/.local/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this . version of SciPy (detected version 1.24.2. warnings.warn(f""A NumPy version >={np_minversion} and <{np_maxversion}"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 45, in <module>. from deepvariant import modeling. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 56, in <module>. from tensorflow.python.tpu import tpu_config . File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/tpu_config.py"", line 18, in <module>. from tensorflow_estimator.python.estimator.tpu.tpu_config import *. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1 import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1.estimator import experimental. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 8, in <module>. from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder. File ""/mnt.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py"", line 26, in <module>. from tensorflow_estimator.python.estimator import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimat",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/673
https://github.com/google/deepvariant/issues/673:558,modifiability,version,version,558,"ImportError: cannot import name 'path_helpers' from 'tensorflow.python.saved_model'; Dear all,. I was trying to run deepvariant from singularity but I encountered this error:. time /opt/deepvariant/bin/call_variants --outfile ""deepvariant/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""deepvariant/interm. ediate_results_dir/make_examples.tfrecord@12.gz"" --checkpoint ""/opt/models/wes/model.ckpt"". /mnt/.local/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this . version of SciPy (detected version 1.24.2. warnings.warn(f""A NumPy version >={np_minversion} and <{np_maxversion}"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 45, in <module>. from deepvariant import modeling. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 56, in <module>. from tensorflow.python.tpu import tpu_config . File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/tpu_config.py"", line 18, in <module>. from tensorflow_estimator.python.estimator.tpu.tpu_config import *. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1 import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1.estimator import experimental. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 8, in <module>. from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder. File ""/mnt.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py"", line 26, in <module>. from tensorflow_estimator.python.estimator import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimat",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/673
https://github.com/google/deepvariant/issues/673:585,modifiability,version,version,585,"ImportError: cannot import name 'path_helpers' from 'tensorflow.python.saved_model'; Dear all,. I was trying to run deepvariant from singularity but I encountered this error:. time /opt/deepvariant/bin/call_variants --outfile ""deepvariant/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""deepvariant/interm. ediate_results_dir/make_examples.tfrecord@12.gz"" --checkpoint ""/opt/models/wes/model.ckpt"". /mnt/.local/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this . version of SciPy (detected version 1.24.2. warnings.warn(f""A NumPy version >={np_minversion} and <{np_maxversion}"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 45, in <module>. from deepvariant import modeling. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 56, in <module>. from tensorflow.python.tpu import tpu_config . File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/tpu_config.py"", line 18, in <module>. from tensorflow_estimator.python.estimator.tpu.tpu_config import *. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1 import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1.estimator import experimental. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 8, in <module>. from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder. File ""/mnt.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py"", line 26, in <module>. from tensorflow_estimator.python.estimator import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimat",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/673
https://github.com/google/deepvariant/issues/673:625,modifiability,version,version,625,"ImportError: cannot import name 'path_helpers' from 'tensorflow.python.saved_model'; Dear all,. I was trying to run deepvariant from singularity but I encountered this error:. time /opt/deepvariant/bin/call_variants --outfile ""deepvariant/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""deepvariant/interm. ediate_results_dir/make_examples.tfrecord@12.gz"" --checkpoint ""/opt/models/wes/model.ckpt"". /mnt/.local/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this . version of SciPy (detected version 1.24.2. warnings.warn(f""A NumPy version >={np_minversion} and <{np_maxversion}"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 45, in <module>. from deepvariant import modeling. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 56, in <module>. from tensorflow.python.tpu import tpu_config . File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/tpu_config.py"", line 18, in <module>. from tensorflow_estimator.python.estimator.tpu.tpu_config import *. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1 import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1.estimator import experimental. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 8, in <module>. from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder. File ""/mnt.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py"", line 26, in <module>. from tensorflow_estimator.python.estimator import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimat",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/673
https://github.com/google/deepvariant/issues/673:821,modifiability,modul,module,821,"ImportError: cannot import name 'path_helpers' from 'tensorflow.python.saved_model'; Dear all,. I was trying to run deepvariant from singularity but I encountered this error:. time /opt/deepvariant/bin/call_variants --outfile ""deepvariant/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""deepvariant/interm. ediate_results_dir/make_examples.tfrecord@12.gz"" --checkpoint ""/opt/models/wes/model.ckpt"". /mnt/.local/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this . version of SciPy (detected version 1.24.2. warnings.warn(f""A NumPy version >={np_minversion} and <{np_maxversion}"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 45, in <module>. from deepvariant import modeling. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 56, in <module>. from tensorflow.python.tpu import tpu_config . File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/tpu_config.py"", line 18, in <module>. from tensorflow_estimator.python.estimator.tpu.tpu_config import *. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1 import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1.estimator import experimental. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 8, in <module>. from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder. File ""/mnt.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py"", line 26, in <module>. from tensorflow_estimator.python.estimator import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimat",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/673
https://github.com/google/deepvariant/issues/673:970,modifiability,modul,module,970,"ImportError: cannot import name 'path_helpers' from 'tensorflow.python.saved_model'; Dear all,. I was trying to run deepvariant from singularity but I encountered this error:. time /opt/deepvariant/bin/call_variants --outfile ""deepvariant/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""deepvariant/interm. ediate_results_dir/make_examples.tfrecord@12.gz"" --checkpoint ""/opt/models/wes/model.ckpt"". /mnt/.local/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this . version of SciPy (detected version 1.24.2. warnings.warn(f""A NumPy version >={np_minversion} and <{np_maxversion}"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 45, in <module>. from deepvariant import modeling. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 56, in <module>. from tensorflow.python.tpu import tpu_config . File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/tpu_config.py"", line 18, in <module>. from tensorflow_estimator.python.estimator.tpu.tpu_config import *. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1 import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1.estimator import experimental. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 8, in <module>. from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder. File ""/mnt.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py"", line 26, in <module>. from tensorflow_estimator.python.estimator import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimat",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/673
https://github.com/google/deepvariant/issues/673:1062,modifiability,pac,packages,1062,"thon.saved_model'; Dear all,. I was trying to run deepvariant from singularity but I encountered this error:. time /opt/deepvariant/bin/call_variants --outfile ""deepvariant/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""deepvariant/interm. ediate_results_dir/make_examples.tfrecord@12.gz"" --checkpoint ""/opt/models/wes/model.ckpt"". /mnt/.local/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this . version of SciPy (detected version 1.24.2. warnings.warn(f""A NumPy version >={np_minversion} and <{np_maxversion}"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 45, in <module>. from deepvariant import modeling. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 56, in <module>. from tensorflow.python.tpu import tpu_config . File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/tpu_config.py"", line 18, in <module>. from tensorflow_estimator.python.estimator.tpu.tpu_config import *. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1 import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1.estimator import experimental. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 8, in <module>. from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder. File ""/mnt.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py"", line 26, in <module>. from tensorflow_estimator.python.estimator import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 40, in <module>. from tens",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/673
https://github.com/google/deepvariant/issues/673:1122,modifiability,modul,module,1122,"nt from singularity but I encountered this error:. time /opt/deepvariant/bin/call_variants --outfile ""deepvariant/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""deepvariant/interm. ediate_results_dir/make_examples.tfrecord@12.gz"" --checkpoint ""/opt/models/wes/model.ckpt"". /mnt/.local/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this . version of SciPy (detected version 1.24.2. warnings.warn(f""A NumPy version >={np_minversion} and <{np_maxversion}"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 45, in <module>. from deepvariant import modeling. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 56, in <module>. from tensorflow.python.tpu import tpu_config . File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/tpu_config.py"", line 18, in <module>. from tensorflow_estimator.python.estimator.tpu.tpu_config import *. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1 import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1.estimator import experimental. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 8, in <module>. from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder. File ""/mnt.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py"", line 26, in <module>. from tensorflow_estimator.python.estimator import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 40, in <module>. from tensorflow.python.saved_model import path_helpers. ImportError:",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/673
https://github.com/google/deepvariant/issues/673:1236,modifiability,pac,packages,1236,"ntermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""deepvariant/interm. ediate_results_dir/make_examples.tfrecord@12.gz"" --checkpoint ""/opt/models/wes/model.ckpt"". /mnt/.local/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this . version of SciPy (detected version 1.24.2. warnings.warn(f""A NumPy version >={np_minversion} and <{np_maxversion}"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 45, in <module>. from deepvariant import modeling. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 56, in <module>. from tensorflow.python.tpu import tpu_config . File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/tpu_config.py"", line 18, in <module>. from tensorflow_estimator.python.estimator.tpu.tpu_config import *. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1 import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1.estimator import experimental. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 8, in <module>. from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder. File ""/mnt.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py"", line 26, in <module>. from tensorflow_estimator.python.estimator import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 40, in <module>. from tensorflow.python.saved_model import path_helpers. ImportError: cannot import name 'path_helpers' from 'tensorflow.python.saved_model' (/usr/local/lib/python3.8/dist-packages/ten",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/673
https://github.com/google/deepvariant/issues/673:1292,modifiability,modul,module,1292,"z"" --examples ""deepvariant/interm. ediate_results_dir/make_examples.tfrecord@12.gz"" --checkpoint ""/opt/models/wes/model.ckpt"". /mnt/.local/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this . version of SciPy (detected version 1.24.2. warnings.warn(f""A NumPy version >={np_minversion} and <{np_maxversion}"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 45, in <module>. from deepvariant import modeling. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 56, in <module>. from tensorflow.python.tpu import tpu_config . File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/tpu_config.py"", line 18, in <module>. from tensorflow_estimator.python.estimator.tpu.tpu_config import *. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1 import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1.estimator import experimental. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 8, in <module>. from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder. File ""/mnt.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py"", line 26, in <module>. from tensorflow_estimator.python.estimator import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 40, in <module>. from tensorflow.python.saved_model import path_helpers. ImportError: cannot import name 'path_helpers' from 'tensorflow.python.saved_model' (/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_mode. l/__init__.py). Strangely, w",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/673
https://github.com/google/deepvariant/issues/673:1390,modifiability,pac,packages,1390,"opt/models/wes/model.ckpt"". /mnt/.local/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this . version of SciPy (detected version 1.24.2. warnings.warn(f""A NumPy version >={np_minversion} and <{np_maxversion}"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 45, in <module>. from deepvariant import modeling. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 56, in <module>. from tensorflow.python.tpu import tpu_config . File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/tpu_config.py"", line 18, in <module>. from tensorflow_estimator.python.estimator.tpu.tpu_config import *. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1 import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1.estimator import experimental. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 8, in <module>. from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder. File ""/mnt.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py"", line 26, in <module>. from tensorflow_estimator.python.estimator import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 40, in <module>. from tensorflow.python.saved_model import path_helpers. ImportError: cannot import name 'path_helpers' from 'tensorflow.python.saved_model' (/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_mode. l/__init__.py). Strangely, when I checked the Numpy installed in the conda environment it says 1.22.4. - Command: singularity e",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/673
https://github.com/google/deepvariant/issues/673:1464,modifiability,modul,module,1464,"/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this . version of SciPy (detected version 1.24.2. warnings.warn(f""A NumPy version >={np_minversion} and <{np_maxversion}"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 45, in <module>. from deepvariant import modeling. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 56, in <module>. from tensorflow.python.tpu import tpu_config . File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/tpu_config.py"", line 18, in <module>. from tensorflow_estimator.python.estimator.tpu.tpu_config import *. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1 import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1.estimator import experimental. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 8, in <module>. from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder. File ""/mnt.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py"", line 26, in <module>. from tensorflow_estimator.python.estimator import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 40, in <module>. from tensorflow.python.saved_model import path_helpers. ImportError: cannot import name 'path_helpers' from 'tensorflow.python.saved_model' (/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_mode. l/__init__.py). Strangely, when I checked the Numpy installed in the conda environment it says 1.22.4. - Command: singularity exec deepvariant.simg /opt/deepvariant/bin/run_deepvariant --model_type WE",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/673
https://github.com/google/deepvariant/issues/673:1575,modifiability,pac,packages,1575,"ected version 1.24.2. warnings.warn(f""A NumPy version >={np_minversion} and <{np_maxversion}"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 45, in <module>. from deepvariant import modeling. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 56, in <module>. from tensorflow.python.tpu import tpu_config . File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/tpu_config.py"", line 18, in <module>. from tensorflow_estimator.python.estimator.tpu.tpu_config import *. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1 import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1.estimator import experimental. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 8, in <module>. from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder. File ""/mnt.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py"", line 26, in <module>. from tensorflow_estimator.python.estimator import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 40, in <module>. from tensorflow.python.saved_model import path_helpers. ImportError: cannot import name 'path_helpers' from 'tensorflow.python.saved_model' (/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_mode. l/__init__.py). Strangely, when I checked the Numpy installed in the conda environment it says 1.22.4. - Command: singularity exec deepvariant.simg /opt/deepvariant/bin/run_deepvariant --model_type WES --ref ref.fasta --reads aln_sncr_fc.bam --output_vcf deepvariant/deepvariant_calls.vcf --num_shards 12 --inter",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/673
https://github.com/google/deepvariant/issues/673:1662,modifiability,modul,module,1662,"rsion}"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 45, in <module>. from deepvariant import modeling. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 56, in <module>. from tensorflow.python.tpu import tpu_config . File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/tpu_config.py"", line 18, in <module>. from tensorflow_estimator.python.estimator.tpu.tpu_config import *. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1 import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1.estimator import experimental. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 8, in <module>. from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder. File ""/mnt.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py"", line 26, in <module>. from tensorflow_estimator.python.estimator import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 40, in <module>. from tensorflow.python.saved_model import path_helpers. ImportError: cannot import name 'path_helpers' from 'tensorflow.python.saved_model' (/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_mode. l/__init__.py). Strangely, when I checked the Numpy installed in the conda environment it says 1.22.4. - Command: singularity exec deepvariant.simg /opt/deepvariant/bin/run_deepvariant --model_type WES --ref ref.fasta --reads aln_sncr_fc.bam --output_vcf deepvariant/deepvariant_calls.vcf --num_shards 12 --intermediate_results_dir deepvariant/intermediate_results_dir . I am running it on a slurm ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/673
https://github.com/google/deepvariant/issues/673:1790,modifiability,pac,packages,1790,":. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 45, in <module>. from deepvariant import modeling. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 56, in <module>. from tensorflow.python.tpu import tpu_config . File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/tpu_config.py"", line 18, in <module>. from tensorflow_estimator.python.estimator.tpu.tpu_config import *. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1 import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1.estimator import experimental. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 8, in <module>. from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder. File ""/mnt.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py"", line 26, in <module>. from tensorflow_estimator.python.estimator import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 40, in <module>. from tensorflow.python.saved_model import path_helpers. ImportError: cannot import name 'path_helpers' from 'tensorflow.python.saved_model' (/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_mode. l/__init__.py). Strangely, when I checked the Numpy installed in the conda environment it says 1.22.4. - Command: singularity exec deepvariant.simg /opt/deepvariant/bin/run_deepvariant --model_type WES --ref ref.fasta --reads aln_sncr_fc.bam --output_vcf deepvariant/deepvariant_calls.vcf --num_shards 12 --intermediate_results_dir deepvariant/intermediate_results_dir . I am running it on a slurm cluster. . Thank you very much! Best,. CW.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/673
https://github.com/google/deepvariant/issues/673:1866,modifiability,modul,module,1866,":. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 45, in <module>. from deepvariant import modeling. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 56, in <module>. from tensorflow.python.tpu import tpu_config . File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/tpu_config.py"", line 18, in <module>. from tensorflow_estimator.python.estimator.tpu.tpu_config import *. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1 import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1.estimator import experimental. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 8, in <module>. from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder. File ""/mnt.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py"", line 26, in <module>. from tensorflow_estimator.python.estimator import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 40, in <module>. from tensorflow.python.saved_model import path_helpers. ImportError: cannot import name 'path_helpers' from 'tensorflow.python.saved_model' (/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_mode. l/__init__.py). Strangely, when I checked the Numpy installed in the conda environment it says 1.22.4. - Command: singularity exec deepvariant.simg /opt/deepvariant/bin/run_deepvariant --model_type WES --ref ref.fasta --reads aln_sncr_fc.bam --output_vcf deepvariant/deepvariant_calls.vcf --num_shards 12 --intermediate_results_dir deepvariant/intermediate_results_dir . I am running it on a slurm cluster. . Thank you very much! Best,. CW.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/673
https://github.com/google/deepvariant/issues/673:1973,modifiability,pac,packages,1973,":. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 45, in <module>. from deepvariant import modeling. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 56, in <module>. from tensorflow.python.tpu import tpu_config . File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/tpu_config.py"", line 18, in <module>. from tensorflow_estimator.python.estimator.tpu.tpu_config import *. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1 import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1.estimator import experimental. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 8, in <module>. from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder. File ""/mnt.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py"", line 26, in <module>. from tensorflow_estimator.python.estimator import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 40, in <module>. from tensorflow.python.saved_model import path_helpers. ImportError: cannot import name 'path_helpers' from 'tensorflow.python.saved_model' (/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_mode. l/__init__.py). Strangely, when I checked the Numpy installed in the conda environment it says 1.22.4. - Command: singularity exec deepvariant.simg /opt/deepvariant/bin/run_deepvariant --model_type WES --ref ref.fasta --reads aln_sncr_fc.bam --output_vcf deepvariant/deepvariant_calls.vcf --num_shards 12 --intermediate_results_dir deepvariant/intermediate_results_dir . I am running it on a slurm cluster. . Thank you very much! Best,. CW.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/673
https://github.com/google/deepvariant/issues/673:2048,modifiability,modul,module,2048,":. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 45, in <module>. from deepvariant import modeling. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 56, in <module>. from tensorflow.python.tpu import tpu_config . File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/tpu_config.py"", line 18, in <module>. from tensorflow_estimator.python.estimator.tpu.tpu_config import *. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1 import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1.estimator import experimental. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 8, in <module>. from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder. File ""/mnt.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py"", line 26, in <module>. from tensorflow_estimator.python.estimator import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 40, in <module>. from tensorflow.python.saved_model import path_helpers. ImportError: cannot import name 'path_helpers' from 'tensorflow.python.saved_model' (/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_mode. l/__init__.py). Strangely, when I checked the Numpy installed in the conda environment it says 1.22.4. - Command: singularity exec deepvariant.simg /opt/deepvariant/bin/run_deepvariant --model_type WES --ref ref.fasta --reads aln_sncr_fc.bam --output_vcf deepvariant/deepvariant_calls.vcf --num_shards 12 --intermediate_results_dir deepvariant/intermediate_results_dir . I am running it on a slurm cluster. . Thank you very much! Best,. CW.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/673
https://github.com/google/deepvariant/issues/673:2228,modifiability,pac,packages,2228,":. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 45, in <module>. from deepvariant import modeling. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 56, in <module>. from tensorflow.python.tpu import tpu_config . File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/tpu_config.py"", line 18, in <module>. from tensorflow_estimator.python.estimator.tpu.tpu_config import *. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1 import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1.estimator import experimental. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 8, in <module>. from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder. File ""/mnt.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py"", line 26, in <module>. from tensorflow_estimator.python.estimator import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 40, in <module>. from tensorflow.python.saved_model import path_helpers. ImportError: cannot import name 'path_helpers' from 'tensorflow.python.saved_model' (/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_mode. l/__init__.py). Strangely, when I checked the Numpy installed in the conda environment it says 1.22.4. - Command: singularity exec deepvariant.simg /opt/deepvariant/bin/run_deepvariant --model_type WES --ref ref.fasta --reads aln_sncr_fc.bam --output_vcf deepvariant/deepvariant_calls.vcf --num_shards 12 --intermediate_results_dir deepvariant/intermediate_results_dir . I am running it on a slurm cluster. . Thank you very much! Best,. CW.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/673
https://github.com/google/deepvariant/issues/673:168,performance,error,error,168,"ImportError: cannot import name 'path_helpers' from 'tensorflow.python.saved_model'; Dear all,. I was trying to run deepvariant from singularity but I encountered this error:. time /opt/deepvariant/bin/call_variants --outfile ""deepvariant/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""deepvariant/interm. ediate_results_dir/make_examples.tfrecord@12.gz"" --checkpoint ""/opt/models/wes/model.ckpt"". /mnt/.local/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this . version of SciPy (detected version 1.24.2. warnings.warn(f""A NumPy version >={np_minversion} and <{np_maxversion}"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 45, in <module>. from deepvariant import modeling. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 56, in <module>. from tensorflow.python.tpu import tpu_config . File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/tpu_config.py"", line 18, in <module>. from tensorflow_estimator.python.estimator.tpu.tpu_config import *. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1 import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1.estimator import experimental. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 8, in <module>. from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder. File ""/mnt.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py"", line 26, in <module>. from tensorflow_estimator.python.estimator import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimat",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/673
https://github.com/google/deepvariant/issues/673:176,performance,time,time,176,"ImportError: cannot import name 'path_helpers' from 'tensorflow.python.saved_model'; Dear all,. I was trying to run deepvariant from singularity but I encountered this error:. time /opt/deepvariant/bin/call_variants --outfile ""deepvariant/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""deepvariant/interm. ediate_results_dir/make_examples.tfrecord@12.gz"" --checkpoint ""/opt/models/wes/model.ckpt"". /mnt/.local/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this . version of SciPy (detected version 1.24.2. warnings.warn(f""A NumPy version >={np_minversion} and <{np_maxversion}"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 45, in <module>. from deepvariant import modeling. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 56, in <module>. from tensorflow.python.tpu import tpu_config . File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/tpu_config.py"", line 18, in <module>. from tensorflow_estimator.python.estimator.tpu.tpu_config import *. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1 import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1.estimator import experimental. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 8, in <module>. from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder. File ""/mnt.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py"", line 26, in <module>. from tensorflow_estimator.python.estimator import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimat",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/673
https://github.com/google/deepvariant/issues/673:381,reliability,checkpoint,checkpoint,381,"ImportError: cannot import name 'path_helpers' from 'tensorflow.python.saved_model'; Dear all,. I was trying to run deepvariant from singularity but I encountered this error:. time /opt/deepvariant/bin/call_variants --outfile ""deepvariant/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""deepvariant/interm. ediate_results_dir/make_examples.tfrecord@12.gz"" --checkpoint ""/opt/models/wes/model.ckpt"". /mnt/.local/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this . version of SciPy (detected version 1.24.2. warnings.warn(f""A NumPy version >={np_minversion} and <{np_maxversion}"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 45, in <module>. from deepvariant import modeling. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 56, in <module>. from tensorflow.python.tpu import tpu_config . File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/tpu_config.py"", line 18, in <module>. from tensorflow_estimator.python.estimator.tpu.tpu_config import *. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1 import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1.estimator import experimental. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 8, in <module>. from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder. File ""/mnt.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py"", line 26, in <module>. from tensorflow_estimator.python.estimator import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimat",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/673
https://github.com/google/deepvariant/issues/673:168,safety,error,error,168,"ImportError: cannot import name 'path_helpers' from 'tensorflow.python.saved_model'; Dear all,. I was trying to run deepvariant from singularity but I encountered this error:. time /opt/deepvariant/bin/call_variants --outfile ""deepvariant/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""deepvariant/interm. ediate_results_dir/make_examples.tfrecord@12.gz"" --checkpoint ""/opt/models/wes/model.ckpt"". /mnt/.local/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this . version of SciPy (detected version 1.24.2. warnings.warn(f""A NumPy version >={np_minversion} and <{np_maxversion}"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 45, in <module>. from deepvariant import modeling. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 56, in <module>. from tensorflow.python.tpu import tpu_config . File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/tpu_config.py"", line 18, in <module>. from tensorflow_estimator.python.estimator.tpu.tpu_config import *. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1 import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1.estimator import experimental. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 8, in <module>. from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder. File ""/mnt.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py"", line 26, in <module>. from tensorflow_estimator.python.estimator import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimat",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/673
https://github.com/google/deepvariant/issues/673:576,safety,detect,detected,576,"ImportError: cannot import name 'path_helpers' from 'tensorflow.python.saved_model'; Dear all,. I was trying to run deepvariant from singularity but I encountered this error:. time /opt/deepvariant/bin/call_variants --outfile ""deepvariant/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""deepvariant/interm. ediate_results_dir/make_examples.tfrecord@12.gz"" --checkpoint ""/opt/models/wes/model.ckpt"". /mnt/.local/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this . version of SciPy (detected version 1.24.2. warnings.warn(f""A NumPy version >={np_minversion} and <{np_maxversion}"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 45, in <module>. from deepvariant import modeling. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 56, in <module>. from tensorflow.python.tpu import tpu_config . File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/tpu_config.py"", line 18, in <module>. from tensorflow_estimator.python.estimator.tpu.tpu_config import *. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1 import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1.estimator import experimental. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 8, in <module>. from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder. File ""/mnt.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py"", line 26, in <module>. from tensorflow_estimator.python.estimator import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimat",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/673
https://github.com/google/deepvariant/issues/673:821,safety,modul,module,821,"ImportError: cannot import name 'path_helpers' from 'tensorflow.python.saved_model'; Dear all,. I was trying to run deepvariant from singularity but I encountered this error:. time /opt/deepvariant/bin/call_variants --outfile ""deepvariant/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""deepvariant/interm. ediate_results_dir/make_examples.tfrecord@12.gz"" --checkpoint ""/opt/models/wes/model.ckpt"". /mnt/.local/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this . version of SciPy (detected version 1.24.2. warnings.warn(f""A NumPy version >={np_minversion} and <{np_maxversion}"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 45, in <module>. from deepvariant import modeling. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 56, in <module>. from tensorflow.python.tpu import tpu_config . File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/tpu_config.py"", line 18, in <module>. from tensorflow_estimator.python.estimator.tpu.tpu_config import *. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1 import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1.estimator import experimental. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 8, in <module>. from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder. File ""/mnt.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py"", line 26, in <module>. from tensorflow_estimator.python.estimator import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimat",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/673
https://github.com/google/deepvariant/issues/673:970,safety,modul,module,970,"ImportError: cannot import name 'path_helpers' from 'tensorflow.python.saved_model'; Dear all,. I was trying to run deepvariant from singularity but I encountered this error:. time /opt/deepvariant/bin/call_variants --outfile ""deepvariant/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""deepvariant/interm. ediate_results_dir/make_examples.tfrecord@12.gz"" --checkpoint ""/opt/models/wes/model.ckpt"". /mnt/.local/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this . version of SciPy (detected version 1.24.2. warnings.warn(f""A NumPy version >={np_minversion} and <{np_maxversion}"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 45, in <module>. from deepvariant import modeling. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 56, in <module>. from tensorflow.python.tpu import tpu_config . File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/tpu_config.py"", line 18, in <module>. from tensorflow_estimator.python.estimator.tpu.tpu_config import *. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1 import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1.estimator import experimental. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 8, in <module>. from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder. File ""/mnt.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py"", line 26, in <module>. from tensorflow_estimator.python.estimator import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimat",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/673
https://github.com/google/deepvariant/issues/673:1122,safety,modul,module,1122,"nt from singularity but I encountered this error:. time /opt/deepvariant/bin/call_variants --outfile ""deepvariant/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""deepvariant/interm. ediate_results_dir/make_examples.tfrecord@12.gz"" --checkpoint ""/opt/models/wes/model.ckpt"". /mnt/.local/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this . version of SciPy (detected version 1.24.2. warnings.warn(f""A NumPy version >={np_minversion} and <{np_maxversion}"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 45, in <module>. from deepvariant import modeling. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 56, in <module>. from tensorflow.python.tpu import tpu_config . File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/tpu_config.py"", line 18, in <module>. from tensorflow_estimator.python.estimator.tpu.tpu_config import *. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1 import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1.estimator import experimental. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 8, in <module>. from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder. File ""/mnt.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py"", line 26, in <module>. from tensorflow_estimator.python.estimator import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 40, in <module>. from tensorflow.python.saved_model import path_helpers. ImportError:",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/673
https://github.com/google/deepvariant/issues/673:1292,safety,modul,module,1292,"z"" --examples ""deepvariant/interm. ediate_results_dir/make_examples.tfrecord@12.gz"" --checkpoint ""/opt/models/wes/model.ckpt"". /mnt/.local/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this . version of SciPy (detected version 1.24.2. warnings.warn(f""A NumPy version >={np_minversion} and <{np_maxversion}"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 45, in <module>. from deepvariant import modeling. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 56, in <module>. from tensorflow.python.tpu import tpu_config . File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/tpu_config.py"", line 18, in <module>. from tensorflow_estimator.python.estimator.tpu.tpu_config import *. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1 import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1.estimator import experimental. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 8, in <module>. from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder. File ""/mnt.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py"", line 26, in <module>. from tensorflow_estimator.python.estimator import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 40, in <module>. from tensorflow.python.saved_model import path_helpers. ImportError: cannot import name 'path_helpers' from 'tensorflow.python.saved_model' (/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_mode. l/__init__.py). Strangely, w",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/673
https://github.com/google/deepvariant/issues/673:1464,safety,modul,module,1464,"/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this . version of SciPy (detected version 1.24.2. warnings.warn(f""A NumPy version >={np_minversion} and <{np_maxversion}"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 45, in <module>. from deepvariant import modeling. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 56, in <module>. from tensorflow.python.tpu import tpu_config . File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/tpu_config.py"", line 18, in <module>. from tensorflow_estimator.python.estimator.tpu.tpu_config import *. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1 import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1.estimator import experimental. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 8, in <module>. from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder. File ""/mnt.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py"", line 26, in <module>. from tensorflow_estimator.python.estimator import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 40, in <module>. from tensorflow.python.saved_model import path_helpers. ImportError: cannot import name 'path_helpers' from 'tensorflow.python.saved_model' (/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_mode. l/__init__.py). Strangely, when I checked the Numpy installed in the conda environment it says 1.22.4. - Command: singularity exec deepvariant.simg /opt/deepvariant/bin/run_deepvariant --model_type WE",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/673
https://github.com/google/deepvariant/issues/673:1662,safety,modul,module,1662,"rsion}"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 45, in <module>. from deepvariant import modeling. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 56, in <module>. from tensorflow.python.tpu import tpu_config . File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/tpu_config.py"", line 18, in <module>. from tensorflow_estimator.python.estimator.tpu.tpu_config import *. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1 import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1.estimator import experimental. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 8, in <module>. from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder. File ""/mnt.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py"", line 26, in <module>. from tensorflow_estimator.python.estimator import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 40, in <module>. from tensorflow.python.saved_model import path_helpers. ImportError: cannot import name 'path_helpers' from 'tensorflow.python.saved_model' (/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_mode. l/__init__.py). Strangely, when I checked the Numpy installed in the conda environment it says 1.22.4. - Command: singularity exec deepvariant.simg /opt/deepvariant/bin/run_deepvariant --model_type WES --ref ref.fasta --reads aln_sncr_fc.bam --output_vcf deepvariant/deepvariant_calls.vcf --num_shards 12 --intermediate_results_dir deepvariant/intermediate_results_dir . I am running it on a slurm ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/673
https://github.com/google/deepvariant/issues/673:1866,safety,modul,module,1866,":. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 45, in <module>. from deepvariant import modeling. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 56, in <module>. from tensorflow.python.tpu import tpu_config . File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/tpu_config.py"", line 18, in <module>. from tensorflow_estimator.python.estimator.tpu.tpu_config import *. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1 import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1.estimator import experimental. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 8, in <module>. from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder. File ""/mnt.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py"", line 26, in <module>. from tensorflow_estimator.python.estimator import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 40, in <module>. from tensorflow.python.saved_model import path_helpers. ImportError: cannot import name 'path_helpers' from 'tensorflow.python.saved_model' (/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_mode. l/__init__.py). Strangely, when I checked the Numpy installed in the conda environment it says 1.22.4. - Command: singularity exec deepvariant.simg /opt/deepvariant/bin/run_deepvariant --model_type WES --ref ref.fasta --reads aln_sncr_fc.bam --output_vcf deepvariant/deepvariant_calls.vcf --num_shards 12 --intermediate_results_dir deepvariant/intermediate_results_dir . I am running it on a slurm cluster. . Thank you very much! Best,. CW.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/673
https://github.com/google/deepvariant/issues/673:2048,safety,modul,module,2048,":. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 45, in <module>. from deepvariant import modeling. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 56, in <module>. from tensorflow.python.tpu import tpu_config . File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/tpu_config.py"", line 18, in <module>. from tensorflow_estimator.python.estimator.tpu.tpu_config import *. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1 import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1.estimator import experimental. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 8, in <module>. from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder. File ""/mnt.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py"", line 26, in <module>. from tensorflow_estimator.python.estimator import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 40, in <module>. from tensorflow.python.saved_model import path_helpers. ImportError: cannot import name 'path_helpers' from 'tensorflow.python.saved_model' (/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_mode. l/__init__.py). Strangely, when I checked the Numpy installed in the conda environment it says 1.22.4. - Command: singularity exec deepvariant.simg /opt/deepvariant/bin/run_deepvariant --model_type WES --ref ref.fasta --reads aln_sncr_fc.bam --output_vcf deepvariant/deepvariant_calls.vcf --num_shards 12 --intermediate_results_dir deepvariant/intermediate_results_dir . I am running it on a slurm cluster. . Thank you very much! Best,. CW.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/673
https://github.com/google/deepvariant/issues/673:398,security,model,models,398,"ImportError: cannot import name 'path_helpers' from 'tensorflow.python.saved_model'; Dear all,. I was trying to run deepvariant from singularity but I encountered this error:. time /opt/deepvariant/bin/call_variants --outfile ""deepvariant/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""deepvariant/interm. ediate_results_dir/make_examples.tfrecord@12.gz"" --checkpoint ""/opt/models/wes/model.ckpt"". /mnt/.local/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this . version of SciPy (detected version 1.24.2. warnings.warn(f""A NumPy version >={np_minversion} and <{np_maxversion}"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 45, in <module>. from deepvariant import modeling. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 56, in <module>. from tensorflow.python.tpu import tpu_config . File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/tpu_config.py"", line 18, in <module>. from tensorflow_estimator.python.estimator.tpu.tpu_config import *. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1 import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1.estimator import experimental. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 8, in <module>. from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder. File ""/mnt.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py"", line 26, in <module>. from tensorflow_estimator.python.estimator import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimat",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/673
https://github.com/google/deepvariant/issues/673:409,security,model,model,409,"ImportError: cannot import name 'path_helpers' from 'tensorflow.python.saved_model'; Dear all,. I was trying to run deepvariant from singularity but I encountered this error:. time /opt/deepvariant/bin/call_variants --outfile ""deepvariant/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""deepvariant/interm. ediate_results_dir/make_examples.tfrecord@12.gz"" --checkpoint ""/opt/models/wes/model.ckpt"". /mnt/.local/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this . version of SciPy (detected version 1.24.2. warnings.warn(f""A NumPy version >={np_minversion} and <{np_maxversion}"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 45, in <module>. from deepvariant import modeling. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 56, in <module>. from tensorflow.python.tpu import tpu_config . File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/tpu_config.py"", line 18, in <module>. from tensorflow_estimator.python.estimator.tpu.tpu_config import *. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1 import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1.estimator import experimental. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 8, in <module>. from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder. File ""/mnt.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py"", line 26, in <module>. from tensorflow_estimator.python.estimator import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimat",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/673
https://github.com/google/deepvariant/issues/673:576,security,detect,detected,576,"ImportError: cannot import name 'path_helpers' from 'tensorflow.python.saved_model'; Dear all,. I was trying to run deepvariant from singularity but I encountered this error:. time /opt/deepvariant/bin/call_variants --outfile ""deepvariant/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""deepvariant/interm. ediate_results_dir/make_examples.tfrecord@12.gz"" --checkpoint ""/opt/models/wes/model.ckpt"". /mnt/.local/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this . version of SciPy (detected version 1.24.2. warnings.warn(f""A NumPy version >={np_minversion} and <{np_maxversion}"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 45, in <module>. from deepvariant import modeling. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 56, in <module>. from tensorflow.python.tpu import tpu_config . File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/tpu_config.py"", line 18, in <module>. from tensorflow_estimator.python.estimator.tpu.tpu_config import *. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1 import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1.estimator import experimental. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 8, in <module>. from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder. File ""/mnt.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py"", line 26, in <module>. from tensorflow_estimator.python.estimator import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimat",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/673
https://github.com/google/deepvariant/issues/673:854,security,model,modeling,854,"ImportError: cannot import name 'path_helpers' from 'tensorflow.python.saved_model'; Dear all,. I was trying to run deepvariant from singularity but I encountered this error:. time /opt/deepvariant/bin/call_variants --outfile ""deepvariant/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""deepvariant/interm. ediate_results_dir/make_examples.tfrecord@12.gz"" --checkpoint ""/opt/models/wes/model.ckpt"". /mnt/.local/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this . version of SciPy (detected version 1.24.2. warnings.warn(f""A NumPy version >={np_minversion} and <{np_maxversion}"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 45, in <module>. from deepvariant import modeling. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 56, in <module>. from tensorflow.python.tpu import tpu_config . File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/tpu_config.py"", line 18, in <module>. from tensorflow_estimator.python.estimator.tpu.tpu_config import *. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1 import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1.estimator import experimental. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 8, in <module>. from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder. File ""/mnt.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py"", line 26, in <module>. from tensorflow_estimator.python.estimator import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimat",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/673
https://github.com/google/deepvariant/issues/673:943,security,model,modeling,943,"ImportError: cannot import name 'path_helpers' from 'tensorflow.python.saved_model'; Dear all,. I was trying to run deepvariant from singularity but I encountered this error:. time /opt/deepvariant/bin/call_variants --outfile ""deepvariant/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""deepvariant/interm. ediate_results_dir/make_examples.tfrecord@12.gz"" --checkpoint ""/opt/models/wes/model.ckpt"". /mnt/.local/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this . version of SciPy (detected version 1.24.2. warnings.warn(f""A NumPy version >={np_minversion} and <{np_maxversion}"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 45, in <module>. from deepvariant import modeling. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 56, in <module>. from tensorflow.python.tpu import tpu_config . File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/tpu_config.py"", line 18, in <module>. from tensorflow_estimator.python.estimator.tpu.tpu_config import *. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1 import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1.estimator import experimental. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 8, in <module>. from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder. File ""/mnt.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py"", line 26, in <module>. from tensorflow_estimator.python.estimator import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimat",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/673
https://github.com/google/deepvariant/issues/673:674,testability,Trace,Traceback,674,"ImportError: cannot import name 'path_helpers' from 'tensorflow.python.saved_model'; Dear all,. I was trying to run deepvariant from singularity but I encountered this error:. time /opt/deepvariant/bin/call_variants --outfile ""deepvariant/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""deepvariant/interm. ediate_results_dir/make_examples.tfrecord@12.gz"" --checkpoint ""/opt/models/wes/model.ckpt"". /mnt/.local/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this . version of SciPy (detected version 1.24.2. warnings.warn(f""A NumPy version >={np_minversion} and <{np_maxversion}"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 45, in <module>. from deepvariant import modeling. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 56, in <module>. from tensorflow.python.tpu import tpu_config . File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/tpu_config.py"", line 18, in <module>. from tensorflow_estimator.python.estimator.tpu.tpu_config import *. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1 import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1.estimator import experimental. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 8, in <module>. from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder. File ""/mnt.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py"", line 26, in <module>. from tensorflow_estimator.python.estimator import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimat",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/673
https://github.com/google/deepvariant/issues/673:168,usability,error,error,168,"ImportError: cannot import name 'path_helpers' from 'tensorflow.python.saved_model'; Dear all,. I was trying to run deepvariant from singularity but I encountered this error:. time /opt/deepvariant/bin/call_variants --outfile ""deepvariant/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""deepvariant/interm. ediate_results_dir/make_examples.tfrecord@12.gz"" --checkpoint ""/opt/models/wes/model.ckpt"". /mnt/.local/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this . version of SciPy (detected version 1.24.2. warnings.warn(f""A NumPy version >={np_minversion} and <{np_maxversion}"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 45, in <module>. from deepvariant import modeling. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 56, in <module>. from tensorflow.python.tpu import tpu_config . File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/tpu_config.py"", line 18, in <module>. from tensorflow_estimator.python.estimator.tpu.tpu_config import *. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1 import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1.estimator import experimental. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 8, in <module>. from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder. File ""/mnt.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py"", line 26, in <module>. from tensorflow_estimator.python.estimator import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimat",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/673
https://github.com/google/deepvariant/issues/673:485,usability,User,UserWarning,485,"ImportError: cannot import name 'path_helpers' from 'tensorflow.python.saved_model'; Dear all,. I was trying to run deepvariant from singularity but I encountered this error:. time /opt/deepvariant/bin/call_variants --outfile ""deepvariant/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""deepvariant/interm. ediate_results_dir/make_examples.tfrecord@12.gz"" --checkpoint ""/opt/models/wes/model.ckpt"". /mnt/.local/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this . version of SciPy (detected version 1.24.2. warnings.warn(f""A NumPy version >={np_minversion} and <{np_maxversion}"". Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 45, in <module>. from deepvariant import modeling. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 56, in <module>. from tensorflow.python.tpu import tpu_config . File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/tpu_config.py"", line 18, in <module>. from tensorflow_estimator.python.estimator.tpu.tpu_config import *. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1 import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1.estimator import experimental. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 8, in <module>. from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder. File ""/mnt.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py"", line 26, in <module>. from tensorflow_estimator.python.estimator import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimat",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/673
https://github.com/google/deepvariant/issues/673:2372,usability,Command,Command,2372,":. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 45, in <module>. from deepvariant import modeling. File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 56, in <module>. from tensorflow.python.tpu import tpu_config . File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/tpu_config.py"", line 18, in <module>. from tensorflow_estimator.python.estimator.tpu.tpu_config import *. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1 import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 8, in <module>. from tensorflow_estimator._api.v1.estimator import experimental. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 8, in <module>. from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder. File ""/mnt.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py"", line 26, in <module>. from tensorflow_estimator.python.estimator import estimator. File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 40, in <module>. from tensorflow.python.saved_model import path_helpers. ImportError: cannot import name 'path_helpers' from 'tensorflow.python.saved_model' (/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_mode. l/__init__.py). Strangely, when I checked the Numpy installed in the conda environment it says 1.22.4. - Command: singularity exec deepvariant.simg /opt/deepvariant/bin/run_deepvariant --model_type WES --ref ref.fasta --reads aln_sncr_fc.bam --output_vcf deepvariant/deepvariant_calls.vcf --num_shards 12 --intermediate_results_dir deepvariant/intermediate_results_dir . I am running it on a slurm cluster. . Thank you very much! Best,. CW.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/673
https://github.com/google/deepvariant/issues/674:58,energy efficiency,CPU,CPU,58,"using the whole RAM and freezing; Hi,. I am running DV on CPU on a fragmented reference 3GB genome with pseudochromosomes + a very large number of small contigs. having these seems to eat up my RAM and I rapidly reach 250 of the 252 GB in use and my system freezes with oom. After adding a line to include only reference contigs larger than 500bps I got the command to work and it now uses only 100GB ram. ``--regions=/inref/${ref}_ge500.bed \``. I therefore edited this ticket (originally trying to allocate a max memory argument) and it this post can be closed and marked solved. best regards",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/674
https://github.com/google/deepvariant/issues/674:500,energy efficiency,alloc,allocate,500,"using the whole RAM and freezing; Hi,. I am running DV on CPU on a fragmented reference 3GB genome with pseudochromosomes + a very large number of small contigs. having these seems to eat up my RAM and I rapidly reach 250 of the 252 GB in use and my system freezes with oom. After adding a line to include only reference contigs larger than 500bps I got the command to work and it now uses only 100GB ram. ``--regions=/inref/${ref}_ge500.bed \``. I therefore edited this ticket (originally trying to allocate a max memory argument) and it this post can be closed and marked solved. best regards",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/674
https://github.com/google/deepvariant/issues/674:58,performance,CPU,CPU,58,"using the whole RAM and freezing; Hi,. I am running DV on CPU on a fragmented reference 3GB genome with pseudochromosomes + a very large number of small contigs. having these seems to eat up my RAM and I rapidly reach 250 of the 252 GB in use and my system freezes with oom. After adding a line to include only reference contigs larger than 500bps I got the command to work and it now uses only 100GB ram. ``--regions=/inref/${ref}_ge500.bed \``. I therefore edited this ticket (originally trying to allocate a max memory argument) and it this post can be closed and marked solved. best regards",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/674
https://github.com/google/deepvariant/issues/674:515,performance,memor,memory,515,"using the whole RAM and freezing; Hi,. I am running DV on CPU on a fragmented reference 3GB genome with pseudochromosomes + a very large number of small contigs. having these seems to eat up my RAM and I rapidly reach 250 of the 252 GB in use and my system freezes with oom. After adding a line to include only reference contigs larger than 500bps I got the command to work and it now uses only 100GB ram. ``--regions=/inref/${ref}_ge500.bed \``. I therefore edited this ticket (originally trying to allocate a max memory argument) and it this post can be closed and marked solved. best regards",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/674
https://github.com/google/deepvariant/issues/674:358,usability,command,command,358,"using the whole RAM and freezing; Hi,. I am running DV on CPU on a fragmented reference 3GB genome with pseudochromosomes + a very large number of small contigs. having these seems to eat up my RAM and I rapidly reach 250 of the 252 GB in use and my system freezes with oom. After adding a line to include only reference contigs larger than 500bps I got the command to work and it now uses only 100GB ram. ``--regions=/inref/${ref}_ge500.bed \``. I therefore edited this ticket (originally trying to allocate a max memory argument) and it this post can be closed and marked solved. best regards",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/674
https://github.com/google/deepvariant/issues/674:515,usability,memor,memory,515,"using the whole RAM and freezing; Hi,. I am running DV on CPU on a fragmented reference 3GB genome with pseudochromosomes + a very large number of small contigs. having these seems to eat up my RAM and I rapidly reach 250 of the 252 GB in use and my system freezes with oom. After adding a line to include only reference contigs larger than 500bps I got the command to work and it now uses only 100GB ram. ``--regions=/inref/${ref}_ge500.bed \``. I therefore edited this ticket (originally trying to allocate a max memory argument) and it this post can be closed and marked solved. best regards",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/674
https://github.com/google/deepvariant/issues/674:556,usability,close,closed,556,"using the whole RAM and freezing; Hi,. I am running DV on CPU on a fragmented reference 3GB genome with pseudochromosomes + a very large number of small contigs. having these seems to eat up my RAM and I rapidly reach 250 of the 252 GB in use and my system freezes with oom. After adding a line to include only reference contigs larger than 500bps I got the command to work and it now uses only 100GB ram. ``--regions=/inref/${ref}_ge500.bed \``. I therefore edited this ticket (originally trying to allocate a max memory argument) and it this post can be closed and marked solved. best regards",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/674
https://github.com/google/deepvariant/issues/675:119,availability,Operat,Operating,119,"Unable to get vcf output file; Describe the issue:. I can't get vcf output after bind mount a root directory. Setup. - Operating system: Windows 11, but mount an Ubuntu VM through multipass. - Type of data: fasta, bam and vcf file. Steps to reproduce:. - Command:. #Configure the DeepVariant environment variables (missing input directory,...). BIN_VERSION=""1.5.0"". sudo apt -y update. sudo apt-get -y install docker.io. sudo docker pull google/deepvariant:""${BIN_VERSION}"". # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". PWD=/mountpoint/fastQ. INPUT_DIR=""${PWD}/testdata_input"". mkdir -p ${INPUT_DIR}. OUTPUT_DIR=""${PWD}/04.deepvariant_out"". mkdir -p ""${OUTPUT_DIR}"". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/input/Homo_sapiens_assembly38.fasta \. --reads=/input/$FQ.align.sort.marked.bam \. --output_vcf=/output/$FQ.vcf.gz \. --output_gvcf=/output/$FQ.g.vcf.gz \. --num_shards=2 . - Error trace: . . It displays: Task reading input the .bam file but it ends up with 0 candidates. I suppose it can read the input files. Does the quick start test work on your system? This the tutorial I've used: https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/675
https://github.com/google/deepvariant/issues/675:1033,availability,Error,Error,1033,"Unable to get vcf output file; Describe the issue:. I can't get vcf output after bind mount a root directory. Setup. - Operating system: Windows 11, but mount an Ubuntu VM through multipass. - Type of data: fasta, bam and vcf file. Steps to reproduce:. - Command:. #Configure the DeepVariant environment variables (missing input directory,...). BIN_VERSION=""1.5.0"". sudo apt -y update. sudo apt-get -y install docker.io. sudo docker pull google/deepvariant:""${BIN_VERSION}"". # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". PWD=/mountpoint/fastQ. INPUT_DIR=""${PWD}/testdata_input"". mkdir -p ${INPUT_DIR}. OUTPUT_DIR=""${PWD}/04.deepvariant_out"". mkdir -p ""${OUTPUT_DIR}"". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/input/Homo_sapiens_assembly38.fasta \. --reads=/input/$FQ.align.sort.marked.bam \. --output_vcf=/output/$FQ.vcf.gz \. --output_gvcf=/output/$FQ.g.vcf.gz \. --num_shards=2 . - Error trace: . . It displays: Task reading input the .bam file but it ends up with 0 candidates. I suppose it can read the input files. Does the quick start test work on your system? This the tutorial I've used: https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/675
https://github.com/google/deepvariant/issues/675:378,deployability,updat,update,378,"Unable to get vcf output file; Describe the issue:. I can't get vcf output after bind mount a root directory. Setup. - Operating system: Windows 11, but mount an Ubuntu VM through multipass. - Type of data: fasta, bam and vcf file. Steps to reproduce:. - Command:. #Configure the DeepVariant environment variables (missing input directory,...). BIN_VERSION=""1.5.0"". sudo apt -y update. sudo apt-get -y install docker.io. sudo docker pull google/deepvariant:""${BIN_VERSION}"". # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". PWD=/mountpoint/fastQ. INPUT_DIR=""${PWD}/testdata_input"". mkdir -p ${INPUT_DIR}. OUTPUT_DIR=""${PWD}/04.deepvariant_out"". mkdir -p ""${OUTPUT_DIR}"". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/input/Homo_sapiens_assembly38.fasta \. --reads=/input/$FQ.align.sort.marked.bam \. --output_vcf=/output/$FQ.vcf.gz \. --output_gvcf=/output/$FQ.g.vcf.gz \. --num_shards=2 . - Error trace: . . It displays: Task reading input the .bam file but it ends up with 0 candidates. I suppose it can read the input files. Does the quick start test work on your system? This the tutorial I've used: https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/675
https://github.com/google/deepvariant/issues/675:402,deployability,instal,install,402,"Unable to get vcf output file; Describe the issue:. I can't get vcf output after bind mount a root directory. Setup. - Operating system: Windows 11, but mount an Ubuntu VM through multipass. - Type of data: fasta, bam and vcf file. Steps to reproduce:. - Command:. #Configure the DeepVariant environment variables (missing input directory,...). BIN_VERSION=""1.5.0"". sudo apt -y update. sudo apt-get -y install docker.io. sudo docker pull google/deepvariant:""${BIN_VERSION}"". # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". PWD=/mountpoint/fastQ. INPUT_DIR=""${PWD}/testdata_input"". mkdir -p ${INPUT_DIR}. OUTPUT_DIR=""${PWD}/04.deepvariant_out"". mkdir -p ""${OUTPUT_DIR}"". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/input/Homo_sapiens_assembly38.fasta \. --reads=/input/$FQ.align.sort.marked.bam \. --output_vcf=/output/$FQ.vcf.gz \. --output_gvcf=/output/$FQ.g.vcf.gz \. --num_shards=2 . - Error trace: . . It displays: Task reading input the .bam file but it ends up with 0 candidates. I suppose it can read the input files. Does the quick start test work on your system? This the tutorial I've used: https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/675
https://github.com/google/deepvariant/issues/675:266,integrability,Configur,Configure,266,"Unable to get vcf output file; Describe the issue:. I can't get vcf output after bind mount a root directory. Setup. - Operating system: Windows 11, but mount an Ubuntu VM through multipass. - Type of data: fasta, bam and vcf file. Steps to reproduce:. - Command:. #Configure the DeepVariant environment variables (missing input directory,...). BIN_VERSION=""1.5.0"". sudo apt -y update. sudo apt-get -y install docker.io. sudo docker pull google/deepvariant:""${BIN_VERSION}"". # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". PWD=/mountpoint/fastQ. INPUT_DIR=""${PWD}/testdata_input"". mkdir -p ${INPUT_DIR}. OUTPUT_DIR=""${PWD}/04.deepvariant_out"". mkdir -p ""${OUTPUT_DIR}"". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/input/Homo_sapiens_assembly38.fasta \. --reads=/input/$FQ.align.sort.marked.bam \. --output_vcf=/output/$FQ.vcf.gz \. --output_gvcf=/output/$FQ.g.vcf.gz \. --num_shards=2 . - Error trace: . . It displays: Task reading input the .bam file but it ends up with 0 candidates. I suppose it can read the input files. Does the quick start test work on your system? This the tutorial I've used: https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/675
https://github.com/google/deepvariant/issues/675:81,interoperability,bind,bind,81,"Unable to get vcf output file; Describe the issue:. I can't get vcf output after bind mount a root directory. Setup. - Operating system: Windows 11, but mount an Ubuntu VM through multipass. - Type of data: fasta, bam and vcf file. Steps to reproduce:. - Command:. #Configure the DeepVariant environment variables (missing input directory,...). BIN_VERSION=""1.5.0"". sudo apt -y update. sudo apt-get -y install docker.io. sudo docker pull google/deepvariant:""${BIN_VERSION}"". # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". PWD=/mountpoint/fastQ. INPUT_DIR=""${PWD}/testdata_input"". mkdir -p ${INPUT_DIR}. OUTPUT_DIR=""${PWD}/04.deepvariant_out"". mkdir -p ""${OUTPUT_DIR}"". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/input/Homo_sapiens_assembly38.fasta \. --reads=/input/$FQ.align.sort.marked.bam \. --output_vcf=/output/$FQ.vcf.gz \. --output_gvcf=/output/$FQ.g.vcf.gz \. --num_shards=2 . - Error trace: . . It displays: Task reading input the .bam file but it ends up with 0 candidates. I suppose it can read the input files. Does the quick start test work on your system? This the tutorial I've used: https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/675
https://github.com/google/deepvariant/issues/675:81,modifiability,bind,bind,81,"Unable to get vcf output file; Describe the issue:. I can't get vcf output after bind mount a root directory. Setup. - Operating system: Windows 11, but mount an Ubuntu VM through multipass. - Type of data: fasta, bam and vcf file. Steps to reproduce:. - Command:. #Configure the DeepVariant environment variables (missing input directory,...). BIN_VERSION=""1.5.0"". sudo apt -y update. sudo apt-get -y install docker.io. sudo docker pull google/deepvariant:""${BIN_VERSION}"". # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". PWD=/mountpoint/fastQ. INPUT_DIR=""${PWD}/testdata_input"". mkdir -p ${INPUT_DIR}. OUTPUT_DIR=""${PWD}/04.deepvariant_out"". mkdir -p ""${OUTPUT_DIR}"". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/input/Homo_sapiens_assembly38.fasta \. --reads=/input/$FQ.align.sort.marked.bam \. --output_vcf=/output/$FQ.vcf.gz \. --output_gvcf=/output/$FQ.g.vcf.gz \. --num_shards=2 . - Error trace: . . It displays: Task reading input the .bam file but it ends up with 0 candidates. I suppose it can read the input files. Does the quick start test work on your system? This the tutorial I've used: https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/675
https://github.com/google/deepvariant/issues/675:266,modifiability,Configur,Configure,266,"Unable to get vcf output file; Describe the issue:. I can't get vcf output after bind mount a root directory. Setup. - Operating system: Windows 11, but mount an Ubuntu VM through multipass. - Type of data: fasta, bam and vcf file. Steps to reproduce:. - Command:. #Configure the DeepVariant environment variables (missing input directory,...). BIN_VERSION=""1.5.0"". sudo apt -y update. sudo apt-get -y install docker.io. sudo docker pull google/deepvariant:""${BIN_VERSION}"". # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". PWD=/mountpoint/fastQ. INPUT_DIR=""${PWD}/testdata_input"". mkdir -p ${INPUT_DIR}. OUTPUT_DIR=""${PWD}/04.deepvariant_out"". mkdir -p ""${OUTPUT_DIR}"". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/input/Homo_sapiens_assembly38.fasta \. --reads=/input/$FQ.align.sort.marked.bam \. --output_vcf=/output/$FQ.vcf.gz \. --output_gvcf=/output/$FQ.g.vcf.gz \. --num_shards=2 . - Error trace: . . It displays: Task reading input the .bam file but it ends up with 0 candidates. I suppose it can read the input files. Does the quick start test work on your system? This the tutorial I've used: https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/675
https://github.com/google/deepvariant/issues/675:304,modifiability,variab,variables,304,"Unable to get vcf output file; Describe the issue:. I can't get vcf output after bind mount a root directory. Setup. - Operating system: Windows 11, but mount an Ubuntu VM through multipass. - Type of data: fasta, bam and vcf file. Steps to reproduce:. - Command:. #Configure the DeepVariant environment variables (missing input directory,...). BIN_VERSION=""1.5.0"". sudo apt -y update. sudo apt-get -y install docker.io. sudo docker pull google/deepvariant:""${BIN_VERSION}"". # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". PWD=/mountpoint/fastQ. INPUT_DIR=""${PWD}/testdata_input"". mkdir -p ${INPUT_DIR}. OUTPUT_DIR=""${PWD}/04.deepvariant_out"". mkdir -p ""${OUTPUT_DIR}"". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/input/Homo_sapiens_assembly38.fasta \. --reads=/input/$FQ.align.sort.marked.bam \. --output_vcf=/output/$FQ.vcf.gz \. --output_gvcf=/output/$FQ.g.vcf.gz \. --num_shards=2 . - Error trace: . . It displays: Task reading input the .bam file but it ends up with 0 candidates. I suppose it can read the input files. Does the quick start test work on your system? This the tutorial I've used: https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/675
https://github.com/google/deepvariant/issues/675:1033,performance,Error,Error,1033,"Unable to get vcf output file; Describe the issue:. I can't get vcf output after bind mount a root directory. Setup. - Operating system: Windows 11, but mount an Ubuntu VM through multipass. - Type of data: fasta, bam and vcf file. Steps to reproduce:. - Command:. #Configure the DeepVariant environment variables (missing input directory,...). BIN_VERSION=""1.5.0"". sudo apt -y update. sudo apt-get -y install docker.io. sudo docker pull google/deepvariant:""${BIN_VERSION}"". # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". PWD=/mountpoint/fastQ. INPUT_DIR=""${PWD}/testdata_input"". mkdir -p ${INPUT_DIR}. OUTPUT_DIR=""${PWD}/04.deepvariant_out"". mkdir -p ""${OUTPUT_DIR}"". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/input/Homo_sapiens_assembly38.fasta \. --reads=/input/$FQ.align.sort.marked.bam \. --output_vcf=/output/$FQ.vcf.gz \. --output_gvcf=/output/$FQ.g.vcf.gz \. --num_shards=2 . - Error trace: . . It displays: Task reading input the .bam file but it ends up with 0 candidates. I suppose it can read the input files. Does the quick start test work on your system? This the tutorial I've used: https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/675
https://github.com/google/deepvariant/issues/675:1169,reliability,Doe,Does,1169,"Unable to get vcf output file; Describe the issue:. I can't get vcf output after bind mount a root directory. Setup. - Operating system: Windows 11, but mount an Ubuntu VM through multipass. - Type of data: fasta, bam and vcf file. Steps to reproduce:. - Command:. #Configure the DeepVariant environment variables (missing input directory,...). BIN_VERSION=""1.5.0"". sudo apt -y update. sudo apt-get -y install docker.io. sudo docker pull google/deepvariant:""${BIN_VERSION}"". # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". PWD=/mountpoint/fastQ. INPUT_DIR=""${PWD}/testdata_input"". mkdir -p ${INPUT_DIR}. OUTPUT_DIR=""${PWD}/04.deepvariant_out"". mkdir -p ""${OUTPUT_DIR}"". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/input/Homo_sapiens_assembly38.fasta \. --reads=/input/$FQ.align.sort.marked.bam \. --output_vcf=/output/$FQ.vcf.gz \. --output_gvcf=/output/$FQ.g.vcf.gz \. --num_shards=2 . - Error trace: . . It displays: Task reading input the .bam file but it ends up with 0 candidates. I suppose it can read the input files. Does the quick start test work on your system? This the tutorial I've used: https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/675
https://github.com/google/deepvariant/issues/675:323,safety,input,input,323,"Unable to get vcf output file; Describe the issue:. I can't get vcf output after bind mount a root directory. Setup. - Operating system: Windows 11, but mount an Ubuntu VM through multipass. - Type of data: fasta, bam and vcf file. Steps to reproduce:. - Command:. #Configure the DeepVariant environment variables (missing input directory,...). BIN_VERSION=""1.5.0"". sudo apt -y update. sudo apt-get -y install docker.io. sudo docker pull google/deepvariant:""${BIN_VERSION}"". # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". PWD=/mountpoint/fastQ. INPUT_DIR=""${PWD}/testdata_input"". mkdir -p ${INPUT_DIR}. OUTPUT_DIR=""${PWD}/04.deepvariant_out"". mkdir -p ""${OUTPUT_DIR}"". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/input/Homo_sapiens_assembly38.fasta \. --reads=/input/$FQ.align.sort.marked.bam \. --output_vcf=/output/$FQ.vcf.gz \. --output_gvcf=/output/$FQ.g.vcf.gz \. --num_shards=2 . - Error trace: . . It displays: Task reading input the .bam file but it ends up with 0 candidates. I suppose it can read the input files. Does the quick start test work on your system? This the tutorial I've used: https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/675
https://github.com/google/deepvariant/issues/675:378,safety,updat,update,378,"Unable to get vcf output file; Describe the issue:. I can't get vcf output after bind mount a root directory. Setup. - Operating system: Windows 11, but mount an Ubuntu VM through multipass. - Type of data: fasta, bam and vcf file. Steps to reproduce:. - Command:. #Configure the DeepVariant environment variables (missing input directory,...). BIN_VERSION=""1.5.0"". sudo apt -y update. sudo apt-get -y install docker.io. sudo docker pull google/deepvariant:""${BIN_VERSION}"". # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". PWD=/mountpoint/fastQ. INPUT_DIR=""${PWD}/testdata_input"". mkdir -p ${INPUT_DIR}. OUTPUT_DIR=""${PWD}/04.deepvariant_out"". mkdir -p ""${OUTPUT_DIR}"". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/input/Homo_sapiens_assembly38.fasta \. --reads=/input/$FQ.align.sort.marked.bam \. --output_vcf=/output/$FQ.vcf.gz \. --output_gvcf=/output/$FQ.g.vcf.gz \. --num_shards=2 . - Error trace: . . It displays: Task reading input the .bam file but it ends up with 0 candidates. I suppose it can read the input files. Does the quick start test work on your system? This the tutorial I've used: https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/675
https://github.com/google/deepvariant/issues/675:742,safety,input,input,742,"Unable to get vcf output file; Describe the issue:. I can't get vcf output after bind mount a root directory. Setup. - Operating system: Windows 11, but mount an Ubuntu VM through multipass. - Type of data: fasta, bam and vcf file. Steps to reproduce:. - Command:. #Configure the DeepVariant environment variables (missing input directory,...). BIN_VERSION=""1.5.0"". sudo apt -y update. sudo apt-get -y install docker.io. sudo docker pull google/deepvariant:""${BIN_VERSION}"". # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". PWD=/mountpoint/fastQ. INPUT_DIR=""${PWD}/testdata_input"". mkdir -p ${INPUT_DIR}. OUTPUT_DIR=""${PWD}/04.deepvariant_out"". mkdir -p ""${OUTPUT_DIR}"". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/input/Homo_sapiens_assembly38.fasta \. --reads=/input/$FQ.align.sort.marked.bam \. --output_vcf=/output/$FQ.vcf.gz \. --output_gvcf=/output/$FQ.g.vcf.gz \. --num_shards=2 . - Error trace: . . It displays: Task reading input the .bam file but it ends up with 0 candidates. I suppose it can read the input files. Does the quick start test work on your system? This the tutorial I've used: https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/675
https://github.com/google/deepvariant/issues/675:858,safety,input,input,858,"Unable to get vcf output file; Describe the issue:. I can't get vcf output after bind mount a root directory. Setup. - Operating system: Windows 11, but mount an Ubuntu VM through multipass. - Type of data: fasta, bam and vcf file. Steps to reproduce:. - Command:. #Configure the DeepVariant environment variables (missing input directory,...). BIN_VERSION=""1.5.0"". sudo apt -y update. sudo apt-get -y install docker.io. sudo docker pull google/deepvariant:""${BIN_VERSION}"". # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". PWD=/mountpoint/fastQ. INPUT_DIR=""${PWD}/testdata_input"". mkdir -p ${INPUT_DIR}. OUTPUT_DIR=""${PWD}/04.deepvariant_out"". mkdir -p ""${OUTPUT_DIR}"". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/input/Homo_sapiens_assembly38.fasta \. --reads=/input/$FQ.align.sort.marked.bam \. --output_vcf=/output/$FQ.vcf.gz \. --output_gvcf=/output/$FQ.g.vcf.gz \. --num_shards=2 . - Error trace: . . It displays: Task reading input the .bam file but it ends up with 0 candidates. I suppose it can read the input files. Does the quick start test work on your system? This the tutorial I've used: https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/675
https://github.com/google/deepvariant/issues/675:906,safety,input,input,906,"Unable to get vcf output file; Describe the issue:. I can't get vcf output after bind mount a root directory. Setup. - Operating system: Windows 11, but mount an Ubuntu VM through multipass. - Type of data: fasta, bam and vcf file. Steps to reproduce:. - Command:. #Configure the DeepVariant environment variables (missing input directory,...). BIN_VERSION=""1.5.0"". sudo apt -y update. sudo apt-get -y install docker.io. sudo docker pull google/deepvariant:""${BIN_VERSION}"". # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". PWD=/mountpoint/fastQ. INPUT_DIR=""${PWD}/testdata_input"". mkdir -p ${INPUT_DIR}. OUTPUT_DIR=""${PWD}/04.deepvariant_out"". mkdir -p ""${OUTPUT_DIR}"". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/input/Homo_sapiens_assembly38.fasta \. --reads=/input/$FQ.align.sort.marked.bam \. --output_vcf=/output/$FQ.vcf.gz \. --output_gvcf=/output/$FQ.g.vcf.gz \. --num_shards=2 . - Error trace: . . It displays: Task reading input the .bam file but it ends up with 0 candidates. I suppose it can read the input files. Does the quick start test work on your system? This the tutorial I've used: https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/675
https://github.com/google/deepvariant/issues/675:1033,safety,Error,Error,1033,"Unable to get vcf output file; Describe the issue:. I can't get vcf output after bind mount a root directory. Setup. - Operating system: Windows 11, but mount an Ubuntu VM through multipass. - Type of data: fasta, bam and vcf file. Steps to reproduce:. - Command:. #Configure the DeepVariant environment variables (missing input directory,...). BIN_VERSION=""1.5.0"". sudo apt -y update. sudo apt-get -y install docker.io. sudo docker pull google/deepvariant:""${BIN_VERSION}"". # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". PWD=/mountpoint/fastQ. INPUT_DIR=""${PWD}/testdata_input"". mkdir -p ${INPUT_DIR}. OUTPUT_DIR=""${PWD}/04.deepvariant_out"". mkdir -p ""${OUTPUT_DIR}"". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/input/Homo_sapiens_assembly38.fasta \. --reads=/input/$FQ.align.sort.marked.bam \. --output_vcf=/output/$FQ.vcf.gz \. --output_gvcf=/output/$FQ.g.vcf.gz \. --num_shards=2 . - Error trace: . . It displays: Task reading input the .bam file but it ends up with 0 candidates. I suppose it can read the input files. Does the quick start test work on your system? This the tutorial I've used: https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/675
https://github.com/google/deepvariant/issues/675:1076,safety,input,input,1076,"Unable to get vcf output file; Describe the issue:. I can't get vcf output after bind mount a root directory. Setup. - Operating system: Windows 11, but mount an Ubuntu VM through multipass. - Type of data: fasta, bam and vcf file. Steps to reproduce:. - Command:. #Configure the DeepVariant environment variables (missing input directory,...). BIN_VERSION=""1.5.0"". sudo apt -y update. sudo apt-get -y install docker.io. sudo docker pull google/deepvariant:""${BIN_VERSION}"". # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". PWD=/mountpoint/fastQ. INPUT_DIR=""${PWD}/testdata_input"". mkdir -p ${INPUT_DIR}. OUTPUT_DIR=""${PWD}/04.deepvariant_out"". mkdir -p ""${OUTPUT_DIR}"". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/input/Homo_sapiens_assembly38.fasta \. --reads=/input/$FQ.align.sort.marked.bam \. --output_vcf=/output/$FQ.vcf.gz \. --output_gvcf=/output/$FQ.g.vcf.gz \. --num_shards=2 . - Error trace: . . It displays: Task reading input the .bam file but it ends up with 0 candidates. I suppose it can read the input files. Does the quick start test work on your system? This the tutorial I've used: https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/675
https://github.com/google/deepvariant/issues/675:1156,safety,input,input,1156,"Unable to get vcf output file; Describe the issue:. I can't get vcf output after bind mount a root directory. Setup. - Operating system: Windows 11, but mount an Ubuntu VM through multipass. - Type of data: fasta, bam and vcf file. Steps to reproduce:. - Command:. #Configure the DeepVariant environment variables (missing input directory,...). BIN_VERSION=""1.5.0"". sudo apt -y update. sudo apt-get -y install docker.io. sudo docker pull google/deepvariant:""${BIN_VERSION}"". # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". PWD=/mountpoint/fastQ. INPUT_DIR=""${PWD}/testdata_input"". mkdir -p ${INPUT_DIR}. OUTPUT_DIR=""${PWD}/04.deepvariant_out"". mkdir -p ""${OUTPUT_DIR}"". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/input/Homo_sapiens_assembly38.fasta \. --reads=/input/$FQ.align.sort.marked.bam \. --output_vcf=/output/$FQ.vcf.gz \. --output_gvcf=/output/$FQ.g.vcf.gz \. --num_shards=2 . - Error trace: . . It displays: Task reading input the .bam file but it ends up with 0 candidates. I suppose it can read the input files. Does the quick start test work on your system? This the tutorial I've used: https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/675
https://github.com/google/deepvariant/issues/675:1190,safety,test,test,1190,"Unable to get vcf output file; Describe the issue:. I can't get vcf output after bind mount a root directory. Setup. - Operating system: Windows 11, but mount an Ubuntu VM through multipass. - Type of data: fasta, bam and vcf file. Steps to reproduce:. - Command:. #Configure the DeepVariant environment variables (missing input directory,...). BIN_VERSION=""1.5.0"". sudo apt -y update. sudo apt-get -y install docker.io. sudo docker pull google/deepvariant:""${BIN_VERSION}"". # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". PWD=/mountpoint/fastQ. INPUT_DIR=""${PWD}/testdata_input"". mkdir -p ${INPUT_DIR}. OUTPUT_DIR=""${PWD}/04.deepvariant_out"". mkdir -p ""${OUTPUT_DIR}"". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/input/Homo_sapiens_assembly38.fasta \. --reads=/input/$FQ.align.sort.marked.bam \. --output_vcf=/output/$FQ.vcf.gz \. --output_gvcf=/output/$FQ.g.vcf.gz \. --num_shards=2 . - Error trace: . . It displays: Task reading input the .bam file but it ends up with 0 candidates. I suppose it can read the input files. Does the quick start test work on your system? This the tutorial I've used: https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/675
https://github.com/google/deepvariant/issues/675:266,security,Configur,Configure,266,"Unable to get vcf output file; Describe the issue:. I can't get vcf output after bind mount a root directory. Setup. - Operating system: Windows 11, but mount an Ubuntu VM through multipass. - Type of data: fasta, bam and vcf file. Steps to reproduce:. - Command:. #Configure the DeepVariant environment variables (missing input directory,...). BIN_VERSION=""1.5.0"". sudo apt -y update. sudo apt-get -y install docker.io. sudo docker pull google/deepvariant:""${BIN_VERSION}"". # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". PWD=/mountpoint/fastQ. INPUT_DIR=""${PWD}/testdata_input"". mkdir -p ${INPUT_DIR}. OUTPUT_DIR=""${PWD}/04.deepvariant_out"". mkdir -p ""${OUTPUT_DIR}"". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/input/Homo_sapiens_assembly38.fasta \. --reads=/input/$FQ.align.sort.marked.bam \. --output_vcf=/output/$FQ.vcf.gz \. --output_gvcf=/output/$FQ.g.vcf.gz \. --num_shards=2 . - Error trace: . . It displays: Task reading input the .bam file but it ends up with 0 candidates. I suppose it can read the input files. Does the quick start test work on your system? This the tutorial I've used: https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/675
https://github.com/google/deepvariant/issues/675:371,security,apt,apt,371,"Unable to get vcf output file; Describe the issue:. I can't get vcf output after bind mount a root directory. Setup. - Operating system: Windows 11, but mount an Ubuntu VM through multipass. - Type of data: fasta, bam and vcf file. Steps to reproduce:. - Command:. #Configure the DeepVariant environment variables (missing input directory,...). BIN_VERSION=""1.5.0"". sudo apt -y update. sudo apt-get -y install docker.io. sudo docker pull google/deepvariant:""${BIN_VERSION}"". # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". PWD=/mountpoint/fastQ. INPUT_DIR=""${PWD}/testdata_input"". mkdir -p ${INPUT_DIR}. OUTPUT_DIR=""${PWD}/04.deepvariant_out"". mkdir -p ""${OUTPUT_DIR}"". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/input/Homo_sapiens_assembly38.fasta \. --reads=/input/$FQ.align.sort.marked.bam \. --output_vcf=/output/$FQ.vcf.gz \. --output_gvcf=/output/$FQ.g.vcf.gz \. --num_shards=2 . - Error trace: . . It displays: Task reading input the .bam file but it ends up with 0 candidates. I suppose it can read the input files. Does the quick start test work on your system? This the tutorial I've used: https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/675
https://github.com/google/deepvariant/issues/675:378,security,updat,update,378,"Unable to get vcf output file; Describe the issue:. I can't get vcf output after bind mount a root directory. Setup. - Operating system: Windows 11, but mount an Ubuntu VM through multipass. - Type of data: fasta, bam and vcf file. Steps to reproduce:. - Command:. #Configure the DeepVariant environment variables (missing input directory,...). BIN_VERSION=""1.5.0"". sudo apt -y update. sudo apt-get -y install docker.io. sudo docker pull google/deepvariant:""${BIN_VERSION}"". # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". PWD=/mountpoint/fastQ. INPUT_DIR=""${PWD}/testdata_input"". mkdir -p ${INPUT_DIR}. OUTPUT_DIR=""${PWD}/04.deepvariant_out"". mkdir -p ""${OUTPUT_DIR}"". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/input/Homo_sapiens_assembly38.fasta \. --reads=/input/$FQ.align.sort.marked.bam \. --output_vcf=/output/$FQ.vcf.gz \. --output_gvcf=/output/$FQ.g.vcf.gz \. --num_shards=2 . - Error trace: . . It displays: Task reading input the .bam file but it ends up with 0 candidates. I suppose it can read the input files. Does the quick start test work on your system? This the tutorial I've used: https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/675
https://github.com/google/deepvariant/issues/675:391,security,apt,apt-get,391,"Unable to get vcf output file; Describe the issue:. I can't get vcf output after bind mount a root directory. Setup. - Operating system: Windows 11, but mount an Ubuntu VM through multipass. - Type of data: fasta, bam and vcf file. Steps to reproduce:. - Command:. #Configure the DeepVariant environment variables (missing input directory,...). BIN_VERSION=""1.5.0"". sudo apt -y update. sudo apt-get -y install docker.io. sudo docker pull google/deepvariant:""${BIN_VERSION}"". # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". PWD=/mountpoint/fastQ. INPUT_DIR=""${PWD}/testdata_input"". mkdir -p ${INPUT_DIR}. OUTPUT_DIR=""${PWD}/04.deepvariant_out"". mkdir -p ""${OUTPUT_DIR}"". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/input/Homo_sapiens_assembly38.fasta \. --reads=/input/$FQ.align.sort.marked.bam \. --output_vcf=/output/$FQ.vcf.gz \. --output_gvcf=/output/$FQ.g.vcf.gz \. --num_shards=2 . - Error trace: . . It displays: Task reading input the .bam file but it ends up with 0 candidates. I suppose it can read the input files. Does the quick start test work on your system? This the tutorial I've used: https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/675
https://github.com/google/deepvariant/issues/675:1039,testability,trace,trace,1039,"Unable to get vcf output file; Describe the issue:. I can't get vcf output after bind mount a root directory. Setup. - Operating system: Windows 11, but mount an Ubuntu VM through multipass. - Type of data: fasta, bam and vcf file. Steps to reproduce:. - Command:. #Configure the DeepVariant environment variables (missing input directory,...). BIN_VERSION=""1.5.0"". sudo apt -y update. sudo apt-get -y install docker.io. sudo docker pull google/deepvariant:""${BIN_VERSION}"". # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". PWD=/mountpoint/fastQ. INPUT_DIR=""${PWD}/testdata_input"". mkdir -p ${INPUT_DIR}. OUTPUT_DIR=""${PWD}/04.deepvariant_out"". mkdir -p ""${OUTPUT_DIR}"". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/input/Homo_sapiens_assembly38.fasta \. --reads=/input/$FQ.align.sort.marked.bam \. --output_vcf=/output/$FQ.vcf.gz \. --output_gvcf=/output/$FQ.g.vcf.gz \. --num_shards=2 . - Error trace: . . It displays: Task reading input the .bam file but it ends up with 0 candidates. I suppose it can read the input files. Does the quick start test work on your system? This the tutorial I've used: https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/675
https://github.com/google/deepvariant/issues/675:1190,testability,test,test,1190,"Unable to get vcf output file; Describe the issue:. I can't get vcf output after bind mount a root directory. Setup. - Operating system: Windows 11, but mount an Ubuntu VM through multipass. - Type of data: fasta, bam and vcf file. Steps to reproduce:. - Command:. #Configure the DeepVariant environment variables (missing input directory,...). BIN_VERSION=""1.5.0"". sudo apt -y update. sudo apt-get -y install docker.io. sudo docker pull google/deepvariant:""${BIN_VERSION}"". # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". PWD=/mountpoint/fastQ. INPUT_DIR=""${PWD}/testdata_input"". mkdir -p ${INPUT_DIR}. OUTPUT_DIR=""${PWD}/04.deepvariant_out"". mkdir -p ""${OUTPUT_DIR}"". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/input/Homo_sapiens_assembly38.fasta \. --reads=/input/$FQ.align.sort.marked.bam \. --output_vcf=/output/$FQ.vcf.gz \. --output_gvcf=/output/$FQ.g.vcf.gz \. --num_shards=2 . - Error trace: . . It displays: Task reading input the .bam file but it ends up with 0 candidates. I suppose it can read the input files. Does the quick start test work on your system? This the tutorial I've used: https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/675
https://github.com/google/deepvariant/issues/675:255,usability,Command,Command,255,"Unable to get vcf output file; Describe the issue:. I can't get vcf output after bind mount a root directory. Setup. - Operating system: Windows 11, but mount an Ubuntu VM through multipass. - Type of data: fasta, bam and vcf file. Steps to reproduce:. - Command:. #Configure the DeepVariant environment variables (missing input directory,...). BIN_VERSION=""1.5.0"". sudo apt -y update. sudo apt-get -y install docker.io. sudo docker pull google/deepvariant:""${BIN_VERSION}"". # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". PWD=/mountpoint/fastQ. INPUT_DIR=""${PWD}/testdata_input"". mkdir -p ${INPUT_DIR}. OUTPUT_DIR=""${PWD}/04.deepvariant_out"". mkdir -p ""${OUTPUT_DIR}"". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/input/Homo_sapiens_assembly38.fasta \. --reads=/input/$FQ.align.sort.marked.bam \. --output_vcf=/output/$FQ.vcf.gz \. --output_gvcf=/output/$FQ.g.vcf.gz \. --num_shards=2 . - Error trace: . . It displays: Task reading input the .bam file but it ends up with 0 candidates. I suppose it can read the input files. Does the quick start test work on your system? This the tutorial I've used: https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/675
https://github.com/google/deepvariant/issues/675:323,usability,input,input,323,"Unable to get vcf output file; Describe the issue:. I can't get vcf output after bind mount a root directory. Setup. - Operating system: Windows 11, but mount an Ubuntu VM through multipass. - Type of data: fasta, bam and vcf file. Steps to reproduce:. - Command:. #Configure the DeepVariant environment variables (missing input directory,...). BIN_VERSION=""1.5.0"". sudo apt -y update. sudo apt-get -y install docker.io. sudo docker pull google/deepvariant:""${BIN_VERSION}"". # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". PWD=/mountpoint/fastQ. INPUT_DIR=""${PWD}/testdata_input"". mkdir -p ${INPUT_DIR}. OUTPUT_DIR=""${PWD}/04.deepvariant_out"". mkdir -p ""${OUTPUT_DIR}"". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/input/Homo_sapiens_assembly38.fasta \. --reads=/input/$FQ.align.sort.marked.bam \. --output_vcf=/output/$FQ.vcf.gz \. --output_gvcf=/output/$FQ.g.vcf.gz \. --num_shards=2 . - Error trace: . . It displays: Task reading input the .bam file but it ends up with 0 candidates. I suppose it can read the input files. Does the quick start test work on your system? This the tutorial I've used: https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/675
https://github.com/google/deepvariant/issues/675:742,usability,input,input,742,"Unable to get vcf output file; Describe the issue:. I can't get vcf output after bind mount a root directory. Setup. - Operating system: Windows 11, but mount an Ubuntu VM through multipass. - Type of data: fasta, bam and vcf file. Steps to reproduce:. - Command:. #Configure the DeepVariant environment variables (missing input directory,...). BIN_VERSION=""1.5.0"". sudo apt -y update. sudo apt-get -y install docker.io. sudo docker pull google/deepvariant:""${BIN_VERSION}"". # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". PWD=/mountpoint/fastQ. INPUT_DIR=""${PWD}/testdata_input"". mkdir -p ${INPUT_DIR}. OUTPUT_DIR=""${PWD}/04.deepvariant_out"". mkdir -p ""${OUTPUT_DIR}"". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/input/Homo_sapiens_assembly38.fasta \. --reads=/input/$FQ.align.sort.marked.bam \. --output_vcf=/output/$FQ.vcf.gz \. --output_gvcf=/output/$FQ.g.vcf.gz \. --num_shards=2 . - Error trace: . . It displays: Task reading input the .bam file but it ends up with 0 candidates. I suppose it can read the input files. Does the quick start test work on your system? This the tutorial I've used: https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/675
https://github.com/google/deepvariant/issues/675:858,usability,input,input,858,"Unable to get vcf output file; Describe the issue:. I can't get vcf output after bind mount a root directory. Setup. - Operating system: Windows 11, but mount an Ubuntu VM through multipass. - Type of data: fasta, bam and vcf file. Steps to reproduce:. - Command:. #Configure the DeepVariant environment variables (missing input directory,...). BIN_VERSION=""1.5.0"". sudo apt -y update. sudo apt-get -y install docker.io. sudo docker pull google/deepvariant:""${BIN_VERSION}"". # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". PWD=/mountpoint/fastQ. INPUT_DIR=""${PWD}/testdata_input"". mkdir -p ${INPUT_DIR}. OUTPUT_DIR=""${PWD}/04.deepvariant_out"". mkdir -p ""${OUTPUT_DIR}"". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/input/Homo_sapiens_assembly38.fasta \. --reads=/input/$FQ.align.sort.marked.bam \. --output_vcf=/output/$FQ.vcf.gz \. --output_gvcf=/output/$FQ.g.vcf.gz \. --num_shards=2 . - Error trace: . . It displays: Task reading input the .bam file but it ends up with 0 candidates. I suppose it can read the input files. Does the quick start test work on your system? This the tutorial I've used: https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/675
https://github.com/google/deepvariant/issues/675:906,usability,input,input,906,"Unable to get vcf output file; Describe the issue:. I can't get vcf output after bind mount a root directory. Setup. - Operating system: Windows 11, but mount an Ubuntu VM through multipass. - Type of data: fasta, bam and vcf file. Steps to reproduce:. - Command:. #Configure the DeepVariant environment variables (missing input directory,...). BIN_VERSION=""1.5.0"". sudo apt -y update. sudo apt-get -y install docker.io. sudo docker pull google/deepvariant:""${BIN_VERSION}"". # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". PWD=/mountpoint/fastQ. INPUT_DIR=""${PWD}/testdata_input"". mkdir -p ${INPUT_DIR}. OUTPUT_DIR=""${PWD}/04.deepvariant_out"". mkdir -p ""${OUTPUT_DIR}"". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/input/Homo_sapiens_assembly38.fasta \. --reads=/input/$FQ.align.sort.marked.bam \. --output_vcf=/output/$FQ.vcf.gz \. --output_gvcf=/output/$FQ.g.vcf.gz \. --num_shards=2 . - Error trace: . . It displays: Task reading input the .bam file but it ends up with 0 candidates. I suppose it can read the input files. Does the quick start test work on your system? This the tutorial I've used: https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/675
https://github.com/google/deepvariant/issues/675:1033,usability,Error,Error,1033,"Unable to get vcf output file; Describe the issue:. I can't get vcf output after bind mount a root directory. Setup. - Operating system: Windows 11, but mount an Ubuntu VM through multipass. - Type of data: fasta, bam and vcf file. Steps to reproduce:. - Command:. #Configure the DeepVariant environment variables (missing input directory,...). BIN_VERSION=""1.5.0"". sudo apt -y update. sudo apt-get -y install docker.io. sudo docker pull google/deepvariant:""${BIN_VERSION}"". # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". PWD=/mountpoint/fastQ. INPUT_DIR=""${PWD}/testdata_input"". mkdir -p ${INPUT_DIR}. OUTPUT_DIR=""${PWD}/04.deepvariant_out"". mkdir -p ""${OUTPUT_DIR}"". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/input/Homo_sapiens_assembly38.fasta \. --reads=/input/$FQ.align.sort.marked.bam \. --output_vcf=/output/$FQ.vcf.gz \. --output_gvcf=/output/$FQ.g.vcf.gz \. --num_shards=2 . - Error trace: . . It displays: Task reading input the .bam file but it ends up with 0 candidates. I suppose it can read the input files. Does the quick start test work on your system? This the tutorial I've used: https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/675
https://github.com/google/deepvariant/issues/675:1076,usability,input,input,1076,"Unable to get vcf output file; Describe the issue:. I can't get vcf output after bind mount a root directory. Setup. - Operating system: Windows 11, but mount an Ubuntu VM through multipass. - Type of data: fasta, bam and vcf file. Steps to reproduce:. - Command:. #Configure the DeepVariant environment variables (missing input directory,...). BIN_VERSION=""1.5.0"". sudo apt -y update. sudo apt-get -y install docker.io. sudo docker pull google/deepvariant:""${BIN_VERSION}"". # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". PWD=/mountpoint/fastQ. INPUT_DIR=""${PWD}/testdata_input"". mkdir -p ${INPUT_DIR}. OUTPUT_DIR=""${PWD}/04.deepvariant_out"". mkdir -p ""${OUTPUT_DIR}"". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/input/Homo_sapiens_assembly38.fasta \. --reads=/input/$FQ.align.sort.marked.bam \. --output_vcf=/output/$FQ.vcf.gz \. --output_gvcf=/output/$FQ.g.vcf.gz \. --num_shards=2 . - Error trace: . . It displays: Task reading input the .bam file but it ends up with 0 candidates. I suppose it can read the input files. Does the quick start test work on your system? This the tutorial I've used: https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/675
https://github.com/google/deepvariant/issues/675:1156,usability,input,input,1156,"Unable to get vcf output file; Describe the issue:. I can't get vcf output after bind mount a root directory. Setup. - Operating system: Windows 11, but mount an Ubuntu VM through multipass. - Type of data: fasta, bam and vcf file. Steps to reproduce:. - Command:. #Configure the DeepVariant environment variables (missing input directory,...). BIN_VERSION=""1.5.0"". sudo apt -y update. sudo apt-get -y install docker.io. sudo docker pull google/deepvariant:""${BIN_VERSION}"". # Pull the image. singularity pull docker://google/deepvariant:""${BIN_VERSION}"". PWD=/mountpoint/fastQ. INPUT_DIR=""${PWD}/testdata_input"". mkdir -p ${INPUT_DIR}. OUTPUT_DIR=""${PWD}/04.deepvariant_out"". mkdir -p ""${OUTPUT_DIR}"". sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/input/Homo_sapiens_assembly38.fasta \. --reads=/input/$FQ.align.sort.marked.bam \. --output_vcf=/output/$FQ.vcf.gz \. --output_gvcf=/output/$FQ.g.vcf.gz \. --num_shards=2 . - Error trace: . . It displays: Task reading input the .bam file but it ends up with 0 candidates. I suppose it can read the input files. Does the quick start test work on your system? This the tutorial I've used: https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/675
https://github.com/google/deepvariant/issues/676:31,availability,avail,available,31,nvidia base image is no longer available; **Describe the issue:**. nvidia/cuda:11.3.0-cudnn8-devel-ubuntu20.04 docker image is no longer available to be used as a base-image. **Setup**. - Operating system: linux. - DeepVariant version: 1.5. **Steps to reproduce:**. - Command: docker pull nvidia/cuda:11.3.0-cudnn8-devel-ubuntu20.04. - Error trace: Error response from daemon: manifest for nvidia/cuda:11.3.0-cudnn8-devel-ubuntu20.04 not found: manifest unknown: manifest unknown. . Do you have a recommendation for an alternative base-image that deepvariant docker could be built with GPU support? Thanks!,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/676
https://github.com/google/deepvariant/issues/676:137,availability,avail,available,137,nvidia base image is no longer available; **Describe the issue:**. nvidia/cuda:11.3.0-cudnn8-devel-ubuntu20.04 docker image is no longer available to be used as a base-image. **Setup**. - Operating system: linux. - DeepVariant version: 1.5. **Steps to reproduce:**. - Command: docker pull nvidia/cuda:11.3.0-cudnn8-devel-ubuntu20.04. - Error trace: Error response from daemon: manifest for nvidia/cuda:11.3.0-cudnn8-devel-ubuntu20.04 not found: manifest unknown: manifest unknown. . Do you have a recommendation for an alternative base-image that deepvariant docker could be built with GPU support? Thanks!,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/676
https://github.com/google/deepvariant/issues/676:188,availability,Operat,Operating,188,nvidia base image is no longer available; **Describe the issue:**. nvidia/cuda:11.3.0-cudnn8-devel-ubuntu20.04 docker image is no longer available to be used as a base-image. **Setup**. - Operating system: linux. - DeepVariant version: 1.5. **Steps to reproduce:**. - Command: docker pull nvidia/cuda:11.3.0-cudnn8-devel-ubuntu20.04. - Error trace: Error response from daemon: manifest for nvidia/cuda:11.3.0-cudnn8-devel-ubuntu20.04 not found: manifest unknown: manifest unknown. . Do you have a recommendation for an alternative base-image that deepvariant docker could be built with GPU support? Thanks!,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/676
https://github.com/google/deepvariant/issues/676:336,availability,Error,Error,336,nvidia base image is no longer available; **Describe the issue:**. nvidia/cuda:11.3.0-cudnn8-devel-ubuntu20.04 docker image is no longer available to be used as a base-image. **Setup**. - Operating system: linux. - DeepVariant version: 1.5. **Steps to reproduce:**. - Command: docker pull nvidia/cuda:11.3.0-cudnn8-devel-ubuntu20.04. - Error trace: Error response from daemon: manifest for nvidia/cuda:11.3.0-cudnn8-devel-ubuntu20.04 not found: manifest unknown: manifest unknown. . Do you have a recommendation for an alternative base-image that deepvariant docker could be built with GPU support? Thanks!,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/676
https://github.com/google/deepvariant/issues/676:349,availability,Error,Error,349,nvidia base image is no longer available; **Describe the issue:**. nvidia/cuda:11.3.0-cudnn8-devel-ubuntu20.04 docker image is no longer available to be used as a base-image. **Setup**. - Operating system: linux. - DeepVariant version: 1.5. **Steps to reproduce:**. - Command: docker pull nvidia/cuda:11.3.0-cudnn8-devel-ubuntu20.04. - Error trace: Error response from daemon: manifest for nvidia/cuda:11.3.0-cudnn8-devel-ubuntu20.04 not found: manifest unknown: manifest unknown. . Do you have a recommendation for an alternative base-image that deepvariant docker could be built with GPU support? Thanks!,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/676
https://github.com/google/deepvariant/issues/676:227,deployability,version,version,227,nvidia base image is no longer available; **Describe the issue:**. nvidia/cuda:11.3.0-cudnn8-devel-ubuntu20.04 docker image is no longer available to be used as a base-image. **Setup**. - Operating system: linux. - DeepVariant version: 1.5. **Steps to reproduce:**. - Command: docker pull nvidia/cuda:11.3.0-cudnn8-devel-ubuntu20.04. - Error trace: Error response from daemon: manifest for nvidia/cuda:11.3.0-cudnn8-devel-ubuntu20.04 not found: manifest unknown: manifest unknown. . Do you have a recommendation for an alternative base-image that deepvariant docker could be built with GPU support? Thanks!,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/676
https://github.com/google/deepvariant/issues/676:586,energy efficiency,GPU,GPU,586,nvidia base image is no longer available; **Describe the issue:**. nvidia/cuda:11.3.0-cudnn8-devel-ubuntu20.04 docker image is no longer available to be used as a base-image. **Setup**. - Operating system: linux. - DeepVariant version: 1.5. **Steps to reproduce:**. - Command: docker pull nvidia/cuda:11.3.0-cudnn8-devel-ubuntu20.04. - Error trace: Error response from daemon: manifest for nvidia/cuda:11.3.0-cudnn8-devel-ubuntu20.04 not found: manifest unknown: manifest unknown. . Do you have a recommendation for an alternative base-image that deepvariant docker could be built with GPU support? Thanks!,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/676
https://github.com/google/deepvariant/issues/676:227,integrability,version,version,227,nvidia base image is no longer available; **Describe the issue:**. nvidia/cuda:11.3.0-cudnn8-devel-ubuntu20.04 docker image is no longer available to be used as a base-image. **Setup**. - Operating system: linux. - DeepVariant version: 1.5. **Steps to reproduce:**. - Command: docker pull nvidia/cuda:11.3.0-cudnn8-devel-ubuntu20.04. - Error trace: Error response from daemon: manifest for nvidia/cuda:11.3.0-cudnn8-devel-ubuntu20.04 not found: manifest unknown: manifest unknown. . Do you have a recommendation for an alternative base-image that deepvariant docker could be built with GPU support? Thanks!,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/676
https://github.com/google/deepvariant/issues/676:227,modifiability,version,version,227,nvidia base image is no longer available; **Describe the issue:**. nvidia/cuda:11.3.0-cudnn8-devel-ubuntu20.04 docker image is no longer available to be used as a base-image. **Setup**. - Operating system: linux. - DeepVariant version: 1.5. **Steps to reproduce:**. - Command: docker pull nvidia/cuda:11.3.0-cudnn8-devel-ubuntu20.04. - Error trace: Error response from daemon: manifest for nvidia/cuda:11.3.0-cudnn8-devel-ubuntu20.04 not found: manifest unknown: manifest unknown. . Do you have a recommendation for an alternative base-image that deepvariant docker could be built with GPU support? Thanks!,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/676
https://github.com/google/deepvariant/issues/676:336,performance,Error,Error,336,nvidia base image is no longer available; **Describe the issue:**. nvidia/cuda:11.3.0-cudnn8-devel-ubuntu20.04 docker image is no longer available to be used as a base-image. **Setup**. - Operating system: linux. - DeepVariant version: 1.5. **Steps to reproduce:**. - Command: docker pull nvidia/cuda:11.3.0-cudnn8-devel-ubuntu20.04. - Error trace: Error response from daemon: manifest for nvidia/cuda:11.3.0-cudnn8-devel-ubuntu20.04 not found: manifest unknown: manifest unknown. . Do you have a recommendation for an alternative base-image that deepvariant docker could be built with GPU support? Thanks!,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/676
https://github.com/google/deepvariant/issues/676:349,performance,Error,Error,349,nvidia base image is no longer available; **Describe the issue:**. nvidia/cuda:11.3.0-cudnn8-devel-ubuntu20.04 docker image is no longer available to be used as a base-image. **Setup**. - Operating system: linux. - DeepVariant version: 1.5. **Steps to reproduce:**. - Command: docker pull nvidia/cuda:11.3.0-cudnn8-devel-ubuntu20.04. - Error trace: Error response from daemon: manifest for nvidia/cuda:11.3.0-cudnn8-devel-ubuntu20.04 not found: manifest unknown: manifest unknown. . Do you have a recommendation for an alternative base-image that deepvariant docker could be built with GPU support? Thanks!,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/676
https://github.com/google/deepvariant/issues/676:586,performance,GPU,GPU,586,nvidia base image is no longer available; **Describe the issue:**. nvidia/cuda:11.3.0-cudnn8-devel-ubuntu20.04 docker image is no longer available to be used as a base-image. **Setup**. - Operating system: linux. - DeepVariant version: 1.5. **Steps to reproduce:**. - Command: docker pull nvidia/cuda:11.3.0-cudnn8-devel-ubuntu20.04. - Error trace: Error response from daemon: manifest for nvidia/cuda:11.3.0-cudnn8-devel-ubuntu20.04 not found: manifest unknown: manifest unknown. . Do you have a recommendation for an alternative base-image that deepvariant docker could be built with GPU support? Thanks!,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/676
https://github.com/google/deepvariant/issues/676:31,reliability,availab,available,31,nvidia base image is no longer available; **Describe the issue:**. nvidia/cuda:11.3.0-cudnn8-devel-ubuntu20.04 docker image is no longer available to be used as a base-image. **Setup**. - Operating system: linux. - DeepVariant version: 1.5. **Steps to reproduce:**. - Command: docker pull nvidia/cuda:11.3.0-cudnn8-devel-ubuntu20.04. - Error trace: Error response from daemon: manifest for nvidia/cuda:11.3.0-cudnn8-devel-ubuntu20.04 not found: manifest unknown: manifest unknown. . Do you have a recommendation for an alternative base-image that deepvariant docker could be built with GPU support? Thanks!,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/676
https://github.com/google/deepvariant/issues/676:137,reliability,availab,available,137,nvidia base image is no longer available; **Describe the issue:**. nvidia/cuda:11.3.0-cudnn8-devel-ubuntu20.04 docker image is no longer available to be used as a base-image. **Setup**. - Operating system: linux. - DeepVariant version: 1.5. **Steps to reproduce:**. - Command: docker pull nvidia/cuda:11.3.0-cudnn8-devel-ubuntu20.04. - Error trace: Error response from daemon: manifest for nvidia/cuda:11.3.0-cudnn8-devel-ubuntu20.04 not found: manifest unknown: manifest unknown. . Do you have a recommendation for an alternative base-image that deepvariant docker could be built with GPU support? Thanks!,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/676
https://github.com/google/deepvariant/issues/676:31,safety,avail,available,31,nvidia base image is no longer available; **Describe the issue:**. nvidia/cuda:11.3.0-cudnn8-devel-ubuntu20.04 docker image is no longer available to be used as a base-image. **Setup**. - Operating system: linux. - DeepVariant version: 1.5. **Steps to reproduce:**. - Command: docker pull nvidia/cuda:11.3.0-cudnn8-devel-ubuntu20.04. - Error trace: Error response from daemon: manifest for nvidia/cuda:11.3.0-cudnn8-devel-ubuntu20.04 not found: manifest unknown: manifest unknown. . Do you have a recommendation for an alternative base-image that deepvariant docker could be built with GPU support? Thanks!,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/676
https://github.com/google/deepvariant/issues/676:137,safety,avail,available,137,nvidia base image is no longer available; **Describe the issue:**. nvidia/cuda:11.3.0-cudnn8-devel-ubuntu20.04 docker image is no longer available to be used as a base-image. **Setup**. - Operating system: linux. - DeepVariant version: 1.5. **Steps to reproduce:**. - Command: docker pull nvidia/cuda:11.3.0-cudnn8-devel-ubuntu20.04. - Error trace: Error response from daemon: manifest for nvidia/cuda:11.3.0-cudnn8-devel-ubuntu20.04 not found: manifest unknown: manifest unknown. . Do you have a recommendation for an alternative base-image that deepvariant docker could be built with GPU support? Thanks!,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/676
https://github.com/google/deepvariant/issues/676:336,safety,Error,Error,336,nvidia base image is no longer available; **Describe the issue:**. nvidia/cuda:11.3.0-cudnn8-devel-ubuntu20.04 docker image is no longer available to be used as a base-image. **Setup**. - Operating system: linux. - DeepVariant version: 1.5. **Steps to reproduce:**. - Command: docker pull nvidia/cuda:11.3.0-cudnn8-devel-ubuntu20.04. - Error trace: Error response from daemon: manifest for nvidia/cuda:11.3.0-cudnn8-devel-ubuntu20.04 not found: manifest unknown: manifest unknown. . Do you have a recommendation for an alternative base-image that deepvariant docker could be built with GPU support? Thanks!,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/676
https://github.com/google/deepvariant/issues/676:349,safety,Error,Error,349,nvidia base image is no longer available; **Describe the issue:**. nvidia/cuda:11.3.0-cudnn8-devel-ubuntu20.04 docker image is no longer available to be used as a base-image. **Setup**. - Operating system: linux. - DeepVariant version: 1.5. **Steps to reproduce:**. - Command: docker pull nvidia/cuda:11.3.0-cudnn8-devel-ubuntu20.04. - Error trace: Error response from daemon: manifest for nvidia/cuda:11.3.0-cudnn8-devel-ubuntu20.04 not found: manifest unknown: manifest unknown. . Do you have a recommendation for an alternative base-image that deepvariant docker could be built with GPU support? Thanks!,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/676
https://github.com/google/deepvariant/issues/676:31,security,availab,available,31,nvidia base image is no longer available; **Describe the issue:**. nvidia/cuda:11.3.0-cudnn8-devel-ubuntu20.04 docker image is no longer available to be used as a base-image. **Setup**. - Operating system: linux. - DeepVariant version: 1.5. **Steps to reproduce:**. - Command: docker pull nvidia/cuda:11.3.0-cudnn8-devel-ubuntu20.04. - Error trace: Error response from daemon: manifest for nvidia/cuda:11.3.0-cudnn8-devel-ubuntu20.04 not found: manifest unknown: manifest unknown. . Do you have a recommendation for an alternative base-image that deepvariant docker could be built with GPU support? Thanks!,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/676
https://github.com/google/deepvariant/issues/676:137,security,availab,available,137,nvidia base image is no longer available; **Describe the issue:**. nvidia/cuda:11.3.0-cudnn8-devel-ubuntu20.04 docker image is no longer available to be used as a base-image. **Setup**. - Operating system: linux. - DeepVariant version: 1.5. **Steps to reproduce:**. - Command: docker pull nvidia/cuda:11.3.0-cudnn8-devel-ubuntu20.04. - Error trace: Error response from daemon: manifest for nvidia/cuda:11.3.0-cudnn8-devel-ubuntu20.04 not found: manifest unknown: manifest unknown. . Do you have a recommendation for an alternative base-image that deepvariant docker could be built with GPU support? Thanks!,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/676
https://github.com/google/deepvariant/issues/676:342,testability,trace,trace,342,nvidia base image is no longer available; **Describe the issue:**. nvidia/cuda:11.3.0-cudnn8-devel-ubuntu20.04 docker image is no longer available to be used as a base-image. **Setup**. - Operating system: linux. - DeepVariant version: 1.5. **Steps to reproduce:**. - Command: docker pull nvidia/cuda:11.3.0-cudnn8-devel-ubuntu20.04. - Error trace: Error response from daemon: manifest for nvidia/cuda:11.3.0-cudnn8-devel-ubuntu20.04 not found: manifest unknown: manifest unknown. . Do you have a recommendation for an alternative base-image that deepvariant docker could be built with GPU support? Thanks!,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/676
https://github.com/google/deepvariant/issues/676:268,usability,Command,Command,268,nvidia base image is no longer available; **Describe the issue:**. nvidia/cuda:11.3.0-cudnn8-devel-ubuntu20.04 docker image is no longer available to be used as a base-image. **Setup**. - Operating system: linux. - DeepVariant version: 1.5. **Steps to reproduce:**. - Command: docker pull nvidia/cuda:11.3.0-cudnn8-devel-ubuntu20.04. - Error trace: Error response from daemon: manifest for nvidia/cuda:11.3.0-cudnn8-devel-ubuntu20.04 not found: manifest unknown: manifest unknown. . Do you have a recommendation for an alternative base-image that deepvariant docker could be built with GPU support? Thanks!,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/676
https://github.com/google/deepvariant/issues/676:336,usability,Error,Error,336,nvidia base image is no longer available; **Describe the issue:**. nvidia/cuda:11.3.0-cudnn8-devel-ubuntu20.04 docker image is no longer available to be used as a base-image. **Setup**. - Operating system: linux. - DeepVariant version: 1.5. **Steps to reproduce:**. - Command: docker pull nvidia/cuda:11.3.0-cudnn8-devel-ubuntu20.04. - Error trace: Error response from daemon: manifest for nvidia/cuda:11.3.0-cudnn8-devel-ubuntu20.04 not found: manifest unknown: manifest unknown. . Do you have a recommendation for an alternative base-image that deepvariant docker could be built with GPU support? Thanks!,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/676
https://github.com/google/deepvariant/issues/676:349,usability,Error,Error,349,nvidia base image is no longer available; **Describe the issue:**. nvidia/cuda:11.3.0-cudnn8-devel-ubuntu20.04 docker image is no longer available to be used as a base-image. **Setup**. - Operating system: linux. - DeepVariant version: 1.5. **Steps to reproduce:**. - Command: docker pull nvidia/cuda:11.3.0-cudnn8-devel-ubuntu20.04. - Error trace: Error response from daemon: manifest for nvidia/cuda:11.3.0-cudnn8-devel-ubuntu20.04 not found: manifest unknown: manifest unknown. . Do you have a recommendation for an alternative base-image that deepvariant docker could be built with GPU support? Thanks!,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/676
https://github.com/google/deepvariant/issues/676:590,usability,support,support,590,nvidia base image is no longer available; **Describe the issue:**. nvidia/cuda:11.3.0-cudnn8-devel-ubuntu20.04 docker image is no longer available to be used as a base-image. **Setup**. - Operating system: linux. - DeepVariant version: 1.5. **Steps to reproduce:**. - Command: docker pull nvidia/cuda:11.3.0-cudnn8-devel-ubuntu20.04. - Error trace: Error response from daemon: manifest for nvidia/cuda:11.3.0-cudnn8-devel-ubuntu20.04 not found: manifest unknown: manifest unknown. . Do you have a recommendation for an alternative base-image that deepvariant docker could be built with GPU support? Thanks!,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/676
https://github.com/google/deepvariant/issues/677:190,availability,error,error,190,"RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed; **Describe the issue:**. Hello everyone, i am trying to run a Pacbio Workflow with deepvariant in it but i get an error in the make example step ( Rule and log below) i allready have an open Issue on the Workflow but we are at the Point that we think its ether Nucleus or Tensorflow that produces the error PacificBiosciences/HiFiTargetEnrichment#4 , since i cant find what the error is and how to fix it i opend the Issue. Many thanks in advance. **Setup**. - Operating system: Ubuntu 20.04.6 LTS. - DeepVariant version: 1.5.0. - Tensorflow 2.11.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: PacBio HIFI reads. **Steps to reproduce:**. ```. rule deepvariant_make_examples:. input:. bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",. bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",. reference=config[""ref""][""fasta""],. output:. tfrecord=temp(. f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz"". ),. nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>. log:. f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",. benchmark:. f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv"". container:. f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channel",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:377,availability,error,error,377,"RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed; **Describe the issue:**. Hello everyone, i am trying to run a Pacbio Workflow with deepvariant in it but i get an error in the make example step ( Rule and log below) i allready have an open Issue on the Workflow but we are at the Point that we think its ether Nucleus or Tensorflow that produces the error PacificBiosciences/HiFiTargetEnrichment#4 , since i cant find what the error is and how to fix it i opend the Issue. Many thanks in advance. **Setup**. - Operating system: Ubuntu 20.04.6 LTS. - DeepVariant version: 1.5.0. - Tensorflow 2.11.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: PacBio HIFI reads. **Steps to reproduce:**. ```. rule deepvariant_make_examples:. input:. bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",. bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",. reference=config[""ref""][""fasta""],. output:. tfrecord=temp(. f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz"". ),. nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>. log:. f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",. benchmark:. f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv"". container:. f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channel",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:454,availability,error,error,454,"RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed; **Describe the issue:**. Hello everyone, i am trying to run a Pacbio Workflow with deepvariant in it but i get an error in the make example step ( Rule and log below) i allready have an open Issue on the Workflow but we are at the Point that we think its ether Nucleus or Tensorflow that produces the error PacificBiosciences/HiFiTargetEnrichment#4 , since i cant find what the error is and how to fix it i opend the Issue. Many thanks in advance. **Setup**. - Operating system: Ubuntu 20.04.6 LTS. - DeepVariant version: 1.5.0. - Tensorflow 2.11.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: PacBio HIFI reads. **Steps to reproduce:**. ```. rule deepvariant_make_examples:. input:. bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",. bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",. reference=config[""ref""][""fasta""],. output:. tfrecord=temp(. f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz"". ),. nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>. log:. f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",. benchmark:. f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv"". container:. f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channel",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:537,availability,Operat,Operating,537,"RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed; **Describe the issue:**. Hello everyone, i am trying to run a Pacbio Workflow with deepvariant in it but i get an error in the make example step ( Rule and log below) i allready have an open Issue on the Workflow but we are at the Point that we think its ether Nucleus or Tensorflow that produces the error PacificBiosciences/HiFiTargetEnrichment#4 , since i cant find what the error is and how to fix it i opend the Issue. Many thanks in advance. **Setup**. - Operating system: Ubuntu 20.04.6 LTS. - DeepVariant version: 1.5.0. - Tensorflow 2.11.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: PacBio HIFI reads. **Steps to reproduce:**. ```. rule deepvariant_make_examples:. input:. bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",. bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",. reference=config[""ref""][""fasta""],. output:. tfrecord=temp(. f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz"". ),. nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>. log:. f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",. benchmark:. f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv"". container:. f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channel",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:2485,availability,Error,Error,2485,"nt:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channels \. --min_mapping_quality=1 \. --parse_sam_aux_fields \. --partition_size=25000 \. --max_reads_per_partition=600 \. --phase_reads \. --pileup_image_width {params.pileup_image_width} \. --norealign_reads \. --sort_by_haplotypes \. --track_ref_reads \. --vsc_min_fraction_indels {params.vsc_min_fraction_indels} \. --mode calling \. --ref {input.reference} \. --reads {input.bam} \. --examples {params.examples} \. --gvcf {params.gvcf} \. --task {params.shard}) > {log} 2>&1. """""". ```. Error:. ```. 2023-07-12 15:19:55.926661: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI De>. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-07-12 15:19:59.038994: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.6 SO:>. 2023-07-12 15:19:59.039047: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag BC: in RG line, ignoring: @RG ID:d98f52ac>. 2023-07-12 15:19:59.039065: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG ID:d98f52ac>. I0712 15:19:59.039340 140472429422400 genomics_reader.py:222] Reading batches/Test349/D18757/aligned/D18757.hs37d5.bam with NativeS>. I0712 15:19:59.044630 140472429422400 make_examples_core.py:257] Task 32/96: Preparing inputs. 2023-07-12 15:19:59.049074: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.6 SO:>. 2023-07-12 15:19:",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:2656,availability,operat,operations,2656,"xamples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channels \. --min_mapping_quality=1 \. --parse_sam_aux_fields \. --partition_size=25000 \. --max_reads_per_partition=600 \. --phase_reads \. --pileup_image_width {params.pileup_image_width} \. --norealign_reads \. --sort_by_haplotypes \. --track_ref_reads \. --vsc_min_fraction_indels {params.vsc_min_fraction_indels} \. --mode calling \. --ref {input.reference} \. --reads {input.bam} \. --examples {params.examples} \. --gvcf {params.gvcf} \. --task {params.shard}) > {log} 2>&1. """""". ```. Error:. ```. 2023-07-12 15:19:55.926661: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI De>. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-07-12 15:19:59.038994: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.6 SO:>. 2023-07-12 15:19:59.039047: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag BC: in RG line, ignoring: @RG ID:d98f52ac>. 2023-07-12 15:19:59.039065: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG ID:d98f52ac>. I0712 15:19:59.039340 140472429422400 genomics_reader.py:222] Reading batches/Test349/D18757/aligned/D18757.hs37d5.bam with NativeS>. I0712 15:19:59.044630 140472429422400 make_examples_core.py:257] Task 32/96: Preparing inputs. 2023-07-12 15:19:59.049074: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.6 SO:>. 2023-07-12 15:19:59.049123: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag BC: in RG line, ignoring: @RG ID:d98f52ac>. 2023-07-12 15:19:59.049140: W third_party/nucleus/io/sam_reade",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:68,deployability,fail,failed,68,"RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed; **Describe the issue:**. Hello everyone, i am trying to run a Pacbio Workflow with deepvariant in it but i get an error in the make example step ( Rule and log below) i allready have an open Issue on the Workflow but we are at the Point that we think its ether Nucleus or Tensorflow that produces the error PacificBiosciences/HiFiTargetEnrichment#4 , since i cant find what the error is and how to fix it i opend the Issue. Many thanks in advance. **Setup**. - Operating system: Ubuntu 20.04.6 LTS. - DeepVariant version: 1.5.0. - Tensorflow 2.11.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: PacBio HIFI reads. **Steps to reproduce:**. ```. rule deepvariant_make_examples:. input:. bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",. bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",. reference=config[""ref""][""fasta""],. output:. tfrecord=temp(. f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz"". ),. nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>. log:. f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",. benchmark:. f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv"". container:. f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channel",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:232,deployability,log,log,232,"RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed; **Describe the issue:**. Hello everyone, i am trying to run a Pacbio Workflow with deepvariant in it but i get an error in the make example step ( Rule and log below) i allready have an open Issue on the Workflow but we are at the Point that we think its ether Nucleus or Tensorflow that produces the error PacificBiosciences/HiFiTargetEnrichment#4 , since i cant find what the error is and how to fix it i opend the Issue. Many thanks in advance. **Setup**. - Operating system: Ubuntu 20.04.6 LTS. - DeepVariant version: 1.5.0. - Tensorflow 2.11.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: PacBio HIFI reads. **Steps to reproduce:**. ```. rule deepvariant_make_examples:. input:. bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",. bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",. reference=config[""ref""][""fasta""],. output:. tfrecord=temp(. f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz"". ),. nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>. log:. f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",. benchmark:. f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv"". container:. f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channel",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:589,deployability,version,version,589,"RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed; **Describe the issue:**. Hello everyone, i am trying to run a Pacbio Workflow with deepvariant in it but i get an error in the make example step ( Rule and log below) i allready have an open Issue on the Workflow but we are at the Point that we think its ether Nucleus or Tensorflow that produces the error PacificBiosciences/HiFiTargetEnrichment#4 , since i cant find what the error is and how to fix it i opend the Issue. Many thanks in advance. **Setup**. - Operating system: Ubuntu 20.04.6 LTS. - DeepVariant version: 1.5.0. - Tensorflow 2.11.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: PacBio HIFI reads. **Steps to reproduce:**. ```. rule deepvariant_make_examples:. input:. bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",. bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",. reference=config[""ref""][""fasta""],. output:. tfrecord=temp(. f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz"". ),. nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>. log:. f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",. benchmark:. f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv"". container:. f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channel",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:628,deployability,Instal,Installation,628,"RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed; **Describe the issue:**. Hello everyone, i am trying to run a Pacbio Workflow with deepvariant in it but i get an error in the make example step ( Rule and log below) i allready have an open Issue on the Workflow but we are at the Point that we think its ether Nucleus or Tensorflow that produces the error PacificBiosciences/HiFiTargetEnrichment#4 , since i cant find what the error is and how to fix it i opend the Issue. Many thanks in advance. **Setup**. - Operating system: Ubuntu 20.04.6 LTS. - DeepVariant version: 1.5.0. - Tensorflow 2.11.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: PacBio HIFI reads. **Steps to reproduce:**. ```. rule deepvariant_make_examples:. input:. bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",. bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",. reference=config[""ref""][""fasta""],. output:. tfrecord=temp(. f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz"". ),. nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>. log:. f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",. benchmark:. f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv"". container:. f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channel",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:1236,deployability,log,log,1236,"low) i allready have an open Issue on the Workflow but we are at the Point that we think its ether Nucleus or Tensorflow that produces the error PacificBiosciences/HiFiTargetEnrichment#4 , since i cant find what the error is and how to fix it i opend the Issue. Many thanks in advance. **Setup**. - Operating system: Ubuntu 20.04.6 LTS. - DeepVariant version: 1.5.0. - Tensorflow 2.11.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: PacBio HIFI reads. **Steps to reproduce:**. ```. rule deepvariant_make_examples:. input:. bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",. bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",. reference=config[""ref""][""fasta""],. output:. tfrecord=temp(. f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz"". ),. nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>. log:. f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",. benchmark:. f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv"". container:. f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channels \. --min_mapping_quality=1 \. --parse_sam_aux_fields \. --partition_size=25000 \. --max_reads_per_partition=600 \. --phase_reads \. --pileup_image_width {params.pileup_image_width} \. --norealign_reads \. --sort_by_haplotypes \. --track",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:1260,deployability,log,logs,1260,"open Issue on the Workflow but we are at the Point that we think its ether Nucleus or Tensorflow that produces the error PacificBiosciences/HiFiTargetEnrichment#4 , since i cant find what the error is and how to fix it i opend the Issue. Many thanks in advance. **Setup**. - Operating system: Ubuntu 20.04.6 LTS. - DeepVariant version: 1.5.0. - Tensorflow 2.11.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: PacBio HIFI reads. **Steps to reproduce:**. ```. rule deepvariant_make_examples:. input:. bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",. bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",. reference=config[""ref""][""fasta""],. output:. tfrecord=temp(. f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz"". ),. nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>. log:. f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",. benchmark:. f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv"". container:. f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channels \. --min_mapping_quality=1 \. --parse_sam_aux_fields \. --partition_size=25000 \. --max_reads_per_partition=600 \. --phase_reads \. --pileup_image_width {params.pileup_image_width} \. --norealign_reads \. --sort_by_haplotypes \. --track_ref_reads \. --vsc_min_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:1345,deployability,log,log,1345," Tensorflow that produces the error PacificBiosciences/HiFiTargetEnrichment#4 , since i cant find what the error is and how to fix it i opend the Issue. Many thanks in advance. **Setup**. - Operating system: Ubuntu 20.04.6 LTS. - DeepVariant version: 1.5.0. - Tensorflow 2.11.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: PacBio HIFI reads. **Steps to reproduce:**. ```. rule deepvariant_make_examples:. input:. bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",. bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",. reference=config[""ref""][""fasta""],. output:. tfrecord=temp(. f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz"". ),. nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>. log:. f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",. benchmark:. f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv"". container:. f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channels \. --min_mapping_quality=1 \. --parse_sam_aux_fields \. --partition_size=25000 \. --max_reads_per_partition=600 \. --phase_reads \. --pileup_image_width {params.pileup_image_width} \. --norealign_reads \. --sort_by_haplotypes \. --track_ref_reads \. --vsc_min_fraction_indels {params.vsc_min_fraction_indels} \. --mode calling \. --ref {input.re",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:1449,deployability,contain,container,1449,"error is and how to fix it i opend the Issue. Many thanks in advance. **Setup**. - Operating system: Ubuntu 20.04.6 LTS. - DeepVariant version: 1.5.0. - Tensorflow 2.11.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: PacBio HIFI reads. **Steps to reproduce:**. ```. rule deepvariant_make_examples:. input:. bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",. bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",. reference=config[""ref""][""fasta""],. output:. tfrecord=temp(. f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz"". ),. nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>. log:. f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",. benchmark:. f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv"". container:. f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channels \. --min_mapping_quality=1 \. --parse_sam_aux_fields \. --partition_size=25000 \. --max_reads_per_partition=600 \. --phase_reads \. --pileup_image_width {params.pileup_image_width} \. --norealign_reads \. --sort_by_haplotypes \. --track_ref_reads \. --vsc_min_fraction_indels {params.vsc_min_fraction_indels} \. --mode calling \. --ref {input.reference} \. --reads {input.bam} \. --examples {params.examples} \. --gvcf {params.gvcf} \. --task {params.s",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:2464,deployability,log,log,2464,"ker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channels \. --min_mapping_quality=1 \. --parse_sam_aux_fields \. --partition_size=25000 \. --max_reads_per_partition=600 \. --phase_reads \. --pileup_image_width {params.pileup_image_width} \. --norealign_reads \. --sort_by_haplotypes \. --track_ref_reads \. --vsc_min_fraction_indels {params.vsc_min_fraction_indels} \. --mode calling \. --ref {input.reference} \. --reads {input.bam} \. --examples {params.examples} \. --gvcf {params.gvcf} \. --task {params.shard}) > {log} 2>&1. """""". ```. Error:. ```. 2023-07-12 15:19:55.926661: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI De>. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-07-12 15:19:59.038994: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.6 SO:>. 2023-07-12 15:19:59.039047: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag BC: in RG line, ignoring: @RG ID:d98f52ac>. 2023-07-12 15:19:59.039065: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG ID:d98f52ac>. I0712 15:19:59.039340 140472429422400 genomics_reader.py:222] Reading batches/Test349/D18757/aligned/D18757.hs37d5.bam with NativeS>. I0712 15:19:59.044630 140472429422400 make_examples_core.py:257] Task 32/96: Preparing inputs. 2023-07-12 15:19:59.049074: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.6 S",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:5755,deployability,Fail,Failed,5755,"eS>. 2023-07-12 15:19:59.535088: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.6 SO:>. 2023-07-12 15:19:59.535146: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag BC: in RG line, ignoring: @RG ID:d98f52ac>. 2023-07-12 15:19:59.535166: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG ID:d98f52ac>. I0712 15:19:59.535452 140472429422400 genomics_reader.py:222] Reading batches/Test349/D18757/aligned/D18757.hs37d5.bam with NativeS>. I0712 15:19:59.536432 140472429422400 make_examples_core.py:257] Task 32/96: Writing gvcf records to batches/Test349/D18757/deepvar>. I0712 15:19:59.536834 140472429422400 make_examples_core.py:257] Task 32/96: Writing examples to batches/Test349/D18757/deepvariant>. I0712 15:19:59.536924 140472429422400 make_examples_core.py:257] Task 32/96: Overhead for preparing inputs: 0 seconds. 2023-07-12 15:19:59.545470: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicM>. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2183, in make_example>. runtimes) = region_processor.process(region). File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1285, in process. sample_reads = self.region_reads_norealign(. File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/co",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:5953,deployability,modul,module,5953,"r.cc:174] Unknown tag BC: in RG line, ignoring: @RG ID:d98f52ac>. 2023-07-12 15:19:59.535166: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG ID:d98f52ac>. I0712 15:19:59.535452 140472429422400 genomics_reader.py:222] Reading batches/Test349/D18757/aligned/D18757.hs37d5.bam with NativeS>. I0712 15:19:59.536432 140472429422400 make_examples_core.py:257] Task 32/96: Writing gvcf records to batches/Test349/D18757/deepvar>. I0712 15:19:59.536834 140472429422400 make_examples_core.py:257] Task 32/96: Writing examples to batches/Test349/D18757/deepvariant>. I0712 15:19:59.536924 140472429422400 make_examples_core.py:257] Task 32/96: Overhead for preparing inputs: 0 seconds. 2023-07-12 15:19:59.545470: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicM>. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2183, in make_example>. runtimes) = region_processor.process(region). File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1285, in process. sample_reads = self.region_reads_norealign(. File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1381, in region_reads>. reads = utils.reservoir_sample(reads,. File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/com_google_deepvariant/third_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:7469,deployability,fail,failed,7469,"] Task 32/96: Writing examples to batches/Test349/D18757/deepvariant>. I0712 15:19:59.536924 140472429422400 make_examples_core.py:257] Task 32/96: Overhead for preparing inputs: 0 seconds. 2023-07-12 15:19:59.545470: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicM>. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2183, in make_example>. runtimes) = region_processor.process(region). File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1285, in process. sample_reads = self.region_reads_norealign(. File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1381, in region_reads>. reads = utils.reservoir_sample(reads,. File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/com_google_deepvariant/third_party/nucleus/util/utils.py"", line 117, in reservoir_sam>. for i, item in enumerate(iterable):. File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_ne>. not_done = self._cc_iterable.PythonNext(record). RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed. ```.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:2539,energy efficiency,core,core,2539,"_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channels \. --min_mapping_quality=1 \. --parse_sam_aux_fields \. --partition_size=25000 \. --max_reads_per_partition=600 \. --phase_reads \. --pileup_image_width {params.pileup_image_width} \. --norealign_reads \. --sort_by_haplotypes \. --track_ref_reads \. --vsc_min_fraction_indels {params.vsc_min_fraction_indels} \. --mode calling \. --ref {input.reference} \. --reads {input.bam} \. --examples {params.examples} \. --gvcf {params.gvcf} \. --task {params.shard}) > {log} 2>&1. """""". ```. Error:. ```. 2023-07-12 15:19:55.926661: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI De>. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-07-12 15:19:59.038994: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.6 SO:>. 2023-07-12 15:19:59.039047: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag BC: in RG line, ignoring: @RG ID:d98f52ac>. 2023-07-12 15:19:59.039065: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG ID:d98f52ac>. I0712 15:19:59.039340 140472429422400 genomics_reader.py:222] Reading batches/Test349/D18757/aligned/D18757.hs37d5.bam with NativeS>. I0712 15:19:59.044630 140472429422400 make_examples_core.py:257] Task 32/96: Preparing inputs. 2023-07-12 15:19:59.049074: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.6 SO:>. 2023-07-12 15:19:59.049123: W third_party/nucleus/io/sam_reader.cc:174",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:2605,energy efficiency,optim,optimized,2605,"examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channels \. --min_mapping_quality=1 \. --parse_sam_aux_fields \. --partition_size=25000 \. --max_reads_per_partition=600 \. --phase_reads \. --pileup_image_width {params.pileup_image_width} \. --norealign_reads \. --sort_by_haplotypes \. --track_ref_reads \. --vsc_min_fraction_indels {params.vsc_min_fraction_indels} \. --mode calling \. --ref {input.reference} \. --reads {input.bam} \. --examples {params.examples} \. --gvcf {params.gvcf} \. --task {params.shard}) > {log} 2>&1. """""". ```. Error:. ```. 2023-07-12 15:19:55.926661: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI De>. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-07-12 15:19:59.038994: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.6 SO:>. 2023-07-12 15:19:59.039047: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag BC: in RG line, ignoring: @RG ID:d98f52ac>. 2023-07-12 15:19:59.039065: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG ID:d98f52ac>. I0712 15:19:59.039340 140472429422400 genomics_reader.py:222] Reading batches/Test349/D18757/aligned/D18757.hs37d5.bam with NativeS>. I0712 15:19:59.044630 140472429422400 make_examples_core.py:257] Task 32/96: Preparing inputs. 2023-07-12 15:19:59.049074: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.6 SO:>. 2023-07-12 15:19:59.049123: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag BC: in RG line, ignoring: @RG ID:d98f52ac>. 2023-07-12 ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:589,integrability,version,version,589,"RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed; **Describe the issue:**. Hello everyone, i am trying to run a Pacbio Workflow with deepvariant in it but i get an error in the make example step ( Rule and log below) i allready have an open Issue on the Workflow but we are at the Point that we think its ether Nucleus or Tensorflow that produces the error PacificBiosciences/HiFiTargetEnrichment#4 , since i cant find what the error is and how to fix it i opend the Issue. Many thanks in advance. **Setup**. - Operating system: Ubuntu 20.04.6 LTS. - DeepVariant version: 1.5.0. - Tensorflow 2.11.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: PacBio HIFI reads. **Steps to reproduce:**. ```. rule deepvariant_make_examples:. input:. bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",. bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",. reference=config[""ref""][""fasta""],. output:. tfrecord=temp(. f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz"". ),. nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>. log:. f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",. benchmark:. f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv"". container:. f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channel",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:808,integrability,batch,batches,808,"RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed; **Describe the issue:**. Hello everyone, i am trying to run a Pacbio Workflow with deepvariant in it but i get an error in the make example step ( Rule and log below) i allready have an open Issue on the Workflow but we are at the Point that we think its ether Nucleus or Tensorflow that produces the error PacificBiosciences/HiFiTargetEnrichment#4 , since i cant find what the error is and how to fix it i opend the Issue. Many thanks in advance. **Setup**. - Operating system: Ubuntu 20.04.6 LTS. - DeepVariant version: 1.5.0. - Tensorflow 2.11.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: PacBio HIFI reads. **Steps to reproduce:**. ```. rule deepvariant_make_examples:. input:. bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",. bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",. reference=config[""ref""][""fasta""],. output:. tfrecord=temp(. f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz"". ),. nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>. log:. f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",. benchmark:. f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv"". container:. f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channel",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:817,integrability,batch,batch,817,"RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed; **Describe the issue:**. Hello everyone, i am trying to run a Pacbio Workflow with deepvariant in it but i get an error in the make example step ( Rule and log below) i allready have an open Issue on the Workflow but we are at the Point that we think its ether Nucleus or Tensorflow that produces the error PacificBiosciences/HiFiTargetEnrichment#4 , since i cant find what the error is and how to fix it i opend the Issue. Many thanks in advance. **Setup**. - Operating system: Ubuntu 20.04.6 LTS. - DeepVariant version: 1.5.0. - Tensorflow 2.11.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: PacBio HIFI reads. **Steps to reproduce:**. ```. rule deepvariant_make_examples:. input:. bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",. bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",. reference=config[""ref""][""fasta""],. output:. tfrecord=temp(. f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz"". ),. nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>. log:. f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",. benchmark:. f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv"". container:. f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channel",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:873,integrability,batch,batches,873,"RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed; **Describe the issue:**. Hello everyone, i am trying to run a Pacbio Workflow with deepvariant in it but i get an error in the make example step ( Rule and log below) i allready have an open Issue on the Workflow but we are at the Point that we think its ether Nucleus or Tensorflow that produces the error PacificBiosciences/HiFiTargetEnrichment#4 , since i cant find what the error is and how to fix it i opend the Issue. Many thanks in advance. **Setup**. - Operating system: Ubuntu 20.04.6 LTS. - DeepVariant version: 1.5.0. - Tensorflow 2.11.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: PacBio HIFI reads. **Steps to reproduce:**. ```. rule deepvariant_make_examples:. input:. bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",. bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",. reference=config[""ref""][""fasta""],. output:. tfrecord=temp(. f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz"". ),. nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>. log:. f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",. benchmark:. f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv"". container:. f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channel",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:882,integrability,batch,batch,882,"RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed; **Describe the issue:**. Hello everyone, i am trying to run a Pacbio Workflow with deepvariant in it but i get an error in the make example step ( Rule and log below) i allready have an open Issue on the Workflow but we are at the Point that we think its ether Nucleus or Tensorflow that produces the error PacificBiosciences/HiFiTargetEnrichment#4 , since i cant find what the error is and how to fix it i opend the Issue. Many thanks in advance. **Setup**. - Operating system: Ubuntu 20.04.6 LTS. - DeepVariant version: 1.5.0. - Tensorflow 2.11.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: PacBio HIFI reads. **Steps to reproduce:**. ```. rule deepvariant_make_examples:. input:. bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",. bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",. reference=config[""ref""][""fasta""],. output:. tfrecord=temp(. f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz"". ),. nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>. log:. f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",. benchmark:. f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv"". container:. f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channel",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:998,integrability,batch,batches,998,"ntimeError: PythonNext() argument read is not valid: Dynamic cast failed; **Describe the issue:**. Hello everyone, i am trying to run a Pacbio Workflow with deepvariant in it but i get an error in the make example step ( Rule and log below) i allready have an open Issue on the Workflow but we are at the Point that we think its ether Nucleus or Tensorflow that produces the error PacificBiosciences/HiFiTargetEnrichment#4 , since i cant find what the error is and how to fix it i opend the Issue. Many thanks in advance. **Setup**. - Operating system: Ubuntu 20.04.6 LTS. - DeepVariant version: 1.5.0. - Tensorflow 2.11.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: PacBio HIFI reads. **Steps to reproduce:**. ```. rule deepvariant_make_examples:. input:. bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",. bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",. reference=config[""ref""][""fasta""],. output:. tfrecord=temp(. f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz"". ),. nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>. log:. f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",. benchmark:. f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv"". container:. f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channels ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:1007,integrability,batch,batch,1007,"or: PythonNext() argument read is not valid: Dynamic cast failed; **Describe the issue:**. Hello everyone, i am trying to run a Pacbio Workflow with deepvariant in it but i get an error in the make example step ( Rule and log below) i allready have an open Issue on the Workflow but we are at the Point that we think its ether Nucleus or Tensorflow that produces the error PacificBiosciences/HiFiTargetEnrichment#4 , since i cant find what the error is and how to fix it i opend the Issue. Many thanks in advance. **Setup**. - Operating system: Ubuntu 20.04.6 LTS. - DeepVariant version: 1.5.0. - Tensorflow 2.11.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: PacBio HIFI reads. **Steps to reproduce:**. ```. rule deepvariant_make_examples:. input:. bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",. bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",. reference=config[""ref""][""fasta""],. output:. tfrecord=temp(. f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz"". ),. nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>. log:. f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",. benchmark:. f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv"". container:. f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channels \. --min",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:1137,integrability,batch,batches,1137,"bio Workflow with deepvariant in it but i get an error in the make example step ( Rule and log below) i allready have an open Issue on the Workflow but we are at the Point that we think its ether Nucleus or Tensorflow that produces the error PacificBiosciences/HiFiTargetEnrichment#4 , since i cant find what the error is and how to fix it i opend the Issue. Many thanks in advance. **Setup**. - Operating system: Ubuntu 20.04.6 LTS. - DeepVariant version: 1.5.0. - Tensorflow 2.11.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: PacBio HIFI reads. **Steps to reproduce:**. ```. rule deepvariant_make_examples:. input:. bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",. bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",. reference=config[""ref""][""fasta""],. output:. tfrecord=temp(. f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz"". ),. nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>. log:. f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",. benchmark:. f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv"". container:. f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channels \. --min_mapping_quality=1 \. --parse_sam_aux_fields \. --partition_size=25000 \. --max_reads_per_partition=600 \. --phase_reads \. --pileu",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:1146,integrability,batch,batch,1146,"flow with deepvariant in it but i get an error in the make example step ( Rule and log below) i allready have an open Issue on the Workflow but we are at the Point that we think its ether Nucleus or Tensorflow that produces the error PacificBiosciences/HiFiTargetEnrichment#4 , since i cant find what the error is and how to fix it i opend the Issue. Many thanks in advance. **Setup**. - Operating system: Ubuntu 20.04.6 LTS. - DeepVariant version: 1.5.0. - Tensorflow 2.11.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: PacBio HIFI reads. **Steps to reproduce:**. ```. rule deepvariant_make_examples:. input:. bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",. bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",. reference=config[""ref""][""fasta""],. output:. tfrecord=temp(. f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz"". ),. nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>. log:. f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",. benchmark:. f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv"". container:. f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channels \. --min_mapping_quality=1 \. --parse_sam_aux_fields \. --partition_size=25000 \. --max_reads_per_partition=600 \. --phase_reads \. --pileup_image_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:1244,integrability,batch,batches,1244,"ready have an open Issue on the Workflow but we are at the Point that we think its ether Nucleus or Tensorflow that produces the error PacificBiosciences/HiFiTargetEnrichment#4 , since i cant find what the error is and how to fix it i opend the Issue. Many thanks in advance. **Setup**. - Operating system: Ubuntu 20.04.6 LTS. - DeepVariant version: 1.5.0. - Tensorflow 2.11.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: PacBio HIFI reads. **Steps to reproduce:**. ```. rule deepvariant_make_examples:. input:. bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",. bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",. reference=config[""ref""][""fasta""],. output:. tfrecord=temp(. f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz"". ),. nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>. log:. f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",. benchmark:. f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv"". container:. f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channels \. --min_mapping_quality=1 \. --parse_sam_aux_fields \. --partition_size=25000 \. --max_reads_per_partition=600 \. --phase_reads \. --pileup_image_width {params.pileup_image_width} \. --norealign_reads \. --sort_by_haplotypes \. --track_ref_reads",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:1253,integrability,batch,batch,1253,"ve an open Issue on the Workflow but we are at the Point that we think its ether Nucleus or Tensorflow that produces the error PacificBiosciences/HiFiTargetEnrichment#4 , since i cant find what the error is and how to fix it i opend the Issue. Many thanks in advance. **Setup**. - Operating system: Ubuntu 20.04.6 LTS. - DeepVariant version: 1.5.0. - Tensorflow 2.11.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: PacBio HIFI reads. **Steps to reproduce:**. ```. rule deepvariant_make_examples:. input:. bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",. bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",. reference=config[""ref""][""fasta""],. output:. tfrecord=temp(. f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz"". ),. nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>. log:. f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",. benchmark:. f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv"". container:. f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channels \. --min_mapping_quality=1 \. --parse_sam_aux_fields \. --partition_size=25000 \. --max_reads_per_partition=600 \. --phase_reads \. --pileup_image_width {params.pileup_image_width} \. --norealign_reads \. --sort_by_haplotypes \. --track_ref_reads \. --vs",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:1366,integrability,batch,batches,1366,"es the error PacificBiosciences/HiFiTargetEnrichment#4 , since i cant find what the error is and how to fix it i opend the Issue. Many thanks in advance. **Setup**. - Operating system: Ubuntu 20.04.6 LTS. - DeepVariant version: 1.5.0. - Tensorflow 2.11.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: PacBio HIFI reads. **Steps to reproduce:**. ```. rule deepvariant_make_examples:. input:. bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",. bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",. reference=config[""ref""][""fasta""],. output:. tfrecord=temp(. f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz"". ),. nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>. log:. f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",. benchmark:. f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv"". container:. f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channels \. --min_mapping_quality=1 \. --parse_sam_aux_fields \. --partition_size=25000 \. --max_reads_per_partition=600 \. --phase_reads \. --pileup_image_width {params.pileup_image_width} \. --norealign_reads \. --sort_by_haplotypes \. --track_ref_reads \. --vsc_min_fraction_indels {params.vsc_min_fraction_indels} \. --mode calling \. --ref {input.reference} \. --reads {in",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:1375,integrability,batch,batch,1375,"rror PacificBiosciences/HiFiTargetEnrichment#4 , since i cant find what the error is and how to fix it i opend the Issue. Many thanks in advance. **Setup**. - Operating system: Ubuntu 20.04.6 LTS. - DeepVariant version: 1.5.0. - Tensorflow 2.11.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: PacBio HIFI reads. **Steps to reproduce:**. ```. rule deepvariant_make_examples:. input:. bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",. bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",. reference=config[""ref""][""fasta""],. output:. tfrecord=temp(. f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz"". ),. nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>. log:. f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",. benchmark:. f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv"". container:. f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channels \. --min_mapping_quality=1 \. --parse_sam_aux_fields \. --partition_size=25000 \. --max_reads_per_partition=600 \. --phase_reads \. --pileup_image_width {params.pileup_image_width} \. --norealign_reads \. --sort_by_haplotypes \. --track_ref_reads \. --vsc_min_fraction_indels {params.vsc_min_fraction_indels} \. --mode calling \. --ref {input.reference} \. --reads {input.bam}",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:1621,integrability,batch,batches,1621," - Installation method (Docker, built from source, etc.): singularity. - Type of data: PacBio HIFI reads. **Steps to reproduce:**. ```. rule deepvariant_make_examples:. input:. bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",. bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",. reference=config[""ref""][""fasta""],. output:. tfrecord=temp(. f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz"". ),. nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>. log:. f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",. benchmark:. f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv"". container:. f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channels \. --min_mapping_quality=1 \. --parse_sam_aux_fields \. --partition_size=25000 \. --max_reads_per_partition=600 \. --phase_reads \. --pileup_image_width {params.pileup_image_width} \. --norealign_reads \. --sort_by_haplotypes \. --track_ref_reads \. --vsc_min_fraction_indels {params.vsc_min_fraction_indels} \. --mode calling \. --ref {input.reference} \. --reads {input.bam} \. --examples {params.examples} \. --gvcf {params.gvcf} \. --task {params.shard}) > {log} 2>&1. """""". ```. Error:. ```. 2023-07-12 15:19:55.926661: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAP",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:1630,integrability,batch,batch,1630,"llation method (Docker, built from source, etc.): singularity. - Type of data: PacBio HIFI reads. **Steps to reproduce:**. ```. rule deepvariant_make_examples:. input:. bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",. bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",. reference=config[""ref""][""fasta""],. output:. tfrecord=temp(. f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz"". ),. nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>. log:. f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",. benchmark:. f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv"". container:. f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channels \. --min_mapping_quality=1 \. --parse_sam_aux_fields \. --partition_size=25000 \. --max_reads_per_partition=600 \. --phase_reads \. --pileup_image_width {params.pileup_image_width} \. --norealign_reads \. --sort_by_haplotypes \. --track_ref_reads \. --vsc_min_fraction_indels {params.vsc_min_fraction_indels} \. --mode calling \. --ref {input.reference} \. --reads {input.bam} \. --examples {params.examples} \. --gvcf {params.gvcf} \. --task {params.shard}) > {log} 2>&1. """""". ```. Error:. ```. 2023-07-12 15:19:55.926661: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI De>. T",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:1721,integrability,batch,batches,1721,"eads. **Steps to reproduce:**. ```. rule deepvariant_make_examples:. input:. bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",. bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",. reference=config[""ref""][""fasta""],. output:. tfrecord=temp(. f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz"". ),. nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>. log:. f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",. benchmark:. f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv"". container:. f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channels \. --min_mapping_quality=1 \. --parse_sam_aux_fields \. --partition_size=25000 \. --max_reads_per_partition=600 \. --phase_reads \. --pileup_image_width {params.pileup_image_width} \. --norealign_reads \. --sort_by_haplotypes \. --track_ref_reads \. --vsc_min_fraction_indels {params.vsc_min_fraction_indels} \. --mode calling \. --ref {input.reference} \. --reads {input.bam} \. --examples {params.examples} \. --gvcf {params.gvcf} \. --task {params.shard}) > {log} 2>&1. """""". ```. Error:. ```. 2023-07-12 15:19:55.926661: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI De>. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:1730,integrability,batch,batch,1730,"Steps to reproduce:**. ```. rule deepvariant_make_examples:. input:. bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",. bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",. reference=config[""ref""][""fasta""],. output:. tfrecord=temp(. f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz"". ),. nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>. log:. f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",. benchmark:. f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv"". container:. f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channels \. --min_mapping_quality=1 \. --parse_sam_aux_fields \. --partition_size=25000 \. --max_reads_per_partition=600 \. --phase_reads \. --pileup_image_width {params.pileup_image_width} \. --norealign_reads \. --sort_by_haplotypes \. --track_ref_reads \. --vsc_min_fraction_indels {params.vsc_min_fraction_indels} \. --mode calling \. --ref {input.reference} \. --reads {input.bam} \. --examples {params.examples} \. --gvcf {params.gvcf} \. --task {params.shard}) > {log} 2>&1. """""". ```. Error:. ```. 2023-07-12 15:19:55.926661: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI De>. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-07-1",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:1810,integrability,messag,message,1810,"s/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",. bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",. reference=config[""ref""][""fasta""],. output:. tfrecord=temp(. f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz"". ),. nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>. log:. f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",. benchmark:. f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv"". container:. f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channels \. --min_mapping_quality=1 \. --parse_sam_aux_fields \. --partition_size=25000 \. --max_reads_per_partition=600 \. --phase_reads \. --pileup_image_width {params.pileup_image_width} \. --norealign_reads \. --sort_by_haplotypes \. --track_ref_reads \. --vsc_min_fraction_indels {params.vsc_min_fraction_indels} \. --mode calling \. --ref {input.reference} \. --reads {input.bam} \. --examples {params.examples} \. --gvcf {params.gvcf} \. --task {params.shard}) > {log} 2>&1. """""". ```. Error:. ```. 2023-07-12 15:19:55.926661: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI De>. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-07-12 15:19:59.038994: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:3181,integrability,batch,batches,3181," --norealign_reads \. --sort_by_haplotypes \. --track_ref_reads \. --vsc_min_fraction_indels {params.vsc_min_fraction_indels} \. --mode calling \. --ref {input.reference} \. --reads {input.bam} \. --examples {params.examples} \. --gvcf {params.gvcf} \. --task {params.shard}) > {log} 2>&1. """""". ```. Error:. ```. 2023-07-12 15:19:55.926661: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI De>. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-07-12 15:19:59.038994: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.6 SO:>. 2023-07-12 15:19:59.039047: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag BC: in RG line, ignoring: @RG ID:d98f52ac>. 2023-07-12 15:19:59.039065: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG ID:d98f52ac>. I0712 15:19:59.039340 140472429422400 genomics_reader.py:222] Reading batches/Test349/D18757/aligned/D18757.hs37d5.bam with NativeS>. I0712 15:19:59.044630 140472429422400 make_examples_core.py:257] Task 32/96: Preparing inputs. 2023-07-12 15:19:59.049074: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.6 SO:>. 2023-07-12 15:19:59.049123: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag BC: in RG line, ignoring: @RG ID:d98f52ac>. 2023-07-12 15:19:59.049140: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG ID:d98f52ac>. I0712 15:19:59.049325 140472429422400 genomics_reader.py:222] Reading batches/Test349/D18757/aligned/D18757.hs37d5.bam with NativeS>. I0712 15:19:59.059888 140472429422400 make_examples_core.py:257] Task 32/96: Common contigs are ['1', '2', '3', '4', '5', '6', '7',>. I0712 15:19:59.247122 140472429422400 make_examples_core.py:257] Task 32/96: Starting from v0.9.0, --use_ref_for_cram is default to>. 2023-07-12 15:19:59.247444: I third_party/nucleus/io/sam",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:3797,integrability,batch,batches,3797,"wn tag pb: in header line, ignoring: @HD VN:1.6 SO:>. 2023-07-12 15:19:59.039047: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag BC: in RG line, ignoring: @RG ID:d98f52ac>. 2023-07-12 15:19:59.039065: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG ID:d98f52ac>. I0712 15:19:59.039340 140472429422400 genomics_reader.py:222] Reading batches/Test349/D18757/aligned/D18757.hs37d5.bam with NativeS>. I0712 15:19:59.044630 140472429422400 make_examples_core.py:257] Task 32/96: Preparing inputs. 2023-07-12 15:19:59.049074: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.6 SO:>. 2023-07-12 15:19:59.049123: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag BC: in RG line, ignoring: @RG ID:d98f52ac>. 2023-07-12 15:19:59.049140: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG ID:d98f52ac>. I0712 15:19:59.049325 140472429422400 genomics_reader.py:222] Reading batches/Test349/D18757/aligned/D18757.hs37d5.bam with NativeS>. I0712 15:19:59.059888 140472429422400 make_examples_core.py:257] Task 32/96: Common contigs are ['1', '2', '3', '4', '5', '6', '7',>. I0712 15:19:59.247122 140472429422400 make_examples_core.py:257] Task 32/96: Starting from v0.9.0, --use_ref_for_cram is default to>. 2023-07-12 15:19:59.247444: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2023-07-12 15:19:59.364871: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.6 SO:>. 2023-07-12 15:19:59.364924: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag BC: in RG line, ignoring: @RG ID:d98f52ac>. 2023-07-12 15:19:59.364944: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG ID:d98f52ac>. I0712 15:19:59.365195 140472429422400 genomics_reader.py:222] Reading batches/Test349/D18757/aligned/D18757.hs37d5.bam with NativeS>. 2023-07-12 15:19:59.535088: W third_pa",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:4699,integrability,batch,batches,4699,"ring: @RG ID:d98f52ac>. I0712 15:19:59.049325 140472429422400 genomics_reader.py:222] Reading batches/Test349/D18757/aligned/D18757.hs37d5.bam with NativeS>. I0712 15:19:59.059888 140472429422400 make_examples_core.py:257] Task 32/96: Common contigs are ['1', '2', '3', '4', '5', '6', '7',>. I0712 15:19:59.247122 140472429422400 make_examples_core.py:257] Task 32/96: Starting from v0.9.0, --use_ref_for_cram is default to>. 2023-07-12 15:19:59.247444: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2023-07-12 15:19:59.364871: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.6 SO:>. 2023-07-12 15:19:59.364924: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag BC: in RG line, ignoring: @RG ID:d98f52ac>. 2023-07-12 15:19:59.364944: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG ID:d98f52ac>. I0712 15:19:59.365195 140472429422400 genomics_reader.py:222] Reading batches/Test349/D18757/aligned/D18757.hs37d5.bam with NativeS>. 2023-07-12 15:19:59.535088: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.6 SO:>. 2023-07-12 15:19:59.535146: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag BC: in RG line, ignoring: @RG ID:d98f52ac>. 2023-07-12 15:19:59.535166: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG ID:d98f52ac>. I0712 15:19:59.535452 140472429422400 genomics_reader.py:222] Reading batches/Test349/D18757/aligned/D18757.hs37d5.bam with NativeS>. I0712 15:19:59.536432 140472429422400 make_examples_core.py:257] Task 32/96: Writing gvcf records to batches/Test349/D18757/deepvar>. I0712 15:19:59.536834 140472429422400 make_examples_core.py:257] Task 32/96: Writing examples to batches/Test349/D18757/deepvariant>. I0712 15:19:59.536924 140472429422400 make_examples_core.py:257] Task 32/96: Overhead for preparing inputs: 0 seconds. 2023-07-12 15:19:59.545470: W ./",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:5220,integrability,batch,batches,5220,"IZE to 134217728. 2023-07-12 15:19:59.364871: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.6 SO:>. 2023-07-12 15:19:59.364924: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag BC: in RG line, ignoring: @RG ID:d98f52ac>. 2023-07-12 15:19:59.364944: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG ID:d98f52ac>. I0712 15:19:59.365195 140472429422400 genomics_reader.py:222] Reading batches/Test349/D18757/aligned/D18757.hs37d5.bam with NativeS>. 2023-07-12 15:19:59.535088: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.6 SO:>. 2023-07-12 15:19:59.535146: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag BC: in RG line, ignoring: @RG ID:d98f52ac>. 2023-07-12 15:19:59.535166: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG ID:d98f52ac>. I0712 15:19:59.535452 140472429422400 genomics_reader.py:222] Reading batches/Test349/D18757/aligned/D18757.hs37d5.bam with NativeS>. I0712 15:19:59.536432 140472429422400 make_examples_core.py:257] Task 32/96: Writing gvcf records to batches/Test349/D18757/deepvar>. I0712 15:19:59.536834 140472429422400 make_examples_core.py:257] Task 32/96: Writing examples to batches/Test349/D18757/deepvariant>. I0712 15:19:59.536924 140472429422400 make_examples_core.py:257] Task 32/96: Overhead for preparing inputs: 0 seconds. 2023-07-12 15:19:59.545470: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicM>. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_ch",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:5385,integrability,batch,batches,5385,":59.364924: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag BC: in RG line, ignoring: @RG ID:d98f52ac>. 2023-07-12 15:19:59.364944: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG ID:d98f52ac>. I0712 15:19:59.365195 140472429422400 genomics_reader.py:222] Reading batches/Test349/D18757/aligned/D18757.hs37d5.bam with NativeS>. 2023-07-12 15:19:59.535088: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.6 SO:>. 2023-07-12 15:19:59.535146: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag BC: in RG line, ignoring: @RG ID:d98f52ac>. 2023-07-12 15:19:59.535166: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG ID:d98f52ac>. I0712 15:19:59.535452 140472429422400 genomics_reader.py:222] Reading batches/Test349/D18757/aligned/D18757.hs37d5.bam with NativeS>. I0712 15:19:59.536432 140472429422400 make_examples_core.py:257] Task 32/96: Writing gvcf records to batches/Test349/D18757/deepvar>. I0712 15:19:59.536834 140472429422400 make_examples_core.py:257] Task 32/96: Writing examples to batches/Test349/D18757/deepvariant>. I0712 15:19:59.536924 140472429422400 make_examples_core.py:257] Task 32/96: Overhead for preparing inputs: 0 seconds. 2023-07-12 15:19:59.545470: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicM>. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:5515,integrability,batch,batches,5515,"9.364944: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG ID:d98f52ac>. I0712 15:19:59.365195 140472429422400 genomics_reader.py:222] Reading batches/Test349/D18757/aligned/D18757.hs37d5.bam with NativeS>. 2023-07-12 15:19:59.535088: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.6 SO:>. 2023-07-12 15:19:59.535146: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag BC: in RG line, ignoring: @RG ID:d98f52ac>. 2023-07-12 15:19:59.535166: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG ID:d98f52ac>. I0712 15:19:59.535452 140472429422400 genomics_reader.py:222] Reading batches/Test349/D18757/aligned/D18757.hs37d5.bam with NativeS>. I0712 15:19:59.536432 140472429422400 make_examples_core.py:257] Task 32/96: Writing gvcf records to batches/Test349/D18757/deepvar>. I0712 15:19:59.536834 140472429422400 make_examples_core.py:257] Task 32/96: Writing examples to batches/Test349/D18757/deepvariant>. I0712 15:19:59.536924 140472429422400 make_examples_core.py:257] Task 32/96: Overhead for preparing inputs: 0 seconds. 2023-07-12 15:19:59.545470: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicM>. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2183, in make_example>. runtimes) = region_proce",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:1810,interoperability,messag,message,1810,"s/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",. bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",. reference=config[""ref""][""fasta""],. output:. tfrecord=temp(. f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz"". ),. nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>. log:. f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",. benchmark:. f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv"". container:. f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channels \. --min_mapping_quality=1 \. --parse_sam_aux_fields \. --partition_size=25000 \. --max_reads_per_partition=600 \. --phase_reads \. --pileup_image_width {params.pileup_image_width} \. --norealign_reads \. --sort_by_haplotypes \. --track_ref_reads \. --vsc_min_fraction_indels {params.vsc_min_fraction_indels} \. --mode calling \. --ref {input.reference} \. --reads {input.bam} \. --examples {params.examples} \. --gvcf {params.gvcf} \. --task {params.shard}) > {log} 2>&1. """""". ```. Error:. ```. 2023-07-12 15:19:55.926661: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI De>. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-07-12 15:19:59.038994: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:2544,interoperability,platform,platform,2544,"on_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channels \. --min_mapping_quality=1 \. --parse_sam_aux_fields \. --partition_size=25000 \. --max_reads_per_partition=600 \. --phase_reads \. --pileup_image_width {params.pileup_image_width} \. --norealign_reads \. --sort_by_haplotypes \. --track_ref_reads \. --vsc_min_fraction_indels {params.vsc_min_fraction_indels} \. --mode calling \. --ref {input.reference} \. --reads {input.bam} \. --examples {params.examples} \. --gvcf {params.gvcf} \. --task {params.shard}) > {log} 2>&1. """""". ```. Error:. ```. 2023-07-12 15:19:55.926661: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI De>. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-07-12 15:19:59.038994: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.6 SO:>. 2023-07-12 15:19:59.039047: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag BC: in RG line, ignoring: @RG ID:d98f52ac>. 2023-07-12 15:19:59.039065: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG ID:d98f52ac>. I0712 15:19:59.039340 140472429422400 genomics_reader.py:222] Reading batches/Test349/D18757/aligned/D18757.hs37d5.bam with NativeS>. I0712 15:19:59.044630 140472429422400 make_examples_core.py:257] Task 32/96: Preparing inputs. 2023-07-12 15:19:59.049074: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.6 SO:>. 2023-07-12 15:19:59.049123: W third_party/nucleus/io/sam_reader.cc:174] Unkno",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:138,modifiability,Pac,Pacbio,138,"RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed; **Describe the issue:**. Hello everyone, i am trying to run a Pacbio Workflow with deepvariant in it but i get an error in the make example step ( Rule and log below) i allready have an open Issue on the Workflow but we are at the Point that we think its ether Nucleus or Tensorflow that produces the error PacificBiosciences/HiFiTargetEnrichment#4 , since i cant find what the error is and how to fix it i opend the Issue. Many thanks in advance. **Setup**. - Operating system: Ubuntu 20.04.6 LTS. - DeepVariant version: 1.5.0. - Tensorflow 2.11.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: PacBio HIFI reads. **Steps to reproduce:**. ```. rule deepvariant_make_examples:. input:. bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",. bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",. reference=config[""ref""][""fasta""],. output:. tfrecord=temp(. f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz"". ),. nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>. log:. f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",. benchmark:. f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv"". container:. f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channel",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:383,modifiability,Pac,PacificBiosciences,383,"RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed; **Describe the issue:**. Hello everyone, i am trying to run a Pacbio Workflow with deepvariant in it but i get an error in the make example step ( Rule and log below) i allready have an open Issue on the Workflow but we are at the Point that we think its ether Nucleus or Tensorflow that produces the error PacificBiosciences/HiFiTargetEnrichment#4 , since i cant find what the error is and how to fix it i opend the Issue. Many thanks in advance. **Setup**. - Operating system: Ubuntu 20.04.6 LTS. - DeepVariant version: 1.5.0. - Tensorflow 2.11.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: PacBio HIFI reads. **Steps to reproduce:**. ```. rule deepvariant_make_examples:. input:. bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",. bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",. reference=config[""ref""][""fasta""],. output:. tfrecord=temp(. f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz"". ),. nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>. log:. f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",. benchmark:. f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv"". container:. f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channel",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:589,modifiability,version,version,589,"RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed; **Describe the issue:**. Hello everyone, i am trying to run a Pacbio Workflow with deepvariant in it but i get an error in the make example step ( Rule and log below) i allready have an open Issue on the Workflow but we are at the Point that we think its ether Nucleus or Tensorflow that produces the error PacificBiosciences/HiFiTargetEnrichment#4 , since i cant find what the error is and how to fix it i opend the Issue. Many thanks in advance. **Setup**. - Operating system: Ubuntu 20.04.6 LTS. - DeepVariant version: 1.5.0. - Tensorflow 2.11.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: PacBio HIFI reads. **Steps to reproduce:**. ```. rule deepvariant_make_examples:. input:. bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",. bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",. reference=config[""ref""][""fasta""],. output:. tfrecord=temp(. f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz"". ),. nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>. log:. f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",. benchmark:. f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv"". container:. f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channel",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:712,modifiability,Pac,PacBio,712,"RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed; **Describe the issue:**. Hello everyone, i am trying to run a Pacbio Workflow with deepvariant in it but i get an error in the make example step ( Rule and log below) i allready have an open Issue on the Workflow but we are at the Point that we think its ether Nucleus or Tensorflow that produces the error PacificBiosciences/HiFiTargetEnrichment#4 , since i cant find what the error is and how to fix it i opend the Issue. Many thanks in advance. **Setup**. - Operating system: Ubuntu 20.04.6 LTS. - DeepVariant version: 1.5.0. - Tensorflow 2.11.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: PacBio HIFI reads. **Steps to reproduce:**. ```. rule deepvariant_make_examples:. input:. bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",. bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",. reference=config[""ref""][""fasta""],. output:. tfrecord=temp(. f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz"". ),. nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>. log:. f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",. benchmark:. f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv"". container:. f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channel",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:5953,modifiability,modul,module,5953,"r.cc:174] Unknown tag BC: in RG line, ignoring: @RG ID:d98f52ac>. 2023-07-12 15:19:59.535166: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG ID:d98f52ac>. I0712 15:19:59.535452 140472429422400 genomics_reader.py:222] Reading batches/Test349/D18757/aligned/D18757.hs37d5.bam with NativeS>. I0712 15:19:59.536432 140472429422400 make_examples_core.py:257] Task 32/96: Writing gvcf records to batches/Test349/D18757/deepvar>. I0712 15:19:59.536834 140472429422400 make_examples_core.py:257] Task 32/96: Writing examples to batches/Test349/D18757/deepvariant>. I0712 15:19:59.536924 140472429422400 make_examples_core.py:257] Task 32/96: Overhead for preparing inputs: 0 seconds. 2023-07-12 15:19:59.545470: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicM>. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2183, in make_example>. runtimes) = region_processor.process(region). File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1285, in process. sample_reads = self.region_reads_norealign(. File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1381, in region_reads>. reads = utils.reservoir_sample(reads,. File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/com_google_deepvariant/third_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:190,performance,error,error,190,"RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed; **Describe the issue:**. Hello everyone, i am trying to run a Pacbio Workflow with deepvariant in it but i get an error in the make example step ( Rule and log below) i allready have an open Issue on the Workflow but we are at the Point that we think its ether Nucleus or Tensorflow that produces the error PacificBiosciences/HiFiTargetEnrichment#4 , since i cant find what the error is and how to fix it i opend the Issue. Many thanks in advance. **Setup**. - Operating system: Ubuntu 20.04.6 LTS. - DeepVariant version: 1.5.0. - Tensorflow 2.11.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: PacBio HIFI reads. **Steps to reproduce:**. ```. rule deepvariant_make_examples:. input:. bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",. bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",. reference=config[""ref""][""fasta""],. output:. tfrecord=temp(. f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz"". ),. nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>. log:. f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",. benchmark:. f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv"". container:. f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channel",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:377,performance,error,error,377,"RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed; **Describe the issue:**. Hello everyone, i am trying to run a Pacbio Workflow with deepvariant in it but i get an error in the make example step ( Rule and log below) i allready have an open Issue on the Workflow but we are at the Point that we think its ether Nucleus or Tensorflow that produces the error PacificBiosciences/HiFiTargetEnrichment#4 , since i cant find what the error is and how to fix it i opend the Issue. Many thanks in advance. **Setup**. - Operating system: Ubuntu 20.04.6 LTS. - DeepVariant version: 1.5.0. - Tensorflow 2.11.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: PacBio HIFI reads. **Steps to reproduce:**. ```. rule deepvariant_make_examples:. input:. bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",. bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",. reference=config[""ref""][""fasta""],. output:. tfrecord=temp(. f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz"". ),. nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>. log:. f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",. benchmark:. f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv"". container:. f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channel",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:454,performance,error,error,454,"RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed; **Describe the issue:**. Hello everyone, i am trying to run a Pacbio Workflow with deepvariant in it but i get an error in the make example step ( Rule and log below) i allready have an open Issue on the Workflow but we are at the Point that we think its ether Nucleus or Tensorflow that produces the error PacificBiosciences/HiFiTargetEnrichment#4 , since i cant find what the error is and how to fix it i opend the Issue. Many thanks in advance. **Setup**. - Operating system: Ubuntu 20.04.6 LTS. - DeepVariant version: 1.5.0. - Tensorflow 2.11.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: PacBio HIFI reads. **Steps to reproduce:**. ```. rule deepvariant_make_examples:. input:. bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",. bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",. reference=config[""ref""][""fasta""],. output:. tfrecord=temp(. f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz"". ),. nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>. log:. f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",. benchmark:. f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv"". container:. f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channel",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:808,performance,batch,batches,808,"RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed; **Describe the issue:**. Hello everyone, i am trying to run a Pacbio Workflow with deepvariant in it but i get an error in the make example step ( Rule and log below) i allready have an open Issue on the Workflow but we are at the Point that we think its ether Nucleus or Tensorflow that produces the error PacificBiosciences/HiFiTargetEnrichment#4 , since i cant find what the error is and how to fix it i opend the Issue. Many thanks in advance. **Setup**. - Operating system: Ubuntu 20.04.6 LTS. - DeepVariant version: 1.5.0. - Tensorflow 2.11.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: PacBio HIFI reads. **Steps to reproduce:**. ```. rule deepvariant_make_examples:. input:. bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",. bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",. reference=config[""ref""][""fasta""],. output:. tfrecord=temp(. f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz"". ),. nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>. log:. f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",. benchmark:. f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv"". container:. f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channel",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:817,performance,batch,batch,817,"RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed; **Describe the issue:**. Hello everyone, i am trying to run a Pacbio Workflow with deepvariant in it but i get an error in the make example step ( Rule and log below) i allready have an open Issue on the Workflow but we are at the Point that we think its ether Nucleus or Tensorflow that produces the error PacificBiosciences/HiFiTargetEnrichment#4 , since i cant find what the error is and how to fix it i opend the Issue. Many thanks in advance. **Setup**. - Operating system: Ubuntu 20.04.6 LTS. - DeepVariant version: 1.5.0. - Tensorflow 2.11.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: PacBio HIFI reads. **Steps to reproduce:**. ```. rule deepvariant_make_examples:. input:. bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",. bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",. reference=config[""ref""][""fasta""],. output:. tfrecord=temp(. f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz"". ),. nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>. log:. f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",. benchmark:. f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv"". container:. f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channel",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:873,performance,batch,batches,873,"RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed; **Describe the issue:**. Hello everyone, i am trying to run a Pacbio Workflow with deepvariant in it but i get an error in the make example step ( Rule and log below) i allready have an open Issue on the Workflow but we are at the Point that we think its ether Nucleus or Tensorflow that produces the error PacificBiosciences/HiFiTargetEnrichment#4 , since i cant find what the error is and how to fix it i opend the Issue. Many thanks in advance. **Setup**. - Operating system: Ubuntu 20.04.6 LTS. - DeepVariant version: 1.5.0. - Tensorflow 2.11.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: PacBio HIFI reads. **Steps to reproduce:**. ```. rule deepvariant_make_examples:. input:. bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",. bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",. reference=config[""ref""][""fasta""],. output:. tfrecord=temp(. f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz"". ),. nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>. log:. f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",. benchmark:. f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv"". container:. f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channel",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:882,performance,batch,batch,882,"RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed; **Describe the issue:**. Hello everyone, i am trying to run a Pacbio Workflow with deepvariant in it but i get an error in the make example step ( Rule and log below) i allready have an open Issue on the Workflow but we are at the Point that we think its ether Nucleus or Tensorflow that produces the error PacificBiosciences/HiFiTargetEnrichment#4 , since i cant find what the error is and how to fix it i opend the Issue. Many thanks in advance. **Setup**. - Operating system: Ubuntu 20.04.6 LTS. - DeepVariant version: 1.5.0. - Tensorflow 2.11.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: PacBio HIFI reads. **Steps to reproduce:**. ```. rule deepvariant_make_examples:. input:. bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",. bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",. reference=config[""ref""][""fasta""],. output:. tfrecord=temp(. f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz"". ),. nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>. log:. f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",. benchmark:. f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv"". container:. f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channel",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:998,performance,batch,batches,998,"ntimeError: PythonNext() argument read is not valid: Dynamic cast failed; **Describe the issue:**. Hello everyone, i am trying to run a Pacbio Workflow with deepvariant in it but i get an error in the make example step ( Rule and log below) i allready have an open Issue on the Workflow but we are at the Point that we think its ether Nucleus or Tensorflow that produces the error PacificBiosciences/HiFiTargetEnrichment#4 , since i cant find what the error is and how to fix it i opend the Issue. Many thanks in advance. **Setup**. - Operating system: Ubuntu 20.04.6 LTS. - DeepVariant version: 1.5.0. - Tensorflow 2.11.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: PacBio HIFI reads. **Steps to reproduce:**. ```. rule deepvariant_make_examples:. input:. bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",. bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",. reference=config[""ref""][""fasta""],. output:. tfrecord=temp(. f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz"". ),. nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>. log:. f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",. benchmark:. f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv"". container:. f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channels ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:1007,performance,batch,batch,1007,"or: PythonNext() argument read is not valid: Dynamic cast failed; **Describe the issue:**. Hello everyone, i am trying to run a Pacbio Workflow with deepvariant in it but i get an error in the make example step ( Rule and log below) i allready have an open Issue on the Workflow but we are at the Point that we think its ether Nucleus or Tensorflow that produces the error PacificBiosciences/HiFiTargetEnrichment#4 , since i cant find what the error is and how to fix it i opend the Issue. Many thanks in advance. **Setup**. - Operating system: Ubuntu 20.04.6 LTS. - DeepVariant version: 1.5.0. - Tensorflow 2.11.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: PacBio HIFI reads. **Steps to reproduce:**. ```. rule deepvariant_make_examples:. input:. bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",. bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",. reference=config[""ref""][""fasta""],. output:. tfrecord=temp(. f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz"". ),. nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>. log:. f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",. benchmark:. f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv"". container:. f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channels \. --min",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:1137,performance,batch,batches,1137,"bio Workflow with deepvariant in it but i get an error in the make example step ( Rule and log below) i allready have an open Issue on the Workflow but we are at the Point that we think its ether Nucleus or Tensorflow that produces the error PacificBiosciences/HiFiTargetEnrichment#4 , since i cant find what the error is and how to fix it i opend the Issue. Many thanks in advance. **Setup**. - Operating system: Ubuntu 20.04.6 LTS. - DeepVariant version: 1.5.0. - Tensorflow 2.11.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: PacBio HIFI reads. **Steps to reproduce:**. ```. rule deepvariant_make_examples:. input:. bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",. bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",. reference=config[""ref""][""fasta""],. output:. tfrecord=temp(. f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz"". ),. nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>. log:. f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",. benchmark:. f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv"". container:. f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channels \. --min_mapping_quality=1 \. --parse_sam_aux_fields \. --partition_size=25000 \. --max_reads_per_partition=600 \. --phase_reads \. --pileu",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:1146,performance,batch,batch,1146,"flow with deepvariant in it but i get an error in the make example step ( Rule and log below) i allready have an open Issue on the Workflow but we are at the Point that we think its ether Nucleus or Tensorflow that produces the error PacificBiosciences/HiFiTargetEnrichment#4 , since i cant find what the error is and how to fix it i opend the Issue. Many thanks in advance. **Setup**. - Operating system: Ubuntu 20.04.6 LTS. - DeepVariant version: 1.5.0. - Tensorflow 2.11.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: PacBio HIFI reads. **Steps to reproduce:**. ```. rule deepvariant_make_examples:. input:. bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",. bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",. reference=config[""ref""][""fasta""],. output:. tfrecord=temp(. f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz"". ),. nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>. log:. f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",. benchmark:. f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv"". container:. f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channels \. --min_mapping_quality=1 \. --parse_sam_aux_fields \. --partition_size=25000 \. --max_reads_per_partition=600 \. --phase_reads \. --pileup_image_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:1244,performance,batch,batches,1244,"ready have an open Issue on the Workflow but we are at the Point that we think its ether Nucleus or Tensorflow that produces the error PacificBiosciences/HiFiTargetEnrichment#4 , since i cant find what the error is and how to fix it i opend the Issue. Many thanks in advance. **Setup**. - Operating system: Ubuntu 20.04.6 LTS. - DeepVariant version: 1.5.0. - Tensorflow 2.11.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: PacBio HIFI reads. **Steps to reproduce:**. ```. rule deepvariant_make_examples:. input:. bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",. bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",. reference=config[""ref""][""fasta""],. output:. tfrecord=temp(. f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz"". ),. nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>. log:. f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",. benchmark:. f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv"". container:. f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channels \. --min_mapping_quality=1 \. --parse_sam_aux_fields \. --partition_size=25000 \. --max_reads_per_partition=600 \. --phase_reads \. --pileup_image_width {params.pileup_image_width} \. --norealign_reads \. --sort_by_haplotypes \. --track_ref_reads",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:1253,performance,batch,batch,1253,"ve an open Issue on the Workflow but we are at the Point that we think its ether Nucleus or Tensorflow that produces the error PacificBiosciences/HiFiTargetEnrichment#4 , since i cant find what the error is and how to fix it i opend the Issue. Many thanks in advance. **Setup**. - Operating system: Ubuntu 20.04.6 LTS. - DeepVariant version: 1.5.0. - Tensorflow 2.11.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: PacBio HIFI reads. **Steps to reproduce:**. ```. rule deepvariant_make_examples:. input:. bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",. bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",. reference=config[""ref""][""fasta""],. output:. tfrecord=temp(. f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz"". ),. nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>. log:. f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",. benchmark:. f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv"". container:. f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channels \. --min_mapping_quality=1 \. --parse_sam_aux_fields \. --partition_size=25000 \. --max_reads_per_partition=600 \. --phase_reads \. --pileup_image_width {params.pileup_image_width} \. --norealign_reads \. --sort_by_haplotypes \. --track_ref_reads \. --vs",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:1366,performance,batch,batches,1366,"es the error PacificBiosciences/HiFiTargetEnrichment#4 , since i cant find what the error is and how to fix it i opend the Issue. Many thanks in advance. **Setup**. - Operating system: Ubuntu 20.04.6 LTS. - DeepVariant version: 1.5.0. - Tensorflow 2.11.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: PacBio HIFI reads. **Steps to reproduce:**. ```. rule deepvariant_make_examples:. input:. bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",. bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",. reference=config[""ref""][""fasta""],. output:. tfrecord=temp(. f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz"". ),. nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>. log:. f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",. benchmark:. f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv"". container:. f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channels \. --min_mapping_quality=1 \. --parse_sam_aux_fields \. --partition_size=25000 \. --max_reads_per_partition=600 \. --phase_reads \. --pileup_image_width {params.pileup_image_width} \. --norealign_reads \. --sort_by_haplotypes \. --track_ref_reads \. --vsc_min_fraction_indels {params.vsc_min_fraction_indels} \. --mode calling \. --ref {input.reference} \. --reads {in",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:1375,performance,batch,batch,1375,"rror PacificBiosciences/HiFiTargetEnrichment#4 , since i cant find what the error is and how to fix it i opend the Issue. Many thanks in advance. **Setup**. - Operating system: Ubuntu 20.04.6 LTS. - DeepVariant version: 1.5.0. - Tensorflow 2.11.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: PacBio HIFI reads. **Steps to reproduce:**. ```. rule deepvariant_make_examples:. input:. bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",. bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",. reference=config[""ref""][""fasta""],. output:. tfrecord=temp(. f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz"". ),. nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>. log:. f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",. benchmark:. f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv"". container:. f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channels \. --min_mapping_quality=1 \. --parse_sam_aux_fields \. --partition_size=25000 \. --max_reads_per_partition=600 \. --phase_reads \. --pileup_image_width {params.pileup_image_width} \. --norealign_reads \. --sort_by_haplotypes \. --track_ref_reads \. --vsc_min_fraction_indels {params.vsc_min_fraction_indels} \. --mode calling \. --ref {input.reference} \. --reads {input.bam}",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:1621,performance,batch,batches,1621," - Installation method (Docker, built from source, etc.): singularity. - Type of data: PacBio HIFI reads. **Steps to reproduce:**. ```. rule deepvariant_make_examples:. input:. bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",. bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",. reference=config[""ref""][""fasta""],. output:. tfrecord=temp(. f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz"". ),. nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>. log:. f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",. benchmark:. f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv"". container:. f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channels \. --min_mapping_quality=1 \. --parse_sam_aux_fields \. --partition_size=25000 \. --max_reads_per_partition=600 \. --phase_reads \. --pileup_image_width {params.pileup_image_width} \. --norealign_reads \. --sort_by_haplotypes \. --track_ref_reads \. --vsc_min_fraction_indels {params.vsc_min_fraction_indels} \. --mode calling \. --ref {input.reference} \. --reads {input.bam} \. --examples {params.examples} \. --gvcf {params.gvcf} \. --task {params.shard}) > {log} 2>&1. """""". ```. Error:. ```. 2023-07-12 15:19:55.926661: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAP",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:1630,performance,batch,batch,1630,"llation method (Docker, built from source, etc.): singularity. - Type of data: PacBio HIFI reads. **Steps to reproduce:**. ```. rule deepvariant_make_examples:. input:. bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",. bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",. reference=config[""ref""][""fasta""],. output:. tfrecord=temp(. f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz"". ),. nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>. log:. f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",. benchmark:. f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv"". container:. f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channels \. --min_mapping_quality=1 \. --parse_sam_aux_fields \. --partition_size=25000 \. --max_reads_per_partition=600 \. --phase_reads \. --pileup_image_width {params.pileup_image_width} \. --norealign_reads \. --sort_by_haplotypes \. --track_ref_reads \. --vsc_min_fraction_indels {params.vsc_min_fraction_indels} \. --mode calling \. --ref {input.reference} \. --reads {input.bam} \. --examples {params.examples} \. --gvcf {params.gvcf} \. --task {params.shard}) > {log} 2>&1. """""". ```. Error:. ```. 2023-07-12 15:19:55.926661: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI De>. T",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:1721,performance,batch,batches,1721,"eads. **Steps to reproduce:**. ```. rule deepvariant_make_examples:. input:. bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",. bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",. reference=config[""ref""][""fasta""],. output:. tfrecord=temp(. f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz"". ),. nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>. log:. f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",. benchmark:. f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv"". container:. f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channels \. --min_mapping_quality=1 \. --parse_sam_aux_fields \. --partition_size=25000 \. --max_reads_per_partition=600 \. --phase_reads \. --pileup_image_width {params.pileup_image_width} \. --norealign_reads \. --sort_by_haplotypes \. --track_ref_reads \. --vsc_min_fraction_indels {params.vsc_min_fraction_indels} \. --mode calling \. --ref {input.reference} \. --reads {input.bam} \. --examples {params.examples} \. --gvcf {params.gvcf} \. --task {params.shard}) > {log} 2>&1. """""". ```. Error:. ```. 2023-07-12 15:19:55.926661: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI De>. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:1730,performance,batch,batch,1730,"Steps to reproduce:**. ```. rule deepvariant_make_examples:. input:. bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",. bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",. reference=config[""ref""][""fasta""],. output:. tfrecord=temp(. f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz"". ),. nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>. log:. f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",. benchmark:. f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv"". container:. f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channels \. --min_mapping_quality=1 \. --parse_sam_aux_fields \. --partition_size=25000 \. --max_reads_per_partition=600 \. --phase_reads \. --pileup_image_width {params.pileup_image_width} \. --norealign_reads \. --sort_by_haplotypes \. --track_ref_reads \. --vsc_min_fraction_indels {params.vsc_min_fraction_indels} \. --mode calling \. --ref {input.reference} \. --reads {input.bam} \. --examples {params.examples} \. --gvcf {params.gvcf} \. --task {params.shard}) > {log} 2>&1. """""". ```. Error:. ```. 2023-07-12 15:19:55.926661: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI De>. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-07-1",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:2485,performance,Error,Error,2485,"nt:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channels \. --min_mapping_quality=1 \. --parse_sam_aux_fields \. --partition_size=25000 \. --max_reads_per_partition=600 \. --phase_reads \. --pileup_image_width {params.pileup_image_width} \. --norealign_reads \. --sort_by_haplotypes \. --track_ref_reads \. --vsc_min_fraction_indels {params.vsc_min_fraction_indels} \. --mode calling \. --ref {input.reference} \. --reads {input.bam} \. --examples {params.examples} \. --gvcf {params.gvcf} \. --task {params.shard}) > {log} 2>&1. """""". ```. Error:. ```. 2023-07-12 15:19:55.926661: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI De>. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-07-12 15:19:59.038994: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.6 SO:>. 2023-07-12 15:19:59.039047: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag BC: in RG line, ignoring: @RG ID:d98f52ac>. 2023-07-12 15:19:59.039065: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG ID:d98f52ac>. I0712 15:19:59.039340 140472429422400 genomics_reader.py:222] Reading batches/Test349/D18757/aligned/D18757.hs37d5.bam with NativeS>. I0712 15:19:59.044630 140472429422400 make_examples_core.py:257] Task 32/96: Preparing inputs. 2023-07-12 15:19:59.049074: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.6 SO:>. 2023-07-12 15:19:",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:2605,performance,optimiz,optimized,2605,"examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channels \. --min_mapping_quality=1 \. --parse_sam_aux_fields \. --partition_size=25000 \. --max_reads_per_partition=600 \. --phase_reads \. --pileup_image_width {params.pileup_image_width} \. --norealign_reads \. --sort_by_haplotypes \. --track_ref_reads \. --vsc_min_fraction_indels {params.vsc_min_fraction_indels} \. --mode calling \. --ref {input.reference} \. --reads {input.bam} \. --examples {params.examples} \. --gvcf {params.gvcf} \. --task {params.shard}) > {log} 2>&1. """""". ```. Error:. ```. 2023-07-12 15:19:55.926661: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI De>. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-07-12 15:19:59.038994: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.6 SO:>. 2023-07-12 15:19:59.039047: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag BC: in RG line, ignoring: @RG ID:d98f52ac>. 2023-07-12 15:19:59.039065: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG ID:d98f52ac>. I0712 15:19:59.039340 140472429422400 genomics_reader.py:222] Reading batches/Test349/D18757/aligned/D18757.hs37d5.bam with NativeS>. I0712 15:19:59.044630 140472429422400 make_examples_core.py:257] Task 32/96: Preparing inputs. 2023-07-12 15:19:59.049074: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.6 SO:>. 2023-07-12 15:19:59.049123: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag BC: in RG line, ignoring: @RG ID:d98f52ac>. 2023-07-12 ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:3181,performance,batch,batches,3181," --norealign_reads \. --sort_by_haplotypes \. --track_ref_reads \. --vsc_min_fraction_indels {params.vsc_min_fraction_indels} \. --mode calling \. --ref {input.reference} \. --reads {input.bam} \. --examples {params.examples} \. --gvcf {params.gvcf} \. --task {params.shard}) > {log} 2>&1. """""". ```. Error:. ```. 2023-07-12 15:19:55.926661: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI De>. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-07-12 15:19:59.038994: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.6 SO:>. 2023-07-12 15:19:59.039047: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag BC: in RG line, ignoring: @RG ID:d98f52ac>. 2023-07-12 15:19:59.039065: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG ID:d98f52ac>. I0712 15:19:59.039340 140472429422400 genomics_reader.py:222] Reading batches/Test349/D18757/aligned/D18757.hs37d5.bam with NativeS>. I0712 15:19:59.044630 140472429422400 make_examples_core.py:257] Task 32/96: Preparing inputs. 2023-07-12 15:19:59.049074: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.6 SO:>. 2023-07-12 15:19:59.049123: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag BC: in RG line, ignoring: @RG ID:d98f52ac>. 2023-07-12 15:19:59.049140: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG ID:d98f52ac>. I0712 15:19:59.049325 140472429422400 genomics_reader.py:222] Reading batches/Test349/D18757/aligned/D18757.hs37d5.bam with NativeS>. I0712 15:19:59.059888 140472429422400 make_examples_core.py:257] Task 32/96: Common contigs are ['1', '2', '3', '4', '5', '6', '7',>. I0712 15:19:59.247122 140472429422400 make_examples_core.py:257] Task 32/96: Starting from v0.9.0, --use_ref_for_cram is default to>. 2023-07-12 15:19:59.247444: I third_party/nucleus/io/sam",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:3797,performance,batch,batches,3797,"wn tag pb: in header line, ignoring: @HD VN:1.6 SO:>. 2023-07-12 15:19:59.039047: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag BC: in RG line, ignoring: @RG ID:d98f52ac>. 2023-07-12 15:19:59.039065: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG ID:d98f52ac>. I0712 15:19:59.039340 140472429422400 genomics_reader.py:222] Reading batches/Test349/D18757/aligned/D18757.hs37d5.bam with NativeS>. I0712 15:19:59.044630 140472429422400 make_examples_core.py:257] Task 32/96: Preparing inputs. 2023-07-12 15:19:59.049074: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.6 SO:>. 2023-07-12 15:19:59.049123: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag BC: in RG line, ignoring: @RG ID:d98f52ac>. 2023-07-12 15:19:59.049140: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG ID:d98f52ac>. I0712 15:19:59.049325 140472429422400 genomics_reader.py:222] Reading batches/Test349/D18757/aligned/D18757.hs37d5.bam with NativeS>. I0712 15:19:59.059888 140472429422400 make_examples_core.py:257] Task 32/96: Common contigs are ['1', '2', '3', '4', '5', '6', '7',>. I0712 15:19:59.247122 140472429422400 make_examples_core.py:257] Task 32/96: Starting from v0.9.0, --use_ref_for_cram is default to>. 2023-07-12 15:19:59.247444: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2023-07-12 15:19:59.364871: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.6 SO:>. 2023-07-12 15:19:59.364924: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag BC: in RG line, ignoring: @RG ID:d98f52ac>. 2023-07-12 15:19:59.364944: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG ID:d98f52ac>. I0712 15:19:59.365195 140472429422400 genomics_reader.py:222] Reading batches/Test349/D18757/aligned/D18757.hs37d5.bam with NativeS>. 2023-07-12 15:19:59.535088: W third_pa",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:4699,performance,batch,batches,4699,"ring: @RG ID:d98f52ac>. I0712 15:19:59.049325 140472429422400 genomics_reader.py:222] Reading batches/Test349/D18757/aligned/D18757.hs37d5.bam with NativeS>. I0712 15:19:59.059888 140472429422400 make_examples_core.py:257] Task 32/96: Common contigs are ['1', '2', '3', '4', '5', '6', '7',>. I0712 15:19:59.247122 140472429422400 make_examples_core.py:257] Task 32/96: Starting from v0.9.0, --use_ref_for_cram is default to>. 2023-07-12 15:19:59.247444: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2023-07-12 15:19:59.364871: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.6 SO:>. 2023-07-12 15:19:59.364924: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag BC: in RG line, ignoring: @RG ID:d98f52ac>. 2023-07-12 15:19:59.364944: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG ID:d98f52ac>. I0712 15:19:59.365195 140472429422400 genomics_reader.py:222] Reading batches/Test349/D18757/aligned/D18757.hs37d5.bam with NativeS>. 2023-07-12 15:19:59.535088: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.6 SO:>. 2023-07-12 15:19:59.535146: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag BC: in RG line, ignoring: @RG ID:d98f52ac>. 2023-07-12 15:19:59.535166: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG ID:d98f52ac>. I0712 15:19:59.535452 140472429422400 genomics_reader.py:222] Reading batches/Test349/D18757/aligned/D18757.hs37d5.bam with NativeS>. I0712 15:19:59.536432 140472429422400 make_examples_core.py:257] Task 32/96: Writing gvcf records to batches/Test349/D18757/deepvar>. I0712 15:19:59.536834 140472429422400 make_examples_core.py:257] Task 32/96: Writing examples to batches/Test349/D18757/deepvariant>. I0712 15:19:59.536924 140472429422400 make_examples_core.py:257] Task 32/96: Overhead for preparing inputs: 0 seconds. 2023-07-12 15:19:59.545470: W ./",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:5220,performance,batch,batches,5220,"IZE to 134217728. 2023-07-12 15:19:59.364871: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.6 SO:>. 2023-07-12 15:19:59.364924: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag BC: in RG line, ignoring: @RG ID:d98f52ac>. 2023-07-12 15:19:59.364944: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG ID:d98f52ac>. I0712 15:19:59.365195 140472429422400 genomics_reader.py:222] Reading batches/Test349/D18757/aligned/D18757.hs37d5.bam with NativeS>. 2023-07-12 15:19:59.535088: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.6 SO:>. 2023-07-12 15:19:59.535146: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag BC: in RG line, ignoring: @RG ID:d98f52ac>. 2023-07-12 15:19:59.535166: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG ID:d98f52ac>. I0712 15:19:59.535452 140472429422400 genomics_reader.py:222] Reading batches/Test349/D18757/aligned/D18757.hs37d5.bam with NativeS>. I0712 15:19:59.536432 140472429422400 make_examples_core.py:257] Task 32/96: Writing gvcf records to batches/Test349/D18757/deepvar>. I0712 15:19:59.536834 140472429422400 make_examples_core.py:257] Task 32/96: Writing examples to batches/Test349/D18757/deepvariant>. I0712 15:19:59.536924 140472429422400 make_examples_core.py:257] Task 32/96: Overhead for preparing inputs: 0 seconds. 2023-07-12 15:19:59.545470: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicM>. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_ch",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:5385,performance,batch,batches,5385,":59.364924: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag BC: in RG line, ignoring: @RG ID:d98f52ac>. 2023-07-12 15:19:59.364944: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG ID:d98f52ac>. I0712 15:19:59.365195 140472429422400 genomics_reader.py:222] Reading batches/Test349/D18757/aligned/D18757.hs37d5.bam with NativeS>. 2023-07-12 15:19:59.535088: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.6 SO:>. 2023-07-12 15:19:59.535146: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag BC: in RG line, ignoring: @RG ID:d98f52ac>. 2023-07-12 15:19:59.535166: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG ID:d98f52ac>. I0712 15:19:59.535452 140472429422400 genomics_reader.py:222] Reading batches/Test349/D18757/aligned/D18757.hs37d5.bam with NativeS>. I0712 15:19:59.536432 140472429422400 make_examples_core.py:257] Task 32/96: Writing gvcf records to batches/Test349/D18757/deepvar>. I0712 15:19:59.536834 140472429422400 make_examples_core.py:257] Task 32/96: Writing examples to batches/Test349/D18757/deepvariant>. I0712 15:19:59.536924 140472429422400 make_examples_core.py:257] Task 32/96: Overhead for preparing inputs: 0 seconds. 2023-07-12 15:19:59.545470: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicM>. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:5515,performance,batch,batches,5515,"9.364944: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG ID:d98f52ac>. I0712 15:19:59.365195 140472429422400 genomics_reader.py:222] Reading batches/Test349/D18757/aligned/D18757.hs37d5.bam with NativeS>. 2023-07-12 15:19:59.535088: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.6 SO:>. 2023-07-12 15:19:59.535146: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag BC: in RG line, ignoring: @RG ID:d98f52ac>. 2023-07-12 15:19:59.535166: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG ID:d98f52ac>. I0712 15:19:59.535452 140472429422400 genomics_reader.py:222] Reading batches/Test349/D18757/aligned/D18757.hs37d5.bam with NativeS>. I0712 15:19:59.536432 140472429422400 make_examples_core.py:257] Task 32/96: Writing gvcf records to batches/Test349/D18757/deepvar>. I0712 15:19:59.536834 140472429422400 make_examples_core.py:257] Task 32/96: Writing examples to batches/Test349/D18757/deepvariant>. I0712 15:19:59.536924 140472429422400 make_examples_core.py:257] Task 32/96: Overhead for preparing inputs: 0 seconds. 2023-07-12 15:19:59.545470: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicM>. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2183, in make_example>. runtimes) = region_proce",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:5629,performance,Overhead,Overhead,5629,"2 15:19:59.365195 140472429422400 genomics_reader.py:222] Reading batches/Test349/D18757/aligned/D18757.hs37d5.bam with NativeS>. 2023-07-12 15:19:59.535088: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.6 SO:>. 2023-07-12 15:19:59.535146: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag BC: in RG line, ignoring: @RG ID:d98f52ac>. 2023-07-12 15:19:59.535166: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG ID:d98f52ac>. I0712 15:19:59.535452 140472429422400 genomics_reader.py:222] Reading batches/Test349/D18757/aligned/D18757.hs37d5.bam with NativeS>. I0712 15:19:59.536432 140472429422400 make_examples_core.py:257] Task 32/96: Writing gvcf records to batches/Test349/D18757/deepvar>. I0712 15:19:59.536834 140472429422400 make_examples_core.py:257] Task 32/96: Writing examples to batches/Test349/D18757/deepvariant>. I0712 15:19:59.536924 140472429422400 make_examples_core.py:257] Task 32/96: Overhead for preparing inputs: 0 seconds. 2023-07-12 15:19:59.545470: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicM>. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2183, in make_example>. runtimes) = region_processor.process(region). File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/com_google_deepvariant/deepvariant/make_examples",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:68,reliability,fail,failed,68,"RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed; **Describe the issue:**. Hello everyone, i am trying to run a Pacbio Workflow with deepvariant in it but i get an error in the make example step ( Rule and log below) i allready have an open Issue on the Workflow but we are at the Point that we think its ether Nucleus or Tensorflow that produces the error PacificBiosciences/HiFiTargetEnrichment#4 , since i cant find what the error is and how to fix it i opend the Issue. Many thanks in advance. **Setup**. - Operating system: Ubuntu 20.04.6 LTS. - DeepVariant version: 1.5.0. - Tensorflow 2.11.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: PacBio HIFI reads. **Steps to reproduce:**. ```. rule deepvariant_make_examples:. input:. bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",. bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",. reference=config[""ref""][""fasta""],. output:. tfrecord=temp(. f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz"". ),. nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>. log:. f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",. benchmark:. f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv"". container:. f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channel",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:5755,reliability,Fail,Failed,5755,"eS>. 2023-07-12 15:19:59.535088: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.6 SO:>. 2023-07-12 15:19:59.535146: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag BC: in RG line, ignoring: @RG ID:d98f52ac>. 2023-07-12 15:19:59.535166: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG ID:d98f52ac>. I0712 15:19:59.535452 140472429422400 genomics_reader.py:222] Reading batches/Test349/D18757/aligned/D18757.hs37d5.bam with NativeS>. I0712 15:19:59.536432 140472429422400 make_examples_core.py:257] Task 32/96: Writing gvcf records to batches/Test349/D18757/deepvar>. I0712 15:19:59.536834 140472429422400 make_examples_core.py:257] Task 32/96: Writing examples to batches/Test349/D18757/deepvariant>. I0712 15:19:59.536924 140472429422400 make_examples_core.py:257] Task 32/96: Overhead for preparing inputs: 0 seconds. 2023-07-12 15:19:59.545470: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicM>. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2183, in make_example>. runtimes) = region_processor.process(region). File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1285, in process. sample_reads = self.region_reads_norealign(. File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/co",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:7469,reliability,fail,failed,7469,"] Task 32/96: Writing examples to batches/Test349/D18757/deepvariant>. I0712 15:19:59.536924 140472429422400 make_examples_core.py:257] Task 32/96: Overhead for preparing inputs: 0 seconds. 2023-07-12 15:19:59.545470: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicM>. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2183, in make_example>. runtimes) = region_processor.process(region). File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1285, in process. sample_reads = self.region_reads_norealign(. File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1381, in region_reads>. reads = utils.reservoir_sample(reads,. File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/com_google_deepvariant/third_party/nucleus/util/utils.py"", line 117, in reservoir_sam>. for i, item in enumerate(iterable):. File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_ne>. not_done = self._cc_iterable.PythonNext(record). RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed. ```.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:48,safety,valid,valid,48,"RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed; **Describe the issue:**. Hello everyone, i am trying to run a Pacbio Workflow with deepvariant in it but i get an error in the make example step ( Rule and log below) i allready have an open Issue on the Workflow but we are at the Point that we think its ether Nucleus or Tensorflow that produces the error PacificBiosciences/HiFiTargetEnrichment#4 , since i cant find what the error is and how to fix it i opend the Issue. Many thanks in advance. **Setup**. - Operating system: Ubuntu 20.04.6 LTS. - DeepVariant version: 1.5.0. - Tensorflow 2.11.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: PacBio HIFI reads. **Steps to reproduce:**. ```. rule deepvariant_make_examples:. input:. bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",. bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",. reference=config[""ref""][""fasta""],. output:. tfrecord=temp(. f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz"". ),. nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>. log:. f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",. benchmark:. f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv"". container:. f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channel",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:190,safety,error,error,190,"RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed; **Describe the issue:**. Hello everyone, i am trying to run a Pacbio Workflow with deepvariant in it but i get an error in the make example step ( Rule and log below) i allready have an open Issue on the Workflow but we are at the Point that we think its ether Nucleus or Tensorflow that produces the error PacificBiosciences/HiFiTargetEnrichment#4 , since i cant find what the error is and how to fix it i opend the Issue. Many thanks in advance. **Setup**. - Operating system: Ubuntu 20.04.6 LTS. - DeepVariant version: 1.5.0. - Tensorflow 2.11.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: PacBio HIFI reads. **Steps to reproduce:**. ```. rule deepvariant_make_examples:. input:. bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",. bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",. reference=config[""ref""][""fasta""],. output:. tfrecord=temp(. f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz"". ),. nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>. log:. f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",. benchmark:. f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv"". container:. f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channel",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:232,safety,log,log,232,"RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed; **Describe the issue:**. Hello everyone, i am trying to run a Pacbio Workflow with deepvariant in it but i get an error in the make example step ( Rule and log below) i allready have an open Issue on the Workflow but we are at the Point that we think its ether Nucleus or Tensorflow that produces the error PacificBiosciences/HiFiTargetEnrichment#4 , since i cant find what the error is and how to fix it i opend the Issue. Many thanks in advance. **Setup**. - Operating system: Ubuntu 20.04.6 LTS. - DeepVariant version: 1.5.0. - Tensorflow 2.11.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: PacBio HIFI reads. **Steps to reproduce:**. ```. rule deepvariant_make_examples:. input:. bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",. bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",. reference=config[""ref""][""fasta""],. output:. tfrecord=temp(. f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz"". ),. nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>. log:. f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",. benchmark:. f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv"". container:. f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channel",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:377,safety,error,error,377,"RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed; **Describe the issue:**. Hello everyone, i am trying to run a Pacbio Workflow with deepvariant in it but i get an error in the make example step ( Rule and log below) i allready have an open Issue on the Workflow but we are at the Point that we think its ether Nucleus or Tensorflow that produces the error PacificBiosciences/HiFiTargetEnrichment#4 , since i cant find what the error is and how to fix it i opend the Issue. Many thanks in advance. **Setup**. - Operating system: Ubuntu 20.04.6 LTS. - DeepVariant version: 1.5.0. - Tensorflow 2.11.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: PacBio HIFI reads. **Steps to reproduce:**. ```. rule deepvariant_make_examples:. input:. bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",. bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",. reference=config[""ref""][""fasta""],. output:. tfrecord=temp(. f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz"". ),. nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>. log:. f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",. benchmark:. f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv"". container:. f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channel",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:454,safety,error,error,454,"RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed; **Describe the issue:**. Hello everyone, i am trying to run a Pacbio Workflow with deepvariant in it but i get an error in the make example step ( Rule and log below) i allready have an open Issue on the Workflow but we are at the Point that we think its ether Nucleus or Tensorflow that produces the error PacificBiosciences/HiFiTargetEnrichment#4 , since i cant find what the error is and how to fix it i opend the Issue. Many thanks in advance. **Setup**. - Operating system: Ubuntu 20.04.6 LTS. - DeepVariant version: 1.5.0. - Tensorflow 2.11.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: PacBio HIFI reads. **Steps to reproduce:**. ```. rule deepvariant_make_examples:. input:. bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",. bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",. reference=config[""ref""][""fasta""],. output:. tfrecord=temp(. f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz"". ),. nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>. log:. f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",. benchmark:. f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv"". container:. f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channel",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:794,safety,input,input,794,"RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed; **Describe the issue:**. Hello everyone, i am trying to run a Pacbio Workflow with deepvariant in it but i get an error in the make example step ( Rule and log below) i allready have an open Issue on the Workflow but we are at the Point that we think its ether Nucleus or Tensorflow that produces the error PacificBiosciences/HiFiTargetEnrichment#4 , since i cant find what the error is and how to fix it i opend the Issue. Many thanks in advance. **Setup**. - Operating system: Ubuntu 20.04.6 LTS. - DeepVariant version: 1.5.0. - Tensorflow 2.11.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: PacBio HIFI reads. **Steps to reproduce:**. ```. rule deepvariant_make_examples:. input:. bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",. bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",. reference=config[""ref""][""fasta""],. output:. tfrecord=temp(. f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz"". ),. nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>. log:. f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",. benchmark:. f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv"". container:. f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channel",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:1236,safety,log,log,1236,"low) i allready have an open Issue on the Workflow but we are at the Point that we think its ether Nucleus or Tensorflow that produces the error PacificBiosciences/HiFiTargetEnrichment#4 , since i cant find what the error is and how to fix it i opend the Issue. Many thanks in advance. **Setup**. - Operating system: Ubuntu 20.04.6 LTS. - DeepVariant version: 1.5.0. - Tensorflow 2.11.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: PacBio HIFI reads. **Steps to reproduce:**. ```. rule deepvariant_make_examples:. input:. bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",. bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",. reference=config[""ref""][""fasta""],. output:. tfrecord=temp(. f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz"". ),. nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>. log:. f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",. benchmark:. f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv"". container:. f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channels \. --min_mapping_quality=1 \. --parse_sam_aux_fields \. --partition_size=25000 \. --max_reads_per_partition=600 \. --phase_reads \. --pileup_image_width {params.pileup_image_width} \. --norealign_reads \. --sort_by_haplotypes \. --track",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:1260,safety,log,logs,1260,"open Issue on the Workflow but we are at the Point that we think its ether Nucleus or Tensorflow that produces the error PacificBiosciences/HiFiTargetEnrichment#4 , since i cant find what the error is and how to fix it i opend the Issue. Many thanks in advance. **Setup**. - Operating system: Ubuntu 20.04.6 LTS. - DeepVariant version: 1.5.0. - Tensorflow 2.11.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: PacBio HIFI reads. **Steps to reproduce:**. ```. rule deepvariant_make_examples:. input:. bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",. bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",. reference=config[""ref""][""fasta""],. output:. tfrecord=temp(. f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz"". ),. nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>. log:. f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",. benchmark:. f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv"". container:. f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channels \. --min_mapping_quality=1 \. --parse_sam_aux_fields \. --partition_size=25000 \. --max_reads_per_partition=600 \. --phase_reads \. --pileup_image_width {params.pileup_image_width} \. --norealign_reads \. --sort_by_haplotypes \. --track_ref_reads \. --vsc_min_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:1345,safety,log,log,1345," Tensorflow that produces the error PacificBiosciences/HiFiTargetEnrichment#4 , since i cant find what the error is and how to fix it i opend the Issue. Many thanks in advance. **Setup**. - Operating system: Ubuntu 20.04.6 LTS. - DeepVariant version: 1.5.0. - Tensorflow 2.11.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: PacBio HIFI reads. **Steps to reproduce:**. ```. rule deepvariant_make_examples:. input:. bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",. bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",. reference=config[""ref""][""fasta""],. output:. tfrecord=temp(. f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz"". ),. nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>. log:. f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",. benchmark:. f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv"". container:. f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channels \. --min_mapping_quality=1 \. --parse_sam_aux_fields \. --partition_size=25000 \. --max_reads_per_partition=600 \. --phase_reads \. --pileup_image_width {params.pileup_image_width} \. --norealign_reads \. --sort_by_haplotypes \. --track_ref_reads \. --vsc_min_fraction_indels {params.vsc_min_fraction_indels} \. --mode calling \. --ref {input.re",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:1870,safety,input,input,1870,"batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",. reference=config[""ref""][""fasta""],. output:. tfrecord=temp(. f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz"". ),. nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>. log:. f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",. benchmark:. f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv"". container:. f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channels \. --min_mapping_quality=1 \. --parse_sam_aux_fields \. --partition_size=25000 \. --max_reads_per_partition=600 \. --phase_reads \. --pileup_image_width {params.pileup_image_width} \. --norealign_reads \. --sort_by_haplotypes \. --track_ref_reads \. --vsc_min_fraction_indels {params.vsc_min_fraction_indels} \. --mode calling \. --ref {input.reference} \. --reads {input.bam} \. --examples {params.examples} \. --gvcf {params.gvcf} \. --task {params.shard}) > {log} 2>&1. """""". ```. Error:. ```. 2023-07-12 15:19:55.926661: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI De>. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-07-12 15:19:59.038994: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.6 SO:>. 2023-07-12 15:19:5",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:2339,safety,input,input,2339,"5}.log"",. benchmark:. f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv"". container:. f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channels \. --min_mapping_quality=1 \. --parse_sam_aux_fields \. --partition_size=25000 \. --max_reads_per_partition=600 \. --phase_reads \. --pileup_image_width {params.pileup_image_width} \. --norealign_reads \. --sort_by_haplotypes \. --track_ref_reads \. --vsc_min_fraction_indels {params.vsc_min_fraction_indels} \. --mode calling \. --ref {input.reference} \. --reads {input.bam} \. --examples {params.examples} \. --gvcf {params.gvcf} \. --task {params.shard}) > {log} 2>&1. """""". ```. Error:. ```. 2023-07-12 15:19:55.926661: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI De>. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-07-12 15:19:59.038994: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.6 SO:>. 2023-07-12 15:19:59.039047: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag BC: in RG line, ignoring: @RG ID:d98f52ac>. 2023-07-12 15:19:59.039065: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG ID:d98f52ac>. I0712 15:19:59.039340 140472429422400 genomics_reader.py:222] Reading batches/Test349/D18757/aligned/D18757.hs37d5.bam with NativeS>. I0712 15:19:59.044630 140472429422400 make_examples_core.py:257] Task 32/96: Preparing inputs. 20",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:2368,safety,input,input,2368,"es/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv"". container:. f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channels \. --min_mapping_quality=1 \. --parse_sam_aux_fields \. --partition_size=25000 \. --max_reads_per_partition=600 \. --phase_reads \. --pileup_image_width {params.pileup_image_width} \. --norealign_reads \. --sort_by_haplotypes \. --track_ref_reads \. --vsc_min_fraction_indels {params.vsc_min_fraction_indels} \. --mode calling \. --ref {input.reference} \. --reads {input.bam} \. --examples {params.examples} \. --gvcf {params.gvcf} \. --task {params.shard}) > {log} 2>&1. """""". ```. Error:. ```. 2023-07-12 15:19:55.926661: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI De>. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-07-12 15:19:59.038994: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.6 SO:>. 2023-07-12 15:19:59.039047: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag BC: in RG line, ignoring: @RG ID:d98f52ac>. 2023-07-12 15:19:59.039065: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG ID:d98f52ac>. I0712 15:19:59.039340 140472429422400 genomics_reader.py:222] Reading batches/Test349/D18757/aligned/D18757.hs37d5.bam with NativeS>. I0712 15:19:59.044630 140472429422400 make_examples_core.py:257] Task 32/96: Preparing inputs. 2023-07-12 15:19:59.049074: W t",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:2464,safety,log,log,2464,"ker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channels \. --min_mapping_quality=1 \. --parse_sam_aux_fields \. --partition_size=25000 \. --max_reads_per_partition=600 \. --phase_reads \. --pileup_image_width {params.pileup_image_width} \. --norealign_reads \. --sort_by_haplotypes \. --track_ref_reads \. --vsc_min_fraction_indels {params.vsc_min_fraction_indels} \. --mode calling \. --ref {input.reference} \. --reads {input.bam} \. --examples {params.examples} \. --gvcf {params.gvcf} \. --task {params.shard}) > {log} 2>&1. """""". ```. Error:. ```. 2023-07-12 15:19:55.926661: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI De>. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-07-12 15:19:59.038994: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.6 SO:>. 2023-07-12 15:19:59.039047: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag BC: in RG line, ignoring: @RG ID:d98f52ac>. 2023-07-12 15:19:59.039065: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG ID:d98f52ac>. I0712 15:19:59.039340 140472429422400 genomics_reader.py:222] Reading batches/Test349/D18757/aligned/D18757.hs37d5.bam with NativeS>. I0712 15:19:59.044630 140472429422400 make_examples_core.py:257] Task 32/96: Preparing inputs. 2023-07-12 15:19:59.049074: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.6 S",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:2485,safety,Error,Error,2485,"nt:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channels \. --min_mapping_quality=1 \. --parse_sam_aux_fields \. --partition_size=25000 \. --max_reads_per_partition=600 \. --phase_reads \. --pileup_image_width {params.pileup_image_width} \. --norealign_reads \. --sort_by_haplotypes \. --track_ref_reads \. --vsc_min_fraction_indels {params.vsc_min_fraction_indels} \. --mode calling \. --ref {input.reference} \. --reads {input.bam} \. --examples {params.examples} \. --gvcf {params.gvcf} \. --task {params.shard}) > {log} 2>&1. """""". ```. Error:. ```. 2023-07-12 15:19:55.926661: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI De>. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-07-12 15:19:59.038994: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.6 SO:>. 2023-07-12 15:19:59.039047: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag BC: in RG line, ignoring: @RG ID:d98f52ac>. 2023-07-12 15:19:59.039065: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG ID:d98f52ac>. I0712 15:19:59.039340 140472429422400 genomics_reader.py:222] Reading batches/Test349/D18757/aligned/D18757.hs37d5.bam with NativeS>. I0712 15:19:59.044630 140472429422400 make_examples_core.py:257] Task 32/96: Preparing inputs. 2023-07-12 15:19:59.049074: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.6 SO:>. 2023-07-12 15:19:",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:3332,safety,input,inputs,3332,"ef {input.reference} \. --reads {input.bam} \. --examples {params.examples} \. --gvcf {params.gvcf} \. --task {params.shard}) > {log} 2>&1. """""". ```. Error:. ```. 2023-07-12 15:19:55.926661: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI De>. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-07-12 15:19:59.038994: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.6 SO:>. 2023-07-12 15:19:59.039047: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag BC: in RG line, ignoring: @RG ID:d98f52ac>. 2023-07-12 15:19:59.039065: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG ID:d98f52ac>. I0712 15:19:59.039340 140472429422400 genomics_reader.py:222] Reading batches/Test349/D18757/aligned/D18757.hs37d5.bam with NativeS>. I0712 15:19:59.044630 140472429422400 make_examples_core.py:257] Task 32/96: Preparing inputs. 2023-07-12 15:19:59.049074: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.6 SO:>. 2023-07-12 15:19:59.049123: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag BC: in RG line, ignoring: @RG ID:d98f52ac>. 2023-07-12 15:19:59.049140: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG ID:d98f52ac>. I0712 15:19:59.049325 140472429422400 genomics_reader.py:222] Reading batches/Test349/D18757/aligned/D18757.hs37d5.bam with NativeS>. I0712 15:19:59.059888 140472429422400 make_examples_core.py:257] Task 32/96: Common contigs are ['1', '2', '3', '4', '5', '6', '7',>. I0712 15:19:59.247122 140472429422400 make_examples_core.py:257] Task 32/96: Starting from v0.9.0, --use_ref_for_cram is default to>. 2023-07-12 15:19:59.247444: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2023-07-12 15:19:59.364871: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in he",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:5652,safety,input,inputs,5652,"72429422400 genomics_reader.py:222] Reading batches/Test349/D18757/aligned/D18757.hs37d5.bam with NativeS>. 2023-07-12 15:19:59.535088: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.6 SO:>. 2023-07-12 15:19:59.535146: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag BC: in RG line, ignoring: @RG ID:d98f52ac>. 2023-07-12 15:19:59.535166: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG ID:d98f52ac>. I0712 15:19:59.535452 140472429422400 genomics_reader.py:222] Reading batches/Test349/D18757/aligned/D18757.hs37d5.bam with NativeS>. I0712 15:19:59.536432 140472429422400 make_examples_core.py:257] Task 32/96: Writing gvcf records to batches/Test349/D18757/deepvar>. I0712 15:19:59.536834 140472429422400 make_examples_core.py:257] Task 32/96: Writing examples to batches/Test349/D18757/deepvariant>. I0712 15:19:59.536924 140472429422400 make_examples_core.py:257] Task 32/96: Overhead for preparing inputs: 0 seconds. 2023-07-12 15:19:59.545470: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicM>. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2183, in make_example>. runtimes) = region_processor.process(region). File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1285, ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:5953,safety,modul,module,5953,"r.cc:174] Unknown tag BC: in RG line, ignoring: @RG ID:d98f52ac>. 2023-07-12 15:19:59.535166: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG ID:d98f52ac>. I0712 15:19:59.535452 140472429422400 genomics_reader.py:222] Reading batches/Test349/D18757/aligned/D18757.hs37d5.bam with NativeS>. I0712 15:19:59.536432 140472429422400 make_examples_core.py:257] Task 32/96: Writing gvcf records to batches/Test349/D18757/deepvar>. I0712 15:19:59.536834 140472429422400 make_examples_core.py:257] Task 32/96: Writing examples to batches/Test349/D18757/deepvariant>. I0712 15:19:59.536924 140472429422400 make_examples_core.py:257] Task 32/96: Overhead for preparing inputs: 0 seconds. 2023-07-12 15:19:59.545470: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicM>. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2183, in make_example>. runtimes) = region_processor.process(region). File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1285, in process. sample_reads = self.region_reads_norealign(. File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1381, in region_reads>. reads = utils.reservoir_sample(reads,. File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/com_google_deepvariant/third_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:7449,safety,valid,valid,7449,"] Task 32/96: Writing examples to batches/Test349/D18757/deepvariant>. I0712 15:19:59.536924 140472429422400 make_examples_core.py:257] Task 32/96: Overhead for preparing inputs: 0 seconds. 2023-07-12 15:19:59.545470: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicM>. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2183, in make_example>. runtimes) = region_processor.process(region). File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1285, in process. sample_reads = self.region_reads_norealign(. File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1381, in region_reads>. reads = utils.reservoir_sample(reads,. File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/com_google_deepvariant/third_party/nucleus/util/utils.py"", line 117, in reservoir_sam>. for i, item in enumerate(iterable):. File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__. record, not_done = self._raw_next(). File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_ne>. not_done = self._cc_iterable.PythonNext(record). RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed. ```.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:232,security,log,log,232,"RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed; **Describe the issue:**. Hello everyone, i am trying to run a Pacbio Workflow with deepvariant in it but i get an error in the make example step ( Rule and log below) i allready have an open Issue on the Workflow but we are at the Point that we think its ether Nucleus or Tensorflow that produces the error PacificBiosciences/HiFiTargetEnrichment#4 , since i cant find what the error is and how to fix it i opend the Issue. Many thanks in advance. **Setup**. - Operating system: Ubuntu 20.04.6 LTS. - DeepVariant version: 1.5.0. - Tensorflow 2.11.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: PacBio HIFI reads. **Steps to reproduce:**. ```. rule deepvariant_make_examples:. input:. bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",. bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",. reference=config[""ref""][""fasta""],. output:. tfrecord=temp(. f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz"". ),. nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>. log:. f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",. benchmark:. f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv"". container:. f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channel",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:1236,security,log,log,1236,"low) i allready have an open Issue on the Workflow but we are at the Point that we think its ether Nucleus or Tensorflow that produces the error PacificBiosciences/HiFiTargetEnrichment#4 , since i cant find what the error is and how to fix it i opend the Issue. Many thanks in advance. **Setup**. - Operating system: Ubuntu 20.04.6 LTS. - DeepVariant version: 1.5.0. - Tensorflow 2.11.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: PacBio HIFI reads. **Steps to reproduce:**. ```. rule deepvariant_make_examples:. input:. bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",. bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",. reference=config[""ref""][""fasta""],. output:. tfrecord=temp(. f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz"". ),. nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>. log:. f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",. benchmark:. f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv"". container:. f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channels \. --min_mapping_quality=1 \. --parse_sam_aux_fields \. --partition_size=25000 \. --max_reads_per_partition=600 \. --phase_reads \. --pileup_image_width {params.pileup_image_width} \. --norealign_reads \. --sort_by_haplotypes \. --track",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:1260,security,log,logs,1260,"open Issue on the Workflow but we are at the Point that we think its ether Nucleus or Tensorflow that produces the error PacificBiosciences/HiFiTargetEnrichment#4 , since i cant find what the error is and how to fix it i opend the Issue. Many thanks in advance. **Setup**. - Operating system: Ubuntu 20.04.6 LTS. - DeepVariant version: 1.5.0. - Tensorflow 2.11.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: PacBio HIFI reads. **Steps to reproduce:**. ```. rule deepvariant_make_examples:. input:. bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",. bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",. reference=config[""ref""][""fasta""],. output:. tfrecord=temp(. f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz"". ),. nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>. log:. f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",. benchmark:. f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv"". container:. f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channels \. --min_mapping_quality=1 \. --parse_sam_aux_fields \. --partition_size=25000 \. --max_reads_per_partition=600 \. --phase_reads \. --pileup_image_width {params.pileup_image_width} \. --norealign_reads \. --sort_by_haplotypes \. --track_ref_reads \. --vsc_min_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:1345,security,log,log,1345," Tensorflow that produces the error PacificBiosciences/HiFiTargetEnrichment#4 , since i cant find what the error is and how to fix it i opend the Issue. Many thanks in advance. **Setup**. - Operating system: Ubuntu 20.04.6 LTS. - DeepVariant version: 1.5.0. - Tensorflow 2.11.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: PacBio HIFI reads. **Steps to reproduce:**. ```. rule deepvariant_make_examples:. input:. bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",. bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",. reference=config[""ref""][""fasta""],. output:. tfrecord=temp(. f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz"". ),. nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>. log:. f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",. benchmark:. f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv"". container:. f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channels \. --min_mapping_quality=1 \. --parse_sam_aux_fields \. --partition_size=25000 \. --max_reads_per_partition=600 \. --phase_reads \. --pileup_image_width {params.pileup_image_width} \. --norealign_reads \. --sort_by_haplotypes \. --track_ref_reads \. --vsc_min_fraction_indels {params.vsc_min_fraction_indels} \. --mode calling \. --ref {input.re",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:2464,security,log,log,2464,"ker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channels \. --min_mapping_quality=1 \. --parse_sam_aux_fields \. --partition_size=25000 \. --max_reads_per_partition=600 \. --phase_reads \. --pileup_image_width {params.pileup_image_width} \. --norealign_reads \. --sort_by_haplotypes \. --track_ref_reads \. --vsc_min_fraction_indels {params.vsc_min_fraction_indels} \. --mode calling \. --ref {input.reference} \. --reads {input.bam} \. --examples {params.examples} \. --gvcf {params.gvcf} \. --task {params.shard}) > {log} 2>&1. """""". ```. Error:. ```. 2023-07-12 15:19:55.926661: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI De>. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-07-12 15:19:59.038994: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.6 SO:>. 2023-07-12 15:19:59.039047: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag BC: in RG line, ignoring: @RG ID:d98f52ac>. 2023-07-12 15:19:59.039065: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG ID:d98f52ac>. I0712 15:19:59.039340 140472429422400 genomics_reader.py:222] Reading batches/Test349/D18757/aligned/D18757.hs37d5.bam with NativeS>. I0712 15:19:59.044630 140472429422400 make_examples_core.py:257] Task 32/96: Preparing inputs. 2023-07-12 15:19:59.049074: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.6 S",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:232,testability,log,log,232,"RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed; **Describe the issue:**. Hello everyone, i am trying to run a Pacbio Workflow with deepvariant in it but i get an error in the make example step ( Rule and log below) i allready have an open Issue on the Workflow but we are at the Point that we think its ether Nucleus or Tensorflow that produces the error PacificBiosciences/HiFiTargetEnrichment#4 , since i cant find what the error is and how to fix it i opend the Issue. Many thanks in advance. **Setup**. - Operating system: Ubuntu 20.04.6 LTS. - DeepVariant version: 1.5.0. - Tensorflow 2.11.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: PacBio HIFI reads. **Steps to reproduce:**. ```. rule deepvariant_make_examples:. input:. bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",. bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",. reference=config[""ref""][""fasta""],. output:. tfrecord=temp(. f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz"". ),. nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>. log:. f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",. benchmark:. f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv"". container:. f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channel",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:1236,testability,log,log,1236,"low) i allready have an open Issue on the Workflow but we are at the Point that we think its ether Nucleus or Tensorflow that produces the error PacificBiosciences/HiFiTargetEnrichment#4 , since i cant find what the error is and how to fix it i opend the Issue. Many thanks in advance. **Setup**. - Operating system: Ubuntu 20.04.6 LTS. - DeepVariant version: 1.5.0. - Tensorflow 2.11.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: PacBio HIFI reads. **Steps to reproduce:**. ```. rule deepvariant_make_examples:. input:. bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",. bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",. reference=config[""ref""][""fasta""],. output:. tfrecord=temp(. f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz"". ),. nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>. log:. f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",. benchmark:. f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv"". container:. f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channels \. --min_mapping_quality=1 \. --parse_sam_aux_fields \. --partition_size=25000 \. --max_reads_per_partition=600 \. --phase_reads \. --pileup_image_width {params.pileup_image_width} \. --norealign_reads \. --sort_by_haplotypes \. --track",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:1260,testability,log,logs,1260,"open Issue on the Workflow but we are at the Point that we think its ether Nucleus or Tensorflow that produces the error PacificBiosciences/HiFiTargetEnrichment#4 , since i cant find what the error is and how to fix it i opend the Issue. Many thanks in advance. **Setup**. - Operating system: Ubuntu 20.04.6 LTS. - DeepVariant version: 1.5.0. - Tensorflow 2.11.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: PacBio HIFI reads. **Steps to reproduce:**. ```. rule deepvariant_make_examples:. input:. bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",. bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",. reference=config[""ref""][""fasta""],. output:. tfrecord=temp(. f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz"". ),. nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>. log:. f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",. benchmark:. f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv"". container:. f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channels \. --min_mapping_quality=1 \. --parse_sam_aux_fields \. --partition_size=25000 \. --max_reads_per_partition=600 \. --phase_reads \. --pileup_image_width {params.pileup_image_width} \. --norealign_reads \. --sort_by_haplotypes \. --track_ref_reads \. --vsc_min_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:1345,testability,log,log,1345," Tensorflow that produces the error PacificBiosciences/HiFiTargetEnrichment#4 , since i cant find what the error is and how to fix it i opend the Issue. Many thanks in advance. **Setup**. - Operating system: Ubuntu 20.04.6 LTS. - DeepVariant version: 1.5.0. - Tensorflow 2.11.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: PacBio HIFI reads. **Steps to reproduce:**. ```. rule deepvariant_make_examples:. input:. bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",. bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",. reference=config[""ref""][""fasta""],. output:. tfrecord=temp(. f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz"". ),. nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>. log:. f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",. benchmark:. f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv"". container:. f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channels \. --min_mapping_quality=1 \. --parse_sam_aux_fields \. --partition_size=25000 \. --max_reads_per_partition=600 \. --phase_reads \. --pileup_image_width {params.pileup_image_width} \. --norealign_reads \. --sort_by_haplotypes \. --track_ref_reads \. --vsc_min_fraction_indels {params.vsc_min_fraction_indels} \. --mode calling \. --ref {input.re",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:2464,testability,log,log,2464,"ker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channels \. --min_mapping_quality=1 \. --parse_sam_aux_fields \. --partition_size=25000 \. --max_reads_per_partition=600 \. --phase_reads \. --pileup_image_width {params.pileup_image_width} \. --norealign_reads \. --sort_by_haplotypes \. --track_ref_reads \. --vsc_min_fraction_indels {params.vsc_min_fraction_indels} \. --mode calling \. --ref {input.reference} \. --reads {input.bam} \. --examples {params.examples} \. --gvcf {params.gvcf} \. --task {params.shard}) > {log} 2>&1. """""". ```. Error:. ```. 2023-07-12 15:19:55.926661: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI De>. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-07-12 15:19:59.038994: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.6 SO:>. 2023-07-12 15:19:59.039047: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag BC: in RG line, ignoring: @RG ID:d98f52ac>. 2023-07-12 15:19:59.039065: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG ID:d98f52ac>. I0712 15:19:59.039340 140472429422400 genomics_reader.py:222] Reading batches/Test349/D18757/aligned/D18757.hs37d5.bam with NativeS>. I0712 15:19:59.044630 140472429422400 make_examples_core.py:257] Task 32/96: Preparing inputs. 2023-07-12 15:19:59.049074: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.6 S",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:5805,testability,Trace,Traceback,5805,"us/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.6 SO:>. 2023-07-12 15:19:59.535146: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag BC: in RG line, ignoring: @RG ID:d98f52ac>. 2023-07-12 15:19:59.535166: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG ID:d98f52ac>. I0712 15:19:59.535452 140472429422400 genomics_reader.py:222] Reading batches/Test349/D18757/aligned/D18757.hs37d5.bam with NativeS>. I0712 15:19:59.536432 140472429422400 make_examples_core.py:257] Task 32/96: Writing gvcf records to batches/Test349/D18757/deepvar>. I0712 15:19:59.536834 140472429422400 make_examples_core.py:257] Task 32/96: Writing examples to batches/Test349/D18757/deepvariant>. I0712 15:19:59.536924 140472429422400 make_examples_core.py:257] Task 32/96: Overhead for preparing inputs: 0 seconds. 2023-07-12 15:19:59.545470: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicM>. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2183, in make_example>. runtimes) = region_processor.process(region). File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1285, in process. sample_reads = self.region_reads_norealign(. File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/com_google_deepvariant/deepvariant/make_examples_core.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:145,usability,Workflow,Workflow,145,"RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed; **Describe the issue:**. Hello everyone, i am trying to run a Pacbio Workflow with deepvariant in it but i get an error in the make example step ( Rule and log below) i allready have an open Issue on the Workflow but we are at the Point that we think its ether Nucleus or Tensorflow that produces the error PacificBiosciences/HiFiTargetEnrichment#4 , since i cant find what the error is and how to fix it i opend the Issue. Many thanks in advance. **Setup**. - Operating system: Ubuntu 20.04.6 LTS. - DeepVariant version: 1.5.0. - Tensorflow 2.11.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: PacBio HIFI reads. **Steps to reproduce:**. ```. rule deepvariant_make_examples:. input:. bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",. bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",. reference=config[""ref""][""fasta""],. output:. tfrecord=temp(. f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz"". ),. nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>. log:. f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",. benchmark:. f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv"". container:. f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channel",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:190,usability,error,error,190,"RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed; **Describe the issue:**. Hello everyone, i am trying to run a Pacbio Workflow with deepvariant in it but i get an error in the make example step ( Rule and log below) i allready have an open Issue on the Workflow but we are at the Point that we think its ether Nucleus or Tensorflow that produces the error PacificBiosciences/HiFiTargetEnrichment#4 , since i cant find what the error is and how to fix it i opend the Issue. Many thanks in advance. **Setup**. - Operating system: Ubuntu 20.04.6 LTS. - DeepVariant version: 1.5.0. - Tensorflow 2.11.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: PacBio HIFI reads. **Steps to reproduce:**. ```. rule deepvariant_make_examples:. input:. bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",. bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",. reference=config[""ref""][""fasta""],. output:. tfrecord=temp(. f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz"". ),. nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>. log:. f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",. benchmark:. f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv"". container:. f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channel",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:280,usability,Workflow,Workflow,280,"RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed; **Describe the issue:**. Hello everyone, i am trying to run a Pacbio Workflow with deepvariant in it but i get an error in the make example step ( Rule and log below) i allready have an open Issue on the Workflow but we are at the Point that we think its ether Nucleus or Tensorflow that produces the error PacificBiosciences/HiFiTargetEnrichment#4 , since i cant find what the error is and how to fix it i opend the Issue. Many thanks in advance. **Setup**. - Operating system: Ubuntu 20.04.6 LTS. - DeepVariant version: 1.5.0. - Tensorflow 2.11.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: PacBio HIFI reads. **Steps to reproduce:**. ```. rule deepvariant_make_examples:. input:. bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",. bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",. reference=config[""ref""][""fasta""],. output:. tfrecord=temp(. f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz"". ),. nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>. log:. f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",. benchmark:. f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv"". container:. f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channel",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:377,usability,error,error,377,"RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed; **Describe the issue:**. Hello everyone, i am trying to run a Pacbio Workflow with deepvariant in it but i get an error in the make example step ( Rule and log below) i allready have an open Issue on the Workflow but we are at the Point that we think its ether Nucleus or Tensorflow that produces the error PacificBiosciences/HiFiTargetEnrichment#4 , since i cant find what the error is and how to fix it i opend the Issue. Many thanks in advance. **Setup**. - Operating system: Ubuntu 20.04.6 LTS. - DeepVariant version: 1.5.0. - Tensorflow 2.11.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: PacBio HIFI reads. **Steps to reproduce:**. ```. rule deepvariant_make_examples:. input:. bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",. bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",. reference=config[""ref""][""fasta""],. output:. tfrecord=temp(. f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz"". ),. nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>. log:. f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",. benchmark:. f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv"". container:. f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channel",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:454,usability,error,error,454,"RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed; **Describe the issue:**. Hello everyone, i am trying to run a Pacbio Workflow with deepvariant in it but i get an error in the make example step ( Rule and log below) i allready have an open Issue on the Workflow but we are at the Point that we think its ether Nucleus or Tensorflow that produces the error PacificBiosciences/HiFiTargetEnrichment#4 , since i cant find what the error is and how to fix it i opend the Issue. Many thanks in advance. **Setup**. - Operating system: Ubuntu 20.04.6 LTS. - DeepVariant version: 1.5.0. - Tensorflow 2.11.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: PacBio HIFI reads. **Steps to reproduce:**. ```. rule deepvariant_make_examples:. input:. bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",. bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",. reference=config[""ref""][""fasta""],. output:. tfrecord=temp(. f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz"". ),. nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>. log:. f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",. benchmark:. f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv"". container:. f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channel",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:794,usability,input,input,794,"RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed; **Describe the issue:**. Hello everyone, i am trying to run a Pacbio Workflow with deepvariant in it but i get an error in the make example step ( Rule and log below) i allready have an open Issue on the Workflow but we are at the Point that we think its ether Nucleus or Tensorflow that produces the error PacificBiosciences/HiFiTargetEnrichment#4 , since i cant find what the error is and how to fix it i opend the Issue. Many thanks in advance. **Setup**. - Operating system: Ubuntu 20.04.6 LTS. - DeepVariant version: 1.5.0. - Tensorflow 2.11.0. - Installation method (Docker, built from source, etc.): singularity. - Type of data: PacBio HIFI reads. **Steps to reproduce:**. ```. rule deepvariant_make_examples:. input:. bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",. bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",. reference=config[""ref""][""fasta""],. output:. tfrecord=temp(. f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz"". ),. nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>. log:. f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",. benchmark:. f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv"". container:. f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channel",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:1870,usability,input,input,1870,"batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",. reference=config[""ref""][""fasta""],. output:. tfrecord=temp(. f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz"". ),. nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>. log:. f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",. benchmark:. f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv"". container:. f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channels \. --min_mapping_quality=1 \. --parse_sam_aux_fields \. --partition_size=25000 \. --max_reads_per_partition=600 \. --phase_reads \. --pileup_image_width {params.pileup_image_width} \. --norealign_reads \. --sort_by_haplotypes \. --track_ref_reads \. --vsc_min_fraction_indels {params.vsc_min_fraction_indels} \. --mode calling \. --ref {input.reference} \. --reads {input.bam} \. --examples {params.examples} \. --gvcf {params.gvcf} \. --task {params.shard}) > {log} 2>&1. """""". ```. Error:. ```. 2023-07-12 15:19:55.926661: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI De>. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-07-12 15:19:59.038994: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.6 SO:>. 2023-07-12 15:19:5",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:2339,usability,input,input,2339,"5}.log"",. benchmark:. f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv"". container:. f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channels \. --min_mapping_quality=1 \. --parse_sam_aux_fields \. --partition_size=25000 \. --max_reads_per_partition=600 \. --phase_reads \. --pileup_image_width {params.pileup_image_width} \. --norealign_reads \. --sort_by_haplotypes \. --track_ref_reads \. --vsc_min_fraction_indels {params.vsc_min_fraction_indels} \. --mode calling \. --ref {input.reference} \. --reads {input.bam} \. --examples {params.examples} \. --gvcf {params.gvcf} \. --task {params.shard}) > {log} 2>&1. """""". ```. Error:. ```. 2023-07-12 15:19:55.926661: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI De>. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-07-12 15:19:59.038994: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.6 SO:>. 2023-07-12 15:19:59.039047: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag BC: in RG line, ignoring: @RG ID:d98f52ac>. 2023-07-12 15:19:59.039065: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG ID:d98f52ac>. I0712 15:19:59.039340 140472429422400 genomics_reader.py:222] Reading batches/Test349/D18757/aligned/D18757.hs37d5.bam with NativeS>. I0712 15:19:59.044630 140472429422400 make_examples_core.py:257] Task 32/96: Preparing inputs. 20",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:2368,usability,input,input,2368,"es/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv"". container:. f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channels \. --min_mapping_quality=1 \. --parse_sam_aux_fields \. --partition_size=25000 \. --max_reads_per_partition=600 \. --phase_reads \. --pileup_image_width {params.pileup_image_width} \. --norealign_reads \. --sort_by_haplotypes \. --track_ref_reads \. --vsc_min_fraction_indels {params.vsc_min_fraction_indels} \. --mode calling \. --ref {input.reference} \. --reads {input.bam} \. --examples {params.examples} \. --gvcf {params.gvcf} \. --task {params.shard}) > {log} 2>&1. """""". ```. Error:. ```. 2023-07-12 15:19:55.926661: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI De>. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-07-12 15:19:59.038994: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.6 SO:>. 2023-07-12 15:19:59.039047: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag BC: in RG line, ignoring: @RG ID:d98f52ac>. 2023-07-12 15:19:59.039065: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG ID:d98f52ac>. I0712 15:19:59.039340 140472429422400 genomics_reader.py:222] Reading batches/Test349/D18757/aligned/D18757.hs37d5.bam with NativeS>. I0712 15:19:59.044630 140472429422400 make_examples_core.py:257] Task 32/96: Preparing inputs. 2023-07-12 15:19:59.049074: W t",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:2485,usability,Error,Error,2485,"nt:{config['DEEPVARIANT_VERSION']}"". params:. vsc_min_fraction_indels=""0.12"",. pileup_image_width=199,. shard='{shard}',. examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",. gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",. message:. ""DeepVariant make_examples {wildcards.shard} for {input.bam}."". shell:. """""". sleep 180; (/opt/deepvariant/bin/make_examples \. --add_hp_channel \. --alt_aligned_pileup=diff_channels \. --min_mapping_quality=1 \. --parse_sam_aux_fields \. --partition_size=25000 \. --max_reads_per_partition=600 \. --phase_reads \. --pileup_image_width {params.pileup_image_width} \. --norealign_reads \. --sort_by_haplotypes \. --track_ref_reads \. --vsc_min_fraction_indels {params.vsc_min_fraction_indels} \. --mode calling \. --ref {input.reference} \. --reads {input.bam} \. --examples {params.examples} \. --gvcf {params.gvcf} \. --task {params.shard}) > {log} 2>&1. """""". ```. Error:. ```. 2023-07-12 15:19:55.926661: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI De>. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-07-12 15:19:59.038994: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.6 SO:>. 2023-07-12 15:19:59.039047: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag BC: in RG line, ignoring: @RG ID:d98f52ac>. 2023-07-12 15:19:59.039065: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG ID:d98f52ac>. I0712 15:19:59.039340 140472429422400 genomics_reader.py:222] Reading batches/Test349/D18757/aligned/D18757.hs37d5.bam with NativeS>. I0712 15:19:59.044630 140472429422400 make_examples_core.py:257] Task 32/96: Preparing inputs. 2023-07-12 15:19:59.049074: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.6 SO:>. 2023-07-12 15:19:",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:3332,usability,input,inputs,3332,"ef {input.reference} \. --reads {input.bam} \. --examples {params.examples} \. --gvcf {params.gvcf} \. --task {params.shard}) > {log} 2>&1. """""". ```. Error:. ```. 2023-07-12 15:19:55.926661: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI De>. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-07-12 15:19:59.038994: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.6 SO:>. 2023-07-12 15:19:59.039047: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag BC: in RG line, ignoring: @RG ID:d98f52ac>. 2023-07-12 15:19:59.039065: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG ID:d98f52ac>. I0712 15:19:59.039340 140472429422400 genomics_reader.py:222] Reading batches/Test349/D18757/aligned/D18757.hs37d5.bam with NativeS>. I0712 15:19:59.044630 140472429422400 make_examples_core.py:257] Task 32/96: Preparing inputs. 2023-07-12 15:19:59.049074: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.6 SO:>. 2023-07-12 15:19:59.049123: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag BC: in RG line, ignoring: @RG ID:d98f52ac>. 2023-07-12 15:19:59.049140: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG ID:d98f52ac>. I0712 15:19:59.049325 140472429422400 genomics_reader.py:222] Reading batches/Test349/D18757/aligned/D18757.hs37d5.bam with NativeS>. I0712 15:19:59.059888 140472429422400 make_examples_core.py:257] Task 32/96: Common contigs are ['1', '2', '3', '4', '5', '6', '7',>. I0712 15:19:59.247122 140472429422400 make_examples_core.py:257] Task 32/96: Starting from v0.9.0, --use_ref_for_cram is default to>. 2023-07-12 15:19:59.247444: I third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728. 2023-07-12 15:19:59.364871: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in he",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/677:5652,usability,input,inputs,5652,"72429422400 genomics_reader.py:222] Reading batches/Test349/D18757/aligned/D18757.hs37d5.bam with NativeS>. 2023-07-12 15:19:59.535088: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.6 SO:>. 2023-07-12 15:19:59.535146: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag BC: in RG line, ignoring: @RG ID:d98f52ac>. 2023-07-12 15:19:59.535166: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG ID:d98f52ac>. I0712 15:19:59.535452 140472429422400 genomics_reader.py:222] Reading batches/Test349/D18757/aligned/D18757.hs37d5.bam with NativeS>. I0712 15:19:59.536432 140472429422400 make_examples_core.py:257] Task 32/96: Writing gvcf records to batches/Test349/D18757/deepvar>. I0712 15:19:59.536834 140472429422400 make_examples_core.py:257] Task 32/96: Writing examples to batches/Test349/D18757/deepvariant>. I0712 15:19:59.536924 140472429422400 make_examples_core.py:257] Task 32/96: Overhead for preparing inputs: 0 seconds. 2023-07-12 15:19:59.545470: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicM>. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main. make_examples_core.make_examples_runner(options). File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2183, in make_example>. runtimes) = region_processor.process(region). File ""/tmp/Bazel.runfiles_ch76vyw0/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1285, ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/677
https://github.com/google/deepvariant/issues/678:167,availability,error,error,167,"could not open 'file' when using signularity to run deepvariant; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. Yes. Same error msgs were observed. But I was lunching deepvariant with singularity. **Describe the issue:**. (A clear and concise description of what the issue is.). The same error msgs were observed just like described in FAQ. But this time I was lunching deepvariant and testing dataset with singularity. **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). module load singularity. BIN_VERSION=""1.5.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". LABASE=""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools"". INPUT_DIR=""${LABASE}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". OUTPUT_DIR=""${LABASE}/quickstart-output"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ls -1 ${INPUT_DIR}. mkdir -p ${OUTPUT_DIR}. singularity run -B /usr/lib/locale/:/usr/lib/locale",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:333,availability,error,error,333,"could not open 'file' when using signularity to run deepvariant; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. Yes. Same error msgs were observed. But I was lunching deepvariant with singularity. **Describe the issue:**. (A clear and concise description of what the issue is.). The same error msgs were observed just like described in FAQ. But this time I was lunching deepvariant and testing dataset with singularity. **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). module load singularity. BIN_VERSION=""1.5.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". LABASE=""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools"". INPUT_DIR=""${LABASE}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". OUTPUT_DIR=""${LABASE}/quickstart-output"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ls -1 ${INPUT_DIR}. mkdir -p ${OUTPUT_DIR}. singularity run -B /usr/lib/locale/:/usr/lib/locale",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:478,availability,Operat,Operating,478,"could not open 'file' when using signularity to run deepvariant; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. Yes. Same error msgs were observed. But I was lunching deepvariant with singularity. **Describe the issue:**. (A clear and concise description of what the issue is.). The same error msgs were observed just like described in FAQ. But this time I was lunching deepvariant and testing dataset with singularity. **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). module load singularity. BIN_VERSION=""1.5.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". LABASE=""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools"". INPUT_DIR=""${LABASE}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". OUTPUT_DIR=""${LABASE}/quickstart-output"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ls -1 ${INPUT_DIR}. mkdir -p ${OUTPUT_DIR}. singularity run -B /usr/lib/locale/:/usr/lib/locale",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:728,availability,Error,Error,728,"could not open 'file' when using signularity to run deepvariant; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. Yes. Same error msgs were observed. But I was lunching deepvariant with singularity. **Describe the issue:**. (A clear and concise description of what the issue is.). The same error msgs were observed just like described in FAQ. But this time I was lunching deepvariant and testing dataset with singularity. **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). module load singularity. BIN_VERSION=""1.5.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". LABASE=""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools"". INPUT_DIR=""${LABASE}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". OUTPUT_DIR=""${LABASE}/quickstart-output"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ls -1 ${INPUT_DIR}. mkdir -p ${OUTPUT_DIR}. singularity run -B /usr/lib/locale/:/usr/lib/locale",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:2620,availability,error,error,2620,"A_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ls -1 ${INPUT_DIR}. mkdir -p ${OUTPUT_DIR}. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --num_shards=1. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Here is the complete error msg:. #############################################. **Any additional context:**. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). 2023-07-13 21:50:44.574140: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. [E::hts_open_format] Failed to open file ""/N/project/Walk",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:3452,availability,operat,operations,3452,"//github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Here is the complete error msg:. #############################################. **Any additional context:**. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). 2023-07-13 21:50:44.574140: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. [E::hts_open_format] Failed to open file ""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 182, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 133, in default_options. samples_in_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:3498,availability,operat,operations,3498,"s/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Here is the complete error msg:. #############################################. **Any additional context:**. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). 2023-07-13 21:50:44.574140: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. [E::hts_open_format] Failed to open file ""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 182, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 133, in default_options. samples_in_order, sample_role_to_train = one_sample_from_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:183,deployability,observ,observed,183,"could not open 'file' when using signularity to run deepvariant; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. Yes. Same error msgs were observed. But I was lunching deepvariant with singularity. **Describe the issue:**. (A clear and concise description of what the issue is.). The same error msgs were observed just like described in FAQ. But this time I was lunching deepvariant and testing dataset with singularity. **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). module load singularity. BIN_VERSION=""1.5.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". LABASE=""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools"". INPUT_DIR=""${LABASE}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". OUTPUT_DIR=""${LABASE}/quickstart-output"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ls -1 ${INPUT_DIR}. mkdir -p ${OUTPUT_DIR}. singularity run -B /usr/lib/locale/:/usr/lib/locale",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:349,deployability,observ,observed,349,"could not open 'file' when using signularity to run deepvariant; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. Yes. Same error msgs were observed. But I was lunching deepvariant with singularity. **Describe the issue:**. (A clear and concise description of what the issue is.). The same error msgs were observed just like described in FAQ. But this time I was lunching deepvariant and testing dataset with singularity. **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). module load singularity. BIN_VERSION=""1.5.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". LABASE=""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools"". INPUT_DIR=""${LABASE}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". OUTPUT_DIR=""${LABASE}/quickstart-output"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ls -1 ${INPUT_DIR}. mkdir -p ${OUTPUT_DIR}. singularity run -B /usr/lib/locale/:/usr/lib/locale",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:511,deployability,version,version,511,"could not open 'file' when using signularity to run deepvariant; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. Yes. Same error msgs were observed. But I was lunching deepvariant with singularity. **Describe the issue:**. (A clear and concise description of what the issue is.). The same error msgs were observed just like described in FAQ. But this time I was lunching deepvariant and testing dataset with singularity. **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). module load singularity. BIN_VERSION=""1.5.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". LABASE=""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools"". INPUT_DIR=""${LABASE}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". OUTPUT_DIR=""${LABASE}/quickstart-output"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ls -1 ${INPUT_DIR}. mkdir -p ${OUTPUT_DIR}. singularity run -B /usr/lib/locale/:/usr/lib/locale",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:523,deployability,Instal,Installation,523,"could not open 'file' when using signularity to run deepvariant; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. Yes. Same error msgs were observed. But I was lunching deepvariant with singularity. **Describe the issue:**. (A clear and concise description of what the issue is.). The same error msgs were observed just like described in FAQ. But this time I was lunching deepvariant and testing dataset with singularity. **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). module load singularity. BIN_VERSION=""1.5.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". LABASE=""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools"". INPUT_DIR=""${LABASE}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". OUTPUT_DIR=""${LABASE}/quickstart-output"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ls -1 ${INPUT_DIR}. mkdir -p ${OUTPUT_DIR}. singularity run -B /usr/lib/locale/:/usr/lib/locale",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:758,deployability,modul,module,758,"could not open 'file' when using signularity to run deepvariant; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. Yes. Same error msgs were observed. But I was lunching deepvariant with singularity. **Describe the issue:**. (A clear and concise description of what the issue is.). The same error msgs were observed just like described in FAQ. But this time I was lunching deepvariant and testing dataset with singularity. **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). module load singularity. BIN_VERSION=""1.5.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". LABASE=""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools"". INPUT_DIR=""${LABASE}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". OUTPUT_DIR=""${LABASE}/quickstart-output"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ls -1 ${INPUT_DIR}. mkdir -p ${OUTPUT_DIR}. singularity run -B /usr/lib/locale/:/usr/lib/locale",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:2738,deployability,fail,failed,2738,"a.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ls -1 ${INPUT_DIR}. mkdir -p ${OUTPUT_DIR}. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --num_shards=1. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Here is the complete error msg:. #############################################. **Any additional context:**. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). 2023-07-13 21:50:44.574140: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. [E::hts_open_format] Failed to open file ""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" : No such file or di",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:2882,deployability,instal,installed,2882,"est.fasta.gz.gzi. ls -1 ${INPUT_DIR}. mkdir -p ${OUTPUT_DIR}. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --num_shards=1. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Here is the complete error msg:. #############################################. **Any additional context:**. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). 2023-07-13 21:50:44.574140: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. [E::hts_open_format] Failed to open file ""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", lin",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:2996,deployability,fail,failed,2996,"e/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --num_shards=1. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Here is the complete error msg:. #############################################. **Any additional context:**. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). 2023-07-13 21:50:44.574140: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. [E::hts_open_format] Failed to open file ""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/absl_py/absl/app.py"", line 312, i",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:3140,deployability,instal,installed,3140,"9.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --num_shards=1. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Here is the complete error msg:. #############################################. **Any additional context:**. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). 2023-07-13 21:50:44.574140: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. [E::hts_open_format] Failed to open file ""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:3587,deployability,Fail,Failed,3587,"k start? Here is the complete error msg:. #############################################. **Any additional context:**. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). 2023-07-13 21:50:44.574140: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. [E::hts_open_format] Failed to open file ""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 182, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 133, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:3898,deployability,modul,module,3898,"ystem. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). 2023-07-13 21:50:44.574140: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. [E::hts_open_format] Failed to open file ""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 182, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 133, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 88, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 147, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. F",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:5606,deployability,fail,failed,5606,"ject/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 182, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 133, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 88, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 147, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 260, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: NOT_FOUND: Could not open /N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. parallel: This job failed:.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:765,energy efficiency,load,load,765,"could not open 'file' when using signularity to run deepvariant; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. Yes. Same error msgs were observed. But I was lunching deepvariant with singularity. **Describe the issue:**. (A clear and concise description of what the issue is.). The same error msgs were observed just like described in FAQ. But this time I was lunching deepvariant and testing dataset with singularity. **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). module load singularity. BIN_VERSION=""1.5.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". LABASE=""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools"". INPUT_DIR=""${LABASE}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". OUTPUT_DIR=""${LABASE}/quickstart-output"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ls -1 ${INPUT_DIR}. mkdir -p ${OUTPUT_DIR}. singularity run -B /usr/lib/locale/:/usr/lib/locale",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:3265,energy efficiency,core,core,3265," \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --num_shards=1. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Here is the complete error msg:. #############################################. **Any additional context:**. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). 2023-07-13 21:50:44.574140: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. [E::hts_open_format] Failed to open file ""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 182, in main. options =",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:3331,energy efficiency,optim,optimized,3331,"TPUT_DIR}""/output.g.vcf.gz \. --num_shards=1. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Here is the complete error msg:. #############################################. **Any additional context:**. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). 2023-07-13 21:50:44.574140: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. [E::hts_open_format] Failed to open file ""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 182, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.r",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:3411,energy efficiency,CPU,CPU,3411,"k on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Here is the complete error msg:. #############################################. **Any additional context:**. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). 2023-07-13 21:50:44.574140: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. [E::hts_open_format] Failed to open file ""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 182, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:511,integrability,version,version,511,"could not open 'file' when using signularity to run deepvariant; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. Yes. Same error msgs were observed. But I was lunching deepvariant with singularity. **Describe the issue:**. (A clear and concise description of what the issue is.). The same error msgs were observed just like described in FAQ. But this time I was lunching deepvariant and testing dataset with singularity. **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). module load singularity. BIN_VERSION=""1.5.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". LABASE=""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools"". INPUT_DIR=""${LABASE}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". OUTPUT_DIR=""${LABASE}/quickstart-output"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ls -1 ${INPUT_DIR}. mkdir -p ${OUTPUT_DIR}. singularity run -B /usr/lib/locale/:/usr/lib/locale",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:2943,interoperability,standard,standard,2943,". singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --num_shards=1. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Here is the complete error msg:. #############################################. **Any additional context:**. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). 2023-07-13 21:50:44.574140: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. [E::hts_open_format] Failed to open file ""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:3201,interoperability,standard,standard,3201,"chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --num_shards=1. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Here is the complete error msg:. #############################################. **Any additional context:**. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). 2023-07-13 21:50:44.574140: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. [E::hts_open_format] Failed to open file ""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvaria",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:3270,interoperability,platform,platform,3270,"utput_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --num_shards=1. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Here is the complete error msg:. #############################################. **Any additional context:**. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). 2023-07-13 21:50:44.574140: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. [E::hts_open_format] Failed to open file ""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 182, in main. options = defaul",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:511,modifiability,version,version,511,"could not open 'file' when using signularity to run deepvariant; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. Yes. Same error msgs were observed. But I was lunching deepvariant with singularity. **Describe the issue:**. (A clear and concise description of what the issue is.). The same error msgs were observed just like described in FAQ. But this time I was lunching deepvariant and testing dataset with singularity. **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). module load singularity. BIN_VERSION=""1.5.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". LABASE=""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools"". INPUT_DIR=""${LABASE}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". OUTPUT_DIR=""${LABASE}/quickstart-output"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ls -1 ${INPUT_DIR}. mkdir -p ${OUTPUT_DIR}. singularity run -B /usr/lib/locale/:/usr/lib/locale",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:758,modifiability,modul,module,758,"could not open 'file' when using signularity to run deepvariant; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. Yes. Same error msgs were observed. But I was lunching deepvariant with singularity. **Describe the issue:**. (A clear and concise description of what the issue is.). The same error msgs were observed just like described in FAQ. But this time I was lunching deepvariant and testing dataset with singularity. **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). module load singularity. BIN_VERSION=""1.5.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". LABASE=""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools"". INPUT_DIR=""${LABASE}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". OUTPUT_DIR=""${LABASE}/quickstart-output"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ls -1 ${INPUT_DIR}. mkdir -p ${OUTPUT_DIR}. singularity run -B /usr/lib/locale/:/usr/lib/locale",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:2106,modifiability,PAC,PACBIO,2106,"-p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ls -1 ${INPUT_DIR}. mkdir -p ${OUTPUT_DIR}. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --num_shards=1. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Here is the complete error msg:. #############################################. **Any additional context:**. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""e",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:3898,modifiability,modul,module,3898,"ystem. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). 2023-07-13 21:50:44.574140: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. [E::hts_open_format] Failed to open file ""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 182, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 133, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 88, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 147, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. F",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:167,performance,error,error,167,"could not open 'file' when using signularity to run deepvariant; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. Yes. Same error msgs were observed. But I was lunching deepvariant with singularity. **Describe the issue:**. (A clear and concise description of what the issue is.). The same error msgs were observed just like described in FAQ. But this time I was lunching deepvariant and testing dataset with singularity. **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). module load singularity. BIN_VERSION=""1.5.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". LABASE=""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools"". INPUT_DIR=""${LABASE}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". OUTPUT_DIR=""${LABASE}/quickstart-output"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ls -1 ${INPUT_DIR}. mkdir -p ${OUTPUT_DIR}. singularity run -B /usr/lib/locale/:/usr/lib/locale",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:333,performance,error,error,333,"could not open 'file' when using signularity to run deepvariant; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. Yes. Same error msgs were observed. But I was lunching deepvariant with singularity. **Describe the issue:**. (A clear and concise description of what the issue is.). The same error msgs were observed just like described in FAQ. But this time I was lunching deepvariant and testing dataset with singularity. **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). module load singularity. BIN_VERSION=""1.5.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". LABASE=""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools"". INPUT_DIR=""${LABASE}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". OUTPUT_DIR=""${LABASE}/quickstart-output"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ls -1 ${INPUT_DIR}. mkdir -p ${OUTPUT_DIR}. singularity run -B /usr/lib/locale/:/usr/lib/locale",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:395,performance,time,time,395,"could not open 'file' when using signularity to run deepvariant; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. Yes. Same error msgs were observed. But I was lunching deepvariant with singularity. **Describe the issue:**. (A clear and concise description of what the issue is.). The same error msgs were observed just like described in FAQ. But this time I was lunching deepvariant and testing dataset with singularity. **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). module load singularity. BIN_VERSION=""1.5.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". LABASE=""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools"". INPUT_DIR=""${LABASE}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". OUTPUT_DIR=""${LABASE}/quickstart-output"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ls -1 ${INPUT_DIR}. mkdir -p ${OUTPUT_DIR}. singularity run -B /usr/lib/locale/:/usr/lib/locale",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:728,performance,Error,Error,728,"could not open 'file' when using signularity to run deepvariant; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. Yes. Same error msgs were observed. But I was lunching deepvariant with singularity. **Describe the issue:**. (A clear and concise description of what the issue is.). The same error msgs were observed just like described in FAQ. But this time I was lunching deepvariant and testing dataset with singularity. **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). module load singularity. BIN_VERSION=""1.5.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". LABASE=""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools"". INPUT_DIR=""${LABASE}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". OUTPUT_DIR=""${LABASE}/quickstart-output"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ls -1 ${INPUT_DIR}. mkdir -p ${OUTPUT_DIR}. singularity run -B /usr/lib/locale/:/usr/lib/locale",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:765,performance,load,load,765,"could not open 'file' when using signularity to run deepvariant; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. Yes. Same error msgs were observed. But I was lunching deepvariant with singularity. **Describe the issue:**. (A clear and concise description of what the issue is.). The same error msgs were observed just like described in FAQ. But this time I was lunching deepvariant and testing dataset with singularity. **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). module load singularity. BIN_VERSION=""1.5.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". LABASE=""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools"". INPUT_DIR=""${LABASE}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". OUTPUT_DIR=""${LABASE}/quickstart-output"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ls -1 ${INPUT_DIR}. mkdir -p ${OUTPUT_DIR}. singularity run -B /usr/lib/locale/:/usr/lib/locale",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:2620,performance,error,error,2620,"A_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ls -1 ${INPUT_DIR}. mkdir -p ${OUTPUT_DIR}. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --num_shards=1. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Here is the complete error msg:. #############################################. **Any additional context:**. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). 2023-07-13 21:50:44.574140: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. [E::hts_open_format] Failed to open file ""/N/project/Walk",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:3331,performance,optimiz,optimized,3331,"TPUT_DIR}""/output.g.vcf.gz \. --num_shards=1. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Here is the complete error msg:. #############################################. **Any additional context:**. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). 2023-07-13 21:50:44.574140: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. [E::hts_open_format] Failed to open file ""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 182, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.r",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:3365,performance,Network,Network,3365,"um_shards=1. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Here is the complete error msg:. #############################################. **Any additional context:**. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). 2023-07-13 21:50:44.574140: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. [E::hts_open_format] Failed to open file ""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 182, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_goo",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:3411,performance,CPU,CPU,3411,"k on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Here is the complete error msg:. #############################################. **Any additional context:**. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). 2023-07-13 21:50:44.574140: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. [E::hts_open_format] Failed to open file ""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 182, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:3431,performance,perform,performance-critical,3431,"test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Here is the complete error msg:. #############################################. **Any additional context:**. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). 2023-07-13 21:50:44.574140: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. [E::hts_open_format] Failed to open file ""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 182, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 133, in default_opti",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:5587,performance,parallel,parallel,5587,"ject/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 182, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 133, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 88, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 147, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 260, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: NOT_FOUND: Could not open /N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. parallel: This job failed:.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:2384,reliability,Doe,Does,2384,"P_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ls -1 ${INPUT_DIR}. mkdir -p ${OUTPUT_DIR}. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --num_shards=1. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Here is the complete error msg:. #############################################. **Any additional context:**. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). 2023-07-13 21:50:44.574140: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneD",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:2738,reliability,fail,failed,2738,"a.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ls -1 ${INPUT_DIR}. mkdir -p ${OUTPUT_DIR}. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --num_shards=1. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Here is the complete error msg:. #############################################. **Any additional context:**. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). 2023-07-13 21:50:44.574140: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. [E::hts_open_format] Failed to open file ""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" : No such file or di",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:2996,reliability,fail,failed,2996,"e/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --num_shards=1. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Here is the complete error msg:. #############################################. **Any additional context:**. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). 2023-07-13 21:50:44.574140: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. [E::hts_open_format] Failed to open file ""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/absl_py/absl/app.py"", line 312, i",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:3587,reliability,Fail,Failed,3587,"k start? Here is the complete error msg:. #############################################. **Any additional context:**. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). 2023-07-13 21:50:44.574140: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. [E::hts_open_format] Failed to open file ""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 182, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 133, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:5606,reliability,fail,failed,5606,"ject/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 182, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 133, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 88, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 147, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 260, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: NOT_FOUND: Could not open /N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. parallel: This job failed:.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:167,safety,error,error,167,"could not open 'file' when using signularity to run deepvariant; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. Yes. Same error msgs were observed. But I was lunching deepvariant with singularity. **Describe the issue:**. (A clear and concise description of what the issue is.). The same error msgs were observed just like described in FAQ. But this time I was lunching deepvariant and testing dataset with singularity. **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). module load singularity. BIN_VERSION=""1.5.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". LABASE=""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools"". INPUT_DIR=""${LABASE}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". OUTPUT_DIR=""${LABASE}/quickstart-output"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ls -1 ${INPUT_DIR}. mkdir -p ${OUTPUT_DIR}. singularity run -B /usr/lib/locale/:/usr/lib/locale",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:333,safety,error,error,333,"could not open 'file' when using signularity to run deepvariant; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. Yes. Same error msgs were observed. But I was lunching deepvariant with singularity. **Describe the issue:**. (A clear and concise description of what the issue is.). The same error msgs were observed just like described in FAQ. But this time I was lunching deepvariant and testing dataset with singularity. **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). module load singularity. BIN_VERSION=""1.5.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". LABASE=""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools"". INPUT_DIR=""${LABASE}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". OUTPUT_DIR=""${LABASE}/quickstart-output"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ls -1 ${INPUT_DIR}. mkdir -p ${OUTPUT_DIR}. singularity run -B /usr/lib/locale/:/usr/lib/locale",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:431,safety,test,testing,431,"could not open 'file' when using signularity to run deepvariant; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. Yes. Same error msgs were observed. But I was lunching deepvariant with singularity. **Describe the issue:**. (A clear and concise description of what the issue is.). The same error msgs were observed just like described in FAQ. But this time I was lunching deepvariant and testing dataset with singularity. **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). module load singularity. BIN_VERSION=""1.5.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". LABASE=""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools"". INPUT_DIR=""${LABASE}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". OUTPUT_DIR=""${LABASE}/quickstart-output"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ls -1 ${INPUT_DIR}. mkdir -p ${OUTPUT_DIR}. singularity run -B /usr/lib/locale/:/usr/lib/locale",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:728,safety,Error,Error,728,"could not open 'file' when using signularity to run deepvariant; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. Yes. Same error msgs were observed. But I was lunching deepvariant with singularity. **Describe the issue:**. (A clear and concise description of what the issue is.). The same error msgs were observed just like described in FAQ. But this time I was lunching deepvariant and testing dataset with singularity. **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). module load singularity. BIN_VERSION=""1.5.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". LABASE=""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools"". INPUT_DIR=""${LABASE}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". OUTPUT_DIR=""${LABASE}/quickstart-output"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ls -1 ${INPUT_DIR}. mkdir -p ${OUTPUT_DIR}. singularity run -B /usr/lib/locale/:/usr/lib/locale",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:758,safety,modul,module,758,"could not open 'file' when using signularity to run deepvariant; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. Yes. Same error msgs were observed. But I was lunching deepvariant with singularity. **Describe the issue:**. (A clear and concise description of what the issue is.). The same error msgs were observed just like described in FAQ. But this time I was lunching deepvariant and testing dataset with singularity. **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). module load singularity. BIN_VERSION=""1.5.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". LABASE=""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools"". INPUT_DIR=""${LABASE}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". OUTPUT_DIR=""${LABASE}/quickstart-output"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ls -1 ${INPUT_DIR}. mkdir -p ${OUTPUT_DIR}. singularity run -B /usr/lib/locale/:/usr/lib/locale",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:970,safety,test,testdata,970,"could not open 'file' when using signularity to run deepvariant; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. Yes. Same error msgs were observed. But I was lunching deepvariant with singularity. **Describe the issue:**. (A clear and concise description of what the issue is.). The same error msgs were observed just like described in FAQ. But this time I was lunching deepvariant and testing dataset with singularity. **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). module load singularity. BIN_VERSION=""1.5.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". LABASE=""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools"". INPUT_DIR=""${LABASE}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". OUTPUT_DIR=""${LABASE}/quickstart-output"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ls -1 ${INPUT_DIR}. mkdir -p ${OUTPUT_DIR}. singularity run -B /usr/lib/locale/:/usr/lib/locale",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:1050,safety,test,testdata,1050,"epvariant; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. Yes. Same error msgs were observed. But I was lunching deepvariant with singularity. **Describe the issue:**. (A clear and concise description of what the issue is.). The same error msgs were observed just like described in FAQ. But this time I was lunching deepvariant and testing dataset with singularity. **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). module load singularity. BIN_VERSION=""1.5.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". LABASE=""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools"". INPUT_DIR=""${LABASE}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". OUTPUT_DIR=""${LABASE}/quickstart-output"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ls -1 ${INPUT_DIR}. mkdir -p ${OUTPUT_DIR}. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:2405,safety,test,test,2405,"_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ls -1 ${INPUT_DIR}. mkdir -p ${OUTPUT_DIR}. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --num_shards=1. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Here is the complete error msg:. #############################################. **Any additional context:**. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). 2023-07-13 21:50:44.574140: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the follow",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:2441,safety,test,test,2441,"P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ls -1 ${INPUT_DIR}. mkdir -p ${OUTPUT_DIR}. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --num_shards=1. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Here is the complete error msg:. #############################################. **Any additional context:**. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). 2023-07-13 21:50:44.574140: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:2611,safety,compl,complete,2611,"} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ls -1 ${INPUT_DIR}. mkdir -p ${OUTPUT_DIR}. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --num_shards=1. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Here is the complete error msg:. #############################################. **Any additional context:**. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). 2023-07-13 21:50:44.574140: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. [E::hts_open_format] Failed to open file ""/N/proj",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:2620,safety,error,error,2620,"A_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ls -1 ${INPUT_DIR}. mkdir -p ${OUTPUT_DIR}. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --num_shards=1. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Here is the complete error msg:. #############################################. **Any additional context:**. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). 2023-07-13 21:50:44.574140: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. [E::hts_open_format] Failed to open file ""/N/project/Walk",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:3680,safety,test,testdata,3680," additional context:**. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). 2023-07-13 21:50:44.574140: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. [E::hts_open_format] Failed to open file ""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 182, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 133, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 88, in one_sample_from_flags. sample_name = make_examples_core.assign_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:3898,safety,modul,module,3898,"ystem. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). 2023-07-13 21:50:44.574140: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. [E::hts_open_format] Failed to open file ""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 182, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 133, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 88, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 147, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. F",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:5546,safety,test,testdata,5546,"ject/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 182, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 133, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 88, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 147, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 260, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: NOT_FOUND: Could not open /N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. parallel: This job failed:.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:33,security,sign,signularity,33,"could not open 'file' when using signularity to run deepvariant; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. Yes. Same error msgs were observed. But I was lunching deepvariant with singularity. **Describe the issue:**. (A clear and concise description of what the issue is.). The same error msgs were observed just like described in FAQ. But this time I was lunching deepvariant and testing dataset with singularity. **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). module load singularity. BIN_VERSION=""1.5.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". LABASE=""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools"". INPUT_DIR=""${LABASE}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". OUTPUT_DIR=""${LABASE}/quickstart-output"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ls -1 ${INPUT_DIR}. mkdir -p ${OUTPUT_DIR}. singularity run -B /usr/lib/locale/:/usr/lib/locale",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:2611,security,compl,complete,2611,"} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ls -1 ${INPUT_DIR}. mkdir -p ${OUTPUT_DIR}. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --num_shards=1. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Here is the complete error msg:. #############################################. **Any additional context:**. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). 2023-07-13 21:50:44.574140: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. [E::hts_open_format] Failed to open file ""/N/proj",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:3365,security,Network,Network,3365,"um_shards=1. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Here is the complete error msg:. #############################################. **Any additional context:**. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). 2023-07-13 21:50:44.574140: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. [E::hts_open_format] Failed to open file ""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 182, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_goo",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:183,testability,observ,observed,183,"could not open 'file' when using signularity to run deepvariant; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. Yes. Same error msgs were observed. But I was lunching deepvariant with singularity. **Describe the issue:**. (A clear and concise description of what the issue is.). The same error msgs were observed just like described in FAQ. But this time I was lunching deepvariant and testing dataset with singularity. **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). module load singularity. BIN_VERSION=""1.5.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". LABASE=""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools"". INPUT_DIR=""${LABASE}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". OUTPUT_DIR=""${LABASE}/quickstart-output"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ls -1 ${INPUT_DIR}. mkdir -p ${OUTPUT_DIR}. singularity run -B /usr/lib/locale/:/usr/lib/locale",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:349,testability,observ,observed,349,"could not open 'file' when using signularity to run deepvariant; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. Yes. Same error msgs were observed. But I was lunching deepvariant with singularity. **Describe the issue:**. (A clear and concise description of what the issue is.). The same error msgs were observed just like described in FAQ. But this time I was lunching deepvariant and testing dataset with singularity. **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). module load singularity. BIN_VERSION=""1.5.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". LABASE=""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools"". INPUT_DIR=""${LABASE}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". OUTPUT_DIR=""${LABASE}/quickstart-output"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ls -1 ${INPUT_DIR}. mkdir -p ${OUTPUT_DIR}. singularity run -B /usr/lib/locale/:/usr/lib/locale",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:431,testability,test,testing,431,"could not open 'file' when using signularity to run deepvariant; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. Yes. Same error msgs were observed. But I was lunching deepvariant with singularity. **Describe the issue:**. (A clear and concise description of what the issue is.). The same error msgs were observed just like described in FAQ. But this time I was lunching deepvariant and testing dataset with singularity. **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). module load singularity. BIN_VERSION=""1.5.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". LABASE=""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools"". INPUT_DIR=""${LABASE}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". OUTPUT_DIR=""${LABASE}/quickstart-output"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ls -1 ${INPUT_DIR}. mkdir -p ${OUTPUT_DIR}. singularity run -B /usr/lib/locale/:/usr/lib/locale",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:607,testability,instrument,instrument,607,"could not open 'file' when using signularity to run deepvariant; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. Yes. Same error msgs were observed. But I was lunching deepvariant with singularity. **Describe the issue:**. (A clear and concise description of what the issue is.). The same error msgs were observed just like described in FAQ. But this time I was lunching deepvariant and testing dataset with singularity. **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). module load singularity. BIN_VERSION=""1.5.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". LABASE=""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools"". INPUT_DIR=""${LABASE}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". OUTPUT_DIR=""${LABASE}/quickstart-output"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ls -1 ${INPUT_DIR}. mkdir -p ${OUTPUT_DIR}. singularity run -B /usr/lib/locale/:/usr/lib/locale",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:734,testability,trace,trace,734,"could not open 'file' when using signularity to run deepvariant; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. Yes. Same error msgs were observed. But I was lunching deepvariant with singularity. **Describe the issue:**. (A clear and concise description of what the issue is.). The same error msgs were observed just like described in FAQ. But this time I was lunching deepvariant and testing dataset with singularity. **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). module load singularity. BIN_VERSION=""1.5.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". LABASE=""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools"". INPUT_DIR=""${LABASE}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". OUTPUT_DIR=""${LABASE}/quickstart-output"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ls -1 ${INPUT_DIR}. mkdir -p ${OUTPUT_DIR}. singularity run -B /usr/lib/locale/:/usr/lib/locale",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:970,testability,test,testdata,970,"could not open 'file' when using signularity to run deepvariant; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. Yes. Same error msgs were observed. But I was lunching deepvariant with singularity. **Describe the issue:**. (A clear and concise description of what the issue is.). The same error msgs were observed just like described in FAQ. But this time I was lunching deepvariant and testing dataset with singularity. **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). module load singularity. BIN_VERSION=""1.5.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". LABASE=""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools"". INPUT_DIR=""${LABASE}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". OUTPUT_DIR=""${LABASE}/quickstart-output"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ls -1 ${INPUT_DIR}. mkdir -p ${OUTPUT_DIR}. singularity run -B /usr/lib/locale/:/usr/lib/locale",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:1050,testability,test,testdata,1050,"epvariant; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. Yes. Same error msgs were observed. But I was lunching deepvariant with singularity. **Describe the issue:**. (A clear and concise description of what the issue is.). The same error msgs were observed just like described in FAQ. But this time I was lunching deepvariant and testing dataset with singularity. **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). module load singularity. BIN_VERSION=""1.5.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". LABASE=""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools"". INPUT_DIR=""${LABASE}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". OUTPUT_DIR=""${LABASE}/quickstart-output"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ls -1 ${INPUT_DIR}. mkdir -p ${OUTPUT_DIR}. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:1580,testability,unit,unittest,1580,"e of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). module load singularity. BIN_VERSION=""1.5.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". LABASE=""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools"". INPUT_DIR=""${LABASE}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". OUTPUT_DIR=""${LABASE}/quickstart-output"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ls -1 ${INPUT_DIR}. mkdir -p ${OUTPUT_DIR}. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --num_shards=1. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using th",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:1652,testability,unit,unittest,1652,"at is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). module load singularity. BIN_VERSION=""1.5.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". LABASE=""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools"". INPUT_DIR=""${LABASE}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". OUTPUT_DIR=""${LABASE}/quickstart-output"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ls -1 ${INPUT_DIR}. mkdir -p ${OUTPUT_DIR}. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --num_shards=1. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Here is the complete error msg:. ########################",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:1728,testability,unit,unittest,1728,"r trace: (if applicable). module load singularity. BIN_VERSION=""1.5.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". LABASE=""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools"". INPUT_DIR=""${LABASE}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". OUTPUT_DIR=""${LABASE}/quickstart-output"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ls -1 ${INPUT_DIR}. mkdir -p ${OUTPUT_DIR}. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --num_shards=1. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Here is the complete error msg:. #############################################. **Any additional context:**. perl: warning: Setting l",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:1803,testability,unit,unittest,1803,"gularity pull docker://google/deepvariant:""${BIN_VERSION}"". LABASE=""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools"". INPUT_DIR=""${LABASE}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". OUTPUT_DIR=""${LABASE}/quickstart-output"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ls -1 ${INPUT_DIR}. mkdir -p ${OUTPUT_DIR}. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --num_shards=1. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Here is the complete error msg:. #############################################. **Any additional context:**. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGU",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:1882,testability,unit,unittest,1882,"Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools"". INPUT_DIR=""${LABASE}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". OUTPUT_DIR=""${LABASE}/quickstart-output"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ls -1 ${INPUT_DIR}. mkdir -p ${OUTPUT_DIR}. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --num_shards=1. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Here is the complete error msg:. #############################################. **Any additional context:**. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""en_US.UTF-8"". are supported and inst",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:2153,testability,unit,unittest,2153,"TP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ls -1 ${INPUT_DIR}. mkdir -p ${OUTPUT_DIR}. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --num_shards=1. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Here is the complete error msg:. #############################################. **Any additional context:**. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""en_US.UTF-8"". are supported and installed on your",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:2405,testability,test,test,2405,"_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ls -1 ${INPUT_DIR}. mkdir -p ${OUTPUT_DIR}. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --num_shards=1. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Here is the complete error msg:. #############################################. **Any additional context:**. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). 2023-07-13 21:50:44.574140: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the follow",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:2441,testability,test,test,2441,"P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ls -1 ${INPUT_DIR}. mkdir -p ${OUTPUT_DIR}. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --num_shards=1. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Here is the complete error msg:. #############################################. **Any additional context:**. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). 2023-07-13 21:50:44.574140: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:2696,testability,context,context,2696,"_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ls -1 ${INPUT_DIR}. mkdir -p ${OUTPUT_DIR}. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --num_shards=1. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Here is the complete error msg:. #############################################. **Any additional context:**. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). 2023-07-13 21:50:44.574140: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. [E::hts_open_format] Failed to open file ""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools/quickstart-testdata/NA12878_S1.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:3680,testability,test,testdata,3680," additional context:**. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). 2023-07-13 21:50:44.574140: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. [E::hts_open_format] Failed to open file ""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 182, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 133, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 88, in one_sample_from_flags. sample_name = make_examples_core.assign_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:3750,testability,Trace,Traceback,3750,"ning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). 2023-07-13 21:50:44.574140: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. [E::hts_open_format] Failed to open file ""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 182, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 133, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 88, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_de",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:5546,testability,test,testdata,5546,"ject/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 182, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 133, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 88, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 147, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 260, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: NOT_FOUND: Could not open /N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. parallel: This job failed:.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:167,usability,error,error,167,"could not open 'file' when using signularity to run deepvariant; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. Yes. Same error msgs were observed. But I was lunching deepvariant with singularity. **Describe the issue:**. (A clear and concise description of what the issue is.). The same error msgs were observed just like described in FAQ. But this time I was lunching deepvariant and testing dataset with singularity. **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). module load singularity. BIN_VERSION=""1.5.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". LABASE=""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools"". INPUT_DIR=""${LABASE}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". OUTPUT_DIR=""${LABASE}/quickstart-output"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ls -1 ${INPUT_DIR}. mkdir -p ${OUTPUT_DIR}. singularity run -B /usr/lib/locale/:/usr/lib/locale",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:270,usability,clear,clear,270,"could not open 'file' when using signularity to run deepvariant; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. Yes. Same error msgs were observed. But I was lunching deepvariant with singularity. **Describe the issue:**. (A clear and concise description of what the issue is.). The same error msgs were observed just like described in FAQ. But this time I was lunching deepvariant and testing dataset with singularity. **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). module load singularity. BIN_VERSION=""1.5.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". LABASE=""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools"". INPUT_DIR=""${LABASE}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". OUTPUT_DIR=""${LABASE}/quickstart-output"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ls -1 ${INPUT_DIR}. mkdir -p ${OUTPUT_DIR}. singularity run -B /usr/lib/locale/:/usr/lib/locale",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:333,usability,error,error,333,"could not open 'file' when using signularity to run deepvariant; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. Yes. Same error msgs were observed. But I was lunching deepvariant with singularity. **Describe the issue:**. (A clear and concise description of what the issue is.). The same error msgs were observed just like described in FAQ. But this time I was lunching deepvariant and testing dataset with singularity. **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). module load singularity. BIN_VERSION=""1.5.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". LABASE=""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools"". INPUT_DIR=""${LABASE}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". OUTPUT_DIR=""${LABASE}/quickstart-output"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ls -1 ${INPUT_DIR}. mkdir -p ${OUTPUT_DIR}. singularity run -B /usr/lib/locale/:/usr/lib/locale",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:716,usability,Command,Command,716,"could not open 'file' when using signularity to run deepvariant; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. Yes. Same error msgs were observed. But I was lunching deepvariant with singularity. **Describe the issue:**. (A clear and concise description of what the issue is.). The same error msgs were observed just like described in FAQ. But this time I was lunching deepvariant and testing dataset with singularity. **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). module load singularity. BIN_VERSION=""1.5.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". LABASE=""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools"". INPUT_DIR=""${LABASE}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". OUTPUT_DIR=""${LABASE}/quickstart-output"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ls -1 ${INPUT_DIR}. mkdir -p ${OUTPUT_DIR}. singularity run -B /usr/lib/locale/:/usr/lib/locale",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:728,usability,Error,Error,728,"could not open 'file' when using signularity to run deepvariant; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. Yes. Same error msgs were observed. But I was lunching deepvariant with singularity. **Describe the issue:**. (A clear and concise description of what the issue is.). The same error msgs were observed just like described in FAQ. But this time I was lunching deepvariant and testing dataset with singularity. **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). module load singularity. BIN_VERSION=""1.5.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". LABASE=""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools"". INPUT_DIR=""${LABASE}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". OUTPUT_DIR=""${LABASE}/quickstart-output"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ls -1 ${INPUT_DIR}. mkdir -p ${OUTPUT_DIR}. singularity run -B /usr/lib/locale/:/usr/lib/locale",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:930,usability,tool,tools,930,"could not open 'file' when using signularity to run deepvariant; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. Yes. Same error msgs were observed. But I was lunching deepvariant with singularity. **Describe the issue:**. (A clear and concise description of what the issue is.). The same error msgs were observed just like described in FAQ. But this time I was lunching deepvariant and testing dataset with singularity. **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**. - Command:. - Error trace: (if applicable). module load singularity. BIN_VERSION=""1.5.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". LABASE=""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools"". INPUT_DIR=""${LABASE}/quickstart-testdata"". DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". OUTPUT_DIR=""${LABASE}/quickstart-output"". mkdir -p ${INPUT_DIR}. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ls -1 ${INPUT_DIR}. mkdir -p ${OUTPUT_DIR}. singularity run -B /usr/lib/locale/:/usr/lib/locale",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:2620,usability,error,error,2620,"A_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai. wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. ls -1 ${INPUT_DIR}. mkdir -p ${OUTPUT_DIR}. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --num_shards=1. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Here is the complete error msg:. #############################################. **Any additional context:**. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). 2023-07-13 21:50:44.574140: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. [E::hts_open_format] Failed to open file ""/N/project/Walk",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:2868,usability,support,supported,2868,"19.chr20.unittest.fasta.gz.gzi. ls -1 ${INPUT_DIR}. mkdir -p ${OUTPUT_DIR}. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --num_shards=1. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Here is the complete error msg:. #############################################. **Any additional context:**. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). 2023-07-13 21:50:44.574140: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. [E::hts_open_format] Failed to open file ""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_exa",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:3126,usability,support,supported,3126,"DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --num_shards=1. **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Here is the complete error msg:. #############################################. **Any additional context:**. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). 2023-07-13 21:50:44.574140: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. [E::hts_open_format] Failed to open file ""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(mai",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:3431,usability,perform,performance-critical,3431,"test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Here is the complete error msg:. #############################################. **Any additional context:**. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). 2023-07-13 21:50:44.574140: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. [E::hts_open_format] Failed to open file ""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 182, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 133, in default_opti",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:3663,usability,tool,tools,3663,"###########. **Any additional context:**. perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). perl: warning: Setting locale failed. perl: warning: Please check that your locale settings:. LANGUAGE = (unset),. LC_ALL = (unset),. LANG = ""en_US.UTF-8"". are supported and installed on your system. perl: warning: Falling back to the standard locale (""C""). 2023-07-13 21:50:44.574140: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. [E::hts_open_format] Failed to open file ""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 182, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 133, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 88, in one_sample_from_flags. sample_name = make_exa",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/678:5529,usability,tool,tools,5529,"ject/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 182, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 133, in default_options. samples_in_order, sample_role_to_train = one_sample_from_flags(. File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 88, in one_sample_from_flags. sample_name = make_examples_core.assign_sample_name(. File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 147, in assign_sample_name. with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 260, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__. self._reader = sam_reader.SamReader.from_file(. ValueError: NOT_FOUND: Could not open /N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam. parallel: This job failed:.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/678
https://github.com/google/deepvariant/issues/679:0,availability,Error,Error,0,"Error running deepvariant from singularity in cluster; Dear all,. I was trying to run deepvariant from singularity in cluster, but I always meet same error, I don't know hou to fix it:. /share/app/singularity/3.8.1/bin/singularity exec --containall --bind /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/tmp:/tmp --bind /usr/lib/locale/:/usr/lib/locale/ --bind $ccsbam:$ccsbam --bind $ccsbam.bai:$ccsbam.bai --bind $fasta:$fasta --bind $fasta.fai:$fasta.fai --bind /hwfssz5/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/translatome/HCT116/PacBio/analysis/deepvariant.v3/T202302180201:/output /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/software/deepvariant/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=$fasta --reads=$ccsbam --output_vcf=/output/T202302180201.deepvariant.vcf.gz --output_gvcf=/output/T202302180201.deepvariant.g.vcf.gz --num_shards=20 --intermediate_results_dir=/tmp. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/call_variants_output.tfrecord.gz"" --examples ""/tmp/make_examples.tfrecord@20.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"". 2023-07-15 14:06:58.063861: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0715 14:07:10.199614 47821886322496 call_variants.py:317] From /tmp/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:46,availability,cluster,cluster,46,"Error running deepvariant from singularity in cluster; Dear all,. I was trying to run deepvariant from singularity in cluster, but I always meet same error, I don't know hou to fix it:. /share/app/singularity/3.8.1/bin/singularity exec --containall --bind /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/tmp:/tmp --bind /usr/lib/locale/:/usr/lib/locale/ --bind $ccsbam:$ccsbam --bind $ccsbam.bai:$ccsbam.bai --bind $fasta:$fasta --bind $fasta.fai:$fasta.fai --bind /hwfssz5/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/translatome/HCT116/PacBio/analysis/deepvariant.v3/T202302180201:/output /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/software/deepvariant/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=$fasta --reads=$ccsbam --output_vcf=/output/T202302180201.deepvariant.vcf.gz --output_gvcf=/output/T202302180201.deepvariant.g.vcf.gz --num_shards=20 --intermediate_results_dir=/tmp. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/call_variants_output.tfrecord.gz"" --examples ""/tmp/make_examples.tfrecord@20.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"". 2023-07-15 14:06:58.063861: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0715 14:07:10.199614 47821886322496 call_variants.py:317] From /tmp/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:118,availability,cluster,cluster,118,"Error running deepvariant from singularity in cluster; Dear all,. I was trying to run deepvariant from singularity in cluster, but I always meet same error, I don't know hou to fix it:. /share/app/singularity/3.8.1/bin/singularity exec --containall --bind /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/tmp:/tmp --bind /usr/lib/locale/:/usr/lib/locale/ --bind $ccsbam:$ccsbam --bind $ccsbam.bai:$ccsbam.bai --bind $fasta:$fasta --bind $fasta.fai:$fasta.fai --bind /hwfssz5/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/translatome/HCT116/PacBio/analysis/deepvariant.v3/T202302180201:/output /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/software/deepvariant/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=$fasta --reads=$ccsbam --output_vcf=/output/T202302180201.deepvariant.vcf.gz --output_gvcf=/output/T202302180201.deepvariant.g.vcf.gz --num_shards=20 --intermediate_results_dir=/tmp. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/call_variants_output.tfrecord.gz"" --examples ""/tmp/make_examples.tfrecord@20.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"". 2023-07-15 14:06:58.063861: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0715 14:07:10.199614 47821886322496 call_variants.py:317] From /tmp/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:150,availability,error,error,150,"Error running deepvariant from singularity in cluster; Dear all,. I was trying to run deepvariant from singularity in cluster, but I always meet same error, I don't know hou to fix it:. /share/app/singularity/3.8.1/bin/singularity exec --containall --bind /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/tmp:/tmp --bind /usr/lib/locale/:/usr/lib/locale/ --bind $ccsbam:$ccsbam --bind $ccsbam.bai:$ccsbam.bai --bind $fasta:$fasta --bind $fasta.fai:$fasta.fai --bind /hwfssz5/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/translatome/HCT116/PacBio/analysis/deepvariant.v3/T202302180201:/output /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/software/deepvariant/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=$fasta --reads=$ccsbam --output_vcf=/output/T202302180201.deepvariant.vcf.gz --output_gvcf=/output/T202302180201.deepvariant.g.vcf.gz --num_shards=20 --intermediate_results_dir=/tmp. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/call_variants_output.tfrecord.gz"" --examples ""/tmp/make_examples.tfrecord@20.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"". 2023-07-15 14:06:58.063861: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0715 14:07:10.199614 47821886322496 call_variants.py:317] From /tmp/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:1086,availability,checkpoint,checkpoint,1086,"ariant from singularity in cluster, but I always meet same error, I don't know hou to fix it:. /share/app/singularity/3.8.1/bin/singularity exec --containall --bind /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/tmp:/tmp --bind /usr/lib/locale/:/usr/lib/locale/ --bind $ccsbam:$ccsbam --bind $ccsbam.bai:$ccsbam.bai --bind $fasta:$fasta --bind $fasta.fai:$fasta.fai --bind /hwfssz5/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/translatome/HCT116/PacBio/analysis/deepvariant.v3/T202302180201:/output /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/software/deepvariant/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=$fasta --reads=$ccsbam --output_vcf=/output/T202302180201.deepvariant.vcf.gz --output_gvcf=/output/T202302180201.deepvariant.g.vcf.gz --num_shards=20 --intermediate_results_dir=/tmp. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/call_variants_output.tfrecord.gz"" --examples ""/tmp/make_examples.tfrecord@20.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"". 2023-07-15 14:06:58.063861: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0715 14:07:10.199614 47821886322496 call_variants.py:317] From /tmp/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instru",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:1388,availability,operat,operations,1388,".bai --bind $fasta:$fasta --bind $fasta.fai:$fasta.fai --bind /hwfssz5/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/translatome/HCT116/PacBio/analysis/deepvariant.v3/T202302180201:/output /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/software/deepvariant/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=$fasta --reads=$ccsbam --output_vcf=/output/T202302180201.deepvariant.vcf.gz --output_gvcf=/output/T202302180201.deepvariant.g.vcf.gz --num_shards=20 --intermediate_results_dir=/tmp. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/call_variants_output.tfrecord.gz"" --examples ""/tmp/make_examples.tfrecord@20.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"". 2023-07-15 14:06:58.063861: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0715 14:07:10.199614 47821886322496 call_variants.py:317] From /tmp/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-07-15 14:07:10.223654: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:2152,availability,operat,operations,2152," I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0715 14:07:10.199614 47821886322496 call_variants.py:317] From /tmp/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-07-15 14:07:10.223654: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. 2023-07-15 14:07:10.226851: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpl_zjylv7. W0715 14:07:10.308488 47821886322496 estimator.py:1864] Using temporary folder as model directory: /tmp/tmpl_zjylv7. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpl_zjylv7', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 10000. I0715 14:07:10.309390 47821886322496 estimator.py:202] Using config: {'_model_dir': '/tmp/tmpl_zjylv7', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_check",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:4172,availability,Restor,Restoring,4172," '_save_checkpoints_secs': 600, '_session_c. I0715 14:07:10.309847 47821886322496 call_variants.py:446] Writing calls to /tmp/call_variants_output.tfrecord.gz. INFO:tensorflow:Calling model_fn. I0715 14:07:10.758818 47821886322496 estimator.py:1173] Calling model_fn. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0715 14:07:16.873909 47821886322496 estimator.py:1175] Done calling model_fn. 2023-07-15 14:07:17.753203: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. INFO:tensorflow:Graph was finalized. I0715 14:07:18.197918 47821886322496 monitored_session.py:240] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:18.198770 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. INFO:tensorflow:Running local_init_op. I0715 14:07:20.793029 47821886322496 session_manager.py:526] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0715 14:07:20.857252 47821886322496 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0715 14:07:21.417166 47821886322496 modeling.py:418] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:21.417759 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:25.965068 47821886322496 call_variants.py:462] Processed 1 examples in 1 batches [1565.044 sec per 100]. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1378, in _do_call. return fn(*args). File ""/usr/local/li",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:4281,availability,Restor,Restoring,4281,"ng calls to /tmp/call_variants_output.tfrecord.gz. INFO:tensorflow:Calling model_fn. I0715 14:07:10.758818 47821886322496 estimator.py:1173] Calling model_fn. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0715 14:07:16.873909 47821886322496 estimator.py:1175] Done calling model_fn. 2023-07-15 14:07:17.753203: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. INFO:tensorflow:Graph was finalized. I0715 14:07:18.197918 47821886322496 monitored_session.py:240] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:18.198770 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. INFO:tensorflow:Running local_init_op. I0715 14:07:20.793029 47821886322496 session_manager.py:526] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0715 14:07:20.857252 47821886322496 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0715 14:07:21.417166 47821886322496 modeling.py:418] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:21.417759 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:25.965068 47821886322496 call_variants.py:462] Processed 1 examples in 1 batches [1565.044 sec per 100]. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1378, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1361, in _run_fn. return self._call_tf_s",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:4714,availability,Restor,Restoring,4714,"tensorflow:Done calling model_fn. I0715 14:07:16.873909 47821886322496 estimator.py:1175] Done calling model_fn. 2023-07-15 14:07:17.753203: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. INFO:tensorflow:Graph was finalized. I0715 14:07:18.197918 47821886322496 monitored_session.py:240] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:18.198770 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. INFO:tensorflow:Running local_init_op. I0715 14:07:20.793029 47821886322496 session_manager.py:526] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0715 14:07:20.857252 47821886322496 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0715 14:07:21.417166 47821886322496 modeling.py:418] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:21.417759 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:25.965068 47821886322496 call_variants.py:462] Processed 1 examples in 1 batches [1565.044 sec per 100]. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1378, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1361, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1454, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.DataLossError: corrupted record at 484865306. [[{{node IteratorGetNext}}]]. During handling of the above exception, another exception occurred:. Traceback (",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:4823,availability,Restor,Restoring,4823,"fn. 2023-07-15 14:07:17.753203: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. INFO:tensorflow:Graph was finalized. I0715 14:07:18.197918 47821886322496 monitored_session.py:240] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:18.198770 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. INFO:tensorflow:Running local_init_op. I0715 14:07:20.793029 47821886322496 session_manager.py:526] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0715 14:07:20.857252 47821886322496 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0715 14:07:21.417166 47821886322496 modeling.py:418] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:21.417759 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:25.965068 47821886322496 call_variants.py:462] Processed 1 examples in 1 batches [1565.044 sec per 100]. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1378, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1361, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1454, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.DataLossError: corrupted record at 484865306. [[{{node IteratorGetNext}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:8214,availability,error,error,8214,"python/training/monitored_session.py"", line 1464, in run. outputs = _WrappedSession.run(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1228, in run. return self._sess.run(*args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 968, in run. result = self._run(None, fetches, feed_dict, options_ptr,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1191, in _run. results = self._do_run(handle, final_targets, final_fetches,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1371, in _do_run. return self._do_call(_run_fn, feeds, fetches, targets, options,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1397, in _do_call. raise type(e)(node_def, op, message) # pylint: disable=no-value-for-parameter. tensorflow.python.framework.errors_impl.DataLossError: Graph execution error:. Detected at node 'IteratorGetNext' defined at (most recent call last):. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 621, in predict. features, input_hooks = self._get_features_from_input_fn(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estima",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:11637,availability,Operat,Operation,11637,"/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 621, in predict. features, input_hooks = self._get_features_from_input_fn(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1019, in _get_features_from_input_fn. result, _, hooks = estimator_util.parse_input_fn_result(result). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/util.py"", line 60, in parse_input_fn_result. result = iterator.get_next(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/iterator_ops.py"", line 445, in get_next. flat_ret = gen_dataset_ops.iterator_get_next(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_dataset_ops.py"", line 3037, in iterator_get_next. _, _, _op, _outputs = _op_def_library._apply_op_helper(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py"", line 795, in _apply_op_helper. op = g._create_op_internal(op_type_name, inputs, dtypes=None,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 3798, in _create_op_internal. ret = Operation(. Best Regards! Bo.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:46,deployability,cluster,cluster,46,"Error running deepvariant from singularity in cluster; Dear all,. I was trying to run deepvariant from singularity in cluster, but I always meet same error, I don't know hou to fix it:. /share/app/singularity/3.8.1/bin/singularity exec --containall --bind /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/tmp:/tmp --bind /usr/lib/locale/:/usr/lib/locale/ --bind $ccsbam:$ccsbam --bind $ccsbam.bai:$ccsbam.bai --bind $fasta:$fasta --bind $fasta.fai:$fasta.fai --bind /hwfssz5/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/translatome/HCT116/PacBio/analysis/deepvariant.v3/T202302180201:/output /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/software/deepvariant/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=$fasta --reads=$ccsbam --output_vcf=/output/T202302180201.deepvariant.vcf.gz --output_gvcf=/output/T202302180201.deepvariant.g.vcf.gz --num_shards=20 --intermediate_results_dir=/tmp. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/call_variants_output.tfrecord.gz"" --examples ""/tmp/make_examples.tfrecord@20.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"". 2023-07-15 14:06:58.063861: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0715 14:07:10.199614 47821886322496 call_variants.py:317] From /tmp/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:118,deployability,cluster,cluster,118,"Error running deepvariant from singularity in cluster; Dear all,. I was trying to run deepvariant from singularity in cluster, but I always meet same error, I don't know hou to fix it:. /share/app/singularity/3.8.1/bin/singularity exec --containall --bind /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/tmp:/tmp --bind /usr/lib/locale/:/usr/lib/locale/ --bind $ccsbam:$ccsbam --bind $ccsbam.bai:$ccsbam.bai --bind $fasta:$fasta --bind $fasta.fai:$fasta.fai --bind /hwfssz5/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/translatome/HCT116/PacBio/analysis/deepvariant.v3/T202302180201:/output /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/software/deepvariant/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=$fasta --reads=$ccsbam --output_vcf=/output/T202302180201.deepvariant.vcf.gz --output_gvcf=/output/T202302180201.deepvariant.g.vcf.gz --num_shards=20 --intermediate_results_dir=/tmp. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/call_variants_output.tfrecord.gz"" --examples ""/tmp/make_examples.tfrecord@20.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"". 2023-07-15 14:06:58.063861: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0715 14:07:10.199614 47821886322496 call_variants.py:317] From /tmp/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:238,deployability,contain,containall,238,"Error running deepvariant from singularity in cluster; Dear all,. I was trying to run deepvariant from singularity in cluster, but I always meet same error, I don't know hou to fix it:. /share/app/singularity/3.8.1/bin/singularity exec --containall --bind /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/tmp:/tmp --bind /usr/lib/locale/:/usr/lib/locale/ --bind $ccsbam:$ccsbam --bind $ccsbam.bai:$ccsbam.bai --bind $fasta:$fasta --bind $fasta.fai:$fasta.fai --bind /hwfssz5/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/translatome/HCT116/PacBio/analysis/deepvariant.v3/T202302180201:/output /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/software/deepvariant/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=$fasta --reads=$ccsbam --output_vcf=/output/T202302180201.deepvariant.vcf.gz --output_gvcf=/output/T202302180201.deepvariant.g.vcf.gz --num_shards=20 --intermediate_results_dir=/tmp. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/call_variants_output.tfrecord.gz"" --examples ""/tmp/make_examples.tfrecord@20.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"". 2023-07-15 14:06:58.063861: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0715 14:07:10.199614 47821886322496 call_variants.py:317] From /tmp/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:3611,deployability,version,version,3611,"ry: /tmp/tmpl_zjylv7. W0715 14:07:10.308488 47821886322496 estimator.py:1864] Using temporary folder as model directory: /tmp/tmpl_zjylv7. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpl_zjylv7', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 10000. I0715 14:07:10.309390 47821886322496 estimator.py:202] Using config: {'_model_dir': '/tmp/tmpl_zjylv7', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_c. I0715 14:07:10.309847 47821886322496 call_variants.py:446] Writing calls to /tmp/call_variants_output.tfrecord.gz. INFO:tensorflow:Calling model_fn. I0715 14:07:10.758818 47821886322496 estimator.py:1173] Calling model_fn. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0715 14:07:16.873909 47821886322496 estimator.py:1175] Done calling model_fn. 2023-07-15 14:07:17.753203: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. INFO:tensorflow:Graph was finalized. I0715 14:07:18.197918 47821886322496 monitored_session.py:240] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:18.198770 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. INFO:tensorflow:Running local_init_op. I0715 14:07:20.793029 47821886322496 session_manager.py:526] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0715 14:07:20.857252 47821886322496 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloa",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:5856,deployability,modul,module,5856,"s/pacbio/model.ckpt. I0715 14:07:25.965068 47821886322496 call_variants.py:462] Processed 1 examples in 1 batches [1565.044 sec per 100]. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1378, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1361, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1454, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.DataLossError: corrupted record at 484865306. [[{{node IteratorGetNext}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 642, in predict. preds_evaluated = mon_sess.run(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 778, in run. return self._s",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:8406,deployability,modul,module,8406,"28, in run. return self._sess.run(*args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 968, in run. result = self._run(None, fetches, feed_dict, options_ptr,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1191, in _run. results = self._do_run(handle, final_targets, final_fetches,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1371, in _do_run. return self._do_call(_run_fn, feeds, fetches, targets, options,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1397, in _do_call. raise type(e)(node_def, op, message) # pylint: disable=no-value-for-parameter. tensorflow.python.framework.errors_impl.DataLossError: Graph execution error:. Detected at node 'IteratorGetNext' defined at (most recent call last):. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 621, in predict. features, input_hooks = self._get_features_from_input_fn(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1019, in _get_features_from_input_fn. result, _, hooks = estimator_util.parse_input_fn_result(result). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/es",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:9587,deployability,stack,stack,9587,"absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 621, in predict. features, input_hooks = self._get_features_from_input_fn(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1019, in _get_features_from_input_fn. result, _, hooks = estimator_util.parse_input_fn_result(result). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/util.py"", line 60, in parse_input_fn_result. result = iterator.get_next(). Node: 'IteratorGetNext'. corrupted record at 484865306. [[{{node IteratorGetNext}}]]. Original stack trace for 'IteratorGetNext':. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 621, in predict. features, input_hooks = ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:9735,deployability,modul,module,9735,"all_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 621, in predict. features, input_hooks = self._get_features_from_input_fn(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1019, in _get_features_from_input_fn. result, _, hooks = estimator_util.parse_input_fn_result(result). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/util.py"", line 60, in parse_input_fn_result. result = iterator.get_next(). Node: 'IteratorGetNext'. corrupted record at 484865306. [[{{node IteratorGetNext}}]]. Original stack trace for 'IteratorGetNext':. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 621, in predict. features, input_hooks = self._get_features_from_input_fn(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1019, in _",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:1103,energy efficiency,model,models,1103,"gularity in cluster, but I always meet same error, I don't know hou to fix it:. /share/app/singularity/3.8.1/bin/singularity exec --containall --bind /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/tmp:/tmp --bind /usr/lib/locale/:/usr/lib/locale/ --bind $ccsbam:$ccsbam --bind $ccsbam.bai:$ccsbam.bai --bind $fasta:$fasta --bind $fasta.fai:$fasta.fai --bind /hwfssz5/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/translatome/HCT116/PacBio/analysis/deepvariant.v3/T202302180201:/output /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/software/deepvariant/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=$fasta --reads=$ccsbam --output_vcf=/output/T202302180201.deepvariant.vcf.gz --output_gvcf=/output/T202302180201.deepvariant.g.vcf.gz --num_shards=20 --intermediate_results_dir=/tmp. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/call_variants_output.tfrecord.gz"" --examples ""/tmp/make_examples.tfrecord@20.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"". 2023-07-15 14:06:58.063861: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0715 14:07:10.199614 47821886322496 call_variants.py:317] From /tmp/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in perfo",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:1117,energy efficiency,model,model,1117,"uster, but I always meet same error, I don't know hou to fix it:. /share/app/singularity/3.8.1/bin/singularity exec --containall --bind /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/tmp:/tmp --bind /usr/lib/locale/:/usr/lib/locale/ --bind $ccsbam:$ccsbam --bind $ccsbam.bai:$ccsbam.bai --bind $fasta:$fasta --bind $fasta.fai:$fasta.fai --bind /hwfssz5/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/translatome/HCT116/PacBio/analysis/deepvariant.v3/T202302180201:/output /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/software/deepvariant/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=$fasta --reads=$ccsbam --output_vcf=/output/T202302180201.deepvariant.vcf.gz --output_gvcf=/output/T202302180201.deepvariant.g.vcf.gz --num_shards=20 --intermediate_results_dir=/tmp. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/call_variants_output.tfrecord.gz"" --examples ""/tmp/make_examples.tfrecord@20.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"". 2023-07-15 14:06:58.063861: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0715 14:07:10.199614 47821886322496 call_variants.py:317] From /tmp/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critica",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:1171,energy efficiency,core,core,1171," to fix it:. /share/app/singularity/3.8.1/bin/singularity exec --containall --bind /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/tmp:/tmp --bind /usr/lib/locale/:/usr/lib/locale/ --bind $ccsbam:$ccsbam --bind $ccsbam.bai:$ccsbam.bai --bind $fasta:$fasta --bind $fasta.fai:$fasta.fai --bind /hwfssz5/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/translatome/HCT116/PacBio/analysis/deepvariant.v3/T202302180201:/output /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/software/deepvariant/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=$fasta --reads=$ccsbam --output_vcf=/output/T202302180201.deepvariant.vcf.gz --output_gvcf=/output/T202302180201.deepvariant.g.vcf.gz --num_shards=20 --intermediate_results_dir=/tmp. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/call_variants_output.tfrecord.gz"" --examples ""/tmp/make_examples.tfrecord@20.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"". 2023-07-15 14:06:58.063861: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0715 14:07:10.199614 47821886322496 call_variants.py:317] From /tmp/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild T",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:1237,energy efficiency,optim,optimized,1237,"ainall --bind /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/tmp:/tmp --bind /usr/lib/locale/:/usr/lib/locale/ --bind $ccsbam:$ccsbam --bind $ccsbam.bai:$ccsbam.bai --bind $fasta:$fasta --bind $fasta.fai:$fasta.fai --bind /hwfssz5/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/translatome/HCT116/PacBio/analysis/deepvariant.v3/T202302180201:/output /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/software/deepvariant/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=$fasta --reads=$ccsbam --output_vcf=/output/T202302180201.deepvariant.vcf.gz --output_gvcf=/output/T202302180201.deepvariant.g.vcf.gz --num_shards=20 --intermediate_results_dir=/tmp. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/call_variants_output.tfrecord.gz"" --examples ""/tmp/make_examples.tfrecord@20.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"". 2023-07-15 14:06:58.063861: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0715 14:07:10.199614 47821886322496 call_variants.py:317] From /tmp/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-07-15 14:07:10.22",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:1317,energy efficiency,CPU,CPU,1317,"locale/:/usr/lib/locale/ --bind $ccsbam:$ccsbam --bind $ccsbam.bai:$ccsbam.bai --bind $fasta:$fasta --bind $fasta.fai:$fasta.fai --bind /hwfssz5/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/translatome/HCT116/PacBio/analysis/deepvariant.v3/T202302180201:/output /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/software/deepvariant/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=$fasta --reads=$ccsbam --output_vcf=/output/T202302180201.deepvariant.vcf.gz --output_gvcf=/output/T202302180201.deepvariant.g.vcf.gz --num_shards=20 --intermediate_results_dir=/tmp. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/call_variants_output.tfrecord.gz"" --examples ""/tmp/make_examples.tfrecord@20.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"". 2023-07-15 14:06:58.063861: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0715 14:07:10.199614 47821886322496 call_variants.py:317] From /tmp/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-07-15 14:07:10.223654: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thre",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:1752,energy efficiency,model,models,1752,"-output_vcf=/output/T202302180201.deepvariant.vcf.gz --output_gvcf=/output/T202302180201.deepvariant.g.vcf.gz --num_shards=20 --intermediate_results_dir=/tmp. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/call_variants_output.tfrecord.gz"" --examples ""/tmp/make_examples.tfrecord@20.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"". 2023-07-15 14:06:58.063861: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0715 14:07:10.199614 47821886322496 call_variants.py:317] From /tmp/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-07-15 14:07:10.223654: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. 2023-07-15 14:07:10.226851: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpl_zjylv7. W0715 14:07:10.308488 47821886322496 estimator.py:1864] Using temporary folder as model directory: /tmp/tmpl_zjylv7. I",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:1766,energy efficiency,model,model,1766,"utput/T202302180201.deepvariant.vcf.gz --output_gvcf=/output/T202302180201.deepvariant.g.vcf.gz --num_shards=20 --intermediate_results_dir=/tmp. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/call_variants_output.tfrecord.gz"" --examples ""/tmp/make_examples.tfrecord@20.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"". 2023-07-15 14:06:58.063861: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0715 14:07:10.199614 47821886322496 call_variants.py:317] From /tmp/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-07-15 14:07:10.223654: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. 2023-07-15 14:07:10.226851: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpl_zjylv7. W0715 14:07:10.308488 47821886322496 estimator.py:1864] Using temporary folder as model directory: /tmp/tmpl_zjylv7. INFO:tensorflow",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:1935,energy efficiency,core,core,1935,"nd:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/call_variants_output.tfrecord.gz"" --examples ""/tmp/make_examples.tfrecord@20.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"". 2023-07-15 14:06:58.063861: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0715 14:07:10.199614 47821886322496 call_variants.py:317] From /tmp/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-07-15 14:07:10.223654: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. 2023-07-15 14:07:10.226851: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpl_zjylv7. W0715 14:07:10.308488 47821886322496 estimator.py:1864] Using temporary folder as model directory: /tmp/tmpl_zjylv7. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpl_zjylv7', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:2001,energy efficiency,optim,optimized,2001,"l_variants_output.tfrecord.gz"" --examples ""/tmp/make_examples.tfrecord@20.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"". 2023-07-15 14:06:58.063861: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0715 14:07:10.199614 47821886322496 call_variants.py:317] From /tmp/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-07-15 14:07:10.223654: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. 2023-07-15 14:07:10.226851: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpl_zjylv7. W0715 14:07:10.308488 47821886322496 estimator.py:1864] Using temporary folder as model directory: /tmp/tmpl_zjylv7. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpl_zjylv7', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 10000. I0715 14:07:10.309",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:2081,energy efficiency,CPU,CPU,2081," --checkpoint ""/opt/models/pacbio/model.ckpt"". 2023-07-15 14:06:58.063861: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0715 14:07:10.199614 47821886322496 call_variants.py:317] From /tmp/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-07-15 14:07:10.223654: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. 2023-07-15 14:07:10.226851: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpl_zjylv7. W0715 14:07:10.308488 47821886322496 estimator.py:1864] Using temporary folder as model directory: /tmp/tmpl_zjylv7. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpl_zjylv7', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 10000. I0715 14:07:10.309390 47821886322496 estimator.py:202] Using config: {'_model_dir': '/tmp/tmpl_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:2261,energy efficiency,core,core,2261," Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0715 14:07:10.199614 47821886322496 call_variants.py:317] From /tmp/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-07-15 14:07:10.223654: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. 2023-07-15 14:07:10.226851: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpl_zjylv7. W0715 14:07:10.308488 47821886322496 estimator.py:1864] Using temporary folder as model directory: /tmp/tmpl_zjylv7. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpl_zjylv7', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 10000. I0715 14:07:10.309390 47821886322496 estimator.py:202] Using config: {'_model_dir': '/tmp/tmpl_zjylv7', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_c. I0715 14:07:10.309847 47821886322496 call",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:2523,energy efficiency,optim,optimization,2523,"_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-07-15 14:07:10.223654: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. 2023-07-15 14:07:10.226851: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpl_zjylv7. W0715 14:07:10.308488 47821886322496 estimator.py:1864] Using temporary folder as model directory: /tmp/tmpl_zjylv7. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpl_zjylv7', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 10000. I0715 14:07:10.309390 47821886322496 estimator.py:202] Using config: {'_model_dir': '/tmp/tmpl_zjylv7', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_c. I0715 14:07:10.309847 47821886322496 call_variants.py:446] Writing calls to /tmp/call_variants_output.tfrecord.gz. INFO:tensorflow:Calling model_fn. I0715 14:07:10.758818 47821886322496 estimator.py:1173] Calling model_fn. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:2602,energy efficiency,model,model,2602,"les: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-07-15 14:07:10.223654: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. 2023-07-15 14:07:10.226851: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpl_zjylv7. W0715 14:07:10.308488 47821886322496 estimator.py:1864] Using temporary folder as model directory: /tmp/tmpl_zjylv7. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpl_zjylv7', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 10000. I0715 14:07:10.309390 47821886322496 estimator.py:202] Using config: {'_model_dir': '/tmp/tmpl_zjylv7', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_c. I0715 14:07:10.309847 47821886322496 call_variants.py:446] Writing calls to /tmp/call_variants_output.tfrecord.gz. INFO:tensorflow:Calling model_fn. I0715 14:07:10.758818 47821886322496 estimator.py:1173] Calling model_fn. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a f",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:2674,energy efficiency,estimat,estimator,2674,"0]. I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-07-15 14:07:10.223654: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. 2023-07-15 14:07:10.226851: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpl_zjylv7. W0715 14:07:10.308488 47821886322496 estimator.py:1864] Using temporary folder as model directory: /tmp/tmpl_zjylv7. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpl_zjylv7', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 10000. I0715 14:07:10.309390 47821886322496 estimator.py:202] Using config: {'_model_dir': '/tmp/tmpl_zjylv7', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_c. I0715 14:07:10.309847 47821886322496 call_variants.py:446] Writing calls to /tmp/call_variants_output.tfrecord.gz. INFO:tensorflow:Calling model_fn. I0715 14:07:10.758818 47821886322496 estimator.py:1173] Calling model_fn. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:2719,energy efficiency,model,model,2719,"ll_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-07-15 14:07:10.223654: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. 2023-07-15 14:07:10.226851: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpl_zjylv7. W0715 14:07:10.308488 47821886322496 estimator.py:1864] Using temporary folder as model directory: /tmp/tmpl_zjylv7. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpl_zjylv7', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 10000. I0715 14:07:10.309390 47821886322496 estimator.py:202] Using config: {'_model_dir': '/tmp/tmpl_zjylv7', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_c. I0715 14:07:10.309847 47821886322496 call_variants.py:446] Writing calls to /tmp/call_variants_output.tfrecord.gz. INFO:tensorflow:Calling model_fn. I0715 14:07:10.758818 47821886322496 estimator.py:1173] Calling model_fn. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:ten",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:3025,energy efficiency,estimat,estimator,3025,"ural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-07-15 14:07:10.223654: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. 2023-07-15 14:07:10.226851: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpl_zjylv7. W0715 14:07:10.308488 47821886322496 estimator.py:1864] Using temporary folder as model directory: /tmp/tmpl_zjylv7. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpl_zjylv7', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 10000. I0715 14:07:10.309390 47821886322496 estimator.py:202] Using config: {'_model_dir': '/tmp/tmpl_zjylv7', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_c. I0715 14:07:10.309847 47821886322496 call_variants.py:446] Writing calls to /tmp/call_variants_output.tfrecord.gz. INFO:tensorflow:Calling model_fn. I0715 14:07:10.758818 47821886322496 estimator.py:1173] Calling model_fn. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0715 14:07:16.873909 47821886322496 estimator.py:1175] Done calling model_fn. 2023-07-15 14:07:17.753203: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performa",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:3408,energy efficiency,estimat,estimator,3408,"formance. 2023-07-15 14:07:10.226851: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpl_zjylv7. W0715 14:07:10.308488 47821886322496 estimator.py:1864] Using temporary folder as model directory: /tmp/tmpl_zjylv7. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpl_zjylv7', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 10000. I0715 14:07:10.309390 47821886322496 estimator.py:202] Using config: {'_model_dir': '/tmp/tmpl_zjylv7', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_c. I0715 14:07:10.309847 47821886322496 call_variants.py:446] Writing calls to /tmp/call_variants_output.tfrecord.gz. INFO:tensorflow:Calling model_fn. I0715 14:07:10.758818 47821886322496 estimator.py:1173] Calling model_fn. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0715 14:07:16.873909 47821886322496 estimator.py:1175] Done calling model_fn. 2023-07-15 14:07:17.753203: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. INFO:tensorflow:Graph was finalized. I0715 14:07:18.197918 47821886322496 monitored_session.py:240] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:18.198770 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. INFO:tensorflow:Running local_init_op. I0715 14:07:20.793029 47821886322496",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:3790,energy efficiency,estimat,estimator,3790,"r': '/tmp/tmpl_zjylv7', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 10000. I0715 14:07:10.309390 47821886322496 estimator.py:202] Using config: {'_model_dir': '/tmp/tmpl_zjylv7', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_c. I0715 14:07:10.309847 47821886322496 call_variants.py:446] Writing calls to /tmp/call_variants_output.tfrecord.gz. INFO:tensorflow:Calling model_fn. I0715 14:07:10.758818 47821886322496 estimator.py:1173] Calling model_fn. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0715 14:07:16.873909 47821886322496 estimator.py:1175] Done calling model_fn. 2023-07-15 14:07:17.753203: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. INFO:tensorflow:Graph was finalized. I0715 14:07:18.197918 47821886322496 monitored_session.py:240] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:18.198770 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. INFO:tensorflow:Running local_init_op. I0715 14:07:20.793029 47821886322496 session_manager.py:526] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0715 14:07:20.857252 47821886322496 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0715 14:07:21.417166 47821886322496 modeling.py:418] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:21.417759 47",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:3873,energy efficiency,core,core,3873,"ave_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 10000. I0715 14:07:10.309390 47821886322496 estimator.py:202] Using config: {'_model_dir': '/tmp/tmpl_zjylv7', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_c. I0715 14:07:10.309847 47821886322496 call_variants.py:446] Writing calls to /tmp/call_variants_output.tfrecord.gz. INFO:tensorflow:Calling model_fn. I0715 14:07:10.758818 47821886322496 estimator.py:1173] Calling model_fn. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0715 14:07:16.873909 47821886322496 estimator.py:1175] Done calling model_fn. 2023-07-15 14:07:17.753203: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. INFO:tensorflow:Graph was finalized. I0715 14:07:18.197918 47821886322496 monitored_session.py:240] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:18.198770 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. INFO:tensorflow:Running local_init_op. I0715 14:07:20.793029 47821886322496 session_manager.py:526] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0715 14:07:20.857252 47821886322496 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0715 14:07:21.417166 47821886322496 modeling.py:418] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:21.417759 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.c",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:4203,energy efficiency,model,models,4203,"0, '_session_c. I0715 14:07:10.309847 47821886322496 call_variants.py:446] Writing calls to /tmp/call_variants_output.tfrecord.gz. INFO:tensorflow:Calling model_fn. I0715 14:07:10.758818 47821886322496 estimator.py:1173] Calling model_fn. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0715 14:07:16.873909 47821886322496 estimator.py:1175] Done calling model_fn. 2023-07-15 14:07:17.753203: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. INFO:tensorflow:Graph was finalized. I0715 14:07:18.197918 47821886322496 monitored_session.py:240] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:18.198770 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. INFO:tensorflow:Running local_init_op. I0715 14:07:20.793029 47821886322496 session_manager.py:526] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0715 14:07:20.857252 47821886322496 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0715 14:07:21.417166 47821886322496 modeling.py:418] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:21.417759 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:25.965068 47821886322496 call_variants.py:462] Processed 1 examples in 1 batches [1565.044 sec per 100]. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1378, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/ten",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:4217,energy efficiency,model,model,4217,". I0715 14:07:10.309847 47821886322496 call_variants.py:446] Writing calls to /tmp/call_variants_output.tfrecord.gz. INFO:tensorflow:Calling model_fn. I0715 14:07:10.758818 47821886322496 estimator.py:1173] Calling model_fn. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0715 14:07:16.873909 47821886322496 estimator.py:1175] Done calling model_fn. 2023-07-15 14:07:17.753203: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. INFO:tensorflow:Graph was finalized. I0715 14:07:18.197918 47821886322496 monitored_session.py:240] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:18.198770 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. INFO:tensorflow:Running local_init_op. I0715 14:07:20.793029 47821886322496 session_manager.py:526] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0715 14:07:20.857252 47821886322496 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0715 14:07:21.417166 47821886322496 modeling.py:418] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:21.417759 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:25.965068 47821886322496 call_variants.py:462] Processed 1 examples in 1 batches [1565.044 sec per 100]. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1378, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:4312,energy efficiency,model,models,4312,"s_output.tfrecord.gz. INFO:tensorflow:Calling model_fn. I0715 14:07:10.758818 47821886322496 estimator.py:1173] Calling model_fn. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0715 14:07:16.873909 47821886322496 estimator.py:1175] Done calling model_fn. 2023-07-15 14:07:17.753203: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. INFO:tensorflow:Graph was finalized. I0715 14:07:18.197918 47821886322496 monitored_session.py:240] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:18.198770 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. INFO:tensorflow:Running local_init_op. I0715 14:07:20.793029 47821886322496 session_manager.py:526] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0715 14:07:20.857252 47821886322496 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0715 14:07:21.417166 47821886322496 modeling.py:418] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:21.417759 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:25.965068 47821886322496 call_variants.py:462] Processed 1 examples in 1 batches [1565.044 sec per 100]. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1378, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1361, in _run_fn. return self._call_tf_sessionrun(options, feed_dict,",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:4326,energy efficiency,model,model,4326,"ord.gz. INFO:tensorflow:Calling model_fn. I0715 14:07:10.758818 47821886322496 estimator.py:1173] Calling model_fn. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0715 14:07:16.873909 47821886322496 estimator.py:1175] Done calling model_fn. 2023-07-15 14:07:17.753203: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. INFO:tensorflow:Graph was finalized. I0715 14:07:18.197918 47821886322496 monitored_session.py:240] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:18.198770 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. INFO:tensorflow:Running local_init_op. I0715 14:07:20.793029 47821886322496 session_manager.py:526] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0715 14:07:20.857252 47821886322496 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0715 14:07:21.417166 47821886322496 modeling.py:418] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:21.417759 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:25.965068 47821886322496 call_variants.py:462] Processed 1 examples in 1 batches [1565.044 sec per 100]. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1378, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1361, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:4664,energy efficiency,model,modeling,4664,"ings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0715 14:07:16.873909 47821886322496 estimator.py:1175] Done calling model_fn. 2023-07-15 14:07:17.753203: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. INFO:tensorflow:Graph was finalized. I0715 14:07:18.197918 47821886322496 monitored_session.py:240] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:18.198770 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. INFO:tensorflow:Running local_init_op. I0715 14:07:20.793029 47821886322496 session_manager.py:526] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0715 14:07:20.857252 47821886322496 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0715 14:07:21.417166 47821886322496 modeling.py:418] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:21.417759 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:25.965068 47821886322496 call_variants.py:462] Processed 1 examples in 1 batches [1565.044 sec per 100]. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1378, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1361, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1454, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.DataLossError: corrupted record at 484865306. [[{{node IteratorGetNext}}]]. During handling of the above ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:4745,energy efficiency,model,models,4745,"_fn. I0715 14:07:16.873909 47821886322496 estimator.py:1175] Done calling model_fn. 2023-07-15 14:07:17.753203: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. INFO:tensorflow:Graph was finalized. I0715 14:07:18.197918 47821886322496 monitored_session.py:240] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:18.198770 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. INFO:tensorflow:Running local_init_op. I0715 14:07:20.793029 47821886322496 session_manager.py:526] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0715 14:07:20.857252 47821886322496 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0715 14:07:21.417166 47821886322496 modeling.py:418] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:21.417759 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:25.965068 47821886322496 call_variants.py:462] Processed 1 examples in 1 batches [1565.044 sec per 100]. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1378, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1361, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1454, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.DataLossError: corrupted record at 484865306. [[{{node IteratorGetNext}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:4759,energy efficiency,model,model,4759,"07:16.873909 47821886322496 estimator.py:1175] Done calling model_fn. 2023-07-15 14:07:17.753203: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. INFO:tensorflow:Graph was finalized. I0715 14:07:18.197918 47821886322496 monitored_session.py:240] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:18.198770 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. INFO:tensorflow:Running local_init_op. I0715 14:07:20.793029 47821886322496 session_manager.py:526] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0715 14:07:20.857252 47821886322496 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0715 14:07:21.417166 47821886322496 modeling.py:418] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:21.417759 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:25.965068 47821886322496 call_variants.py:462] Processed 1 examples in 1 batches [1565.044 sec per 100]. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1378, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1361, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1454, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.DataLossError: corrupted record at 484865306. [[{{node IteratorGetNext}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.r",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:4854,energy efficiency,model,models,4854,"3: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. INFO:tensorflow:Graph was finalized. I0715 14:07:18.197918 47821886322496 monitored_session.py:240] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:18.198770 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. INFO:tensorflow:Running local_init_op. I0715 14:07:20.793029 47821886322496 session_manager.py:526] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0715 14:07:20.857252 47821886322496 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0715 14:07:21.417166 47821886322496 modeling.py:418] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:21.417759 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:25.965068 47821886322496 call_variants.py:462] Processed 1 examples in 1 batches [1565.044 sec per 100]. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1378, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1361, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1454, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.DataLossError: corrupted record at 484865306. [[{{node IteratorGetNext}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <m",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:4868,energy efficiency,model,model,4868,"w/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. INFO:tensorflow:Graph was finalized. I0715 14:07:18.197918 47821886322496 monitored_session.py:240] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:18.198770 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. INFO:tensorflow:Running local_init_op. I0715 14:07:20.793029 47821886322496 session_manager.py:526] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0715 14:07:20.857252 47821886322496 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0715 14:07:21.417166 47821886322496 modeling.py:418] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:21.417759 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:25.965068 47821886322496 call_variants.py:462] Processed 1 examples in 1 batches [1565.044 sec per 100]. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1378, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1361, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1454, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.DataLossError: corrupted record at 484865306. [[{{node IteratorGetNext}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.com",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:6535,energy efficiency,predict,prediction,6535,".framework.errors_impl.DataLossError: corrupted record at 484865306. [[{{node IteratorGetNext}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 642, in predict. preds_evaluated = mon_sess.run(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 778, in run. return self._sess.run(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1307, in run. return self._sess.run(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1397, in run. return self._sess.run(*args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1464, in run. outputs = _WrappedSession.run(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1228, in run. return self._sess.run(*args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.p",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:6553,energy efficiency,predict,predictions,6553,"mpl.DataLossError: corrupted record at 484865306. [[{{node IteratorGetNext}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 642, in predict. preds_evaluated = mon_sess.run(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 778, in run. return self._sess.run(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1307, in run. return self._sess.run(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1397, in run. return self._sess.run(*args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1464, in run. outputs = _WrappedSession.run(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1228, in run. return self._sess.run(*args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 968, in ru",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:6640,energy efficiency,estimat,estimator,6640," handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 642, in predict. preds_evaluated = mon_sess.run(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 778, in run. return self._sess.run(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1307, in run. return self._sess.run(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1397, in run. return self._sess.run(*args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1464, in run. outputs = _WrappedSession.run(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1228, in run. return self._sess.run(*args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 968, in run. result = self._run(None, fetches, feed_dict, options_ptr,. File ""/usr/local/lib/pyt",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:6650,energy efficiency,estimat,estimator,6650,"of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 642, in predict. preds_evaluated = mon_sess.run(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 778, in run. return self._sess.run(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1307, in run. return self._sess.run(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1397, in run. return self._sess.run(*args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1464, in run. outputs = _WrappedSession.run(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1228, in run. return self._sess.run(*args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 968, in run. result = self._run(None, fetches, feed_dict, options_ptr,. File ""/usr/local/lib/python3.8/dis",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:6678,energy efficiency,predict,predict,6678,"ther exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 642, in predict. preds_evaluated = mon_sess.run(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 778, in run. return self._sess.run(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1307, in run. return self._sess.run(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1397, in run. return self._sess.run(*args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1464, in run. outputs = _WrappedSession.run(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1228, in run. return self._sess.run(*args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 968, in run. result = self._run(None, fetches, feed_dict, options_ptr,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/pytho",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:6718,energy efficiency,predict,predictions,6718,"recent call last):. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 642, in predict. preds_evaluated = mon_sess.run(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 778, in run. return self._sess.run(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1307, in run. return self._sess.run(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1397, in run. return self._sess.run(*args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1464, in run. outputs = _WrappedSession.run(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1228, in run. return self._sess.run(*args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 968, in run. result = self._run(None, fetches, feed_dict, options_ptr,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1191, in _run. ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:8917,energy efficiency,predict,prediction,8917,"feeds, fetches, targets, options,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1397, in _do_call. raise type(e)(node_def, op, message) # pylint: disable=no-value-for-parameter. tensorflow.python.framework.errors_impl.DataLossError: Graph execution error:. Detected at node 'IteratorGetNext' defined at (most recent call last):. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 621, in predict. features, input_hooks = self._get_features_from_input_fn(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1019, in _get_features_from_input_fn. result, _, hooks = estimator_util.parse_input_fn_result(result). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/util.py"", line 60, in parse_input_fn_result. result = iterator.get_next(). Node: 'IteratorGetNext'. corrupted record at 484865306. [[{{node IteratorGetNext}}]]. Original stack trace for 'IteratorGetNext':. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tol",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:8935,energy efficiency,predict,predictions,8935,"gets, options,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1397, in _do_call. raise type(e)(node_def, op, message) # pylint: disable=no-value-for-parameter. tensorflow.python.framework.errors_impl.DataLossError: Graph execution error:. Detected at node 'IteratorGetNext' defined at (most recent call last):. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 621, in predict. features, input_hooks = self._get_features_from_input_fn(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1019, in _get_features_from_input_fn. result, _, hooks = estimator_util.parse_input_fn_result(result). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/util.py"", line 60, in parse_input_fn_result. result = iterator.get_next(). Node: 'IteratorGetNext'. corrupted record at 484865306. [[{{node IteratorGetNext}}]]. Original stack trace for 'IteratorGetNext':. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:9022,energy efficiency,estimat,estimator,9022,"session.py"", line 1397, in _do_call. raise type(e)(node_def, op, message) # pylint: disable=no-value-for-parameter. tensorflow.python.framework.errors_impl.DataLossError: Graph execution error:. Detected at node 'IteratorGetNext' defined at (most recent call last):. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 621, in predict. features, input_hooks = self._get_features_from_input_fn(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1019, in _get_features_from_input_fn. result, _, hooks = estimator_util.parse_input_fn_result(result). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/util.py"", line 60, in parse_input_fn_result. result = iterator.get_next(). Node: 'IteratorGetNext'. corrupted record at 484865306. [[{{node IteratorGetNext}}]]. Original stack trace for 'IteratorGetNext':. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_ma",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:9032,energy efficiency,estimat,estimator,9032,""", line 1397, in _do_call. raise type(e)(node_def, op, message) # pylint: disable=no-value-for-parameter. tensorflow.python.framework.errors_impl.DataLossError: Graph execution error:. Detected at node 'IteratorGetNext' defined at (most recent call last):. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 621, in predict. features, input_hooks = self._get_features_from_input_fn(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1019, in _get_features_from_input_fn. result, _, hooks = estimator_util.parse_input_fn_result(result). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/util.py"", line 60, in parse_input_fn_result. result = iterator.get_next(). Node: 'IteratorGetNext'. corrupted record at 484865306. [[{{node IteratorGetNext}}]]. Original stack trace for 'IteratorGetNext':. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, a",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:9060,energy efficiency,predict,predict,9060,"raise type(e)(node_def, op, message) # pylint: disable=no-value-for-parameter. tensorflow.python.framework.errors_impl.DataLossError: Graph execution error:. Detected at node 'IteratorGetNext' defined at (most recent call last):. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 621, in predict. features, input_hooks = self._get_features_from_input_fn(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1019, in _get_features_from_input_fn. result, _, hooks = estimator_util.parse_input_fn_result(result). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/util.py"", line 60, in parse_input_fn_result. result = iterator.get_next(). Node: 'IteratorGetNext'. corrupted record at 484865306. [[{{node IteratorGetNext}}]]. Original stack trace for 'IteratorGetNext':. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runf",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:9201,energy efficiency,estimat,estimator,9201,"ecution error:. Detected at node 'IteratorGetNext' defined at (most recent call last):. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 621, in predict. features, input_hooks = self._get_features_from_input_fn(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1019, in _get_features_from_input_fn. result, _, hooks = estimator_util.parse_input_fn_result(result). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/util.py"", line 60, in parse_input_fn_result. result = iterator.get_next(). Node: 'IteratorGetNext'. corrupted record at 484865306. [[{{node IteratorGetNext}}]]. Original stack trace for 'IteratorGetNext':. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_goo",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:9211,energy efficiency,estimat,estimator,9211,"ror:. Detected at node 'IteratorGetNext' defined at (most recent call last):. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 621, in predict. features, input_hooks = self._get_features_from_input_fn(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1019, in _get_features_from_input_fn. result, _, hooks = estimator_util.parse_input_fn_result(result). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/util.py"", line 60, in parse_input_fn_result. result = iterator.get_next(). Node: 'IteratorGetNext'. corrupted record at 484865306. [[{{node IteratorGetNext}}]]. Original stack trace for 'IteratorGetNext':. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepva",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:9407,energy efficiency,estimat,estimator,9407,">. tf.compat.v1.app.run(). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 621, in predict. features, input_hooks = self._get_features_from_input_fn(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1019, in _get_features_from_input_fn. result, _, hooks = estimator_util.parse_input_fn_result(result). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/util.py"", line 60, in parse_input_fn_result. result = iterator.get_next(). Node: 'IteratorGetNext'. corrupted record at 484865306. [[{{node IteratorGetNext}}]]. Original stack trace for 'IteratorGetNext':. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:10414,energy efficiency,predict,prediction,10414,"il.py"", line 60, in parse_input_fn_result. result = iterator.get_next(). Node: 'IteratorGetNext'. corrupted record at 484865306. [[{{node IteratorGetNext}}]]. Original stack trace for 'IteratorGetNext':. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 621, in predict. features, input_hooks = self._get_features_from_input_fn(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1019, in _get_features_from_input_fn. result, _, hooks = estimator_util.parse_input_fn_result(result). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/util.py"", line 60, in parse_input_fn_result. result = iterator.get_next(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/iterator_ops.py"", line 445, in get_next. flat_ret = gen_dataset_ops.iterator_get_next(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_dataset_ops.py"", line 3037, in iterator_get_next. _, _, _op, _outputs = _op_def_library._apply_op_helper(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py"",",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:10432,energy efficiency,predict,predictions,10432," parse_input_fn_result. result = iterator.get_next(). Node: 'IteratorGetNext'. corrupted record at 484865306. [[{{node IteratorGetNext}}]]. Original stack trace for 'IteratorGetNext':. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 621, in predict. features, input_hooks = self._get_features_from_input_fn(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1019, in _get_features_from_input_fn. result, _, hooks = estimator_util.parse_input_fn_result(result). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/util.py"", line 60, in parse_input_fn_result. result = iterator.get_next(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/iterator_ops.py"", line 445, in get_next. flat_ret = gen_dataset_ops.iterator_get_next(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_dataset_ops.py"", line 3037, in iterator_get_next. _, _, _op, _outputs = _op_def_library._apply_op_helper(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py"", line 795, in _appl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:10519,energy efficiency,estimat,estimator,10519,"ed record at 484865306. [[{{node IteratorGetNext}}]]. Original stack trace for 'IteratorGetNext':. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 621, in predict. features, input_hooks = self._get_features_from_input_fn(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1019, in _get_features_from_input_fn. result, _, hooks = estimator_util.parse_input_fn_result(result). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/util.py"", line 60, in parse_input_fn_result. result = iterator.get_next(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/iterator_ops.py"", line 445, in get_next. flat_ret = gen_dataset_ops.iterator_get_next(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_dataset_ops.py"", line 3037, in iterator_get_next. _, _, _op, _outputs = _op_def_library._apply_op_helper(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py"", line 795, in _apply_op_helper. op = g._create_op_internal(op_type_name, inputs, dtypes=None,. File ""/usr",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:10529,energy efficiency,estimat,estimator,10529,"at 484865306. [[{{node IteratorGetNext}}]]. Original stack trace for 'IteratorGetNext':. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 621, in predict. features, input_hooks = self._get_features_from_input_fn(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1019, in _get_features_from_input_fn. result, _, hooks = estimator_util.parse_input_fn_result(result). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/util.py"", line 60, in parse_input_fn_result. result = iterator.get_next(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/iterator_ops.py"", line 445, in get_next. flat_ret = gen_dataset_ops.iterator_get_next(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_dataset_ops.py"", line 3037, in iterator_get_next. _, _, _op, _outputs = _op_def_library._apply_op_helper(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py"", line 795, in _apply_op_helper. op = g._create_op_internal(op_type_name, inputs, dtypes=None,. File ""/usr/local/lib",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:10557,energy efficiency,predict,predict,10557,"atorGetNext}}]]. Original stack trace for 'IteratorGetNext':. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 621, in predict. features, input_hooks = self._get_features_from_input_fn(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1019, in _get_features_from_input_fn. result, _, hooks = estimator_util.parse_input_fn_result(result). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/util.py"", line 60, in parse_input_fn_result. result = iterator.get_next(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/iterator_ops.py"", line 445, in get_next. flat_ret = gen_dataset_ops.iterator_get_next(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_dataset_ops.py"", line 3037, in iterator_get_next. _, _, _op, _outputs = _op_def_library._apply_op_helper(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py"", line 795, in _apply_op_helper. op = g._create_op_internal(op_type_name, inputs, dtypes=None,. File ""/usr/local/lib/python3.8/dist-packages/te",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:10698,energy efficiency,estimat,estimator,10698,"/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 621, in predict. features, input_hooks = self._get_features_from_input_fn(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1019, in _get_features_from_input_fn. result, _, hooks = estimator_util.parse_input_fn_result(result). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/util.py"", line 60, in parse_input_fn_result. result = iterator.get_next(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/iterator_ops.py"", line 445, in get_next. flat_ret = gen_dataset_ops.iterator_get_next(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_dataset_ops.py"", line 3037, in iterator_get_next. _, _, _op, _outputs = _op_def_library._apply_op_helper(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py"", line 795, in _apply_op_helper. op = g._create_op_internal(op_type_name, inputs, dtypes=None,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 3798, in _create_op_internal. ret = Operation(. Best Regards! Bo.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:10708,energy efficiency,estimat,estimator,10708,"/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 621, in predict. features, input_hooks = self._get_features_from_input_fn(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1019, in _get_features_from_input_fn. result, _, hooks = estimator_util.parse_input_fn_result(result). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/util.py"", line 60, in parse_input_fn_result. result = iterator.get_next(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/iterator_ops.py"", line 445, in get_next. flat_ret = gen_dataset_ops.iterator_get_next(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_dataset_ops.py"", line 3037, in iterator_get_next. _, _, _op, _outputs = _op_def_library._apply_op_helper(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py"", line 795, in _apply_op_helper. op = g._create_op_internal(op_type_name, inputs, dtypes=None,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 3798, in _create_op_internal. ret = Operation(. Best Regards! Bo.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:10904,energy efficiency,estimat,estimator,10904,"/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 621, in predict. features, input_hooks = self._get_features_from_input_fn(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1019, in _get_features_from_input_fn. result, _, hooks = estimator_util.parse_input_fn_result(result). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/util.py"", line 60, in parse_input_fn_result. result = iterator.get_next(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/iterator_ops.py"", line 445, in get_next. flat_ret = gen_dataset_ops.iterator_get_next(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_dataset_ops.py"", line 3037, in iterator_get_next. _, _, _op, _outputs = _op_def_library._apply_op_helper(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py"", line 795, in _apply_op_helper. op = g._create_op_internal(op_type_name, inputs, dtypes=None,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 3798, in _create_op_internal. ret = Operation(. Best Regards! Bo.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:504,integrability,translat,translatome,504,"Error running deepvariant from singularity in cluster; Dear all,. I was trying to run deepvariant from singularity in cluster, but I always meet same error, I don't know hou to fix it:. /share/app/singularity/3.8.1/bin/singularity exec --containall --bind /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/tmp:/tmp --bind /usr/lib/locale/:/usr/lib/locale/ --bind $ccsbam:$ccsbam --bind $ccsbam.bai:$ccsbam.bai --bind $fasta:$fasta --bind $fasta.fai:$fasta.fai --bind /hwfssz5/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/translatome/HCT116/PacBio/analysis/deepvariant.v3/T202302180201:/output /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/software/deepvariant/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=$fasta --reads=$ccsbam --output_vcf=/output/T202302180201.deepvariant.vcf.gz --output_gvcf=/output/T202302180201.deepvariant.g.vcf.gz --num_shards=20 --intermediate_results_dir=/tmp. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/call_variants_output.tfrecord.gz"" --examples ""/tmp/make_examples.tfrecord@20.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"". 2023-07-15 14:06:58.063861: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0715 14:07:10.199614 47821886322496 call_variants.py:317] From /tmp/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:3611,integrability,version,version,3611,"ry: /tmp/tmpl_zjylv7. W0715 14:07:10.308488 47821886322496 estimator.py:1864] Using temporary folder as model directory: /tmp/tmpl_zjylv7. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpl_zjylv7', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 10000. I0715 14:07:10.309390 47821886322496 estimator.py:202] Using config: {'_model_dir': '/tmp/tmpl_zjylv7', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_c. I0715 14:07:10.309847 47821886322496 call_variants.py:446] Writing calls to /tmp/call_variants_output.tfrecord.gz. INFO:tensorflow:Calling model_fn. I0715 14:07:10.758818 47821886322496 estimator.py:1173] Calling model_fn. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0715 14:07:16.873909 47821886322496 estimator.py:1175] Done calling model_fn. 2023-07-15 14:07:17.753203: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. INFO:tensorflow:Graph was finalized. I0715 14:07:18.197918 47821886322496 monitored_session.py:240] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:18.198770 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. INFO:tensorflow:Running local_init_op. I0715 14:07:20.793029 47821886322496 session_manager.py:526] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0715 14:07:20.857252 47821886322496 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloa",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:4965,integrability,batch,batches,4965," 2. Tune using inter_op_parallelism_threads for best performance. INFO:tensorflow:Graph was finalized. I0715 14:07:18.197918 47821886322496 monitored_session.py:240] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:18.198770 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. INFO:tensorflow:Running local_init_op. I0715 14:07:20.793029 47821886322496 session_manager.py:526] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0715 14:07:20.857252 47821886322496 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0715 14:07:21.417166 47821886322496 modeling.py:418] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:21.417759 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:25.965068 47821886322496 call_variants.py:462] Processed 1 examples in 1 batches [1565.044 sec per 100]. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1378, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1361, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1454, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.DataLossError: corrupted record at 484865306. [[{{node IteratorGetNext}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"",",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:8092,integrability,messag,message,8092,".py"", line 1397, in run. return self._sess.run(*args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1464, in run. outputs = _WrappedSession.run(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1228, in run. return self._sess.run(*args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 968, in run. result = self._run(None, fetches, feed_dict, options_ptr,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1191, in _run. results = self._do_run(handle, final_targets, final_fetches,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1371, in _do_run. return self._do_call(_run_fn, feeds, fetches, targets, options,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1397, in _do_call. raise type(e)(node_def, op, message) # pylint: disable=no-value-for-parameter. tensorflow.python.framework.errors_impl.DataLossError: Graph execution error:. Detected at node 'IteratorGetNext' defined at (most recent call last):. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 621, in predict. features, input_hooks = sel",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:187,interoperability,share,share,187,"Error running deepvariant from singularity in cluster; Dear all,. I was trying to run deepvariant from singularity in cluster, but I always meet same error, I don't know hou to fix it:. /share/app/singularity/3.8.1/bin/singularity exec --containall --bind /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/tmp:/tmp --bind /usr/lib/locale/:/usr/lib/locale/ --bind $ccsbam:$ccsbam --bind $ccsbam.bai:$ccsbam.bai --bind $fasta:$fasta --bind $fasta.fai:$fasta.fai --bind /hwfssz5/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/translatome/HCT116/PacBio/analysis/deepvariant.v3/T202302180201:/output /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/software/deepvariant/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=$fasta --reads=$ccsbam --output_vcf=/output/T202302180201.deepvariant.vcf.gz --output_gvcf=/output/T202302180201.deepvariant.g.vcf.gz --num_shards=20 --intermediate_results_dir=/tmp. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/call_variants_output.tfrecord.gz"" --examples ""/tmp/make_examples.tfrecord@20.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"". 2023-07-15 14:06:58.063861: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0715 14:07:10.199614 47821886322496 call_variants.py:317] From /tmp/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:251,interoperability,bind,bind,251,"Error running deepvariant from singularity in cluster; Dear all,. I was trying to run deepvariant from singularity in cluster, but I always meet same error, I don't know hou to fix it:. /share/app/singularity/3.8.1/bin/singularity exec --containall --bind /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/tmp:/tmp --bind /usr/lib/locale/:/usr/lib/locale/ --bind $ccsbam:$ccsbam --bind $ccsbam.bai:$ccsbam.bai --bind $fasta:$fasta --bind $fasta.fai:$fasta.fai --bind /hwfssz5/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/translatome/HCT116/PacBio/analysis/deepvariant.v3/T202302180201:/output /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/software/deepvariant/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=$fasta --reads=$ccsbam --output_vcf=/output/T202302180201.deepvariant.vcf.gz --output_gvcf=/output/T202302180201.deepvariant.g.vcf.gz --num_shards=20 --intermediate_results_dir=/tmp. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/call_variants_output.tfrecord.gz"" --examples ""/tmp/make_examples.tfrecord@20.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"". 2023-07-15 14:06:58.063861: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0715 14:07:10.199614 47821886322496 call_variants.py:317] From /tmp/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:305,interoperability,bind,bind,305,"Error running deepvariant from singularity in cluster; Dear all,. I was trying to run deepvariant from singularity in cluster, but I always meet same error, I don't know hou to fix it:. /share/app/singularity/3.8.1/bin/singularity exec --containall --bind /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/tmp:/tmp --bind /usr/lib/locale/:/usr/lib/locale/ --bind $ccsbam:$ccsbam --bind $ccsbam.bai:$ccsbam.bai --bind $fasta:$fasta --bind $fasta.fai:$fasta.fai --bind /hwfssz5/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/translatome/HCT116/PacBio/analysis/deepvariant.v3/T202302180201:/output /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/software/deepvariant/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=$fasta --reads=$ccsbam --output_vcf=/output/T202302180201.deepvariant.vcf.gz --output_gvcf=/output/T202302180201.deepvariant.g.vcf.gz --num_shards=20 --intermediate_results_dir=/tmp. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/call_variants_output.tfrecord.gz"" --examples ""/tmp/make_examples.tfrecord@20.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"". 2023-07-15 14:06:58.063861: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0715 14:07:10.199614 47821886322496 call_variants.py:317] From /tmp/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:346,interoperability,bind,bind,346,"Error running deepvariant from singularity in cluster; Dear all,. I was trying to run deepvariant from singularity in cluster, but I always meet same error, I don't know hou to fix it:. /share/app/singularity/3.8.1/bin/singularity exec --containall --bind /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/tmp:/tmp --bind /usr/lib/locale/:/usr/lib/locale/ --bind $ccsbam:$ccsbam --bind $ccsbam.bai:$ccsbam.bai --bind $fasta:$fasta --bind $fasta.fai:$fasta.fai --bind /hwfssz5/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/translatome/HCT116/PacBio/analysis/deepvariant.v3/T202302180201:/output /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/software/deepvariant/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=$fasta --reads=$ccsbam --output_vcf=/output/T202302180201.deepvariant.vcf.gz --output_gvcf=/output/T202302180201.deepvariant.g.vcf.gz --num_shards=20 --intermediate_results_dir=/tmp. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/call_variants_output.tfrecord.gz"" --examples ""/tmp/make_examples.tfrecord@20.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"". 2023-07-15 14:06:58.063861: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0715 14:07:10.199614 47821886322496 call_variants.py:317] From /tmp/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:369,interoperability,bind,bind,369,"Error running deepvariant from singularity in cluster; Dear all,. I was trying to run deepvariant from singularity in cluster, but I always meet same error, I don't know hou to fix it:. /share/app/singularity/3.8.1/bin/singularity exec --containall --bind /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/tmp:/tmp --bind /usr/lib/locale/:/usr/lib/locale/ --bind $ccsbam:$ccsbam --bind $ccsbam.bai:$ccsbam.bai --bind $fasta:$fasta --bind $fasta.fai:$fasta.fai --bind /hwfssz5/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/translatome/HCT116/PacBio/analysis/deepvariant.v3/T202302180201:/output /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/software/deepvariant/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=$fasta --reads=$ccsbam --output_vcf=/output/T202302180201.deepvariant.vcf.gz --output_gvcf=/output/T202302180201.deepvariant.g.vcf.gz --num_shards=20 --intermediate_results_dir=/tmp. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/call_variants_output.tfrecord.gz"" --examples ""/tmp/make_examples.tfrecord@20.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"". 2023-07-15 14:06:58.063861: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0715 14:07:10.199614 47821886322496 call_variants.py:317] From /tmp/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:400,interoperability,bind,bind,400,"Error running deepvariant from singularity in cluster; Dear all,. I was trying to run deepvariant from singularity in cluster, but I always meet same error, I don't know hou to fix it:. /share/app/singularity/3.8.1/bin/singularity exec --containall --bind /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/tmp:/tmp --bind /usr/lib/locale/:/usr/lib/locale/ --bind $ccsbam:$ccsbam --bind $ccsbam.bai:$ccsbam.bai --bind $fasta:$fasta --bind $fasta.fai:$fasta.fai --bind /hwfssz5/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/translatome/HCT116/PacBio/analysis/deepvariant.v3/T202302180201:/output /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/software/deepvariant/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=$fasta --reads=$ccsbam --output_vcf=/output/T202302180201.deepvariant.vcf.gz --output_gvcf=/output/T202302180201.deepvariant.g.vcf.gz --num_shards=20 --intermediate_results_dir=/tmp. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/call_variants_output.tfrecord.gz"" --examples ""/tmp/make_examples.tfrecord@20.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"". 2023-07-15 14:06:58.063861: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0715 14:07:10.199614 47821886322496 call_variants.py:317] From /tmp/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:421,interoperability,bind,bind,421,"Error running deepvariant from singularity in cluster; Dear all,. I was trying to run deepvariant from singularity in cluster, but I always meet same error, I don't know hou to fix it:. /share/app/singularity/3.8.1/bin/singularity exec --containall --bind /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/tmp:/tmp --bind /usr/lib/locale/:/usr/lib/locale/ --bind $ccsbam:$ccsbam --bind $ccsbam.bai:$ccsbam.bai --bind $fasta:$fasta --bind $fasta.fai:$fasta.fai --bind /hwfssz5/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/translatome/HCT116/PacBio/analysis/deepvariant.v3/T202302180201:/output /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/software/deepvariant/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=$fasta --reads=$ccsbam --output_vcf=/output/T202302180201.deepvariant.vcf.gz --output_gvcf=/output/T202302180201.deepvariant.g.vcf.gz --num_shards=20 --intermediate_results_dir=/tmp. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/call_variants_output.tfrecord.gz"" --examples ""/tmp/make_examples.tfrecord@20.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"". 2023-07-15 14:06:58.063861: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0715 14:07:10.199614 47821886322496 call_variants.py:317] From /tmp/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:450,interoperability,bind,bind,450,"Error running deepvariant from singularity in cluster; Dear all,. I was trying to run deepvariant from singularity in cluster, but I always meet same error, I don't know hou to fix it:. /share/app/singularity/3.8.1/bin/singularity exec --containall --bind /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/tmp:/tmp --bind /usr/lib/locale/:/usr/lib/locale/ --bind $ccsbam:$ccsbam --bind $ccsbam.bai:$ccsbam.bai --bind $fasta:$fasta --bind $fasta.fai:$fasta.fai --bind /hwfssz5/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/translatome/HCT116/PacBio/analysis/deepvariant.v3/T202302180201:/output /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/software/deepvariant/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=$fasta --reads=$ccsbam --output_vcf=/output/T202302180201.deepvariant.vcf.gz --output_gvcf=/output/T202302180201.deepvariant.g.vcf.gz --num_shards=20 --intermediate_results_dir=/tmp. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/call_variants_output.tfrecord.gz"" --examples ""/tmp/make_examples.tfrecord@20.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"". 2023-07-15 14:06:58.063861: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0715 14:07:10.199614 47821886322496 call_variants.py:317] From /tmp/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:504,interoperability,translat,translatome,504,"Error running deepvariant from singularity in cluster; Dear all,. I was trying to run deepvariant from singularity in cluster, but I always meet same error, I don't know hou to fix it:. /share/app/singularity/3.8.1/bin/singularity exec --containall --bind /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/tmp:/tmp --bind /usr/lib/locale/:/usr/lib/locale/ --bind $ccsbam:$ccsbam --bind $ccsbam.bai:$ccsbam.bai --bind $fasta:$fasta --bind $fasta.fai:$fasta.fai --bind /hwfssz5/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/translatome/HCT116/PacBio/analysis/deepvariant.v3/T202302180201:/output /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/software/deepvariant/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=$fasta --reads=$ccsbam --output_vcf=/output/T202302180201.deepvariant.vcf.gz --output_gvcf=/output/T202302180201.deepvariant.g.vcf.gz --num_shards=20 --intermediate_results_dir=/tmp. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/call_variants_output.tfrecord.gz"" --examples ""/tmp/make_examples.tfrecord@20.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"". 2023-07-15 14:06:58.063861: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0715 14:07:10.199614 47821886322496 call_variants.py:317] From /tmp/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:1176,interoperability,platform,platform,1176," it:. /share/app/singularity/3.8.1/bin/singularity exec --containall --bind /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/tmp:/tmp --bind /usr/lib/locale/:/usr/lib/locale/ --bind $ccsbam:$ccsbam --bind $ccsbam.bai:$ccsbam.bai --bind $fasta:$fasta --bind $fasta.fai:$fasta.fai --bind /hwfssz5/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/translatome/HCT116/PacBio/analysis/deepvariant.v3/T202302180201:/output /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/software/deepvariant/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=$fasta --reads=$ccsbam --output_vcf=/output/T202302180201.deepvariant.vcf.gz --output_gvcf=/output/T202302180201.deepvariant.g.vcf.gz --num_shards=20 --intermediate_results_dir=/tmp. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/call_variants_output.tfrecord.gz"" --examples ""/tmp/make_examples.tfrecord@20.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"". 2023-07-15 14:06:58.063861: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0715 14:07:10.199614 47821886322496 call_variants.py:317] From /tmp/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:1940,interoperability,platform,platform,1940,"*. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/call_variants_output.tfrecord.gz"" --examples ""/tmp/make_examples.tfrecord@20.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"". 2023-07-15 14:06:58.063861: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0715 14:07:10.199614 47821886322496 call_variants.py:317] From /tmp/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-07-15 14:07:10.223654: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. 2023-07-15 14:07:10.226851: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpl_zjylv7. W0715 14:07:10.308488 47821886322496 estimator.py:1864] Using temporary folder as model directory: /tmp/tmpl_zjylv7. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpl_zjylv7', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_sessio",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:5952,interoperability,platform,platform,5952,"les in 1 batches [1565.044 sec per 100]. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1378, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1361, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1454, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.DataLossError: corrupted record at 484865306. [[{{node IteratorGetNext}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 642, in predict. preds_evaluated = mon_sess.run(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 778, in run. return self._sess.run(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_sessi",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:8092,interoperability,messag,message,8092,".py"", line 1397, in run. return self._sess.run(*args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1464, in run. outputs = _WrappedSession.run(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1228, in run. return self._sess.run(*args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 968, in run. result = self._run(None, fetches, feed_dict, options_ptr,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1191, in _run. results = self._do_run(handle, final_targets, final_fetches,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1371, in _do_run. return self._do_call(_run_fn, feeds, fetches, targets, options,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1397, in _do_call. raise type(e)(node_def, op, message) # pylint: disable=no-value-for-parameter. tensorflow.python.framework.errors_impl.DataLossError: Graph execution error:. Detected at node 'IteratorGetNext' defined at (most recent call last):. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 621, in predict. features, input_hooks = sel",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:9831,interoperability,platform,platform,9831,"com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 621, in predict. features, input_hooks = self._get_features_from_input_fn(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1019, in _get_features_from_input_fn. result, _, hooks = estimator_util.parse_input_fn_result(result). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/util.py"", line 60, in parse_input_fn_result. result = iterator.get_next(). Node: 'IteratorGetNext'. corrupted record at 484865306. [[{{node IteratorGetNext}}]]. Original stack trace for 'IteratorGetNext':. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 621, in predict. features, input_hooks = self._get_features_from_input_fn(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1019, in _get_features_from_input_fn. result, _, hooks = estimator_util.parse_input_fn_result(result). File",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:251,modifiability,bind,bind,251,"Error running deepvariant from singularity in cluster; Dear all,. I was trying to run deepvariant from singularity in cluster, but I always meet same error, I don't know hou to fix it:. /share/app/singularity/3.8.1/bin/singularity exec --containall --bind /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/tmp:/tmp --bind /usr/lib/locale/:/usr/lib/locale/ --bind $ccsbam:$ccsbam --bind $ccsbam.bai:$ccsbam.bai --bind $fasta:$fasta --bind $fasta.fai:$fasta.fai --bind /hwfssz5/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/translatome/HCT116/PacBio/analysis/deepvariant.v3/T202302180201:/output /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/software/deepvariant/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=$fasta --reads=$ccsbam --output_vcf=/output/T202302180201.deepvariant.vcf.gz --output_gvcf=/output/T202302180201.deepvariant.g.vcf.gz --num_shards=20 --intermediate_results_dir=/tmp. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/call_variants_output.tfrecord.gz"" --examples ""/tmp/make_examples.tfrecord@20.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"". 2023-07-15 14:06:58.063861: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0715 14:07:10.199614 47821886322496 call_variants.py:317] From /tmp/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:305,modifiability,bind,bind,305,"Error running deepvariant from singularity in cluster; Dear all,. I was trying to run deepvariant from singularity in cluster, but I always meet same error, I don't know hou to fix it:. /share/app/singularity/3.8.1/bin/singularity exec --containall --bind /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/tmp:/tmp --bind /usr/lib/locale/:/usr/lib/locale/ --bind $ccsbam:$ccsbam --bind $ccsbam.bai:$ccsbam.bai --bind $fasta:$fasta --bind $fasta.fai:$fasta.fai --bind /hwfssz5/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/translatome/HCT116/PacBio/analysis/deepvariant.v3/T202302180201:/output /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/software/deepvariant/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=$fasta --reads=$ccsbam --output_vcf=/output/T202302180201.deepvariant.vcf.gz --output_gvcf=/output/T202302180201.deepvariant.g.vcf.gz --num_shards=20 --intermediate_results_dir=/tmp. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/call_variants_output.tfrecord.gz"" --examples ""/tmp/make_examples.tfrecord@20.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"". 2023-07-15 14:06:58.063861: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0715 14:07:10.199614 47821886322496 call_variants.py:317] From /tmp/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:346,modifiability,bind,bind,346,"Error running deepvariant from singularity in cluster; Dear all,. I was trying to run deepvariant from singularity in cluster, but I always meet same error, I don't know hou to fix it:. /share/app/singularity/3.8.1/bin/singularity exec --containall --bind /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/tmp:/tmp --bind /usr/lib/locale/:/usr/lib/locale/ --bind $ccsbam:$ccsbam --bind $ccsbam.bai:$ccsbam.bai --bind $fasta:$fasta --bind $fasta.fai:$fasta.fai --bind /hwfssz5/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/translatome/HCT116/PacBio/analysis/deepvariant.v3/T202302180201:/output /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/software/deepvariant/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=$fasta --reads=$ccsbam --output_vcf=/output/T202302180201.deepvariant.vcf.gz --output_gvcf=/output/T202302180201.deepvariant.g.vcf.gz --num_shards=20 --intermediate_results_dir=/tmp. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/call_variants_output.tfrecord.gz"" --examples ""/tmp/make_examples.tfrecord@20.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"". 2023-07-15 14:06:58.063861: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0715 14:07:10.199614 47821886322496 call_variants.py:317] From /tmp/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:369,modifiability,bind,bind,369,"Error running deepvariant from singularity in cluster; Dear all,. I was trying to run deepvariant from singularity in cluster, but I always meet same error, I don't know hou to fix it:. /share/app/singularity/3.8.1/bin/singularity exec --containall --bind /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/tmp:/tmp --bind /usr/lib/locale/:/usr/lib/locale/ --bind $ccsbam:$ccsbam --bind $ccsbam.bai:$ccsbam.bai --bind $fasta:$fasta --bind $fasta.fai:$fasta.fai --bind /hwfssz5/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/translatome/HCT116/PacBio/analysis/deepvariant.v3/T202302180201:/output /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/software/deepvariant/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=$fasta --reads=$ccsbam --output_vcf=/output/T202302180201.deepvariant.vcf.gz --output_gvcf=/output/T202302180201.deepvariant.g.vcf.gz --num_shards=20 --intermediate_results_dir=/tmp. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/call_variants_output.tfrecord.gz"" --examples ""/tmp/make_examples.tfrecord@20.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"". 2023-07-15 14:06:58.063861: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0715 14:07:10.199614 47821886322496 call_variants.py:317] From /tmp/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:400,modifiability,bind,bind,400,"Error running deepvariant from singularity in cluster; Dear all,. I was trying to run deepvariant from singularity in cluster, but I always meet same error, I don't know hou to fix it:. /share/app/singularity/3.8.1/bin/singularity exec --containall --bind /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/tmp:/tmp --bind /usr/lib/locale/:/usr/lib/locale/ --bind $ccsbam:$ccsbam --bind $ccsbam.bai:$ccsbam.bai --bind $fasta:$fasta --bind $fasta.fai:$fasta.fai --bind /hwfssz5/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/translatome/HCT116/PacBio/analysis/deepvariant.v3/T202302180201:/output /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/software/deepvariant/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=$fasta --reads=$ccsbam --output_vcf=/output/T202302180201.deepvariant.vcf.gz --output_gvcf=/output/T202302180201.deepvariant.g.vcf.gz --num_shards=20 --intermediate_results_dir=/tmp. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/call_variants_output.tfrecord.gz"" --examples ""/tmp/make_examples.tfrecord@20.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"". 2023-07-15 14:06:58.063861: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0715 14:07:10.199614 47821886322496 call_variants.py:317] From /tmp/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:421,modifiability,bind,bind,421,"Error running deepvariant from singularity in cluster; Dear all,. I was trying to run deepvariant from singularity in cluster, but I always meet same error, I don't know hou to fix it:. /share/app/singularity/3.8.1/bin/singularity exec --containall --bind /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/tmp:/tmp --bind /usr/lib/locale/:/usr/lib/locale/ --bind $ccsbam:$ccsbam --bind $ccsbam.bai:$ccsbam.bai --bind $fasta:$fasta --bind $fasta.fai:$fasta.fai --bind /hwfssz5/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/translatome/HCT116/PacBio/analysis/deepvariant.v3/T202302180201:/output /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/software/deepvariant/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=$fasta --reads=$ccsbam --output_vcf=/output/T202302180201.deepvariant.vcf.gz --output_gvcf=/output/T202302180201.deepvariant.g.vcf.gz --num_shards=20 --intermediate_results_dir=/tmp. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/call_variants_output.tfrecord.gz"" --examples ""/tmp/make_examples.tfrecord@20.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"". 2023-07-15 14:06:58.063861: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0715 14:07:10.199614 47821886322496 call_variants.py:317] From /tmp/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:450,modifiability,bind,bind,450,"Error running deepvariant from singularity in cluster; Dear all,. I was trying to run deepvariant from singularity in cluster, but I always meet same error, I don't know hou to fix it:. /share/app/singularity/3.8.1/bin/singularity exec --containall --bind /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/tmp:/tmp --bind /usr/lib/locale/:/usr/lib/locale/ --bind $ccsbam:$ccsbam --bind $ccsbam.bai:$ccsbam.bai --bind $fasta:$fasta --bind $fasta.fai:$fasta.fai --bind /hwfssz5/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/translatome/HCT116/PacBio/analysis/deepvariant.v3/T202302180201:/output /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/software/deepvariant/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=$fasta --reads=$ccsbam --output_vcf=/output/T202302180201.deepvariant.vcf.gz --output_gvcf=/output/T202302180201.deepvariant.g.vcf.gz --num_shards=20 --intermediate_results_dir=/tmp. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/call_variants_output.tfrecord.gz"" --examples ""/tmp/make_examples.tfrecord@20.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"". 2023-07-15 14:06:58.063861: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0715 14:07:10.199614 47821886322496 call_variants.py:317] From /tmp/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:523,modifiability,Pac,PacBio,523,"Error running deepvariant from singularity in cluster; Dear all,. I was trying to run deepvariant from singularity in cluster, but I always meet same error, I don't know hou to fix it:. /share/app/singularity/3.8.1/bin/singularity exec --containall --bind /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/tmp:/tmp --bind /usr/lib/locale/:/usr/lib/locale/ --bind $ccsbam:$ccsbam --bind $ccsbam.bai:$ccsbam.bai --bind $fasta:$fasta --bind $fasta.fai:$fasta.fai --bind /hwfssz5/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/translatome/HCT116/PacBio/analysis/deepvariant.v3/T202302180201:/output /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/software/deepvariant/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=$fasta --reads=$ccsbam --output_vcf=/output/T202302180201.deepvariant.vcf.gz --output_gvcf=/output/T202302180201.deepvariant.g.vcf.gz --num_shards=20 --intermediate_results_dir=/tmp. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/call_variants_output.tfrecord.gz"" --examples ""/tmp/make_examples.tfrecord@20.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"". 2023-07-15 14:06:58.063861: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0715 14:07:10.199614 47821886322496 call_variants.py:317] From /tmp/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:718,modifiability,PAC,PACBIO,718,"Error running deepvariant from singularity in cluster; Dear all,. I was trying to run deepvariant from singularity in cluster, but I always meet same error, I don't know hou to fix it:. /share/app/singularity/3.8.1/bin/singularity exec --containall --bind /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/tmp:/tmp --bind /usr/lib/locale/:/usr/lib/locale/ --bind $ccsbam:$ccsbam --bind $ccsbam.bai:$ccsbam.bai --bind $fasta:$fasta --bind $fasta.fai:$fasta.fai --bind /hwfssz5/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/translatome/HCT116/PacBio/analysis/deepvariant.v3/T202302180201:/output /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/software/deepvariant/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=$fasta --reads=$ccsbam --output_vcf=/output/T202302180201.deepvariant.vcf.gz --output_gvcf=/output/T202302180201.deepvariant.g.vcf.gz --num_shards=20 --intermediate_results_dir=/tmp. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/call_variants_output.tfrecord.gz"" --examples ""/tmp/make_examples.tfrecord@20.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"". 2023-07-15 14:06:58.063861: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0715 14:07:10.199614 47821886322496 call_variants.py:317] From /tmp/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:1110,modifiability,pac,pacbio,1110,"y in cluster, but I always meet same error, I don't know hou to fix it:. /share/app/singularity/3.8.1/bin/singularity exec --containall --bind /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/tmp:/tmp --bind /usr/lib/locale/:/usr/lib/locale/ --bind $ccsbam:$ccsbam --bind $ccsbam.bai:$ccsbam.bai --bind $fasta:$fasta --bind $fasta.fai:$fasta.fai --bind /hwfssz5/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/translatome/HCT116/PacBio/analysis/deepvariant.v3/T202302180201:/output /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/software/deepvariant/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=$fasta --reads=$ccsbam --output_vcf=/output/T202302180201.deepvariant.vcf.gz --output_gvcf=/output/T202302180201.deepvariant.g.vcf.gz --num_shards=20 --intermediate_results_dir=/tmp. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/call_variants_output.tfrecord.gz"" --examples ""/tmp/make_examples.tfrecord@20.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"". 2023-07-15 14:06:58.063861: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0715 14:07:10.199614 47821886322496 call_variants.py:317] From /tmp/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:1759,modifiability,pac,pacbio,1759,"_vcf=/output/T202302180201.deepvariant.vcf.gz --output_gvcf=/output/T202302180201.deepvariant.g.vcf.gz --num_shards=20 --intermediate_results_dir=/tmp. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/call_variants_output.tfrecord.gz"" --examples ""/tmp/make_examples.tfrecord@20.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"". 2023-07-15 14:06:58.063861: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0715 14:07:10.199614 47821886322496 call_variants.py:317] From /tmp/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-07-15 14:07:10.223654: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. 2023-07-15 14:07:10.226851: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpl_zjylv7. W0715 14:07:10.308488 47821886322496 estimator.py:1864] Using temporary folder as model directory: /tmp/tmpl_zjylv7. INFO:ten",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:3475,modifiability,pac,packages,3475,"lir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpl_zjylv7. W0715 14:07:10.308488 47821886322496 estimator.py:1864] Using temporary folder as model directory: /tmp/tmpl_zjylv7. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpl_zjylv7', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 10000. I0715 14:07:10.309390 47821886322496 estimator.py:202] Using config: {'_model_dir': '/tmp/tmpl_zjylv7', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_c. I0715 14:07:10.309847 47821886322496 call_variants.py:446] Writing calls to /tmp/call_variants_output.tfrecord.gz. INFO:tensorflow:Calling model_fn. I0715 14:07:10.758818 47821886322496 estimator.py:1173] Calling model_fn. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0715 14:07:16.873909 47821886322496 estimator.py:1175] Done calling model_fn. 2023-07-15 14:07:17.753203: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. INFO:tensorflow:Graph was finalized. I0715 14:07:18.197918 47821886322496 monitored_session.py:240] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:18.198770 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. INFO:tensorflow:Running local_init_op. I0715 14:07:20.793029 47821886322496 session_manager.py:526] Running local_init_op. INFO:tensorflow:Do",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:3552,modifiability,layer,layer,3552,". WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpl_zjylv7. W0715 14:07:10.308488 47821886322496 estimator.py:1864] Using temporary folder as model directory: /tmp/tmpl_zjylv7. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpl_zjylv7', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 10000. I0715 14:07:10.309390 47821886322496 estimator.py:202] Using config: {'_model_dir': '/tmp/tmpl_zjylv7', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_c. I0715 14:07:10.309847 47821886322496 call_variants.py:446] Writing calls to /tmp/call_variants_output.tfrecord.gz. INFO:tensorflow:Calling model_fn. I0715 14:07:10.758818 47821886322496 estimator.py:1173] Calling model_fn. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0715 14:07:16.873909 47821886322496 estimator.py:1175] Done calling model_fn. 2023-07-15 14:07:17.753203: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. INFO:tensorflow:Graph was finalized. I0715 14:07:18.197918 47821886322496 monitored_session.py:240] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:18.198770 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. INFO:tensorflow:Running local_init_op. I0715 14:07:20.793029 47821886322496 session_manager.py:526] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0715 14:07:20.857252 47821886322496 session_manag",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:3611,modifiability,version,version,3611,"ry: /tmp/tmpl_zjylv7. W0715 14:07:10.308488 47821886322496 estimator.py:1864] Using temporary folder as model directory: /tmp/tmpl_zjylv7. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpl_zjylv7', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 10000. I0715 14:07:10.309390 47821886322496 estimator.py:202] Using config: {'_model_dir': '/tmp/tmpl_zjylv7', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_c. I0715 14:07:10.309847 47821886322496 call_variants.py:446] Writing calls to /tmp/call_variants_output.tfrecord.gz. INFO:tensorflow:Calling model_fn. I0715 14:07:10.758818 47821886322496 estimator.py:1173] Calling model_fn. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0715 14:07:16.873909 47821886322496 estimator.py:1175] Done calling model_fn. 2023-07-15 14:07:17.753203: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. INFO:tensorflow:Graph was finalized. I0715 14:07:18.197918 47821886322496 monitored_session.py:240] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:18.198770 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. INFO:tensorflow:Running local_init_op. I0715 14:07:20.793029 47821886322496 session_manager.py:526] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0715 14:07:20.857252 47821886322496 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloa",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:3632,modifiability,layer,layer,3632,". W0715 14:07:10.308488 47821886322496 estimator.py:1864] Using temporary folder as model directory: /tmp/tmpl_zjylv7. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpl_zjylv7', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 10000. I0715 14:07:10.309390 47821886322496 estimator.py:202] Using config: {'_model_dir': '/tmp/tmpl_zjylv7', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_c. I0715 14:07:10.309847 47821886322496 call_variants.py:446] Writing calls to /tmp/call_variants_output.tfrecord.gz. INFO:tensorflow:Calling model_fn. I0715 14:07:10.758818 47821886322496 estimator.py:1173] Calling model_fn. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0715 14:07:16.873909 47821886322496 estimator.py:1175] Done calling model_fn. 2023-07-15 14:07:17.753203: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. INFO:tensorflow:Graph was finalized. I0715 14:07:18.197918 47821886322496 monitored_session.py:240] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:18.198770 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. INFO:tensorflow:Running local_init_op. I0715 14:07:20.793029 47821886322496 session_manager.py:526] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0715 14:07:20.857252 47821886322496 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0715 14",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:3680,modifiability,layer,layer,3680,".py:1864] Using temporary folder as model directory: /tmp/tmpl_zjylv7. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpl_zjylv7', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 10000. I0715 14:07:10.309390 47821886322496 estimator.py:202] Using config: {'_model_dir': '/tmp/tmpl_zjylv7', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_c. I0715 14:07:10.309847 47821886322496 call_variants.py:446] Writing calls to /tmp/call_variants_output.tfrecord.gz. INFO:tensorflow:Calling model_fn. I0715 14:07:10.758818 47821886322496 estimator.py:1173] Calling model_fn. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0715 14:07:16.873909 47821886322496 estimator.py:1175] Done calling model_fn. 2023-07-15 14:07:17.753203: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. INFO:tensorflow:Graph was finalized. I0715 14:07:18.197918 47821886322496 monitored_session.py:240] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:18.198770 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. INFO:tensorflow:Running local_init_op. I0715 14:07:20.793029 47821886322496 session_manager.py:526] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0715 14:07:20.857252 47821886322496 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0715 14:07:21.417166 47821886322496 modeling.py:418] Re",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:4182,modifiability,paramet,parameters,4182,"eckpoints_secs': 600, '_session_c. I0715 14:07:10.309847 47821886322496 call_variants.py:446] Writing calls to /tmp/call_variants_output.tfrecord.gz. INFO:tensorflow:Calling model_fn. I0715 14:07:10.758818 47821886322496 estimator.py:1173] Calling model_fn. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0715 14:07:16.873909 47821886322496 estimator.py:1175] Done calling model_fn. 2023-07-15 14:07:17.753203: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. INFO:tensorflow:Graph was finalized. I0715 14:07:18.197918 47821886322496 monitored_session.py:240] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:18.198770 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. INFO:tensorflow:Running local_init_op. I0715 14:07:20.793029 47821886322496 session_manager.py:526] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0715 14:07:20.857252 47821886322496 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0715 14:07:21.417166 47821886322496 modeling.py:418] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:21.417759 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:25.965068 47821886322496 call_variants.py:462] Processed 1 examples in 1 batches [1565.044 sec per 100]. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1378, in _do_call. return fn(*args). File ""/usr/local/lib/python3.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:4210,modifiability,pac,pacbio,4210,"ssion_c. I0715 14:07:10.309847 47821886322496 call_variants.py:446] Writing calls to /tmp/call_variants_output.tfrecord.gz. INFO:tensorflow:Calling model_fn. I0715 14:07:10.758818 47821886322496 estimator.py:1173] Calling model_fn. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0715 14:07:16.873909 47821886322496 estimator.py:1175] Done calling model_fn. 2023-07-15 14:07:17.753203: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. INFO:tensorflow:Graph was finalized. I0715 14:07:18.197918 47821886322496 monitored_session.py:240] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:18.198770 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. INFO:tensorflow:Running local_init_op. I0715 14:07:20.793029 47821886322496 session_manager.py:526] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0715 14:07:20.857252 47821886322496 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0715 14:07:21.417166 47821886322496 modeling.py:418] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:21.417759 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:25.965068 47821886322496 call_variants.py:462] Processed 1 examples in 1 batches [1565.044 sec per 100]. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1378, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:4291,modifiability,paramet,parameters,4291,"o /tmp/call_variants_output.tfrecord.gz. INFO:tensorflow:Calling model_fn. I0715 14:07:10.758818 47821886322496 estimator.py:1173] Calling model_fn. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0715 14:07:16.873909 47821886322496 estimator.py:1175] Done calling model_fn. 2023-07-15 14:07:17.753203: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. INFO:tensorflow:Graph was finalized. I0715 14:07:18.197918 47821886322496 monitored_session.py:240] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:18.198770 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. INFO:tensorflow:Running local_init_op. I0715 14:07:20.793029 47821886322496 session_manager.py:526] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0715 14:07:20.857252 47821886322496 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0715 14:07:21.417166 47821886322496 modeling.py:418] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:21.417759 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:25.965068 47821886322496 call_variants.py:462] Processed 1 examples in 1 batches [1565.044 sec per 100]. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1378, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1361, in _run_fn. return self._call_tf_sessionrun(",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:4319,modifiability,pac,pacbio,4319,"t.tfrecord.gz. INFO:tensorflow:Calling model_fn. I0715 14:07:10.758818 47821886322496 estimator.py:1173] Calling model_fn. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0715 14:07:16.873909 47821886322496 estimator.py:1175] Done calling model_fn. 2023-07-15 14:07:17.753203: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. INFO:tensorflow:Graph was finalized. I0715 14:07:18.197918 47821886322496 monitored_session.py:240] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:18.198770 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. INFO:tensorflow:Running local_init_op. I0715 14:07:20.793029 47821886322496 session_manager.py:526] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0715 14:07:20.857252 47821886322496 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0715 14:07:21.417166 47821886322496 modeling.py:418] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:21.417759 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:25.965068 47821886322496 call_variants.py:462] Processed 1 examples in 1 batches [1565.044 sec per 100]. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1378, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1361, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:4724,modifiability,paramet,parameters,4724,":Done calling model_fn. I0715 14:07:16.873909 47821886322496 estimator.py:1175] Done calling model_fn. 2023-07-15 14:07:17.753203: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. INFO:tensorflow:Graph was finalized. I0715 14:07:18.197918 47821886322496 monitored_session.py:240] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:18.198770 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. INFO:tensorflow:Running local_init_op. I0715 14:07:20.793029 47821886322496 session_manager.py:526] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0715 14:07:20.857252 47821886322496 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0715 14:07:21.417166 47821886322496 modeling.py:418] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:21.417759 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:25.965068 47821886322496 call_variants.py:462] Processed 1 examples in 1 batches [1565.044 sec per 100]. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1378, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1361, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1454, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.DataLossError: corrupted record at 484865306. [[{{node IteratorGetNext}}]]. During handling of the above exception, another exception occurred:. Traceback (most recen",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:4752,modifiability,pac,pacbio,4752,"715 14:07:16.873909 47821886322496 estimator.py:1175] Done calling model_fn. 2023-07-15 14:07:17.753203: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. INFO:tensorflow:Graph was finalized. I0715 14:07:18.197918 47821886322496 monitored_session.py:240] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:18.198770 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. INFO:tensorflow:Running local_init_op. I0715 14:07:20.793029 47821886322496 session_manager.py:526] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0715 14:07:20.857252 47821886322496 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0715 14:07:21.417166 47821886322496 modeling.py:418] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:21.417759 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:25.965068 47821886322496 call_variants.py:462] Processed 1 examples in 1 batches [1565.044 sec per 100]. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1378, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1361, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1454, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.DataLossError: corrupted record at 484865306. [[{{node IteratorGetNext}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:4833,modifiability,paramet,parameters,4833,"7-15 14:07:17.753203: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. INFO:tensorflow:Graph was finalized. I0715 14:07:18.197918 47821886322496 monitored_session.py:240] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:18.198770 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. INFO:tensorflow:Running local_init_op. I0715 14:07:20.793029 47821886322496 session_manager.py:526] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0715 14:07:20.857252 47821886322496 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0715 14:07:21.417166 47821886322496 modeling.py:418] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:21.417759 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:25.965068 47821886322496 call_variants.py:462] Processed 1 examples in 1 batches [1565.044 sec per 100]. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1378, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1361, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1454, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.DataLossError: corrupted record at 484865306. [[{{node IteratorGetNext}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.p",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:4861,modifiability,pac,pacbio,4861,"nsorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. INFO:tensorflow:Graph was finalized. I0715 14:07:18.197918 47821886322496 monitored_session.py:240] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:18.198770 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. INFO:tensorflow:Running local_init_op. I0715 14:07:20.793029 47821886322496 session_manager.py:526] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0715 14:07:20.857252 47821886322496 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0715 14:07:21.417166 47821886322496 modeling.py:418] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:21.417759 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:25.965068 47821886322496 call_variants.py:462] Processed 1 examples in 1 batches [1565.044 sec per 100]. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1378, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1361, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1454, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.DataLossError: corrupted record at 484865306. [[{{node IteratorGetNext}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:5069,modifiability,pac,packages,5069,"0715 14:07:18.197918 47821886322496 monitored_session.py:240] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:18.198770 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. INFO:tensorflow:Running local_init_op. I0715 14:07:20.793029 47821886322496 session_manager.py:526] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0715 14:07:20.857252 47821886322496 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0715 14:07:21.417166 47821886322496 modeling.py:418] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:21.417759 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:25.965068 47821886322496 call_variants.py:462] Processed 1 examples in 1 batches [1565.044 sec per 100]. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1378, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1361, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1454, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.DataLossError: corrupted record at 484865306. [[{{node IteratorGetNext}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:5194,modifiability,pac,packages,5194,"/opt/models/pacbio/model.ckpt. I0715 14:07:18.198770 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. INFO:tensorflow:Running local_init_op. I0715 14:07:20.793029 47821886322496 session_manager.py:526] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0715 14:07:20.857252 47821886322496 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0715 14:07:21.417166 47821886322496 modeling.py:418] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:21.417759 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:25.965068 47821886322496 call_variants.py:462] Processed 1 examples in 1 batches [1565.044 sec per 100]. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1378, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1361, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1454, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.DataLossError: corrupted record at 484865306. [[{{node IteratorGetNext}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:5365,modifiability,pac,packages,5365,"nit_op. I0715 14:07:20.793029 47821886322496 session_manager.py:526] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0715 14:07:20.857252 47821886322496 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0715 14:07:21.417166 47821886322496 modeling.py:418] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:21.417759 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:25.965068 47821886322496 call_variants.py:462] Processed 1 examples in 1 batches [1565.044 sec per 100]. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1378, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1361, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1454, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.DataLossError: corrupted record at 484865306. [[{{node IteratorGetNext}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:5856,modifiability,modul,module,5856,"s/pacbio/model.ckpt. I0715 14:07:25.965068 47821886322496 call_variants.py:462] Processed 1 examples in 1 batches [1565.044 sec per 100]. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1378, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1361, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1454, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.DataLossError: corrupted record at 484865306. [[{{node IteratorGetNext}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 642, in predict. preds_evaluated = mon_sess.run(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 778, in run. return self._s",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:5925,modifiability,pac,packages,5925,"s.py:462] Processed 1 examples in 1 batches [1565.044 sec per 100]. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1378, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1361, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1454, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.DataLossError: corrupted record at 484865306. [[{{node IteratorGetNext}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 642, in predict. preds_evaluated = mon_sess.run(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 778, in run. return self._sess.run(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/pyth",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:6603,modifiability,pac,packages,6603,". [[{{node IteratorGetNext}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 642, in predict. preds_evaluated = mon_sess.run(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 778, in run. return self._sess.run(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1307, in run. return self._sess.run(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1397, in run. return self._sess.run(*args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1464, in run. outputs = _WrappedSession.run(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1228, in run. return self._sess.run(*args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 968, in run. result = self._run(None, fetches, feed_dict, ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:6768,modifiability,pac,packages,6768,"t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 642, in predict. preds_evaluated = mon_sess.run(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 778, in run. return self._sess.run(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1307, in run. return self._sess.run(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1397, in run. return self._sess.run(*args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1464, in run. outputs = _WrappedSession.run(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1228, in run. return self._sess.run(*args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 968, in run. result = self._run(None, fetches, feed_dict, options_ptr,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1191, in _run. results = self._do_run(handle, final_targets, fi",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:6905,modifiability,pac,packages,6905,"/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 642, in predict. preds_evaluated = mon_sess.run(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 778, in run. return self._sess.run(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1307, in run. return self._sess.run(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1397, in run. return self._sess.run(*args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1464, in run. outputs = _WrappedSession.run(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1228, in run. return self._sess.run(*args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 968, in run. result = self._run(None, fetches, feed_dict, options_ptr,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1191, in _run. results = self._do_run(handle, final_targets, final_fetches,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1371, in _do_run. return self._do_c",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:7043,modifiability,pac,packages,7043,"e_undef). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 642, in predict. preds_evaluated = mon_sess.run(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 778, in run. return self._sess.run(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1307, in run. return self._sess.run(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1397, in run. return self._sess.run(*args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1464, in run. outputs = _WrappedSession.run(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1228, in run. return self._sess.run(*args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 968, in run. result = self._run(None, fetches, feed_dict, options_ptr,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1191, in _run. results = self._do_run(handle, final_targets, final_fetches,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1371, in _do_run. return self._do_call(_run_fn, feeds, fetches, targets, options,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 13",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:7197,modifiability,pac,packages,7197,"nfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 642, in predict. preds_evaluated = mon_sess.run(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 778, in run. return self._sess.run(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1307, in run. return self._sess.run(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1397, in run. return self._sess.run(*args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1464, in run. outputs = _WrappedSession.run(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1228, in run. return self._sess.run(*args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 968, in run. result = self._run(None, fetches, feed_dict, options_ptr,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1191, in _run. results = self._do_run(handle, final_targets, final_fetches,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1371, in _do_run. return self._do_call(_run_fn, feeds, fetches, targets, options,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1397, in _do_call. raise type(e)(node_def, op, message) # pylint: disable=no-value-for-parameter. tensorflow.python.framework.errors_impl.DataLossError: Gra",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:7343,modifiability,pac,packages,7343,"variant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 642, in predict. preds_evaluated = mon_sess.run(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 778, in run. return self._sess.run(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1307, in run. return self._sess.run(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1397, in run. return self._sess.run(*args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1464, in run. outputs = _WrappedSession.run(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1228, in run. return self._sess.run(*args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 968, in run. result = self._run(None, fetches, feed_dict, options_ptr,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1191, in _run. results = self._do_run(handle, final_targets, final_fetches,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1371, in _do_run. return self._do_call(_run_fn, feeds, fetches, targets, options,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1397, in _do_call. raise type(e)(node_def, op, message) # pylint: disable=no-value-for-parameter. tensorflow.python.framework.errors_impl.DataLossError: Graph execution error:. Detected at node 'IteratorGetNext' defined at (most recent call last):. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_googl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:7497,modifiability,pac,packages,7497,".py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 642, in predict. preds_evaluated = mon_sess.run(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 778, in run. return self._sess.run(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1307, in run. return self._sess.run(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1397, in run. return self._sess.run(*args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1464, in run. outputs = _WrappedSession.run(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1228, in run. return self._sess.run(*args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 968, in run. result = self._run(None, fetches, feed_dict, options_ptr,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1191, in _run. results = self._do_run(handle, final_targets, final_fetches,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1371, in _do_run. return self._do_call(_run_fn, feeds, fetches, targets, options,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1397, in _do_call. raise type(e)(node_def, op, message) # pylint: disable=no-value-for-parameter. tensorflow.python.framework.errors_impl.DataLossError: Graph execution error:. Detected at node 'IteratorGetNext' defined at (most recent call last):. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.p",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:7657,modifiability,pac,packages,7657,"y"", line 642, in predict. preds_evaluated = mon_sess.run(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 778, in run. return self._sess.run(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1307, in run. return self._sess.run(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1397, in run. return self._sess.run(*args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1464, in run. outputs = _WrappedSession.run(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1228, in run. return self._sess.run(*args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 968, in run. result = self._run(None, fetches, feed_dict, options_ptr,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1191, in _run. results = self._do_run(handle, final_targets, final_fetches,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1371, in _do_run. return self._do_call(_run_fn, feeds, fetches, targets, options,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1397, in _do_call. raise type(e)(node_def, op, message) # pylint: disable=no-value-for-parameter. tensorflow.python.framework.errors_impl.DataLossError: Graph execution error:. Detected at node 'IteratorGetNext' defined at (most recent call last):. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). Fil",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:7822,modifiability,pac,packages,7822," line 778, in run. return self._sess.run(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1307, in run. return self._sess.run(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1397, in run. return self._sess.run(*args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1464, in run. outputs = _WrappedSession.run(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1228, in run. return self._sess.run(*args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 968, in run. result = self._run(None, fetches, feed_dict, options_ptr,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1191, in _run. results = self._do_run(handle, final_targets, final_fetches,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1371, in _do_run. return self._do_call(_run_fn, feeds, fetches, targets, options,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1397, in _do_call. raise type(e)(node_def, op, message) # pylint: disable=no-value-for-parameter. tensorflow.python.framework.errors_impl.DataLossError: Graph execution error:. Detected at node 'IteratorGetNext' defined at (most recent call last):. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:7993,modifiability,pac,packages,7993,"ess.run(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1397, in run. return self._sess.run(*args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1464, in run. outputs = _WrappedSession.run(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1228, in run. return self._sess.run(*args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 968, in run. result = self._run(None, fetches, feed_dict, options_ptr,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1191, in _run. results = self._do_run(handle, final_targets, final_fetches,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1371, in _do_run. return self._do_call(_run_fn, feeds, fetches, targets, options,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1397, in _do_call. raise type(e)(node_def, op, message) # pylint: disable=no-value-for-parameter. tensorflow.python.framework.errors_impl.DataLossError: Graph execution error:. Detected at node 'IteratorGetNext' defined at (most recent call last):. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/ten",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:8132,modifiability,paramet,parameter,8132,"s.run(*args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1464, in run. outputs = _WrappedSession.run(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1228, in run. return self._sess.run(*args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 968, in run. result = self._run(None, fetches, feed_dict, options_ptr,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1191, in _run. results = self._do_run(handle, final_targets, final_fetches,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1371, in _do_run. return self._do_call(_run_fn, feeds, fetches, targets, options,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1397, in _do_call. raise type(e)(node_def, op, message) # pylint: disable=no-value-for-parameter. tensorflow.python.framework.errors_impl.DataLossError: Graph execution error:. Detected at node 'IteratorGetNext' defined at (most recent call last):. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 621, in predict. features, input_hooks = self._get_features_from_input_fn(. File ""/us",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:8406,modifiability,modul,module,8406,"28, in run. return self._sess.run(*args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 968, in run. result = self._run(None, fetches, feed_dict, options_ptr,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1191, in _run. results = self._do_run(handle, final_targets, final_fetches,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1371, in _do_run. return self._do_call(_run_fn, feeds, fetches, targets, options,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1397, in _do_call. raise type(e)(node_def, op, message) # pylint: disable=no-value-for-parameter. tensorflow.python.framework.errors_impl.DataLossError: Graph execution error:. Detected at node 'IteratorGetNext' defined at (most recent call last):. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 621, in predict. features, input_hooks = self._get_features_from_input_fn(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1019, in _get_features_from_input_fn. result, _, hooks = estimator_util.parse_input_fn_result(result). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/es",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:8985,modifiability,pac,packages,8985,"ist-packages/tensorflow/python/client/session.py"", line 1397, in _do_call. raise type(e)(node_def, op, message) # pylint: disable=no-value-for-parameter. tensorflow.python.framework.errors_impl.DataLossError: Graph execution error:. Detected at node 'IteratorGetNext' defined at (most recent call last):. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 621, in predict. features, input_hooks = self._get_features_from_input_fn(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1019, in _get_features_from_input_fn. result, _, hooks = estimator_util.parse_input_fn_result(result). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/util.py"", line 60, in parse_input_fn_result. result = iterator.get_next(). Node: 'IteratorGetNext'. corrupted record at 484865306. [[{{node IteratorGetNext}}]]. Original stack trace for 'IteratorGetNext':. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/a",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:9164,modifiability,pac,packages,9164,"rk.errors_impl.DataLossError: Graph execution error:. Detected at node 'IteratorGetNext' defined at (most recent call last):. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 621, in predict. features, input_hooks = self._get_features_from_input_fn(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1019, in _get_features_from_input_fn. result, _, hooks = estimator_util.parse_input_fn_result(result). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/util.py"", line 60, in parse_input_fn_result. result = iterator.get_next(). Node: 'IteratorGetNext'. corrupted record at 484865306. [[{{node IteratorGetNext}}]]. Original stack trace for 'IteratorGetNext':. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Ba",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:9370,modifiability,pac,packages,9370,"all_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 621, in predict. features, input_hooks = self._get_features_from_input_fn(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1019, in _get_features_from_input_fn. result, _, hooks = estimator_util.parse_input_fn_result(result). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/util.py"", line 60, in parse_input_fn_result. result = iterator.get_next(). Node: 'IteratorGetNext'. corrupted record at 484865306. [[{{node IteratorGetNext}}]]. Original stack trace for 'IteratorGetNext':. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_va",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:9735,modifiability,modul,module,9735,"all_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 621, in predict. features, input_hooks = self._get_features_from_input_fn(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1019, in _get_features_from_input_fn. result, _, hooks = estimator_util.parse_input_fn_result(result). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/util.py"", line 60, in parse_input_fn_result. result = iterator.get_next(). Node: 'IteratorGetNext'. corrupted record at 484865306. [[{{node IteratorGetNext}}]]. Original stack trace for 'IteratorGetNext':. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 621, in predict. features, input_hooks = self._get_features_from_input_fn(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1019, in _",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:9804,modifiability,pac,packages,9804,"runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 621, in predict. features, input_hooks = self._get_features_from_input_fn(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1019, in _get_features_from_input_fn. result, _, hooks = estimator_util.parse_input_fn_result(result). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/util.py"", line 60, in parse_input_fn_result. result = iterator.get_next(). Node: 'IteratorGetNext'. corrupted record at 484865306. [[{{node IteratorGetNext}}]]. Original stack trace for 'IteratorGetNext':. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 621, in predict. features, input_hooks = self._get_features_from_input_fn(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1019, in _get_features_from_input_fn. result, _, hooks = estimator_util.parse_in",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:10482,modifiability,pac,packages,10482,"xt(). Node: 'IteratorGetNext'. corrupted record at 484865306. [[{{node IteratorGetNext}}]]. Original stack trace for 'IteratorGetNext':. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 621, in predict. features, input_hooks = self._get_features_from_input_fn(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1019, in _get_features_from_input_fn. result, _, hooks = estimator_util.parse_input_fn_result(result). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/util.py"", line 60, in parse_input_fn_result. result = iterator.get_next(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/iterator_ops.py"", line 445, in get_next. flat_ret = gen_dataset_ops.iterator_get_next(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_dataset_ops.py"", line 3037, in iterator_get_next. _, _, _op, _outputs = _op_def_library._apply_op_helper(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py"", line 795, in _apply_op_helper. op = g._create_op_internal(op_type_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:10661,modifiability,pac,packages,10661,"s/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 621, in predict. features, input_hooks = self._get_features_from_input_fn(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1019, in _get_features_from_input_fn. result, _, hooks = estimator_util.parse_input_fn_result(result). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/util.py"", line 60, in parse_input_fn_result. result = iterator.get_next(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/iterator_ops.py"", line 445, in get_next. flat_ret = gen_dataset_ops.iterator_get_next(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_dataset_ops.py"", line 3037, in iterator_get_next. _, _, _op, _outputs = _op_def_library._apply_op_helper(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py"", line 795, in _apply_op_helper. op = g._create_op_internal(op_type_name, inputs, dtypes=None,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 3798, in _create_op_internal. ret = Operation(. Best Regards! Bo",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:10867,modifiability,pac,packages,10867,"/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 621, in predict. features, input_hooks = self._get_features_from_input_fn(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1019, in _get_features_from_input_fn. result, _, hooks = estimator_util.parse_input_fn_result(result). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/util.py"", line 60, in parse_input_fn_result. result = iterator.get_next(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/iterator_ops.py"", line 445, in get_next. flat_ret = gen_dataset_ops.iterator_get_next(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_dataset_ops.py"", line 3037, in iterator_get_next. _, _, _op, _outputs = _op_def_library._apply_op_helper(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py"", line 795, in _apply_op_helper. op = g._create_op_internal(op_type_name, inputs, dtypes=None,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 3798, in _create_op_internal. ret = Operation(. Best Regards! Bo.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:11025,modifiability,pac,packages,11025,"/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 621, in predict. features, input_hooks = self._get_features_from_input_fn(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1019, in _get_features_from_input_fn. result, _, hooks = estimator_util.parse_input_fn_result(result). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/util.py"", line 60, in parse_input_fn_result. result = iterator.get_next(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/iterator_ops.py"", line 445, in get_next. flat_ret = gen_dataset_ops.iterator_get_next(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_dataset_ops.py"", line 3037, in iterator_get_next. _, _, _op, _outputs = _op_def_library._apply_op_helper(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py"", line 795, in _apply_op_helper. op = g._create_op_internal(op_type_name, inputs, dtypes=None,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 3798, in _create_op_internal. ret = Operation(. Best Regards! Bo.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:11185,modifiability,pac,packages,11185,"/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 621, in predict. features, input_hooks = self._get_features_from_input_fn(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1019, in _get_features_from_input_fn. result, _, hooks = estimator_util.parse_input_fn_result(result). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/util.py"", line 60, in parse_input_fn_result. result = iterator.get_next(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/iterator_ops.py"", line 445, in get_next. flat_ret = gen_dataset_ops.iterator_get_next(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_dataset_ops.py"", line 3037, in iterator_get_next. _, _, _op, _outputs = _op_def_library._apply_op_helper(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py"", line 795, in _apply_op_helper. op = g._create_op_internal(op_type_name, inputs, dtypes=None,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 3798, in _create_op_internal. ret = Operation(. Best Regards! Bo.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:11363,modifiability,pac,packages,11363,"/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 621, in predict. features, input_hooks = self._get_features_from_input_fn(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1019, in _get_features_from_input_fn. result, _, hooks = estimator_util.parse_input_fn_result(result). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/util.py"", line 60, in parse_input_fn_result. result = iterator.get_next(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/iterator_ops.py"", line 445, in get_next. flat_ret = gen_dataset_ops.iterator_get_next(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_dataset_ops.py"", line 3037, in iterator_get_next. _, _, _op, _outputs = _op_def_library._apply_op_helper(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py"", line 795, in _apply_op_helper. op = g._create_op_internal(op_type_name, inputs, dtypes=None,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 3798, in _create_op_internal. ret = Operation(. Best Regards! Bo.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:11550,modifiability,pac,packages,11550,"/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 621, in predict. features, input_hooks = self._get_features_from_input_fn(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1019, in _get_features_from_input_fn. result, _, hooks = estimator_util.parse_input_fn_result(result). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/util.py"", line 60, in parse_input_fn_result. result = iterator.get_next(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/iterator_ops.py"", line 445, in get_next. flat_ret = gen_dataset_ops.iterator_get_next(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_dataset_ops.py"", line 3037, in iterator_get_next. _, _, _op, _outputs = _op_def_library._apply_op_helper(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py"", line 795, in _apply_op_helper. op = g._create_op_internal(op_type_name, inputs, dtypes=None,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 3798, in _create_op_internal. ret = Operation(. Best Regards! Bo.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:0,performance,Error,Error,0,"Error running deepvariant from singularity in cluster; Dear all,. I was trying to run deepvariant from singularity in cluster, but I always meet same error, I don't know hou to fix it:. /share/app/singularity/3.8.1/bin/singularity exec --containall --bind /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/tmp:/tmp --bind /usr/lib/locale/:/usr/lib/locale/ --bind $ccsbam:$ccsbam --bind $ccsbam.bai:$ccsbam.bai --bind $fasta:$fasta --bind $fasta.fai:$fasta.fai --bind /hwfssz5/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/translatome/HCT116/PacBio/analysis/deepvariant.v3/T202302180201:/output /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/software/deepvariant/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=$fasta --reads=$ccsbam --output_vcf=/output/T202302180201.deepvariant.vcf.gz --output_gvcf=/output/T202302180201.deepvariant.g.vcf.gz --num_shards=20 --intermediate_results_dir=/tmp. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/call_variants_output.tfrecord.gz"" --examples ""/tmp/make_examples.tfrecord@20.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"". 2023-07-15 14:06:58.063861: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0715 14:07:10.199614 47821886322496 call_variants.py:317] From /tmp/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:150,performance,error,error,150,"Error running deepvariant from singularity in cluster; Dear all,. I was trying to run deepvariant from singularity in cluster, but I always meet same error, I don't know hou to fix it:. /share/app/singularity/3.8.1/bin/singularity exec --containall --bind /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/tmp:/tmp --bind /usr/lib/locale/:/usr/lib/locale/ --bind $ccsbam:$ccsbam --bind $ccsbam.bai:$ccsbam.bai --bind $fasta:$fasta --bind $fasta.fai:$fasta.fai --bind /hwfssz5/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/translatome/HCT116/PacBio/analysis/deepvariant.v3/T202302180201:/output /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/software/deepvariant/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=$fasta --reads=$ccsbam --output_vcf=/output/T202302180201.deepvariant.vcf.gz --output_gvcf=/output/T202302180201.deepvariant.g.vcf.gz --num_shards=20 --intermediate_results_dir=/tmp. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/call_variants_output.tfrecord.gz"" --examples ""/tmp/make_examples.tfrecord@20.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"". 2023-07-15 14:06:58.063861: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0715 14:07:10.199614 47821886322496 call_variants.py:317] From /tmp/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:947,performance,time,time,947,"Error running deepvariant from singularity in cluster; Dear all,. I was trying to run deepvariant from singularity in cluster, but I always meet same error, I don't know hou to fix it:. /share/app/singularity/3.8.1/bin/singularity exec --containall --bind /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/tmp:/tmp --bind /usr/lib/locale/:/usr/lib/locale/ --bind $ccsbam:$ccsbam --bind $ccsbam.bai:$ccsbam.bai --bind $fasta:$fasta --bind $fasta.fai:$fasta.fai --bind /hwfssz5/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/translatome/HCT116/PacBio/analysis/deepvariant.v3/T202302180201:/output /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/software/deepvariant/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=$fasta --reads=$ccsbam --output_vcf=/output/T202302180201.deepvariant.vcf.gz --output_gvcf=/output/T202302180201.deepvariant.g.vcf.gz --num_shards=20 --intermediate_results_dir=/tmp. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/call_variants_output.tfrecord.gz"" --examples ""/tmp/make_examples.tfrecord@20.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"". 2023-07-15 14:06:58.063861: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0715 14:07:10.199614 47821886322496 call_variants.py:317] From /tmp/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:1237,performance,optimiz,optimized,1237,"ainall --bind /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/tmp:/tmp --bind /usr/lib/locale/:/usr/lib/locale/ --bind $ccsbam:$ccsbam --bind $ccsbam.bai:$ccsbam.bai --bind $fasta:$fasta --bind $fasta.fai:$fasta.fai --bind /hwfssz5/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/translatome/HCT116/PacBio/analysis/deepvariant.v3/T202302180201:/output /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/software/deepvariant/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=$fasta --reads=$ccsbam --output_vcf=/output/T202302180201.deepvariant.vcf.gz --output_gvcf=/output/T202302180201.deepvariant.g.vcf.gz --num_shards=20 --intermediate_results_dir=/tmp. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/call_variants_output.tfrecord.gz"" --examples ""/tmp/make_examples.tfrecord@20.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"". 2023-07-15 14:06:58.063861: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0715 14:07:10.199614 47821886322496 call_variants.py:317] From /tmp/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-07-15 14:07:10.22",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:1271,performance,Network,Network,1271,"LLS/P21Z10200N0125/tmp:/tmp --bind /usr/lib/locale/:/usr/lib/locale/ --bind $ccsbam:$ccsbam --bind $ccsbam.bai:$ccsbam.bai --bind $fasta:$fasta --bind $fasta.fai:$fasta.fai --bind /hwfssz5/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/translatome/HCT116/PacBio/analysis/deepvariant.v3/T202302180201:/output /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/software/deepvariant/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=$fasta --reads=$ccsbam --output_vcf=/output/T202302180201.deepvariant.vcf.gz --output_gvcf=/output/T202302180201.deepvariant.g.vcf.gz --num_shards=20 --intermediate_results_dir=/tmp. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/call_variants_output.tfrecord.gz"" --examples ""/tmp/make_examples.tfrecord@20.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"". 2023-07-15 14:06:58.063861: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0715 14:07:10.199614 47821886322496 call_variants.py:317] From /tmp/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-07-15 14:07:10.223654: I tensorflow/core/common_ru",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:1317,performance,CPU,CPU,1317,"locale/:/usr/lib/locale/ --bind $ccsbam:$ccsbam --bind $ccsbam.bai:$ccsbam.bai --bind $fasta:$fasta --bind $fasta.fai:$fasta.fai --bind /hwfssz5/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/translatome/HCT116/PacBio/analysis/deepvariant.v3/T202302180201:/output /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/software/deepvariant/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=$fasta --reads=$ccsbam --output_vcf=/output/T202302180201.deepvariant.vcf.gz --output_gvcf=/output/T202302180201.deepvariant.g.vcf.gz --num_shards=20 --intermediate_results_dir=/tmp. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/call_variants_output.tfrecord.gz"" --examples ""/tmp/make_examples.tfrecord@20.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"". 2023-07-15 14:06:58.063861: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0715 14:07:10.199614 47821886322496 call_variants.py:317] From /tmp/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-07-15 14:07:10.223654: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thre",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:1337,performance,perform,performance-critical,1337,"ind $ccsbam:$ccsbam --bind $ccsbam.bai:$ccsbam.bai --bind $fasta:$fasta --bind $fasta.fai:$fasta.fai --bind /hwfssz5/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/translatome/HCT116/PacBio/analysis/deepvariant.v3/T202302180201:/output /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/software/deepvariant/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=$fasta --reads=$ccsbam --output_vcf=/output/T202302180201.deepvariant.vcf.gz --output_gvcf=/output/T202302180201.deepvariant.g.vcf.gz --num_shards=20 --intermediate_results_dir=/tmp. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/call_variants_output.tfrecord.gz"" --examples ""/tmp/make_examples.tfrecord@20.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"". 2023-07-15 14:06:58.063861: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0715 14:07:10.199614 47821886322496 call_variants.py:317] From /tmp/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-07-15 14:07:10.223654: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter o",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:2001,performance,optimiz,optimized,2001,"l_variants_output.tfrecord.gz"" --examples ""/tmp/make_examples.tfrecord@20.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"". 2023-07-15 14:06:58.063861: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0715 14:07:10.199614 47821886322496 call_variants.py:317] From /tmp/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-07-15 14:07:10.223654: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. 2023-07-15 14:07:10.226851: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpl_zjylv7. W0715 14:07:10.308488 47821886322496 estimator.py:1864] Using temporary folder as model directory: /tmp/tmpl_zjylv7. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpl_zjylv7', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 10000. I0715 14:07:10.309",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:2035,performance,Network,Network,2035,"examples ""/tmp/make_examples.tfrecord@20.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"". 2023-07-15 14:06:58.063861: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0715 14:07:10.199614 47821886322496 call_variants.py:317] From /tmp/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-07-15 14:07:10.223654: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. 2023-07-15 14:07:10.226851: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpl_zjylv7. W0715 14:07:10.308488 47821886322496 estimator.py:1864] Using temporary folder as model directory: /tmp/tmpl_zjylv7. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpl_zjylv7', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 10000. I0715 14:07:10.309390 47821886322496 estimator.py:2",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:2081,performance,CPU,CPU,2081," --checkpoint ""/opt/models/pacbio/model.ckpt"". 2023-07-15 14:06:58.063861: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0715 14:07:10.199614 47821886322496 call_variants.py:317] From /tmp/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-07-15 14:07:10.223654: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. 2023-07-15 14:07:10.226851: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpl_zjylv7. W0715 14:07:10.308488 47821886322496 estimator.py:1864] Using temporary folder as model directory: /tmp/tmpl_zjylv7. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpl_zjylv7', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 10000. I0715 14:07:10.309390 47821886322496 estimator.py:202] Using config: {'_model_dir': '/tmp/tmpl_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:2101,performance,perform,performance-critical,2101,"acbio/model.ckpt"". 2023-07-15 14:06:58.063861: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0715 14:07:10.199614 47821886322496 call_variants.py:317] From /tmp/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-07-15 14:07:10.223654: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. 2023-07-15 14:07:10.226851: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpl_zjylv7. W0715 14:07:10.308488 47821886322496 estimator.py:1864] Using temporary folder as model directory: /tmp/tmpl_zjylv7. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpl_zjylv7', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 10000. I0715 14:07:10.309390 47821886322496 estimator.py:202] Using config: {'_model_dir': '/tmp/tmpl_zjylv7', '_tf_random_seed': ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:2361,performance,Tune,Tune,2361," To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0715 14:07:10.199614 47821886322496 call_variants.py:317] From /tmp/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-07-15 14:07:10.223654: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. 2023-07-15 14:07:10.226851: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpl_zjylv7. W0715 14:07:10.308488 47821886322496 estimator.py:1864] Using temporary folder as model directory: /tmp/tmpl_zjylv7. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpl_zjylv7', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 10000. I0715 14:07:10.309390 47821886322496 estimator.py:202] Using config: {'_model_dir': '/tmp/tmpl_zjylv7', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_c. I0715 14:07:10.309847 47821886322496 call_variants.py:446] Writing calls to /tmp/call_variants_output.tfrecord.gz. INFO:tensorflow:Calling mo",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:2410,performance,perform,performance,2410,"ow with the appropriate compiler flags. I0715 14:07:10.199614 47821886322496 call_variants.py:317] From /tmp/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-07-15 14:07:10.223654: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. 2023-07-15 14:07:10.226851: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpl_zjylv7. W0715 14:07:10.308488 47821886322496 estimator.py:1864] Using temporary folder as model directory: /tmp/tmpl_zjylv7. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpl_zjylv7', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 10000. I0715 14:07:10.309390 47821886322496 estimator.py:202] Using config: {'_model_dir': '/tmp/tmpl_zjylv7', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_c. I0715 14:07:10.309847 47821886322496 call_variants.py:446] Writing calls to /tmp/call_variants_output.tfrecord.gz. INFO:tensorflow:Calling model_fn. I0715 14:07:10.758818 47821886322496 estimato",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:2523,performance,optimiz,optimization,2523,"_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-07-15 14:07:10.223654: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. 2023-07-15 14:07:10.226851: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpl_zjylv7. W0715 14:07:10.308488 47821886322496 estimator.py:1864] Using temporary folder as model directory: /tmp/tmpl_zjylv7. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpl_zjylv7', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 10000. I0715 14:07:10.309390 47821886322496 estimator.py:202] Using config: {'_model_dir': '/tmp/tmpl_zjylv7', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_c. I0715 14:07:10.309847 47821886322496 call_variants.py:446] Writing calls to /tmp/call_variants_output.tfrecord.gz. INFO:tensorflow:Calling model_fn. I0715 14:07:10.758818 47821886322496 estimator.py:1173] Calling model_fn. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:3973,performance,Tune,Tune,3973,"max': 10000. I0715 14:07:10.309390 47821886322496 estimator.py:202] Using config: {'_model_dir': '/tmp/tmpl_zjylv7', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_c. I0715 14:07:10.309847 47821886322496 call_variants.py:446] Writing calls to /tmp/call_variants_output.tfrecord.gz. INFO:tensorflow:Calling model_fn. I0715 14:07:10.758818 47821886322496 estimator.py:1173] Calling model_fn. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0715 14:07:16.873909 47821886322496 estimator.py:1175] Done calling model_fn. 2023-07-15 14:07:17.753203: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. INFO:tensorflow:Graph was finalized. I0715 14:07:18.197918 47821886322496 monitored_session.py:240] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:18.198770 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. INFO:tensorflow:Running local_init_op. I0715 14:07:20.793029 47821886322496 session_manager.py:526] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0715 14:07:20.857252 47821886322496 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0715 14:07:21.417166 47821886322496 modeling.py:418] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:21.417759 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:25.965068 47821886322496 call_variants.py:462] Processed 1 examples in 1 batches [1",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:4022,performance,perform,performance,4022,"imator.py:202] Using config: {'_model_dir': '/tmp/tmpl_zjylv7', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_c. I0715 14:07:10.309847 47821886322496 call_variants.py:446] Writing calls to /tmp/call_variants_output.tfrecord.gz. INFO:tensorflow:Calling model_fn. I0715 14:07:10.758818 47821886322496 estimator.py:1173] Calling model_fn. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0715 14:07:16.873909 47821886322496 estimator.py:1175] Done calling model_fn. 2023-07-15 14:07:17.753203: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. INFO:tensorflow:Graph was finalized. I0715 14:07:18.197918 47821886322496 monitored_session.py:240] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:18.198770 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. INFO:tensorflow:Running local_init_op. I0715 14:07:20.793029 47821886322496 session_manager.py:526] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0715 14:07:20.857252 47821886322496 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0715 14:07:21.417166 47821886322496 modeling.py:418] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:21.417759 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:25.965068 47821886322496 call_variants.py:462] Processed 1 examples in 1 batches [1565.044 sec per 100]. Traceback (most recent call las",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:4965,performance,batch,batches,4965," 2. Tune using inter_op_parallelism_threads for best performance. INFO:tensorflow:Graph was finalized. I0715 14:07:18.197918 47821886322496 monitored_session.py:240] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:18.198770 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. INFO:tensorflow:Running local_init_op. I0715 14:07:20.793029 47821886322496 session_manager.py:526] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0715 14:07:20.857252 47821886322496 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0715 14:07:21.417166 47821886322496 modeling.py:418] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:21.417759 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:25.965068 47821886322496 call_variants.py:462] Processed 1 examples in 1 batches [1565.044 sec per 100]. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1378, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1361, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1454, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.DataLossError: corrupted record at 484865306. [[{{node IteratorGetNext}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"",",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:8214,performance,error,error,8214,"python/training/monitored_session.py"", line 1464, in run. outputs = _WrappedSession.run(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1228, in run. return self._sess.run(*args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 968, in run. result = self._run(None, fetches, feed_dict, options_ptr,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1191, in _run. results = self._do_run(handle, final_targets, final_fetches,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1371, in _do_run. return self._do_call(_run_fn, feeds, fetches, targets, options,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1397, in _do_call. raise type(e)(node_def, op, message) # pylint: disable=no-value-for-parameter. tensorflow.python.framework.errors_impl.DataLossError: Graph execution error:. Detected at node 'IteratorGetNext' defined at (most recent call last):. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 621, in predict. features, input_hooks = self._get_features_from_input_fn(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estima",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:1086,reliability,checkpoint,checkpoint,1086,"ariant from singularity in cluster, but I always meet same error, I don't know hou to fix it:. /share/app/singularity/3.8.1/bin/singularity exec --containall --bind /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/tmp:/tmp --bind /usr/lib/locale/:/usr/lib/locale/ --bind $ccsbam:$ccsbam --bind $ccsbam.bai:$ccsbam.bai --bind $fasta:$fasta --bind $fasta.fai:$fasta.fai --bind /hwfssz5/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/translatome/HCT116/PacBio/analysis/deepvariant.v3/T202302180201:/output /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/software/deepvariant/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=$fasta --reads=$ccsbam --output_vcf=/output/T202302180201.deepvariant.vcf.gz --output_gvcf=/output/T202302180201.deepvariant.g.vcf.gz --num_shards=20 --intermediate_results_dir=/tmp. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/call_variants_output.tfrecord.gz"" --examples ""/tmp/make_examples.tfrecord@20.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"". 2023-07-15 14:06:58.063861: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0715 14:07:10.199614 47821886322496 call_variants.py:317] From /tmp/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instru",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:4172,reliability,Restor,Restoring,4172," '_save_checkpoints_secs': 600, '_session_c. I0715 14:07:10.309847 47821886322496 call_variants.py:446] Writing calls to /tmp/call_variants_output.tfrecord.gz. INFO:tensorflow:Calling model_fn. I0715 14:07:10.758818 47821886322496 estimator.py:1173] Calling model_fn. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0715 14:07:16.873909 47821886322496 estimator.py:1175] Done calling model_fn. 2023-07-15 14:07:17.753203: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. INFO:tensorflow:Graph was finalized. I0715 14:07:18.197918 47821886322496 monitored_session.py:240] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:18.198770 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. INFO:tensorflow:Running local_init_op. I0715 14:07:20.793029 47821886322496 session_manager.py:526] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0715 14:07:20.857252 47821886322496 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0715 14:07:21.417166 47821886322496 modeling.py:418] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:21.417759 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:25.965068 47821886322496 call_variants.py:462] Processed 1 examples in 1 batches [1565.044 sec per 100]. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1378, in _do_call. return fn(*args). File ""/usr/local/li",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:4281,reliability,Restor,Restoring,4281,"ng calls to /tmp/call_variants_output.tfrecord.gz. INFO:tensorflow:Calling model_fn. I0715 14:07:10.758818 47821886322496 estimator.py:1173] Calling model_fn. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0715 14:07:16.873909 47821886322496 estimator.py:1175] Done calling model_fn. 2023-07-15 14:07:17.753203: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. INFO:tensorflow:Graph was finalized. I0715 14:07:18.197918 47821886322496 monitored_session.py:240] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:18.198770 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. INFO:tensorflow:Running local_init_op. I0715 14:07:20.793029 47821886322496 session_manager.py:526] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0715 14:07:20.857252 47821886322496 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0715 14:07:21.417166 47821886322496 modeling.py:418] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:21.417759 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:25.965068 47821886322496 call_variants.py:462] Processed 1 examples in 1 batches [1565.044 sec per 100]. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1378, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1361, in _run_fn. return self._call_tf_s",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:4714,reliability,Restor,Restoring,4714,"tensorflow:Done calling model_fn. I0715 14:07:16.873909 47821886322496 estimator.py:1175] Done calling model_fn. 2023-07-15 14:07:17.753203: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. INFO:tensorflow:Graph was finalized. I0715 14:07:18.197918 47821886322496 monitored_session.py:240] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:18.198770 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. INFO:tensorflow:Running local_init_op. I0715 14:07:20.793029 47821886322496 session_manager.py:526] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0715 14:07:20.857252 47821886322496 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0715 14:07:21.417166 47821886322496 modeling.py:418] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:21.417759 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:25.965068 47821886322496 call_variants.py:462] Processed 1 examples in 1 batches [1565.044 sec per 100]. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1378, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1361, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1454, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.DataLossError: corrupted record at 484865306. [[{{node IteratorGetNext}}]]. During handling of the above exception, another exception occurred:. Traceback (",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:4823,reliability,Restor,Restoring,4823,"fn. 2023-07-15 14:07:17.753203: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. INFO:tensorflow:Graph was finalized. I0715 14:07:18.197918 47821886322496 monitored_session.py:240] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:18.198770 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. INFO:tensorflow:Running local_init_op. I0715 14:07:20.793029 47821886322496 session_manager.py:526] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0715 14:07:20.857252 47821886322496 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0715 14:07:21.417166 47821886322496 modeling.py:418] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:21.417759 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:25.965068 47821886322496 call_variants.py:462] Processed 1 examples in 1 batches [1565.044 sec per 100]. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1378, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1361, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1454, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.DataLossError: corrupted record at 484865306. [[{{node IteratorGetNext}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:0,safety,Error,Error,0,"Error running deepvariant from singularity in cluster; Dear all,. I was trying to run deepvariant from singularity in cluster, but I always meet same error, I don't know hou to fix it:. /share/app/singularity/3.8.1/bin/singularity exec --containall --bind /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/tmp:/tmp --bind /usr/lib/locale/:/usr/lib/locale/ --bind $ccsbam:$ccsbam --bind $ccsbam.bai:$ccsbam.bai --bind $fasta:$fasta --bind $fasta.fai:$fasta.fai --bind /hwfssz5/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/translatome/HCT116/PacBio/analysis/deepvariant.v3/T202302180201:/output /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/software/deepvariant/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=$fasta --reads=$ccsbam --output_vcf=/output/T202302180201.deepvariant.vcf.gz --output_gvcf=/output/T202302180201.deepvariant.g.vcf.gz --num_shards=20 --intermediate_results_dir=/tmp. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/call_variants_output.tfrecord.gz"" --examples ""/tmp/make_examples.tfrecord@20.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"". 2023-07-15 14:06:58.063861: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0715 14:07:10.199614 47821886322496 call_variants.py:317] From /tmp/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:150,safety,error,error,150,"Error running deepvariant from singularity in cluster; Dear all,. I was trying to run deepvariant from singularity in cluster, but I always meet same error, I don't know hou to fix it:. /share/app/singularity/3.8.1/bin/singularity exec --containall --bind /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/tmp:/tmp --bind /usr/lib/locale/:/usr/lib/locale/ --bind $ccsbam:$ccsbam --bind $ccsbam.bai:$ccsbam.bai --bind $fasta:$fasta --bind $fasta.fai:$fasta.fai --bind /hwfssz5/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/translatome/HCT116/PacBio/analysis/deepvariant.v3/T202302180201:/output /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/software/deepvariant/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=$fasta --reads=$ccsbam --output_vcf=/output/T202302180201.deepvariant.vcf.gz --output_gvcf=/output/T202302180201.deepvariant.g.vcf.gz --num_shards=20 --intermediate_results_dir=/tmp. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/call_variants_output.tfrecord.gz"" --examples ""/tmp/make_examples.tfrecord@20.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"". 2023-07-15 14:06:58.063861: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0715 14:07:10.199614 47821886322496 call_variants.py:317] From /tmp/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:1594,safety,input,input,1594,"S/P21Z10200N0125/zhanghaibo/software/deepvariant/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=$fasta --reads=$ccsbam --output_vcf=/output/T202302180201.deepvariant.vcf.gz --output_gvcf=/output/T202302180201.deepvariant.g.vcf.gz --num_shards=20 --intermediate_results_dir=/tmp. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/call_variants_output.tfrecord.gz"" --examples ""/tmp/make_examples.tfrecord@20.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"". 2023-07-15 14:06:58.063861: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0715 14:07:10.199614 47821886322496 call_variants.py:317] From /tmp/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-07-15 14:07:10.223654: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. 2023-07-15 14:07:10.226851: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled. WARNING:tensorflow:Using temporary folde",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:1637,safety,input,input,1637,"riant/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=$fasta --reads=$ccsbam --output_vcf=/output/T202302180201.deepvariant.vcf.gz --output_gvcf=/output/T202302180201.deepvariant.g.vcf.gz --num_shards=20 --intermediate_results_dir=/tmp. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/call_variants_output.tfrecord.gz"" --examples ""/tmp/make_examples.tfrecord@20.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"". 2023-07-15 14:06:58.063861: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0715 14:07:10.199614 47821886322496 call_variants.py:317] From /tmp/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-07-15 14:07:10.223654: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. 2023-07-15 14:07:10.226851: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpl_zjylv7. W07",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:1805,safety,input,input,1805,"--output_gvcf=/output/T202302180201.deepvariant.g.vcf.gz --num_shards=20 --intermediate_results_dir=/tmp. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/call_variants_output.tfrecord.gz"" --examples ""/tmp/make_examples.tfrecord@20.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"". 2023-07-15 14:06:58.063861: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0715 14:07:10.199614 47821886322496 call_variants.py:317] From /tmp/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-07-15 14:07:10.223654: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. 2023-07-15 14:07:10.226851: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpl_zjylv7. W0715 14:07:10.308488 47821886322496 estimator.py:1864] Using temporary folder as model directory: /tmp/tmpl_zjylv7. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmp",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:1848,safety,input,input,1848,"iant.g.vcf.gz --num_shards=20 --intermediate_results_dir=/tmp. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/call_variants_output.tfrecord.gz"" --examples ""/tmp/make_examples.tfrecord@20.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"". 2023-07-15 14:06:58.063861: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0715 14:07:10.199614 47821886322496 call_variants.py:317] From /tmp/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-07-15 14:07:10.223654: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. 2023-07-15 14:07:10.226851: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpl_zjylv7. W0715 14:07:10.308488 47821886322496 estimator.py:1864] Using temporary folder as model directory: /tmp/tmpl_zjylv7. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpl_zjylv7', '_tf_random_seed': None, '_save_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:5668,safety,except,exception,5668,"py:418] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:21.417759 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:25.965068 47821886322496 call_variants.py:462] Processed 1 examples in 1 batches [1565.044 sec per 100]. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1378, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1361, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1454, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.DataLossError: corrupted record at 484865306. [[{{node IteratorGetNext}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 642",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:5687,safety,except,exception,5687,"MA... INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:21.417759 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:25.965068 47821886322496 call_variants.py:462] Processed 1 examples in 1 batches [1565.044 sec per 100]. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1378, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1361, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1454, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.DataLossError: corrupted record at 484865306. [[{{node IteratorGetNext}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 642, in predict. preds",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:5856,safety,modul,module,5856,"s/pacbio/model.ckpt. I0715 14:07:25.965068 47821886322496 call_variants.py:462] Processed 1 examples in 1 batches [1565.044 sec per 100]. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1378, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1361, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1454, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.DataLossError: corrupted record at 484865306. [[{{node IteratorGetNext}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 642, in predict. preds_evaluated = mon_sess.run(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 778, in run. return self._s",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:6535,safety,predict,prediction,6535,".framework.errors_impl.DataLossError: corrupted record at 484865306. [[{{node IteratorGetNext}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 642, in predict. preds_evaluated = mon_sess.run(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 778, in run. return self._sess.run(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1307, in run. return self._sess.run(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1397, in run. return self._sess.run(*args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1464, in run. outputs = _WrappedSession.run(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1228, in run. return self._sess.run(*args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.p",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:6553,safety,predict,predictions,6553,"mpl.DataLossError: corrupted record at 484865306. [[{{node IteratorGetNext}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 642, in predict. preds_evaluated = mon_sess.run(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 778, in run. return self._sess.run(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1307, in run. return self._sess.run(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1397, in run. return self._sess.run(*args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1464, in run. outputs = _WrappedSession.run(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1228, in run. return self._sess.run(*args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 968, in ru",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:6678,safety,predict,predict,6678,"ther exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 642, in predict. preds_evaluated = mon_sess.run(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 778, in run. return self._sess.run(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1307, in run. return self._sess.run(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1397, in run. return self._sess.run(*args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1464, in run. outputs = _WrappedSession.run(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1228, in run. return self._sess.run(*args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 968, in run. result = self._run(None, fetches, feed_dict, options_ptr,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/pytho",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:6718,safety,predict,predictions,6718,"recent call last):. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 642, in predict. preds_evaluated = mon_sess.run(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 778, in run. return self._sess.run(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1307, in run. return self._sess.run(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1397, in run. return self._sess.run(*args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1464, in run. outputs = _WrappedSession.run(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1228, in run. return self._sess.run(*args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 968, in run. result = self._run(None, fetches, feed_dict, options_ptr,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1191, in _run. ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:8214,safety,error,error,8214,"python/training/monitored_session.py"", line 1464, in run. outputs = _WrappedSession.run(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1228, in run. return self._sess.run(*args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 968, in run. result = self._run(None, fetches, feed_dict, options_ptr,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1191, in _run. results = self._do_run(handle, final_targets, final_fetches,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1371, in _do_run. return self._do_call(_run_fn, feeds, fetches, targets, options,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1397, in _do_call. raise type(e)(node_def, op, message) # pylint: disable=no-value-for-parameter. tensorflow.python.framework.errors_impl.DataLossError: Graph execution error:. Detected at node 'IteratorGetNext' defined at (most recent call last):. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 621, in predict. features, input_hooks = self._get_features_from_input_fn(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estima",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:8222,safety,Detect,Detected,8222,"aining/monitored_session.py"", line 1464, in run. outputs = _WrappedSession.run(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1228, in run. return self._sess.run(*args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 968, in run. result = self._run(None, fetches, feed_dict, options_ptr,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1191, in _run. results = self._do_run(handle, final_targets, final_fetches,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1371, in _do_run. return self._do_call(_run_fn, feeds, fetches, targets, options,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1397, in _do_call. raise type(e)(node_def, op, message) # pylint: disable=no-value-for-parameter. tensorflow.python.framework.errors_impl.DataLossError: Graph execution error:. Detected at node 'IteratorGetNext' defined at (most recent call last):. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 621, in predict. features, input_hooks = self._get_features_from_input_fn(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:8406,safety,modul,module,8406,"28, in run. return self._sess.run(*args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 968, in run. result = self._run(None, fetches, feed_dict, options_ptr,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1191, in _run. results = self._do_run(handle, final_targets, final_fetches,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1371, in _do_run. return self._do_call(_run_fn, feeds, fetches, targets, options,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1397, in _do_call. raise type(e)(node_def, op, message) # pylint: disable=no-value-for-parameter. tensorflow.python.framework.errors_impl.DataLossError: Graph execution error:. Detected at node 'IteratorGetNext' defined at (most recent call last):. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 621, in predict. features, input_hooks = self._get_features_from_input_fn(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1019, in _get_features_from_input_fn. result, _, hooks = estimator_util.parse_input_fn_result(result). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/es",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:8917,safety,predict,prediction,8917,"feeds, fetches, targets, options,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1397, in _do_call. raise type(e)(node_def, op, message) # pylint: disable=no-value-for-parameter. tensorflow.python.framework.errors_impl.DataLossError: Graph execution error:. Detected at node 'IteratorGetNext' defined at (most recent call last):. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 621, in predict. features, input_hooks = self._get_features_from_input_fn(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1019, in _get_features_from_input_fn. result, _, hooks = estimator_util.parse_input_fn_result(result). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/util.py"", line 60, in parse_input_fn_result. result = iterator.get_next(). Node: 'IteratorGetNext'. corrupted record at 484865306. [[{{node IteratorGetNext}}]]. Original stack trace for 'IteratorGetNext':. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tol",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:8935,safety,predict,predictions,8935,"gets, options,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1397, in _do_call. raise type(e)(node_def, op, message) # pylint: disable=no-value-for-parameter. tensorflow.python.framework.errors_impl.DataLossError: Graph execution error:. Detected at node 'IteratorGetNext' defined at (most recent call last):. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 621, in predict. features, input_hooks = self._get_features_from_input_fn(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1019, in _get_features_from_input_fn. result, _, hooks = estimator_util.parse_input_fn_result(result). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/util.py"", line 60, in parse_input_fn_result. result = iterator.get_next(). Node: 'IteratorGetNext'. corrupted record at 484865306. [[{{node IteratorGetNext}}]]. Original stack trace for 'IteratorGetNext':. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:9060,safety,predict,predict,9060,"raise type(e)(node_def, op, message) # pylint: disable=no-value-for-parameter. tensorflow.python.framework.errors_impl.DataLossError: Graph execution error:. Detected at node 'IteratorGetNext' defined at (most recent call last):. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 621, in predict. features, input_hooks = self._get_features_from_input_fn(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1019, in _get_features_from_input_fn. result, _, hooks = estimator_util.parse_input_fn_result(result). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/util.py"", line 60, in parse_input_fn_result. result = iterator.get_next(). Node: 'IteratorGetNext'. corrupted record at 484865306. [[{{node IteratorGetNext}}]]. Original stack trace for 'IteratorGetNext':. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runf",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:9735,safety,modul,module,9735,"all_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 621, in predict. features, input_hooks = self._get_features_from_input_fn(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1019, in _get_features_from_input_fn. result, _, hooks = estimator_util.parse_input_fn_result(result). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/util.py"", line 60, in parse_input_fn_result. result = iterator.get_next(). Node: 'IteratorGetNext'. corrupted record at 484865306. [[{{node IteratorGetNext}}]]. Original stack trace for 'IteratorGetNext':. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 621, in predict. features, input_hooks = self._get_features_from_input_fn(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1019, in _",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:10414,safety,predict,prediction,10414,"il.py"", line 60, in parse_input_fn_result. result = iterator.get_next(). Node: 'IteratorGetNext'. corrupted record at 484865306. [[{{node IteratorGetNext}}]]. Original stack trace for 'IteratorGetNext':. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 621, in predict. features, input_hooks = self._get_features_from_input_fn(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1019, in _get_features_from_input_fn. result, _, hooks = estimator_util.parse_input_fn_result(result). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/util.py"", line 60, in parse_input_fn_result. result = iterator.get_next(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/iterator_ops.py"", line 445, in get_next. flat_ret = gen_dataset_ops.iterator_get_next(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_dataset_ops.py"", line 3037, in iterator_get_next. _, _, _op, _outputs = _op_def_library._apply_op_helper(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py"",",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:10432,safety,predict,predictions,10432," parse_input_fn_result. result = iterator.get_next(). Node: 'IteratorGetNext'. corrupted record at 484865306. [[{{node IteratorGetNext}}]]. Original stack trace for 'IteratorGetNext':. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 621, in predict. features, input_hooks = self._get_features_from_input_fn(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1019, in _get_features_from_input_fn. result, _, hooks = estimator_util.parse_input_fn_result(result). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/util.py"", line 60, in parse_input_fn_result. result = iterator.get_next(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/iterator_ops.py"", line 445, in get_next. flat_ret = gen_dataset_ops.iterator_get_next(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_dataset_ops.py"", line 3037, in iterator_get_next. _, _, _op, _outputs = _op_def_library._apply_op_helper(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py"", line 795, in _appl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:10557,safety,predict,predict,10557,"atorGetNext}}]]. Original stack trace for 'IteratorGetNext':. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 621, in predict. features, input_hooks = self._get_features_from_input_fn(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1019, in _get_features_from_input_fn. result, _, hooks = estimator_util.parse_input_fn_result(result). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/util.py"", line 60, in parse_input_fn_result. result = iterator.get_next(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/iterator_ops.py"", line 445, in get_next. flat_ret = gen_dataset_ops.iterator_get_next(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_dataset_ops.py"", line 3037, in iterator_get_next. _, _, _op, _outputs = _op_def_library._apply_op_helper(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py"", line 795, in _apply_op_helper. op = g._create_op_internal(op_type_name, inputs, dtypes=None,. File ""/usr/local/lib/python3.8/dist-packages/te",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:11492,safety,input,inputs,11492,"/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 621, in predict. features, input_hooks = self._get_features_from_input_fn(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1019, in _get_features_from_input_fn. result, _, hooks = estimator_util.parse_input_fn_result(result). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/util.py"", line 60, in parse_input_fn_result. result = iterator.get_next(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/iterator_ops.py"", line 445, in get_next. flat_ret = gen_dataset_ops.iterator_get_next(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_dataset_ops.py"", line 3037, in iterator_get_next. _, _, _op, _outputs = _op_def_library._apply_op_helper(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py"", line 795, in _apply_op_helper. op = g._create_op_internal(op_type_name, inputs, dtypes=None,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 3798, in _create_op_internal. ret = Operation(. Best Regards! Bo.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:1103,security,model,models,1103,"gularity in cluster, but I always meet same error, I don't know hou to fix it:. /share/app/singularity/3.8.1/bin/singularity exec --containall --bind /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/tmp:/tmp --bind /usr/lib/locale/:/usr/lib/locale/ --bind $ccsbam:$ccsbam --bind $ccsbam.bai:$ccsbam.bai --bind $fasta:$fasta --bind $fasta.fai:$fasta.fai --bind /hwfssz5/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/translatome/HCT116/PacBio/analysis/deepvariant.v3/T202302180201:/output /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/software/deepvariant/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=$fasta --reads=$ccsbam --output_vcf=/output/T202302180201.deepvariant.vcf.gz --output_gvcf=/output/T202302180201.deepvariant.g.vcf.gz --num_shards=20 --intermediate_results_dir=/tmp. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/call_variants_output.tfrecord.gz"" --examples ""/tmp/make_examples.tfrecord@20.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"". 2023-07-15 14:06:58.063861: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0715 14:07:10.199614 47821886322496 call_variants.py:317] From /tmp/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in perfo",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:1117,security,model,model,1117,"uster, but I always meet same error, I don't know hou to fix it:. /share/app/singularity/3.8.1/bin/singularity exec --containall --bind /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/tmp:/tmp --bind /usr/lib/locale/:/usr/lib/locale/ --bind $ccsbam:$ccsbam --bind $ccsbam.bai:$ccsbam.bai --bind $fasta:$fasta --bind $fasta.fai:$fasta.fai --bind /hwfssz5/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/translatome/HCT116/PacBio/analysis/deepvariant.v3/T202302180201:/output /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/software/deepvariant/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=$fasta --reads=$ccsbam --output_vcf=/output/T202302180201.deepvariant.vcf.gz --output_gvcf=/output/T202302180201.deepvariant.g.vcf.gz --num_shards=20 --intermediate_results_dir=/tmp. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/call_variants_output.tfrecord.gz"" --examples ""/tmp/make_examples.tfrecord@20.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"". 2023-07-15 14:06:58.063861: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0715 14:07:10.199614 47821886322496 call_variants.py:317] From /tmp/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critica",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:1271,security,Network,Network,1271,"LLS/P21Z10200N0125/tmp:/tmp --bind /usr/lib/locale/:/usr/lib/locale/ --bind $ccsbam:$ccsbam --bind $ccsbam.bai:$ccsbam.bai --bind $fasta:$fasta --bind $fasta.fai:$fasta.fai --bind /hwfssz5/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/translatome/HCT116/PacBio/analysis/deepvariant.v3/T202302180201:/output /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/software/deepvariant/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=$fasta --reads=$ccsbam --output_vcf=/output/T202302180201.deepvariant.vcf.gz --output_gvcf=/output/T202302180201.deepvariant.g.vcf.gz --num_shards=20 --intermediate_results_dir=/tmp. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/call_variants_output.tfrecord.gz"" --examples ""/tmp/make_examples.tfrecord@20.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"". 2023-07-15 14:06:58.063861: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0715 14:07:10.199614 47821886322496 call_variants.py:317] From /tmp/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-07-15 14:07:10.223654: I tensorflow/core/common_ru",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:1752,security,model,models,1752,"-output_vcf=/output/T202302180201.deepvariant.vcf.gz --output_gvcf=/output/T202302180201.deepvariant.g.vcf.gz --num_shards=20 --intermediate_results_dir=/tmp. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/call_variants_output.tfrecord.gz"" --examples ""/tmp/make_examples.tfrecord@20.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"". 2023-07-15 14:06:58.063861: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0715 14:07:10.199614 47821886322496 call_variants.py:317] From /tmp/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-07-15 14:07:10.223654: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. 2023-07-15 14:07:10.226851: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpl_zjylv7. W0715 14:07:10.308488 47821886322496 estimator.py:1864] Using temporary folder as model directory: /tmp/tmpl_zjylv7. I",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:1766,security,model,model,1766,"utput/T202302180201.deepvariant.vcf.gz --output_gvcf=/output/T202302180201.deepvariant.g.vcf.gz --num_shards=20 --intermediate_results_dir=/tmp. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/call_variants_output.tfrecord.gz"" --examples ""/tmp/make_examples.tfrecord@20.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"". 2023-07-15 14:06:58.063861: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0715 14:07:10.199614 47821886322496 call_variants.py:317] From /tmp/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-07-15 14:07:10.223654: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. 2023-07-15 14:07:10.226851: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpl_zjylv7. W0715 14:07:10.308488 47821886322496 estimator.py:1864] Using temporary folder as model directory: /tmp/tmpl_zjylv7. INFO:tensorflow",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:2035,security,Network,Network,2035,"examples ""/tmp/make_examples.tfrecord@20.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"". 2023-07-15 14:06:58.063861: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0715 14:07:10.199614 47821886322496 call_variants.py:317] From /tmp/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-07-15 14:07:10.223654: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. 2023-07-15 14:07:10.226851: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpl_zjylv7. W0715 14:07:10.308488 47821886322496 estimator.py:1864] Using temporary folder as model directory: /tmp/tmpl_zjylv7. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpl_zjylv7', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 10000. I0715 14:07:10.309390 47821886322496 estimator.py:2",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:2602,security,model,model,2602,"les: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-07-15 14:07:10.223654: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. 2023-07-15 14:07:10.226851: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpl_zjylv7. W0715 14:07:10.308488 47821886322496 estimator.py:1864] Using temporary folder as model directory: /tmp/tmpl_zjylv7. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpl_zjylv7', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 10000. I0715 14:07:10.309390 47821886322496 estimator.py:202] Using config: {'_model_dir': '/tmp/tmpl_zjylv7', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_c. I0715 14:07:10.309847 47821886322496 call_variants.py:446] Writing calls to /tmp/call_variants_output.tfrecord.gz. INFO:tensorflow:Calling model_fn. I0715 14:07:10.758818 47821886322496 estimator.py:1173] Calling model_fn. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a f",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:2719,security,model,model,2719,"ll_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-07-15 14:07:10.223654: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. 2023-07-15 14:07:10.226851: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpl_zjylv7. W0715 14:07:10.308488 47821886322496 estimator.py:1864] Using temporary folder as model directory: /tmp/tmpl_zjylv7. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpl_zjylv7', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 10000. I0715 14:07:10.309390 47821886322496 estimator.py:202] Using config: {'_model_dir': '/tmp/tmpl_zjylv7', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_c. I0715 14:07:10.309847 47821886322496 call_variants.py:446] Writing calls to /tmp/call_variants_output.tfrecord.gz. INFO:tensorflow:Calling model_fn. I0715 14:07:10.758818 47821886322496 estimator.py:1173] Calling model_fn. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:ten",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:4203,security,model,models,4203,"0, '_session_c. I0715 14:07:10.309847 47821886322496 call_variants.py:446] Writing calls to /tmp/call_variants_output.tfrecord.gz. INFO:tensorflow:Calling model_fn. I0715 14:07:10.758818 47821886322496 estimator.py:1173] Calling model_fn. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0715 14:07:16.873909 47821886322496 estimator.py:1175] Done calling model_fn. 2023-07-15 14:07:17.753203: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. INFO:tensorflow:Graph was finalized. I0715 14:07:18.197918 47821886322496 monitored_session.py:240] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:18.198770 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. INFO:tensorflow:Running local_init_op. I0715 14:07:20.793029 47821886322496 session_manager.py:526] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0715 14:07:20.857252 47821886322496 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0715 14:07:21.417166 47821886322496 modeling.py:418] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:21.417759 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:25.965068 47821886322496 call_variants.py:462] Processed 1 examples in 1 batches [1565.044 sec per 100]. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1378, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/ten",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:4217,security,model,model,4217,". I0715 14:07:10.309847 47821886322496 call_variants.py:446] Writing calls to /tmp/call_variants_output.tfrecord.gz. INFO:tensorflow:Calling model_fn. I0715 14:07:10.758818 47821886322496 estimator.py:1173] Calling model_fn. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0715 14:07:16.873909 47821886322496 estimator.py:1175] Done calling model_fn. 2023-07-15 14:07:17.753203: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. INFO:tensorflow:Graph was finalized. I0715 14:07:18.197918 47821886322496 monitored_session.py:240] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:18.198770 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. INFO:tensorflow:Running local_init_op. I0715 14:07:20.793029 47821886322496 session_manager.py:526] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0715 14:07:20.857252 47821886322496 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0715 14:07:21.417166 47821886322496 modeling.py:418] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:21.417759 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:25.965068 47821886322496 call_variants.py:462] Processed 1 examples in 1 batches [1565.044 sec per 100]. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1378, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:4312,security,model,models,4312,"s_output.tfrecord.gz. INFO:tensorflow:Calling model_fn. I0715 14:07:10.758818 47821886322496 estimator.py:1173] Calling model_fn. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0715 14:07:16.873909 47821886322496 estimator.py:1175] Done calling model_fn. 2023-07-15 14:07:17.753203: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. INFO:tensorflow:Graph was finalized. I0715 14:07:18.197918 47821886322496 monitored_session.py:240] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:18.198770 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. INFO:tensorflow:Running local_init_op. I0715 14:07:20.793029 47821886322496 session_manager.py:526] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0715 14:07:20.857252 47821886322496 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0715 14:07:21.417166 47821886322496 modeling.py:418] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:21.417759 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:25.965068 47821886322496 call_variants.py:462] Processed 1 examples in 1 batches [1565.044 sec per 100]. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1378, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1361, in _run_fn. return self._call_tf_sessionrun(options, feed_dict,",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:4326,security,model,model,4326,"ord.gz. INFO:tensorflow:Calling model_fn. I0715 14:07:10.758818 47821886322496 estimator.py:1173] Calling model_fn. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0715 14:07:16.873909 47821886322496 estimator.py:1175] Done calling model_fn. 2023-07-15 14:07:17.753203: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. INFO:tensorflow:Graph was finalized. I0715 14:07:18.197918 47821886322496 monitored_session.py:240] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:18.198770 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. INFO:tensorflow:Running local_init_op. I0715 14:07:20.793029 47821886322496 session_manager.py:526] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0715 14:07:20.857252 47821886322496 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0715 14:07:21.417166 47821886322496 modeling.py:418] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:21.417759 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:25.965068 47821886322496 call_variants.py:462] Processed 1 examples in 1 batches [1565.044 sec per 100]. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1378, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1361, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:4664,security,model,modeling,4664,"ings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0715 14:07:16.873909 47821886322496 estimator.py:1175] Done calling model_fn. 2023-07-15 14:07:17.753203: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. INFO:tensorflow:Graph was finalized. I0715 14:07:18.197918 47821886322496 monitored_session.py:240] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:18.198770 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. INFO:tensorflow:Running local_init_op. I0715 14:07:20.793029 47821886322496 session_manager.py:526] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0715 14:07:20.857252 47821886322496 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0715 14:07:21.417166 47821886322496 modeling.py:418] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:21.417759 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:25.965068 47821886322496 call_variants.py:462] Processed 1 examples in 1 batches [1565.044 sec per 100]. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1378, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1361, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1454, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.DataLossError: corrupted record at 484865306. [[{{node IteratorGetNext}}]]. During handling of the above ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:4745,security,model,models,4745,"_fn. I0715 14:07:16.873909 47821886322496 estimator.py:1175] Done calling model_fn. 2023-07-15 14:07:17.753203: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. INFO:tensorflow:Graph was finalized. I0715 14:07:18.197918 47821886322496 monitored_session.py:240] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:18.198770 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. INFO:tensorflow:Running local_init_op. I0715 14:07:20.793029 47821886322496 session_manager.py:526] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0715 14:07:20.857252 47821886322496 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0715 14:07:21.417166 47821886322496 modeling.py:418] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:21.417759 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:25.965068 47821886322496 call_variants.py:462] Processed 1 examples in 1 batches [1565.044 sec per 100]. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1378, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1361, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1454, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.DataLossError: corrupted record at 484865306. [[{{node IteratorGetNext}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:4759,security,model,model,4759,"07:16.873909 47821886322496 estimator.py:1175] Done calling model_fn. 2023-07-15 14:07:17.753203: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. INFO:tensorflow:Graph was finalized. I0715 14:07:18.197918 47821886322496 monitored_session.py:240] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:18.198770 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. INFO:tensorflow:Running local_init_op. I0715 14:07:20.793029 47821886322496 session_manager.py:526] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0715 14:07:20.857252 47821886322496 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0715 14:07:21.417166 47821886322496 modeling.py:418] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:21.417759 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:25.965068 47821886322496 call_variants.py:462] Processed 1 examples in 1 batches [1565.044 sec per 100]. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1378, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1361, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1454, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.DataLossError: corrupted record at 484865306. [[{{node IteratorGetNext}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.r",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:4854,security,model,models,4854,"3: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. INFO:tensorflow:Graph was finalized. I0715 14:07:18.197918 47821886322496 monitored_session.py:240] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:18.198770 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. INFO:tensorflow:Running local_init_op. I0715 14:07:20.793029 47821886322496 session_manager.py:526] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0715 14:07:20.857252 47821886322496 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0715 14:07:21.417166 47821886322496 modeling.py:418] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:21.417759 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:25.965068 47821886322496 call_variants.py:462] Processed 1 examples in 1 batches [1565.044 sec per 100]. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1378, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1361, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1454, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.DataLossError: corrupted record at 484865306. [[{{node IteratorGetNext}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <m",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:4868,security,model,model,4868,"w/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. INFO:tensorflow:Graph was finalized. I0715 14:07:18.197918 47821886322496 monitored_session.py:240] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:18.198770 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. INFO:tensorflow:Running local_init_op. I0715 14:07:20.793029 47821886322496 session_manager.py:526] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0715 14:07:20.857252 47821886322496 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0715 14:07:21.417166 47821886322496 modeling.py:418] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:21.417759 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:25.965068 47821886322496 call_variants.py:462] Processed 1 examples in 1 batches [1565.044 sec per 100]. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1378, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1361, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1454, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.DataLossError: corrupted record at 484865306. [[{{node IteratorGetNext}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.com",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:5103,security,session,session,5103,"6 monitored_session.py:240] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:18.198770 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. INFO:tensorflow:Running local_init_op. I0715 14:07:20.793029 47821886322496 session_manager.py:526] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0715 14:07:20.857252 47821886322496 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0715 14:07:21.417166 47821886322496 modeling.py:418] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:21.417759 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:25.965068 47821886322496 call_variants.py:462] Processed 1 examples in 1 batches [1565.044 sec per 100]. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1378, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1361, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1454, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.DataLossError: corrupted record at 484865306. [[{{node IteratorGetNext}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_p",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:5228,security,session,session,5228,"15 14:07:18.198770 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. INFO:tensorflow:Running local_init_op. I0715 14:07:20.793029 47821886322496 session_manager.py:526] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0715 14:07:20.857252 47821886322496 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0715 14:07:21.417166 47821886322496 modeling.py:418] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:21.417759 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:25.965068 47821886322496 call_variants.py:462] Processed 1 examples in 1 batches [1565.044 sec per 100]. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1378, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1361, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1454, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.DataLossError: corrupted record at 484865306. [[{{node IteratorGetNext}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", li",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:5399,security,session,session,5399,"1886322496 session_manager.py:526] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0715 14:07:20.857252 47821886322496 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0715 14:07:21.417166 47821886322496 modeling.py:418] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:21.417759 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:25.965068 47821886322496 call_variants.py:462] Processed 1 examples in 1 batches [1565.044 sec per 100]. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1378, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1361, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1454, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.DataLossError: corrupted record at 484865306. [[{{node IteratorGetNext}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_varia",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:7531,security,session,session,7531,"prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 642, in predict. preds_evaluated = mon_sess.run(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 778, in run. return self._sess.run(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1307, in run. return self._sess.run(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1397, in run. return self._sess.run(*args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1464, in run. outputs = _WrappedSession.run(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1228, in run. return self._sess.run(*args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 968, in run. result = self._run(None, fetches, feed_dict, options_ptr,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1191, in _run. results = self._do_run(handle, final_targets, final_fetches,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1371, in _do_run. return self._do_call(_run_fn, feeds, fetches, targets, options,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1397, in _do_call. raise type(e)(node_def, op, message) # pylint: disable=no-value-for-parameter. tensorflow.python.framework.errors_impl.DataLossError: Graph execution error:. Detected at node 'IteratorGetNext' defined at (most recent call last):. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(ma",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:7691,security,session,session,7691,"aluated = mon_sess.run(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 778, in run. return self._sess.run(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1307, in run. return self._sess.run(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1397, in run. return self._sess.run(*args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1464, in run. outputs = _WrappedSession.run(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1228, in run. return self._sess.run(*args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 968, in run. result = self._run(None, fetches, feed_dict, options_ptr,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1191, in _run. results = self._do_run(handle, final_targets, final_fetches,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1371, in _do_run. return self._do_call(_run_fn, feeds, fetches, targets, options,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1397, in _do_call. raise type(e)(node_def, op, message) # pylint: disable=no-value-for-parameter. tensorflow.python.framework.errors_impl.DataLossError: Graph execution error:. Detected at node 'IteratorGetNext' defined at (most recent call last):. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/ru",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:7856,security,session,session,7856,"ss.run(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1307, in run. return self._sess.run(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1397, in run. return self._sess.run(*args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1464, in run. outputs = _WrappedSession.run(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1228, in run. return self._sess.run(*args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 968, in run. result = self._run(None, fetches, feed_dict, options_ptr,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1191, in _run. results = self._do_run(handle, final_targets, final_fetches,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1371, in _do_run. return self._do_call(_run_fn, feeds, fetches, targets, options,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1397, in _do_call. raise type(e)(node_def, op, message) # pylint: disable=no-value-for-parameter. tensorflow.python.framework.errors_impl.DataLossError: Graph execution error:. Detected at node 'IteratorGetNext' defined at (most recent call last):. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/de",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:8027,security,session,session,8027,"hon3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1397, in run. return self._sess.run(*args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1464, in run. outputs = _WrappedSession.run(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1228, in run. return self._sess.run(*args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 968, in run. result = self._run(None, fetches, feed_dict, options_ptr,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1191, in _run. results = self._do_run(handle, final_targets, final_fetches,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1371, in _do_run. return self._do_call(_run_fn, feeds, fetches, targets, options,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1397, in _do_call. raise type(e)(node_def, op, message) # pylint: disable=no-value-for-parameter. tensorflow.python.framework.errors_impl.DataLossError: Graph execution error:. Detected at node 'IteratorGetNext' defined at (most recent call last):. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:8222,security,Detect,Detected,8222,"aining/monitored_session.py"", line 1464, in run. outputs = _WrappedSession.run(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1228, in run. return self._sess.run(*args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 968, in run. result = self._run(None, fetches, feed_dict, options_ptr,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1191, in _run. results = self._do_run(handle, final_targets, final_fetches,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1371, in _do_run. return self._do_call(_run_fn, feeds, fetches, targets, options,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1397, in _do_call. raise type(e)(node_def, op, message) # pylint: disable=no-value-for-parameter. tensorflow.python.framework.errors_impl.DataLossError: Graph execution error:. Detected at node 'IteratorGetNext' defined at (most recent call last):. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 621, in predict. features, input_hooks = self._get_features_from_input_fn(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:4997,testability,Trace,Traceback,4997,"sm_threads for best performance. INFO:tensorflow:Graph was finalized. I0715 14:07:18.197918 47821886322496 monitored_session.py:240] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:18.198770 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. INFO:tensorflow:Running local_init_op. I0715 14:07:20.793029 47821886322496 session_manager.py:526] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0715 14:07:20.857252 47821886322496 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0715 14:07:21.417166 47821886322496 modeling.py:418] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:21.417759 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:25.965068 47821886322496 call_variants.py:462] Processed 1 examples in 1 batches [1565.044 sec per 100]. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1378, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1361, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1454, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.DataLossError: corrupted record at 484865306. [[{{node IteratorGetNext}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run. _run(main=main,",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:5708,testability,Trace,Traceback,5708,":Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:21.417759 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:25.965068 47821886322496 call_variants.py:462] Processed 1 examples in 1 batches [1565.044 sec per 100]. Traceback (most recent call last):. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1378, in _do_call. return fn(*args). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1361, in _run_fn. return self._call_tf_sessionrun(options, feed_dict, fetch_list,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1454, in _call_tf_sessionrun. return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,. tensorflow.python.framework.errors_impl.DataLossError: corrupted record at 484865306. [[{{node IteratorGetNext}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 642, in predict. preds_evaluated = mon_sess",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:9280,testability,hook,hooks,9280,"ll last):. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 621, in predict. features, input_hooks = self._get_features_from_input_fn(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1019, in _get_features_from_input_fn. result, _, hooks = estimator_util.parse_input_fn_result(result). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/util.py"", line 60, in parse_input_fn_result. result = iterator.get_next(). Node: 'IteratorGetNext'. corrupted record at 484865306. [[{{node IteratorGetNext}}]]. Original stack trace for 'IteratorGetNext':. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_varian",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:9593,testability,trace,trace,9593,"y/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 621, in predict. features, input_hooks = self._get_features_from_input_fn(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1019, in _get_features_from_input_fn. result, _, hooks = estimator_util.parse_input_fn_result(result). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/util.py"", line 60, in parse_input_fn_result. result = iterator.get_next(). Node: 'IteratorGetNext'. corrupted record at 484865306. [[{{node IteratorGetNext}}]]. Original stack trace for 'IteratorGetNext':. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 621, in predict. features, input_hooks = self._",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:10777,testability,hook,hooks,10777,"/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 621, in predict. features, input_hooks = self._get_features_from_input_fn(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1019, in _get_features_from_input_fn. result, _, hooks = estimator_util.parse_input_fn_result(result). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/util.py"", line 60, in parse_input_fn_result. result = iterator.get_next(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/iterator_ops.py"", line 445, in get_next. flat_ret = gen_dataset_ops.iterator_get_next(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_dataset_ops.py"", line 3037, in iterator_get_next. _, _, _op, _outputs = _op_def_library._apply_op_helper(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py"", line 795, in _apply_op_helper. op = g._create_op_internal(op_type_name, inputs, dtypes=None,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 3798, in _create_op_internal. ret = Operation(. Best Regards! Bo.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:0,usability,Error,Error,0,"Error running deepvariant from singularity in cluster; Dear all,. I was trying to run deepvariant from singularity in cluster, but I always meet same error, I don't know hou to fix it:. /share/app/singularity/3.8.1/bin/singularity exec --containall --bind /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/tmp:/tmp --bind /usr/lib/locale/:/usr/lib/locale/ --bind $ccsbam:$ccsbam --bind $ccsbam.bai:$ccsbam.bai --bind $fasta:$fasta --bind $fasta.fai:$fasta.fai --bind /hwfssz5/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/translatome/HCT116/PacBio/analysis/deepvariant.v3/T202302180201:/output /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/software/deepvariant/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=$fasta --reads=$ccsbam --output_vcf=/output/T202302180201.deepvariant.vcf.gz --output_gvcf=/output/T202302180201.deepvariant.g.vcf.gz --num_shards=20 --intermediate_results_dir=/tmp. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/call_variants_output.tfrecord.gz"" --examples ""/tmp/make_examples.tfrecord@20.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"". 2023-07-15 14:06:58.063861: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0715 14:07:10.199614 47821886322496 call_variants.py:317] From /tmp/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:150,usability,error,error,150,"Error running deepvariant from singularity in cluster; Dear all,. I was trying to run deepvariant from singularity in cluster, but I always meet same error, I don't know hou to fix it:. /share/app/singularity/3.8.1/bin/singularity exec --containall --bind /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/tmp:/tmp --bind /usr/lib/locale/:/usr/lib/locale/ --bind $ccsbam:$ccsbam --bind $ccsbam.bai:$ccsbam.bai --bind $fasta:$fasta --bind $fasta.fai:$fasta.fai --bind /hwfssz5/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/translatome/HCT116/PacBio/analysis/deepvariant.v3/T202302180201:/output /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/software/deepvariant/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=$fasta --reads=$ccsbam --output_vcf=/output/T202302180201.deepvariant.vcf.gz --output_gvcf=/output/T202302180201.deepvariant.g.vcf.gz --num_shards=20 --intermediate_results_dir=/tmp. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/call_variants_output.tfrecord.gz"" --examples ""/tmp/make_examples.tfrecord@20.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"". 2023-07-15 14:06:58.063861: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0715 14:07:10.199614 47821886322496 call_variants.py:317] From /tmp/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:932,usability,command,command,932,"Error running deepvariant from singularity in cluster; Dear all,. I was trying to run deepvariant from singularity in cluster, but I always meet same error, I don't know hou to fix it:. /share/app/singularity/3.8.1/bin/singularity exec --containall --bind /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/tmp:/tmp --bind /usr/lib/locale/:/usr/lib/locale/ --bind $ccsbam:$ccsbam --bind $ccsbam.bai:$ccsbam.bai --bind $fasta:$fasta --bind $fasta.fai:$fasta.fai --bind /hwfssz5/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/translatome/HCT116/PacBio/analysis/deepvariant.v3/T202302180201:/output /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/software/deepvariant/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=$fasta --reads=$ccsbam --output_vcf=/output/T202302180201.deepvariant.vcf.gz --output_gvcf=/output/T202302180201.deepvariant.g.vcf.gz --num_shards=20 --intermediate_results_dir=/tmp. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/call_variants_output.tfrecord.gz"" --examples ""/tmp/make_examples.tfrecord@20.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"". 2023-07-15 14:06:58.063861: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0715 14:07:10.199614 47821886322496 call_variants.py:317] From /tmp/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:1337,usability,perform,performance-critical,1337,"ind $ccsbam:$ccsbam --bind $ccsbam.bai:$ccsbam.bai --bind $fasta:$fasta --bind $fasta.fai:$fasta.fai --bind /hwfssz5/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/translatome/HCT116/PacBio/analysis/deepvariant.v3/T202302180201:/output /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/software/deepvariant/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=$fasta --reads=$ccsbam --output_vcf=/output/T202302180201.deepvariant.vcf.gz --output_gvcf=/output/T202302180201.deepvariant.g.vcf.gz --num_shards=20 --intermediate_results_dir=/tmp. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/call_variants_output.tfrecord.gz"" --examples ""/tmp/make_examples.tfrecord@20.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"". 2023-07-15 14:06:58.063861: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0715 14:07:10.199614 47821886322496 call_variants.py:317] From /tmp/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-07-15 14:07:10.223654: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter o",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:1594,usability,input,input,1594,"S/P21Z10200N0125/zhanghaibo/software/deepvariant/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=$fasta --reads=$ccsbam --output_vcf=/output/T202302180201.deepvariant.vcf.gz --output_gvcf=/output/T202302180201.deepvariant.g.vcf.gz --num_shards=20 --intermediate_results_dir=/tmp. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/call_variants_output.tfrecord.gz"" --examples ""/tmp/make_examples.tfrecord@20.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"". 2023-07-15 14:06:58.063861: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0715 14:07:10.199614 47821886322496 call_variants.py:317] From /tmp/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-07-15 14:07:10.223654: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. 2023-07-15 14:07:10.226851: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled. WARNING:tensorflow:Using temporary folde",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:1637,usability,input,input,1637,"riant/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=$fasta --reads=$ccsbam --output_vcf=/output/T202302180201.deepvariant.vcf.gz --output_gvcf=/output/T202302180201.deepvariant.g.vcf.gz --num_shards=20 --intermediate_results_dir=/tmp. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/call_variants_output.tfrecord.gz"" --examples ""/tmp/make_examples.tfrecord@20.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"". 2023-07-15 14:06:58.063861: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0715 14:07:10.199614 47821886322496 call_variants.py:317] From /tmp/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-07-15 14:07:10.223654: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. 2023-07-15 14:07:10.226851: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpl_zjylv7. W07",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:1805,usability,input,input,1805,"--output_gvcf=/output/T202302180201.deepvariant.g.vcf.gz --num_shards=20 --intermediate_results_dir=/tmp. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/call_variants_output.tfrecord.gz"" --examples ""/tmp/make_examples.tfrecord@20.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"". 2023-07-15 14:06:58.063861: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0715 14:07:10.199614 47821886322496 call_variants.py:317] From /tmp/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-07-15 14:07:10.223654: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. 2023-07-15 14:07:10.226851: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpl_zjylv7. W0715 14:07:10.308488 47821886322496 estimator.py:1864] Using temporary folder as model directory: /tmp/tmpl_zjylv7. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmp",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:1848,usability,input,input,1848,"iant.g.vcf.gz --num_shards=20 --intermediate_results_dir=/tmp. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/call_variants_output.tfrecord.gz"" --examples ""/tmp/make_examples.tfrecord@20.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"". 2023-07-15 14:06:58.063861: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0715 14:07:10.199614 47821886322496 call_variants.py:317] From /tmp/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-07-15 14:07:10.223654: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. 2023-07-15 14:07:10.226851: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpl_zjylv7. W0715 14:07:10.308488 47821886322496 estimator.py:1864] Using temporary folder as model directory: /tmp/tmpl_zjylv7. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpl_zjylv7', '_tf_random_seed': None, '_save_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:2101,usability,perform,performance-critical,2101,"acbio/model.ckpt"". 2023-07-15 14:06:58.063861: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0715 14:07:10.199614 47821886322496 call_variants.py:317] From /tmp/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-07-15 14:07:10.223654: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. 2023-07-15 14:07:10.226851: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpl_zjylv7. W0715 14:07:10.308488 47821886322496 estimator.py:1864] Using temporary folder as model directory: /tmp/tmpl_zjylv7. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpl_zjylv7', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 10000. I0715 14:07:10.309390 47821886322496 estimator.py:202] Using config: {'_model_dir': '/tmp/tmpl_zjylv7', '_tf_random_seed': ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:2410,usability,perform,performance,2410,"ow with the appropriate compiler flags. I0715 14:07:10.199614 47821886322496 call_variants.py:317] From /tmp/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10]. 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2023-07-15 14:07:10.223654: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. 2023-07-15 14:07:10.226851: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpl_zjylv7. W0715 14:07:10.308488 47821886322496 estimator.py:1864] Using temporary folder as model directory: /tmp/tmpl_zjylv7. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpl_zjylv7', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 10000. I0715 14:07:10.309390 47821886322496 estimator.py:202] Using config: {'_model_dir': '/tmp/tmpl_zjylv7', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_c. I0715 14:07:10.309847 47821886322496 call_variants.py:446] Writing calls to /tmp/call_variants_output.tfrecord.gz. INFO:tensorflow:Calling model_fn. I0715 14:07:10.758818 47821886322496 estimato",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:3538,usability,User,UserWarning,3538,"not enabled. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpl_zjylv7. W0715 14:07:10.308488 47821886322496 estimator.py:1864] Using temporary folder as model directory: /tmp/tmpl_zjylv7. INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpl_zjylv7', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 10000. I0715 14:07:10.309390 47821886322496 estimator.py:202] Using config: {'_model_dir': '/tmp/tmpl_zjylv7', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_c. I0715 14:07:10.309847 47821886322496 call_variants.py:446] Writing calls to /tmp/call_variants_output.tfrecord.gz. INFO:tensorflow:Calling model_fn. I0715 14:07:10.758818 47821886322496 estimator.py:1173] Calling model_fn. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0715 14:07:16.873909 47821886322496 estimator.py:1175] Done calling model_fn. 2023-07-15 14:07:17.753203: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. INFO:tensorflow:Graph was finalized. I0715 14:07:18.197918 47821886322496 monitored_session.py:240] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:18.198770 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. INFO:tensorflow:Running local_init_op. I0715 14:07:20.793029 47821886322496 session_manager.py:526] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0715 14:07:20.857252 47821886322496 se",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:4022,usability,perform,performance,4022,"imator.py:202] Using config: {'_model_dir': '/tmp/tmpl_zjylv7', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_c. I0715 14:07:10.309847 47821886322496 call_variants.py:446] Writing calls to /tmp/call_variants_output.tfrecord.gz. INFO:tensorflow:Calling model_fn. I0715 14:07:10.758818 47821886322496 estimator.py:1173] Calling model_fn. /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead. warnings.warn('`layer.apply` is deprecated and '. INFO:tensorflow:Done calling model_fn. I0715 14:07:16.873909 47821886322496 estimator.py:1175] Done calling model_fn. 2023-07-15 14:07:17.753203: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. INFO:tensorflow:Graph was finalized. I0715 14:07:18.197918 47821886322496 monitored_session.py:240] Graph was finalized. INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:18.198770 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. INFO:tensorflow:Running local_init_op. I0715 14:07:20.793029 47821886322496 session_manager.py:526] Running local_init_op. INFO:tensorflow:Done running local_init_op. I0715 14:07:20.857252 47821886322496 session_manager.py:529] Done running local_init_op. INFO:tensorflow:Reloading EMA... I0715 14:07:21.417166 47821886322496 modeling.py:418] Reloading EMA... INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:21.417759 47821886322496 saver.py:1410] Restoring parameters from /opt/models/pacbio/model.ckpt. I0715 14:07:25.965068 47821886322496 call_variants.py:462] Processed 1 examples in 1 batches [1565.044 sec per 100]. Traceback (most recent call las",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:8214,usability,error,error,8214,"python/training/monitored_session.py"", line 1464, in run. outputs = _WrappedSession.run(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1228, in run. return self._sess.run(*args, **kwargs). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 968, in run. result = self._run(None, fetches, feed_dict, options_ptr,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1191, in _run. results = self._do_run(handle, final_targets, final_fetches,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1371, in _do_run. return self._do_call(_run_fn, feeds, fetches, targets, options,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1397, in _do_call. raise type(e)(node_def, op, message) # pylint: disable=no-value-for-parameter. tensorflow.python.framework.errors_impl.DataLossError: Graph execution error:. Detected at node 'IteratorGetNext' defined at (most recent call last):. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 621, in predict. features, input_hooks = self._get_features_from_input_fn(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estima",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/679:11492,usability,input,inputs,11492,"/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main. call_variants(. File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants. prediction = next(predictions). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 621, in predict. features, input_hooks = self._get_features_from_input_fn(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1019, in _get_features_from_input_fn. result, _, hooks = estimator_util.parse_input_fn_result(result). File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/util.py"", line 60, in parse_input_fn_result. result = iterator.get_next(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/iterator_ops.py"", line 445, in get_next. flat_ret = gen_dataset_ops.iterator_get_next(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_dataset_ops.py"", line 3037, in iterator_get_next. _, _, _op, _outputs = _op_def_library._apply_op_helper(. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py"", line 795, in _apply_op_helper. op = g._create_op_internal(op_type_name, inputs, dtypes=None,. File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 3798, in _create_op_internal. ret = Operation(. Best Regards! Bo.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/679
https://github.com/google/deepvariant/issues/680:146,integrability,sub,subsequently,146,"How to use DeepVariant for multisamples call snp？; Hey there,. I have done a separate call snp on individual bam using this tool under docker and subsequently merged the vcf files using GLnexus. But I would like to ask if I can do a group call snp on this group of individual bam at the same time? yours sincercely.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/680
https://github.com/google/deepvariant/issues/680:292,performance,time,time,292,"How to use DeepVariant for multisamples call snp？; Hey there,. I have done a separate call snp on individual bam using this tool under docker and subsequently merged the vcf files using GLnexus. But I would like to ask if I can do a group call snp on this group of individual bam at the same time? yours sincercely.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/680
https://github.com/google/deepvariant/issues/680:124,usability,tool,tool,124,"How to use DeepVariant for multisamples call snp？; Hey there,. I have done a separate call snp on individual bam using this tool under docker and subsequently merged the vcf files using GLnexus. But I would like to ask if I can do a group call snp on this group of individual bam at the same time? yours sincercely.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/680
https://github.com/google/deepvariant/issues/681:350,interoperability,bind,bind,350,"Prolonged DeepVariant Script Execution Time; Hi. I have attempted to execute a script using DeepVariant, and it has been running for approximately six days now without completion. I've only received intermediate outputs so far, without the expected final results. Here are the details of the run:. ```. singularity run -B ${SOFTWARE_DIR}:/software --bind ${INPUT_DIR}:/input --bind ${OUTPUT_DIR}:/output \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=ONT_R104 \. --ref=/input/references/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \. --reads=/input/HG004-hg38.ont.mm2.bam \. --output_vcf=/output/HG004_hg38.vcf.gz \. --output_gvcf=/output/HG004_hg38.g.vcf.gz \. --num_shards=12 \. --intermediate_results_dir=/output/ \. --dry_run=false. ```. Can you please provide any insight on this issue? . Thank you very much for your time and assistance.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/681
https://github.com/google/deepvariant/issues/681:377,interoperability,bind,bind,377,"Prolonged DeepVariant Script Execution Time; Hi. I have attempted to execute a script using DeepVariant, and it has been running for approximately six days now without completion. I've only received intermediate outputs so far, without the expected final results. Here are the details of the run:. ```. singularity run -B ${SOFTWARE_DIR}:/software --bind ${INPUT_DIR}:/input --bind ${OUTPUT_DIR}:/output \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=ONT_R104 \. --ref=/input/references/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \. --reads=/input/HG004-hg38.ont.mm2.bam \. --output_vcf=/output/HG004_hg38.vcf.gz \. --output_gvcf=/output/HG004_hg38.g.vcf.gz \. --num_shards=12 \. --intermediate_results_dir=/output/ \. --dry_run=false. ```. Can you please provide any insight on this issue? . Thank you very much for your time and assistance.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/681
https://github.com/google/deepvariant/issues/681:199,modifiability,interm,intermediate,199,"Prolonged DeepVariant Script Execution Time; Hi. I have attempted to execute a script using DeepVariant, and it has been running for approximately six days now without completion. I've only received intermediate outputs so far, without the expected final results. Here are the details of the run:. ```. singularity run -B ${SOFTWARE_DIR}:/software --bind ${INPUT_DIR}:/input --bind ${OUTPUT_DIR}:/output \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=ONT_R104 \. --ref=/input/references/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \. --reads=/input/HG004-hg38.ont.mm2.bam \. --output_vcf=/output/HG004_hg38.vcf.gz \. --output_gvcf=/output/HG004_hg38.g.vcf.gz \. --num_shards=12 \. --intermediate_results_dir=/output/ \. --dry_run=false. ```. Can you please provide any insight on this issue? . Thank you very much for your time and assistance.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/681
https://github.com/google/deepvariant/issues/681:350,modifiability,bind,bind,350,"Prolonged DeepVariant Script Execution Time; Hi. I have attempted to execute a script using DeepVariant, and it has been running for approximately six days now without completion. I've only received intermediate outputs so far, without the expected final results. Here are the details of the run:. ```. singularity run -B ${SOFTWARE_DIR}:/software --bind ${INPUT_DIR}:/input --bind ${OUTPUT_DIR}:/output \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=ONT_R104 \. --ref=/input/references/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \. --reads=/input/HG004-hg38.ont.mm2.bam \. --output_vcf=/output/HG004_hg38.vcf.gz \. --output_gvcf=/output/HG004_hg38.g.vcf.gz \. --num_shards=12 \. --intermediate_results_dir=/output/ \. --dry_run=false. ```. Can you please provide any insight on this issue? . Thank you very much for your time and assistance.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/681
https://github.com/google/deepvariant/issues/681:377,modifiability,bind,bind,377,"Prolonged DeepVariant Script Execution Time; Hi. I have attempted to execute a script using DeepVariant, and it has been running for approximately six days now without completion. I've only received intermediate outputs so far, without the expected final results. Here are the details of the run:. ```. singularity run -B ${SOFTWARE_DIR}:/software --bind ${INPUT_DIR}:/input --bind ${OUTPUT_DIR}:/output \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=ONT_R104 \. --ref=/input/references/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \. --reads=/input/HG004-hg38.ont.mm2.bam \. --output_vcf=/output/HG004_hg38.vcf.gz \. --output_gvcf=/output/HG004_hg38.g.vcf.gz \. --num_shards=12 \. --intermediate_results_dir=/output/ \. --dry_run=false. ```. Can you please provide any insight on this issue? . Thank you very much for your time and assistance.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/681
https://github.com/google/deepvariant/issues/681:29,performance,Execution Time,Execution Time,29,"Prolonged DeepVariant Script Execution Time; Hi. I have attempted to execute a script using DeepVariant, and it has been running for approximately six days now without completion. I've only received intermediate outputs so far, without the expected final results. Here are the details of the run:. ```. singularity run -B ${SOFTWARE_DIR}:/software --bind ${INPUT_DIR}:/input --bind ${OUTPUT_DIR}:/output \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=ONT_R104 \. --ref=/input/references/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \. --reads=/input/HG004-hg38.ont.mm2.bam \. --output_vcf=/output/HG004_hg38.vcf.gz \. --output_gvcf=/output/HG004_hg38.g.vcf.gz \. --num_shards=12 \. --intermediate_results_dir=/output/ \. --dry_run=false. ```. Can you please provide any insight on this issue? . Thank you very much for your time and assistance.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/681
https://github.com/google/deepvariant/issues/681:884,performance,time,time,884,"Prolonged DeepVariant Script Execution Time; Hi. I have attempted to execute a script using DeepVariant, and it has been running for approximately six days now without completion. I've only received intermediate outputs so far, without the expected final results. Here are the details of the run:. ```. singularity run -B ${SOFTWARE_DIR}:/software --bind ${INPUT_DIR}:/input --bind ${OUTPUT_DIR}:/output \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=ONT_R104 \. --ref=/input/references/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \. --reads=/input/HG004-hg38.ont.mm2.bam \. --output_vcf=/output/HG004_hg38.vcf.gz \. --output_gvcf=/output/HG004_hg38.g.vcf.gz \. --num_shards=12 \. --intermediate_results_dir=/output/ \. --dry_run=false. ```. Can you please provide any insight on this issue? . Thank you very much for your time and assistance.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/681
https://github.com/google/deepvariant/issues/681:168,safety,compl,completion,168,"Prolonged DeepVariant Script Execution Time; Hi. I have attempted to execute a script using DeepVariant, and it has been running for approximately six days now without completion. I've only received intermediate outputs so far, without the expected final results. Here are the details of the run:. ```. singularity run -B ${SOFTWARE_DIR}:/software --bind ${INPUT_DIR}:/input --bind ${OUTPUT_DIR}:/output \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=ONT_R104 \. --ref=/input/references/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \. --reads=/input/HG004-hg38.ont.mm2.bam \. --output_vcf=/output/HG004_hg38.vcf.gz \. --output_gvcf=/output/HG004_hg38.g.vcf.gz \. --num_shards=12 \. --intermediate_results_dir=/output/ \. --dry_run=false. ```. Can you please provide any insight on this issue? . Thank you very much for your time and assistance.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/681
https://github.com/google/deepvariant/issues/681:369,safety,input,input,369,"Prolonged DeepVariant Script Execution Time; Hi. I have attempted to execute a script using DeepVariant, and it has been running for approximately six days now without completion. I've only received intermediate outputs so far, without the expected final results. Here are the details of the run:. ```. singularity run -B ${SOFTWARE_DIR}:/software --bind ${INPUT_DIR}:/input --bind ${OUTPUT_DIR}:/output \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=ONT_R104 \. --ref=/input/references/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \. --reads=/input/HG004-hg38.ont.mm2.bam \. --output_vcf=/output/HG004_hg38.vcf.gz \. --output_gvcf=/output/HG004_hg38.g.vcf.gz \. --num_shards=12 \. --intermediate_results_dir=/output/ \. --dry_run=false. ```. Can you please provide any insight on this issue? . Thank you very much for your time and assistance.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/681
https://github.com/google/deepvariant/issues/681:527,safety,input,input,527,"Prolonged DeepVariant Script Execution Time; Hi. I have attempted to execute a script using DeepVariant, and it has been running for approximately six days now without completion. I've only received intermediate outputs so far, without the expected final results. Here are the details of the run:. ```. singularity run -B ${SOFTWARE_DIR}:/software --bind ${INPUT_DIR}:/input --bind ${OUTPUT_DIR}:/output \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=ONT_R104 \. --ref=/input/references/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \. --reads=/input/HG004-hg38.ont.mm2.bam \. --output_vcf=/output/HG004_hg38.vcf.gz \. --output_gvcf=/output/HG004_hg38.g.vcf.gz \. --num_shards=12 \. --intermediate_results_dir=/output/ \. --dry_run=false. ```. Can you please provide any insight on this issue? . Thank you very much for your time and assistance.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/681
https://github.com/google/deepvariant/issues/681:604,safety,input,input,604,"Prolonged DeepVariant Script Execution Time; Hi. I have attempted to execute a script using DeepVariant, and it has been running for approximately six days now without completion. I've only received intermediate outputs so far, without the expected final results. Here are the details of the run:. ```. singularity run -B ${SOFTWARE_DIR}:/software --bind ${INPUT_DIR}:/input --bind ${OUTPUT_DIR}:/output \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=ONT_R104 \. --ref=/input/references/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \. --reads=/input/HG004-hg38.ont.mm2.bam \. --output_vcf=/output/HG004_hg38.vcf.gz \. --output_gvcf=/output/HG004_hg38.g.vcf.gz \. --num_shards=12 \. --intermediate_results_dir=/output/ \. --dry_run=false. ```. Can you please provide any insight on this issue? . Thank you very much for your time and assistance.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/681
https://github.com/google/deepvariant/issues/681:168,security,compl,completion,168,"Prolonged DeepVariant Script Execution Time; Hi. I have attempted to execute a script using DeepVariant, and it has been running for approximately six days now without completion. I've only received intermediate outputs so far, without the expected final results. Here are the details of the run:. ```. singularity run -B ${SOFTWARE_DIR}:/software --bind ${INPUT_DIR}:/input --bind ${OUTPUT_DIR}:/output \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=ONT_R104 \. --ref=/input/references/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \. --reads=/input/HG004-hg38.ont.mm2.bam \. --output_vcf=/output/HG004_hg38.vcf.gz \. --output_gvcf=/output/HG004_hg38.g.vcf.gz \. --num_shards=12 \. --intermediate_results_dir=/output/ \. --dry_run=false. ```. Can you please provide any insight on this issue? . Thank you very much for your time and assistance.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/681
https://github.com/google/deepvariant/issues/681:369,usability,input,input,369,"Prolonged DeepVariant Script Execution Time; Hi. I have attempted to execute a script using DeepVariant, and it has been running for approximately six days now without completion. I've only received intermediate outputs so far, without the expected final results. Here are the details of the run:. ```. singularity run -B ${SOFTWARE_DIR}:/software --bind ${INPUT_DIR}:/input --bind ${OUTPUT_DIR}:/output \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=ONT_R104 \. --ref=/input/references/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \. --reads=/input/HG004-hg38.ont.mm2.bam \. --output_vcf=/output/HG004_hg38.vcf.gz \. --output_gvcf=/output/HG004_hg38.g.vcf.gz \. --num_shards=12 \. --intermediate_results_dir=/output/ \. --dry_run=false. ```. Can you please provide any insight on this issue? . Thank you very much for your time and assistance.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/681
https://github.com/google/deepvariant/issues/681:527,usability,input,input,527,"Prolonged DeepVariant Script Execution Time; Hi. I have attempted to execute a script using DeepVariant, and it has been running for approximately six days now without completion. I've only received intermediate outputs so far, without the expected final results. Here are the details of the run:. ```. singularity run -B ${SOFTWARE_DIR}:/software --bind ${INPUT_DIR}:/input --bind ${OUTPUT_DIR}:/output \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=ONT_R104 \. --ref=/input/references/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \. --reads=/input/HG004-hg38.ont.mm2.bam \. --output_vcf=/output/HG004_hg38.vcf.gz \. --output_gvcf=/output/HG004_hg38.g.vcf.gz \. --num_shards=12 \. --intermediate_results_dir=/output/ \. --dry_run=false. ```. Can you please provide any insight on this issue? . Thank you very much for your time and assistance.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/681
https://github.com/google/deepvariant/issues/681:604,usability,input,input,604,"Prolonged DeepVariant Script Execution Time; Hi. I have attempted to execute a script using DeepVariant, and it has been running for approximately six days now without completion. I've only received intermediate outputs so far, without the expected final results. Here are the details of the run:. ```. singularity run -B ${SOFTWARE_DIR}:/software --bind ${INPUT_DIR}:/input --bind ${OUTPUT_DIR}:/output \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=ONT_R104 \. --ref=/input/references/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \. --reads=/input/HG004-hg38.ont.mm2.bam \. --output_vcf=/output/HG004_hg38.vcf.gz \. --output_gvcf=/output/HG004_hg38.g.vcf.gz \. --num_shards=12 \. --intermediate_results_dir=/output/ \. --dry_run=false. ```. Can you please provide any insight on this issue? . Thank you very much for your time and assistance.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/681
https://github.com/google/deepvariant/issues/682:251,energy efficiency,model,models,251,"Low MAF for standing variation ; Hello. I am working on a clonal organism but we believe more and more it might not be so clonal. We have HiFi data and I want to use them to detect MAF. Is it possible with DeepVariant to detect that? Do you think the models are capable of that? I have asked the question over there to Clair3 people, they said the model was not conceived with this in mind so it might or might not work. Your opinion? . Thank you. Cheers",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682
https://github.com/google/deepvariant/issues/682:348,energy efficiency,model,model,348,"Low MAF for standing variation ; Hello. I am working on a clonal organism but we believe more and more it might not be so clonal. We have HiFi data and I want to use them to detect MAF. Is it possible with DeepVariant to detect that? Do you think the models are capable of that? I have asked the question over there to Clair3 people, they said the model was not conceived with this in mind so it might or might not work. Your opinion? . Thank you. Cheers",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682
https://github.com/google/deepvariant/issues/682:174,safety,detect,detect,174,"Low MAF for standing variation ; Hello. I am working on a clonal organism but we believe more and more it might not be so clonal. We have HiFi data and I want to use them to detect MAF. Is it possible with DeepVariant to detect that? Do you think the models are capable of that? I have asked the question over there to Clair3 people, they said the model was not conceived with this in mind so it might or might not work. Your opinion? . Thank you. Cheers",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682
https://github.com/google/deepvariant/issues/682:221,safety,detect,detect,221,"Low MAF for standing variation ; Hello. I am working on a clonal organism but we believe more and more it might not be so clonal. We have HiFi data and I want to use them to detect MAF. Is it possible with DeepVariant to detect that? Do you think the models are capable of that? I have asked the question over there to Clair3 people, they said the model was not conceived with this in mind so it might or might not work. Your opinion? . Thank you. Cheers",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682
https://github.com/google/deepvariant/issues/682:174,security,detect,detect,174,"Low MAF for standing variation ; Hello. I am working on a clonal organism but we believe more and more it might not be so clonal. We have HiFi data and I want to use them to detect MAF. Is it possible with DeepVariant to detect that? Do you think the models are capable of that? I have asked the question over there to Clair3 people, they said the model was not conceived with this in mind so it might or might not work. Your opinion? . Thank you. Cheers",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682
https://github.com/google/deepvariant/issues/682:221,security,detect,detect,221,"Low MAF for standing variation ; Hello. I am working on a clonal organism but we believe more and more it might not be so clonal. We have HiFi data and I want to use them to detect MAF. Is it possible with DeepVariant to detect that? Do you think the models are capable of that? I have asked the question over there to Clair3 people, they said the model was not conceived with this in mind so it might or might not work. Your opinion? . Thank you. Cheers",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682
https://github.com/google/deepvariant/issues/682:251,security,model,models,251,"Low MAF for standing variation ; Hello. I am working on a clonal organism but we believe more and more it might not be so clonal. We have HiFi data and I want to use them to detect MAF. Is it possible with DeepVariant to detect that? Do you think the models are capable of that? I have asked the question over there to Clair3 people, they said the model was not conceived with this in mind so it might or might not work. Your opinion? . Thank you. Cheers",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682
https://github.com/google/deepvariant/issues/682:348,security,model,model,348,"Low MAF for standing variation ; Hello. I am working on a clonal organism but we believe more and more it might not be so clonal. We have HiFi data and I want to use them to detect MAF. Is it possible with DeepVariant to detect that? Do you think the models are capable of that? I have asked the question over there to Clair3 people, they said the model was not conceived with this in mind so it might or might not work. Your opinion? . Thank you. Cheers",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/682
https://github.com/google/deepvariant/issues/683:433,availability,Operat,Operating,433,"very long running time; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). I have several human samples of PacBio HIFI reads with on average 20X depth. I was trying to call out small variants using deepvariant. However, it's been three days and the program is still at 'make_examples' stage. **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Operating system:. Redhat enterprise v7.9, x86_64. **Steps to reproduce:**. - Command:. BIN_VERSION=""1.5.0"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ --bind ${INPUT_DIR} --bind ${OUTPUT_DIR} \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/HG38.fa \. --reads=""${INPUT_DIR}""/0661-349-4156123_PDX_m15.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --num_shards=4 \. --intermediate_results_dir=""${OUTPUT_DIR}""/tmp_dir \. --make_examples_extra_args=""vsc_min_fraction_snps=0.2,vsc_min_fraction_indels=0.2"". I allocated 4 cores and 70GBs to run that program. I added the VAF thresholds for SNPs and Indels because I read the reported issues:. https://github.com/google/deepvariant/issues/578. - Error trace: (if applicable). And here are some most recent results I got from stdout:. I0720 09:27:03.965433 47167827691328 make_examples_core.py:257] 7300984 candidates (8293381 examples) [14.77s elapsed]. I0720 09:27:18.676311 47167827691328 make_examples_core.py:257] 7302320 candidates (8294814 examples) [14.71s elapsed]. I0720 09:28:15.982849 47167827691328 make_examples_core.py:257] 7304006 candidates (8296543 examples) [57.31s elapsed]. I0720 09:30:09.747373 47167827691328 make_examples_core.py:257] 73",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/683
https://github.com/google/deepvariant/issues/683:644,availability,Operat,Operating,644,"very long running time; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). I have several human samples of PacBio HIFI reads with on average 20X depth. I was trying to call out small variants using deepvariant. However, it's been three days and the program is still at 'make_examples' stage. **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Operating system:. Redhat enterprise v7.9, x86_64. **Steps to reproduce:**. - Command:. BIN_VERSION=""1.5.0"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ --bind ${INPUT_DIR} --bind ${OUTPUT_DIR} \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/HG38.fa \. --reads=""${INPUT_DIR}""/0661-349-4156123_PDX_m15.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --num_shards=4 \. --intermediate_results_dir=""${OUTPUT_DIR}""/tmp_dir \. --make_examples_extra_args=""vsc_min_fraction_snps=0.2,vsc_min_fraction_indels=0.2"". I allocated 4 cores and 70GBs to run that program. I added the VAF thresholds for SNPs and Indels because I read the reported issues:. https://github.com/google/deepvariant/issues/578. - Error trace: (if applicable). And here are some most recent results I got from stdout:. I0720 09:27:03.965433 47167827691328 make_examples_core.py:257] 7300984 candidates (8293381 examples) [14.77s elapsed]. I0720 09:27:18.676311 47167827691328 make_examples_core.py:257] 7302320 candidates (8294814 examples) [14.71s elapsed]. I0720 09:28:15.982849 47167827691328 make_examples_core.py:257] 7304006 candidates (8296543 examples) [57.31s elapsed]. I0720 09:30:09.747373 47167827691328 make_examples_core.py:257] 73",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/683
https://github.com/google/deepvariant/issues/683:1486,availability,Error,Error,1486,"n method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Operating system:. Redhat enterprise v7.9, x86_64. **Steps to reproduce:**. - Command:. BIN_VERSION=""1.5.0"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ --bind ${INPUT_DIR} --bind ${OUTPUT_DIR} \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/HG38.fa \. --reads=""${INPUT_DIR}""/0661-349-4156123_PDX_m15.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --num_shards=4 \. --intermediate_results_dir=""${OUTPUT_DIR}""/tmp_dir \. --make_examples_extra_args=""vsc_min_fraction_snps=0.2,vsc_min_fraction_indels=0.2"". I allocated 4 cores and 70GBs to run that program. I added the VAF thresholds for SNPs and Indels because I read the reported issues:. https://github.com/google/deepvariant/issues/578. - Error trace: (if applicable). And here are some most recent results I got from stdout:. I0720 09:27:03.965433 47167827691328 make_examples_core.py:257] 7300984 candidates (8293381 examples) [14.77s elapsed]. I0720 09:27:18.676311 47167827691328 make_examples_core.py:257] 7302320 candidates (8294814 examples) [14.71s elapsed]. I0720 09:28:15.982849 47167827691328 make_examples_core.py:257] 7304006 candidates (8296543 examples) [57.31s elapsed]. I0720 09:30:09.747373 47167827691328 make_examples_core.py:257] 7306537 candidates (8299251 examples) [113.76s elapsed]. I0720 09:30:47.913182 47167827691328 make_examples_core.py:257] 7309312 candidates (8302162 examples) [38.17s elapsed]. I0720 09:30:54.306647 47167827691328 make_examples_core.py:257] 7310398 candidates (8303278 examples) [6.39s elapsed]. I0720 09:31:09.601874 47167827691328 make_examples_core.py:257] 7312009 candidates (8304925 examples) [15.30s elapsed]. I0720 09:31:26.455226 47167827691328 make_examples_core.py:257] 7314037 can",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/683
https://github.com/google/deepvariant/issues/683:413,deployability,stage,stage,413,"very long running time; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). I have several human samples of PacBio HIFI reads with on average 20X depth. I was trying to call out small variants using deepvariant. However, it's been three days and the program is still at 'make_examples' stage. **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Operating system:. Redhat enterprise v7.9, x86_64. **Steps to reproduce:**. - Command:. BIN_VERSION=""1.5.0"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ --bind ${INPUT_DIR} --bind ${OUTPUT_DIR} \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/HG38.fa \. --reads=""${INPUT_DIR}""/0661-349-4156123_PDX_m15.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --num_shards=4 \. --intermediate_results_dir=""${OUTPUT_DIR}""/tmp_dir \. --make_examples_extra_args=""vsc_min_fraction_snps=0.2,vsc_min_fraction_indels=0.2"". I allocated 4 cores and 70GBs to run that program. I added the VAF thresholds for SNPs and Indels because I read the reported issues:. https://github.com/google/deepvariant/issues/578. - Error trace: (if applicable). And here are some most recent results I got from stdout:. I0720 09:27:03.965433 47167827691328 make_examples_core.py:257] 7300984 candidates (8293381 examples) [14.77s elapsed]. I0720 09:27:18.676311 47167827691328 make_examples_core.py:257] 7302320 candidates (8294814 examples) [14.71s elapsed]. I0720 09:28:15.982849 47167827691328 make_examples_core.py:257] 7304006 candidates (8296543 examples) [57.31s elapsed]. I0720 09:30:09.747373 47167827691328 make_examples_core.py:257] 73",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/683
https://github.com/google/deepvariant/issues/683:466,deployability,version,version,466,"very long running time; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). I have several human samples of PacBio HIFI reads with on average 20X depth. I was trying to call out small variants using deepvariant. However, it's been three days and the program is still at 'make_examples' stage. **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Operating system:. Redhat enterprise v7.9, x86_64. **Steps to reproduce:**. - Command:. BIN_VERSION=""1.5.0"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ --bind ${INPUT_DIR} --bind ${OUTPUT_DIR} \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/HG38.fa \. --reads=""${INPUT_DIR}""/0661-349-4156123_PDX_m15.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --num_shards=4 \. --intermediate_results_dir=""${OUTPUT_DIR}""/tmp_dir \. --make_examples_extra_args=""vsc_min_fraction_snps=0.2,vsc_min_fraction_indels=0.2"". I allocated 4 cores and 70GBs to run that program. I added the VAF thresholds for SNPs and Indels because I read the reported issues:. https://github.com/google/deepvariant/issues/578. - Error trace: (if applicable). And here are some most recent results I got from stdout:. I0720 09:27:03.965433 47167827691328 make_examples_core.py:257] 7300984 candidates (8293381 examples) [14.77s elapsed]. I0720 09:27:18.676311 47167827691328 make_examples_core.py:257] 7302320 candidates (8294814 examples) [14.71s elapsed]. I0720 09:28:15.982849 47167827691328 make_examples_core.py:257] 7304006 candidates (8296543 examples) [57.31s elapsed]. I0720 09:30:09.747373 47167827691328 make_examples_core.py:257] 73",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/683
https://github.com/google/deepvariant/issues/683:478,deployability,Instal,Installation,478,"very long running time; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). I have several human samples of PacBio HIFI reads with on average 20X depth. I was trying to call out small variants using deepvariant. However, it's been three days and the program is still at 'make_examples' stage. **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Operating system:. Redhat enterprise v7.9, x86_64. **Steps to reproduce:**. - Command:. BIN_VERSION=""1.5.0"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ --bind ${INPUT_DIR} --bind ${OUTPUT_DIR} \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/HG38.fa \. --reads=""${INPUT_DIR}""/0661-349-4156123_PDX_m15.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --num_shards=4 \. --intermediate_results_dir=""${OUTPUT_DIR}""/tmp_dir \. --make_examples_extra_args=""vsc_min_fraction_snps=0.2,vsc_min_fraction_indels=0.2"". I allocated 4 cores and 70GBs to run that program. I added the VAF thresholds for SNPs and Indels because I read the reported issues:. https://github.com/google/deepvariant/issues/578. - Error trace: (if applicable). And here are some most recent results I got from stdout:. I0720 09:27:03.965433 47167827691328 make_examples_core.py:257] 7300984 candidates (8293381 examples) [14.77s elapsed]. I0720 09:27:18.676311 47167827691328 make_examples_core.py:257] 7302320 candidates (8294814 examples) [14.71s elapsed]. I0720 09:28:15.982849 47167827691328 make_examples_core.py:257] 7304006 candidates (8296543 examples) [57.31s elapsed]. I0720 09:30:09.747373 47167827691328 make_examples_core.py:257] 73",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/683
https://github.com/google/deepvariant/issues/683:1301,energy efficiency,alloc,allocated,1301,"mall variants using deepvariant. However, it's been three days and the program is still at 'make_examples' stage. **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Operating system:. Redhat enterprise v7.9, x86_64. **Steps to reproduce:**. - Command:. BIN_VERSION=""1.5.0"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ --bind ${INPUT_DIR} --bind ${OUTPUT_DIR} \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/HG38.fa \. --reads=""${INPUT_DIR}""/0661-349-4156123_PDX_m15.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --num_shards=4 \. --intermediate_results_dir=""${OUTPUT_DIR}""/tmp_dir \. --make_examples_extra_args=""vsc_min_fraction_snps=0.2,vsc_min_fraction_indels=0.2"". I allocated 4 cores and 70GBs to run that program. I added the VAF thresholds for SNPs and Indels because I read the reported issues:. https://github.com/google/deepvariant/issues/578. - Error trace: (if applicable). And here are some most recent results I got from stdout:. I0720 09:27:03.965433 47167827691328 make_examples_core.py:257] 7300984 candidates (8293381 examples) [14.77s elapsed]. I0720 09:27:18.676311 47167827691328 make_examples_core.py:257] 7302320 candidates (8294814 examples) [14.71s elapsed]. I0720 09:28:15.982849 47167827691328 make_examples_core.py:257] 7304006 candidates (8296543 examples) [57.31s elapsed]. I0720 09:30:09.747373 47167827691328 make_examples_core.py:257] 7306537 candidates (8299251 examples) [113.76s elapsed]. I0720 09:30:47.913182 47167827691328 make_examples_core.py:257] 7309312 candidates (8302162 examples) [38.17s elapsed]. I0720 09:30:54.306647 47167827691328 make_examples_core.py:257] 7310398 candidates (8303278 examples) [6.39s elapsed]. I0720 09:31:",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/683
https://github.com/google/deepvariant/issues/683:1313,energy efficiency,core,cores,1313,"nts using deepvariant. However, it's been three days and the program is still at 'make_examples' stage. **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Operating system:. Redhat enterprise v7.9, x86_64. **Steps to reproduce:**. - Command:. BIN_VERSION=""1.5.0"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ --bind ${INPUT_DIR} --bind ${OUTPUT_DIR} \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/HG38.fa \. --reads=""${INPUT_DIR}""/0661-349-4156123_PDX_m15.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --num_shards=4 \. --intermediate_results_dir=""${OUTPUT_DIR}""/tmp_dir \. --make_examples_extra_args=""vsc_min_fraction_snps=0.2,vsc_min_fraction_indels=0.2"". I allocated 4 cores and 70GBs to run that program. I added the VAF thresholds for SNPs and Indels because I read the reported issues:. https://github.com/google/deepvariant/issues/578. - Error trace: (if applicable). And here are some most recent results I got from stdout:. I0720 09:27:03.965433 47167827691328 make_examples_core.py:257] 7300984 candidates (8293381 examples) [14.77s elapsed]. I0720 09:27:18.676311 47167827691328 make_examples_core.py:257] 7302320 candidates (8294814 examples) [14.71s elapsed]. I0720 09:28:15.982849 47167827691328 make_examples_core.py:257] 7304006 candidates (8296543 examples) [57.31s elapsed]. I0720 09:30:09.747373 47167827691328 make_examples_core.py:257] 7306537 candidates (8299251 examples) [113.76s elapsed]. I0720 09:30:47.913182 47167827691328 make_examples_core.py:257] 7309312 candidates (8302162 examples) [38.17s elapsed]. I0720 09:30:54.306647 47167827691328 make_examples_core.py:257] 7310398 candidates (8303278 examples) [6.39s elapsed]. I0720 09:31:09.601874 ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/683
https://github.com/google/deepvariant/issues/683:5145,energy efficiency,alloc,allocate,5145,09:34:11.184109 47167827691328 make_examples_core.py:257] 7332195 candidates (8325720 examples) [13.01s elapsed]. I0720 09:34:37.427601 47167827691328 make_examples_core.py:257] 7334598 candidates (8328193 examples) [26.24s elapsed]. I0720 09:34:53.436255 47167827691328 make_examples_core.py:257] 7336041 candidates (8329660 examples) [16.01s elapsed]. I0720 09:35:18.419953 47167827691328 make_examples_core.py:257] 7338676 candidates (8332321 examples) [24.98s elapsed]. I0720 09:35:32.331245 47167827691328 make_examples_core.py:257] 7340143 candidates (8333833 examples) [13.91s elapsed]. I0720 09:35:48.913797 47167827691328 make_examples_core.py:257] 7342460 candidates (8336192 examples) [16.58s elapsed]. I0720 09:36:05.716144 47167827691328 make_examples_core.py:257] 7344261 candidates (8338017 examples) [16.80s elapsed]. I0720 09:36:24.168858 47167827691328 make_examples_core.py:257] 7347004 candidates (8340828 examples) [18.45s elapsed]. I0720 09:36:42.921647 47167827691328 make_examples_core.py:257] 7349633 candidates (8343603 examples) [18.75s elapsed]. I0720 09:36:47.287455 47167827691328 make_examples_core.py:257] 7350149 candidates (8344123 examples) [4.37s elapsed]. I0720 09:37:06.694804 47167827691328 make_examples_core.py:257] 7352217 candidates (8346237 examples) [19.41s elapsed]. I0720 09:37:27.305462 47167827691328 make_examples_core.py:257] 7354095 candidates (8348169 examples) [20.61s elapsed]. I0720 09:37:48.644136 47167827691328 make_examples_core.py:257] 7356001 candidates (8350103 examples) [21.34s elapsed]. My question is: . Is the long running time due to the complexity of my samples? Do I need to allocate more threads to speed up this process? As I recall make_examples is a single-thread program. . **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes. **Any additional context:**.,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/683
https://github.com/google/deepvariant/issues/683:466,integrability,version,version,466,"very long running time; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). I have several human samples of PacBio HIFI reads with on average 20X depth. I was trying to call out small variants using deepvariant. However, it's been three days and the program is still at 'make_examples' stage. **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Operating system:. Redhat enterprise v7.9, x86_64. **Steps to reproduce:**. - Command:. BIN_VERSION=""1.5.0"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ --bind ${INPUT_DIR} --bind ${OUTPUT_DIR} \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/HG38.fa \. --reads=""${INPUT_DIR}""/0661-349-4156123_PDX_m15.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --num_shards=4 \. --intermediate_results_dir=""${OUTPUT_DIR}""/tmp_dir \. --make_examples_extra_args=""vsc_min_fraction_snps=0.2,vsc_min_fraction_indels=0.2"". I allocated 4 cores and 70GBs to run that program. I added the VAF thresholds for SNPs and Indels because I read the reported issues:. https://github.com/google/deepvariant/issues/578. - Error trace: (if applicable). And here are some most recent results I got from stdout:. I0720 09:27:03.965433 47167827691328 make_examples_core.py:257] 7300984 candidates (8293381 examples) [14.77s elapsed]. I0720 09:27:18.676311 47167827691328 make_examples_core.py:257] 7302320 candidates (8294814 examples) [14.71s elapsed]. I0720 09:28:15.982849 47167827691328 make_examples_core.py:257] 7304006 candidates (8296543 examples) [57.31s elapsed]. I0720 09:30:09.747373 47167827691328 make_examples_core.py:257] 73",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/683
https://github.com/google/deepvariant/issues/683:808,interoperability,bind,bind,808,"very long running time; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). I have several human samples of PacBio HIFI reads with on average 20X depth. I was trying to call out small variants using deepvariant. However, it's been three days and the program is still at 'make_examples' stage. **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Operating system:. Redhat enterprise v7.9, x86_64. **Steps to reproduce:**. - Command:. BIN_VERSION=""1.5.0"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ --bind ${INPUT_DIR} --bind ${OUTPUT_DIR} \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/HG38.fa \. --reads=""${INPUT_DIR}""/0661-349-4156123_PDX_m15.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --num_shards=4 \. --intermediate_results_dir=""${OUTPUT_DIR}""/tmp_dir \. --make_examples_extra_args=""vsc_min_fraction_snps=0.2,vsc_min_fraction_indels=0.2"". I allocated 4 cores and 70GBs to run that program. I added the VAF thresholds for SNPs and Indels because I read the reported issues:. https://github.com/google/deepvariant/issues/578. - Error trace: (if applicable). And here are some most recent results I got from stdout:. I0720 09:27:03.965433 47167827691328 make_examples_core.py:257] 7300984 candidates (8293381 examples) [14.77s elapsed]. I0720 09:27:18.676311 47167827691328 make_examples_core.py:257] 7302320 candidates (8294814 examples) [14.71s elapsed]. I0720 09:28:15.982849 47167827691328 make_examples_core.py:257] 7304006 candidates (8296543 examples) [57.31s elapsed]. I0720 09:30:09.747373 47167827691328 make_examples_core.py:257] 73",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/683
https://github.com/google/deepvariant/issues/683:828,interoperability,bind,bind,828,"very long running time; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). I have several human samples of PacBio HIFI reads with on average 20X depth. I was trying to call out small variants using deepvariant. However, it's been three days and the program is still at 'make_examples' stage. **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Operating system:. Redhat enterprise v7.9, x86_64. **Steps to reproduce:**. - Command:. BIN_VERSION=""1.5.0"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ --bind ${INPUT_DIR} --bind ${OUTPUT_DIR} \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/HG38.fa \. --reads=""${INPUT_DIR}""/0661-349-4156123_PDX_m15.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --num_shards=4 \. --intermediate_results_dir=""${OUTPUT_DIR}""/tmp_dir \. --make_examples_extra_args=""vsc_min_fraction_snps=0.2,vsc_min_fraction_indels=0.2"". I allocated 4 cores and 70GBs to run that program. I added the VAF thresholds for SNPs and Indels because I read the reported issues:. https://github.com/google/deepvariant/issues/578. - Error trace: (if applicable). And here are some most recent results I got from stdout:. I0720 09:27:03.965433 47167827691328 make_examples_core.py:257] 7300984 candidates (8293381 examples) [14.77s elapsed]. I0720 09:27:18.676311 47167827691328 make_examples_core.py:257] 7302320 candidates (8294814 examples) [14.71s elapsed]. I0720 09:28:15.982849 47167827691328 make_examples_core.py:257] 7304006 candidates (8296543 examples) [57.31s elapsed]. I0720 09:30:09.747373 47167827691328 make_examples_core.py:257] 73",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/683
https://github.com/google/deepvariant/issues/683:235,modifiability,Pac,PacBio,235,"very long running time; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). I have several human samples of PacBio HIFI reads with on average 20X depth. I was trying to call out small variants using deepvariant. However, it's been three days and the program is still at 'make_examples' stage. **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Operating system:. Redhat enterprise v7.9, x86_64. **Steps to reproduce:**. - Command:. BIN_VERSION=""1.5.0"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ --bind ${INPUT_DIR} --bind ${OUTPUT_DIR} \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/HG38.fa \. --reads=""${INPUT_DIR}""/0661-349-4156123_PDX_m15.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --num_shards=4 \. --intermediate_results_dir=""${OUTPUT_DIR}""/tmp_dir \. --make_examples_extra_args=""vsc_min_fraction_snps=0.2,vsc_min_fraction_indels=0.2"". I allocated 4 cores and 70GBs to run that program. I added the VAF thresholds for SNPs and Indels because I read the reported issues:. https://github.com/google/deepvariant/issues/578. - Error trace: (if applicable). And here are some most recent results I got from stdout:. I0720 09:27:03.965433 47167827691328 make_examples_core.py:257] 7300984 candidates (8293381 examples) [14.77s elapsed]. I0720 09:27:18.676311 47167827691328 make_examples_core.py:257] 7302320 candidates (8294814 examples) [14.71s elapsed]. I0720 09:28:15.982849 47167827691328 make_examples_core.py:257] 7304006 candidates (8296543 examples) [57.31s elapsed]. I0720 09:30:09.747373 47167827691328 make_examples_core.py:257] 73",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/683
https://github.com/google/deepvariant/issues/683:466,modifiability,version,version,466,"very long running time; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). I have several human samples of PacBio HIFI reads with on average 20X depth. I was trying to call out small variants using deepvariant. However, it's been three days and the program is still at 'make_examples' stage. **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Operating system:. Redhat enterprise v7.9, x86_64. **Steps to reproduce:**. - Command:. BIN_VERSION=""1.5.0"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ --bind ${INPUT_DIR} --bind ${OUTPUT_DIR} \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/HG38.fa \. --reads=""${INPUT_DIR}""/0661-349-4156123_PDX_m15.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --num_shards=4 \. --intermediate_results_dir=""${OUTPUT_DIR}""/tmp_dir \. --make_examples_extra_args=""vsc_min_fraction_snps=0.2,vsc_min_fraction_indels=0.2"". I allocated 4 cores and 70GBs to run that program. I added the VAF thresholds for SNPs and Indels because I read the reported issues:. https://github.com/google/deepvariant/issues/578. - Error trace: (if applicable). And here are some most recent results I got from stdout:. I0720 09:27:03.965433 47167827691328 make_examples_core.py:257] 7300984 candidates (8293381 examples) [14.77s elapsed]. I0720 09:27:18.676311 47167827691328 make_examples_core.py:257] 7302320 candidates (8294814 examples) [14.71s elapsed]. I0720 09:28:15.982849 47167827691328 make_examples_core.py:257] 7304006 candidates (8296543 examples) [57.31s elapsed]. I0720 09:30:09.747373 47167827691328 make_examples_core.py:257] 73",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/683
https://github.com/google/deepvariant/issues/683:808,modifiability,bind,bind,808,"very long running time; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). I have several human samples of PacBio HIFI reads with on average 20X depth. I was trying to call out small variants using deepvariant. However, it's been three days and the program is still at 'make_examples' stage. **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Operating system:. Redhat enterprise v7.9, x86_64. **Steps to reproduce:**. - Command:. BIN_VERSION=""1.5.0"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ --bind ${INPUT_DIR} --bind ${OUTPUT_DIR} \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/HG38.fa \. --reads=""${INPUT_DIR}""/0661-349-4156123_PDX_m15.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --num_shards=4 \. --intermediate_results_dir=""${OUTPUT_DIR}""/tmp_dir \. --make_examples_extra_args=""vsc_min_fraction_snps=0.2,vsc_min_fraction_indels=0.2"". I allocated 4 cores and 70GBs to run that program. I added the VAF thresholds for SNPs and Indels because I read the reported issues:. https://github.com/google/deepvariant/issues/578. - Error trace: (if applicable). And here are some most recent results I got from stdout:. I0720 09:27:03.965433 47167827691328 make_examples_core.py:257] 7300984 candidates (8293381 examples) [14.77s elapsed]. I0720 09:27:18.676311 47167827691328 make_examples_core.py:257] 7302320 candidates (8294814 examples) [14.71s elapsed]. I0720 09:28:15.982849 47167827691328 make_examples_core.py:257] 7304006 candidates (8296543 examples) [57.31s elapsed]. I0720 09:30:09.747373 47167827691328 make_examples_core.py:257] 73",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/683
https://github.com/google/deepvariant/issues/683:828,modifiability,bind,bind,828,"very long running time; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). I have several human samples of PacBio HIFI reads with on average 20X depth. I was trying to call out small variants using deepvariant. However, it's been three days and the program is still at 'make_examples' stage. **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Operating system:. Redhat enterprise v7.9, x86_64. **Steps to reproduce:**. - Command:. BIN_VERSION=""1.5.0"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ --bind ${INPUT_DIR} --bind ${OUTPUT_DIR} \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/HG38.fa \. --reads=""${INPUT_DIR}""/0661-349-4156123_PDX_m15.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --num_shards=4 \. --intermediate_results_dir=""${OUTPUT_DIR}""/tmp_dir \. --make_examples_extra_args=""vsc_min_fraction_snps=0.2,vsc_min_fraction_indels=0.2"". I allocated 4 cores and 70GBs to run that program. I added the VAF thresholds for SNPs and Indels because I read the reported issues:. https://github.com/google/deepvariant/issues/578. - Error trace: (if applicable). And here are some most recent results I got from stdout:. I0720 09:27:03.965433 47167827691328 make_examples_core.py:257] 7300984 candidates (8293381 examples) [14.77s elapsed]. I0720 09:27:18.676311 47167827691328 make_examples_core.py:257] 7302320 candidates (8294814 examples) [14.71s elapsed]. I0720 09:28:15.982849 47167827691328 make_examples_core.py:257] 7304006 candidates (8296543 examples) [57.31s elapsed]. I0720 09:30:09.747373 47167827691328 make_examples_core.py:257] 73",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/683
https://github.com/google/deepvariant/issues/683:951,modifiability,PAC,PACBIO,951,"very long running time; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). I have several human samples of PacBio HIFI reads with on average 20X depth. I was trying to call out small variants using deepvariant. However, it's been three days and the program is still at 'make_examples' stage. **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Operating system:. Redhat enterprise v7.9, x86_64. **Steps to reproduce:**. - Command:. BIN_VERSION=""1.5.0"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ --bind ${INPUT_DIR} --bind ${OUTPUT_DIR} \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/HG38.fa \. --reads=""${INPUT_DIR}""/0661-349-4156123_PDX_m15.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --num_shards=4 \. --intermediate_results_dir=""${OUTPUT_DIR}""/tmp_dir \. --make_examples_extra_args=""vsc_min_fraction_snps=0.2,vsc_min_fraction_indels=0.2"". I allocated 4 cores and 70GBs to run that program. I added the VAF thresholds for SNPs and Indels because I read the reported issues:. https://github.com/google/deepvariant/issues/578. - Error trace: (if applicable). And here are some most recent results I got from stdout:. I0720 09:27:03.965433 47167827691328 make_examples_core.py:257] 7300984 candidates (8293381 examples) [14.77s elapsed]. I0720 09:27:18.676311 47167827691328 make_examples_core.py:257] 7302320 candidates (8294814 examples) [14.71s elapsed]. I0720 09:28:15.982849 47167827691328 make_examples_core.py:257] 7304006 candidates (8296543 examples) [57.31s elapsed]. I0720 09:30:09.747373 47167827691328 make_examples_core.py:257] 73",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/683
https://github.com/google/deepvariant/issues/683:18,performance,time,time,18,"very long running time; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). I have several human samples of PacBio HIFI reads with on average 20X depth. I was trying to call out small variants using deepvariant. However, it's been three days and the program is still at 'make_examples' stage. **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Operating system:. Redhat enterprise v7.9, x86_64. **Steps to reproduce:**. - Command:. BIN_VERSION=""1.5.0"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ --bind ${INPUT_DIR} --bind ${OUTPUT_DIR} \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/HG38.fa \. --reads=""${INPUT_DIR}""/0661-349-4156123_PDX_m15.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --num_shards=4 \. --intermediate_results_dir=""${OUTPUT_DIR}""/tmp_dir \. --make_examples_extra_args=""vsc_min_fraction_snps=0.2,vsc_min_fraction_indels=0.2"". I allocated 4 cores and 70GBs to run that program. I added the VAF thresholds for SNPs and Indels because I read the reported issues:. https://github.com/google/deepvariant/issues/578. - Error trace: (if applicable). And here are some most recent results I got from stdout:. I0720 09:27:03.965433 47167827691328 make_examples_core.py:257] 7300984 candidates (8293381 examples) [14.77s elapsed]. I0720 09:27:18.676311 47167827691328 make_examples_core.py:257] 7302320 candidates (8294814 examples) [14.71s elapsed]. I0720 09:28:15.982849 47167827691328 make_examples_core.py:257] 7304006 candidates (8296543 examples) [57.31s elapsed]. I0720 09:30:09.747373 47167827691328 make_examples_core.py:257] 73",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/683
https://github.com/google/deepvariant/issues/683:1486,performance,Error,Error,1486,"n method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Operating system:. Redhat enterprise v7.9, x86_64. **Steps to reproduce:**. - Command:. BIN_VERSION=""1.5.0"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ --bind ${INPUT_DIR} --bind ${OUTPUT_DIR} \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/HG38.fa \. --reads=""${INPUT_DIR}""/0661-349-4156123_PDX_m15.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --num_shards=4 \. --intermediate_results_dir=""${OUTPUT_DIR}""/tmp_dir \. --make_examples_extra_args=""vsc_min_fraction_snps=0.2,vsc_min_fraction_indels=0.2"". I allocated 4 cores and 70GBs to run that program. I added the VAF thresholds for SNPs and Indels because I read the reported issues:. https://github.com/google/deepvariant/issues/578. - Error trace: (if applicable). And here are some most recent results I got from stdout:. I0720 09:27:03.965433 47167827691328 make_examples_core.py:257] 7300984 candidates (8293381 examples) [14.77s elapsed]. I0720 09:27:18.676311 47167827691328 make_examples_core.py:257] 7302320 candidates (8294814 examples) [14.71s elapsed]. I0720 09:28:15.982849 47167827691328 make_examples_core.py:257] 7304006 candidates (8296543 examples) [57.31s elapsed]. I0720 09:30:09.747373 47167827691328 make_examples_core.py:257] 7306537 candidates (8299251 examples) [113.76s elapsed]. I0720 09:30:47.913182 47167827691328 make_examples_core.py:257] 7309312 candidates (8302162 examples) [38.17s elapsed]. I0720 09:30:54.306647 47167827691328 make_examples_core.py:257] 7310398 candidates (8303278 examples) [6.39s elapsed]. I0720 09:31:09.601874 47167827691328 make_examples_core.py:257] 7312009 candidates (8304925 examples) [15.30s elapsed]. I0720 09:31:26.455226 47167827691328 make_examples_core.py:257] 7314037 can",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/683
https://github.com/google/deepvariant/issues/683:5090,performance,time,time,5090,09:34:11.184109 47167827691328 make_examples_core.py:257] 7332195 candidates (8325720 examples) [13.01s elapsed]. I0720 09:34:37.427601 47167827691328 make_examples_core.py:257] 7334598 candidates (8328193 examples) [26.24s elapsed]. I0720 09:34:53.436255 47167827691328 make_examples_core.py:257] 7336041 candidates (8329660 examples) [16.01s elapsed]. I0720 09:35:18.419953 47167827691328 make_examples_core.py:257] 7338676 candidates (8332321 examples) [24.98s elapsed]. I0720 09:35:32.331245 47167827691328 make_examples_core.py:257] 7340143 candidates (8333833 examples) [13.91s elapsed]. I0720 09:35:48.913797 47167827691328 make_examples_core.py:257] 7342460 candidates (8336192 examples) [16.58s elapsed]. I0720 09:36:05.716144 47167827691328 make_examples_core.py:257] 7344261 candidates (8338017 examples) [16.80s elapsed]. I0720 09:36:24.168858 47167827691328 make_examples_core.py:257] 7347004 candidates (8340828 examples) [18.45s elapsed]. I0720 09:36:42.921647 47167827691328 make_examples_core.py:257] 7349633 candidates (8343603 examples) [18.75s elapsed]. I0720 09:36:47.287455 47167827691328 make_examples_core.py:257] 7350149 candidates (8344123 examples) [4.37s elapsed]. I0720 09:37:06.694804 47167827691328 make_examples_core.py:257] 7352217 candidates (8346237 examples) [19.41s elapsed]. I0720 09:37:27.305462 47167827691328 make_examples_core.py:257] 7354095 candidates (8348169 examples) [20.61s elapsed]. I0720 09:37:48.644136 47167827691328 make_examples_core.py:257] 7356001 candidates (8350103 examples) [21.34s elapsed]. My question is: . Is the long running time due to the complexity of my samples? Do I need to allocate more threads to speed up this process? As I recall make_examples is a single-thread program. . **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes. **Any additional context:**.,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/683
https://github.com/google/deepvariant/issues/683:5251,reliability,Doe,Does,5251,09:34:11.184109 47167827691328 make_examples_core.py:257] 7332195 candidates (8325720 examples) [13.01s elapsed]. I0720 09:34:37.427601 47167827691328 make_examples_core.py:257] 7334598 candidates (8328193 examples) [26.24s elapsed]. I0720 09:34:53.436255 47167827691328 make_examples_core.py:257] 7336041 candidates (8329660 examples) [16.01s elapsed]. I0720 09:35:18.419953 47167827691328 make_examples_core.py:257] 7338676 candidates (8332321 examples) [24.98s elapsed]. I0720 09:35:32.331245 47167827691328 make_examples_core.py:257] 7340143 candidates (8333833 examples) [13.91s elapsed]. I0720 09:35:48.913797 47167827691328 make_examples_core.py:257] 7342460 candidates (8336192 examples) [16.58s elapsed]. I0720 09:36:05.716144 47167827691328 make_examples_core.py:257] 7344261 candidates (8338017 examples) [16.80s elapsed]. I0720 09:36:24.168858 47167827691328 make_examples_core.py:257] 7347004 candidates (8340828 examples) [18.45s elapsed]. I0720 09:36:42.921647 47167827691328 make_examples_core.py:257] 7349633 candidates (8343603 examples) [18.75s elapsed]. I0720 09:36:47.287455 47167827691328 make_examples_core.py:257] 7350149 candidates (8344123 examples) [4.37s elapsed]. I0720 09:37:06.694804 47167827691328 make_examples_core.py:257] 7352217 candidates (8346237 examples) [19.41s elapsed]. I0720 09:37:27.305462 47167827691328 make_examples_core.py:257] 7354095 candidates (8348169 examples) [20.61s elapsed]. I0720 09:37:48.644136 47167827691328 make_examples_core.py:257] 7356001 candidates (8350103 examples) [21.34s elapsed]. My question is: . Is the long running time due to the complexity of my samples? Do I need to allocate more threads to speed up this process? As I recall make_examples is a single-thread program. . **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes. **Any additional context:**.,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/683
https://github.com/google/deepvariant/issues/683:1486,safety,Error,Error,1486,"n method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Operating system:. Redhat enterprise v7.9, x86_64. **Steps to reproduce:**. - Command:. BIN_VERSION=""1.5.0"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ --bind ${INPUT_DIR} --bind ${OUTPUT_DIR} \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/HG38.fa \. --reads=""${INPUT_DIR}""/0661-349-4156123_PDX_m15.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --num_shards=4 \. --intermediate_results_dir=""${OUTPUT_DIR}""/tmp_dir \. --make_examples_extra_args=""vsc_min_fraction_snps=0.2,vsc_min_fraction_indels=0.2"". I allocated 4 cores and 70GBs to run that program. I added the VAF thresholds for SNPs and Indels because I read the reported issues:. https://github.com/google/deepvariant/issues/578. - Error trace: (if applicable). And here are some most recent results I got from stdout:. I0720 09:27:03.965433 47167827691328 make_examples_core.py:257] 7300984 candidates (8293381 examples) [14.77s elapsed]. I0720 09:27:18.676311 47167827691328 make_examples_core.py:257] 7302320 candidates (8294814 examples) [14.71s elapsed]. I0720 09:28:15.982849 47167827691328 make_examples_core.py:257] 7304006 candidates (8296543 examples) [57.31s elapsed]. I0720 09:30:09.747373 47167827691328 make_examples_core.py:257] 7306537 candidates (8299251 examples) [113.76s elapsed]. I0720 09:30:47.913182 47167827691328 make_examples_core.py:257] 7309312 candidates (8302162 examples) [38.17s elapsed]. I0720 09:30:54.306647 47167827691328 make_examples_core.py:257] 7310398 candidates (8303278 examples) [6.39s elapsed]. I0720 09:31:09.601874 47167827691328 make_examples_core.py:257] 7312009 candidates (8304925 examples) [15.30s elapsed]. I0720 09:31:26.455226 47167827691328 make_examples_core.py:257] 7314037 can",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/683
https://github.com/google/deepvariant/issues/683:5106,safety,compl,complexity,5106,09:34:11.184109 47167827691328 make_examples_core.py:257] 7332195 candidates (8325720 examples) [13.01s elapsed]. I0720 09:34:37.427601 47167827691328 make_examples_core.py:257] 7334598 candidates (8328193 examples) [26.24s elapsed]. I0720 09:34:53.436255 47167827691328 make_examples_core.py:257] 7336041 candidates (8329660 examples) [16.01s elapsed]. I0720 09:35:18.419953 47167827691328 make_examples_core.py:257] 7338676 candidates (8332321 examples) [24.98s elapsed]. I0720 09:35:32.331245 47167827691328 make_examples_core.py:257] 7340143 candidates (8333833 examples) [13.91s elapsed]. I0720 09:35:48.913797 47167827691328 make_examples_core.py:257] 7342460 candidates (8336192 examples) [16.58s elapsed]. I0720 09:36:05.716144 47167827691328 make_examples_core.py:257] 7344261 candidates (8338017 examples) [16.80s elapsed]. I0720 09:36:24.168858 47167827691328 make_examples_core.py:257] 7347004 candidates (8340828 examples) [18.45s elapsed]. I0720 09:36:42.921647 47167827691328 make_examples_core.py:257] 7349633 candidates (8343603 examples) [18.75s elapsed]. I0720 09:36:47.287455 47167827691328 make_examples_core.py:257] 7350149 candidates (8344123 examples) [4.37s elapsed]. I0720 09:37:06.694804 47167827691328 make_examples_core.py:257] 7352217 candidates (8346237 examples) [19.41s elapsed]. I0720 09:37:27.305462 47167827691328 make_examples_core.py:257] 7354095 candidates (8348169 examples) [20.61s elapsed]. I0720 09:37:48.644136 47167827691328 make_examples_core.py:257] 7356001 candidates (8350103 examples) [21.34s elapsed]. My question is: . Is the long running time due to the complexity of my samples? Do I need to allocate more threads to speed up this process? As I recall make_examples is a single-thread program. . **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes. **Any additional context:**.,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/683
https://github.com/google/deepvariant/issues/683:5272,safety,test,test,5272,09:34:11.184109 47167827691328 make_examples_core.py:257] 7332195 candidates (8325720 examples) [13.01s elapsed]. I0720 09:34:37.427601 47167827691328 make_examples_core.py:257] 7334598 candidates (8328193 examples) [26.24s elapsed]. I0720 09:34:53.436255 47167827691328 make_examples_core.py:257] 7336041 candidates (8329660 examples) [16.01s elapsed]. I0720 09:35:18.419953 47167827691328 make_examples_core.py:257] 7338676 candidates (8332321 examples) [24.98s elapsed]. I0720 09:35:32.331245 47167827691328 make_examples_core.py:257] 7340143 candidates (8333833 examples) [13.91s elapsed]. I0720 09:35:48.913797 47167827691328 make_examples_core.py:257] 7342460 candidates (8336192 examples) [16.58s elapsed]. I0720 09:36:05.716144 47167827691328 make_examples_core.py:257] 7344261 candidates (8338017 examples) [16.80s elapsed]. I0720 09:36:24.168858 47167827691328 make_examples_core.py:257] 7347004 candidates (8340828 examples) [18.45s elapsed]. I0720 09:36:42.921647 47167827691328 make_examples_core.py:257] 7349633 candidates (8343603 examples) [18.75s elapsed]. I0720 09:36:47.287455 47167827691328 make_examples_core.py:257] 7350149 candidates (8344123 examples) [4.37s elapsed]. I0720 09:37:06.694804 47167827691328 make_examples_core.py:257] 7352217 candidates (8346237 examples) [19.41s elapsed]. I0720 09:37:27.305462 47167827691328 make_examples_core.py:257] 7354095 candidates (8348169 examples) [20.61s elapsed]. I0720 09:37:48.644136 47167827691328 make_examples_core.py:257] 7356001 candidates (8350103 examples) [21.34s elapsed]. My question is: . Is the long running time due to the complexity of my samples? Do I need to allocate more threads to speed up this process? As I recall make_examples is a single-thread program. . **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes. **Any additional context:**.,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/683
https://github.com/google/deepvariant/issues/683:5308,safety,test,test,5308,09:34:11.184109 47167827691328 make_examples_core.py:257] 7332195 candidates (8325720 examples) [13.01s elapsed]. I0720 09:34:37.427601 47167827691328 make_examples_core.py:257] 7334598 candidates (8328193 examples) [26.24s elapsed]. I0720 09:34:53.436255 47167827691328 make_examples_core.py:257] 7336041 candidates (8329660 examples) [16.01s elapsed]. I0720 09:35:18.419953 47167827691328 make_examples_core.py:257] 7338676 candidates (8332321 examples) [24.98s elapsed]. I0720 09:35:32.331245 47167827691328 make_examples_core.py:257] 7340143 candidates (8333833 examples) [13.91s elapsed]. I0720 09:35:48.913797 47167827691328 make_examples_core.py:257] 7342460 candidates (8336192 examples) [16.58s elapsed]. I0720 09:36:05.716144 47167827691328 make_examples_core.py:257] 7344261 candidates (8338017 examples) [16.80s elapsed]. I0720 09:36:24.168858 47167827691328 make_examples_core.py:257] 7347004 candidates (8340828 examples) [18.45s elapsed]. I0720 09:36:42.921647 47167827691328 make_examples_core.py:257] 7349633 candidates (8343603 examples) [18.75s elapsed]. I0720 09:36:47.287455 47167827691328 make_examples_core.py:257] 7350149 candidates (8344123 examples) [4.37s elapsed]. I0720 09:37:06.694804 47167827691328 make_examples_core.py:257] 7352217 candidates (8346237 examples) [19.41s elapsed]. I0720 09:37:27.305462 47167827691328 make_examples_core.py:257] 7354095 candidates (8348169 examples) [20.61s elapsed]. I0720 09:37:48.644136 47167827691328 make_examples_core.py:257] 7356001 candidates (8350103 examples) [21.34s elapsed]. My question is: . Is the long running time due to the complexity of my samples? Do I need to allocate more threads to speed up this process? As I recall make_examples is a single-thread program. . **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes. **Any additional context:**.,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/683
https://github.com/google/deepvariant/issues/683:5106,security,compl,complexity,5106,09:34:11.184109 47167827691328 make_examples_core.py:257] 7332195 candidates (8325720 examples) [13.01s elapsed]. I0720 09:34:37.427601 47167827691328 make_examples_core.py:257] 7334598 candidates (8328193 examples) [26.24s elapsed]. I0720 09:34:53.436255 47167827691328 make_examples_core.py:257] 7336041 candidates (8329660 examples) [16.01s elapsed]. I0720 09:35:18.419953 47167827691328 make_examples_core.py:257] 7338676 candidates (8332321 examples) [24.98s elapsed]. I0720 09:35:32.331245 47167827691328 make_examples_core.py:257] 7340143 candidates (8333833 examples) [13.91s elapsed]. I0720 09:35:48.913797 47167827691328 make_examples_core.py:257] 7342460 candidates (8336192 examples) [16.58s elapsed]. I0720 09:36:05.716144 47167827691328 make_examples_core.py:257] 7344261 candidates (8338017 examples) [16.80s elapsed]. I0720 09:36:24.168858 47167827691328 make_examples_core.py:257] 7347004 candidates (8340828 examples) [18.45s elapsed]. I0720 09:36:42.921647 47167827691328 make_examples_core.py:257] 7349633 candidates (8343603 examples) [18.75s elapsed]. I0720 09:36:47.287455 47167827691328 make_examples_core.py:257] 7350149 candidates (8344123 examples) [4.37s elapsed]. I0720 09:37:06.694804 47167827691328 make_examples_core.py:257] 7352217 candidates (8346237 examples) [19.41s elapsed]. I0720 09:37:27.305462 47167827691328 make_examples_core.py:257] 7354095 candidates (8348169 examples) [20.61s elapsed]. I0720 09:37:48.644136 47167827691328 make_examples_core.py:257] 7356001 candidates (8350103 examples) [21.34s elapsed]. My question is: . Is the long running time due to the complexity of my samples? Do I need to allocate more threads to speed up this process? As I recall make_examples is a single-thread program. . **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes. **Any additional context:**.,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/683
https://github.com/google/deepvariant/issues/683:562,testability,instrument,instrument,562,"very long running time; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). I have several human samples of PacBio HIFI reads with on average 20X depth. I was trying to call out small variants using deepvariant. However, it's been three days and the program is still at 'make_examples' stage. **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Operating system:. Redhat enterprise v7.9, x86_64. **Steps to reproduce:**. - Command:. BIN_VERSION=""1.5.0"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ --bind ${INPUT_DIR} --bind ${OUTPUT_DIR} \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/HG38.fa \. --reads=""${INPUT_DIR}""/0661-349-4156123_PDX_m15.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --num_shards=4 \. --intermediate_results_dir=""${OUTPUT_DIR}""/tmp_dir \. --make_examples_extra_args=""vsc_min_fraction_snps=0.2,vsc_min_fraction_indels=0.2"". I allocated 4 cores and 70GBs to run that program. I added the VAF thresholds for SNPs and Indels because I read the reported issues:. https://github.com/google/deepvariant/issues/578. - Error trace: (if applicable). And here are some most recent results I got from stdout:. I0720 09:27:03.965433 47167827691328 make_examples_core.py:257] 7300984 candidates (8293381 examples) [14.77s elapsed]. I0720 09:27:18.676311 47167827691328 make_examples_core.py:257] 7302320 candidates (8294814 examples) [14.71s elapsed]. I0720 09:28:15.982849 47167827691328 make_examples_core.py:257] 7304006 candidates (8296543 examples) [57.31s elapsed]. I0720 09:30:09.747373 47167827691328 make_examples_core.py:257] 73",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/683
https://github.com/google/deepvariant/issues/683:1492,testability,trace,trace,1492,"od (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Operating system:. Redhat enterprise v7.9, x86_64. **Steps to reproduce:**. - Command:. BIN_VERSION=""1.5.0"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ --bind ${INPUT_DIR} --bind ${OUTPUT_DIR} \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/HG38.fa \. --reads=""${INPUT_DIR}""/0661-349-4156123_PDX_m15.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --num_shards=4 \. --intermediate_results_dir=""${OUTPUT_DIR}""/tmp_dir \. --make_examples_extra_args=""vsc_min_fraction_snps=0.2,vsc_min_fraction_indels=0.2"". I allocated 4 cores and 70GBs to run that program. I added the VAF thresholds for SNPs and Indels because I read the reported issues:. https://github.com/google/deepvariant/issues/578. - Error trace: (if applicable). And here are some most recent results I got from stdout:. I0720 09:27:03.965433 47167827691328 make_examples_core.py:257] 7300984 candidates (8293381 examples) [14.77s elapsed]. I0720 09:27:18.676311 47167827691328 make_examples_core.py:257] 7302320 candidates (8294814 examples) [14.71s elapsed]. I0720 09:28:15.982849 47167827691328 make_examples_core.py:257] 7304006 candidates (8296543 examples) [57.31s elapsed]. I0720 09:30:09.747373 47167827691328 make_examples_core.py:257] 7306537 candidates (8299251 examples) [113.76s elapsed]. I0720 09:30:47.913182 47167827691328 make_examples_core.py:257] 7309312 candidates (8302162 examples) [38.17s elapsed]. I0720 09:30:54.306647 47167827691328 make_examples_core.py:257] 7310398 candidates (8303278 examples) [6.39s elapsed]. I0720 09:31:09.601874 47167827691328 make_examples_core.py:257] 7312009 candidates (8304925 examples) [15.30s elapsed]. I0720 09:31:26.455226 47167827691328 make_examples_core.py:257] 7314037 candidate",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/683
https://github.com/google/deepvariant/issues/683:5272,testability,test,test,5272,09:34:11.184109 47167827691328 make_examples_core.py:257] 7332195 candidates (8325720 examples) [13.01s elapsed]. I0720 09:34:37.427601 47167827691328 make_examples_core.py:257] 7334598 candidates (8328193 examples) [26.24s elapsed]. I0720 09:34:53.436255 47167827691328 make_examples_core.py:257] 7336041 candidates (8329660 examples) [16.01s elapsed]. I0720 09:35:18.419953 47167827691328 make_examples_core.py:257] 7338676 candidates (8332321 examples) [24.98s elapsed]. I0720 09:35:32.331245 47167827691328 make_examples_core.py:257] 7340143 candidates (8333833 examples) [13.91s elapsed]. I0720 09:35:48.913797 47167827691328 make_examples_core.py:257] 7342460 candidates (8336192 examples) [16.58s elapsed]. I0720 09:36:05.716144 47167827691328 make_examples_core.py:257] 7344261 candidates (8338017 examples) [16.80s elapsed]. I0720 09:36:24.168858 47167827691328 make_examples_core.py:257] 7347004 candidates (8340828 examples) [18.45s elapsed]. I0720 09:36:42.921647 47167827691328 make_examples_core.py:257] 7349633 candidates (8343603 examples) [18.75s elapsed]. I0720 09:36:47.287455 47167827691328 make_examples_core.py:257] 7350149 candidates (8344123 examples) [4.37s elapsed]. I0720 09:37:06.694804 47167827691328 make_examples_core.py:257] 7352217 candidates (8346237 examples) [19.41s elapsed]. I0720 09:37:27.305462 47167827691328 make_examples_core.py:257] 7354095 candidates (8348169 examples) [20.61s elapsed]. I0720 09:37:48.644136 47167827691328 make_examples_core.py:257] 7356001 candidates (8350103 examples) [21.34s elapsed]. My question is: . Is the long running time due to the complexity of my samples? Do I need to allocate more threads to speed up this process? As I recall make_examples is a single-thread program. . **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes. **Any additional context:**.,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/683
https://github.com/google/deepvariant/issues/683:5308,testability,test,test,5308,09:34:11.184109 47167827691328 make_examples_core.py:257] 7332195 candidates (8325720 examples) [13.01s elapsed]. I0720 09:34:37.427601 47167827691328 make_examples_core.py:257] 7334598 candidates (8328193 examples) [26.24s elapsed]. I0720 09:34:53.436255 47167827691328 make_examples_core.py:257] 7336041 candidates (8329660 examples) [16.01s elapsed]. I0720 09:35:18.419953 47167827691328 make_examples_core.py:257] 7338676 candidates (8332321 examples) [24.98s elapsed]. I0720 09:35:32.331245 47167827691328 make_examples_core.py:257] 7340143 candidates (8333833 examples) [13.91s elapsed]. I0720 09:35:48.913797 47167827691328 make_examples_core.py:257] 7342460 candidates (8336192 examples) [16.58s elapsed]. I0720 09:36:05.716144 47167827691328 make_examples_core.py:257] 7344261 candidates (8338017 examples) [16.80s elapsed]. I0720 09:36:24.168858 47167827691328 make_examples_core.py:257] 7347004 candidates (8340828 examples) [18.45s elapsed]. I0720 09:36:42.921647 47167827691328 make_examples_core.py:257] 7349633 candidates (8343603 examples) [18.75s elapsed]. I0720 09:36:47.287455 47167827691328 make_examples_core.py:257] 7350149 candidates (8344123 examples) [4.37s elapsed]. I0720 09:37:06.694804 47167827691328 make_examples_core.py:257] 7352217 candidates (8346237 examples) [19.41s elapsed]. I0720 09:37:27.305462 47167827691328 make_examples_core.py:257] 7354095 candidates (8348169 examples) [20.61s elapsed]. I0720 09:37:48.644136 47167827691328 make_examples_core.py:257] 7356001 candidates (8350103 examples) [21.34s elapsed]. My question is: . Is the long running time due to the complexity of my samples? Do I need to allocate more threads to speed up this process? As I recall make_examples is a single-thread program. . **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes. **Any additional context:**.,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/683
https://github.com/google/deepvariant/issues/683:5488,testability,context,context,5488,09:34:11.184109 47167827691328 make_examples_core.py:257] 7332195 candidates (8325720 examples) [13.01s elapsed]. I0720 09:34:37.427601 47167827691328 make_examples_core.py:257] 7334598 candidates (8328193 examples) [26.24s elapsed]. I0720 09:34:53.436255 47167827691328 make_examples_core.py:257] 7336041 candidates (8329660 examples) [16.01s elapsed]. I0720 09:35:18.419953 47167827691328 make_examples_core.py:257] 7338676 candidates (8332321 examples) [24.98s elapsed]. I0720 09:35:32.331245 47167827691328 make_examples_core.py:257] 7340143 candidates (8333833 examples) [13.91s elapsed]. I0720 09:35:48.913797 47167827691328 make_examples_core.py:257] 7342460 candidates (8336192 examples) [16.58s elapsed]. I0720 09:36:05.716144 47167827691328 make_examples_core.py:257] 7344261 candidates (8338017 examples) [16.80s elapsed]. I0720 09:36:24.168858 47167827691328 make_examples_core.py:257] 7347004 candidates (8340828 examples) [18.45s elapsed]. I0720 09:36:42.921647 47167827691328 make_examples_core.py:257] 7349633 candidates (8343603 examples) [18.75s elapsed]. I0720 09:36:47.287455 47167827691328 make_examples_core.py:257] 7350149 candidates (8344123 examples) [4.37s elapsed]. I0720 09:37:06.694804 47167827691328 make_examples_core.py:257] 7352217 candidates (8346237 examples) [19.41s elapsed]. I0720 09:37:27.305462 47167827691328 make_examples_core.py:257] 7354095 candidates (8348169 examples) [20.61s elapsed]. I0720 09:37:48.644136 47167827691328 make_examples_core.py:257] 7356001 candidates (8350103 examples) [21.34s elapsed]. My question is: . Is the long running time due to the complexity of my samples? Do I need to allocate more threads to speed up this process? As I recall make_examples is a single-thread program. . **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? Yes. **Any additional context:**.,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/683
https://github.com/google/deepvariant/issues/683:149,usability,clear,clear,149,"very long running time; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). I have several human samples of PacBio HIFI reads with on average 20X depth. I was trying to call out small variants using deepvariant. However, it's been three days and the program is still at 'make_examples' stage. **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Operating system:. Redhat enterprise v7.9, x86_64. **Steps to reproduce:**. - Command:. BIN_VERSION=""1.5.0"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ --bind ${INPUT_DIR} --bind ${OUTPUT_DIR} \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/HG38.fa \. --reads=""${INPUT_DIR}""/0661-349-4156123_PDX_m15.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --num_shards=4 \. --intermediate_results_dir=""${OUTPUT_DIR}""/tmp_dir \. --make_examples_extra_args=""vsc_min_fraction_snps=0.2,vsc_min_fraction_indels=0.2"". I allocated 4 cores and 70GBs to run that program. I added the VAF thresholds for SNPs and Indels because I read the reported issues:. https://github.com/google/deepvariant/issues/578. - Error trace: (if applicable). And here are some most recent results I got from stdout:. I0720 09:27:03.965433 47167827691328 make_examples_core.py:257] 7300984 candidates (8293381 examples) [14.77s elapsed]. I0720 09:27:18.676311 47167827691328 make_examples_core.py:257] 7302320 candidates (8294814 examples) [14.71s elapsed]. I0720 09:28:15.982849 47167827691328 make_examples_core.py:257] 7304006 candidates (8296543 examples) [57.31s elapsed]. I0720 09:30:09.747373 47167827691328 make_examples_core.py:257] 73",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/683
https://github.com/google/deepvariant/issues/683:722,usability,Command,Command,722,"very long running time; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. Yes. **Describe the issue:**. (A clear and concise description of what the issue is.). I have several human samples of PacBio HIFI reads with on average 20X depth. I was trying to call out small variants using deepvariant. However, it's been three days and the program is still at 'make_examples' stage. **Setup**. - Operating system:. - DeepVariant version:. - Installation method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Operating system:. Redhat enterprise v7.9, x86_64. **Steps to reproduce:**. - Command:. BIN_VERSION=""1.5.0"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ --bind ${INPUT_DIR} --bind ${OUTPUT_DIR} \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/HG38.fa \. --reads=""${INPUT_DIR}""/0661-349-4156123_PDX_m15.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --num_shards=4 \. --intermediate_results_dir=""${OUTPUT_DIR}""/tmp_dir \. --make_examples_extra_args=""vsc_min_fraction_snps=0.2,vsc_min_fraction_indels=0.2"". I allocated 4 cores and 70GBs to run that program. I added the VAF thresholds for SNPs and Indels because I read the reported issues:. https://github.com/google/deepvariant/issues/578. - Error trace: (if applicable). And here are some most recent results I got from stdout:. I0720 09:27:03.965433 47167827691328 make_examples_core.py:257] 7300984 candidates (8293381 examples) [14.77s elapsed]. I0720 09:27:18.676311 47167827691328 make_examples_core.py:257] 7302320 candidates (8294814 examples) [14.71s elapsed]. I0720 09:28:15.982849 47167827691328 make_examples_core.py:257] 7304006 candidates (8296543 examples) [57.31s elapsed]. I0720 09:30:09.747373 47167827691328 make_examples_core.py:257] 73",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/683
https://github.com/google/deepvariant/issues/683:1486,usability,Error,Error,1486,"n method (Docker, built from source, etc.):. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). Operating system:. Redhat enterprise v7.9, x86_64. **Steps to reproduce:**. - Command:. BIN_VERSION=""1.5.0"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ --bind ${INPUT_DIR} --bind ${OUTPUT_DIR} \. docker://google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=PACBIO \. --ref=""${INPUT_DIR}""/HG38.fa \. --reads=""${INPUT_DIR}""/0661-349-4156123_PDX_m15.bam \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --num_shards=4 \. --intermediate_results_dir=""${OUTPUT_DIR}""/tmp_dir \. --make_examples_extra_args=""vsc_min_fraction_snps=0.2,vsc_min_fraction_indels=0.2"". I allocated 4 cores and 70GBs to run that program. I added the VAF thresholds for SNPs and Indels because I read the reported issues:. https://github.com/google/deepvariant/issues/578. - Error trace: (if applicable). And here are some most recent results I got from stdout:. I0720 09:27:03.965433 47167827691328 make_examples_core.py:257] 7300984 candidates (8293381 examples) [14.77s elapsed]. I0720 09:27:18.676311 47167827691328 make_examples_core.py:257] 7302320 candidates (8294814 examples) [14.71s elapsed]. I0720 09:28:15.982849 47167827691328 make_examples_core.py:257] 7304006 candidates (8296543 examples) [57.31s elapsed]. I0720 09:30:09.747373 47167827691328 make_examples_core.py:257] 7306537 candidates (8299251 examples) [113.76s elapsed]. I0720 09:30:47.913182 47167827691328 make_examples_core.py:257] 7309312 candidates (8302162 examples) [38.17s elapsed]. I0720 09:30:54.306647 47167827691328 make_examples_core.py:257] 7310398 candidates (8303278 examples) [6.39s elapsed]. I0720 09:31:09.601874 47167827691328 make_examples_core.py:257] 7312009 candidates (8304925 examples) [15.30s elapsed]. I0720 09:31:26.455226 47167827691328 make_examples_core.py:257] 7314037 can",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/683
https://github.com/google/deepvariant/issues/684:4,reliability,doe,does,4,"Why does DeepVariant PASS variants that have such a low read depth ~2 ? ; Hello, . I just wanted to ask if deepvariants PASS/RECALL excludes variants of low depth? I have some variants that passed but have very low depth in that area. Thanks! Amy.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/684
https://github.com/google/deepvariant/issues/685:162,availability,avail,avail,162,"Works in dry run, not in functional mode; Hello, . I am very sorry because this seems like a kind of s*** I could figure out but I have spent the afternoon to no avail. So, the following command WORKS in dry run mode, no error, but fails when in real operative mode. . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""1.5.0"". OUTPUT_VCF=vaga_lab_hifi_standing_variation.vcf.gz. OUTPUT_GVCF=vaga_lab_hifi_standing_variation.vcf.gz. BAM=HiFi_vaga.sorted.bam. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref=/input/Adineta_vaga.fsa --make_examples_extra_args=vsc_min_count_snps=2,vsc_min_fraction_snps=0.12 --reads=/input/BAM --output_vcf=/output/OUTPUT_VCF --output_gvcf=/output/OUTPUT_GVCF --num_shards=$(nproc) --logging_dir=/output/logs) 2>&1 | tee -a generallog.log. ```. and the output tells me . ```. ValueError: NOT_FOUND: Could not open /input/BAM. [E::hts_open_format] Failed to open file ""/input/BAM"" : No such file or directory. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/Adineta_vaga.fsa --reads /input/BAM --examples /tmp/tmpbkwxdhbf/make_examples.tfrecord@48.gz --add_hp_channel --alt_aligned_pileup diff_channels --gvcf /tmp/tmpbkwxdhbf/gvcf.tfrecord@48.gz --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_count_snps 2 --vsc_min_fraction_indels 0.12 --vsc_min_fraction_snps 0.12 --task 15. ```. the bam file is there, 100% sure, at least to my eyes. But it seems docker fails to see it. Any idea? I have the full log available if needed. Again, I am sorry because it looks like some easy stuff, but my colleague and I can't find it. . Thanks a lot.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685
https://github.com/google/deepvariant/issues/685:221,availability,error,error,221,"Works in dry run, not in functional mode; Hello, . I am very sorry because this seems like a kind of s*** I could figure out but I have spent the afternoon to no avail. So, the following command WORKS in dry run mode, no error, but fails when in real operative mode. . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""1.5.0"". OUTPUT_VCF=vaga_lab_hifi_standing_variation.vcf.gz. OUTPUT_GVCF=vaga_lab_hifi_standing_variation.vcf.gz. BAM=HiFi_vaga.sorted.bam. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref=/input/Adineta_vaga.fsa --make_examples_extra_args=vsc_min_count_snps=2,vsc_min_fraction_snps=0.12 --reads=/input/BAM --output_vcf=/output/OUTPUT_VCF --output_gvcf=/output/OUTPUT_GVCF --num_shards=$(nproc) --logging_dir=/output/logs) 2>&1 | tee -a generallog.log. ```. and the output tells me . ```. ValueError: NOT_FOUND: Could not open /input/BAM. [E::hts_open_format] Failed to open file ""/input/BAM"" : No such file or directory. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/Adineta_vaga.fsa --reads /input/BAM --examples /tmp/tmpbkwxdhbf/make_examples.tfrecord@48.gz --add_hp_channel --alt_aligned_pileup diff_channels --gvcf /tmp/tmpbkwxdhbf/gvcf.tfrecord@48.gz --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_count_snps 2 --vsc_min_fraction_indels 0.12 --vsc_min_fraction_snps 0.12 --task 15. ```. the bam file is there, 100% sure, at least to my eyes. But it seems docker fails to see it. Any idea? I have the full log available if needed. Again, I am sorry because it looks like some easy stuff, but my colleague and I can't find it. . Thanks a lot.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685
https://github.com/google/deepvariant/issues/685:251,availability,operat,operative,251,"Works in dry run, not in functional mode; Hello, . I am very sorry because this seems like a kind of s*** I could figure out but I have spent the afternoon to no avail. So, the following command WORKS in dry run mode, no error, but fails when in real operative mode. . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""1.5.0"". OUTPUT_VCF=vaga_lab_hifi_standing_variation.vcf.gz. OUTPUT_GVCF=vaga_lab_hifi_standing_variation.vcf.gz. BAM=HiFi_vaga.sorted.bam. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref=/input/Adineta_vaga.fsa --make_examples_extra_args=vsc_min_count_snps=2,vsc_min_fraction_snps=0.12 --reads=/input/BAM --output_vcf=/output/OUTPUT_VCF --output_gvcf=/output/OUTPUT_GVCF --num_shards=$(nproc) --logging_dir=/output/logs) 2>&1 | tee -a generallog.log. ```. and the output tells me . ```. ValueError: NOT_FOUND: Could not open /input/BAM. [E::hts_open_format] Failed to open file ""/input/BAM"" : No such file or directory. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/Adineta_vaga.fsa --reads /input/BAM --examples /tmp/tmpbkwxdhbf/make_examples.tfrecord@48.gz --add_hp_channel --alt_aligned_pileup diff_channels --gvcf /tmp/tmpbkwxdhbf/gvcf.tfrecord@48.gz --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_count_snps 2 --vsc_min_fraction_indels 0.12 --vsc_min_fraction_snps 0.12 --task 15. ```. the bam file is there, 100% sure, at least to my eyes. But it seems docker fails to see it. Any idea? I have the full log available if needed. Again, I am sorry because it looks like some easy stuff, but my colleague and I can't find it. . Thanks a lot.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685
https://github.com/google/deepvariant/issues/685:1783,availability,avail,available,1783,"Works in dry run, not in functional mode; Hello, . I am very sorry because this seems like a kind of s*** I could figure out but I have spent the afternoon to no avail. So, the following command WORKS in dry run mode, no error, but fails when in real operative mode. . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""1.5.0"". OUTPUT_VCF=vaga_lab_hifi_standing_variation.vcf.gz. OUTPUT_GVCF=vaga_lab_hifi_standing_variation.vcf.gz. BAM=HiFi_vaga.sorted.bam. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref=/input/Adineta_vaga.fsa --make_examples_extra_args=vsc_min_count_snps=2,vsc_min_fraction_snps=0.12 --reads=/input/BAM --output_vcf=/output/OUTPUT_VCF --output_gvcf=/output/OUTPUT_GVCF --num_shards=$(nproc) --logging_dir=/output/logs) 2>&1 | tee -a generallog.log. ```. and the output tells me . ```. ValueError: NOT_FOUND: Could not open /input/BAM. [E::hts_open_format] Failed to open file ""/input/BAM"" : No such file or directory. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/Adineta_vaga.fsa --reads /input/BAM --examples /tmp/tmpbkwxdhbf/make_examples.tfrecord@48.gz --add_hp_channel --alt_aligned_pileup diff_channels --gvcf /tmp/tmpbkwxdhbf/gvcf.tfrecord@48.gz --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_count_snps 2 --vsc_min_fraction_indels 0.12 --vsc_min_fraction_snps 0.12 --task 15. ```. the bam file is there, 100% sure, at least to my eyes. But it seems docker fails to see it. Any idea? I have the full log available if needed. Again, I am sorry because it looks like some easy stuff, but my colleague and I can't find it. . Thanks a lot.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685
https://github.com/google/deepvariant/issues/685:232,deployability,fail,fails,232,"Works in dry run, not in functional mode; Hello, . I am very sorry because this seems like a kind of s*** I could figure out but I have spent the afternoon to no avail. So, the following command WORKS in dry run mode, no error, but fails when in real operative mode. . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""1.5.0"". OUTPUT_VCF=vaga_lab_hifi_standing_variation.vcf.gz. OUTPUT_GVCF=vaga_lab_hifi_standing_variation.vcf.gz. BAM=HiFi_vaga.sorted.bam. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref=/input/Adineta_vaga.fsa --make_examples_extra_args=vsc_min_count_snps=2,vsc_min_fraction_snps=0.12 --reads=/input/BAM --output_vcf=/output/OUTPUT_VCF --output_gvcf=/output/OUTPUT_GVCF --num_shards=$(nproc) --logging_dir=/output/logs) 2>&1 | tee -a generallog.log. ```. and the output tells me . ```. ValueError: NOT_FOUND: Could not open /input/BAM. [E::hts_open_format] Failed to open file ""/input/BAM"" : No such file or directory. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/Adineta_vaga.fsa --reads /input/BAM --examples /tmp/tmpbkwxdhbf/make_examples.tfrecord@48.gz --add_hp_channel --alt_aligned_pileup diff_channels --gvcf /tmp/tmpbkwxdhbf/gvcf.tfrecord@48.gz --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_count_snps 2 --vsc_min_fraction_indels 0.12 --vsc_min_fraction_snps 0.12 --task 15. ```. the bam file is there, 100% sure, at least to my eyes. But it seems docker fails to see it. Any idea? I have the full log available if needed. Again, I am sorry because it looks like some easy stuff, but my colleague and I can't find it. . Thanks a lot.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685
https://github.com/google/deepvariant/issues/685:881,deployability,log,logs,881,"Works in dry run, not in functional mode; Hello, . I am very sorry because this seems like a kind of s*** I could figure out but I have spent the afternoon to no avail. So, the following command WORKS in dry run mode, no error, but fails when in real operative mode. . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""1.5.0"". OUTPUT_VCF=vaga_lab_hifi_standing_variation.vcf.gz. OUTPUT_GVCF=vaga_lab_hifi_standing_variation.vcf.gz. BAM=HiFi_vaga.sorted.bam. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref=/input/Adineta_vaga.fsa --make_examples_extra_args=vsc_min_count_snps=2,vsc_min_fraction_snps=0.12 --reads=/input/BAM --output_vcf=/output/OUTPUT_VCF --output_gvcf=/output/OUTPUT_GVCF --num_shards=$(nproc) --logging_dir=/output/logs) 2>&1 | tee -a generallog.log. ```. and the output tells me . ```. ValueError: NOT_FOUND: Could not open /input/BAM. [E::hts_open_format] Failed to open file ""/input/BAM"" : No such file or directory. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/Adineta_vaga.fsa --reads /input/BAM --examples /tmp/tmpbkwxdhbf/make_examples.tfrecord@48.gz --add_hp_channel --alt_aligned_pileup diff_channels --gvcf /tmp/tmpbkwxdhbf/gvcf.tfrecord@48.gz --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_count_snps 2 --vsc_min_fraction_indels 0.12 --vsc_min_fraction_snps 0.12 --task 15. ```. the bam file is there, 100% sure, at least to my eyes. But it seems docker fails to see it. Any idea? I have the full log available if needed. Again, I am sorry because it looks like some easy stuff, but my colleague and I can't find it. . Thanks a lot.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685
https://github.com/google/deepvariant/issues/685:912,deployability,log,log,912,"Works in dry run, not in functional mode; Hello, . I am very sorry because this seems like a kind of s*** I could figure out but I have spent the afternoon to no avail. So, the following command WORKS in dry run mode, no error, but fails when in real operative mode. . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""1.5.0"". OUTPUT_VCF=vaga_lab_hifi_standing_variation.vcf.gz. OUTPUT_GVCF=vaga_lab_hifi_standing_variation.vcf.gz. BAM=HiFi_vaga.sorted.bam. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref=/input/Adineta_vaga.fsa --make_examples_extra_args=vsc_min_count_snps=2,vsc_min_fraction_snps=0.12 --reads=/input/BAM --output_vcf=/output/OUTPUT_VCF --output_gvcf=/output/OUTPUT_GVCF --num_shards=$(nproc) --logging_dir=/output/logs) 2>&1 | tee -a generallog.log. ```. and the output tells me . ```. ValueError: NOT_FOUND: Could not open /input/BAM. [E::hts_open_format] Failed to open file ""/input/BAM"" : No such file or directory. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/Adineta_vaga.fsa --reads /input/BAM --examples /tmp/tmpbkwxdhbf/make_examples.tfrecord@48.gz --add_hp_channel --alt_aligned_pileup diff_channels --gvcf /tmp/tmpbkwxdhbf/gvcf.tfrecord@48.gz --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_count_snps 2 --vsc_min_fraction_indels 0.12 --vsc_min_fraction_snps 0.12 --task 15. ```. the bam file is there, 100% sure, at least to my eyes. But it seems docker fails to see it. Any idea? I have the full log available if needed. Again, I am sorry because it looks like some easy stuff, but my colleague and I can't find it. . Thanks a lot.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685
https://github.com/google/deepvariant/issues/685:1024,deployability,Fail,Failed,1024,"Works in dry run, not in functional mode; Hello, . I am very sorry because this seems like a kind of s*** I could figure out but I have spent the afternoon to no avail. So, the following command WORKS in dry run mode, no error, but fails when in real operative mode. . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""1.5.0"". OUTPUT_VCF=vaga_lab_hifi_standing_variation.vcf.gz. OUTPUT_GVCF=vaga_lab_hifi_standing_variation.vcf.gz. BAM=HiFi_vaga.sorted.bam. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref=/input/Adineta_vaga.fsa --make_examples_extra_args=vsc_min_count_snps=2,vsc_min_fraction_snps=0.12 --reads=/input/BAM --output_vcf=/output/OUTPUT_VCF --output_gvcf=/output/OUTPUT_GVCF --num_shards=$(nproc) --logging_dir=/output/logs) 2>&1 | tee -a generallog.log. ```. and the output tells me . ```. ValueError: NOT_FOUND: Could not open /input/BAM. [E::hts_open_format] Failed to open file ""/input/BAM"" : No such file or directory. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/Adineta_vaga.fsa --reads /input/BAM --examples /tmp/tmpbkwxdhbf/make_examples.tfrecord@48.gz --add_hp_channel --alt_aligned_pileup diff_channels --gvcf /tmp/tmpbkwxdhbf/gvcf.tfrecord@48.gz --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_count_snps 2 --vsc_min_fraction_indels 0.12 --vsc_min_fraction_snps 0.12 --task 15. ```. the bam file is there, 100% sure, at least to my eyes. But it seems docker fails to see it. Any idea? I have the full log available if needed. Again, I am sorry because it looks like some easy stuff, but my colleague and I can't find it. . Thanks a lot.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685
https://github.com/google/deepvariant/issues/685:1105,deployability,fail,failed,1105,"Works in dry run, not in functional mode; Hello, . I am very sorry because this seems like a kind of s*** I could figure out but I have spent the afternoon to no avail. So, the following command WORKS in dry run mode, no error, but fails when in real operative mode. . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""1.5.0"". OUTPUT_VCF=vaga_lab_hifi_standing_variation.vcf.gz. OUTPUT_GVCF=vaga_lab_hifi_standing_variation.vcf.gz. BAM=HiFi_vaga.sorted.bam. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref=/input/Adineta_vaga.fsa --make_examples_extra_args=vsc_min_count_snps=2,vsc_min_fraction_snps=0.12 --reads=/input/BAM --output_vcf=/output/OUTPUT_VCF --output_gvcf=/output/OUTPUT_GVCF --num_shards=$(nproc) --logging_dir=/output/logs) 2>&1 | tee -a generallog.log. ```. and the output tells me . ```. ValueError: NOT_FOUND: Could not open /input/BAM. [E::hts_open_format] Failed to open file ""/input/BAM"" : No such file or directory. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/Adineta_vaga.fsa --reads /input/BAM --examples /tmp/tmpbkwxdhbf/make_examples.tfrecord@48.gz --add_hp_channel --alt_aligned_pileup diff_channels --gvcf /tmp/tmpbkwxdhbf/gvcf.tfrecord@48.gz --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_count_snps 2 --vsc_min_fraction_indels 0.12 --vsc_min_fraction_snps 0.12 --task 15. ```. the bam file is there, 100% sure, at least to my eyes. But it seems docker fails to see it. Any idea? I have the full log available if needed. Again, I am sorry because it looks like some easy stuff, but my colleague and I can't find it. . Thanks a lot.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685
https://github.com/google/deepvariant/issues/685:1736,deployability,fail,fails,1736,"Works in dry run, not in functional mode; Hello, . I am very sorry because this seems like a kind of s*** I could figure out but I have spent the afternoon to no avail. So, the following command WORKS in dry run mode, no error, but fails when in real operative mode. . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""1.5.0"". OUTPUT_VCF=vaga_lab_hifi_standing_variation.vcf.gz. OUTPUT_GVCF=vaga_lab_hifi_standing_variation.vcf.gz. BAM=HiFi_vaga.sorted.bam. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref=/input/Adineta_vaga.fsa --make_examples_extra_args=vsc_min_count_snps=2,vsc_min_fraction_snps=0.12 --reads=/input/BAM --output_vcf=/output/OUTPUT_VCF --output_gvcf=/output/OUTPUT_GVCF --num_shards=$(nproc) --logging_dir=/output/logs) 2>&1 | tee -a generallog.log. ```. and the output tells me . ```. ValueError: NOT_FOUND: Could not open /input/BAM. [E::hts_open_format] Failed to open file ""/input/BAM"" : No such file or directory. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/Adineta_vaga.fsa --reads /input/BAM --examples /tmp/tmpbkwxdhbf/make_examples.tfrecord@48.gz --add_hp_channel --alt_aligned_pileup diff_channels --gvcf /tmp/tmpbkwxdhbf/gvcf.tfrecord@48.gz --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_count_snps 2 --vsc_min_fraction_indels 0.12 --vsc_min_fraction_snps 0.12 --task 15. ```. the bam file is there, 100% sure, at least to my eyes. But it seems docker fails to see it. Any idea? I have the full log available if needed. Again, I am sorry because it looks like some easy stuff, but my colleague and I can't find it. . Thanks a lot.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685
https://github.com/google/deepvariant/issues/685:1779,deployability,log,log,1779,"Works in dry run, not in functional mode; Hello, . I am very sorry because this seems like a kind of s*** I could figure out but I have spent the afternoon to no avail. So, the following command WORKS in dry run mode, no error, but fails when in real operative mode. . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""1.5.0"". OUTPUT_VCF=vaga_lab_hifi_standing_variation.vcf.gz. OUTPUT_GVCF=vaga_lab_hifi_standing_variation.vcf.gz. BAM=HiFi_vaga.sorted.bam. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref=/input/Adineta_vaga.fsa --make_examples_extra_args=vsc_min_count_snps=2,vsc_min_fraction_snps=0.12 --reads=/input/BAM --output_vcf=/output/OUTPUT_VCF --output_gvcf=/output/OUTPUT_GVCF --num_shards=$(nproc) --logging_dir=/output/logs) 2>&1 | tee -a generallog.log. ```. and the output tells me . ```. ValueError: NOT_FOUND: Could not open /input/BAM. [E::hts_open_format] Failed to open file ""/input/BAM"" : No such file or directory. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/Adineta_vaga.fsa --reads /input/BAM --examples /tmp/tmpbkwxdhbf/make_examples.tfrecord@48.gz --add_hp_channel --alt_aligned_pileup diff_channels --gvcf /tmp/tmpbkwxdhbf/gvcf.tfrecord@48.gz --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_count_snps 2 --vsc_min_fraction_indels 0.12 --vsc_min_fraction_snps 0.12 --task 15. ```. the bam file is there, 100% sure, at least to my eyes. But it seems docker fails to see it. Any idea? I have the full log available if needed. Again, I am sorry because it looks like some easy stuff, but my colleague and I can't find it. . Thanks a lot.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685
https://github.com/google/deepvariant/issues/685:640,modifiability,PAC,PACBIO,640,"Works in dry run, not in functional mode; Hello, . I am very sorry because this seems like a kind of s*** I could figure out but I have spent the afternoon to no avail. So, the following command WORKS in dry run mode, no error, but fails when in real operative mode. . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""1.5.0"". OUTPUT_VCF=vaga_lab_hifi_standing_variation.vcf.gz. OUTPUT_GVCF=vaga_lab_hifi_standing_variation.vcf.gz. BAM=HiFi_vaga.sorted.bam. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref=/input/Adineta_vaga.fsa --make_examples_extra_args=vsc_min_count_snps=2,vsc_min_fraction_snps=0.12 --reads=/input/BAM --output_vcf=/output/OUTPUT_VCF --output_gvcf=/output/OUTPUT_GVCF --num_shards=$(nproc) --logging_dir=/output/logs) 2>&1 | tee -a generallog.log. ```. and the output tells me . ```. ValueError: NOT_FOUND: Could not open /input/BAM. [E::hts_open_format] Failed to open file ""/input/BAM"" : No such file or directory. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/Adineta_vaga.fsa --reads /input/BAM --examples /tmp/tmpbkwxdhbf/make_examples.tfrecord@48.gz --add_hp_channel --alt_aligned_pileup diff_channels --gvcf /tmp/tmpbkwxdhbf/gvcf.tfrecord@48.gz --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_count_snps 2 --vsc_min_fraction_indels 0.12 --vsc_min_fraction_snps 0.12 --task 15. ```. the bam file is there, 100% sure, at least to my eyes. But it seems docker fails to see it. Any idea? I have the full log available if needed. Again, I am sorry because it looks like some easy stuff, but my colleague and I can't find it. . Thanks a lot.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685
https://github.com/google/deepvariant/issues/685:221,performance,error,error,221,"Works in dry run, not in functional mode; Hello, . I am very sorry because this seems like a kind of s*** I could figure out but I have spent the afternoon to no avail. So, the following command WORKS in dry run mode, no error, but fails when in real operative mode. . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""1.5.0"". OUTPUT_VCF=vaga_lab_hifi_standing_variation.vcf.gz. OUTPUT_GVCF=vaga_lab_hifi_standing_variation.vcf.gz. BAM=HiFi_vaga.sorted.bam. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref=/input/Adineta_vaga.fsa --make_examples_extra_args=vsc_min_count_snps=2,vsc_min_fraction_snps=0.12 --reads=/input/BAM --output_vcf=/output/OUTPUT_VCF --output_gvcf=/output/OUTPUT_GVCF --num_shards=$(nproc) --logging_dir=/output/logs) 2>&1 | tee -a generallog.log. ```. and the output tells me . ```. ValueError: NOT_FOUND: Could not open /input/BAM. [E::hts_open_format] Failed to open file ""/input/BAM"" : No such file or directory. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/Adineta_vaga.fsa --reads /input/BAM --examples /tmp/tmpbkwxdhbf/make_examples.tfrecord@48.gz --add_hp_channel --alt_aligned_pileup diff_channels --gvcf /tmp/tmpbkwxdhbf/gvcf.tfrecord@48.gz --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_count_snps 2 --vsc_min_fraction_indels 0.12 --vsc_min_fraction_snps 0.12 --task 15. ```. the bam file is there, 100% sure, at least to my eyes. But it seems docker fails to see it. Any idea? I have the full log available if needed. Again, I am sorry because it looks like some easy stuff, but my colleague and I can't find it. . Thanks a lot.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685
https://github.com/google/deepvariant/issues/685:483,performance,time,time,483,"Works in dry run, not in functional mode; Hello, . I am very sorry because this seems like a kind of s*** I could figure out but I have spent the afternoon to no avail. So, the following command WORKS in dry run mode, no error, but fails when in real operative mode. . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""1.5.0"". OUTPUT_VCF=vaga_lab_hifi_standing_variation.vcf.gz. OUTPUT_GVCF=vaga_lab_hifi_standing_variation.vcf.gz. BAM=HiFi_vaga.sorted.bam. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref=/input/Adineta_vaga.fsa --make_examples_extra_args=vsc_min_count_snps=2,vsc_min_fraction_snps=0.12 --reads=/input/BAM --output_vcf=/output/OUTPUT_VCF --output_gvcf=/output/OUTPUT_GVCF --num_shards=$(nproc) --logging_dir=/output/logs) 2>&1 | tee -a generallog.log. ```. and the output tells me . ```. ValueError: NOT_FOUND: Could not open /input/BAM. [E::hts_open_format] Failed to open file ""/input/BAM"" : No such file or directory. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/Adineta_vaga.fsa --reads /input/BAM --examples /tmp/tmpbkwxdhbf/make_examples.tfrecord@48.gz --add_hp_channel --alt_aligned_pileup diff_channels --gvcf /tmp/tmpbkwxdhbf/gvcf.tfrecord@48.gz --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_count_snps 2 --vsc_min_fraction_indels 0.12 --vsc_min_fraction_snps 0.12 --task 15. ```. the bam file is there, 100% sure, at least to my eyes. But it seems docker fails to see it. Any idea? I have the full log available if needed. Again, I am sorry because it looks like some easy stuff, but my colleague and I can't find it. . Thanks a lot.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685
https://github.com/google/deepvariant/issues/685:1086,performance,parallel,parallel,1086,"Works in dry run, not in functional mode; Hello, . I am very sorry because this seems like a kind of s*** I could figure out but I have spent the afternoon to no avail. So, the following command WORKS in dry run mode, no error, but fails when in real operative mode. . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""1.5.0"". OUTPUT_VCF=vaga_lab_hifi_standing_variation.vcf.gz. OUTPUT_GVCF=vaga_lab_hifi_standing_variation.vcf.gz. BAM=HiFi_vaga.sorted.bam. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref=/input/Adineta_vaga.fsa --make_examples_extra_args=vsc_min_count_snps=2,vsc_min_fraction_snps=0.12 --reads=/input/BAM --output_vcf=/output/OUTPUT_VCF --output_gvcf=/output/OUTPUT_GVCF --num_shards=$(nproc) --logging_dir=/output/logs) 2>&1 | tee -a generallog.log. ```. and the output tells me . ```. ValueError: NOT_FOUND: Could not open /input/BAM. [E::hts_open_format] Failed to open file ""/input/BAM"" : No such file or directory. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/Adineta_vaga.fsa --reads /input/BAM --examples /tmp/tmpbkwxdhbf/make_examples.tfrecord@48.gz --add_hp_channel --alt_aligned_pileup diff_channels --gvcf /tmp/tmpbkwxdhbf/gvcf.tfrecord@48.gz --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_count_snps 2 --vsc_min_fraction_indels 0.12 --vsc_min_fraction_snps 0.12 --task 15. ```. the bam file is there, 100% sure, at least to my eyes. But it seems docker fails to see it. Any idea? I have the full log available if needed. Again, I am sorry because it looks like some easy stuff, but my colleague and I can't find it. . Thanks a lot.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685
https://github.com/google/deepvariant/issues/685:232,reliability,fail,fails,232,"Works in dry run, not in functional mode; Hello, . I am very sorry because this seems like a kind of s*** I could figure out but I have spent the afternoon to no avail. So, the following command WORKS in dry run mode, no error, but fails when in real operative mode. . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""1.5.0"". OUTPUT_VCF=vaga_lab_hifi_standing_variation.vcf.gz. OUTPUT_GVCF=vaga_lab_hifi_standing_variation.vcf.gz. BAM=HiFi_vaga.sorted.bam. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref=/input/Adineta_vaga.fsa --make_examples_extra_args=vsc_min_count_snps=2,vsc_min_fraction_snps=0.12 --reads=/input/BAM --output_vcf=/output/OUTPUT_VCF --output_gvcf=/output/OUTPUT_GVCF --num_shards=$(nproc) --logging_dir=/output/logs) 2>&1 | tee -a generallog.log. ```. and the output tells me . ```. ValueError: NOT_FOUND: Could not open /input/BAM. [E::hts_open_format] Failed to open file ""/input/BAM"" : No such file or directory. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/Adineta_vaga.fsa --reads /input/BAM --examples /tmp/tmpbkwxdhbf/make_examples.tfrecord@48.gz --add_hp_channel --alt_aligned_pileup diff_channels --gvcf /tmp/tmpbkwxdhbf/gvcf.tfrecord@48.gz --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_count_snps 2 --vsc_min_fraction_indels 0.12 --vsc_min_fraction_snps 0.12 --task 15. ```. the bam file is there, 100% sure, at least to my eyes. But it seems docker fails to see it. Any idea? I have the full log available if needed. Again, I am sorry because it looks like some easy stuff, but my colleague and I can't find it. . Thanks a lot.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685
https://github.com/google/deepvariant/issues/685:1024,reliability,Fail,Failed,1024,"Works in dry run, not in functional mode; Hello, . I am very sorry because this seems like a kind of s*** I could figure out but I have spent the afternoon to no avail. So, the following command WORKS in dry run mode, no error, but fails when in real operative mode. . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""1.5.0"". OUTPUT_VCF=vaga_lab_hifi_standing_variation.vcf.gz. OUTPUT_GVCF=vaga_lab_hifi_standing_variation.vcf.gz. BAM=HiFi_vaga.sorted.bam. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref=/input/Adineta_vaga.fsa --make_examples_extra_args=vsc_min_count_snps=2,vsc_min_fraction_snps=0.12 --reads=/input/BAM --output_vcf=/output/OUTPUT_VCF --output_gvcf=/output/OUTPUT_GVCF --num_shards=$(nproc) --logging_dir=/output/logs) 2>&1 | tee -a generallog.log. ```. and the output tells me . ```. ValueError: NOT_FOUND: Could not open /input/BAM. [E::hts_open_format] Failed to open file ""/input/BAM"" : No such file or directory. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/Adineta_vaga.fsa --reads /input/BAM --examples /tmp/tmpbkwxdhbf/make_examples.tfrecord@48.gz --add_hp_channel --alt_aligned_pileup diff_channels --gvcf /tmp/tmpbkwxdhbf/gvcf.tfrecord@48.gz --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_count_snps 2 --vsc_min_fraction_indels 0.12 --vsc_min_fraction_snps 0.12 --task 15. ```. the bam file is there, 100% sure, at least to my eyes. But it seems docker fails to see it. Any idea? I have the full log available if needed. Again, I am sorry because it looks like some easy stuff, but my colleague and I can't find it. . Thanks a lot.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685
https://github.com/google/deepvariant/issues/685:1105,reliability,fail,failed,1105,"Works in dry run, not in functional mode; Hello, . I am very sorry because this seems like a kind of s*** I could figure out but I have spent the afternoon to no avail. So, the following command WORKS in dry run mode, no error, but fails when in real operative mode. . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""1.5.0"". OUTPUT_VCF=vaga_lab_hifi_standing_variation.vcf.gz. OUTPUT_GVCF=vaga_lab_hifi_standing_variation.vcf.gz. BAM=HiFi_vaga.sorted.bam. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref=/input/Adineta_vaga.fsa --make_examples_extra_args=vsc_min_count_snps=2,vsc_min_fraction_snps=0.12 --reads=/input/BAM --output_vcf=/output/OUTPUT_VCF --output_gvcf=/output/OUTPUT_GVCF --num_shards=$(nproc) --logging_dir=/output/logs) 2>&1 | tee -a generallog.log. ```. and the output tells me . ```. ValueError: NOT_FOUND: Could not open /input/BAM. [E::hts_open_format] Failed to open file ""/input/BAM"" : No such file or directory. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/Adineta_vaga.fsa --reads /input/BAM --examples /tmp/tmpbkwxdhbf/make_examples.tfrecord@48.gz --add_hp_channel --alt_aligned_pileup diff_channels --gvcf /tmp/tmpbkwxdhbf/gvcf.tfrecord@48.gz --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_count_snps 2 --vsc_min_fraction_indels 0.12 --vsc_min_fraction_snps 0.12 --task 15. ```. the bam file is there, 100% sure, at least to my eyes. But it seems docker fails to see it. Any idea? I have the full log available if needed. Again, I am sorry because it looks like some easy stuff, but my colleague and I can't find it. . Thanks a lot.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685
https://github.com/google/deepvariant/issues/685:1736,reliability,fail,fails,1736,"Works in dry run, not in functional mode; Hello, . I am very sorry because this seems like a kind of s*** I could figure out but I have spent the afternoon to no avail. So, the following command WORKS in dry run mode, no error, but fails when in real operative mode. . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""1.5.0"". OUTPUT_VCF=vaga_lab_hifi_standing_variation.vcf.gz. OUTPUT_GVCF=vaga_lab_hifi_standing_variation.vcf.gz. BAM=HiFi_vaga.sorted.bam. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref=/input/Adineta_vaga.fsa --make_examples_extra_args=vsc_min_count_snps=2,vsc_min_fraction_snps=0.12 --reads=/input/BAM --output_vcf=/output/OUTPUT_VCF --output_gvcf=/output/OUTPUT_GVCF --num_shards=$(nproc) --logging_dir=/output/logs) 2>&1 | tee -a generallog.log. ```. and the output tells me . ```. ValueError: NOT_FOUND: Could not open /input/BAM. [E::hts_open_format] Failed to open file ""/input/BAM"" : No such file or directory. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/Adineta_vaga.fsa --reads /input/BAM --examples /tmp/tmpbkwxdhbf/make_examples.tfrecord@48.gz --add_hp_channel --alt_aligned_pileup diff_channels --gvcf /tmp/tmpbkwxdhbf/gvcf.tfrecord@48.gz --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_count_snps 2 --vsc_min_fraction_indels 0.12 --vsc_min_fraction_snps 0.12 --task 15. ```. the bam file is there, 100% sure, at least to my eyes. But it seems docker fails to see it. Any idea? I have the full log available if needed. Again, I am sorry because it looks like some easy stuff, but my colleague and I can't find it. . Thanks a lot.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685
https://github.com/google/deepvariant/issues/685:1783,reliability,availab,available,1783,"Works in dry run, not in functional mode; Hello, . I am very sorry because this seems like a kind of s*** I could figure out but I have spent the afternoon to no avail. So, the following command WORKS in dry run mode, no error, but fails when in real operative mode. . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""1.5.0"". OUTPUT_VCF=vaga_lab_hifi_standing_variation.vcf.gz. OUTPUT_GVCF=vaga_lab_hifi_standing_variation.vcf.gz. BAM=HiFi_vaga.sorted.bam. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref=/input/Adineta_vaga.fsa --make_examples_extra_args=vsc_min_count_snps=2,vsc_min_fraction_snps=0.12 --reads=/input/BAM --output_vcf=/output/OUTPUT_VCF --output_gvcf=/output/OUTPUT_GVCF --num_shards=$(nproc) --logging_dir=/output/logs) 2>&1 | tee -a generallog.log. ```. and the output tells me . ```. ValueError: NOT_FOUND: Could not open /input/BAM. [E::hts_open_format] Failed to open file ""/input/BAM"" : No such file or directory. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/Adineta_vaga.fsa --reads /input/BAM --examples /tmp/tmpbkwxdhbf/make_examples.tfrecord@48.gz --add_hp_channel --alt_aligned_pileup diff_channels --gvcf /tmp/tmpbkwxdhbf/gvcf.tfrecord@48.gz --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_count_snps 2 --vsc_min_fraction_indels 0.12 --vsc_min_fraction_snps 0.12 --task 15. ```. the bam file is there, 100% sure, at least to my eyes. But it seems docker fails to see it. Any idea? I have the full log available if needed. Again, I am sorry because it looks like some easy stuff, but my colleague and I can't find it. . Thanks a lot.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685
https://github.com/google/deepvariant/issues/685:162,safety,avail,avail,162,"Works in dry run, not in functional mode; Hello, . I am very sorry because this seems like a kind of s*** I could figure out but I have spent the afternoon to no avail. So, the following command WORKS in dry run mode, no error, but fails when in real operative mode. . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""1.5.0"". OUTPUT_VCF=vaga_lab_hifi_standing_variation.vcf.gz. OUTPUT_GVCF=vaga_lab_hifi_standing_variation.vcf.gz. BAM=HiFi_vaga.sorted.bam. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref=/input/Adineta_vaga.fsa --make_examples_extra_args=vsc_min_count_snps=2,vsc_min_fraction_snps=0.12 --reads=/input/BAM --output_vcf=/output/OUTPUT_VCF --output_gvcf=/output/OUTPUT_GVCF --num_shards=$(nproc) --logging_dir=/output/logs) 2>&1 | tee -a generallog.log. ```. and the output tells me . ```. ValueError: NOT_FOUND: Could not open /input/BAM. [E::hts_open_format] Failed to open file ""/input/BAM"" : No such file or directory. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/Adineta_vaga.fsa --reads /input/BAM --examples /tmp/tmpbkwxdhbf/make_examples.tfrecord@48.gz --add_hp_channel --alt_aligned_pileup diff_channels --gvcf /tmp/tmpbkwxdhbf/gvcf.tfrecord@48.gz --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_count_snps 2 --vsc_min_fraction_indels 0.12 --vsc_min_fraction_snps 0.12 --task 15. ```. the bam file is there, 100% sure, at least to my eyes. But it seems docker fails to see it. Any idea? I have the full log available if needed. Again, I am sorry because it looks like some easy stuff, but my colleague and I can't find it. . Thanks a lot.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685
https://github.com/google/deepvariant/issues/685:221,safety,error,error,221,"Works in dry run, not in functional mode; Hello, . I am very sorry because this seems like a kind of s*** I could figure out but I have spent the afternoon to no avail. So, the following command WORKS in dry run mode, no error, but fails when in real operative mode. . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""1.5.0"". OUTPUT_VCF=vaga_lab_hifi_standing_variation.vcf.gz. OUTPUT_GVCF=vaga_lab_hifi_standing_variation.vcf.gz. BAM=HiFi_vaga.sorted.bam. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref=/input/Adineta_vaga.fsa --make_examples_extra_args=vsc_min_count_snps=2,vsc_min_fraction_snps=0.12 --reads=/input/BAM --output_vcf=/output/OUTPUT_VCF --output_gvcf=/output/OUTPUT_GVCF --num_shards=$(nproc) --logging_dir=/output/logs) 2>&1 | tee -a generallog.log. ```. and the output tells me . ```. ValueError: NOT_FOUND: Could not open /input/BAM. [E::hts_open_format] Failed to open file ""/input/BAM"" : No such file or directory. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/Adineta_vaga.fsa --reads /input/BAM --examples /tmp/tmpbkwxdhbf/make_examples.tfrecord@48.gz --add_hp_channel --alt_aligned_pileup diff_channels --gvcf /tmp/tmpbkwxdhbf/gvcf.tfrecord@48.gz --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_count_snps 2 --vsc_min_fraction_indels 0.12 --vsc_min_fraction_snps 0.12 --task 15. ```. the bam file is there, 100% sure, at least to my eyes. But it seems docker fails to see it. Any idea? I have the full log available if needed. Again, I am sorry because it looks like some easy stuff, but my colleague and I can't find it. . Thanks a lot.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685
https://github.com/google/deepvariant/issues/685:520,safety,input,input,520,"Works in dry run, not in functional mode; Hello, . I am very sorry because this seems like a kind of s*** I could figure out but I have spent the afternoon to no avail. So, the following command WORKS in dry run mode, no error, but fails when in real operative mode. . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""1.5.0"". OUTPUT_VCF=vaga_lab_hifi_standing_variation.vcf.gz. OUTPUT_GVCF=vaga_lab_hifi_standing_variation.vcf.gz. BAM=HiFi_vaga.sorted.bam. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref=/input/Adineta_vaga.fsa --make_examples_extra_args=vsc_min_count_snps=2,vsc_min_fraction_snps=0.12 --reads=/input/BAM --output_vcf=/output/OUTPUT_VCF --output_gvcf=/output/OUTPUT_GVCF --num_shards=$(nproc) --logging_dir=/output/logs) 2>&1 | tee -a generallog.log. ```. and the output tells me . ```. ValueError: NOT_FOUND: Could not open /input/BAM. [E::hts_open_format] Failed to open file ""/input/BAM"" : No such file or directory. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/Adineta_vaga.fsa --reads /input/BAM --examples /tmp/tmpbkwxdhbf/make_examples.tfrecord@48.gz --add_hp_channel --alt_aligned_pileup diff_channels --gvcf /tmp/tmpbkwxdhbf/gvcf.tfrecord@48.gz --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_count_snps 2 --vsc_min_fraction_indels 0.12 --vsc_min_fraction_snps 0.12 --task 15. ```. the bam file is there, 100% sure, at least to my eyes. But it seems docker fails to see it. Any idea? I have the full log available if needed. Again, I am sorry because it looks like some easy stuff, but my colleague and I can't find it. . Thanks a lot.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685
https://github.com/google/deepvariant/issues/685:654,safety,input,input,654,"Works in dry run, not in functional mode; Hello, . I am very sorry because this seems like a kind of s*** I could figure out but I have spent the afternoon to no avail. So, the following command WORKS in dry run mode, no error, but fails when in real operative mode. . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""1.5.0"". OUTPUT_VCF=vaga_lab_hifi_standing_variation.vcf.gz. OUTPUT_GVCF=vaga_lab_hifi_standing_variation.vcf.gz. BAM=HiFi_vaga.sorted.bam. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref=/input/Adineta_vaga.fsa --make_examples_extra_args=vsc_min_count_snps=2,vsc_min_fraction_snps=0.12 --reads=/input/BAM --output_vcf=/output/OUTPUT_VCF --output_gvcf=/output/OUTPUT_GVCF --num_shards=$(nproc) --logging_dir=/output/logs) 2>&1 | tee -a generallog.log. ```. and the output tells me . ```. ValueError: NOT_FOUND: Could not open /input/BAM. [E::hts_open_format] Failed to open file ""/input/BAM"" : No such file or directory. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/Adineta_vaga.fsa --reads /input/BAM --examples /tmp/tmpbkwxdhbf/make_examples.tfrecord@48.gz --add_hp_channel --alt_aligned_pileup diff_channels --gvcf /tmp/tmpbkwxdhbf/gvcf.tfrecord@48.gz --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_count_snps 2 --vsc_min_fraction_indels 0.12 --vsc_min_fraction_snps 0.12 --task 15. ```. the bam file is there, 100% sure, at least to my eyes. But it seems docker fails to see it. Any idea? I have the full log available if needed. Again, I am sorry because it looks like some easy stuff, but my colleague and I can't find it. . Thanks a lot.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685
https://github.com/google/deepvariant/issues/685:761,safety,input,input,761,"Works in dry run, not in functional mode; Hello, . I am very sorry because this seems like a kind of s*** I could figure out but I have spent the afternoon to no avail. So, the following command WORKS in dry run mode, no error, but fails when in real operative mode. . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""1.5.0"". OUTPUT_VCF=vaga_lab_hifi_standing_variation.vcf.gz. OUTPUT_GVCF=vaga_lab_hifi_standing_variation.vcf.gz. BAM=HiFi_vaga.sorted.bam. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref=/input/Adineta_vaga.fsa --make_examples_extra_args=vsc_min_count_snps=2,vsc_min_fraction_snps=0.12 --reads=/input/BAM --output_vcf=/output/OUTPUT_VCF --output_gvcf=/output/OUTPUT_GVCF --num_shards=$(nproc) --logging_dir=/output/logs) 2>&1 | tee -a generallog.log. ```. and the output tells me . ```. ValueError: NOT_FOUND: Could not open /input/BAM. [E::hts_open_format] Failed to open file ""/input/BAM"" : No such file or directory. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/Adineta_vaga.fsa --reads /input/BAM --examples /tmp/tmpbkwxdhbf/make_examples.tfrecord@48.gz --add_hp_channel --alt_aligned_pileup diff_channels --gvcf /tmp/tmpbkwxdhbf/gvcf.tfrecord@48.gz --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_count_snps 2 --vsc_min_fraction_indels 0.12 --vsc_min_fraction_snps 0.12 --task 15. ```. the bam file is there, 100% sure, at least to my eyes. But it seems docker fails to see it. Any idea? I have the full log available if needed. Again, I am sorry because it looks like some easy stuff, but my colleague and I can't find it. . Thanks a lot.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685
https://github.com/google/deepvariant/issues/685:881,safety,log,logs,881,"Works in dry run, not in functional mode; Hello, . I am very sorry because this seems like a kind of s*** I could figure out but I have spent the afternoon to no avail. So, the following command WORKS in dry run mode, no error, but fails when in real operative mode. . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""1.5.0"". OUTPUT_VCF=vaga_lab_hifi_standing_variation.vcf.gz. OUTPUT_GVCF=vaga_lab_hifi_standing_variation.vcf.gz. BAM=HiFi_vaga.sorted.bam. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref=/input/Adineta_vaga.fsa --make_examples_extra_args=vsc_min_count_snps=2,vsc_min_fraction_snps=0.12 --reads=/input/BAM --output_vcf=/output/OUTPUT_VCF --output_gvcf=/output/OUTPUT_GVCF --num_shards=$(nproc) --logging_dir=/output/logs) 2>&1 | tee -a generallog.log. ```. and the output tells me . ```. ValueError: NOT_FOUND: Could not open /input/BAM. [E::hts_open_format] Failed to open file ""/input/BAM"" : No such file or directory. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/Adineta_vaga.fsa --reads /input/BAM --examples /tmp/tmpbkwxdhbf/make_examples.tfrecord@48.gz --add_hp_channel --alt_aligned_pileup diff_channels --gvcf /tmp/tmpbkwxdhbf/gvcf.tfrecord@48.gz --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_count_snps 2 --vsc_min_fraction_indels 0.12 --vsc_min_fraction_snps 0.12 --task 15. ```. the bam file is there, 100% sure, at least to my eyes. But it seems docker fails to see it. Any idea? I have the full log available if needed. Again, I am sorry because it looks like some easy stuff, but my colleague and I can't find it. . Thanks a lot.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685
https://github.com/google/deepvariant/issues/685:912,safety,log,log,912,"Works in dry run, not in functional mode; Hello, . I am very sorry because this seems like a kind of s*** I could figure out but I have spent the afternoon to no avail. So, the following command WORKS in dry run mode, no error, but fails when in real operative mode. . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""1.5.0"". OUTPUT_VCF=vaga_lab_hifi_standing_variation.vcf.gz. OUTPUT_GVCF=vaga_lab_hifi_standing_variation.vcf.gz. BAM=HiFi_vaga.sorted.bam. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref=/input/Adineta_vaga.fsa --make_examples_extra_args=vsc_min_count_snps=2,vsc_min_fraction_snps=0.12 --reads=/input/BAM --output_vcf=/output/OUTPUT_VCF --output_gvcf=/output/OUTPUT_GVCF --num_shards=$(nproc) --logging_dir=/output/logs) 2>&1 | tee -a generallog.log. ```. and the output tells me . ```. ValueError: NOT_FOUND: Could not open /input/BAM. [E::hts_open_format] Failed to open file ""/input/BAM"" : No such file or directory. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/Adineta_vaga.fsa --reads /input/BAM --examples /tmp/tmpbkwxdhbf/make_examples.tfrecord@48.gz --add_hp_channel --alt_aligned_pileup diff_channels --gvcf /tmp/tmpbkwxdhbf/gvcf.tfrecord@48.gz --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_count_snps 2 --vsc_min_fraction_indels 0.12 --vsc_min_fraction_snps 0.12 --task 15. ```. the bam file is there, 100% sure, at least to my eyes. But it seems docker fails to see it. Any idea? I have the full log available if needed. Again, I am sorry because it looks like some easy stuff, but my colleague and I can't find it. . Thanks a lot.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685
https://github.com/google/deepvariant/issues/685:992,safety,input,input,992,"Works in dry run, not in functional mode; Hello, . I am very sorry because this seems like a kind of s*** I could figure out but I have spent the afternoon to no avail. So, the following command WORKS in dry run mode, no error, but fails when in real operative mode. . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""1.5.0"". OUTPUT_VCF=vaga_lab_hifi_standing_variation.vcf.gz. OUTPUT_GVCF=vaga_lab_hifi_standing_variation.vcf.gz. BAM=HiFi_vaga.sorted.bam. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref=/input/Adineta_vaga.fsa --make_examples_extra_args=vsc_min_count_snps=2,vsc_min_fraction_snps=0.12 --reads=/input/BAM --output_vcf=/output/OUTPUT_VCF --output_gvcf=/output/OUTPUT_GVCF --num_shards=$(nproc) --logging_dir=/output/logs) 2>&1 | tee -a generallog.log. ```. and the output tells me . ```. ValueError: NOT_FOUND: Could not open /input/BAM. [E::hts_open_format] Failed to open file ""/input/BAM"" : No such file or directory. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/Adineta_vaga.fsa --reads /input/BAM --examples /tmp/tmpbkwxdhbf/make_examples.tfrecord@48.gz --add_hp_channel --alt_aligned_pileup diff_channels --gvcf /tmp/tmpbkwxdhbf/gvcf.tfrecord@48.gz --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_count_snps 2 --vsc_min_fraction_indels 0.12 --vsc_min_fraction_snps 0.12 --task 15. ```. the bam file is there, 100% sure, at least to my eyes. But it seems docker fails to see it. Any idea? I have the full log available if needed. Again, I am sorry because it looks like some easy stuff, but my colleague and I can't find it. . Thanks a lot.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685
https://github.com/google/deepvariant/issues/685:1046,safety,input,input,1046,"Works in dry run, not in functional mode; Hello, . I am very sorry because this seems like a kind of s*** I could figure out but I have spent the afternoon to no avail. So, the following command WORKS in dry run mode, no error, but fails when in real operative mode. . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""1.5.0"". OUTPUT_VCF=vaga_lab_hifi_standing_variation.vcf.gz. OUTPUT_GVCF=vaga_lab_hifi_standing_variation.vcf.gz. BAM=HiFi_vaga.sorted.bam. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref=/input/Adineta_vaga.fsa --make_examples_extra_args=vsc_min_count_snps=2,vsc_min_fraction_snps=0.12 --reads=/input/BAM --output_vcf=/output/OUTPUT_VCF --output_gvcf=/output/OUTPUT_GVCF --num_shards=$(nproc) --logging_dir=/output/logs) 2>&1 | tee -a generallog.log. ```. and the output tells me . ```. ValueError: NOT_FOUND: Could not open /input/BAM. [E::hts_open_format] Failed to open file ""/input/BAM"" : No such file or directory. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/Adineta_vaga.fsa --reads /input/BAM --examples /tmp/tmpbkwxdhbf/make_examples.tfrecord@48.gz --add_hp_channel --alt_aligned_pileup diff_channels --gvcf /tmp/tmpbkwxdhbf/gvcf.tfrecord@48.gz --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_count_snps 2 --vsc_min_fraction_indels 0.12 --vsc_min_fraction_snps 0.12 --task 15. ```. the bam file is there, 100% sure, at least to my eyes. But it seems docker fails to see it. Any idea? I have the full log available if needed. Again, I am sorry because it looks like some easy stuff, but my colleague and I can't find it. . Thanks a lot.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685
https://github.com/google/deepvariant/issues/685:1171,safety,input,input,1171,"Works in dry run, not in functional mode; Hello, . I am very sorry because this seems like a kind of s*** I could figure out but I have spent the afternoon to no avail. So, the following command WORKS in dry run mode, no error, but fails when in real operative mode. . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""1.5.0"". OUTPUT_VCF=vaga_lab_hifi_standing_variation.vcf.gz. OUTPUT_GVCF=vaga_lab_hifi_standing_variation.vcf.gz. BAM=HiFi_vaga.sorted.bam. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref=/input/Adineta_vaga.fsa --make_examples_extra_args=vsc_min_count_snps=2,vsc_min_fraction_snps=0.12 --reads=/input/BAM --output_vcf=/output/OUTPUT_VCF --output_gvcf=/output/OUTPUT_GVCF --num_shards=$(nproc) --logging_dir=/output/logs) 2>&1 | tee -a generallog.log. ```. and the output tells me . ```. ValueError: NOT_FOUND: Could not open /input/BAM. [E::hts_open_format] Failed to open file ""/input/BAM"" : No such file or directory. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/Adineta_vaga.fsa --reads /input/BAM --examples /tmp/tmpbkwxdhbf/make_examples.tfrecord@48.gz --add_hp_channel --alt_aligned_pileup diff_channels --gvcf /tmp/tmpbkwxdhbf/gvcf.tfrecord@48.gz --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_count_snps 2 --vsc_min_fraction_indels 0.12 --vsc_min_fraction_snps 0.12 --task 15. ```. the bam file is there, 100% sure, at least to my eyes. But it seems docker fails to see it. Any idea? I have the full log available if needed. Again, I am sorry because it looks like some easy stuff, but my colleague and I can't find it. . Thanks a lot.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685
https://github.com/google/deepvariant/issues/685:1203,safety,input,input,1203,"Works in dry run, not in functional mode; Hello, . I am very sorry because this seems like a kind of s*** I could figure out but I have spent the afternoon to no avail. So, the following command WORKS in dry run mode, no error, but fails when in real operative mode. . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""1.5.0"". OUTPUT_VCF=vaga_lab_hifi_standing_variation.vcf.gz. OUTPUT_GVCF=vaga_lab_hifi_standing_variation.vcf.gz. BAM=HiFi_vaga.sorted.bam. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref=/input/Adineta_vaga.fsa --make_examples_extra_args=vsc_min_count_snps=2,vsc_min_fraction_snps=0.12 --reads=/input/BAM --output_vcf=/output/OUTPUT_VCF --output_gvcf=/output/OUTPUT_GVCF --num_shards=$(nproc) --logging_dir=/output/logs) 2>&1 | tee -a generallog.log. ```. and the output tells me . ```. ValueError: NOT_FOUND: Could not open /input/BAM. [E::hts_open_format] Failed to open file ""/input/BAM"" : No such file or directory. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/Adineta_vaga.fsa --reads /input/BAM --examples /tmp/tmpbkwxdhbf/make_examples.tfrecord@48.gz --add_hp_channel --alt_aligned_pileup diff_channels --gvcf /tmp/tmpbkwxdhbf/gvcf.tfrecord@48.gz --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_count_snps 2 --vsc_min_fraction_indels 0.12 --vsc_min_fraction_snps 0.12 --task 15. ```. the bam file is there, 100% sure, at least to my eyes. But it seems docker fails to see it. Any idea? I have the full log available if needed. Again, I am sorry because it looks like some easy stuff, but my colleague and I can't find it. . Thanks a lot.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685
https://github.com/google/deepvariant/issues/685:1779,safety,log,log,1779,"Works in dry run, not in functional mode; Hello, . I am very sorry because this seems like a kind of s*** I could figure out but I have spent the afternoon to no avail. So, the following command WORKS in dry run mode, no error, but fails when in real operative mode. . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""1.5.0"". OUTPUT_VCF=vaga_lab_hifi_standing_variation.vcf.gz. OUTPUT_GVCF=vaga_lab_hifi_standing_variation.vcf.gz. BAM=HiFi_vaga.sorted.bam. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref=/input/Adineta_vaga.fsa --make_examples_extra_args=vsc_min_count_snps=2,vsc_min_fraction_snps=0.12 --reads=/input/BAM --output_vcf=/output/OUTPUT_VCF --output_gvcf=/output/OUTPUT_GVCF --num_shards=$(nproc) --logging_dir=/output/logs) 2>&1 | tee -a generallog.log. ```. and the output tells me . ```. ValueError: NOT_FOUND: Could not open /input/BAM. [E::hts_open_format] Failed to open file ""/input/BAM"" : No such file or directory. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/Adineta_vaga.fsa --reads /input/BAM --examples /tmp/tmpbkwxdhbf/make_examples.tfrecord@48.gz --add_hp_channel --alt_aligned_pileup diff_channels --gvcf /tmp/tmpbkwxdhbf/gvcf.tfrecord@48.gz --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_count_snps 2 --vsc_min_fraction_indels 0.12 --vsc_min_fraction_snps 0.12 --task 15. ```. the bam file is there, 100% sure, at least to my eyes. But it seems docker fails to see it. Any idea? I have the full log available if needed. Again, I am sorry because it looks like some easy stuff, but my colleague and I can't find it. . Thanks a lot.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685
https://github.com/google/deepvariant/issues/685:1783,safety,avail,available,1783,"Works in dry run, not in functional mode; Hello, . I am very sorry because this seems like a kind of s*** I could figure out but I have spent the afternoon to no avail. So, the following command WORKS in dry run mode, no error, but fails when in real operative mode. . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""1.5.0"". OUTPUT_VCF=vaga_lab_hifi_standing_variation.vcf.gz. OUTPUT_GVCF=vaga_lab_hifi_standing_variation.vcf.gz. BAM=HiFi_vaga.sorted.bam. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref=/input/Adineta_vaga.fsa --make_examples_extra_args=vsc_min_count_snps=2,vsc_min_fraction_snps=0.12 --reads=/input/BAM --output_vcf=/output/OUTPUT_VCF --output_gvcf=/output/OUTPUT_GVCF --num_shards=$(nproc) --logging_dir=/output/logs) 2>&1 | tee -a generallog.log. ```. and the output tells me . ```. ValueError: NOT_FOUND: Could not open /input/BAM. [E::hts_open_format] Failed to open file ""/input/BAM"" : No such file or directory. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/Adineta_vaga.fsa --reads /input/BAM --examples /tmp/tmpbkwxdhbf/make_examples.tfrecord@48.gz --add_hp_channel --alt_aligned_pileup diff_channels --gvcf /tmp/tmpbkwxdhbf/gvcf.tfrecord@48.gz --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_count_snps 2 --vsc_min_fraction_indels 0.12 --vsc_min_fraction_snps 0.12 --task 15. ```. the bam file is there, 100% sure, at least to my eyes. But it seems docker fails to see it. Any idea? I have the full log available if needed. Again, I am sorry because it looks like some easy stuff, but my colleague and I can't find it. . Thanks a lot.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685
https://github.com/google/deepvariant/issues/685:881,security,log,logs,881,"Works in dry run, not in functional mode; Hello, . I am very sorry because this seems like a kind of s*** I could figure out but I have spent the afternoon to no avail. So, the following command WORKS in dry run mode, no error, but fails when in real operative mode. . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""1.5.0"". OUTPUT_VCF=vaga_lab_hifi_standing_variation.vcf.gz. OUTPUT_GVCF=vaga_lab_hifi_standing_variation.vcf.gz. BAM=HiFi_vaga.sorted.bam. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref=/input/Adineta_vaga.fsa --make_examples_extra_args=vsc_min_count_snps=2,vsc_min_fraction_snps=0.12 --reads=/input/BAM --output_vcf=/output/OUTPUT_VCF --output_gvcf=/output/OUTPUT_GVCF --num_shards=$(nproc) --logging_dir=/output/logs) 2>&1 | tee -a generallog.log. ```. and the output tells me . ```. ValueError: NOT_FOUND: Could not open /input/BAM. [E::hts_open_format] Failed to open file ""/input/BAM"" : No such file or directory. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/Adineta_vaga.fsa --reads /input/BAM --examples /tmp/tmpbkwxdhbf/make_examples.tfrecord@48.gz --add_hp_channel --alt_aligned_pileup diff_channels --gvcf /tmp/tmpbkwxdhbf/gvcf.tfrecord@48.gz --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_count_snps 2 --vsc_min_fraction_indels 0.12 --vsc_min_fraction_snps 0.12 --task 15. ```. the bam file is there, 100% sure, at least to my eyes. But it seems docker fails to see it. Any idea? I have the full log available if needed. Again, I am sorry because it looks like some easy stuff, but my colleague and I can't find it. . Thanks a lot.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685
https://github.com/google/deepvariant/issues/685:912,security,log,log,912,"Works in dry run, not in functional mode; Hello, . I am very sorry because this seems like a kind of s*** I could figure out but I have spent the afternoon to no avail. So, the following command WORKS in dry run mode, no error, but fails when in real operative mode. . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""1.5.0"". OUTPUT_VCF=vaga_lab_hifi_standing_variation.vcf.gz. OUTPUT_GVCF=vaga_lab_hifi_standing_variation.vcf.gz. BAM=HiFi_vaga.sorted.bam. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref=/input/Adineta_vaga.fsa --make_examples_extra_args=vsc_min_count_snps=2,vsc_min_fraction_snps=0.12 --reads=/input/BAM --output_vcf=/output/OUTPUT_VCF --output_gvcf=/output/OUTPUT_GVCF --num_shards=$(nproc) --logging_dir=/output/logs) 2>&1 | tee -a generallog.log. ```. and the output tells me . ```. ValueError: NOT_FOUND: Could not open /input/BAM. [E::hts_open_format] Failed to open file ""/input/BAM"" : No such file or directory. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/Adineta_vaga.fsa --reads /input/BAM --examples /tmp/tmpbkwxdhbf/make_examples.tfrecord@48.gz --add_hp_channel --alt_aligned_pileup diff_channels --gvcf /tmp/tmpbkwxdhbf/gvcf.tfrecord@48.gz --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_count_snps 2 --vsc_min_fraction_indels 0.12 --vsc_min_fraction_snps 0.12 --task 15. ```. the bam file is there, 100% sure, at least to my eyes. But it seems docker fails to see it. Any idea? I have the full log available if needed. Again, I am sorry because it looks like some easy stuff, but my colleague and I can't find it. . Thanks a lot.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685
https://github.com/google/deepvariant/issues/685:1779,security,log,log,1779,"Works in dry run, not in functional mode; Hello, . I am very sorry because this seems like a kind of s*** I could figure out but I have spent the afternoon to no avail. So, the following command WORKS in dry run mode, no error, but fails when in real operative mode. . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""1.5.0"". OUTPUT_VCF=vaga_lab_hifi_standing_variation.vcf.gz. OUTPUT_GVCF=vaga_lab_hifi_standing_variation.vcf.gz. BAM=HiFi_vaga.sorted.bam. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref=/input/Adineta_vaga.fsa --make_examples_extra_args=vsc_min_count_snps=2,vsc_min_fraction_snps=0.12 --reads=/input/BAM --output_vcf=/output/OUTPUT_VCF --output_gvcf=/output/OUTPUT_GVCF --num_shards=$(nproc) --logging_dir=/output/logs) 2>&1 | tee -a generallog.log. ```. and the output tells me . ```. ValueError: NOT_FOUND: Could not open /input/BAM. [E::hts_open_format] Failed to open file ""/input/BAM"" : No such file or directory. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/Adineta_vaga.fsa --reads /input/BAM --examples /tmp/tmpbkwxdhbf/make_examples.tfrecord@48.gz --add_hp_channel --alt_aligned_pileup diff_channels --gvcf /tmp/tmpbkwxdhbf/gvcf.tfrecord@48.gz --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_count_snps 2 --vsc_min_fraction_indels 0.12 --vsc_min_fraction_snps 0.12 --task 15. ```. the bam file is there, 100% sure, at least to my eyes. But it seems docker fails to see it. Any idea? I have the full log available if needed. Again, I am sorry because it looks like some easy stuff, but my colleague and I can't find it. . Thanks a lot.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685
https://github.com/google/deepvariant/issues/685:1783,security,availab,available,1783,"Works in dry run, not in functional mode; Hello, . I am very sorry because this seems like a kind of s*** I could figure out but I have spent the afternoon to no avail. So, the following command WORKS in dry run mode, no error, but fails when in real operative mode. . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""1.5.0"". OUTPUT_VCF=vaga_lab_hifi_standing_variation.vcf.gz. OUTPUT_GVCF=vaga_lab_hifi_standing_variation.vcf.gz. BAM=HiFi_vaga.sorted.bam. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref=/input/Adineta_vaga.fsa --make_examples_extra_args=vsc_min_count_snps=2,vsc_min_fraction_snps=0.12 --reads=/input/BAM --output_vcf=/output/OUTPUT_VCF --output_gvcf=/output/OUTPUT_GVCF --num_shards=$(nproc) --logging_dir=/output/logs) 2>&1 | tee -a generallog.log. ```. and the output tells me . ```. ValueError: NOT_FOUND: Could not open /input/BAM. [E::hts_open_format] Failed to open file ""/input/BAM"" : No such file or directory. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/Adineta_vaga.fsa --reads /input/BAM --examples /tmp/tmpbkwxdhbf/make_examples.tfrecord@48.gz --add_hp_channel --alt_aligned_pileup diff_channels --gvcf /tmp/tmpbkwxdhbf/gvcf.tfrecord@48.gz --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_count_snps 2 --vsc_min_fraction_indels 0.12 --vsc_min_fraction_snps 0.12 --task 15. ```. the bam file is there, 100% sure, at least to my eyes. But it seems docker fails to see it. Any idea? I have the full log available if needed. Again, I am sorry because it looks like some easy stuff, but my colleague and I can't find it. . Thanks a lot.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685
https://github.com/google/deepvariant/issues/685:881,testability,log,logs,881,"Works in dry run, not in functional mode; Hello, . I am very sorry because this seems like a kind of s*** I could figure out but I have spent the afternoon to no avail. So, the following command WORKS in dry run mode, no error, but fails when in real operative mode. . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""1.5.0"". OUTPUT_VCF=vaga_lab_hifi_standing_variation.vcf.gz. OUTPUT_GVCF=vaga_lab_hifi_standing_variation.vcf.gz. BAM=HiFi_vaga.sorted.bam. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref=/input/Adineta_vaga.fsa --make_examples_extra_args=vsc_min_count_snps=2,vsc_min_fraction_snps=0.12 --reads=/input/BAM --output_vcf=/output/OUTPUT_VCF --output_gvcf=/output/OUTPUT_GVCF --num_shards=$(nproc) --logging_dir=/output/logs) 2>&1 | tee -a generallog.log. ```. and the output tells me . ```. ValueError: NOT_FOUND: Could not open /input/BAM. [E::hts_open_format] Failed to open file ""/input/BAM"" : No such file or directory. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/Adineta_vaga.fsa --reads /input/BAM --examples /tmp/tmpbkwxdhbf/make_examples.tfrecord@48.gz --add_hp_channel --alt_aligned_pileup diff_channels --gvcf /tmp/tmpbkwxdhbf/gvcf.tfrecord@48.gz --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_count_snps 2 --vsc_min_fraction_indels 0.12 --vsc_min_fraction_snps 0.12 --task 15. ```. the bam file is there, 100% sure, at least to my eyes. But it seems docker fails to see it. Any idea? I have the full log available if needed. Again, I am sorry because it looks like some easy stuff, but my colleague and I can't find it. . Thanks a lot.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685
https://github.com/google/deepvariant/issues/685:912,testability,log,log,912,"Works in dry run, not in functional mode; Hello, . I am very sorry because this seems like a kind of s*** I could figure out but I have spent the afternoon to no avail. So, the following command WORKS in dry run mode, no error, but fails when in real operative mode. . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""1.5.0"". OUTPUT_VCF=vaga_lab_hifi_standing_variation.vcf.gz. OUTPUT_GVCF=vaga_lab_hifi_standing_variation.vcf.gz. BAM=HiFi_vaga.sorted.bam. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref=/input/Adineta_vaga.fsa --make_examples_extra_args=vsc_min_count_snps=2,vsc_min_fraction_snps=0.12 --reads=/input/BAM --output_vcf=/output/OUTPUT_VCF --output_gvcf=/output/OUTPUT_GVCF --num_shards=$(nproc) --logging_dir=/output/logs) 2>&1 | tee -a generallog.log. ```. and the output tells me . ```. ValueError: NOT_FOUND: Could not open /input/BAM. [E::hts_open_format] Failed to open file ""/input/BAM"" : No such file or directory. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/Adineta_vaga.fsa --reads /input/BAM --examples /tmp/tmpbkwxdhbf/make_examples.tfrecord@48.gz --add_hp_channel --alt_aligned_pileup diff_channels --gvcf /tmp/tmpbkwxdhbf/gvcf.tfrecord@48.gz --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_count_snps 2 --vsc_min_fraction_indels 0.12 --vsc_min_fraction_snps 0.12 --task 15. ```. the bam file is there, 100% sure, at least to my eyes. But it seems docker fails to see it. Any idea? I have the full log available if needed. Again, I am sorry because it looks like some easy stuff, but my colleague and I can't find it. . Thanks a lot.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685
https://github.com/google/deepvariant/issues/685:1779,testability,log,log,1779,"Works in dry run, not in functional mode; Hello, . I am very sorry because this seems like a kind of s*** I could figure out but I have spent the afternoon to no avail. So, the following command WORKS in dry run mode, no error, but fails when in real operative mode. . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""1.5.0"". OUTPUT_VCF=vaga_lab_hifi_standing_variation.vcf.gz. OUTPUT_GVCF=vaga_lab_hifi_standing_variation.vcf.gz. BAM=HiFi_vaga.sorted.bam. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref=/input/Adineta_vaga.fsa --make_examples_extra_args=vsc_min_count_snps=2,vsc_min_fraction_snps=0.12 --reads=/input/BAM --output_vcf=/output/OUTPUT_VCF --output_gvcf=/output/OUTPUT_GVCF --num_shards=$(nproc) --logging_dir=/output/logs) 2>&1 | tee -a generallog.log. ```. and the output tells me . ```. ValueError: NOT_FOUND: Could not open /input/BAM. [E::hts_open_format] Failed to open file ""/input/BAM"" : No such file or directory. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/Adineta_vaga.fsa --reads /input/BAM --examples /tmp/tmpbkwxdhbf/make_examples.tfrecord@48.gz --add_hp_channel --alt_aligned_pileup diff_channels --gvcf /tmp/tmpbkwxdhbf/gvcf.tfrecord@48.gz --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_count_snps 2 --vsc_min_fraction_indels 0.12 --vsc_min_fraction_snps 0.12 --task 15. ```. the bam file is there, 100% sure, at least to my eyes. But it seems docker fails to see it. Any idea? I have the full log available if needed. Again, I am sorry because it looks like some easy stuff, but my colleague and I can't find it. . Thanks a lot.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685
https://github.com/google/deepvariant/issues/685:187,usability,command,command,187,"Works in dry run, not in functional mode; Hello, . I am very sorry because this seems like a kind of s*** I could figure out but I have spent the afternoon to no avail. So, the following command WORKS in dry run mode, no error, but fails when in real operative mode. . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""1.5.0"". OUTPUT_VCF=vaga_lab_hifi_standing_variation.vcf.gz. OUTPUT_GVCF=vaga_lab_hifi_standing_variation.vcf.gz. BAM=HiFi_vaga.sorted.bam. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref=/input/Adineta_vaga.fsa --make_examples_extra_args=vsc_min_count_snps=2,vsc_min_fraction_snps=0.12 --reads=/input/BAM --output_vcf=/output/OUTPUT_VCF --output_gvcf=/output/OUTPUT_GVCF --num_shards=$(nproc) --logging_dir=/output/logs) 2>&1 | tee -a generallog.log. ```. and the output tells me . ```. ValueError: NOT_FOUND: Could not open /input/BAM. [E::hts_open_format] Failed to open file ""/input/BAM"" : No such file or directory. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/Adineta_vaga.fsa --reads /input/BAM --examples /tmp/tmpbkwxdhbf/make_examples.tfrecord@48.gz --add_hp_channel --alt_aligned_pileup diff_channels --gvcf /tmp/tmpbkwxdhbf/gvcf.tfrecord@48.gz --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_count_snps 2 --vsc_min_fraction_indels 0.12 --vsc_min_fraction_snps 0.12 --task 15. ```. the bam file is there, 100% sure, at least to my eyes. But it seems docker fails to see it. Any idea? I have the full log available if needed. Again, I am sorry because it looks like some easy stuff, but my colleague and I can't find it. . Thanks a lot.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685
https://github.com/google/deepvariant/issues/685:221,usability,error,error,221,"Works in dry run, not in functional mode; Hello, . I am very sorry because this seems like a kind of s*** I could figure out but I have spent the afternoon to no avail. So, the following command WORKS in dry run mode, no error, but fails when in real operative mode. . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""1.5.0"". OUTPUT_VCF=vaga_lab_hifi_standing_variation.vcf.gz. OUTPUT_GVCF=vaga_lab_hifi_standing_variation.vcf.gz. BAM=HiFi_vaga.sorted.bam. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref=/input/Adineta_vaga.fsa --make_examples_extra_args=vsc_min_count_snps=2,vsc_min_fraction_snps=0.12 --reads=/input/BAM --output_vcf=/output/OUTPUT_VCF --output_gvcf=/output/OUTPUT_GVCF --num_shards=$(nproc) --logging_dir=/output/logs) 2>&1 | tee -a generallog.log. ```. and the output tells me . ```. ValueError: NOT_FOUND: Could not open /input/BAM. [E::hts_open_format] Failed to open file ""/input/BAM"" : No such file or directory. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/Adineta_vaga.fsa --reads /input/BAM --examples /tmp/tmpbkwxdhbf/make_examples.tfrecord@48.gz --add_hp_channel --alt_aligned_pileup diff_channels --gvcf /tmp/tmpbkwxdhbf/gvcf.tfrecord@48.gz --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_count_snps 2 --vsc_min_fraction_indels 0.12 --vsc_min_fraction_snps 0.12 --task 15. ```. the bam file is there, 100% sure, at least to my eyes. But it seems docker fails to see it. Any idea? I have the full log available if needed. Again, I am sorry because it looks like some easy stuff, but my colleague and I can't find it. . Thanks a lot.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685
https://github.com/google/deepvariant/issues/685:520,usability,input,input,520,"Works in dry run, not in functional mode; Hello, . I am very sorry because this seems like a kind of s*** I could figure out but I have spent the afternoon to no avail. So, the following command WORKS in dry run mode, no error, but fails when in real operative mode. . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""1.5.0"". OUTPUT_VCF=vaga_lab_hifi_standing_variation.vcf.gz. OUTPUT_GVCF=vaga_lab_hifi_standing_variation.vcf.gz. BAM=HiFi_vaga.sorted.bam. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref=/input/Adineta_vaga.fsa --make_examples_extra_args=vsc_min_count_snps=2,vsc_min_fraction_snps=0.12 --reads=/input/BAM --output_vcf=/output/OUTPUT_VCF --output_gvcf=/output/OUTPUT_GVCF --num_shards=$(nproc) --logging_dir=/output/logs) 2>&1 | tee -a generallog.log. ```. and the output tells me . ```. ValueError: NOT_FOUND: Could not open /input/BAM. [E::hts_open_format] Failed to open file ""/input/BAM"" : No such file or directory. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/Adineta_vaga.fsa --reads /input/BAM --examples /tmp/tmpbkwxdhbf/make_examples.tfrecord@48.gz --add_hp_channel --alt_aligned_pileup diff_channels --gvcf /tmp/tmpbkwxdhbf/gvcf.tfrecord@48.gz --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_count_snps 2 --vsc_min_fraction_indels 0.12 --vsc_min_fraction_snps 0.12 --task 15. ```. the bam file is there, 100% sure, at least to my eyes. But it seems docker fails to see it. Any idea? I have the full log available if needed. Again, I am sorry because it looks like some easy stuff, but my colleague and I can't find it. . Thanks a lot.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685
https://github.com/google/deepvariant/issues/685:654,usability,input,input,654,"Works in dry run, not in functional mode; Hello, . I am very sorry because this seems like a kind of s*** I could figure out but I have spent the afternoon to no avail. So, the following command WORKS in dry run mode, no error, but fails when in real operative mode. . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""1.5.0"". OUTPUT_VCF=vaga_lab_hifi_standing_variation.vcf.gz. OUTPUT_GVCF=vaga_lab_hifi_standing_variation.vcf.gz. BAM=HiFi_vaga.sorted.bam. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref=/input/Adineta_vaga.fsa --make_examples_extra_args=vsc_min_count_snps=2,vsc_min_fraction_snps=0.12 --reads=/input/BAM --output_vcf=/output/OUTPUT_VCF --output_gvcf=/output/OUTPUT_GVCF --num_shards=$(nproc) --logging_dir=/output/logs) 2>&1 | tee -a generallog.log. ```. and the output tells me . ```. ValueError: NOT_FOUND: Could not open /input/BAM. [E::hts_open_format] Failed to open file ""/input/BAM"" : No such file or directory. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/Adineta_vaga.fsa --reads /input/BAM --examples /tmp/tmpbkwxdhbf/make_examples.tfrecord@48.gz --add_hp_channel --alt_aligned_pileup diff_channels --gvcf /tmp/tmpbkwxdhbf/gvcf.tfrecord@48.gz --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_count_snps 2 --vsc_min_fraction_indels 0.12 --vsc_min_fraction_snps 0.12 --task 15. ```. the bam file is there, 100% sure, at least to my eyes. But it seems docker fails to see it. Any idea? I have the full log available if needed. Again, I am sorry because it looks like some easy stuff, but my colleague and I can't find it. . Thanks a lot.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685
https://github.com/google/deepvariant/issues/685:761,usability,input,input,761,"Works in dry run, not in functional mode; Hello, . I am very sorry because this seems like a kind of s*** I could figure out but I have spent the afternoon to no avail. So, the following command WORKS in dry run mode, no error, but fails when in real operative mode. . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""1.5.0"". OUTPUT_VCF=vaga_lab_hifi_standing_variation.vcf.gz. OUTPUT_GVCF=vaga_lab_hifi_standing_variation.vcf.gz. BAM=HiFi_vaga.sorted.bam. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref=/input/Adineta_vaga.fsa --make_examples_extra_args=vsc_min_count_snps=2,vsc_min_fraction_snps=0.12 --reads=/input/BAM --output_vcf=/output/OUTPUT_VCF --output_gvcf=/output/OUTPUT_GVCF --num_shards=$(nproc) --logging_dir=/output/logs) 2>&1 | tee -a generallog.log. ```. and the output tells me . ```. ValueError: NOT_FOUND: Could not open /input/BAM. [E::hts_open_format] Failed to open file ""/input/BAM"" : No such file or directory. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/Adineta_vaga.fsa --reads /input/BAM --examples /tmp/tmpbkwxdhbf/make_examples.tfrecord@48.gz --add_hp_channel --alt_aligned_pileup diff_channels --gvcf /tmp/tmpbkwxdhbf/gvcf.tfrecord@48.gz --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_count_snps 2 --vsc_min_fraction_indels 0.12 --vsc_min_fraction_snps 0.12 --task 15. ```. the bam file is there, 100% sure, at least to my eyes. But it seems docker fails to see it. Any idea? I have the full log available if needed. Again, I am sorry because it looks like some easy stuff, but my colleague and I can't find it. . Thanks a lot.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685
https://github.com/google/deepvariant/issues/685:992,usability,input,input,992,"Works in dry run, not in functional mode; Hello, . I am very sorry because this seems like a kind of s*** I could figure out but I have spent the afternoon to no avail. So, the following command WORKS in dry run mode, no error, but fails when in real operative mode. . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""1.5.0"". OUTPUT_VCF=vaga_lab_hifi_standing_variation.vcf.gz. OUTPUT_GVCF=vaga_lab_hifi_standing_variation.vcf.gz. BAM=HiFi_vaga.sorted.bam. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref=/input/Adineta_vaga.fsa --make_examples_extra_args=vsc_min_count_snps=2,vsc_min_fraction_snps=0.12 --reads=/input/BAM --output_vcf=/output/OUTPUT_VCF --output_gvcf=/output/OUTPUT_GVCF --num_shards=$(nproc) --logging_dir=/output/logs) 2>&1 | tee -a generallog.log. ```. and the output tells me . ```. ValueError: NOT_FOUND: Could not open /input/BAM. [E::hts_open_format] Failed to open file ""/input/BAM"" : No such file or directory. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/Adineta_vaga.fsa --reads /input/BAM --examples /tmp/tmpbkwxdhbf/make_examples.tfrecord@48.gz --add_hp_channel --alt_aligned_pileup diff_channels --gvcf /tmp/tmpbkwxdhbf/gvcf.tfrecord@48.gz --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_count_snps 2 --vsc_min_fraction_indels 0.12 --vsc_min_fraction_snps 0.12 --task 15. ```. the bam file is there, 100% sure, at least to my eyes. But it seems docker fails to see it. Any idea? I have the full log available if needed. Again, I am sorry because it looks like some easy stuff, but my colleague and I can't find it. . Thanks a lot.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685
https://github.com/google/deepvariant/issues/685:1046,usability,input,input,1046,"Works in dry run, not in functional mode; Hello, . I am very sorry because this seems like a kind of s*** I could figure out but I have spent the afternoon to no avail. So, the following command WORKS in dry run mode, no error, but fails when in real operative mode. . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""1.5.0"". OUTPUT_VCF=vaga_lab_hifi_standing_variation.vcf.gz. OUTPUT_GVCF=vaga_lab_hifi_standing_variation.vcf.gz. BAM=HiFi_vaga.sorted.bam. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref=/input/Adineta_vaga.fsa --make_examples_extra_args=vsc_min_count_snps=2,vsc_min_fraction_snps=0.12 --reads=/input/BAM --output_vcf=/output/OUTPUT_VCF --output_gvcf=/output/OUTPUT_GVCF --num_shards=$(nproc) --logging_dir=/output/logs) 2>&1 | tee -a generallog.log. ```. and the output tells me . ```. ValueError: NOT_FOUND: Could not open /input/BAM. [E::hts_open_format] Failed to open file ""/input/BAM"" : No such file or directory. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/Adineta_vaga.fsa --reads /input/BAM --examples /tmp/tmpbkwxdhbf/make_examples.tfrecord@48.gz --add_hp_channel --alt_aligned_pileup diff_channels --gvcf /tmp/tmpbkwxdhbf/gvcf.tfrecord@48.gz --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_count_snps 2 --vsc_min_fraction_indels 0.12 --vsc_min_fraction_snps 0.12 --task 15. ```. the bam file is there, 100% sure, at least to my eyes. But it seems docker fails to see it. Any idea? I have the full log available if needed. Again, I am sorry because it looks like some easy stuff, but my colleague and I can't find it. . Thanks a lot.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685
https://github.com/google/deepvariant/issues/685:1171,usability,input,input,1171,"Works in dry run, not in functional mode; Hello, . I am very sorry because this seems like a kind of s*** I could figure out but I have spent the afternoon to no avail. So, the following command WORKS in dry run mode, no error, but fails when in real operative mode. . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""1.5.0"". OUTPUT_VCF=vaga_lab_hifi_standing_variation.vcf.gz. OUTPUT_GVCF=vaga_lab_hifi_standing_variation.vcf.gz. BAM=HiFi_vaga.sorted.bam. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref=/input/Adineta_vaga.fsa --make_examples_extra_args=vsc_min_count_snps=2,vsc_min_fraction_snps=0.12 --reads=/input/BAM --output_vcf=/output/OUTPUT_VCF --output_gvcf=/output/OUTPUT_GVCF --num_shards=$(nproc) --logging_dir=/output/logs) 2>&1 | tee -a generallog.log. ```. and the output tells me . ```. ValueError: NOT_FOUND: Could not open /input/BAM. [E::hts_open_format] Failed to open file ""/input/BAM"" : No such file or directory. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/Adineta_vaga.fsa --reads /input/BAM --examples /tmp/tmpbkwxdhbf/make_examples.tfrecord@48.gz --add_hp_channel --alt_aligned_pileup diff_channels --gvcf /tmp/tmpbkwxdhbf/gvcf.tfrecord@48.gz --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_count_snps 2 --vsc_min_fraction_indels 0.12 --vsc_min_fraction_snps 0.12 --task 15. ```. the bam file is there, 100% sure, at least to my eyes. But it seems docker fails to see it. Any idea? I have the full log available if needed. Again, I am sorry because it looks like some easy stuff, but my colleague and I can't find it. . Thanks a lot.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685
https://github.com/google/deepvariant/issues/685:1203,usability,input,input,1203,"Works in dry run, not in functional mode; Hello, . I am very sorry because this seems like a kind of s*** I could figure out but I have spent the afternoon to no avail. So, the following command WORKS in dry run mode, no error, but fails when in real operative mode. . ```. #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}"". INPUT_DIR=""${PWD}"". BIN_VERSION=""1.5.0"". OUTPUT_VCF=vaga_lab_hifi_standing_variation.vcf.gz. OUTPUT_GVCF=vaga_lab_hifi_standing_variation.vcf.gz. BAM=HiFi_vaga.sorted.bam. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref=/input/Adineta_vaga.fsa --make_examples_extra_args=vsc_min_count_snps=2,vsc_min_fraction_snps=0.12 --reads=/input/BAM --output_vcf=/output/OUTPUT_VCF --output_gvcf=/output/OUTPUT_GVCF --num_shards=$(nproc) --logging_dir=/output/logs) 2>&1 | tee -a generallog.log. ```. and the output tells me . ```. ValueError: NOT_FOUND: Could not open /input/BAM. [E::hts_open_format] Failed to open file ""/input/BAM"" : No such file or directory. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/Adineta_vaga.fsa --reads /input/BAM --examples /tmp/tmpbkwxdhbf/make_examples.tfrecord@48.gz --add_hp_channel --alt_aligned_pileup diff_channels --gvcf /tmp/tmpbkwxdhbf/gvcf.tfrecord@48.gz --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_count_snps 2 --vsc_min_fraction_indels 0.12 --vsc_min_fraction_snps 0.12 --task 15. ```. the bam file is there, 100% sure, at least to my eyes. But it seems docker fails to see it. Any idea? I have the full log available if needed. Again, I am sorry because it looks like some easy stuff, but my colleague and I can't find it. . Thanks a lot.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/685
https://github.com/google/deepvariant/issues/686:15,modifiability,Pac,PacBio,15,"question about PacBio mitochondrial data; Hello,. I would like to know if it is possible to use DeepVariant to analyze PacBio mitochondrial data. If not, do you have any suitable tools to recommend? Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/686
https://github.com/google/deepvariant/issues/686:119,modifiability,Pac,PacBio,119,"question about PacBio mitochondrial data; Hello,. I would like to know if it is possible to use DeepVariant to analyze PacBio mitochondrial data. If not, do you have any suitable tools to recommend? Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/686
https://github.com/google/deepvariant/issues/686:179,usability,tool,tools,179,"question about PacBio mitochondrial data; Hello,. I would like to know if it is possible to use DeepVariant to analyze PacBio mitochondrial data. If not, do you have any suitable tools to recommend? Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/686
https://github.com/google/deepvariant/issues/687:89,availability,error,error,89,"Is --dry_run not a flag with DeepTrio?; When running deeptrio with `--dry_run=true`, the error message ""FATAL Flags parsing error: Unknown command line flag 'dry_run'"" pops up and run exits. . Here is part of the command I used:. `. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:deeptrio-""${BIN_VERSION}"" \. /opt/deepvariant/bin/deeptrio/run_deeptrio \. --model_type=WGS \. --ref=Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta \. --reads_parent1=markduplicates/S_500061.md.bam \. --reads_parent2=markduplicates/S_500062.md.bam \. --reads_child=markduplicates/S_500063.md.bam \. --output_vcf_parent1 output/S_500061.output.vcf.gz \. --output_vcf_parent2 output/S_500062.output.vcf.gz \. --output_vcf_child output/S_500063.output.vcf.gz \. --sample_name_parent1 'S_500061' \. --sample_name_parent2 'S_500062' \. --sample_name_child 'S_500063' \. --num_shards $(nproc) \. --intermediate_results_dir output/intermediate_results_dir \. --output_gvcf_parent1 output/S_500061.g.vcf.gz \. --output_gvcf_parent2 output/S_500062.g.vcf.gz \. --output_gvcf_child output/S_500063.g.vcf.gz \. --output_gvcf_merged output/FAM1.g.vcf.gz\. --dry_run=true \. --vcf_stats_report=true. `. Thanks for advice.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:124,availability,error,error,124,"Is --dry_run not a flag with DeepTrio?; When running deeptrio with `--dry_run=true`, the error message ""FATAL Flags parsing error: Unknown command line flag 'dry_run'"" pops up and run exits. . Here is part of the command I used:. `. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:deeptrio-""${BIN_VERSION}"" \. /opt/deepvariant/bin/deeptrio/run_deeptrio \. --model_type=WGS \. --ref=Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta \. --reads_parent1=markduplicates/S_500061.md.bam \. --reads_parent2=markduplicates/S_500062.md.bam \. --reads_child=markduplicates/S_500063.md.bam \. --output_vcf_parent1 output/S_500061.output.vcf.gz \. --output_vcf_parent2 output/S_500062.output.vcf.gz \. --output_vcf_child output/S_500063.output.vcf.gz \. --sample_name_parent1 'S_500061' \. --sample_name_parent2 'S_500062' \. --sample_name_child 'S_500063' \. --num_shards $(nproc) \. --intermediate_results_dir output/intermediate_results_dir \. --output_gvcf_parent1 output/S_500061.g.vcf.gz \. --output_gvcf_parent2 output/S_500062.g.vcf.gz \. --output_gvcf_child output/S_500063.g.vcf.gz \. --output_gvcf_merged output/FAM1.g.vcf.gz\. --dry_run=true \. --vcf_stats_report=true. `. Thanks for advice.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:95,integrability,messag,message,95,"Is --dry_run not a flag with DeepTrio?; When running deeptrio with `--dry_run=true`, the error message ""FATAL Flags parsing error: Unknown command line flag 'dry_run'"" pops up and run exits. . Here is part of the command I used:. `. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:deeptrio-""${BIN_VERSION}"" \. /opt/deepvariant/bin/deeptrio/run_deeptrio \. --model_type=WGS \. --ref=Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta \. --reads_parent1=markduplicates/S_500061.md.bam \. --reads_parent2=markduplicates/S_500062.md.bam \. --reads_child=markduplicates/S_500063.md.bam \. --output_vcf_parent1 output/S_500061.output.vcf.gz \. --output_vcf_parent2 output/S_500062.output.vcf.gz \. --output_vcf_child output/S_500063.output.vcf.gz \. --sample_name_parent1 'S_500061' \. --sample_name_parent2 'S_500062' \. --sample_name_child 'S_500063' \. --num_shards $(nproc) \. --intermediate_results_dir output/intermediate_results_dir \. --output_gvcf_parent1 output/S_500061.g.vcf.gz \. --output_gvcf_parent2 output/S_500062.g.vcf.gz \. --output_gvcf_child output/S_500063.g.vcf.gz \. --output_gvcf_merged output/FAM1.g.vcf.gz\. --dry_run=true \. --vcf_stats_report=true. `. Thanks for advice.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:95,interoperability,messag,message,95,"Is --dry_run not a flag with DeepTrio?; When running deeptrio with `--dry_run=true`, the error message ""FATAL Flags parsing error: Unknown command line flag 'dry_run'"" pops up and run exits. . Here is part of the command I used:. `. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:deeptrio-""${BIN_VERSION}"" \. /opt/deepvariant/bin/deeptrio/run_deeptrio \. --model_type=WGS \. --ref=Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta \. --reads_parent1=markduplicates/S_500061.md.bam \. --reads_parent2=markduplicates/S_500062.md.bam \. --reads_child=markduplicates/S_500063.md.bam \. --output_vcf_parent1 output/S_500061.output.vcf.gz \. --output_vcf_parent2 output/S_500062.output.vcf.gz \. --output_vcf_child output/S_500063.output.vcf.gz \. --sample_name_parent1 'S_500061' \. --sample_name_parent2 'S_500062' \. --sample_name_child 'S_500063' \. --num_shards $(nproc) \. --intermediate_results_dir output/intermediate_results_dir \. --output_gvcf_parent1 output/S_500061.g.vcf.gz \. --output_gvcf_parent2 output/S_500062.g.vcf.gz \. --output_gvcf_child output/S_500063.g.vcf.gz \. --output_gvcf_merged output/FAM1.g.vcf.gz\. --dry_run=true \. --vcf_stats_report=true. `. Thanks for advice.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:89,performance,error,error,89,"Is --dry_run not a flag with DeepTrio?; When running deeptrio with `--dry_run=true`, the error message ""FATAL Flags parsing error: Unknown command line flag 'dry_run'"" pops up and run exits. . Here is part of the command I used:. `. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:deeptrio-""${BIN_VERSION}"" \. /opt/deepvariant/bin/deeptrio/run_deeptrio \. --model_type=WGS \. --ref=Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta \. --reads_parent1=markduplicates/S_500061.md.bam \. --reads_parent2=markduplicates/S_500062.md.bam \. --reads_child=markduplicates/S_500063.md.bam \. --output_vcf_parent1 output/S_500061.output.vcf.gz \. --output_vcf_parent2 output/S_500062.output.vcf.gz \. --output_vcf_child output/S_500063.output.vcf.gz \. --sample_name_parent1 'S_500061' \. --sample_name_parent2 'S_500062' \. --sample_name_child 'S_500063' \. --num_shards $(nproc) \. --intermediate_results_dir output/intermediate_results_dir \. --output_gvcf_parent1 output/S_500061.g.vcf.gz \. --output_gvcf_parent2 output/S_500062.g.vcf.gz \. --output_gvcf_child output/S_500063.g.vcf.gz \. --output_gvcf_merged output/FAM1.g.vcf.gz\. --dry_run=true \. --vcf_stats_report=true. `. Thanks for advice.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:124,performance,error,error,124,"Is --dry_run not a flag with DeepTrio?; When running deeptrio with `--dry_run=true`, the error message ""FATAL Flags parsing error: Unknown command line flag 'dry_run'"" pops up and run exits. . Here is part of the command I used:. `. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:deeptrio-""${BIN_VERSION}"" \. /opt/deepvariant/bin/deeptrio/run_deeptrio \. --model_type=WGS \. --ref=Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta \. --reads_parent1=markduplicates/S_500061.md.bam \. --reads_parent2=markduplicates/S_500062.md.bam \. --reads_child=markduplicates/S_500063.md.bam \. --output_vcf_parent1 output/S_500061.output.vcf.gz \. --output_vcf_parent2 output/S_500062.output.vcf.gz \. --output_vcf_child output/S_500063.output.vcf.gz \. --sample_name_parent1 'S_500061' \. --sample_name_parent2 'S_500062' \. --sample_name_child 'S_500063' \. --num_shards $(nproc) \. --intermediate_results_dir output/intermediate_results_dir \. --output_gvcf_parent1 output/S_500061.g.vcf.gz \. --output_gvcf_parent2 output/S_500062.g.vcf.gz \. --output_gvcf_child output/S_500063.g.vcf.gz \. --output_gvcf_merged output/FAM1.g.vcf.gz\. --dry_run=true \. --vcf_stats_report=true. `. Thanks for advice.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:89,safety,error,error,89,"Is --dry_run not a flag with DeepTrio?; When running deeptrio with `--dry_run=true`, the error message ""FATAL Flags parsing error: Unknown command line flag 'dry_run'"" pops up and run exits. . Here is part of the command I used:. `. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:deeptrio-""${BIN_VERSION}"" \. /opt/deepvariant/bin/deeptrio/run_deeptrio \. --model_type=WGS \. --ref=Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta \. --reads_parent1=markduplicates/S_500061.md.bam \. --reads_parent2=markduplicates/S_500062.md.bam \. --reads_child=markduplicates/S_500063.md.bam \. --output_vcf_parent1 output/S_500061.output.vcf.gz \. --output_vcf_parent2 output/S_500062.output.vcf.gz \. --output_vcf_child output/S_500063.output.vcf.gz \. --sample_name_parent1 'S_500061' \. --sample_name_parent2 'S_500062' \. --sample_name_child 'S_500063' \. --num_shards $(nproc) \. --intermediate_results_dir output/intermediate_results_dir \. --output_gvcf_parent1 output/S_500061.g.vcf.gz \. --output_gvcf_parent2 output/S_500062.g.vcf.gz \. --output_gvcf_child output/S_500063.g.vcf.gz \. --output_gvcf_merged output/FAM1.g.vcf.gz\. --dry_run=true \. --vcf_stats_report=true. `. Thanks for advice.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:124,safety,error,error,124,"Is --dry_run not a flag with DeepTrio?; When running deeptrio with `--dry_run=true`, the error message ""FATAL Flags parsing error: Unknown command line flag 'dry_run'"" pops up and run exits. . Here is part of the command I used:. `. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:deeptrio-""${BIN_VERSION}"" \. /opt/deepvariant/bin/deeptrio/run_deeptrio \. --model_type=WGS \. --ref=Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta \. --reads_parent1=markduplicates/S_500061.md.bam \. --reads_parent2=markduplicates/S_500062.md.bam \. --reads_child=markduplicates/S_500063.md.bam \. --output_vcf_parent1 output/S_500061.output.vcf.gz \. --output_vcf_parent2 output/S_500062.output.vcf.gz \. --output_vcf_child output/S_500063.output.vcf.gz \. --sample_name_parent1 'S_500061' \. --sample_name_parent2 'S_500062' \. --sample_name_child 'S_500063' \. --num_shards $(nproc) \. --intermediate_results_dir output/intermediate_results_dir \. --output_gvcf_parent1 output/S_500061.g.vcf.gz \. --output_gvcf_parent2 output/S_500062.g.vcf.gz \. --output_gvcf_child output/S_500063.g.vcf.gz \. --output_gvcf_merged output/FAM1.g.vcf.gz\. --dry_run=true \. --vcf_stats_report=true. `. Thanks for advice.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:89,usability,error,error,89,"Is --dry_run not a flag with DeepTrio?; When running deeptrio with `--dry_run=true`, the error message ""FATAL Flags parsing error: Unknown command line flag 'dry_run'"" pops up and run exits. . Here is part of the command I used:. `. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:deeptrio-""${BIN_VERSION}"" \. /opt/deepvariant/bin/deeptrio/run_deeptrio \. --model_type=WGS \. --ref=Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta \. --reads_parent1=markduplicates/S_500061.md.bam \. --reads_parent2=markduplicates/S_500062.md.bam \. --reads_child=markduplicates/S_500063.md.bam \. --output_vcf_parent1 output/S_500061.output.vcf.gz \. --output_vcf_parent2 output/S_500062.output.vcf.gz \. --output_vcf_child output/S_500063.output.vcf.gz \. --sample_name_parent1 'S_500061' \. --sample_name_parent2 'S_500062' \. --sample_name_child 'S_500063' \. --num_shards $(nproc) \. --intermediate_results_dir output/intermediate_results_dir \. --output_gvcf_parent1 output/S_500061.g.vcf.gz \. --output_gvcf_parent2 output/S_500062.g.vcf.gz \. --output_gvcf_child output/S_500063.g.vcf.gz \. --output_gvcf_merged output/FAM1.g.vcf.gz\. --dry_run=true \. --vcf_stats_report=true. `. Thanks for advice.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:124,usability,error,error,124,"Is --dry_run not a flag with DeepTrio?; When running deeptrio with `--dry_run=true`, the error message ""FATAL Flags parsing error: Unknown command line flag 'dry_run'"" pops up and run exits. . Here is part of the command I used:. `. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:deeptrio-""${BIN_VERSION}"" \. /opt/deepvariant/bin/deeptrio/run_deeptrio \. --model_type=WGS \. --ref=Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta \. --reads_parent1=markduplicates/S_500061.md.bam \. --reads_parent2=markduplicates/S_500062.md.bam \. --reads_child=markduplicates/S_500063.md.bam \. --output_vcf_parent1 output/S_500061.output.vcf.gz \. --output_vcf_parent2 output/S_500062.output.vcf.gz \. --output_vcf_child output/S_500063.output.vcf.gz \. --sample_name_parent1 'S_500061' \. --sample_name_parent2 'S_500062' \. --sample_name_child 'S_500063' \. --num_shards $(nproc) \. --intermediate_results_dir output/intermediate_results_dir \. --output_gvcf_parent1 output/S_500061.g.vcf.gz \. --output_gvcf_parent2 output/S_500062.g.vcf.gz \. --output_gvcf_child output/S_500063.g.vcf.gz \. --output_gvcf_merged output/FAM1.g.vcf.gz\. --dry_run=true \. --vcf_stats_report=true. `. Thanks for advice.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:139,usability,command,command,139,"Is --dry_run not a flag with DeepTrio?; When running deeptrio with `--dry_run=true`, the error message ""FATAL Flags parsing error: Unknown command line flag 'dry_run'"" pops up and run exits. . Here is part of the command I used:. `. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:deeptrio-""${BIN_VERSION}"" \. /opt/deepvariant/bin/deeptrio/run_deeptrio \. --model_type=WGS \. --ref=Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta \. --reads_parent1=markduplicates/S_500061.md.bam \. --reads_parent2=markduplicates/S_500062.md.bam \. --reads_child=markduplicates/S_500063.md.bam \. --output_vcf_parent1 output/S_500061.output.vcf.gz \. --output_vcf_parent2 output/S_500062.output.vcf.gz \. --output_vcf_child output/S_500063.output.vcf.gz \. --sample_name_parent1 'S_500061' \. --sample_name_parent2 'S_500062' \. --sample_name_child 'S_500063' \. --num_shards $(nproc) \. --intermediate_results_dir output/intermediate_results_dir \. --output_gvcf_parent1 output/S_500061.g.vcf.gz \. --output_gvcf_parent2 output/S_500062.g.vcf.gz \. --output_gvcf_child output/S_500063.g.vcf.gz \. --output_gvcf_merged output/FAM1.g.vcf.gz\. --dry_run=true \. --vcf_stats_report=true. `. Thanks for advice.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/687:213,usability,command,command,213,"Is --dry_run not a flag with DeepTrio?; When running deeptrio with `--dry_run=true`, the error message ""FATAL Flags parsing error: Unknown command line flag 'dry_run'"" pops up and run exits. . Here is part of the command I used:. `. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. docker://google/deepvariant:deeptrio-""${BIN_VERSION}"" \. /opt/deepvariant/bin/deeptrio/run_deeptrio \. --model_type=WGS \. --ref=Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta \. --reads_parent1=markduplicates/S_500061.md.bam \. --reads_parent2=markduplicates/S_500062.md.bam \. --reads_child=markduplicates/S_500063.md.bam \. --output_vcf_parent1 output/S_500061.output.vcf.gz \. --output_vcf_parent2 output/S_500062.output.vcf.gz \. --output_vcf_child output/S_500063.output.vcf.gz \. --sample_name_parent1 'S_500061' \. --sample_name_parent2 'S_500062' \. --sample_name_child 'S_500063' \. --num_shards $(nproc) \. --intermediate_results_dir output/intermediate_results_dir \. --output_gvcf_parent1 output/S_500061.g.vcf.gz \. --output_gvcf_parent2 output/S_500062.g.vcf.gz \. --output_gvcf_child output/S_500063.g.vcf.gz \. --output_gvcf_merged output/FAM1.g.vcf.gz\. --dry_run=true \. --vcf_stats_report=true. `. Thanks for advice.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/687
https://github.com/google/deepvariant/issues/688:356,availability,cluster,cluster,356,"No GPU usage when running GPU version of DV; Hi there,. I have no problem running the CPU version but the GPU version was not function normally either through slurm request or logging in to the GPU node. I found no GPU process through $nvidia-smi with my codes attached as follow:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. --bind /cluster/home/cx/variant_calling/slurm/DV/in:/in \. --bind /cluster/home/cx/variant_calling/slurm/DV/out:/out \. docker://google/deepvariant:1.5.0-gpu \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/in/GRCh37_primary/GRCh37.primary_assembly.genome.fa \. --reads=/in/SRR2962694.sorted.bam \. --regions=/in/all_19.bed \. --output_vcf=/out/SRR2962694.vcf.gz. ```. Here is the GPU status after running the code above, no process on GPU is running. ![微信图片_20230728102005](https://github.com/google/deepvariant/assets/80762999/ae17ac46-1c73-449e-9943-c627a9bc13a7).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/688
https://github.com/google/deepvariant/issues/688:415,availability,cluster,cluster,415,"No GPU usage when running GPU version of DV; Hi there,. I have no problem running the CPU version but the GPU version was not function normally either through slurm request or logging in to the GPU node. I found no GPU process through $nvidia-smi with my codes attached as follow:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. --bind /cluster/home/cx/variant_calling/slurm/DV/in:/in \. --bind /cluster/home/cx/variant_calling/slurm/DV/out:/out \. docker://google/deepvariant:1.5.0-gpu \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/in/GRCh37_primary/GRCh37.primary_assembly.genome.fa \. --reads=/in/SRR2962694.sorted.bam \. --regions=/in/all_19.bed \. --output_vcf=/out/SRR2962694.vcf.gz. ```. Here is the GPU status after running the code above, no process on GPU is running. ![微信图片_20230728102005](https://github.com/google/deepvariant/assets/80762999/ae17ac46-1c73-449e-9943-c627a9bc13a7).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/688
https://github.com/google/deepvariant/issues/688:30,deployability,version,version,30,"No GPU usage when running GPU version of DV; Hi there,. I have no problem running the CPU version but the GPU version was not function normally either through slurm request or logging in to the GPU node. I found no GPU process through $nvidia-smi with my codes attached as follow:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. --bind /cluster/home/cx/variant_calling/slurm/DV/in:/in \. --bind /cluster/home/cx/variant_calling/slurm/DV/out:/out \. docker://google/deepvariant:1.5.0-gpu \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/in/GRCh37_primary/GRCh37.primary_assembly.genome.fa \. --reads=/in/SRR2962694.sorted.bam \. --regions=/in/all_19.bed \. --output_vcf=/out/SRR2962694.vcf.gz. ```. Here is the GPU status after running the code above, no process on GPU is running. ![微信图片_20230728102005](https://github.com/google/deepvariant/assets/80762999/ae17ac46-1c73-449e-9943-c627a9bc13a7).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/688
https://github.com/google/deepvariant/issues/688:90,deployability,version,version,90,"No GPU usage when running GPU version of DV; Hi there,. I have no problem running the CPU version but the GPU version was not function normally either through slurm request or logging in to the GPU node. I found no GPU process through $nvidia-smi with my codes attached as follow:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. --bind /cluster/home/cx/variant_calling/slurm/DV/in:/in \. --bind /cluster/home/cx/variant_calling/slurm/DV/out:/out \. docker://google/deepvariant:1.5.0-gpu \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/in/GRCh37_primary/GRCh37.primary_assembly.genome.fa \. --reads=/in/SRR2962694.sorted.bam \. --regions=/in/all_19.bed \. --output_vcf=/out/SRR2962694.vcf.gz. ```. Here is the GPU status after running the code above, no process on GPU is running. ![微信图片_20230728102005](https://github.com/google/deepvariant/assets/80762999/ae17ac46-1c73-449e-9943-c627a9bc13a7).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/688
https://github.com/google/deepvariant/issues/688:110,deployability,version,version,110,"No GPU usage when running GPU version of DV; Hi there,. I have no problem running the CPU version but the GPU version was not function normally either through slurm request or logging in to the GPU node. I found no GPU process through $nvidia-smi with my codes attached as follow:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. --bind /cluster/home/cx/variant_calling/slurm/DV/in:/in \. --bind /cluster/home/cx/variant_calling/slurm/DV/out:/out \. docker://google/deepvariant:1.5.0-gpu \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/in/GRCh37_primary/GRCh37.primary_assembly.genome.fa \. --reads=/in/SRR2962694.sorted.bam \. --regions=/in/all_19.bed \. --output_vcf=/out/SRR2962694.vcf.gz. ```. Here is the GPU status after running the code above, no process on GPU is running. ![微信图片_20230728102005](https://github.com/google/deepvariant/assets/80762999/ae17ac46-1c73-449e-9943-c627a9bc13a7).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/688
https://github.com/google/deepvariant/issues/688:176,deployability,log,logging,176,"No GPU usage when running GPU version of DV; Hi there,. I have no problem running the CPU version but the GPU version was not function normally either through slurm request or logging in to the GPU node. I found no GPU process through $nvidia-smi with my codes attached as follow:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. --bind /cluster/home/cx/variant_calling/slurm/DV/in:/in \. --bind /cluster/home/cx/variant_calling/slurm/DV/out:/out \. docker://google/deepvariant:1.5.0-gpu \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/in/GRCh37_primary/GRCh37.primary_assembly.genome.fa \. --reads=/in/SRR2962694.sorted.bam \. --regions=/in/all_19.bed \. --output_vcf=/out/SRR2962694.vcf.gz. ```. Here is the GPU status after running the code above, no process on GPU is running. ![微信图片_20230728102005](https://github.com/google/deepvariant/assets/80762999/ae17ac46-1c73-449e-9943-c627a9bc13a7).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/688
https://github.com/google/deepvariant/issues/688:356,deployability,cluster,cluster,356,"No GPU usage when running GPU version of DV; Hi there,. I have no problem running the CPU version but the GPU version was not function normally either through slurm request or logging in to the GPU node. I found no GPU process through $nvidia-smi with my codes attached as follow:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. --bind /cluster/home/cx/variant_calling/slurm/DV/in:/in \. --bind /cluster/home/cx/variant_calling/slurm/DV/out:/out \. docker://google/deepvariant:1.5.0-gpu \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/in/GRCh37_primary/GRCh37.primary_assembly.genome.fa \. --reads=/in/SRR2962694.sorted.bam \. --regions=/in/all_19.bed \. --output_vcf=/out/SRR2962694.vcf.gz. ```. Here is the GPU status after running the code above, no process on GPU is running. ![微信图片_20230728102005](https://github.com/google/deepvariant/assets/80762999/ae17ac46-1c73-449e-9943-c627a9bc13a7).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/688
https://github.com/google/deepvariant/issues/688:415,deployability,cluster,cluster,415,"No GPU usage when running GPU version of DV; Hi there,. I have no problem running the CPU version but the GPU version was not function normally either through slurm request or logging in to the GPU node. I found no GPU process through $nvidia-smi with my codes attached as follow:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. --bind /cluster/home/cx/variant_calling/slurm/DV/in:/in \. --bind /cluster/home/cx/variant_calling/slurm/DV/out:/out \. docker://google/deepvariant:1.5.0-gpu \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/in/GRCh37_primary/GRCh37.primary_assembly.genome.fa \. --reads=/in/SRR2962694.sorted.bam \. --regions=/in/all_19.bed \. --output_vcf=/out/SRR2962694.vcf.gz. ```. Here is the GPU status after running the code above, no process on GPU is running. ![微信图片_20230728102005](https://github.com/google/deepvariant/assets/80762999/ae17ac46-1c73-449e-9943-c627a9bc13a7).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/688
https://github.com/google/deepvariant/issues/688:3,energy efficiency,GPU,GPU,3,"No GPU usage when running GPU version of DV; Hi there,. I have no problem running the CPU version but the GPU version was not function normally either through slurm request or logging in to the GPU node. I found no GPU process through $nvidia-smi with my codes attached as follow:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. --bind /cluster/home/cx/variant_calling/slurm/DV/in:/in \. --bind /cluster/home/cx/variant_calling/slurm/DV/out:/out \. docker://google/deepvariant:1.5.0-gpu \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/in/GRCh37_primary/GRCh37.primary_assembly.genome.fa \. --reads=/in/SRR2962694.sorted.bam \. --regions=/in/all_19.bed \. --output_vcf=/out/SRR2962694.vcf.gz. ```. Here is the GPU status after running the code above, no process on GPU is running. ![微信图片_20230728102005](https://github.com/google/deepvariant/assets/80762999/ae17ac46-1c73-449e-9943-c627a9bc13a7).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/688
https://github.com/google/deepvariant/issues/688:26,energy efficiency,GPU,GPU,26,"No GPU usage when running GPU version of DV; Hi there,. I have no problem running the CPU version but the GPU version was not function normally either through slurm request or logging in to the GPU node. I found no GPU process through $nvidia-smi with my codes attached as follow:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. --bind /cluster/home/cx/variant_calling/slurm/DV/in:/in \. --bind /cluster/home/cx/variant_calling/slurm/DV/out:/out \. docker://google/deepvariant:1.5.0-gpu \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/in/GRCh37_primary/GRCh37.primary_assembly.genome.fa \. --reads=/in/SRR2962694.sorted.bam \. --regions=/in/all_19.bed \. --output_vcf=/out/SRR2962694.vcf.gz. ```. Here is the GPU status after running the code above, no process on GPU is running. ![微信图片_20230728102005](https://github.com/google/deepvariant/assets/80762999/ae17ac46-1c73-449e-9943-c627a9bc13a7).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/688
https://github.com/google/deepvariant/issues/688:86,energy efficiency,CPU,CPU,86,"No GPU usage when running GPU version of DV; Hi there,. I have no problem running the CPU version but the GPU version was not function normally either through slurm request or logging in to the GPU node. I found no GPU process through $nvidia-smi with my codes attached as follow:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. --bind /cluster/home/cx/variant_calling/slurm/DV/in:/in \. --bind /cluster/home/cx/variant_calling/slurm/DV/out:/out \. docker://google/deepvariant:1.5.0-gpu \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/in/GRCh37_primary/GRCh37.primary_assembly.genome.fa \. --reads=/in/SRR2962694.sorted.bam \. --regions=/in/all_19.bed \. --output_vcf=/out/SRR2962694.vcf.gz. ```. Here is the GPU status after running the code above, no process on GPU is running. ![微信图片_20230728102005](https://github.com/google/deepvariant/assets/80762999/ae17ac46-1c73-449e-9943-c627a9bc13a7).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/688
https://github.com/google/deepvariant/issues/688:106,energy efficiency,GPU,GPU,106,"No GPU usage when running GPU version of DV; Hi there,. I have no problem running the CPU version but the GPU version was not function normally either through slurm request or logging in to the GPU node. I found no GPU process through $nvidia-smi with my codes attached as follow:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. --bind /cluster/home/cx/variant_calling/slurm/DV/in:/in \. --bind /cluster/home/cx/variant_calling/slurm/DV/out:/out \. docker://google/deepvariant:1.5.0-gpu \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/in/GRCh37_primary/GRCh37.primary_assembly.genome.fa \. --reads=/in/SRR2962694.sorted.bam \. --regions=/in/all_19.bed \. --output_vcf=/out/SRR2962694.vcf.gz. ```. Here is the GPU status after running the code above, no process on GPU is running. ![微信图片_20230728102005](https://github.com/google/deepvariant/assets/80762999/ae17ac46-1c73-449e-9943-c627a9bc13a7).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/688
https://github.com/google/deepvariant/issues/688:194,energy efficiency,GPU,GPU,194,"No GPU usage when running GPU version of DV; Hi there,. I have no problem running the CPU version but the GPU version was not function normally either through slurm request or logging in to the GPU node. I found no GPU process through $nvidia-smi with my codes attached as follow:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. --bind /cluster/home/cx/variant_calling/slurm/DV/in:/in \. --bind /cluster/home/cx/variant_calling/slurm/DV/out:/out \. docker://google/deepvariant:1.5.0-gpu \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/in/GRCh37_primary/GRCh37.primary_assembly.genome.fa \. --reads=/in/SRR2962694.sorted.bam \. --regions=/in/all_19.bed \. --output_vcf=/out/SRR2962694.vcf.gz. ```. Here is the GPU status after running the code above, no process on GPU is running. ![微信图片_20230728102005](https://github.com/google/deepvariant/assets/80762999/ae17ac46-1c73-449e-9943-c627a9bc13a7).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/688
https://github.com/google/deepvariant/issues/688:215,energy efficiency,GPU,GPU,215,"No GPU usage when running GPU version of DV; Hi there,. I have no problem running the CPU version but the GPU version was not function normally either through slurm request or logging in to the GPU node. I found no GPU process through $nvidia-smi with my codes attached as follow:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. --bind /cluster/home/cx/variant_calling/slurm/DV/in:/in \. --bind /cluster/home/cx/variant_calling/slurm/DV/out:/out \. docker://google/deepvariant:1.5.0-gpu \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/in/GRCh37_primary/GRCh37.primary_assembly.genome.fa \. --reads=/in/SRR2962694.sorted.bam \. --regions=/in/all_19.bed \. --output_vcf=/out/SRR2962694.vcf.gz. ```. Here is the GPU status after running the code above, no process on GPU is running. ![微信图片_20230728102005](https://github.com/google/deepvariant/assets/80762999/ae17ac46-1c73-449e-9943-c627a9bc13a7).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/688
https://github.com/google/deepvariant/issues/688:502,energy efficiency,gpu,gpu,502,"No GPU usage when running GPU version of DV; Hi there,. I have no problem running the CPU version but the GPU version was not function normally either through slurm request or logging in to the GPU node. I found no GPU process through $nvidia-smi with my codes attached as follow:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. --bind /cluster/home/cx/variant_calling/slurm/DV/in:/in \. --bind /cluster/home/cx/variant_calling/slurm/DV/out:/out \. docker://google/deepvariant:1.5.0-gpu \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/in/GRCh37_primary/GRCh37.primary_assembly.genome.fa \. --reads=/in/SRR2962694.sorted.bam \. --regions=/in/all_19.bed \. --output_vcf=/out/SRR2962694.vcf.gz. ```. Here is the GPU status after running the code above, no process on GPU is running. ![微信图片_20230728102005](https://github.com/google/deepvariant/assets/80762999/ae17ac46-1c73-449e-9943-c627a9bc13a7).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/688
https://github.com/google/deepvariant/issues/688:750,energy efficiency,GPU,GPU,750,"No GPU usage when running GPU version of DV; Hi there,. I have no problem running the CPU version but the GPU version was not function normally either through slurm request or logging in to the GPU node. I found no GPU process through $nvidia-smi with my codes attached as follow:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. --bind /cluster/home/cx/variant_calling/slurm/DV/in:/in \. --bind /cluster/home/cx/variant_calling/slurm/DV/out:/out \. docker://google/deepvariant:1.5.0-gpu \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/in/GRCh37_primary/GRCh37.primary_assembly.genome.fa \. --reads=/in/SRR2962694.sorted.bam \. --regions=/in/all_19.bed \. --output_vcf=/out/SRR2962694.vcf.gz. ```. Here is the GPU status after running the code above, no process on GPU is running. ![微信图片_20230728102005](https://github.com/google/deepvariant/assets/80762999/ae17ac46-1c73-449e-9943-c627a9bc13a7).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/688
https://github.com/google/deepvariant/issues/688:805,energy efficiency,GPU,GPU,805,"No GPU usage when running GPU version of DV; Hi there,. I have no problem running the CPU version but the GPU version was not function normally either through slurm request or logging in to the GPU node. I found no GPU process through $nvidia-smi with my codes attached as follow:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. --bind /cluster/home/cx/variant_calling/slurm/DV/in:/in \. --bind /cluster/home/cx/variant_calling/slurm/DV/out:/out \. docker://google/deepvariant:1.5.0-gpu \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/in/GRCh37_primary/GRCh37.primary_assembly.genome.fa \. --reads=/in/SRR2962694.sorted.bam \. --regions=/in/all_19.bed \. --output_vcf=/out/SRR2962694.vcf.gz. ```. Here is the GPU status after running the code above, no process on GPU is running. ![微信图片_20230728102005](https://github.com/google/deepvariant/assets/80762999/ae17ac46-1c73-449e-9943-c627a9bc13a7).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/688
https://github.com/google/deepvariant/issues/688:30,integrability,version,version,30,"No GPU usage when running GPU version of DV; Hi there,. I have no problem running the CPU version but the GPU version was not function normally either through slurm request or logging in to the GPU node. I found no GPU process through $nvidia-smi with my codes attached as follow:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. --bind /cluster/home/cx/variant_calling/slurm/DV/in:/in \. --bind /cluster/home/cx/variant_calling/slurm/DV/out:/out \. docker://google/deepvariant:1.5.0-gpu \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/in/GRCh37_primary/GRCh37.primary_assembly.genome.fa \. --reads=/in/SRR2962694.sorted.bam \. --regions=/in/all_19.bed \. --output_vcf=/out/SRR2962694.vcf.gz. ```. Here is the GPU status after running the code above, no process on GPU is running. ![微信图片_20230728102005](https://github.com/google/deepvariant/assets/80762999/ae17ac46-1c73-449e-9943-c627a9bc13a7).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/688
https://github.com/google/deepvariant/issues/688:90,integrability,version,version,90,"No GPU usage when running GPU version of DV; Hi there,. I have no problem running the CPU version but the GPU version was not function normally either through slurm request or logging in to the GPU node. I found no GPU process through $nvidia-smi with my codes attached as follow:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. --bind /cluster/home/cx/variant_calling/slurm/DV/in:/in \. --bind /cluster/home/cx/variant_calling/slurm/DV/out:/out \. docker://google/deepvariant:1.5.0-gpu \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/in/GRCh37_primary/GRCh37.primary_assembly.genome.fa \. --reads=/in/SRR2962694.sorted.bam \. --regions=/in/all_19.bed \. --output_vcf=/out/SRR2962694.vcf.gz. ```. Here is the GPU status after running the code above, no process on GPU is running. ![微信图片_20230728102005](https://github.com/google/deepvariant/assets/80762999/ae17ac46-1c73-449e-9943-c627a9bc13a7).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/688
https://github.com/google/deepvariant/issues/688:110,integrability,version,version,110,"No GPU usage when running GPU version of DV; Hi there,. I have no problem running the CPU version but the GPU version was not function normally either through slurm request or logging in to the GPU node. I found no GPU process through $nvidia-smi with my codes attached as follow:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. --bind /cluster/home/cx/variant_calling/slurm/DV/in:/in \. --bind /cluster/home/cx/variant_calling/slurm/DV/out:/out \. docker://google/deepvariant:1.5.0-gpu \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/in/GRCh37_primary/GRCh37.primary_assembly.genome.fa \. --reads=/in/SRR2962694.sorted.bam \. --regions=/in/all_19.bed \. --output_vcf=/out/SRR2962694.vcf.gz. ```. Here is the GPU status after running the code above, no process on GPU is running. ![微信图片_20230728102005](https://github.com/google/deepvariant/assets/80762999/ae17ac46-1c73-449e-9943-c627a9bc13a7).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/688
https://github.com/google/deepvariant/issues/688:350,interoperability,bind,bind,350,"No GPU usage when running GPU version of DV; Hi there,. I have no problem running the CPU version but the GPU version was not function normally either through slurm request or logging in to the GPU node. I found no GPU process through $nvidia-smi with my codes attached as follow:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. --bind /cluster/home/cx/variant_calling/slurm/DV/in:/in \. --bind /cluster/home/cx/variant_calling/slurm/DV/out:/out \. docker://google/deepvariant:1.5.0-gpu \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/in/GRCh37_primary/GRCh37.primary_assembly.genome.fa \. --reads=/in/SRR2962694.sorted.bam \. --regions=/in/all_19.bed \. --output_vcf=/out/SRR2962694.vcf.gz. ```. Here is the GPU status after running the code above, no process on GPU is running. ![微信图片_20230728102005](https://github.com/google/deepvariant/assets/80762999/ae17ac46-1c73-449e-9943-c627a9bc13a7).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/688
https://github.com/google/deepvariant/issues/688:409,interoperability,bind,bind,409,"No GPU usage when running GPU version of DV; Hi there,. I have no problem running the CPU version but the GPU version was not function normally either through slurm request or logging in to the GPU node. I found no GPU process through $nvidia-smi with my codes attached as follow:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. --bind /cluster/home/cx/variant_calling/slurm/DV/in:/in \. --bind /cluster/home/cx/variant_calling/slurm/DV/out:/out \. docker://google/deepvariant:1.5.0-gpu \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/in/GRCh37_primary/GRCh37.primary_assembly.genome.fa \. --reads=/in/SRR2962694.sorted.bam \. --regions=/in/all_19.bed \. --output_vcf=/out/SRR2962694.vcf.gz. ```. Here is the GPU status after running the code above, no process on GPU is running. ![微信图片_20230728102005](https://github.com/google/deepvariant/assets/80762999/ae17ac46-1c73-449e-9943-c627a9bc13a7).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/688
https://github.com/google/deepvariant/issues/688:30,modifiability,version,version,30,"No GPU usage when running GPU version of DV; Hi there,. I have no problem running the CPU version but the GPU version was not function normally either through slurm request or logging in to the GPU node. I found no GPU process through $nvidia-smi with my codes attached as follow:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. --bind /cluster/home/cx/variant_calling/slurm/DV/in:/in \. --bind /cluster/home/cx/variant_calling/slurm/DV/out:/out \. docker://google/deepvariant:1.5.0-gpu \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/in/GRCh37_primary/GRCh37.primary_assembly.genome.fa \. --reads=/in/SRR2962694.sorted.bam \. --regions=/in/all_19.bed \. --output_vcf=/out/SRR2962694.vcf.gz. ```. Here is the GPU status after running the code above, no process on GPU is running. ![微信图片_20230728102005](https://github.com/google/deepvariant/assets/80762999/ae17ac46-1c73-449e-9943-c627a9bc13a7).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/688
https://github.com/google/deepvariant/issues/688:90,modifiability,version,version,90,"No GPU usage when running GPU version of DV; Hi there,. I have no problem running the CPU version but the GPU version was not function normally either through slurm request or logging in to the GPU node. I found no GPU process through $nvidia-smi with my codes attached as follow:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. --bind /cluster/home/cx/variant_calling/slurm/DV/in:/in \. --bind /cluster/home/cx/variant_calling/slurm/DV/out:/out \. docker://google/deepvariant:1.5.0-gpu \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/in/GRCh37_primary/GRCh37.primary_assembly.genome.fa \. --reads=/in/SRR2962694.sorted.bam \. --regions=/in/all_19.bed \. --output_vcf=/out/SRR2962694.vcf.gz. ```. Here is the GPU status after running the code above, no process on GPU is running. ![微信图片_20230728102005](https://github.com/google/deepvariant/assets/80762999/ae17ac46-1c73-449e-9943-c627a9bc13a7).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/688
https://github.com/google/deepvariant/issues/688:110,modifiability,version,version,110,"No GPU usage when running GPU version of DV; Hi there,. I have no problem running the CPU version but the GPU version was not function normally either through slurm request or logging in to the GPU node. I found no GPU process through $nvidia-smi with my codes attached as follow:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. --bind /cluster/home/cx/variant_calling/slurm/DV/in:/in \. --bind /cluster/home/cx/variant_calling/slurm/DV/out:/out \. docker://google/deepvariant:1.5.0-gpu \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/in/GRCh37_primary/GRCh37.primary_assembly.genome.fa \. --reads=/in/SRR2962694.sorted.bam \. --regions=/in/all_19.bed \. --output_vcf=/out/SRR2962694.vcf.gz. ```. Here is the GPU status after running the code above, no process on GPU is running. ![微信图片_20230728102005](https://github.com/google/deepvariant/assets/80762999/ae17ac46-1c73-449e-9943-c627a9bc13a7).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/688
https://github.com/google/deepvariant/issues/688:350,modifiability,bind,bind,350,"No GPU usage when running GPU version of DV; Hi there,. I have no problem running the CPU version but the GPU version was not function normally either through slurm request or logging in to the GPU node. I found no GPU process through $nvidia-smi with my codes attached as follow:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. --bind /cluster/home/cx/variant_calling/slurm/DV/in:/in \. --bind /cluster/home/cx/variant_calling/slurm/DV/out:/out \. docker://google/deepvariant:1.5.0-gpu \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/in/GRCh37_primary/GRCh37.primary_assembly.genome.fa \. --reads=/in/SRR2962694.sorted.bam \. --regions=/in/all_19.bed \. --output_vcf=/out/SRR2962694.vcf.gz. ```. Here is the GPU status after running the code above, no process on GPU is running. ![微信图片_20230728102005](https://github.com/google/deepvariant/assets/80762999/ae17ac46-1c73-449e-9943-c627a9bc13a7).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/688
https://github.com/google/deepvariant/issues/688:409,modifiability,bind,bind,409,"No GPU usage when running GPU version of DV; Hi there,. I have no problem running the CPU version but the GPU version was not function normally either through slurm request or logging in to the GPU node. I found no GPU process through $nvidia-smi with my codes attached as follow:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. --bind /cluster/home/cx/variant_calling/slurm/DV/in:/in \. --bind /cluster/home/cx/variant_calling/slurm/DV/out:/out \. docker://google/deepvariant:1.5.0-gpu \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/in/GRCh37_primary/GRCh37.primary_assembly.genome.fa \. --reads=/in/SRR2962694.sorted.bam \. --regions=/in/all_19.bed \. --output_vcf=/out/SRR2962694.vcf.gz. ```. Here is the GPU status after running the code above, no process on GPU is running. ![微信图片_20230728102005](https://github.com/google/deepvariant/assets/80762999/ae17ac46-1c73-449e-9943-c627a9bc13a7).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/688
https://github.com/google/deepvariant/issues/688:3,performance,GPU,GPU,3,"No GPU usage when running GPU version of DV; Hi there,. I have no problem running the CPU version but the GPU version was not function normally either through slurm request or logging in to the GPU node. I found no GPU process through $nvidia-smi with my codes attached as follow:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. --bind /cluster/home/cx/variant_calling/slurm/DV/in:/in \. --bind /cluster/home/cx/variant_calling/slurm/DV/out:/out \. docker://google/deepvariant:1.5.0-gpu \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/in/GRCh37_primary/GRCh37.primary_assembly.genome.fa \. --reads=/in/SRR2962694.sorted.bam \. --regions=/in/all_19.bed \. --output_vcf=/out/SRR2962694.vcf.gz. ```. Here is the GPU status after running the code above, no process on GPU is running. ![微信图片_20230728102005](https://github.com/google/deepvariant/assets/80762999/ae17ac46-1c73-449e-9943-c627a9bc13a7).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/688
https://github.com/google/deepvariant/issues/688:26,performance,GPU,GPU,26,"No GPU usage when running GPU version of DV; Hi there,. I have no problem running the CPU version but the GPU version was not function normally either through slurm request or logging in to the GPU node. I found no GPU process through $nvidia-smi with my codes attached as follow:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. --bind /cluster/home/cx/variant_calling/slurm/DV/in:/in \. --bind /cluster/home/cx/variant_calling/slurm/DV/out:/out \. docker://google/deepvariant:1.5.0-gpu \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/in/GRCh37_primary/GRCh37.primary_assembly.genome.fa \. --reads=/in/SRR2962694.sorted.bam \. --regions=/in/all_19.bed \. --output_vcf=/out/SRR2962694.vcf.gz. ```. Here is the GPU status after running the code above, no process on GPU is running. ![微信图片_20230728102005](https://github.com/google/deepvariant/assets/80762999/ae17ac46-1c73-449e-9943-c627a9bc13a7).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/688
https://github.com/google/deepvariant/issues/688:86,performance,CPU,CPU,86,"No GPU usage when running GPU version of DV; Hi there,. I have no problem running the CPU version but the GPU version was not function normally either through slurm request or logging in to the GPU node. I found no GPU process through $nvidia-smi with my codes attached as follow:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. --bind /cluster/home/cx/variant_calling/slurm/DV/in:/in \. --bind /cluster/home/cx/variant_calling/slurm/DV/out:/out \. docker://google/deepvariant:1.5.0-gpu \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/in/GRCh37_primary/GRCh37.primary_assembly.genome.fa \. --reads=/in/SRR2962694.sorted.bam \. --regions=/in/all_19.bed \. --output_vcf=/out/SRR2962694.vcf.gz. ```. Here is the GPU status after running the code above, no process on GPU is running. ![微信图片_20230728102005](https://github.com/google/deepvariant/assets/80762999/ae17ac46-1c73-449e-9943-c627a9bc13a7).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/688
https://github.com/google/deepvariant/issues/688:106,performance,GPU,GPU,106,"No GPU usage when running GPU version of DV; Hi there,. I have no problem running the CPU version but the GPU version was not function normally either through slurm request or logging in to the GPU node. I found no GPU process through $nvidia-smi with my codes attached as follow:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. --bind /cluster/home/cx/variant_calling/slurm/DV/in:/in \. --bind /cluster/home/cx/variant_calling/slurm/DV/out:/out \. docker://google/deepvariant:1.5.0-gpu \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/in/GRCh37_primary/GRCh37.primary_assembly.genome.fa \. --reads=/in/SRR2962694.sorted.bam \. --regions=/in/all_19.bed \. --output_vcf=/out/SRR2962694.vcf.gz. ```. Here is the GPU status after running the code above, no process on GPU is running. ![微信图片_20230728102005](https://github.com/google/deepvariant/assets/80762999/ae17ac46-1c73-449e-9943-c627a9bc13a7).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/688
https://github.com/google/deepvariant/issues/688:194,performance,GPU,GPU,194,"No GPU usage when running GPU version of DV; Hi there,. I have no problem running the CPU version but the GPU version was not function normally either through slurm request or logging in to the GPU node. I found no GPU process through $nvidia-smi with my codes attached as follow:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. --bind /cluster/home/cx/variant_calling/slurm/DV/in:/in \. --bind /cluster/home/cx/variant_calling/slurm/DV/out:/out \. docker://google/deepvariant:1.5.0-gpu \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/in/GRCh37_primary/GRCh37.primary_assembly.genome.fa \. --reads=/in/SRR2962694.sorted.bam \. --regions=/in/all_19.bed \. --output_vcf=/out/SRR2962694.vcf.gz. ```. Here is the GPU status after running the code above, no process on GPU is running. ![微信图片_20230728102005](https://github.com/google/deepvariant/assets/80762999/ae17ac46-1c73-449e-9943-c627a9bc13a7).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/688
https://github.com/google/deepvariant/issues/688:215,performance,GPU,GPU,215,"No GPU usage when running GPU version of DV; Hi there,. I have no problem running the CPU version but the GPU version was not function normally either through slurm request or logging in to the GPU node. I found no GPU process through $nvidia-smi with my codes attached as follow:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. --bind /cluster/home/cx/variant_calling/slurm/DV/in:/in \. --bind /cluster/home/cx/variant_calling/slurm/DV/out:/out \. docker://google/deepvariant:1.5.0-gpu \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/in/GRCh37_primary/GRCh37.primary_assembly.genome.fa \. --reads=/in/SRR2962694.sorted.bam \. --regions=/in/all_19.bed \. --output_vcf=/out/SRR2962694.vcf.gz. ```. Here is the GPU status after running the code above, no process on GPU is running. ![微信图片_20230728102005](https://github.com/google/deepvariant/assets/80762999/ae17ac46-1c73-449e-9943-c627a9bc13a7).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/688
https://github.com/google/deepvariant/issues/688:502,performance,gpu,gpu,502,"No GPU usage when running GPU version of DV; Hi there,. I have no problem running the CPU version but the GPU version was not function normally either through slurm request or logging in to the GPU node. I found no GPU process through $nvidia-smi with my codes attached as follow:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. --bind /cluster/home/cx/variant_calling/slurm/DV/in:/in \. --bind /cluster/home/cx/variant_calling/slurm/DV/out:/out \. docker://google/deepvariant:1.5.0-gpu \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/in/GRCh37_primary/GRCh37.primary_assembly.genome.fa \. --reads=/in/SRR2962694.sorted.bam \. --regions=/in/all_19.bed \. --output_vcf=/out/SRR2962694.vcf.gz. ```. Here is the GPU status after running the code above, no process on GPU is running. ![微信图片_20230728102005](https://github.com/google/deepvariant/assets/80762999/ae17ac46-1c73-449e-9943-c627a9bc13a7).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/688
https://github.com/google/deepvariant/issues/688:750,performance,GPU,GPU,750,"No GPU usage when running GPU version of DV; Hi there,. I have no problem running the CPU version but the GPU version was not function normally either through slurm request or logging in to the GPU node. I found no GPU process through $nvidia-smi with my codes attached as follow:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. --bind /cluster/home/cx/variant_calling/slurm/DV/in:/in \. --bind /cluster/home/cx/variant_calling/slurm/DV/out:/out \. docker://google/deepvariant:1.5.0-gpu \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/in/GRCh37_primary/GRCh37.primary_assembly.genome.fa \. --reads=/in/SRR2962694.sorted.bam \. --regions=/in/all_19.bed \. --output_vcf=/out/SRR2962694.vcf.gz. ```. Here is the GPU status after running the code above, no process on GPU is running. ![微信图片_20230728102005](https://github.com/google/deepvariant/assets/80762999/ae17ac46-1c73-449e-9943-c627a9bc13a7).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/688
https://github.com/google/deepvariant/issues/688:805,performance,GPU,GPU,805,"No GPU usage when running GPU version of DV; Hi there,. I have no problem running the CPU version but the GPU version was not function normally either through slurm request or logging in to the GPU node. I found no GPU process through $nvidia-smi with my codes attached as follow:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. --bind /cluster/home/cx/variant_calling/slurm/DV/in:/in \. --bind /cluster/home/cx/variant_calling/slurm/DV/out:/out \. docker://google/deepvariant:1.5.0-gpu \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/in/GRCh37_primary/GRCh37.primary_assembly.genome.fa \. --reads=/in/SRR2962694.sorted.bam \. --regions=/in/all_19.bed \. --output_vcf=/out/SRR2962694.vcf.gz. ```. Here is the GPU status after running the code above, no process on GPU is running. ![微信图片_20230728102005](https://github.com/google/deepvariant/assets/80762999/ae17ac46-1c73-449e-9943-c627a9bc13a7).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/688
https://github.com/google/deepvariant/issues/688:176,safety,log,logging,176,"No GPU usage when running GPU version of DV; Hi there,. I have no problem running the CPU version but the GPU version was not function normally either through slurm request or logging in to the GPU node. I found no GPU process through $nvidia-smi with my codes attached as follow:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. --bind /cluster/home/cx/variant_calling/slurm/DV/in:/in \. --bind /cluster/home/cx/variant_calling/slurm/DV/out:/out \. docker://google/deepvariant:1.5.0-gpu \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/in/GRCh37_primary/GRCh37.primary_assembly.genome.fa \. --reads=/in/SRR2962694.sorted.bam \. --regions=/in/all_19.bed \. --output_vcf=/out/SRR2962694.vcf.gz. ```. Here is the GPU status after running the code above, no process on GPU is running. ![微信图片_20230728102005](https://github.com/google/deepvariant/assets/80762999/ae17ac46-1c73-449e-9943-c627a9bc13a7).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/688
https://github.com/google/deepvariant/issues/688:176,security,log,logging,176,"No GPU usage when running GPU version of DV; Hi there,. I have no problem running the CPU version but the GPU version was not function normally either through slurm request or logging in to the GPU node. I found no GPU process through $nvidia-smi with my codes attached as follow:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. --bind /cluster/home/cx/variant_calling/slurm/DV/in:/in \. --bind /cluster/home/cx/variant_calling/slurm/DV/out:/out \. docker://google/deepvariant:1.5.0-gpu \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/in/GRCh37_primary/GRCh37.primary_assembly.genome.fa \. --reads=/in/SRR2962694.sorted.bam \. --regions=/in/all_19.bed \. --output_vcf=/out/SRR2962694.vcf.gz. ```. Here is the GPU status after running the code above, no process on GPU is running. ![微信图片_20230728102005](https://github.com/google/deepvariant/assets/80762999/ae17ac46-1c73-449e-9943-c627a9bc13a7).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/688
https://github.com/google/deepvariant/issues/688:176,testability,log,logging,176,"No GPU usage when running GPU version of DV; Hi there,. I have no problem running the CPU version but the GPU version was not function normally either through slurm request or logging in to the GPU node. I found no GPU process through $nvidia-smi with my codes attached as follow:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. --bind /cluster/home/cx/variant_calling/slurm/DV/in:/in \. --bind /cluster/home/cx/variant_calling/slurm/DV/out:/out \. docker://google/deepvariant:1.5.0-gpu \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/in/GRCh37_primary/GRCh37.primary_assembly.genome.fa \. --reads=/in/SRR2962694.sorted.bam \. --regions=/in/all_19.bed \. --output_vcf=/out/SRR2962694.vcf.gz. ```. Here is the GPU status after running the code above, no process on GPU is running. ![微信图片_20230728102005](https://github.com/google/deepvariant/assets/80762999/ae17ac46-1c73-449e-9943-c627a9bc13a7).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/688
https://github.com/google/deepvariant/issues/688:754,usability,statu,status,754,"No GPU usage when running GPU version of DV; Hi there,. I have no problem running the CPU version but the GPU version was not function normally either through slurm request or logging in to the GPU node. I found no GPU process through $nvidia-smi with my codes attached as follow:. ```. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \. --bind /cluster/home/cx/variant_calling/slurm/DV/in:/in \. --bind /cluster/home/cx/variant_calling/slurm/DV/out:/out \. docker://google/deepvariant:1.5.0-gpu \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=/in/GRCh37_primary/GRCh37.primary_assembly.genome.fa \. --reads=/in/SRR2962694.sorted.bam \. --regions=/in/all_19.bed \. --output_vcf=/out/SRR2962694.vcf.gz. ```. Here is the GPU status after running the code above, no process on GPU is running. ![微信图片_20230728102005](https://github.com/google/deepvariant/assets/80762999/ae17ac46-1c73-449e-9943-c627a9bc13a7).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/688
https://github.com/google/deepvariant/issues/689:1048,availability,echo,echo,1048,"tion and GIAB dataset evaluation; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. yes，i have checked this FAQ document. . **Describe the issue:**. I used Deeprio v1.4.0 version to perform family analysis on three samples, HG002, HG003, and HG004, and evaluated the accuracy of mutation detection using the GIAB database (NISTv4.2.1). I found that the evaluation results through GIAB were particularly unsatisfactory, but I used the same data and Deepvariant v1.5.0 for single sample analysis, and the evaluation results were very ideal, I don't quite understand why the analysis results of a single sample perform so well at the evaluation level compared to the results of family analysis, and why the results of family analysis are relatively poor. The following is my family analysis and analysis code for individual samples, as well as the evaluation results of the GIAB database, for developers to review:. Data comparison to reference genome:. ```. echo HG002.merged.fastq.gz > HG002.fofn . pbmm2 align \. --preset HIFI \. genome/hg38.fa.mmi \. HG002.fofn \. --sample HG002 \. -j 10 \. HG002.aligned.tmp.bam . samtools sort -@ 10 HG002.aligned.tmp.bam -O BAM -o HG002.aligned.tmp.sort.bam . samtools index -@ 10 HG002.aligned.tmp.sort.bam . chromosomes=(chr1 chr2 chr3 chr4 chr5 chr6 chr7 chr8 chr9 chr10 chr11 chr12 chr13 chr14 chr15 chr16 chr17 chr18 chr19 chr20 chr21 chr22 chrX chrY chrM) . for chromosome in ""${chromosomes[@]}""; \. do \. samtools view -@ 2 -b -h HG002.aligned.tmp.sort.bam ""$chromosome"" --output HG002.aligned.$chromosome.tmp.bam & . done . wait . samtools merge HG002.aligned.bam HG002.aligned.chr*.tmp.bam . samtools sort -@ 10 HG002.aligned.bam -O BAM -o HG002.sort.bam . samtools index -@ 10 HG002.sort.bam . ```. Family analysis code:. ```. rm -rf chr20_GLnexus.DB tmp_ramdom_TrioDemo_chr20 . samtools view --write-index --threads 10 -h -b -S HG002.sort.bam chr20 -O BAM -o HG002.chr20.sort.bam . samtools view --write-index",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:6602,availability,Operat,Operating,6602,"-|. | HG002 | SNP | PASS | 3365127 | 3361925 | 3202 | 4312006 | 7891 | 942045 | 1197 | 719 | 0.999048 | 0.997658 | 0.21847 | 0.998353 | 2.100128487 | 1.823839956 | 1.581195853 | 1.50423718 |. | HG003 | INDEL | PASS | 504501 | 501414 | 3087 | 1009147 | 3989 | 471526 | 1814 | 1831 | 0.993881 | 0.99258 | 0.467252 | 0.99323 | | | 1.489759281 | 2.02565724 |. | HG003 | SNP | PASS | 3327495 | 3323623 | 3872 | 4265460 | 4910 | 936912 | 1118 | 617 | 0.998836 | 0.998525 | 0.219651 | 0.998681 | 2.102574954 | 1.831128594 | 1.535137772 | 1.484295493 |. | HG004 | INDEL | PASS | 510519 | 507376 | 3143 | 1013737 | 4102 | 469356 | 1887 | 1729 | 0.993844 | 0.992465 | 0.462996 | 0.993154 | | | 1.516130736 | 2.075927402 |. analysising result：Using the same test data as the scattered samples, it can be found that the variation detection results of the HG002/3/4 family sample are relatively poor when tested using the GIAB standard set，but I don't understand the reason for this difference. **Setup**. - Operating system: image of singularity, transforming from docker image of deeptrio-1.4.0. - DeepVariant version:deeptrio-1.4.0. - Installation method (Docker, built from source, etc.):Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). HiFi data,those data download links follows:. * HG002:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG002/hpp_HG002_NA24385_son_v1/PacBio_HiFi/15kb/;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG002/hpp_HG002_NA24385_son_v1/PacBio_HiFi/20kb/. * HG003:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG003/PacBio_HiFi/Google_15kb;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG003/PacBio_HiFi/HudsonAlpha_15kb. * HG004:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG004/PacBio_HiFi/Google_15kb/;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG004/PacBio_HiFi",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:6925,availability,down,download,6925,"| | | 1.489759281 | 2.02565724 |. | HG003 | SNP | PASS | 3327495 | 3323623 | 3872 | 4265460 | 4910 | 936912 | 1118 | 617 | 0.998836 | 0.998525 | 0.219651 | 0.998681 | 2.102574954 | 1.831128594 | 1.535137772 | 1.484295493 |. | HG004 | INDEL | PASS | 510519 | 507376 | 3143 | 1013737 | 4102 | 469356 | 1887 | 1729 | 0.993844 | 0.992465 | 0.462996 | 0.993154 | | | 1.516130736 | 2.075927402 |. analysising result：Using the same test data as the scattered samples, it can be found that the variation detection results of the HG002/3/4 family sample are relatively poor when tested using the GIAB standard set，but I don't understand the reason for this difference. **Setup**. - Operating system: image of singularity, transforming from docker image of deeptrio-1.4.0. - DeepVariant version:deeptrio-1.4.0. - Installation method (Docker, built from source, etc.):Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). HiFi data,those data download links follows:. * HG002:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG002/hpp_HG002_NA24385_son_v1/PacBio_HiFi/15kb/;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG002/hpp_HG002_NA24385_son_v1/PacBio_HiFi/20kb/. * HG003:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG003/PacBio_HiFi/Google_15kb;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG003/PacBio_HiFi/HudsonAlpha_15kb. * HG004:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG004/PacBio_HiFi/Google_15kb/;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG004/PacBio_HiFi/HudsonAlpha_15kb/PBmixSequel733_2_B01_PBSU_30hours_15kbV2PD_70pM_HumanHG004_CCS/. **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:7729,availability,Error,Error,7729,"| 3872 | 4265460 | 4910 | 936912 | 1118 | 617 | 0.998836 | 0.998525 | 0.219651 | 0.998681 | 2.102574954 | 1.831128594 | 1.535137772 | 1.484295493 |. | HG004 | INDEL | PASS | 510519 | 507376 | 3143 | 1013737 | 4102 | 469356 | 1887 | 1729 | 0.993844 | 0.992465 | 0.462996 | 0.993154 | | | 1.516130736 | 2.075927402 |. analysising result：Using the same test data as the scattered samples, it can be found that the variation detection results of the HG002/3/4 family sample are relatively poor when tested using the GIAB standard set，but I don't understand the reason for this difference. **Setup**. - Operating system: image of singularity, transforming from docker image of deeptrio-1.4.0. - DeepVariant version:deeptrio-1.4.0. - Installation method (Docker, built from source, etc.):Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). HiFi data,those data download links follows:. * HG002:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG002/hpp_HG002_NA24385_son_v1/PacBio_HiFi/15kb/;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG002/hpp_HG002_NA24385_son_v1/PacBio_HiFi/20kb/. * HG003:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG003/PacBio_HiFi/Google_15kb;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG003/PacBio_HiFi/HudsonAlpha_15kb. * HG004:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG004/PacBio_HiFi/Google_15kb/;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG004/PacBio_HiFi/HudsonAlpha_15kb/PBmixSequel733_2_B01_PBSU_30hours_15kbV2PD_70pM_HumanHG004_CCS/. **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:263,deployability,version,version,263,"Feedback on poor results in family variation detection and GIAB dataset evaluation; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. yes，i have checked this FAQ document. . **Describe the issue:**. I used Deeprio v1.4.0 version to perform family analysis on three samples, HG002, HG003, and HG004, and evaluated the accuracy of mutation detection using the GIAB database (NISTv4.2.1). I found that the evaluation results through GIAB were particularly unsatisfactory, but I used the same data and Deepvariant v1.5.0 for single sample analysis, and the evaluation results were very ideal, I don't quite understand why the analysis results of a single sample perform so well at the evaluation level compared to the results of family analysis, and why the results of family analysis are relatively poor. The following is my family analysis and analysis code for individual samples, as well as the evaluation results of the GIAB database, for developers to review:. Data comparison to reference genome:. ```. echo HG002.merged.fastq.gz > HG002.fofn . pbmm2 align \. --preset HIFI \. genome/hg38.fa.mmi \. HG002.fofn \. --sample HG002 \. -j 10 \. HG002.aligned.tmp.bam . samtools sort -@ 10 HG002.aligned.tmp.bam -O BAM -o HG002.aligned.tmp.sort.bam . samtools index -@ 10 HG002.aligned.tmp.sort.bam . chromosomes=(chr1 chr2 chr3 chr4 chr5 chr6 chr7 chr8 chr9 chr10 chr11 chr12 chr13 chr14 chr15 chr16 chr17 chr18 chr19 chr20 chr21 chr22 chrX chrY chrM) . for chromosome in ""${chromosomes[@]}""; \. do \. samtools view -@ 2 -b -h HG002.aligned.tmp.sort.bam ""$chromosome"" --output HG002.aligned.$chromosome.tmp.bam & . done . wait . samtools merge HG002.aligned.bam HG002.aligned.chr*.tmp.bam . samtools sort -@ 10 HG002.aligned.bam -O BAM -o HG002.sort.bam . samtools index -@ 10 HG002.sort.bam . ```. Family analysis code:. ```. rm -rf chr20_GLnexus.DB tmp_ramdom_TrioDemo_chr20 . samtools view --write-index --threads 10 -h -b -S HG002.sort.bam chr20 -O BAM -o ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:6706,deployability,version,version,6706," 0.997658 | 0.21847 | 0.998353 | 2.100128487 | 1.823839956 | 1.581195853 | 1.50423718 |. | HG003 | INDEL | PASS | 504501 | 501414 | 3087 | 1009147 | 3989 | 471526 | 1814 | 1831 | 0.993881 | 0.99258 | 0.467252 | 0.99323 | | | 1.489759281 | 2.02565724 |. | HG003 | SNP | PASS | 3327495 | 3323623 | 3872 | 4265460 | 4910 | 936912 | 1118 | 617 | 0.998836 | 0.998525 | 0.219651 | 0.998681 | 2.102574954 | 1.831128594 | 1.535137772 | 1.484295493 |. | HG004 | INDEL | PASS | 510519 | 507376 | 3143 | 1013737 | 4102 | 469356 | 1887 | 1729 | 0.993844 | 0.992465 | 0.462996 | 0.993154 | | | 1.516130736 | 2.075927402 |. analysising result：Using the same test data as the scattered samples, it can be found that the variation detection results of the HG002/3/4 family sample are relatively poor when tested using the GIAB standard set，but I don't understand the reason for this difference. **Setup**. - Operating system: image of singularity, transforming from docker image of deeptrio-1.4.0. - DeepVariant version:deeptrio-1.4.0. - Installation method (Docker, built from source, etc.):Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). HiFi data,those data download links follows:. * HG002:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG002/hpp_HG002_NA24385_son_v1/PacBio_HiFi/15kb/;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG002/hpp_HG002_NA24385_son_v1/PacBio_HiFi/20kb/. * HG003:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG003/PacBio_HiFi/Google_15kb;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG003/PacBio_HiFi/HudsonAlpha_15kb. * HG004:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG004/PacBio_HiFi/Google_15kb/;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG004/PacBio_HiFi/HudsonAlpha_15kb/PBmixSequel733_2_B01_PBSU_30hours_15kbV2PD_70pM_HumanHG004_CCS/. **Steps to reproduce",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:6732,deployability,Instal,Installation,6732,"53 | 2.100128487 | 1.823839956 | 1.581195853 | 1.50423718 |. | HG003 | INDEL | PASS | 504501 | 501414 | 3087 | 1009147 | 3989 | 471526 | 1814 | 1831 | 0.993881 | 0.99258 | 0.467252 | 0.99323 | | | 1.489759281 | 2.02565724 |. | HG003 | SNP | PASS | 3327495 | 3323623 | 3872 | 4265460 | 4910 | 936912 | 1118 | 617 | 0.998836 | 0.998525 | 0.219651 | 0.998681 | 2.102574954 | 1.831128594 | 1.535137772 | 1.484295493 |. | HG004 | INDEL | PASS | 510519 | 507376 | 3143 | 1013737 | 4102 | 469356 | 1887 | 1729 | 0.993844 | 0.992465 | 0.462996 | 0.993154 | | | 1.516130736 | 2.075927402 |. analysising result：Using the same test data as the scattered samples, it can be found that the variation detection results of the HG002/3/4 family sample are relatively poor when tested using the GIAB standard set，but I don't understand the reason for this difference. **Setup**. - Operating system: image of singularity, transforming from docker image of deeptrio-1.4.0. - DeepVariant version:deeptrio-1.4.0. - Installation method (Docker, built from source, etc.):Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). HiFi data,those data download links follows:. * HG002:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG002/hpp_HG002_NA24385_son_v1/PacBio_HiFi/15kb/;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG002/hpp_HG002_NA24385_son_v1/PacBio_HiFi/20kb/. * HG003:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG003/PacBio_HiFi/Google_15kb;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG003/PacBio_HiFi/HudsonAlpha_15kb. * HG004:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG004/PacBio_HiFi/Google_15kb/;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG004/PacBio_HiFi/HudsonAlpha_15kb/PBmixSequel733_2_B01_PBSU_30hours_15kbV2PD_70pM_HumanHG004_CCS/. **Steps to reproduce:**. - Command:. - Error tra",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:263,integrability,version,version,263,"Feedback on poor results in family variation detection and GIAB dataset evaluation; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. yes，i have checked this FAQ document. . **Describe the issue:**. I used Deeprio v1.4.0 version to perform family analysis on three samples, HG002, HG003, and HG004, and evaluated the accuracy of mutation detection using the GIAB database (NISTv4.2.1). I found that the evaluation results through GIAB were particularly unsatisfactory, but I used the same data and Deepvariant v1.5.0 for single sample analysis, and the evaluation results were very ideal, I don't quite understand why the analysis results of a single sample perform so well at the evaluation level compared to the results of family analysis, and why the results of family analysis are relatively poor. The following is my family analysis and analysis code for individual samples, as well as the evaluation results of the GIAB database, for developers to review:. Data comparison to reference genome:. ```. echo HG002.merged.fastq.gz > HG002.fofn . pbmm2 align \. --preset HIFI \. genome/hg38.fa.mmi \. HG002.fofn \. --sample HG002 \. -j 10 \. HG002.aligned.tmp.bam . samtools sort -@ 10 HG002.aligned.tmp.bam -O BAM -o HG002.aligned.tmp.sort.bam . samtools index -@ 10 HG002.aligned.tmp.sort.bam . chromosomes=(chr1 chr2 chr3 chr4 chr5 chr6 chr7 chr8 chr9 chr10 chr11 chr12 chr13 chr14 chr15 chr16 chr17 chr18 chr19 chr20 chr21 chr22 chrX chrY chrM) . for chromosome in ""${chromosomes[@]}""; \. do \. samtools view -@ 2 -b -h HG002.aligned.tmp.sort.bam ""$chromosome"" --output HG002.aligned.$chromosome.tmp.bam & . done . wait . samtools merge HG002.aligned.bam HG002.aligned.chr*.tmp.bam . samtools sort -@ 10 HG002.aligned.bam -O BAM -o HG002.sort.bam . samtools index -@ 10 HG002.sort.bam . ```. Family analysis code:. ```. rm -rf chr20_GLnexus.DB tmp_ramdom_TrioDemo_chr20 . samtools view --write-index --threads 10 -h -b -S HG002.sort.bam chr20 -O BAM -o ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:6642,integrability,transform,transforming,6642,"925 | 3202 | 4312006 | 7891 | 942045 | 1197 | 719 | 0.999048 | 0.997658 | 0.21847 | 0.998353 | 2.100128487 | 1.823839956 | 1.581195853 | 1.50423718 |. | HG003 | INDEL | PASS | 504501 | 501414 | 3087 | 1009147 | 3989 | 471526 | 1814 | 1831 | 0.993881 | 0.99258 | 0.467252 | 0.99323 | | | 1.489759281 | 2.02565724 |. | HG003 | SNP | PASS | 3327495 | 3323623 | 3872 | 4265460 | 4910 | 936912 | 1118 | 617 | 0.998836 | 0.998525 | 0.219651 | 0.998681 | 2.102574954 | 1.831128594 | 1.535137772 | 1.484295493 |. | HG004 | INDEL | PASS | 510519 | 507376 | 3143 | 1013737 | 4102 | 469356 | 1887 | 1729 | 0.993844 | 0.992465 | 0.462996 | 0.993154 | | | 1.516130736 | 2.075927402 |. analysising result：Using the same test data as the scattered samples, it can be found that the variation detection results of the HG002/3/4 family sample are relatively poor when tested using the GIAB standard set，but I don't understand the reason for this difference. **Setup**. - Operating system: image of singularity, transforming from docker image of deeptrio-1.4.0. - DeepVariant version:deeptrio-1.4.0. - Installation method (Docker, built from source, etc.):Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). HiFi data,those data download links follows:. * HG002:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG002/hpp_HG002_NA24385_son_v1/PacBio_HiFi/15kb/;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG002/hpp_HG002_NA24385_son_v1/PacBio_HiFi/20kb/. * HG003:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG003/PacBio_HiFi/Google_15kb;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG003/PacBio_HiFi/HudsonAlpha_15kb. * HG004:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG004/PacBio_HiFi/Google_15kb/;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG004/PacBio_HiFi/HudsonAlpha_15kb/PBmixSequel733_2_B01_PB",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:6706,integrability,version,version,6706," 0.997658 | 0.21847 | 0.998353 | 2.100128487 | 1.823839956 | 1.581195853 | 1.50423718 |. | HG003 | INDEL | PASS | 504501 | 501414 | 3087 | 1009147 | 3989 | 471526 | 1814 | 1831 | 0.993881 | 0.99258 | 0.467252 | 0.99323 | | | 1.489759281 | 2.02565724 |. | HG003 | SNP | PASS | 3327495 | 3323623 | 3872 | 4265460 | 4910 | 936912 | 1118 | 617 | 0.998836 | 0.998525 | 0.219651 | 0.998681 | 2.102574954 | 1.831128594 | 1.535137772 | 1.484295493 |. | HG004 | INDEL | PASS | 510519 | 507376 | 3143 | 1013737 | 4102 | 469356 | 1887 | 1729 | 0.993844 | 0.992465 | 0.462996 | 0.993154 | | | 1.516130736 | 2.075927402 |. analysising result：Using the same test data as the scattered samples, it can be found that the variation detection results of the HG002/3/4 family sample are relatively poor when tested using the GIAB standard set，but I don't understand the reason for this difference. **Setup**. - Operating system: image of singularity, transforming from docker image of deeptrio-1.4.0. - DeepVariant version:deeptrio-1.4.0. - Installation method (Docker, built from source, etc.):Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). HiFi data,those data download links follows:. * HG002:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG002/hpp_HG002_NA24385_son_v1/PacBio_HiFi/15kb/;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG002/hpp_HG002_NA24385_son_v1/PacBio_HiFi/20kb/. * HG003:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG003/PacBio_HiFi/Google_15kb;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG003/PacBio_HiFi/HudsonAlpha_15kb. * HG004:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG004/PacBio_HiFi/Google_15kb/;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG004/PacBio_HiFi/HudsonAlpha_15kb/PBmixSequel733_2_B01_PBSU_30hours_15kbV2PD_70pM_HumanHG004_CCS/. **Steps to reproduce",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:2272,interoperability,share,share,2272,"tmp.sort.bam . samtools index -@ 10 HG002.aligned.tmp.sort.bam . chromosomes=(chr1 chr2 chr3 chr4 chr5 chr6 chr7 chr8 chr9 chr10 chr11 chr12 chr13 chr14 chr15 chr16 chr17 chr18 chr19 chr20 chr21 chr22 chrX chrY chrM) . for chromosome in ""${chromosomes[@]}""; \. do \. samtools view -@ 2 -b -h HG002.aligned.tmp.sort.bam ""$chromosome"" --output HG002.aligned.$chromosome.tmp.bam & . done . wait . samtools merge HG002.aligned.bam HG002.aligned.chr*.tmp.bam . samtools sort -@ 10 HG002.aligned.bam -O BAM -o HG002.sort.bam . samtools index -@ 10 HG002.sort.bam . ```. Family analysis code:. ```. rm -rf chr20_GLnexus.DB tmp_ramdom_TrioDemo_chr20 . samtools view --write-index --threads 10 -h -b -S HG002.sort.bam chr20 -O BAM -o HG002.chr20.sort.bam . samtools view --write-index --threads 10 -h -b -S HG003.sort.bam chr20 -O BAM -o HG003.chr20.sort.bam . samtools view --write-index --threads 10 -h -b -S HG004.sort.bam chr20 -O BAM -o HG004.chr20.sort.bam . /usr/bin/singularity exec --cleanenv -B /share/:/share/ Singularity/deepvariant.deeptrio-1.4.0.sif /opt/deepvariant/bin/deeptrio/run_deeptrio \. --model_type PACBIO \. --ref=genome/hg38.fa \. --reads_child HG002.chr20.sort.bam \. --reads_parent1 HG003.chr20.sort.bam \. --reads_parent2 HG004.chr20.sort.bam \. --output_vcf_child HG002.chr20.output.vcf.gz \. --output_vcf_parent1 HG003.chr20.output.vcf.gz \. --output_vcf_parent2 HG004.chr20.output.vcf.gz \. --sample_name_child 'HG002' \. --sample_name_parent1 'HG003' \. --sample_name_parent2 'HG004' \. --num_shards 10 \. --intermediate_results_dir tmp_ramdom_TrioDemo_chr20 \. --output_gvcf_child HG002.chr20.g.vcf.gz \. --output_gvcf_parent1 HG003.chr20.g.vcf.gz \. --output_gvcf_parent2 HG004.chr20.g.vcf.gz . glnexus_cli_v1.4.1 \. --config DeepVariant_unfiltered \. --squeeze \. --dir chr20_GLnexus.DB \. HG002.chr20.g.vcf.gz HG003.chr20.g.vcf.gz HG004.chr20.g.vcf.gz \. --threads 10 | \. /opt/conda/envs/bio/bin/bcftools view \. --threads 10 -O z \. -o TrioDemo_chr20.trio_merged.vcf.gz ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:2280,interoperability,share,share,2280,".bam . samtools index -@ 10 HG002.aligned.tmp.sort.bam . chromosomes=(chr1 chr2 chr3 chr4 chr5 chr6 chr7 chr8 chr9 chr10 chr11 chr12 chr13 chr14 chr15 chr16 chr17 chr18 chr19 chr20 chr21 chr22 chrX chrY chrM) . for chromosome in ""${chromosomes[@]}""; \. do \. samtools view -@ 2 -b -h HG002.aligned.tmp.sort.bam ""$chromosome"" --output HG002.aligned.$chromosome.tmp.bam & . done . wait . samtools merge HG002.aligned.bam HG002.aligned.chr*.tmp.bam . samtools sort -@ 10 HG002.aligned.bam -O BAM -o HG002.sort.bam . samtools index -@ 10 HG002.sort.bam . ```. Family analysis code:. ```. rm -rf chr20_GLnexus.DB tmp_ramdom_TrioDemo_chr20 . samtools view --write-index --threads 10 -h -b -S HG002.sort.bam chr20 -O BAM -o HG002.chr20.sort.bam . samtools view --write-index --threads 10 -h -b -S HG003.sort.bam chr20 -O BAM -o HG003.chr20.sort.bam . samtools view --write-index --threads 10 -h -b -S HG004.sort.bam chr20 -O BAM -o HG004.chr20.sort.bam . /usr/bin/singularity exec --cleanenv -B /share/:/share/ Singularity/deepvariant.deeptrio-1.4.0.sif /opt/deepvariant/bin/deeptrio/run_deeptrio \. --model_type PACBIO \. --ref=genome/hg38.fa \. --reads_child HG002.chr20.sort.bam \. --reads_parent1 HG003.chr20.sort.bam \. --reads_parent2 HG004.chr20.sort.bam \. --output_vcf_child HG002.chr20.output.vcf.gz \. --output_vcf_parent1 HG003.chr20.output.vcf.gz \. --output_vcf_parent2 HG004.chr20.output.vcf.gz \. --sample_name_child 'HG002' \. --sample_name_parent1 'HG003' \. --sample_name_parent2 'HG004' \. --num_shards 10 \. --intermediate_results_dir tmp_ramdom_TrioDemo_chr20 \. --output_gvcf_child HG002.chr20.g.vcf.gz \. --output_gvcf_parent1 HG003.chr20.g.vcf.gz \. --output_gvcf_parent2 HG004.chr20.g.vcf.gz . glnexus_cli_v1.4.1 \. --config DeepVariant_unfiltered \. --squeeze \. --dir chr20_GLnexus.DB \. HG002.chr20.g.vcf.gz HG003.chr20.g.vcf.gz HG004.chr20.g.vcf.gz \. --threads 10 | \. /opt/conda/envs/bio/bin/bcftools view \. --threads 10 -O z \. -o TrioDemo_chr20.trio_merged.vcf.gz - . ```.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:3509,interoperability,share,share,3509,ent2 HG004.chr20.sort.bam \. --output_vcf_child HG002.chr20.output.vcf.gz \. --output_vcf_parent1 HG003.chr20.output.vcf.gz \. --output_vcf_parent2 HG004.chr20.output.vcf.gz \. --sample_name_child 'HG002' \. --sample_name_parent1 'HG003' \. --sample_name_parent2 'HG004' \. --num_shards 10 \. --intermediate_results_dir tmp_ramdom_TrioDemo_chr20 \. --output_gvcf_child HG002.chr20.g.vcf.gz \. --output_gvcf_parent1 HG003.chr20.g.vcf.gz \. --output_gvcf_parent2 HG004.chr20.g.vcf.gz . glnexus_cli_v1.4.1 \. --config DeepVariant_unfiltered \. --squeeze \. --dir chr20_GLnexus.DB \. HG002.chr20.g.vcf.gz HG003.chr20.g.vcf.gz HG004.chr20.g.vcf.gz \. --threads 10 | \. /opt/conda/envs/bio/bin/bcftools view \. --threads 10 -O z \. -o TrioDemo_chr20.trio_merged.vcf.gz - . ```. The following is the code for single sample mutation detection:. ```. samtools view --threads 10 -h -b -S HG002.sort.bam chr20 > HG002/HG002.chr20.sort.bam . samtools index -@ 10 HG002/HG002.chr20.sort.bam . singularity run /share:/share Singularity/google-deepvariant.1.5.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref genome/hg38.fa \. --intermediate_results_dir HG002/tmp_ramdom_HG002_chr20 \. --reads HG002/HG002.chr20.sort.bam \. --output_vcf HG002/HG002.chr20.vcf.gz \. --num_shards 10 . rm -fr HG002/tmp_ramdom_HG002_chr20. ```. The evaluation results of the trio family sample are as follows:. | HG002 | INDEL | PASS | 525469 | 190669 | 334800 | 361066 | 7821 | 151401 | 4826 | 2317 | 0.362855 | 0.962698 | 0.419317 | 0.527055 | | | 1.528275734 | 1.988539738 |. |-------|-------|------|---------|---------|---------|---------|-------|--------|-------|------|----------|----------|----------|----------|-------------|------------|-------------|-------------|. | HG002 | SNP | PASS | 3365127 | 1236543 | 2128584 | 1466059 | 19913 | 209456 | 19190 | 463 | 0.367458 | 0.984153 | 0.14287 | 0.535117 | 2.100128487 | 1.97508094 | 1.581195853 | 1.464696928 |. | HG003 | INDEL | PASS | 504501 | 208,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:3516,interoperability,share,share,3516,004.chr20.sort.bam \. --output_vcf_child HG002.chr20.output.vcf.gz \. --output_vcf_parent1 HG003.chr20.output.vcf.gz \. --output_vcf_parent2 HG004.chr20.output.vcf.gz \. --sample_name_child 'HG002' \. --sample_name_parent1 'HG003' \. --sample_name_parent2 'HG004' \. --num_shards 10 \. --intermediate_results_dir tmp_ramdom_TrioDemo_chr20 \. --output_gvcf_child HG002.chr20.g.vcf.gz \. --output_gvcf_parent1 HG003.chr20.g.vcf.gz \. --output_gvcf_parent2 HG004.chr20.g.vcf.gz . glnexus_cli_v1.4.1 \. --config DeepVariant_unfiltered \. --squeeze \. --dir chr20_GLnexus.DB \. HG002.chr20.g.vcf.gz HG003.chr20.g.vcf.gz HG004.chr20.g.vcf.gz \. --threads 10 | \. /opt/conda/envs/bio/bin/bcftools view \. --threads 10 -O z \. -o TrioDemo_chr20.trio_merged.vcf.gz - . ```. The following is the code for single sample mutation detection:. ```. samtools view --threads 10 -h -b -S HG002.sort.bam chr20 > HG002/HG002.chr20.sort.bam . samtools index -@ 10 HG002/HG002.chr20.sort.bam . singularity run /share:/share Singularity/google-deepvariant.1.5.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref genome/hg38.fa \. --intermediate_results_dir HG002/tmp_ramdom_HG002_chr20 \. --reads HG002/HG002.chr20.sort.bam \. --output_vcf HG002/HG002.chr20.vcf.gz \. --num_shards 10 . rm -fr HG002/tmp_ramdom_HG002_chr20. ```. The evaluation results of the trio family sample are as follows:. | HG002 | INDEL | PASS | 525469 | 190669 | 334800 | 361066 | 7821 | 151401 | 4826 | 2317 | 0.362855 | 0.962698 | 0.419317 | 0.527055 | | | 1.528275734 | 1.988539738 |. |-------|-------|------|---------|---------|---------|---------|-------|--------|-------|------|----------|----------|----------|----------|-------------|------------|-------------|-------------|. | HG002 | SNP | PASS | 3365127 | 1236543 | 2128584 | 1466059 | 19913 | 209456 | 19190 | 463 | 0.367458 | 0.984153 | 0.14287 | 0.535117 | 2.100128487 | 1.97508094 | 1.581195853 | 1.464696928 |. | HG003 | INDEL | PASS | 504501 | 208450 | 2,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:6521,interoperability,standard,standard,6521,"-----|----------|----------|-------------|-------------|-------------|-------------|. | HG002 | SNP | PASS | 3365127 | 3361925 | 3202 | 4312006 | 7891 | 942045 | 1197 | 719 | 0.999048 | 0.997658 | 0.21847 | 0.998353 | 2.100128487 | 1.823839956 | 1.581195853 | 1.50423718 |. | HG003 | INDEL | PASS | 504501 | 501414 | 3087 | 1009147 | 3989 | 471526 | 1814 | 1831 | 0.993881 | 0.99258 | 0.467252 | 0.99323 | | | 1.489759281 | 2.02565724 |. | HG003 | SNP | PASS | 3327495 | 3323623 | 3872 | 4265460 | 4910 | 936912 | 1118 | 617 | 0.998836 | 0.998525 | 0.219651 | 0.998681 | 2.102574954 | 1.831128594 | 1.535137772 | 1.484295493 |. | HG004 | INDEL | PASS | 510519 | 507376 | 3143 | 1013737 | 4102 | 469356 | 1887 | 1729 | 0.993844 | 0.992465 | 0.462996 | 0.993154 | | | 1.516130736 | 2.075927402 |. analysising result：Using the same test data as the scattered samples, it can be found that the variation detection results of the HG002/3/4 family sample are relatively poor when tested using the GIAB standard set，but I don't understand the reason for this difference. **Setup**. - Operating system: image of singularity, transforming from docker image of deeptrio-1.4.0. - DeepVariant version:deeptrio-1.4.0. - Installation method (Docker, built from source, etc.):Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). HiFi data,those data download links follows:. * HG002:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG002/hpp_HG002_NA24385_son_v1/PacBio_HiFi/15kb/;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG002/hpp_HG002_NA24385_son_v1/PacBio_HiFi/20kb/. * HG003:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG003/PacBio_HiFi/Google_15kb;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG003/PacBio_HiFi/HudsonAlpha_15kb. * HG004:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG004/PacBio_HiFi/Google_15kb/;https",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:6642,interoperability,transform,transforming,6642,"925 | 3202 | 4312006 | 7891 | 942045 | 1197 | 719 | 0.999048 | 0.997658 | 0.21847 | 0.998353 | 2.100128487 | 1.823839956 | 1.581195853 | 1.50423718 |. | HG003 | INDEL | PASS | 504501 | 501414 | 3087 | 1009147 | 3989 | 471526 | 1814 | 1831 | 0.993881 | 0.99258 | 0.467252 | 0.99323 | | | 1.489759281 | 2.02565724 |. | HG003 | SNP | PASS | 3327495 | 3323623 | 3872 | 4265460 | 4910 | 936912 | 1118 | 617 | 0.998836 | 0.998525 | 0.219651 | 0.998681 | 2.102574954 | 1.831128594 | 1.535137772 | 1.484295493 |. | HG004 | INDEL | PASS | 510519 | 507376 | 3143 | 1013737 | 4102 | 469356 | 1887 | 1729 | 0.993844 | 0.992465 | 0.462996 | 0.993154 | | | 1.516130736 | 2.075927402 |. analysising result：Using the same test data as the scattered samples, it can be found that the variation detection results of the HG002/3/4 family sample are relatively poor when tested using the GIAB standard set，but I don't understand the reason for this difference. **Setup**. - Operating system: image of singularity, transforming from docker image of deeptrio-1.4.0. - DeepVariant version:deeptrio-1.4.0. - Installation method (Docker, built from source, etc.):Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). HiFi data,those data download links follows:. * HG002:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG002/hpp_HG002_NA24385_son_v1/PacBio_HiFi/15kb/;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG002/hpp_HG002_NA24385_son_v1/PacBio_HiFi/20kb/. * HG003:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG003/PacBio_HiFi/Google_15kb;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG003/PacBio_HiFi/HudsonAlpha_15kb. * HG004:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG004/PacBio_HiFi/Google_15kb/;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG004/PacBio_HiFi/HudsonAlpha_15kb/PBmixSequel733_2_B01_PB",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:263,modifiability,version,version,263,"Feedback on poor results in family variation detection and GIAB dataset evaluation; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. yes，i have checked this FAQ document. . **Describe the issue:**. I used Deeprio v1.4.0 version to perform family analysis on three samples, HG002, HG003, and HG004, and evaluated the accuracy of mutation detection using the GIAB database (NISTv4.2.1). I found that the evaluation results through GIAB were particularly unsatisfactory, but I used the same data and Deepvariant v1.5.0 for single sample analysis, and the evaluation results were very ideal, I don't quite understand why the analysis results of a single sample perform so well at the evaluation level compared to the results of family analysis, and why the results of family analysis are relatively poor. The following is my family analysis and analysis code for individual samples, as well as the evaluation results of the GIAB database, for developers to review:. Data comparison to reference genome:. ```. echo HG002.merged.fastq.gz > HG002.fofn . pbmm2 align \. --preset HIFI \. genome/hg38.fa.mmi \. HG002.fofn \. --sample HG002 \. -j 10 \. HG002.aligned.tmp.bam . samtools sort -@ 10 HG002.aligned.tmp.bam -O BAM -o HG002.aligned.tmp.sort.bam . samtools index -@ 10 HG002.aligned.tmp.sort.bam . chromosomes=(chr1 chr2 chr3 chr4 chr5 chr6 chr7 chr8 chr9 chr10 chr11 chr12 chr13 chr14 chr15 chr16 chr17 chr18 chr19 chr20 chr21 chr22 chrX chrY chrM) . for chromosome in ""${chromosomes[@]}""; \. do \. samtools view -@ 2 -b -h HG002.aligned.tmp.sort.bam ""$chromosome"" --output HG002.aligned.$chromosome.tmp.bam & . done . wait . samtools merge HG002.aligned.bam HG002.aligned.chr*.tmp.bam . samtools sort -@ 10 HG002.aligned.bam -O BAM -o HG002.sort.bam . samtools index -@ 10 HG002.sort.bam . ```. Family analysis code:. ```. rm -rf chr20_GLnexus.DB tmp_ramdom_TrioDemo_chr20 . samtools view --write-index --threads 10 -h -b -S HG002.sort.bam chr20 -O BAM -o ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:2389,modifiability,PAC,PACBIO,2389," chr9 chr10 chr11 chr12 chr13 chr14 chr15 chr16 chr17 chr18 chr19 chr20 chr21 chr22 chrX chrY chrM) . for chromosome in ""${chromosomes[@]}""; \. do \. samtools view -@ 2 -b -h HG002.aligned.tmp.sort.bam ""$chromosome"" --output HG002.aligned.$chromosome.tmp.bam & . done . wait . samtools merge HG002.aligned.bam HG002.aligned.chr*.tmp.bam . samtools sort -@ 10 HG002.aligned.bam -O BAM -o HG002.sort.bam . samtools index -@ 10 HG002.sort.bam . ```. Family analysis code:. ```. rm -rf chr20_GLnexus.DB tmp_ramdom_TrioDemo_chr20 . samtools view --write-index --threads 10 -h -b -S HG002.sort.bam chr20 -O BAM -o HG002.chr20.sort.bam . samtools view --write-index --threads 10 -h -b -S HG003.sort.bam chr20 -O BAM -o HG003.chr20.sort.bam . samtools view --write-index --threads 10 -h -b -S HG004.sort.bam chr20 -O BAM -o HG004.chr20.sort.bam . /usr/bin/singularity exec --cleanenv -B /share/:/share/ Singularity/deepvariant.deeptrio-1.4.0.sif /opt/deepvariant/bin/deeptrio/run_deeptrio \. --model_type PACBIO \. --ref=genome/hg38.fa \. --reads_child HG002.chr20.sort.bam \. --reads_parent1 HG003.chr20.sort.bam \. --reads_parent2 HG004.chr20.sort.bam \. --output_vcf_child HG002.chr20.output.vcf.gz \. --output_vcf_parent1 HG003.chr20.output.vcf.gz \. --output_vcf_parent2 HG004.chr20.output.vcf.gz \. --sample_name_child 'HG002' \. --sample_name_parent1 'HG003' \. --sample_name_parent2 'HG004' \. --num_shards 10 \. --intermediate_results_dir tmp_ramdom_TrioDemo_chr20 \. --output_gvcf_child HG002.chr20.g.vcf.gz \. --output_gvcf_parent1 HG003.chr20.g.vcf.gz \. --output_gvcf_parent2 HG004.chr20.g.vcf.gz . glnexus_cli_v1.4.1 \. --config DeepVariant_unfiltered \. --squeeze \. --dir chr20_GLnexus.DB \. HG002.chr20.g.vcf.gz HG003.chr20.g.vcf.gz HG004.chr20.g.vcf.gz \. --threads 10 | \. /opt/conda/envs/bio/bin/bcftools view \. --threads 10 -O z \. -o TrioDemo_chr20.trio_merged.vcf.gz - . ```. The following is the code for single sample mutation detection:. ```. samtools view --threads 10 -h -b -S HG",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:3619,modifiability,PAC,PACBIO,3619,output.vcf.gz \. --output_vcf_parent2 HG004.chr20.output.vcf.gz \. --sample_name_child 'HG002' \. --sample_name_parent1 'HG003' \. --sample_name_parent2 'HG004' \. --num_shards 10 \. --intermediate_results_dir tmp_ramdom_TrioDemo_chr20 \. --output_gvcf_child HG002.chr20.g.vcf.gz \. --output_gvcf_parent1 HG003.chr20.g.vcf.gz \. --output_gvcf_parent2 HG004.chr20.g.vcf.gz . glnexus_cli_v1.4.1 \. --config DeepVariant_unfiltered \. --squeeze \. --dir chr20_GLnexus.DB \. HG002.chr20.g.vcf.gz HG003.chr20.g.vcf.gz HG004.chr20.g.vcf.gz \. --threads 10 | \. /opt/conda/envs/bio/bin/bcftools view \. --threads 10 -O z \. -o TrioDemo_chr20.trio_merged.vcf.gz - . ```. The following is the code for single sample mutation detection:. ```. samtools view --threads 10 -h -b -S HG002.sort.bam chr20 > HG002/HG002.chr20.sort.bam . samtools index -@ 10 HG002/HG002.chr20.sort.bam . singularity run /share:/share Singularity/google-deepvariant.1.5.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref genome/hg38.fa \. --intermediate_results_dir HG002/tmp_ramdom_HG002_chr20 \. --reads HG002/HG002.chr20.sort.bam \. --output_vcf HG002/HG002.chr20.vcf.gz \. --num_shards 10 . rm -fr HG002/tmp_ramdom_HG002_chr20. ```. The evaluation results of the trio family sample are as follows:. | HG002 | INDEL | PASS | 525469 | 190669 | 334800 | 361066 | 7821 | 151401 | 4826 | 2317 | 0.362855 | 0.962698 | 0.419317 | 0.527055 | | | 1.528275734 | 1.988539738 |. |-------|-------|------|---------|---------|---------|---------|-------|--------|-------|------|----------|----------|----------|----------|-------------|------------|-------------|-------------|. | HG002 | SNP | PASS | 3365127 | 1236543 | 2128584 | 1466059 | 19913 | 209456 | 19190 | 463 | 0.367458 | 0.984153 | 0.14287 | 0.535117 | 2.100128487 | 1.97508094 | 1.581195853 | 1.464696928 |. | HG003 | INDEL | PASS | 504501 | 208450 | 296051 | 405929 | 3464 | 179579 | 1622 | 1671 | 0.413181 | 0.984696 | 0.44239 | 0.582108 | | | 1.4897592,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:6706,modifiability,version,version,6706," 0.997658 | 0.21847 | 0.998353 | 2.100128487 | 1.823839956 | 1.581195853 | 1.50423718 |. | HG003 | INDEL | PASS | 504501 | 501414 | 3087 | 1009147 | 3989 | 471526 | 1814 | 1831 | 0.993881 | 0.99258 | 0.467252 | 0.99323 | | | 1.489759281 | 2.02565724 |. | HG003 | SNP | PASS | 3327495 | 3323623 | 3872 | 4265460 | 4910 | 936912 | 1118 | 617 | 0.998836 | 0.998525 | 0.219651 | 0.998681 | 2.102574954 | 1.831128594 | 1.535137772 | 1.484295493 |. | HG004 | INDEL | PASS | 510519 | 507376 | 3143 | 1013737 | 4102 | 469356 | 1887 | 1729 | 0.993844 | 0.992465 | 0.462996 | 0.993154 | | | 1.516130736 | 2.075927402 |. analysising result：Using the same test data as the scattered samples, it can be found that the variation detection results of the HG002/3/4 family sample are relatively poor when tested using the GIAB standard set，but I don't understand the reason for this difference. **Setup**. - Operating system: image of singularity, transforming from docker image of deeptrio-1.4.0. - DeepVariant version:deeptrio-1.4.0. - Installation method (Docker, built from source, etc.):Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). HiFi data,those data download links follows:. * HG002:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG002/hpp_HG002_NA24385_son_v1/PacBio_HiFi/15kb/;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG002/hpp_HG002_NA24385_son_v1/PacBio_HiFi/20kb/. * HG003:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG003/PacBio_HiFi/Google_15kb;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG003/PacBio_HiFi/HudsonAlpha_15kb. * HG004:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG004/PacBio_HiFi/Google_15kb/;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG004/PacBio_HiFi/HudsonAlpha_15kb/PBmixSequel733_2_B01_PBSU_30hours_15kbV2PD_70pM_HumanHG004_CCS/. **Steps to reproduce",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:274,performance,perform,perform,274,"Feedback on poor results in family variation detection and GIAB dataset evaluation; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. yes，i have checked this FAQ document. . **Describe the issue:**. I used Deeprio v1.4.0 version to perform family analysis on three samples, HG002, HG003, and HG004, and evaluated the accuracy of mutation detection using the GIAB database (NISTv4.2.1). I found that the evaluation results through GIAB were particularly unsatisfactory, but I used the same data and Deepvariant v1.5.0 for single sample analysis, and the evaluation results were very ideal, I don't quite understand why the analysis results of a single sample perform so well at the evaluation level compared to the results of family analysis, and why the results of family analysis are relatively poor. The following is my family analysis and analysis code for individual samples, as well as the evaluation results of the GIAB database, for developers to review:. Data comparison to reference genome:. ```. echo HG002.merged.fastq.gz > HG002.fofn . pbmm2 align \. --preset HIFI \. genome/hg38.fa.mmi \. HG002.fofn \. --sample HG002 \. -j 10 \. HG002.aligned.tmp.bam . samtools sort -@ 10 HG002.aligned.tmp.bam -O BAM -o HG002.aligned.tmp.sort.bam . samtools index -@ 10 HG002.aligned.tmp.sort.bam . chromosomes=(chr1 chr2 chr3 chr4 chr5 chr6 chr7 chr8 chr9 chr10 chr11 chr12 chr13 chr14 chr15 chr16 chr17 chr18 chr19 chr20 chr21 chr22 chrX chrY chrM) . for chromosome in ""${chromosomes[@]}""; \. do \. samtools view -@ 2 -b -h HG002.aligned.tmp.sort.bam ""$chromosome"" --output HG002.aligned.$chromosome.tmp.bam & . done . wait . samtools merge HG002.aligned.bam HG002.aligned.chr*.tmp.bam . samtools sort -@ 10 HG002.aligned.bam -O BAM -o HG002.sort.bam . samtools index -@ 10 HG002.sort.bam . ```. Family analysis code:. ```. rm -rf chr20_GLnexus.DB tmp_ramdom_TrioDemo_chr20 . samtools view --write-index --threads 10 -h -b -S HG002.sort.bam chr20 -O BAM -o ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:700,performance,perform,perform,700,"Feedback on poor results in family variation detection and GIAB dataset evaluation; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. yes，i have checked this FAQ document. . **Describe the issue:**. I used Deeprio v1.4.0 version to perform family analysis on three samples, HG002, HG003, and HG004, and evaluated the accuracy of mutation detection using the GIAB database (NISTv4.2.1). I found that the evaluation results through GIAB were particularly unsatisfactory, but I used the same data and Deepvariant v1.5.0 for single sample analysis, and the evaluation results were very ideal, I don't quite understand why the analysis results of a single sample perform so well at the evaluation level compared to the results of family analysis, and why the results of family analysis are relatively poor. The following is my family analysis and analysis code for individual samples, as well as the evaluation results of the GIAB database, for developers to review:. Data comparison to reference genome:. ```. echo HG002.merged.fastq.gz > HG002.fofn . pbmm2 align \. --preset HIFI \. genome/hg38.fa.mmi \. HG002.fofn \. --sample HG002 \. -j 10 \. HG002.aligned.tmp.bam . samtools sort -@ 10 HG002.aligned.tmp.bam -O BAM -o HG002.aligned.tmp.sort.bam . samtools index -@ 10 HG002.aligned.tmp.sort.bam . chromosomes=(chr1 chr2 chr3 chr4 chr5 chr6 chr7 chr8 chr9 chr10 chr11 chr12 chr13 chr14 chr15 chr16 chr17 chr18 chr19 chr20 chr21 chr22 chrX chrY chrM) . for chromosome in ""${chromosomes[@]}""; \. do \. samtools view -@ 2 -b -h HG002.aligned.tmp.sort.bam ""$chromosome"" --output HG002.aligned.$chromosome.tmp.bam & . done . wait . samtools merge HG002.aligned.bam HG002.aligned.chr*.tmp.bam . samtools sort -@ 10 HG002.aligned.bam -O BAM -o HG002.sort.bam . samtools index -@ 10 HG002.sort.bam . ```. Family analysis code:. ```. rm -rf chr20_GLnexus.DB tmp_ramdom_TrioDemo_chr20 . samtools view --write-index --threads 10 -h -b -S HG002.sort.bam chr20 -O BAM -o ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:7729,performance,Error,Error,7729,"| 3872 | 4265460 | 4910 | 936912 | 1118 | 617 | 0.998836 | 0.998525 | 0.219651 | 0.998681 | 2.102574954 | 1.831128594 | 1.535137772 | 1.484295493 |. | HG004 | INDEL | PASS | 510519 | 507376 | 3143 | 1013737 | 4102 | 469356 | 1887 | 1729 | 0.993844 | 0.992465 | 0.462996 | 0.993154 | | | 1.516130736 | 2.075927402 |. analysising result：Using the same test data as the scattered samples, it can be found that the variation detection results of the HG002/3/4 family sample are relatively poor when tested using the GIAB standard set，but I don't understand the reason for this difference. **Setup**. - Operating system: image of singularity, transforming from docker image of deeptrio-1.4.0. - DeepVariant version:deeptrio-1.4.0. - Installation method (Docker, built from source, etc.):Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). HiFi data,those data download links follows:. * HG002:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG002/hpp_HG002_NA24385_son_v1/PacBio_HiFi/15kb/;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG002/hpp_HG002_NA24385_son_v1/PacBio_HiFi/20kb/. * HG003:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG003/PacBio_HiFi/Google_15kb;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG003/PacBio_HiFi/HudsonAlpha_15kb. * HG004:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG004/PacBio_HiFi/Google_15kb/;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG004/PacBio_HiFi/HudsonAlpha_15kb/PBmixSequel733_2_B01_PBSU_30hours_15kbV2PD_70pM_HumanHG004_CCS/. **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:7761,reliability,Doe,Does,7761,"| 3872 | 4265460 | 4910 | 936912 | 1118 | 617 | 0.998836 | 0.998525 | 0.219651 | 0.998681 | 2.102574954 | 1.831128594 | 1.535137772 | 1.484295493 |. | HG004 | INDEL | PASS | 510519 | 507376 | 3143 | 1013737 | 4102 | 469356 | 1887 | 1729 | 0.993844 | 0.992465 | 0.462996 | 0.993154 | | | 1.516130736 | 2.075927402 |. analysising result：Using the same test data as the scattered samples, it can be found that the variation detection results of the HG002/3/4 family sample are relatively poor when tested using the GIAB standard set，but I don't understand the reason for this difference. **Setup**. - Operating system: image of singularity, transforming from docker image of deeptrio-1.4.0. - DeepVariant version:deeptrio-1.4.0. - Installation method (Docker, built from source, etc.):Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). HiFi data,those data download links follows:. * HG002:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG002/hpp_HG002_NA24385_son_v1/PacBio_HiFi/15kb/;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG002/hpp_HG002_NA24385_son_v1/PacBio_HiFi/20kb/. * HG003:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG003/PacBio_HiFi/Google_15kb;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG003/PacBio_HiFi/HudsonAlpha_15kb. * HG004:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG004/PacBio_HiFi/Google_15kb/;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG004/PacBio_HiFi/HudsonAlpha_15kb/PBmixSequel733_2_B01_PBSU_30hours_15kbV2PD_70pM_HumanHG004_CCS/. **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:45,safety,detect,detection,45,"Feedback on poor results in family variation detection and GIAB dataset evaluation; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. yes，i have checked this FAQ document. . **Describe the issue:**. I used Deeprio v1.4.0 version to perform family analysis on three samples, HG002, HG003, and HG004, and evaluated the accuracy of mutation detection using the GIAB database (NISTv4.2.1). I found that the evaluation results through GIAB were particularly unsatisfactory, but I used the same data and Deepvariant v1.5.0 for single sample analysis, and the evaluation results were very ideal, I don't quite understand why the analysis results of a single sample perform so well at the evaluation level compared to the results of family analysis, and why the results of family analysis are relatively poor. The following is my family analysis and analysis code for individual samples, as well as the evaluation results of the GIAB database, for developers to review:. Data comparison to reference genome:. ```. echo HG002.merged.fastq.gz > HG002.fofn . pbmm2 align \. --preset HIFI \. genome/hg38.fa.mmi \. HG002.fofn \. --sample HG002 \. -j 10 \. HG002.aligned.tmp.bam . samtools sort -@ 10 HG002.aligned.tmp.bam -O BAM -o HG002.aligned.tmp.sort.bam . samtools index -@ 10 HG002.aligned.tmp.sort.bam . chromosomes=(chr1 chr2 chr3 chr4 chr5 chr6 chr7 chr8 chr9 chr10 chr11 chr12 chr13 chr14 chr15 chr16 chr17 chr18 chr19 chr20 chr21 chr22 chrX chrY chrM) . for chromosome in ""${chromosomes[@]}""; \. do \. samtools view -@ 2 -b -h HG002.aligned.tmp.sort.bam ""$chromosome"" --output HG002.aligned.$chromosome.tmp.bam & . done . wait . samtools merge HG002.aligned.bam HG002.aligned.chr*.tmp.bam . samtools sort -@ 10 HG002.aligned.bam -O BAM -o HG002.sort.bam . samtools index -@ 10 HG002.sort.bam . ```. Family analysis code:. ```. rm -rf chr20_GLnexus.DB tmp_ramdom_TrioDemo_chr20 . samtools view --write-index --threads 10 -h -b -S HG002.sort.bam chr20 -O BAM -o ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:380,safety,detect,detection,380,"Feedback on poor results in family variation detection and GIAB dataset evaluation; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. yes，i have checked this FAQ document. . **Describe the issue:**. I used Deeprio v1.4.0 version to perform family analysis on three samples, HG002, HG003, and HG004, and evaluated the accuracy of mutation detection using the GIAB database (NISTv4.2.1). I found that the evaluation results through GIAB were particularly unsatisfactory, but I used the same data and Deepvariant v1.5.0 for single sample analysis, and the evaluation results were very ideal, I don't quite understand why the analysis results of a single sample perform so well at the evaluation level compared to the results of family analysis, and why the results of family analysis are relatively poor. The following is my family analysis and analysis code for individual samples, as well as the evaluation results of the GIAB database, for developers to review:. Data comparison to reference genome:. ```. echo HG002.merged.fastq.gz > HG002.fofn . pbmm2 align \. --preset HIFI \. genome/hg38.fa.mmi \. HG002.fofn \. --sample HG002 \. -j 10 \. HG002.aligned.tmp.bam . samtools sort -@ 10 HG002.aligned.tmp.bam -O BAM -o HG002.aligned.tmp.sort.bam . samtools index -@ 10 HG002.aligned.tmp.sort.bam . chromosomes=(chr1 chr2 chr3 chr4 chr5 chr6 chr7 chr8 chr9 chr10 chr11 chr12 chr13 chr14 chr15 chr16 chr17 chr18 chr19 chr20 chr21 chr22 chrX chrY chrM) . for chromosome in ""${chromosomes[@]}""; \. do \. samtools view -@ 2 -b -h HG002.aligned.tmp.sort.bam ""$chromosome"" --output HG002.aligned.$chromosome.tmp.bam & . done . wait . samtools merge HG002.aligned.bam HG002.aligned.chr*.tmp.bam . samtools sort -@ 10 HG002.aligned.bam -O BAM -o HG002.sort.bam . samtools index -@ 10 HG002.sort.bam . ```. Family analysis code:. ```. rm -rf chr20_GLnexus.DB tmp_ramdom_TrioDemo_chr20 . samtools view --write-index --threads 10 -h -b -S HG002.sort.bam chr20 -O BAM -o ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:996,safety,review,review,996,"Feedback on poor results in family variation detection and GIAB dataset evaluation; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. yes，i have checked this FAQ document. . **Describe the issue:**. I used Deeprio v1.4.0 version to perform family analysis on three samples, HG002, HG003, and HG004, and evaluated the accuracy of mutation detection using the GIAB database (NISTv4.2.1). I found that the evaluation results through GIAB were particularly unsatisfactory, but I used the same data and Deepvariant v1.5.0 for single sample analysis, and the evaluation results were very ideal, I don't quite understand why the analysis results of a single sample perform so well at the evaluation level compared to the results of family analysis, and why the results of family analysis are relatively poor. The following is my family analysis and analysis code for individual samples, as well as the evaluation results of the GIAB database, for developers to review:. Data comparison to reference genome:. ```. echo HG002.merged.fastq.gz > HG002.fofn . pbmm2 align \. --preset HIFI \. genome/hg38.fa.mmi \. HG002.fofn \. --sample HG002 \. -j 10 \. HG002.aligned.tmp.bam . samtools sort -@ 10 HG002.aligned.tmp.bam -O BAM -o HG002.aligned.tmp.sort.bam . samtools index -@ 10 HG002.aligned.tmp.sort.bam . chromosomes=(chr1 chr2 chr3 chr4 chr5 chr6 chr7 chr8 chr9 chr10 chr11 chr12 chr13 chr14 chr15 chr16 chr17 chr18 chr19 chr20 chr21 chr22 chrX chrY chrM) . for chromosome in ""${chromosomes[@]}""; \. do \. samtools view -@ 2 -b -h HG002.aligned.tmp.sort.bam ""$chromosome"" --output HG002.aligned.$chromosome.tmp.bam & . done . wait . samtools merge HG002.aligned.bam HG002.aligned.chr*.tmp.bam . samtools sort -@ 10 HG002.aligned.bam -O BAM -o HG002.sort.bam . samtools index -@ 10 HG002.sort.bam . ```. Family analysis code:. ```. rm -rf chr20_GLnexus.DB tmp_ramdom_TrioDemo_chr20 . samtools view --write-index --threads 10 -h -b -S HG002.sort.bam chr20 -O BAM -o ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:3337,safety,detect,detection,3337,iant/bin/deeptrio/run_deeptrio \. --model_type PACBIO \. --ref=genome/hg38.fa \. --reads_child HG002.chr20.sort.bam \. --reads_parent1 HG003.chr20.sort.bam \. --reads_parent2 HG004.chr20.sort.bam \. --output_vcf_child HG002.chr20.output.vcf.gz \. --output_vcf_parent1 HG003.chr20.output.vcf.gz \. --output_vcf_parent2 HG004.chr20.output.vcf.gz \. --sample_name_child 'HG002' \. --sample_name_parent1 'HG003' \. --sample_name_parent2 'HG004' \. --num_shards 10 \. --intermediate_results_dir tmp_ramdom_TrioDemo_chr20 \. --output_gvcf_child HG002.chr20.g.vcf.gz \. --output_gvcf_parent1 HG003.chr20.g.vcf.gz \. --output_gvcf_parent2 HG004.chr20.g.vcf.gz . glnexus_cli_v1.4.1 \. --config DeepVariant_unfiltered \. --squeeze \. --dir chr20_GLnexus.DB \. HG002.chr20.g.vcf.gz HG003.chr20.g.vcf.gz HG004.chr20.g.vcf.gz \. --threads 10 | \. /opt/conda/envs/bio/bin/bcftools view \. --threads 10 -O z \. -o TrioDemo_chr20.trio_merged.vcf.gz - . ```. The following is the code for single sample mutation detection:. ```. samtools view --threads 10 -h -b -S HG002.sort.bam chr20 > HG002/HG002.chr20.sort.bam . samtools index -@ 10 HG002/HG002.chr20.sort.bam . singularity run /share:/share Singularity/google-deepvariant.1.5.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref genome/hg38.fa \. --intermediate_results_dir HG002/tmp_ramdom_HG002_chr20 \. --reads HG002/HG002.chr20.sort.bam \. --output_vcf HG002/HG002.chr20.vcf.gz \. --num_shards 10 . rm -fr HG002/tmp_ramdom_HG002_chr20. ```. The evaluation results of the trio family sample are as follows:. | HG002 | INDEL | PASS | 525469 | 190669 | 334800 | 361066 | 7821 | 151401 | 4826 | 2317 | 0.362855 | 0.962698 | 0.419317 | 0.527055 | | | 1.528275734 | 1.988539738 |. |-------|-------|------|---------|---------|---------|---------|-------|--------|-------|------|----------|----------|----------|----------|-------------|------------|-------------|-------------|. | HG002 | SNP | PASS | 3365127 | 1236543 | 2128584 | 1466059,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:6354,safety,test,test,6354,"82227 | 0.444908 | 0.98379 | | | 1.528275734 | 2.114606634 |. |-------|-------|------|---------|---------|------|---------|------|--------|------|------|----------|----------|----------|----------|-------------|-------------|-------------|-------------|. | HG002 | SNP | PASS | 3365127 | 3361925 | 3202 | 4312006 | 7891 | 942045 | 1197 | 719 | 0.999048 | 0.997658 | 0.21847 | 0.998353 | 2.100128487 | 1.823839956 | 1.581195853 | 1.50423718 |. | HG003 | INDEL | PASS | 504501 | 501414 | 3087 | 1009147 | 3989 | 471526 | 1814 | 1831 | 0.993881 | 0.99258 | 0.467252 | 0.99323 | | | 1.489759281 | 2.02565724 |. | HG003 | SNP | PASS | 3327495 | 3323623 | 3872 | 4265460 | 4910 | 936912 | 1118 | 617 | 0.998836 | 0.998525 | 0.219651 | 0.998681 | 2.102574954 | 1.831128594 | 1.535137772 | 1.484295493 |. | HG004 | INDEL | PASS | 510519 | 507376 | 3143 | 1013737 | 4102 | 469356 | 1887 | 1729 | 0.993844 | 0.992465 | 0.462996 | 0.993154 | | | 1.516130736 | 2.075927402 |. analysising result：Using the same test data as the scattered samples, it can be found that the variation detection results of the HG002/3/4 family sample are relatively poor when tested using the GIAB standard set，but I don't understand the reason for this difference. **Setup**. - Operating system: image of singularity, transforming from docker image of deeptrio-1.4.0. - DeepVariant version:deeptrio-1.4.0. - Installation method (Docker, built from source, etc.):Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). HiFi data,those data download links follows:. * HG002:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG002/hpp_HG002_NA24385_son_v1/PacBio_HiFi/15kb/;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG002/hpp_HG002_NA24385_son_v1/PacBio_HiFi/20kb/. * HG003:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG003/PacBio_HiFi/Google_15kb;https://s3-us-west-2.amazonaws.com/human-pangenomic",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:6425,safety,detect,detection,6425,"----|------|---------|---------|------|---------|------|--------|------|------|----------|----------|----------|----------|-------------|-------------|-------------|-------------|. | HG002 | SNP | PASS | 3365127 | 3361925 | 3202 | 4312006 | 7891 | 942045 | 1197 | 719 | 0.999048 | 0.997658 | 0.21847 | 0.998353 | 2.100128487 | 1.823839956 | 1.581195853 | 1.50423718 |. | HG003 | INDEL | PASS | 504501 | 501414 | 3087 | 1009147 | 3989 | 471526 | 1814 | 1831 | 0.993881 | 0.99258 | 0.467252 | 0.99323 | | | 1.489759281 | 2.02565724 |. | HG003 | SNP | PASS | 3327495 | 3323623 | 3872 | 4265460 | 4910 | 936912 | 1118 | 617 | 0.998836 | 0.998525 | 0.219651 | 0.998681 | 2.102574954 | 1.831128594 | 1.535137772 | 1.484295493 |. | HG004 | INDEL | PASS | 510519 | 507376 | 3143 | 1013737 | 4102 | 469356 | 1887 | 1729 | 0.993844 | 0.992465 | 0.462996 | 0.993154 | | | 1.516130736 | 2.075927402 |. analysising result：Using the same test data as the scattered samples, it can be found that the variation detection results of the HG002/3/4 family sample are relatively poor when tested using the GIAB standard set，but I don't understand the reason for this difference. **Setup**. - Operating system: image of singularity, transforming from docker image of deeptrio-1.4.0. - DeepVariant version:deeptrio-1.4.0. - Installation method (Docker, built from source, etc.):Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). HiFi data,those data download links follows:. * HG002:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG002/hpp_HG002_NA24385_son_v1/PacBio_HiFi/15kb/;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG002/hpp_HG002_NA24385_son_v1/PacBio_HiFi/20kb/. * HG003:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG003/PacBio_HiFi/Google_15kb;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG003/PacBio_HiFi/HudsonAlpha_15kb. * HG004:https://s3-",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:6499,safety,test,tested,6499,"------|----------|----------|----------|----------|-------------|-------------|-------------|-------------|. | HG002 | SNP | PASS | 3365127 | 3361925 | 3202 | 4312006 | 7891 | 942045 | 1197 | 719 | 0.999048 | 0.997658 | 0.21847 | 0.998353 | 2.100128487 | 1.823839956 | 1.581195853 | 1.50423718 |. | HG003 | INDEL | PASS | 504501 | 501414 | 3087 | 1009147 | 3989 | 471526 | 1814 | 1831 | 0.993881 | 0.99258 | 0.467252 | 0.99323 | | | 1.489759281 | 2.02565724 |. | HG003 | SNP | PASS | 3327495 | 3323623 | 3872 | 4265460 | 4910 | 936912 | 1118 | 617 | 0.998836 | 0.998525 | 0.219651 | 0.998681 | 2.102574954 | 1.831128594 | 1.535137772 | 1.484295493 |. | HG004 | INDEL | PASS | 510519 | 507376 | 3143 | 1013737 | 4102 | 469356 | 1887 | 1729 | 0.993844 | 0.992465 | 0.462996 | 0.993154 | | | 1.516130736 | 2.075927402 |. analysising result：Using the same test data as the scattered samples, it can be found that the variation detection results of the HG002/3/4 family sample are relatively poor when tested using the GIAB standard set，but I don't understand the reason for this difference. **Setup**. - Operating system: image of singularity, transforming from docker image of deeptrio-1.4.0. - DeepVariant version:deeptrio-1.4.0. - Installation method (Docker, built from source, etc.):Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). HiFi data,those data download links follows:. * HG002:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG002/hpp_HG002_NA24385_son_v1/PacBio_HiFi/15kb/;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG002/hpp_HG002_NA24385_son_v1/PacBio_HiFi/20kb/. * HG003:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG003/PacBio_HiFi/Google_15kb;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG003/PacBio_HiFi/HudsonAlpha_15kb. * HG004:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG004/PacBio_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:7729,safety,Error,Error,7729,"| 3872 | 4265460 | 4910 | 936912 | 1118 | 617 | 0.998836 | 0.998525 | 0.219651 | 0.998681 | 2.102574954 | 1.831128594 | 1.535137772 | 1.484295493 |. | HG004 | INDEL | PASS | 510519 | 507376 | 3143 | 1013737 | 4102 | 469356 | 1887 | 1729 | 0.993844 | 0.992465 | 0.462996 | 0.993154 | | | 1.516130736 | 2.075927402 |. analysising result：Using the same test data as the scattered samples, it can be found that the variation detection results of the HG002/3/4 family sample are relatively poor when tested using the GIAB standard set，but I don't understand the reason for this difference. **Setup**. - Operating system: image of singularity, transforming from docker image of deeptrio-1.4.0. - DeepVariant version:deeptrio-1.4.0. - Installation method (Docker, built from source, etc.):Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). HiFi data,those data download links follows:. * HG002:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG002/hpp_HG002_NA24385_son_v1/PacBio_HiFi/15kb/;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG002/hpp_HG002_NA24385_son_v1/PacBio_HiFi/20kb/. * HG003:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG003/PacBio_HiFi/Google_15kb;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG003/PacBio_HiFi/HudsonAlpha_15kb. * HG004:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG004/PacBio_HiFi/Google_15kb/;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG004/PacBio_HiFi/HudsonAlpha_15kb/PBmixSequel733_2_B01_PBSU_30hours_15kbV2PD_70pM_HumanHG004_CCS/. **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:7782,safety,test,test,7782,"| 3872 | 4265460 | 4910 | 936912 | 1118 | 617 | 0.998836 | 0.998525 | 0.219651 | 0.998681 | 2.102574954 | 1.831128594 | 1.535137772 | 1.484295493 |. | HG004 | INDEL | PASS | 510519 | 507376 | 3143 | 1013737 | 4102 | 469356 | 1887 | 1729 | 0.993844 | 0.992465 | 0.462996 | 0.993154 | | | 1.516130736 | 2.075927402 |. analysising result：Using the same test data as the scattered samples, it can be found that the variation detection results of the HG002/3/4 family sample are relatively poor when tested using the GIAB standard set，but I don't understand the reason for this difference. **Setup**. - Operating system: image of singularity, transforming from docker image of deeptrio-1.4.0. - DeepVariant version:deeptrio-1.4.0. - Installation method (Docker, built from source, etc.):Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). HiFi data,those data download links follows:. * HG002:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG002/hpp_HG002_NA24385_son_v1/PacBio_HiFi/15kb/;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG002/hpp_HG002_NA24385_son_v1/PacBio_HiFi/20kb/. * HG003:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG003/PacBio_HiFi/Google_15kb;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG003/PacBio_HiFi/HudsonAlpha_15kb. * HG004:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG004/PacBio_HiFi/Google_15kb/;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG004/PacBio_HiFi/HudsonAlpha_15kb/PBmixSequel733_2_B01_PBSU_30hours_15kbV2PD_70pM_HumanHG004_CCS/. **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:7818,safety,test,test,7818,"| 3872 | 4265460 | 4910 | 936912 | 1118 | 617 | 0.998836 | 0.998525 | 0.219651 | 0.998681 | 2.102574954 | 1.831128594 | 1.535137772 | 1.484295493 |. | HG004 | INDEL | PASS | 510519 | 507376 | 3143 | 1013737 | 4102 | 469356 | 1887 | 1729 | 0.993844 | 0.992465 | 0.462996 | 0.993154 | | | 1.516130736 | 2.075927402 |. analysising result：Using the same test data as the scattered samples, it can be found that the variation detection results of the HG002/3/4 family sample are relatively poor when tested using the GIAB standard set，but I don't understand the reason for this difference. **Setup**. - Operating system: image of singularity, transforming from docker image of deeptrio-1.4.0. - DeepVariant version:deeptrio-1.4.0. - Installation method (Docker, built from source, etc.):Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). HiFi data,those data download links follows:. * HG002:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG002/hpp_HG002_NA24385_son_v1/PacBio_HiFi/15kb/;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG002/hpp_HG002_NA24385_son_v1/PacBio_HiFi/20kb/. * HG003:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG003/PacBio_HiFi/Google_15kb;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG003/PacBio_HiFi/HudsonAlpha_15kb. * HG004:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG004/PacBio_HiFi/Google_15kb/;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG004/PacBio_HiFi/HudsonAlpha_15kb/PBmixSequel733_2_B01_PBSU_30hours_15kbV2PD_70pM_HumanHG004_CCS/. **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:45,security,detect,detection,45,"Feedback on poor results in family variation detection and GIAB dataset evaluation; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. yes，i have checked this FAQ document. . **Describe the issue:**. I used Deeprio v1.4.0 version to perform family analysis on three samples, HG002, HG003, and HG004, and evaluated the accuracy of mutation detection using the GIAB database (NISTv4.2.1). I found that the evaluation results through GIAB were particularly unsatisfactory, but I used the same data and Deepvariant v1.5.0 for single sample analysis, and the evaluation results were very ideal, I don't quite understand why the analysis results of a single sample perform so well at the evaluation level compared to the results of family analysis, and why the results of family analysis are relatively poor. The following is my family analysis and analysis code for individual samples, as well as the evaluation results of the GIAB database, for developers to review:. Data comparison to reference genome:. ```. echo HG002.merged.fastq.gz > HG002.fofn . pbmm2 align \. --preset HIFI \. genome/hg38.fa.mmi \. HG002.fofn \. --sample HG002 \. -j 10 \. HG002.aligned.tmp.bam . samtools sort -@ 10 HG002.aligned.tmp.bam -O BAM -o HG002.aligned.tmp.sort.bam . samtools index -@ 10 HG002.aligned.tmp.sort.bam . chromosomes=(chr1 chr2 chr3 chr4 chr5 chr6 chr7 chr8 chr9 chr10 chr11 chr12 chr13 chr14 chr15 chr16 chr17 chr18 chr19 chr20 chr21 chr22 chrX chrY chrM) . for chromosome in ""${chromosomes[@]}""; \. do \. samtools view -@ 2 -b -h HG002.aligned.tmp.sort.bam ""$chromosome"" --output HG002.aligned.$chromosome.tmp.bam & . done . wait . samtools merge HG002.aligned.bam HG002.aligned.chr*.tmp.bam . samtools sort -@ 10 HG002.aligned.bam -O BAM -o HG002.sort.bam . samtools index -@ 10 HG002.sort.bam . ```. Family analysis code:. ```. rm -rf chr20_GLnexus.DB tmp_ramdom_TrioDemo_chr20 . samtools view --write-index --threads 10 -h -b -S HG002.sort.bam chr20 -O BAM -o ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:380,security,detect,detection,380,"Feedback on poor results in family variation detection and GIAB dataset evaluation; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. yes，i have checked this FAQ document. . **Describe the issue:**. I used Deeprio v1.4.0 version to perform family analysis on three samples, HG002, HG003, and HG004, and evaluated the accuracy of mutation detection using the GIAB database (NISTv4.2.1). I found that the evaluation results through GIAB were particularly unsatisfactory, but I used the same data and Deepvariant v1.5.0 for single sample analysis, and the evaluation results were very ideal, I don't quite understand why the analysis results of a single sample perform so well at the evaluation level compared to the results of family analysis, and why the results of family analysis are relatively poor. The following is my family analysis and analysis code for individual samples, as well as the evaluation results of the GIAB database, for developers to review:. Data comparison to reference genome:. ```. echo HG002.merged.fastq.gz > HG002.fofn . pbmm2 align \. --preset HIFI \. genome/hg38.fa.mmi \. HG002.fofn \. --sample HG002 \. -j 10 \. HG002.aligned.tmp.bam . samtools sort -@ 10 HG002.aligned.tmp.bam -O BAM -o HG002.aligned.tmp.sort.bam . samtools index -@ 10 HG002.aligned.tmp.sort.bam . chromosomes=(chr1 chr2 chr3 chr4 chr5 chr6 chr7 chr8 chr9 chr10 chr11 chr12 chr13 chr14 chr15 chr16 chr17 chr18 chr19 chr20 chr21 chr22 chrX chrY chrM) . for chromosome in ""${chromosomes[@]}""; \. do \. samtools view -@ 2 -b -h HG002.aligned.tmp.sort.bam ""$chromosome"" --output HG002.aligned.$chromosome.tmp.bam & . done . wait . samtools merge HG002.aligned.bam HG002.aligned.chr*.tmp.bam . samtools sort -@ 10 HG002.aligned.bam -O BAM -o HG002.sort.bam . samtools index -@ 10 HG002.sort.bam . ```. Family analysis code:. ```. rm -rf chr20_GLnexus.DB tmp_ramdom_TrioDemo_chr20 . samtools view --write-index --threads 10 -h -b -S HG002.sort.bam chr20 -O BAM -o ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:3337,security,detect,detection,3337,iant/bin/deeptrio/run_deeptrio \. --model_type PACBIO \. --ref=genome/hg38.fa \. --reads_child HG002.chr20.sort.bam \. --reads_parent1 HG003.chr20.sort.bam \. --reads_parent2 HG004.chr20.sort.bam \. --output_vcf_child HG002.chr20.output.vcf.gz \. --output_vcf_parent1 HG003.chr20.output.vcf.gz \. --output_vcf_parent2 HG004.chr20.output.vcf.gz \. --sample_name_child 'HG002' \. --sample_name_parent1 'HG003' \. --sample_name_parent2 'HG004' \. --num_shards 10 \. --intermediate_results_dir tmp_ramdom_TrioDemo_chr20 \. --output_gvcf_child HG002.chr20.g.vcf.gz \. --output_gvcf_parent1 HG003.chr20.g.vcf.gz \. --output_gvcf_parent2 HG004.chr20.g.vcf.gz . glnexus_cli_v1.4.1 \. --config DeepVariant_unfiltered \. --squeeze \. --dir chr20_GLnexus.DB \. HG002.chr20.g.vcf.gz HG003.chr20.g.vcf.gz HG004.chr20.g.vcf.gz \. --threads 10 | \. /opt/conda/envs/bio/bin/bcftools view \. --threads 10 -O z \. -o TrioDemo_chr20.trio_merged.vcf.gz - . ```. The following is the code for single sample mutation detection:. ```. samtools view --threads 10 -h -b -S HG002.sort.bam chr20 > HG002/HG002.chr20.sort.bam . samtools index -@ 10 HG002/HG002.chr20.sort.bam . singularity run /share:/share Singularity/google-deepvariant.1.5.0.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref genome/hg38.fa \. --intermediate_results_dir HG002/tmp_ramdom_HG002_chr20 \. --reads HG002/HG002.chr20.sort.bam \. --output_vcf HG002/HG002.chr20.vcf.gz \. --num_shards 10 . rm -fr HG002/tmp_ramdom_HG002_chr20. ```. The evaluation results of the trio family sample are as follows:. | HG002 | INDEL | PASS | 525469 | 190669 | 334800 | 361066 | 7821 | 151401 | 4826 | 2317 | 0.362855 | 0.962698 | 0.419317 | 0.527055 | | | 1.528275734 | 1.988539738 |. |-------|-------|------|---------|---------|---------|---------|-------|--------|-------|------|----------|----------|----------|----------|-------------|------------|-------------|-------------|. | HG002 | SNP | PASS | 3365127 | 1236543 | 2128584 | 1466059,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:6425,security,detect,detection,6425,"----|------|---------|---------|------|---------|------|--------|------|------|----------|----------|----------|----------|-------------|-------------|-------------|-------------|. | HG002 | SNP | PASS | 3365127 | 3361925 | 3202 | 4312006 | 7891 | 942045 | 1197 | 719 | 0.999048 | 0.997658 | 0.21847 | 0.998353 | 2.100128487 | 1.823839956 | 1.581195853 | 1.50423718 |. | HG003 | INDEL | PASS | 504501 | 501414 | 3087 | 1009147 | 3989 | 471526 | 1814 | 1831 | 0.993881 | 0.99258 | 0.467252 | 0.99323 | | | 1.489759281 | 2.02565724 |. | HG003 | SNP | PASS | 3327495 | 3323623 | 3872 | 4265460 | 4910 | 936912 | 1118 | 617 | 0.998836 | 0.998525 | 0.219651 | 0.998681 | 2.102574954 | 1.831128594 | 1.535137772 | 1.484295493 |. | HG004 | INDEL | PASS | 510519 | 507376 | 3143 | 1013737 | 4102 | 469356 | 1887 | 1729 | 0.993844 | 0.992465 | 0.462996 | 0.993154 | | | 1.516130736 | 2.075927402 |. analysising result：Using the same test data as the scattered samples, it can be found that the variation detection results of the HG002/3/4 family sample are relatively poor when tested using the GIAB standard set，but I don't understand the reason for this difference. **Setup**. - Operating system: image of singularity, transforming from docker image of deeptrio-1.4.0. - DeepVariant version:deeptrio-1.4.0. - Installation method (Docker, built from source, etc.):Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). HiFi data,those data download links follows:. * HG002:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG002/hpp_HG002_NA24385_son_v1/PacBio_HiFi/15kb/;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG002/hpp_HG002_NA24385_son_v1/PacBio_HiFi/20kb/. * HG003:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG003/PacBio_HiFi/Google_15kb;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG003/PacBio_HiFi/HudsonAlpha_15kb. * HG004:https://s3-",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:645,testability,understand,understand,645,"Feedback on poor results in family variation detection and GIAB dataset evaluation; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. yes，i have checked this FAQ document. . **Describe the issue:**. I used Deeprio v1.4.0 version to perform family analysis on three samples, HG002, HG003, and HG004, and evaluated the accuracy of mutation detection using the GIAB database (NISTv4.2.1). I found that the evaluation results through GIAB were particularly unsatisfactory, but I used the same data and Deepvariant v1.5.0 for single sample analysis, and the evaluation results were very ideal, I don't quite understand why the analysis results of a single sample perform so well at the evaluation level compared to the results of family analysis, and why the results of family analysis are relatively poor. The following is my family analysis and analysis code for individual samples, as well as the evaluation results of the GIAB database, for developers to review:. Data comparison to reference genome:. ```. echo HG002.merged.fastq.gz > HG002.fofn . pbmm2 align \. --preset HIFI \. genome/hg38.fa.mmi \. HG002.fofn \. --sample HG002 \. -j 10 \. HG002.aligned.tmp.bam . samtools sort -@ 10 HG002.aligned.tmp.bam -O BAM -o HG002.aligned.tmp.sort.bam . samtools index -@ 10 HG002.aligned.tmp.sort.bam . chromosomes=(chr1 chr2 chr3 chr4 chr5 chr6 chr7 chr8 chr9 chr10 chr11 chr12 chr13 chr14 chr15 chr16 chr17 chr18 chr19 chr20 chr21 chr22 chrX chrY chrM) . for chromosome in ""${chromosomes[@]}""; \. do \. samtools view -@ 2 -b -h HG002.aligned.tmp.sort.bam ""$chromosome"" --output HG002.aligned.$chromosome.tmp.bam & . done . wait . samtools merge HG002.aligned.bam HG002.aligned.chr*.tmp.bam . samtools sort -@ 10 HG002.aligned.bam -O BAM -o HG002.sort.bam . samtools index -@ 10 HG002.sort.bam . ```. Family analysis code:. ```. rm -rf chr20_GLnexus.DB tmp_ramdom_TrioDemo_chr20 . samtools view --write-index --threads 10 -h -b -S HG002.sort.bam chr20 -O BAM -o ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:996,testability,review,review,996,"Feedback on poor results in family variation detection and GIAB dataset evaluation; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. yes，i have checked this FAQ document. . **Describe the issue:**. I used Deeprio v1.4.0 version to perform family analysis on three samples, HG002, HG003, and HG004, and evaluated the accuracy of mutation detection using the GIAB database (NISTv4.2.1). I found that the evaluation results through GIAB were particularly unsatisfactory, but I used the same data and Deepvariant v1.5.0 for single sample analysis, and the evaluation results were very ideal, I don't quite understand why the analysis results of a single sample perform so well at the evaluation level compared to the results of family analysis, and why the results of family analysis are relatively poor. The following is my family analysis and analysis code for individual samples, as well as the evaluation results of the GIAB database, for developers to review:. Data comparison to reference genome:. ```. echo HG002.merged.fastq.gz > HG002.fofn . pbmm2 align \. --preset HIFI \. genome/hg38.fa.mmi \. HG002.fofn \. --sample HG002 \. -j 10 \. HG002.aligned.tmp.bam . samtools sort -@ 10 HG002.aligned.tmp.bam -O BAM -o HG002.aligned.tmp.sort.bam . samtools index -@ 10 HG002.aligned.tmp.sort.bam . chromosomes=(chr1 chr2 chr3 chr4 chr5 chr6 chr7 chr8 chr9 chr10 chr11 chr12 chr13 chr14 chr15 chr16 chr17 chr18 chr19 chr20 chr21 chr22 chrX chrY chrM) . for chromosome in ""${chromosomes[@]}""; \. do \. samtools view -@ 2 -b -h HG002.aligned.tmp.sort.bam ""$chromosome"" --output HG002.aligned.$chromosome.tmp.bam & . done . wait . samtools merge HG002.aligned.bam HG002.aligned.chr*.tmp.bam . samtools sort -@ 10 HG002.aligned.bam -O BAM -o HG002.sort.bam . samtools index -@ 10 HG002.sort.bam . ```. Family analysis code:. ```. rm -rf chr20_GLnexus.DB tmp_ramdom_TrioDemo_chr20 . samtools view --write-index --threads 10 -h -b -S HG002.sort.bam chr20 -O BAM -o ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:6354,testability,test,test,6354,"82227 | 0.444908 | 0.98379 | | | 1.528275734 | 2.114606634 |. |-------|-------|------|---------|---------|------|---------|------|--------|------|------|----------|----------|----------|----------|-------------|-------------|-------------|-------------|. | HG002 | SNP | PASS | 3365127 | 3361925 | 3202 | 4312006 | 7891 | 942045 | 1197 | 719 | 0.999048 | 0.997658 | 0.21847 | 0.998353 | 2.100128487 | 1.823839956 | 1.581195853 | 1.50423718 |. | HG003 | INDEL | PASS | 504501 | 501414 | 3087 | 1009147 | 3989 | 471526 | 1814 | 1831 | 0.993881 | 0.99258 | 0.467252 | 0.99323 | | | 1.489759281 | 2.02565724 |. | HG003 | SNP | PASS | 3327495 | 3323623 | 3872 | 4265460 | 4910 | 936912 | 1118 | 617 | 0.998836 | 0.998525 | 0.219651 | 0.998681 | 2.102574954 | 1.831128594 | 1.535137772 | 1.484295493 |. | HG004 | INDEL | PASS | 510519 | 507376 | 3143 | 1013737 | 4102 | 469356 | 1887 | 1729 | 0.993844 | 0.992465 | 0.462996 | 0.993154 | | | 1.516130736 | 2.075927402 |. analysising result：Using the same test data as the scattered samples, it can be found that the variation detection results of the HG002/3/4 family sample are relatively poor when tested using the GIAB standard set，but I don't understand the reason for this difference. **Setup**. - Operating system: image of singularity, transforming from docker image of deeptrio-1.4.0. - DeepVariant version:deeptrio-1.4.0. - Installation method (Docker, built from source, etc.):Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). HiFi data,those data download links follows:. * HG002:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG002/hpp_HG002_NA24385_son_v1/PacBio_HiFi/15kb/;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG002/hpp_HG002_NA24385_son_v1/PacBio_HiFi/20kb/. * HG003:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG003/PacBio_HiFi/Google_15kb;https://s3-us-west-2.amazonaws.com/human-pangenomic",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:6499,testability,test,tested,6499,"------|----------|----------|----------|----------|-------------|-------------|-------------|-------------|. | HG002 | SNP | PASS | 3365127 | 3361925 | 3202 | 4312006 | 7891 | 942045 | 1197 | 719 | 0.999048 | 0.997658 | 0.21847 | 0.998353 | 2.100128487 | 1.823839956 | 1.581195853 | 1.50423718 |. | HG003 | INDEL | PASS | 504501 | 501414 | 3087 | 1009147 | 3989 | 471526 | 1814 | 1831 | 0.993881 | 0.99258 | 0.467252 | 0.99323 | | | 1.489759281 | 2.02565724 |. | HG003 | SNP | PASS | 3327495 | 3323623 | 3872 | 4265460 | 4910 | 936912 | 1118 | 617 | 0.998836 | 0.998525 | 0.219651 | 0.998681 | 2.102574954 | 1.831128594 | 1.535137772 | 1.484295493 |. | HG004 | INDEL | PASS | 510519 | 507376 | 3143 | 1013737 | 4102 | 469356 | 1887 | 1729 | 0.993844 | 0.992465 | 0.462996 | 0.993154 | | | 1.516130736 | 2.075927402 |. analysising result：Using the same test data as the scattered samples, it can be found that the variation detection results of the HG002/3/4 family sample are relatively poor when tested using the GIAB standard set，but I don't understand the reason for this difference. **Setup**. - Operating system: image of singularity, transforming from docker image of deeptrio-1.4.0. - DeepVariant version:deeptrio-1.4.0. - Installation method (Docker, built from source, etc.):Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). HiFi data,those data download links follows:. * HG002:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG002/hpp_HG002_NA24385_son_v1/PacBio_HiFi/15kb/;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG002/hpp_HG002_NA24385_son_v1/PacBio_HiFi/20kb/. * HG003:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG003/PacBio_HiFi/Google_15kb;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG003/PacBio_HiFi/HudsonAlpha_15kb. * HG004:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG004/PacBio_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:6546,testability,understand,understand,6546,"-|-------------|-------------|-------------|-------------|. | HG002 | SNP | PASS | 3365127 | 3361925 | 3202 | 4312006 | 7891 | 942045 | 1197 | 719 | 0.999048 | 0.997658 | 0.21847 | 0.998353 | 2.100128487 | 1.823839956 | 1.581195853 | 1.50423718 |. | HG003 | INDEL | PASS | 504501 | 501414 | 3087 | 1009147 | 3989 | 471526 | 1814 | 1831 | 0.993881 | 0.99258 | 0.467252 | 0.99323 | | | 1.489759281 | 2.02565724 |. | HG003 | SNP | PASS | 3327495 | 3323623 | 3872 | 4265460 | 4910 | 936912 | 1118 | 617 | 0.998836 | 0.998525 | 0.219651 | 0.998681 | 2.102574954 | 1.831128594 | 1.535137772 | 1.484295493 |. | HG004 | INDEL | PASS | 510519 | 507376 | 3143 | 1013737 | 4102 | 469356 | 1887 | 1729 | 0.993844 | 0.992465 | 0.462996 | 0.993154 | | | 1.516130736 | 2.075927402 |. analysising result：Using the same test data as the scattered samples, it can be found that the variation detection results of the HG002/3/4 family sample are relatively poor when tested using the GIAB standard set，but I don't understand the reason for this difference. **Setup**. - Operating system: image of singularity, transforming from docker image of deeptrio-1.4.0. - DeepVariant version:deeptrio-1.4.0. - Installation method (Docker, built from source, etc.):Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). HiFi data,those data download links follows:. * HG002:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG002/hpp_HG002_NA24385_son_v1/PacBio_HiFi/15kb/;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG002/hpp_HG002_NA24385_son_v1/PacBio_HiFi/20kb/. * HG003:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG003/PacBio_HiFi/Google_15kb;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG003/PacBio_HiFi/HudsonAlpha_15kb. * HG004:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG004/PacBio_HiFi/Google_15kb/;https://s3-us-west-2.amazonaws.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:6822,testability,instrument,instrument,6822,"501 | 501414 | 3087 | 1009147 | 3989 | 471526 | 1814 | 1831 | 0.993881 | 0.99258 | 0.467252 | 0.99323 | | | 1.489759281 | 2.02565724 |. | HG003 | SNP | PASS | 3327495 | 3323623 | 3872 | 4265460 | 4910 | 936912 | 1118 | 617 | 0.998836 | 0.998525 | 0.219651 | 0.998681 | 2.102574954 | 1.831128594 | 1.535137772 | 1.484295493 |. | HG004 | INDEL | PASS | 510519 | 507376 | 3143 | 1013737 | 4102 | 469356 | 1887 | 1729 | 0.993844 | 0.992465 | 0.462996 | 0.993154 | | | 1.516130736 | 2.075927402 |. analysising result：Using the same test data as the scattered samples, it can be found that the variation detection results of the HG002/3/4 family sample are relatively poor when tested using the GIAB standard set，but I don't understand the reason for this difference. **Setup**. - Operating system: image of singularity, transforming from docker image of deeptrio-1.4.0. - DeepVariant version:deeptrio-1.4.0. - Installation method (Docker, built from source, etc.):Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). HiFi data,those data download links follows:. * HG002:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG002/hpp_HG002_NA24385_son_v1/PacBio_HiFi/15kb/;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG002/hpp_HG002_NA24385_son_v1/PacBio_HiFi/20kb/. * HG003:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG003/PacBio_HiFi/Google_15kb;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG003/PacBio_HiFi/HudsonAlpha_15kb. * HG004:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG004/PacBio_HiFi/Google_15kb/;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG004/PacBio_HiFi/HudsonAlpha_15kb/PBmixSequel733_2_B01_PBSU_30hours_15kbV2PD_70pM_HumanHG004_CCS/. **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:7735,testability,trace,trace,7735,"| 3872 | 4265460 | 4910 | 936912 | 1118 | 617 | 0.998836 | 0.998525 | 0.219651 | 0.998681 | 2.102574954 | 1.831128594 | 1.535137772 | 1.484295493 |. | HG004 | INDEL | PASS | 510519 | 507376 | 3143 | 1013737 | 4102 | 469356 | 1887 | 1729 | 0.993844 | 0.992465 | 0.462996 | 0.993154 | | | 1.516130736 | 2.075927402 |. analysising result：Using the same test data as the scattered samples, it can be found that the variation detection results of the HG002/3/4 family sample are relatively poor when tested using the GIAB standard set，but I don't understand the reason for this difference. **Setup**. - Operating system: image of singularity, transforming from docker image of deeptrio-1.4.0. - DeepVariant version:deeptrio-1.4.0. - Installation method (Docker, built from source, etc.):Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). HiFi data,those data download links follows:. * HG002:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG002/hpp_HG002_NA24385_son_v1/PacBio_HiFi/15kb/;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG002/hpp_HG002_NA24385_son_v1/PacBio_HiFi/20kb/. * HG003:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG003/PacBio_HiFi/Google_15kb;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG003/PacBio_HiFi/HudsonAlpha_15kb. * HG004:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG004/PacBio_HiFi/Google_15kb/;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG004/PacBio_HiFi/HudsonAlpha_15kb/PBmixSequel733_2_B01_PBSU_30hours_15kbV2PD_70pM_HumanHG004_CCS/. **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:7782,testability,test,test,7782,"| 3872 | 4265460 | 4910 | 936912 | 1118 | 617 | 0.998836 | 0.998525 | 0.219651 | 0.998681 | 2.102574954 | 1.831128594 | 1.535137772 | 1.484295493 |. | HG004 | INDEL | PASS | 510519 | 507376 | 3143 | 1013737 | 4102 | 469356 | 1887 | 1729 | 0.993844 | 0.992465 | 0.462996 | 0.993154 | | | 1.516130736 | 2.075927402 |. analysising result：Using the same test data as the scattered samples, it can be found that the variation detection results of the HG002/3/4 family sample are relatively poor when tested using the GIAB standard set，but I don't understand the reason for this difference. **Setup**. - Operating system: image of singularity, transforming from docker image of deeptrio-1.4.0. - DeepVariant version:deeptrio-1.4.0. - Installation method (Docker, built from source, etc.):Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). HiFi data,those data download links follows:. * HG002:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG002/hpp_HG002_NA24385_son_v1/PacBio_HiFi/15kb/;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG002/hpp_HG002_NA24385_son_v1/PacBio_HiFi/20kb/. * HG003:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG003/PacBio_HiFi/Google_15kb;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG003/PacBio_HiFi/HudsonAlpha_15kb. * HG004:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG004/PacBio_HiFi/Google_15kb/;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG004/PacBio_HiFi/HudsonAlpha_15kb/PBmixSequel733_2_B01_PBSU_30hours_15kbV2PD_70pM_HumanHG004_CCS/. **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:7818,testability,test,test,7818,"| 3872 | 4265460 | 4910 | 936912 | 1118 | 617 | 0.998836 | 0.998525 | 0.219651 | 0.998681 | 2.102574954 | 1.831128594 | 1.535137772 | 1.484295493 |. | HG004 | INDEL | PASS | 510519 | 507376 | 3143 | 1013737 | 4102 | 469356 | 1887 | 1729 | 0.993844 | 0.992465 | 0.462996 | 0.993154 | | | 1.516130736 | 2.075927402 |. analysising result：Using the same test data as the scattered samples, it can be found that the variation detection results of the HG002/3/4 family sample are relatively poor when tested using the GIAB standard set，but I don't understand the reason for this difference. **Setup**. - Operating system: image of singularity, transforming from docker image of deeptrio-1.4.0. - DeepVariant version:deeptrio-1.4.0. - Installation method (Docker, built from source, etc.):Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). HiFi data,those data download links follows:. * HG002:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG002/hpp_HG002_NA24385_son_v1/PacBio_HiFi/15kb/;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG002/hpp_HG002_NA24385_son_v1/PacBio_HiFi/20kb/. * HG003:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG003/PacBio_HiFi/Google_15kb;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG003/PacBio_HiFi/HudsonAlpha_15kb. * HG004:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG004/PacBio_HiFi/Google_15kb/;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG004/PacBio_HiFi/HudsonAlpha_15kb/PBmixSequel733_2_B01_PBSU_30hours_15kbV2PD_70pM_HumanHG004_CCS/. **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:7993,testability,context,context,7993,"| 3872 | 4265460 | 4910 | 936912 | 1118 | 617 | 0.998836 | 0.998525 | 0.219651 | 0.998681 | 2.102574954 | 1.831128594 | 1.535137772 | 1.484295493 |. | HG004 | INDEL | PASS | 510519 | 507376 | 3143 | 1013737 | 4102 | 469356 | 1887 | 1729 | 0.993844 | 0.992465 | 0.462996 | 0.993154 | | | 1.516130736 | 2.075927402 |. analysising result：Using the same test data as the scattered samples, it can be found that the variation detection results of the HG002/3/4 family sample are relatively poor when tested using the GIAB standard set，but I don't understand the reason for this difference. **Setup**. - Operating system: image of singularity, transforming from docker image of deeptrio-1.4.0. - DeepVariant version:deeptrio-1.4.0. - Installation method (Docker, built from source, etc.):Docker. - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). HiFi data,those data download links follows:. * HG002:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG002/hpp_HG002_NA24385_son_v1/PacBio_HiFi/15kb/;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG002/hpp_HG002_NA24385_son_v1/PacBio_HiFi/20kb/. * HG003:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG003/PacBio_HiFi/Google_15kb;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG003/PacBio_HiFi/HudsonAlpha_15kb. * HG004:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG004/PacBio_HiFi/Google_15kb/;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG004/PacBio_HiFi/HudsonAlpha_15kb/PBmixSequel733_2_B01_PBSU_30hours_15kbV2PD_70pM_HumanHG004_CCS/. **Steps to reproduce:**. - Command:. - Error trace: (if applicable). **Does the quick start test work on your system?**. Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. Is there any way to reproduce the issue by using the quick start? **Any additional context:**.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:0,usability,Feedback,Feedback,0,"Feedback on poor results in family variation detection and GIAB dataset evaluation; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. yes，i have checked this FAQ document. . **Describe the issue:**. I used Deeprio v1.4.0 version to perform family analysis on three samples, HG002, HG003, and HG004, and evaluated the accuracy of mutation detection using the GIAB database (NISTv4.2.1). I found that the evaluation results through GIAB were particularly unsatisfactory, but I used the same data and Deepvariant v1.5.0 for single sample analysis, and the evaluation results were very ideal, I don't quite understand why the analysis results of a single sample perform so well at the evaluation level compared to the results of family analysis, and why the results of family analysis are relatively poor. The following is my family analysis and analysis code for individual samples, as well as the evaluation results of the GIAB database, for developers to review:. Data comparison to reference genome:. ```. echo HG002.merged.fastq.gz > HG002.fofn . pbmm2 align \. --preset HIFI \. genome/hg38.fa.mmi \. HG002.fofn \. --sample HG002 \. -j 10 \. HG002.aligned.tmp.bam . samtools sort -@ 10 HG002.aligned.tmp.bam -O BAM -o HG002.aligned.tmp.sort.bam . samtools index -@ 10 HG002.aligned.tmp.sort.bam . chromosomes=(chr1 chr2 chr3 chr4 chr5 chr6 chr7 chr8 chr9 chr10 chr11 chr12 chr13 chr14 chr15 chr16 chr17 chr18 chr19 chr20 chr21 chr22 chrX chrY chrM) . for chromosome in ""${chromosomes[@]}""; \. do \. samtools view -@ 2 -b -h HG002.aligned.tmp.sort.bam ""$chromosome"" --output HG002.aligned.$chromosome.tmp.bam & . done . wait . samtools merge HG002.aligned.bam HG002.aligned.chr*.tmp.bam . samtools sort -@ 10 HG002.aligned.bam -O BAM -o HG002.sort.bam . samtools index -@ 10 HG002.sort.bam . ```. Family analysis code:. ```. rm -rf chr20_GLnexus.DB tmp_ramdom_TrioDemo_chr20 . samtools view --write-index --threads 10 -h -b -S HG002.sort.bam chr20 -O BAM -o ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
https://github.com/google/deepvariant/issues/689:204,usability,document,document,204,"Feedback on poor results in family variation detection and GIAB dataset evaluation; **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. yes，i have checked this FAQ document. . **Describe the issue:**. I used Deeprio v1.4.0 version to perform family analysis on three samples, HG002, HG003, and HG004, and evaluated the accuracy of mutation detection using the GIAB database (NISTv4.2.1). I found that the evaluation results through GIAB were particularly unsatisfactory, but I used the same data and Deepvariant v1.5.0 for single sample analysis, and the evaluation results were very ideal, I don't quite understand why the analysis results of a single sample perform so well at the evaluation level compared to the results of family analysis, and why the results of family analysis are relatively poor. The following is my family analysis and analysis code for individual samples, as well as the evaluation results of the GIAB database, for developers to review:. Data comparison to reference genome:. ```. echo HG002.merged.fastq.gz > HG002.fofn . pbmm2 align \. --preset HIFI \. genome/hg38.fa.mmi \. HG002.fofn \. --sample HG002 \. -j 10 \. HG002.aligned.tmp.bam . samtools sort -@ 10 HG002.aligned.tmp.bam -O BAM -o HG002.aligned.tmp.sort.bam . samtools index -@ 10 HG002.aligned.tmp.sort.bam . chromosomes=(chr1 chr2 chr3 chr4 chr5 chr6 chr7 chr8 chr9 chr10 chr11 chr12 chr13 chr14 chr15 chr16 chr17 chr18 chr19 chr20 chr21 chr22 chrX chrY chrM) . for chromosome in ""${chromosomes[@]}""; \. do \. samtools view -@ 2 -b -h HG002.aligned.tmp.sort.bam ""$chromosome"" --output HG002.aligned.$chromosome.tmp.bam & . done . wait . samtools merge HG002.aligned.bam HG002.aligned.chr*.tmp.bam . samtools sort -@ 10 HG002.aligned.bam -O BAM -o HG002.sort.bam . samtools index -@ 10 HG002.sort.bam . ```. Family analysis code:. ```. rm -rf chr20_GLnexus.DB tmp_ramdom_TrioDemo_chr20 . samtools view --write-index --threads 10 -h -b -S HG002.sort.bam chr20 -O BAM -o ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/689
